{"config": {"lang": ["en"], "separator": "[\\s\\-]+", "pipeline": ["stopWordFilter"]}, "docs": [{"location": "", "title": "Introduction", "text": "<p>This is my personal digital garden where I share everything I learn about a huge variety of topics, from antifascism and other forms of activism, to life management, health, Python, DevOps, software architecture, writing, dancing, or data analysis.</p> <p>Unlike in common blogging where you write an article and forget about it, posts are treated as plants in various stages of growth and nurturing. Some might wither and die, and others will flourish and provide a source of continued knowledge for the gardener and folks in the community that visit.</p> <p>To follow the updates of this site, subscribe to any of the RSS feeds.</p>"}, {"location": "#visiting-the-garden", "title": "Visiting the garden", "text": "<p>If this is your first visit, welcome!, you may be overwhelmed by the amount of content and don't know where to start reading. Start with the first article that grabs your attention of the navigation tree on the left and be ready to follow the internal links to read the rest.</p> <p>Or you can use it as a reference, by using the top search field or by cloning the git repository and using grep like tools.</p>"}, {"location": "#make-your-own-digital-garden", "title": "Make your own digital garden", "text": "<p>Don't be afraid to create one of your own and share what you know with the world. Or contribute to existing ones. I would love to see the blue-book maintained by other people too.</p> <p>If you don't want to start from scratch, you can fork the blue book, change the name and start writing straight away.</p> <p>You can view other similar gardens to get inspiration.</p>"}, {"location": "#history", "title": "History", "text": "<p>I've tried writing blogs in the past, but it doesn't work for me. I can't stand the idea of having to save some time to sit and write a full article of something I've done. I need to document at the same time as I develop or learn. As I usually use incremental reading or work on several projects, I don't write one article, but improve several at the same time in a unordered way. That's why I embrace Gwern's Long Content principle.</p> <p>The only drawback of this format is that there is no friendly way for a reader to keep updated with the changes, that's why I created mkdocs-newsletter.</p> <p>In 2016 I started writing in text files summaries of different concepts, how to install or how to use tools. In the beginning it was plaintext, then came Markdown, then Asciidoc, I did several refactors to reorder the different articles based in different structured ways, but I always did it with myself as the only target audience.</p> <p>Three years, 7422 articles and almost 50 million lines later, I found Gwern's website and Nikita's wiki, which made me think that it was time to do another refactor to give my wiki a semantical structure, go beyond a simple reference making it readable and open it to the internet.</p> <p>And the blue book was born.</p>"}, {"location": "#contributing", "title": "Contributing", "text": "<p>If you find a mistake or want to add new content, please make the changes. You can use the edit button on the top right of any article to add them in a pull request, if you don't know what that means, you can always open an issue or send me an email.</p>"}, {"location": "#thank-you", "title": "Thank you", "text": "<p>If you liked my book and want to show your support, please see if you know how can I fulfill any item of my wish list or contribute to my other projects.</p> <p>Another way to say \"thanks\" is to give me a donation.</p> <p>You can use</p> <p>or </p> <p>If you are using some of my open-source tools or documentation, have enjoyed them, and want to say \"thanks\", this is a very strong way to do it.</p> <p>If your product/company depends on these tools, you can sponsor me to ensure I keep happily maintaining them.</p> <p>If these tools are helping you save money, time, effort, or frustrations; or they are helping you make money, be more productive, efficient, secure, enjoy a bit more your work, or get your product ready faster, this is a great way to show your appreciation. Thanks for that!</p> <p></p>"}, {"location": "abstract_syntax_trees/", "title": "Abstract syntax trees", "text": "<p>Abstract syntax trees (AST) is a tree representation of the abstract syntactic structure of text (often source code) written in a formal language. Each node of the tree denotes a construct occurring in the text.</p> <p>The syntax is \"abstract\" in the sense that it does not represent every detail appearing in the real syntax, but rather just the structural or content-related details. For instance, grouping parentheses are implicit in the tree structure, so these do not have to be represented as separate nodes. Likewise, a syntactic construct like an if-condition-then statement may be denoted by means of a single node with three branches.</p> <p>This distinguishes abstract syntax trees from concrete syntax trees, traditionally designated parse trees. Parse trees are typically built by a parser during the source code translation and compiling process. Once built, additional information is added to the AST by means of subsequent processing, e.g., contextual analysis.</p> <p>Abstract syntax trees are also used in program analysis and program transformation systems.</p>"}, {"location": "abstract_syntax_trees/#how-to-construct-an-ast", "title": "How to construct an AST", "text": "<p>TBD</p> <p><code>pyparsing</code> looks to be a good candidate to implement ASTs.</p>"}, {"location": "activitywatch/", "title": "ActivityWatch", "text": "<p>ActivityWatch is a bundle of software that tracks your computer activity. You are, by default, the sole owner of your data.</p> <p>ActivityWatch is:</p> <ul> <li>A set of watchers that record relevant information about what you do and what     happens on your computer (such as if you are AFK or not, or which window is     currently active).</li> <li>A way of storing data collected by the watchers.</li> <li>A dataformat accomodating most logging needs due to its flexibility.</li> <li>An ecosystem of tools to help users extend the software to fit their needs.</li> </ul>"}, {"location": "activitywatch/#installation", "title": "Installation", "text": "<ul> <li>Download the latest release</li> <li>Unpack it and move it for example to <code>~/.local/bin/activitywatch</code>.</li> <li>Add the <code>aw-qt</code> executable to the autostart.</li> </ul> <p>It will start the web interface at http://localhost:5600 and will capture the data.</p>"}, {"location": "activitywatch/#configuration", "title": "Configuration", "text": "<p>First go to the <code>settings</code> page of the Web UI, you can define there the rules for the categories.</p> <p>More advanced settings can be changed on the files, but I had no need to go there yet.</p> <p>The used directories are:</p> <ul> <li>Data: <code>~/.local/share/activitywatch</code>.</li> <li>Config: <code>~/.config/activitywatch</code>.</li> <li>Logs: <code>~/.cache/activitywatch/log</code>.</li> <li>Cache: <code>~/.cache/activitywatch</code>.</li> </ul>"}, {"location": "activitywatch/#watchers", "title": "Watchers", "text": "<p>By default ActivityWatch comes with the next watchers:</p> <ul> <li>aw-watcher-afk: Watches for     mouse &amp; keyboard activity to detect if the user is active.</li> <li>aw-watcher-window:     Watches the active window and its title.</li> </ul> <p>But you can add more, such as:</p> <ul> <li> <p>aw-watcher-web: The     official browser extension, supports Chrome and Firefox. Watches properties     of the active tab like title, URL, and incognito state.</p> <p>It doesn't work if you Configure it to Never remember history, or if you use incognito mode</p> <p>It's known not to be very accurate. The overall time spent in the browser shown by the <code>aw-watcher-window</code> is greater than the one shown in <code>aw-watcher-web-firefox</code>.</p> </li> <li> <p>aw-watcher-vim: Watches the     actively edited file and associated metadata like path, language, and     project name (folder name of git root).</p> <p>It's impressive, plug and play:</p> <p></p> <p>It still doesn't add the branch information, it could be useful to give hints of what task you're working on inside a project.</p> </li> </ul> <p>They even show you how to create your own watcher.</p>"}, {"location": "activitywatch/#syncing", "title": "Syncing", "text": "<p>There is currently no syncing support. You'll need to export the data (under <code>Raw Data</code>, <code>Export all buckets as JSON</code>), and either tweak it so it can be imported, or analyze the data through other processes.</p>"}, {"location": "activitywatch/#issues", "title": "Issues", "text": "<ul> <li>Syncing support:     See how to merge the data from the different devices.</li> <li>Firefox not logging     data: Once it's     solved, try it again.</li> <li>Making it work in incognito     mode: Try it once     it's solved.</li> <li>Add branch information in vim     watcher: try it     once it's out.</li> <li>Web tracking is not     accurate: Test     the solution once it's implemented.</li> <li>Physical Activity Monitor Integration     (gadgetbridge):     Try it once there is a solution.</li> </ul>"}, {"location": "activitywatch/#references", "title": "References", "text": "<ul> <li>Home</li> <li>Docs</li> <li>Git</li> </ul>"}, {"location": "adr/", "title": "ADR", "text": "<p>ADR are short text documents that captures an important architectural decision made along with its context and consequences.</p> <p>The whole document should be one or two pages long. Written as if it is a conversation with a future developer. This requires good writing style, with full sentences organized into paragraphs. Bullets are acceptable only for visual style, not as an excuse for writing sentence fragments.</p> <p>Pros:</p> <ul> <li>We have a clear log of the different decisions taken, which can help newcomers     to understand past decisions.</li> <li>It can help in the discussion of such changes.</li> <li>Architecture decisions recorded in small modular readable documents.</li> </ul> <p>Cons:</p> <ul> <li>More time is required for each change, as we need to document and discuss it.</li> </ul>"}, {"location": "adr/#how-to-use-them", "title": "How to use them", "text": "<p>We will keep a collection of architecturally significant decisions, those that affect the structure, non-functional characteristics, dependencies, interfaces or construction techniques.</p> <p>There are different templates you can start with, being the most popular Michael Nygard's one.</p> <p>The documents are stored in the project repository under the <code>doc/arch</code> directory, with a name convention of <code>NNN-title_with_underscores.md</code>, where <code>NNN</code> is a monotonically increasing number.</p> <p>If a decision is reversed, we'll keep the old one around, but mark it as superseded, as it's still relevant to know that it was a decision, but is no longer.</p>"}, {"location": "adr/#adr-template", "title": "ADR template", "text": "<p>Using Michael Nygard's template as a starting point, I'm going to use these sections:</p> <ul> <li>Title: A short noun phrase that describes the change. For example, \"ADR 1:     Deployment on Ruby on Rails 3.0.10\".</li> <li>Date: Creation date of the document.</li> <li>Status: The ADRs go through the following phases:<ul> <li>Draft: We are using the ADR to build the idea, so everything can change.</li> <li>Proposed: We have a solid proposal on the Decision to solve the Context.</li> <li>Accepted: We have agreed on the proposal, from now on the document can't     be changed!</li> <li>Rejected: We have agreed not to solve the Context.</li> <li>Deprecated: The Context no longer applies, so the solution is no longer     needed.</li> <li>Superseded: We have found another ADR that better solves the Context.</li> </ul> </li> <li>Context: This section describes the situation we're trying to solve,     including technological, political, social, and project local aspects. The     language in this section is neutral, simply describing facts.</li> <li>Proposals: Analysis of the different solutions for the situation defined     in the Context.</li> <li>Decision: Clear summary of the selected proposal. It is stated in full     sentences, with active voice.</li> <li>Consequences: Description of the resulting context, after applying     the decision. All consequences should be listed here, not just the positive     ones.</li> </ul> <p>I'm using the following Ultisnip vim snippet:</p> <pre><code>snippet adr \"ADR\"\nDate: `date +%Y-%m-%d`\n\n# Status\n&lt;!-- What is the status? Draft, Proposed, Accepted, Rejected, Deprecated or Superseded?\n--&gt;\n$1\n\n# Context\n&lt;!-- What is the issue that we're seeing that is motivating this decision or change? --&gt;\n$0\n\n# Proposals\n&lt;!-- What are the possible solutions to the problem described in the context --&gt;\n\n# Decision\n&lt;!-- What is the change that we're proposing and/or doing? --&gt;\n\n# Consequences\n&lt;!-- What becomes easier or more difficult to do because of this change? --&gt;\nendsnippet\n</code></pre>"}, {"location": "adr/#usage-in-a-project", "title": "Usage in a project", "text": "<p>When starting a project, I'll do it by the ADRs, that way you evaluate the problem, structure the idea and leave a record of your initial train of thought.</p> <p>I found useful to:</p> <ul> <li> <p>Define the general problem at high level in     <code>001-high_level_problem_analysis.md</code>.</p> <ul> <li>Describe the problem you want to solve in the Context.</li> <li>Reflect the key points to solve the problem at the start of the Proposals     section. Go one by one analyzing possible outcomes trying not to dive deep     into details and having at least two proposals for each key point (hard!).</li> <li>Build an initial proposal in the Decision section by reviewing that all     the Context points have been addressed and summarizing each of the     Proposal key points' outcomes.</li> <li>Review the positive and negative Consequences for each actor involved with     the solution, such as:<ul> <li>The final user that is going to consume the outcome.</li> <li>The middle user that is going to host and maintain the solution.</li> <li>Ourselves as developers.</li> </ul> </li> <li>Use the problem definition of <code>001</code> and draft the phases of the solution at <code>002</code>.</li> <li>Create another ADR for each of the phases, getting a level closer to the final implementation.</li> </ul> </li> <li> <p>Use <code>00X</code> for the early drafts. Once you give it a number try not to change     the file name, or you'll need to manually update the references you make.</p> </li> </ul> <p>As the project starts to grow, the relationships between the ADRs will get more complex, it's useful to create an ADR landing page, where the user can follow the logic between them. MermaidJS can be used to create a nice diagram that shows this information.</p> <p>In the mkdocs-newsletter I've used the next structure:</p> <pre><code>graph TD\n    001[001: High level analysis]\n    002[002: Initial MkDocs plugin design]\n    003[003: Selected changes to record]\n    004[004: Article newsletter structure]\n    005[005: Article newsletter creation]\n\n    001 -- Extended --&gt; 002\n    002 -- Extended --&gt; 003\n    002 -- Extended --&gt; 004\n    002 -- Extended --&gt; 005\n    003 -- Extended --&gt; 004\n    004 -- Extended --&gt; 005\n\n    click 001 \"https://lyz-code.github.io/mkdocs-newsletter/adr/001-initial_approach\" _blank\n    click 002 \"https://lyz-code.github.io/mkdocs-newsletter/adr/002-initial_plugin_design\" _blank\n    click 003 \"https://lyz-code.github.io/mkdocs-newsletter/adr/003-select_the_changes_to_record\" _blank\n    click 004 \"https://lyz-code.github.io/mkdocs-newsletter/adr/004-article_newsletter_structure\" _blank\n    click 005 \"https://lyz-code.github.io/mkdocs-newsletter/adr/005-create_the_newsletter_articles\" _blank\n\n    001:::accepted\n    002:::accepted\n    003:::accepted\n    004:::accepted\n    005:::accepted\n\n    classDef draft fill:#CDBFEA;\n    classDef proposed fill:#B1CCE8;\n    classDef accepted fill:#B1E8BA;\n    classDef rejected fill:#E8B1B1;\n    classDef deprecated fill:#E8B1B1;\n    classDef superseeded fill:#E8E5B1;\n</code></pre> <p>Where we define:</p> <ul> <li>The nodes with their title.</li> <li>The relationship between the ADRs.</li> <li>The link to the ADR article so it can be clicked.</li> <li>The state of the ADR.</li> </ul>"}, {"location": "adr/#tools", "title": "Tools", "text": "<p>Although adr-tools exist, I feel it's an overkill to create new documents and search on an existing codebase. We are now used to using other tools for the similar purpose, like Vim or grep.</p>"}, {"location": "adr/#references", "title": "References", "text": "<ul> <li>Joel Parker guide on ADRs</li> <li>Michael Nygard post on ARDs</li> </ul>"}, {"location": "aerial_silk/", "title": "Aerial Silk", "text": "<p>Aerial Silk is a type of performance in which one or more artists perform aerial acrobatics while hanging from a fabric. The fabric may be hung as two pieces, or a single piece, folded to make a loop, classified as hammock silks. Performers climb the suspended fabric without the use of safety lines and rely only on their training and skill to ensure safety. They use the fabric to wrap, suspend, drop, swing, and spiral their bodies into and out of various positions. Aerial silks may be used to fly through the air, striking poses and figures while flying. Some performers use dried or spray rosin on their hands and feet to increase the friction and grip on the fabric.</p>"}, {"location": "aerial_silk/#warming-up", "title": "Warming up", "text": ""}, {"location": "aerial_silk/#arm-twist", "title": "Arm twist", "text": "<p>. Leave the silk at your left, take it with the left hand with your arm straight up and you thumb pointing away from you. . Start twisting in <code>z &gt; 0</code> your arm from your shoulder until the thumb points to your front. . Keep on twisting and follow the movement with the rest of your upper body until you're hanging from that arm and stretching. You shouldn't move your feet in the whole process. . Repeat with the other arm.</p>"}, {"location": "aerial_silk/#ball-controlled-inversions", "title": "Ball controlled inversions", "text": "<p>. From standing position with each hand in a tissue, give it two or three loops to each hand and invert passing your legs between each tissue. . Bend your knees so that you become a ball and from that position. . While alive: . Keep on rotating 90 degrees more until your shins are parallel to the ground facing down. . Change the direction of the rotation and rotate 180 degrees until your shins are parallel to the ground but facing up.</p>"}, {"location": "aerial_silk/#inverted-arm-twist-warmup", "title": "Inverted arm twist warmup", "text": "<p>. From standing position with each hand in a tissue, give it two or three loops to each hand and invert passing your legs between each tissue. . Move your legs up until your body is parallel to the tissues and your shoulders are rotated back so your chest goes forward. . While alive: . Start lowering your feet using the tissues as a guide, while doing so start twisting your arms at shoulder level so that your chest goes to your back in a cat like position until your body limit. . Go back up doing the twist in the other direction till you're back up.</p>"}, {"location": "aerial_silk/#inverted-knee-to-elbow", "title": "Inverted knee to elbow", "text": "<p>. From standing position with each hand in a tissue, give it two or three loops to each hand and invert passing your legs between each tissue. . Move your legs up until your body is parallel to the tissues and your shoulders are rotated back so your chest goes forward. . While alive: . Start lowering your feet using the tissues as a guide for your knees until they are at elbow level. Don't bend your knees! . Go back up.</p>"}, {"location": "aerial_silk/#horizontal-pull-ups", "title": "Horizontal pull-ups", "text": "<p>. From standing position with each hand in a tissue, ask for a partner to take your legs until you're horizontal hanging from your hands and feet. . While alive: . Keeping your body straight, do a pull up with your arms. . Slowly unbend your elbows and stretch back your arms.</p> <p>The next level would be that you use your leg strength to grip your partner's hips instead of her holding your feet.</p>"}, {"location": "aerial_silk/#basic-movements", "title": "Basic movements", "text": ""}, {"location": "aerial_silk/#splitting-the-tissues", "title": "Splitting the tissues", "text": "<p>. Go to the desired height and get into a comfortable position, such as seated over your feet on a Russian climb. . Open the palm of one hand at eye level holding the whole tissue . Keep your bellybutton close to the tissue . With the other hand pinch the side of the silk and start walking with your fingers till you find the break between tissues.</p>"}, {"location": "aerial_silk/#figures", "title": "Figures", "text": ""}, {"location": "aerial_silk/#waist-lock", "title": "Waist lock", "text": "<p>. Climb to the desired height . Leave the tissue to the side you want to do the lock to, for example the left (try the other side as well). . Leave your right hand above your left, with your arm straight. Your left hand should be at breast level with your elbow bent. . With straight legs with pointed toes, bring your left leg a little bit to the front, while the right one catches the silk . Bright the right leg up trying to get the silk as close to your waist as possible. . Rotate your body in <code>z&gt;0</code> towards your left hand until you're looking down . Release your hands.</p>"}, {"location": "aerial_silk/#ideas", "title": "Ideas", "text": ""}, {"location": "aerial_silk/#locking-shoulder-blades-when-gripping-the-silks", "title": "Locking shoulder blades when gripping the silks", "text": "<p>When you are going to hang yourself from your hands:</p> <p>. Unlock your shoulder blades moving your chest to the back. . Embrace the silk with your arms twisting your hands inwards . Grip the silk and lock back your shoulder blades together as if you were holding an apple between them. That movement will make your hands twist in the other direction until your wrists are between you and the tissue.</p>"}, {"location": "aerial_silk/#safety", "title": "Safety", "text": "<ul> <li>When rolling up silk over your legs, always leave room for the knee, do loops     above and below but never over.</li> </ul>"}, {"location": "afew/", "title": "afew", "text": "<p>afew is an initial tagging script for notmuch mail.</p> <p>Its basic task is to provide automatic tagging each time new mail is registered with <code>notmuch</code>. In a classic setup, you might call it after <code>notmuch new</code> in an offlineimap post sync hook.</p> <p>It can do basic thing such as adding tags based on email headers or maildir folders, handling killed threads and spam.</p> <p>In move mode, afew will move mails between maildir folders according to configurable rules that can contain arbitrary notmuch queries to match against any searchable attributes.</p>"}, {"location": "afew/#installation", "title": "Installation", "text": "<p>First install the requirements:</p> <pre><code>sudo apt-get install notmuch python3-notmuch\n</code></pre> <p>Then configure <code>notmuch</code>.</p> <p>Finally install the program:</p> <pre><code>pip3 install afew\n</code></pre>"}, {"location": "afew/#usage", "title": "Usage", "text": "<p>To tag new emails use:</p> <pre><code>afew -v --tag --new\n</code></pre>"}, {"location": "afew/#references", "title": "References", "text": "<ul> <li>Git</li> <li>Docs</li> </ul>"}, {"location": "age_of_empires/", "title": "Age of Empires", "text": ""}, {"location": "age_of_empires/#basic-principles", "title": "Basic principles", "text": "<ul> <li>You need to constantly spawn new workers until you reach 120</li> <li>You should have your resources balanced, don't be afraid of using the market   if you have too much of something.</li> <li>When gathering food from fastest to slowest you can: fish &gt; hunt deer &gt; hunt   boar &gt; hunt sheep &gt; gather berries.</li> </ul>"}, {"location": "age_of_empires/#openings-or-build-orders", "title": "Openings or Build Orders", "text": ""}, {"location": "age_of_empires/#basic-opening", "title": "Basic Opening", "text": "<p>When the match starts:</p> <ul> <li> <p>Create 2 workers on the town center.</p> </li> <li> <p>With two workers build a house (let's call them 1 and 2). It takes 2 seconds   more to build a house than to spawn a worker (25s for the worker), so that's   why we need two workers on one house to finish it before the first worker is   spawned.</p> </li> </ul> <p>Each additional worker constructing a building will help in <code>100/(n + 1) %</code>,   so the second worker will make the building come up a 25% faster.</p> <ul> <li> <p>With the other worker, let's call it 3, build another house.</p> </li> <li> <p>Save the Scout in the keybinding 1 and start scouting in circles around your   town center.</p> </li> <li> <p>If you don't see the sheep, start exploring with workers 2, 3 and the scout.   As you've already worked a little bit on the first house with the two first   workers, the house should be ready when the first worker spawns. It doesn't   matter that the second house is not ready at the start because it will pass a   time before you need it. It's more crucial to find the sheep.</p> </li> <li> <p>Once you find the first four sheep, bring two of them to the town center (TC),   and start exploring with the other two. You should have another 4 sheep a   little further from your town center, and they're usually around the boars.   Assign the keybinding 3 and 4 to the scouting sheep.</p> </li> <li> <p>Once the houses are built, bring all the workers to the TC to gather the   sheep. Mark the new spawned ones to work on them too until you have 6. That   guarantees that you'll have enough food to have a continuous flow of spawning   workers.</p> </li> <li> <p>Once the 6<sup>th</sup> worker is spawned, mark the next one to go to the better wood   close to you TC. When it's spawned build the lumberjack the closest you can to   the forest. Select the 6 sheep workers and assign them to the keybinding 2.</p> </li> <li> <p>Now it's the time to bring all the sheep back to the TC to prevent the enemy   from stealing them. Hopefully you'd found the other two pairs of sheep, group   them in pairs and put them on each side of the TC</p> </li> <li> <p>You need 4 workers on the wood, remember to micromanage them so that there are   no more than two on either side of the lumberjack.</p> </li> <li> <p>The 11<sup>th</sup> worker goes to fetch the boar (keybinding 3), and once it's killed it   gathers on the boar too, that makes 7 on the boar.</p> </li> <li> <p>The 12<sup>th</sup> worker builds a house and then goes for berries</p> </li> <li> <p>The 13<sup>th</sup> worker goes to build the mill on the berries</p> </li> <li> <p>The 14<sup>th</sup> also goes to the berries</p> </li> <li> <p>The 15<sup>th</sup> goes to fetch the 2<sup>nd</sup> boar (keybinding 3) and then keeps on gathering   to the boar</p> </li> <li> <p>The 16<sup>th</sup> goes to the berries</p> </li> <li> <p>Once the 17<sup>th</sup> is spawned, with one lumberjacker build a new house and then go   back to gather wood.</p> </li> <li> <p>From the 17<sup>th</sup> to the 22th goes to the second boar or sheep. Once the resources   of the second boar are exhausted or close to ending, garrison the workers   under binding 2 in the TC to offload the meat and put them to gather sheep,   then put the others on the other group of sheep.</p> </li> <li> <p>Once you start having too much wood, of the boar/sheep gatherers, send the one   that has less health to build a farm.</p> </li> <li> <p>After the 22th is spawned, research the loom</p> </li> <li> <p>After loom is researched, research the feudal age.</p> </li> <li> <p>Send 5 or 6 of the sheep gatherers to the wood</p> </li> </ul>"}, {"location": "age_of_empires/#fast-castle-boom", "title": "Fast castle boom", "text": ""}, {"location": "age_of_empires/#strategy-guides", "title": "Strategy guides", "text": ""}, {"location": "age_of_empires/#how-to-play-maps", "title": "How to play maps", "text": "<ul> <li>How to play Arena:<ul> <li>Hera's guide</li> <li>Tatoh game in arena: First match of the series</li> </ul> </li> <li>How to play Hideout</li> <li>How to play Blackforest</li> </ul>"}, {"location": "age_of_empires/#inside-the-mind-of-a-pro-player", "title": "Inside the mind of a pro player", "text": "<ul> <li>Episode 1</li> <li>Episode 2</li> </ul>"}, {"location": "age_of_empires/#strategies-against-civilisations", "title": "Strategies against civilisations", "text": "<p>I'm using only the mongols, and so far I've seen/heard from the pros the next strategies:</p> <ul> <li>Aztecs: <ul> <li>Steppe lancers good against eagle warriors</li> <li>Heavy scorpions against eagle warriors and skirms</li> </ul> </li> <li>Cumans: <ul> <li>Scout, if it drops two TCs in feudal, tower rush into archers</li> <li>Put initial pressure: Nice initial pressure</li> </ul> </li> <li>Incas: <ul> <li>Steppe lancers good against eagle warriors</li> <li>Heavy scorpions against eagle warriors and skirms</li> </ul> </li> <li>Khmer: boom, map control, monks and albadiers </li> <li>Mayans: <ul> <li>Steppe lancers good against eagle warriors</li> <li>Heavy scorpions against eagle warriors and skirms</li> </ul> </li> <li>Romans:<ul> <li>Hera guide on how to beat them</li> </ul> </li> <li>Tartars: heavy scorpions</li> <li>Turks: <ul> <li>How to defend against them in Arena</li> </ul> </li> <li>Vietnamese:<ul> <li>Gain early map control with scouts, then switch into steppe lancers and front siege, finally castle in the face when you clicked to imperial.<ul> <li>Example Hera vs Mr.Yo in TCI</li> </ul> </li> </ul> </li> </ul>"}, {"location": "age_of_empires/#newbie-pitfalls", "title": "Newbie pitfalls", "text": "<ul> <li>Don't go for the berries first it's slower.</li> <li>Kill the sheep one at a time, their food rots otherwise</li> <li>Research the Loom too soon</li> <li>Not using the key bindings to build buildings, select buildings, spawn units,   control units</li> <li>Not building the farms around the TC</li> <li>Don't let the scout die</li> </ul>"}, {"location": "age_of_empires/#micromanagements", "title": "Micromanagements", "text": ""}, {"location": "age_of_empires/#workers", "title": "Workers", "text": ""}, {"location": "age_of_empires/#sheep-hunting", "title": "Sheep hunting", "text": "<ul> <li>Always gather at the base of the TC or the mill.</li> <li>Use 6 workers on a sheep, that guarantees that you'll have enough food to have   a continuous flow of spawning workers</li> <li>If possible, always have another sheep (and not more!) close where the workers   are gathering. As soon as they kill the new sheep bring another.</li> </ul>"}, {"location": "age_of_empires/#lumber-jacking", "title": "Lumber jacking", "text": "<ul> <li> <p>Assign two workers on each side of the lumberjack. You'll need to continuously   micromanage them</p> </li> <li> <p>The optimum of workers per lumberjack is 5, if you have more build a second   lumberjack, if possible on a different location.</p> </li> </ul>"}, {"location": "age_of_empires/#berry-gathering", "title": "Berry gathering", "text": "<ul> <li>Similar to Lumber jacking, you need two on each side and prevent them from   going all the way around the berries</li> </ul>"}, {"location": "age_of_empires/#boar-hunting", "title": "Boar hunting", "text": "<ul> <li>The worker that fetches the boar hits two arrows to it and then bring it back   to the TC, tell it to go past the TC, and once the others are shooting at the   boar, garrison it on the TC if you're not sure if he'll survive.</li> <li>Once they are close to the TC make the sheep gatherers go for the boar</li> </ul>"}, {"location": "age_of_empires/#deer-hunting", "title": "Deer hunting", "text": "<ul> <li>Bring the deer closer to the TC with the scout</li> </ul>"}, {"location": "age_of_empires/#house-building", "title": "House building", "text": "<p>Build new houses when you're 2 of population down to the limit</p>"}, {"location": "age_of_empires/#nice-games", "title": "Nice games", "text": ""}, {"location": "age_of_empires/#tournaments", "title": "Tournaments", "text": "<ul> <li>2023 Masters of Arena 7 Final Tatoh vs Vinchester:<ul> <li>Casted by T90</li> <li>Pov by Tatoh</li> </ul> </li> </ul>"}, {"location": "age_of_empires/#showmatches", "title": "Showmatches", "text": "<ul> <li>Hera vs TheViper | Battlegrounds 3 | BO5</li> <li>The Viper VS Tatoh PA7</li> </ul>"}, {"location": "age_of_empires/#1vs1-games", "title": "1vs1 games", "text": "<ul> <li>Hindustanis vs Portuguese | Arabia | Hera vs Yo</li> <li>Dravidians vs Turks | African Clearing | Hera vs Yo</li> </ul>"}, {"location": "aiohttp/", "title": "aiohttp", "text": "<p><code>aiohttp</code> is an asynchronous HTTP Client/Server for asyncio and Python. Think of it as the <code>requests</code> for asyncio.</p>"}, {"location": "aiohttp/#installation", "title": "Installation", "text": "<pre><code>pip install aiohttp\n</code></pre> <p><code>aiohttp</code> can be bundled with optional libraries to speed up the DNS resolving and other niceties, install it with:</p> <pre><code>pip install aiohttp[speedups]\n</code></pre> <p>Beware though that some of them don't yet support Python 3.10+</p>"}, {"location": "aiohttp/#usage", "title": "Usage", "text": ""}, {"location": "aiohttp/#basic-example", "title": "Basic example", "text": "<pre><code>import aiohttp\nimport asyncio\n\nasync def main():\n\n    async with aiohttp.ClientSession() as session:\n        async with session.get('http://python.org') as response:\n\n            print(\"Status:\", response.status)\n            print(\"Content-type:\", response.headers['content-type'])\n\n            html = await response.text()\n            print(\"Body:\", html[:15], \"...\")\n\nloop = asyncio.get_event_loop()\nloop.run_until_complete(main())\n</code></pre> <p>This prints:</p> <pre><code>Status: 200\nContent-type: text/html; charset=utf-8\nBody: &lt;!doctype html&gt; ...\n</code></pre> <p>Why so many lines of code? Check it out here.</p>"}, {"location": "aiohttp/#make-a-request", "title": "Make a request", "text": "<p>With the snippet from the basic example we have a <code>ClientSession</code> called <code>session</code> and a <code>ClientResponse</code> object called <code>response</code>. </p> <p>In order to make an HTTP POST request use <code>ClientSession.post()</code> coroutine:</p> <pre><code>session.post('http://httpbin.org/post', data=b'data')\n</code></pre> <p>Other HTTP methods are available as well:</p> <pre><code>session.put('http://httpbin.org/put', data=b'data')\nsession.delete('http://httpbin.org/delete')\nsession.head('http://httpbin.org/get')\nsession.options('http://httpbin.org/get')\nsession.patch('http://httpbin.org/patch', data=b'data')\n</code></pre> <p>To make several requests to the same site more simple, the parameter <code>base_url</code> of <code>ClientSession</code> constructor can be used. For example to request different endpoints of http://httpbin.org can be used the following code:</p> <pre><code>async with aiohttp.ClientSession('http://httpbin.org') as session:\n    async with session.get('/get'):\n        pass\n    async with session.post('/post', data=b'data'):\n        pass\n    async with session.put('/put', data=b'data'):\n        pass\n</code></pre> <p>Use the <code>response.raise_for_status()</code> method to raise an exception if the status code is higher than 400.</p>"}, {"location": "aiohttp/#passing-parameters-in-urls", "title": "Passing parameters in urls", "text": "<p>You often want to send some sort of data in the URL\u2019s query string. If you were constructing the URL by hand, this data would be given as key/value pairs in the URL after a question mark, e.g. httpbin.org/get?key=val. Requests allows you to provide these arguments as a dict, using the params keyword argument. As an example, if you wanted to pass <code>key1=value1</code> and <code>key2=value2</code> to httpbin.org/get, you would use the following code:</p> <pre><code>params = {'key1': 'value1', 'key2': 'value2'}\nasync with session.get('http://httpbin.org/get',\n                       params=params) as resp:\n    expect = 'http://httpbin.org/get?key1=value1&amp;key2=value2'\n    assert str(resp.url) == expect\n</code></pre> <p>You can see that the URL has been correctly encoded by printing the URL.</p>"}, {"location": "aiohttp/#passing-a-json-in-the-request", "title": "Passing a json in the request", "text": "<p>There\u2019s also a built-in JSON decoder, in case you\u2019re dealing with JSON data:</p> <pre><code>async with session.get('https://api.github.com/events') as resp:\n    print(await resp.json())\n</code></pre> <p>In case that JSON decoding fails, <code>json()</code> will raise an exception. </p>"}, {"location": "aiohttp/#setting-custom-headers", "title": "Setting custom headers", "text": "<p>If you need to add HTTP headers to a request, pass them in a <code>dict</code> to the <code>headers</code> parameter.</p> <p>For example, if you want to specify the content-type directly:</p> <pre><code>url = 'http://example.com/image'\npayload = b'GIF89a\\x01\\x00\\x01\\x00\\x00\\xff\\x00,\\x00\\x00'\n          b'\\x00\\x00\\x01\\x00\\x01\\x00\\x00\\x02\\x00;'\nheaders = {'content-type': 'image/gif'}\n\nawait session.post(url,\n                   data=payload,\n                   headers=headers)\n</code></pre> <p>You also can set default headers for all session requests:</p> <pre><code>headers={\"Authorization\": \"Basic bG9naW46cGFzcw==\"}\nasync with aiohttp.ClientSession(headers=headers) as session:\n    async with session.get(\"http://httpbin.org/headers\") as r:\n        json_body = await r.json()\n        assert json_body['headers']['Authorization'] == \\\n            'Basic bG9naW46cGFzcw=='\n</code></pre> <p>Typical use case is sending JSON body. You can specify content type directly as shown above, but it is more convenient to use special keyword json:</p> <pre><code>await session.post(url, json={'example': 'text'})\n</code></pre> <p>For <code>text/plain</code></p> <pre><code>await session.post(url, data='\u041f\u0440\u0438\u0432\u0435\u0442, \u041c\u0438\u0440!')\n</code></pre>"}, {"location": "aiohttp/#set-custom-cookies", "title": "Set custom cookies", "text": "<p>To send your own cookies to the server, you can use the cookies parameter of <code>ClientSession</code> constructor:</p> <pre><code>url = 'http://httpbin.org/cookies'\ncookies = {'cookies_are': 'working'}\nasync with ClientSession(cookies=cookies) as session:\n    async with session.get(url) as resp:\n        assert await resp.json() == {\n           \"cookies\": {\"cookies_are\": \"working\"}}\n</code></pre>"}, {"location": "aiohttp/#proxy-support", "title": "Proxy support", "text": "<p><code>aiohttp</code> supports plain HTTP proxies and HTTP proxies that can be upgraded to HTTPS via the HTTP CONNECT method. To connect, use the proxy parameter:</p> <pre><code>async with aiohttp.ClientSession() as session:\n    async with session.get(\"http://python.org\",\n                           proxy=\"http://proxy.com\") as resp:\n        print(resp.status)\n</code></pre> <p>It also supports proxy authorization:</p> <pre><code>async with aiohttp.ClientSession() as session:\n    proxy_auth = aiohttp.BasicAuth('user', 'pass')\n    async with session.get(\"http://python.org\",\n                           proxy=\"http://proxy.com\",\n                           proxy_auth=proxy_auth) as resp:\n        print(resp.status)\n</code></pre> <p>Authentication credentials can be passed in proxy URL:</p> <pre><code>session.get(\"http://python.org\",\n            proxy=\"http://user:pass@some.proxy.com\")\n</code></pre> <p>Contrary to the <code>requests</code> library, it won\u2019t read environment variables by default. But you can do so by passing <code>trust_env=True</code> into <code>aiohttp.ClientSession</code> constructor for extracting proxy configuration from <code>HTTP_PROXY</code>, <code>HTTPS_PROXY</code>, <code>WS_PROXY</code> or <code>WSS_PROXY</code> environment variables (all are case insensitive):</p> <pre><code>async with aiohttp.ClientSession(trust_env=True) as session:\n    async with session.get(\"http://python.org\") as resp:\n        print(resp.status)\n</code></pre>"}, {"location": "aiohttp/#how-to-use-the-clientsession", "title": "How to use the <code>ClientSession</code>", "text": "<p>By default the <code>aiohttp.ClientSession</code> object will hold a connector with a maximum of 100 connections, putting the rest in a queue. This is quite a big number, this means you must be connected to a hundred different servers (not pages!) concurrently before even having to consider if your task needs resource adjustment.</p> <p>In fact, you can picture the session object as a user starting and closing a browser: it wouldn\u2019t make sense to do that every time you want to load a new tab.</p> <p>So you are expected to reuse a <code>session</code> object and make many requests from it. For most scripts and average-sized software, this means you can create a single session, and reuse it for the entire execution of the program. You can even pass the session around as a parameter in functions. For example, the typical \u201chello world\u201d:</p> <pre><code>import aiohttp\nimport asyncio\n\nasync def main():\n    async with aiohttp.ClientSession() as session:\n        async with session.get('http://python.org') as response:\n            html = await response.text()\n            print(html)\n\nloop = asyncio.get_event_loop()\nloop.run_until_complete(main())\n</code></pre> <p>Can become this:</p> <pre><code>import aiohttp\nimport asyncio\n\nasync def fetch(session, url):\n    async with session.get(url) as response:\n        return await response.text()\n\nasync def main():\n    async with aiohttp.ClientSession() as session:\n        html = await fetch(session, 'http://python.org')\n        print(html)\n\nloop = asyncio.get_event_loop()\nloop.run_until_complete(main())\n</code></pre> <p>When to create more than one session object then? It arises when you want more granularity with your resources management:</p> <ul> <li>You want to group connections by a common configuration. e.g: sessions can set cookies, headers, timeout values, etc. that are shared for all connections they hold.</li> <li>You need several threads and want to avoid sharing a mutable object between them.</li> <li>You want several connection pools to benefit from different queues and assign priorities. e.g: one session never uses the queue and is for high priority requests, the other one has a small concurrency limit and a very long queue, for non important requests.</li> </ul>"}, {"location": "aiohttp/#an-aiohttp-adapter", "title": "An aiohttp adapter", "text": "<pre><code>import asyncio\nimport aiohttp\nimport json\n\nfrom dataclasses import dataclass\n\n\n@dataclass\nclass Config:\n    verify_ssl: bool = True\n    tcp_connections: int = 5\n\n\nclass Http:\n\"\"\"A generic HTTP Rest adapter.\"\"\"\n\n    def __init__(self, config: Optional[Config] = None) -&gt; None:\n        self.config = Config() if config is None else config\n\n    async def __aenter__(self) -&gt; 'Http':\n        self._con = aiohttp.TCPConnector(\n            verify_ssl=self.config.verify_ssl, limit=self.config.tcp_connections\n        )\n        self._session = aiohttp.ClientSession(connector=self._con)\n        return self\n\n    async def __aexit__(self, exc_type, exc, tb) -&gt; None:\n        await self._session.close()\n        await self._con.close()\n\n    async def request(\n        self,\n        url: str,\n        method: str = 'get',\n        query_param: Optional[Dict] = None,\n        headers: Optional[Dict] = None,\n        body: Optional[Dict] = None,\n    ) -&gt; aiohttp.ClientResponse:\n\"\"\"Performs an Async HTTP request.\n\n        Args:\n            method (str): request method ('GET', 'POST', 'PUT', ).\n            url (str): request url.\n            query_param (dict or None): url query parameters.\n            header (dict or None): request headers.\n            body (json or None): request body in case of method POST or PUT.\n        \"\"\"\n        method = method.upper()\n        headers = headers or {}\n\n        if method == \"GET\":\n            log.debug(f\"Fetching page {url}\")\n            async with self._session.get(\n                url, params=query_param, headers=headers\n            ) as response:\n                if response.status != 200:\n                    log.debug(f\"{url} returned an {response.status} code\")\n                    response.raise_for_status()\n                return response\n\n\nasync def main():\n    async with Http() as client:\n        print(await client.request(method=\"GET\", url=\"https://httpbin.org/get\"))\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main) \n</code></pre>"}, {"location": "aiohttp/#why-so-many-lines-of-code", "title": "Why so many lines of code", "text": "<p>The first time you use <code>aiohttp</code>, you\u2019ll notice that a simple HTTP request is performed not with one, but with up to three steps:</p> <pre><code>async with aiohttp.ClientSession() as session:\n    async with session.get('http://python.org') as response:\n        print(await response.text())\n</code></pre> <p>It\u2019s especially unexpected when coming from other libraries such as the very popular requests, where the \u201chello world\u201d looks like this:</p> <pre><code>response = requests.get('http://python.org')\nprint(response.text)\n</code></pre> <p>So why is the <code>aiohttp</code> snippet so verbose?</p> <p>Because <code>aiohttp</code> is asynchronous, its API is designed to make the most out of non-blocking network operations. In code like this, requests will block three times, and does it transparently, while <code>aiohttp</code> gives the event loop three opportunities to switch context:</p> <ul> <li>When doing the <code>.get()</code>, both libraries send a GET request to the remote server. For <code>aiohttp</code>, this means asynchronous I/O, which is marked here with an async with that gives you the guarantee that not only it doesn\u2019t block, but that it\u2019s cleanly finalized.</li> <li>When doing <code>response.text</code> in <code>requests</code>, you just read an attribute. The call to <code>.get()</code> already preloaded and decoded the entire response payload, in a blocking manner. <code>aiohttp</code> loads only the headers when <code>.get()</code> is executed, letting you decide to pay the cost of loading the body afterward, in a second asynchronous operation. Hence the await <code>response.text()</code>.</li> <li><code>async with aiohttp.ClientSession()</code> does not perform I/O when entering the block, but at the end of it, it will ensure all remaining resources are closed correctly. Again, this is done asynchronously and must be marked as such. The session is also a performance tool, as it manages a pool of connections for you, allowing you to reuse them instead of opening and closing a new one at each request. You can even manage the pool size by passing a connector object.</li> </ul>"}, {"location": "aiohttp/#references", "title": "References", "text": "<ul> <li>Docs</li> </ul>"}, {"location": "aleph/", "title": "Aleph", "text": "<p>Aleph is a tool for indexing large amounts of both documents (PDF, Word, HTML) and structured (CSV, XLS, SQL) data for easy browsing and search. It is built with investigative reporting as a primary use case. Aleph allows cross-referencing mentions of well-known entities (such as people and companies) against watchlists, e.g. from prior research or public datasets.</p>"}, {"location": "aleph/#install-the-development-environment", "title": "Install the development environment", "text": "<p>As a first step, check out the source code of Aleph from GitHub:</p> <pre><code>git clone https://github.com/alephdata/aleph.git\ncd aleph/\n</code></pre> <p>Also, please execute the following command to allow ElasticSearch to map its memory:</p> <pre><code>sysctl -w vm.max_map_count=262144\n</code></pre> <p>Then enable the use of <code>pdb</code> by adding the next lines into the <code>docker-compose.dev.yml</code> file, under the <code>api</code> service configuration.</p> <pre><code>stdin_open: true\ntty: true\n</code></pre> <p>With the settings in place, you can use <code>make all</code> to set everything up and launch the web service. This is equivalent to the following steps:</p> <ul> <li><code>make build</code> to build the docker images for the application and relevant   services.</li> <li><code>make upgrade</code> to run the latest database migrations and create/update the   search index.</li> <li><code>make web</code> to run the web-based API server and the user interface.</li> <li>In a separate shell, run <code>make worker</code> to start a worker. If you do not start   a worker, background jobs (for example ingesting new documents) won\u2019t be   processed.</li> </ul> <p>Open http://localhost:8080/ in your browser to visit the web frontend.</p> <ul> <li>Create a shell to do the operations with <code>make shell</code>.</li> <li>Create the main user within that shell running   <pre><code>aleph createuser --name=\"demo\" \\\n--admin \\\n--password=demo \\\ndemo@demo.com\n</code></pre></li> <li>Load some sample data by running <code>aleph crawldir /aleph/contrib/testdata</code></li> </ul>"}, {"location": "aleph/#debugging-the-code", "title": "Debugging the code", "text": "<p>To debug the code, you can create <code>pdb</code> breakpoints in the code you cloned, and run the actions that trigger the breakpoint. To be able to act on it, you need to be attached to the api by running:</p> <pre><code>docker attach aleph_api_1\n</code></pre> <p>You don't need to reload the page for it to load the changes, it does it dynamically.</p>"}, {"location": "aleph/#operation", "title": "Operation", "text": ""}, {"location": "aleph/#upgrade-aleph", "title": "Upgrade Aleph", "text": "<p>Aleph does not perform updates and database migrations automatically. Once you have the latest version, you can run the command bellow to upgrade the existing installation (i.e. apply changes to the database model or the search index format).</p> <p>The first step is to add a notice in Aleph's banner section informing the users that there's going to be a downtime.</p> <p>Before you upgrade, check the  to make sure you understand the latest release and know about new options and features that have been added.</p> <p>In production mode, make sure you perform a backup of the main database and the ElasticSearch index before running an upgrade.</p> <p>Then, make sure you are using the latest <code>docker-compose.yml</code> file. You can do this by checking out the source repo, but really you just need that one file (and your config in aleph.env). There are many <code>docker-compose.yml</code> files, we need to decide the one we want to take as the source of truth.</p> <pre><code># Pull changes\ncd /data/config/aleph\ndocker-compose pull --parallel\n\n# Stop services\nservice aleph stop\n\n# Do database migrations\ndocker-compose up -d redis postgres elasticsearch\n# Wait a minute or so while services boot up...\n# Run upgrade:\ndocker-compose run --rm shell aleph upgrade\n\n# Start the services\nservice aleph start\n</code></pre>"}, {"location": "aleph/#create-aleph-admins", "title": "Create Aleph admins", "text": "<p>Creation of admins depends on how you create users, in our case that we're using Oauth we need to update the database directly (ugly!). So go into the instance you want to do the change and run:</p> <pre><code># Create a terminal inside the aleph environment\ndocker-compose run --rm shell bash\n\n# Connect to the postgres database\n# It will ask you for the password, search it in the docker-compose.yaml file\npsql postgresql://aleph@postgres/aleph\n\n# Once there you can see which users do have the Admin rights with:\nselect * from role where is_admin = 't';\n\n# If you want to make another user admin run the next command:\nUPDATE role SET is_admin = true WHERE email = 'keng@pulitzercenter.org';\n</code></pre> <p>You may also need to run <code>aleph update</code> afterwards to refresh some cached information.</p>"}, {"location": "aleph/#remove-a-group", "title": "Remove a group", "text": "<p>There is currently no web interface that allows this operation, you need to interact with the database directly.</p> <pre><code># Create a terminal inside the aleph environment\ndocker-compose run --rm shell bash\n\n# Connect to the postgres database\n# It will ask you for the password, search it in the docker-compose.yaml file\npsql postgresql://aleph@postgres/aleph\n\n# List the available groups\nselect * from role where type = 'group';\n\n# Delete a group.\n# Imagine that the id of the group we want to delete is 18\ndelete from role where id = 18;\n</code></pre>"}, {"location": "aleph/#role-permission-error", "title": "Role permission error", "text": "<p>You may encounter the next error:</p> <pre><code>ERROR:  update or delete on table \"role\" violates foreign key constraint \"permission_role_id_fkey\" on table \"permission\"\nDETAIL:  Key (id)=(18) is still referenced from table \"permission\".\n</code></pre> <p>That means that the group is still used, to find who is using it use:</p> <pre><code>select * from permission where role_id = 18;\n</code></pre> <p>You can check the elements that have the permission by looking at the <code>collection_id</code> number, imagine it's <code>3</code>, then you can check <code>your.url.com/investigations/3</code>.</p> <p>Once you're sure you can remove that permission, run:</p> <pre><code>delete from permission where role_id = 18;\ndelete from role where id = 18;\n</code></pre>"}, {"location": "aleph/#role-membership-error", "title": "Role membership error", "text": "<p>You may encounter the next error:</p> <p><pre><code>ERROR:  update or delete on table \"role\" violates foreign key constraint \"role_membership_group_id_fkey\" on table \"role_membership\"\nDETAIL:  Key (id)=(8) is still referenced from table \"role_membership\".\n</code></pre> That means that the group is still used, to find who is using it use:</p> <pre><code>select * from role_membership where group_id = 8;\n</code></pre> <p>If you agree to remove that user from the group use:</p> <pre><code>delete from role_membership where group_id = 8;\ndelete from role where id = 8;\n</code></pre>"}, {"location": "aleph/#troubleshooting", "title": "Troubleshooting", "text": ""}, {"location": "aleph/#ingest-gets-stuck", "title": "Ingest gets stuck", "text": "<p>It looks that Aleph doesn't yet give an easy way to debug it. It can be seen in the next webs:</p> <ul> <li>Improve the UX for bulk uploading and processing of large number of files</li> <li>Document ingestion gets stuck effectively at 100%</li> <li>Display detailed ingestion status to see if everything is alright and when the collection is ready</li> </ul> <p>Some interesting ideas I've extracted while diving into these issues is that:</p> <ul> <li>You can also upload files using the <code>alephclient</code> python command line tool</li> <li>Some of the files might fail to be processed without leaving any hint to the uploader or the viewer.</li> <li>This results in an incomplete dataset and the users don't get to know that the dataset is incomplete. This is problematic if the completeness of the dataset is crucial for an investigation.</li> <li>There is no way to upload only the files that failed to be processed without re-uploading the entire set of documents or manually making a list of the failed documents and re-uploading them</li> <li>There is no way for uploaders or Aleph admins to see an overview of processing errors to figure out why some files are failing to be processed without going through docker logs (which is not very user-friendly)</li> <li>There was an attempt to improve the way ingest-files manages the pending tasks, it's merged into the release/4.0.0 branch, but it has not yet arrived <code>main</code>.</li> </ul> <p>There are some tickets that attempt to address these issues on the command line:</p> <ul> <li>Allow users to upload/crawl new files only</li> <li>Check if alephclient crawldir was 100% successful or not</li> </ul> <p>I think it's interesting either to contribute to <code>alephclient</code> to solve those issues or if it's complicated create a small python script to detect which files were not uploaded and try to reindex them and/or open issues that will prevent future ingests to fail.</p>"}, {"location": "aleph/#problems-accessing-redis-locally", "title": "Problems accessing redis locally", "text": "<p>If you're with the VPN connected, turn it off.</p>"}, {"location": "aleph/#pdb-behaves-weird", "title": "PDB behaves weird", "text": "<p>Sometimes you have two traces at the same time, so each time you run a PDB command it jumps from pdb trace. Quite confusing. Try to <code>c</code> the one you don't want so that you're left with the one you want. Or put the <code>pdb</code> trace in a conditional that only matches one of both threads.</p>"}, {"location": "aleph/#references", "title": "References", "text": "<ul> <li>Source</li> <li>Docs</li> </ul>"}, {"location": "alot/", "title": "alot", "text": "<p>alot is a terminal-based mail user agent based on the notmuch mail indexer. It is written in python using the urwid toolkit and features a modular and command prompt driven interface to provide a full MUA experience.</p>"}, {"location": "alot/#installation", "title": "Installation", "text": "<pre><code>sudo apt-get install alot\n</code></pre>"}, {"location": "alot/#configuration", "title": "Configuration", "text": "<p>Alot reads the INI config file <code>~/.config/alot/config</code>. That file is not created by default, if you don't want to start from scratch, you can use pazz's alot configuration, in particular the <code>[accounts]</code> section.</p>"}, {"location": "alot/#ui-interaction", "title": "UI interaction", "text": "<p>Basic movement is done with:</p> <ul> <li>Move up and down: <code>j</code>/<code>k</code>, arrows and page up and page down.</li> <li>Cancel prompts: <code>Escape</code></li> <li>Select highlighted element: <code>Enter</code>.</li> <li>Update buffer: <code>@</code>.</li> </ul> <p>The interface shows one buffer at a time, basic buffer management is done with:</p> <ul> <li>Change buffer: <code>Tab</code> and <code>Shift-Tab</code>.</li> <li>Close the current buffer: <code>d</code></li> <li>List all buffers: <code>;</code>.</li> </ul> <p>The buffer type or mode (displayed at the bottom left) determines which prompt commands are available. Usage information on any command can be listed by typing <code>help YOURCOMMAND</code> to the prompt. The key bindings for the current mode are listed upon pressing <code>?</code>.</p> <p>You can always run commands with <code>:</code>.</p>"}, {"location": "alot/#troubleshooting", "title": "Troubleshooting", "text": ""}, {"location": "alot/#remove-emails", "title": "Remove emails", "text": "<p>Say you want to remove emails from the provider's server but keep them in the notmuch database. There is no straight way to do it, you need to tag them with a special tag like <code>deleted</code> and then remove them from the server with a post-hook.</p>"}, {"location": "alot/#theme-not-found", "title": "Theme not found", "text": "<p>I don't know why but <code>apt-get</code> didn't install the default themes, you need to create the <code>~/.config/alot/themes</code> and copy the contents of the themes directory.</p>"}, {"location": "alot/#references", "title": "References", "text": "<ul> <li>Git</li> <li>Docs</li> <li>Wiki</li> <li>FAQ</li> </ul>"}, {"location": "amazfit_band_5/", "title": "Amazfit Band 5", "text": "<p>Amazfit Band 5 it's the affordable fitness tracker I chose to buy because:</p> <ul> <li>It's supported by gadgetbridge.</li> <li>It has a SpO2 sensor which enhances the quality of the sleep metrics</li> <li>It has sleep metrics, not only time but also type of sleep (light, deep, REM).</li> <li>It has support with Alexa, not that I'd use that, but it would be cool if once     I've got my personal voice assistant, I can use it     through the band.</li> </ul>"}, {"location": "amazfit_band_5/#sleep-detection-quality", "title": "Sleep detection quality", "text": "<p>The sleep tracking using Gadgetbridge is not good at all. After two nights, the band has not been able to detect when I woke in the middle of the night, or when I really woke up, as I usually stay in the bed for a time before standing up. I'll try with the proprietary application soon and compare results.</p> <p>If it doesn't work either, I might think of getting a specific device like withings sleep analyzer which seems to have much more accuracy and useful insights. I've sent them an email to see if it's possible to extract the data before it reach their servers, and they confirmed that there is no way. Maybe you can route the requests to their servers to one of your own, bring up an http server and reverse engineer the communication.</p> <p>Karlicoss, the author of the awesome HPI uses the Emfit QS, so that could be another option.</p>"}, {"location": "amazfit_band_5/#firmware-updates", "title": "Firmware updates", "text": "<p>Gadgetbridge people have a guide on how to upgrade the firmware, you need to get the firmware from the geek doing forum though, so it is interesting to create an account and watch the post.</p>"}, {"location": "android_tips/", "title": "Android tips", "text": ""}, {"location": "android_tips/#extend-the-life-of-your-battery", "title": "Extend the life of your battery", "text": "<p>Research has shown that keeping your battery charged between 0% and 80% can make your battery's lifespan last 2x longer than when you use a full battery cycle from 0-100%.</p> <p>As a non root user you can install Accubattery (not in F-droid :( ) to get an alarm when the battery reaches 80% so that you can manually unplug it. Instead of leaving the mobile charge in the night and stay connected at 100% a lot of hours until you unplug, charge it throughout the day.</p>"}, {"location": "anki/", "title": "Anki", "text": "<p>Anki is a program which makes remembering things easy. Because it's a lot more efficient than traditional study methods, you can either greatly decrease your time spent studying, or greatly increase the amount you learn.</p> <p>Anyone who needs to remember things in their daily life can benefit from Anki. Since it is content-agnostic and supports images, audio, videos and scientific markup (via LaTeX), the possibilities are endless.</p>"}, {"location": "anki/#installation", "title": "Installation", "text": "<p>Install the dependencies:</p> <pre><code>sudo apt-get install zstd\n</code></pre> <p>Download the latest release package.</p> <p>Open a terminal and run the following commands, replacing the filename as appropriate:</p> <pre><code>tar xaf Downloads/anki-2.1.XX-linux-qt6.tar.zst\ncd anki-2.1.XX-linux-qt6\nsudo ./install.sh\n</code></pre>"}, {"location": "anki/#anki-workflow", "title": "Anki workflow", "text": ""}, {"location": "anki/#how-long-to-do-study-sessions", "title": "How long to do study sessions", "text": "<p>I have two study modes:</p> <ul> <li>When I'm up to date with my cards, I study them until I finish, but usually less than 15 minutes.</li> <li>If I have been lazy and haven't checked them in a while (like now) I assume I'm not going to see them all and define a limited amount of time to review them, say 10 to 20 minutes depending on the time/energy I have at the moment. </li> </ul> <p>The relief thought you can have is that as long as you keep a steady pace of 10/20 mins each day, inevitably you'll eventually finish your pending cards as you're more effective reviewing cards than entering new ones</p>"}, {"location": "anki/#what-to-do-with-hard-cards", "title": "What to do with \"hard\" cards", "text": "<p>If you're afraid to be stuck in a loop of reviewing \"hard\" cards, don't be. In reality after you've seen that \"hard\" card three times in a row you won't mark it as hard again, because you will remember. If you don't maybe there are two reasons:</p> <ul> <li>The card has too much information that should be subdivided in smaller cards.</li> <li>You're not doing a good process of memorizing the contents once they show up.</li> </ul>"}, {"location": "anki/#interacting-with-python", "title": "Interacting with python", "text": ""}, {"location": "anki/#configuration", "title": "Configuration", "text": "<p>Although there are some python libraries:</p> <ul> <li>genanki</li> <li>py-anki</li> </ul> <p>I think the best way is to use AnkiConnect</p> <p>The installation process is similar to other Anki plugins and can be accomplished in three steps:</p> <ul> <li>Open the Install Add-on dialog by selecting Tools | Add-ons | Get     Add-ons... in Anki.</li> <li>Input <code>2055492159</code> into the text box labeled Code and press the OK button to     proceed.</li> <li>Restart Anki when prompted to do so in order to complete the installation of     Anki-Connect.</li> </ul> <p>Anki must be kept running in the background in order for other applications to be able to use Anki-Connect. You can verify that Anki-Connect is running at any time by accessing <code>localhost:8765</code> in your browser. If the server is running, you will see the message Anki-Connect displayed in your browser window.</p>"}, {"location": "anki/#usage", "title": "Usage", "text": "<p>Every request consists of a JSON-encoded object containing an <code>action</code>, a <code>version</code>, contextual <code>params</code>, and a <code>key</code> value used for authentication (which is optional and can be omitted by default). Anki-Connect will respond with an object containing two fields: <code>result</code> and <code>error</code>. The <code>result</code> field contains the return value of the executed API, and the <code>error</code> field is a description of any exception thrown during API execution (the value <code>null</code> is used if execution completed successfully).</p> <p>Sample successful response:</p> <pre><code>{\"result\": [\"Default\", \"Filtered Deck 1\"], \"error\": null}\n</code></pre> <p>Samples of failed responses:</p> <pre><code>{\"result\": null, \"error\": \"unsupported action\"}\n\n{\"result\": null, \"error\": \"guiBrowse() got an unexpected keyword argument 'foobar'\"}\n</code></pre> <p>For compatibility with clients designed to work with older versions of Anki-Connect, failing to provide a version field in the request will make the version default to 4.</p> <p>To make the interaction with the API easier, I'm using the next adapter:</p> <pre><code>class Anki:\n\"\"\"Define the Anki adapter.\"\"\"\n\n    def __init__(self, url: str = \"http://localhost:8765\") -&gt; None:\n\"\"\"Initialize the adapter.\"\"\"\n        self.url = url\n\n    def requests(\n        self, action: str, params: Optional[Dict[str, str]] = None\n    ) -&gt; Response:\n\"\"\"Do a request to the server.\"\"\"\n        if params is None:\n            params = {}\n\n        response = requests.post(\n            self.url, json={\"action\": action, \"params\": params, \"version\": 6}\n        ).json()\n        if len(response) != 2:\n            raise Exception(\"response has an unexpected number of fields\")\n        if \"error\" not in response:\n            raise Exception(\"response is missing required error field\")\n        if \"result\" not in response:\n            raise Exception(\"response is missing required result field\")\n        if response[\"error\"] is not None:\n            raise Exception(response[\"error\"])\n        return response[\"result\"]\n</code></pre> <p>You can find the full adapter in the fala project.</p>"}, {"location": "anki/#decks", "title": "Decks", "text": ""}, {"location": "anki/#get-all-decks", "title": "Get all decks", "text": "<p>With the adapter:</p> <pre><code>self.requests(\"deckNames\")\n</code></pre> <p>Or with <code>curl</code>:</p> <pre><code>curl localhost:8765 -X POST -d '{\"action\": \"deckNames\", \"version\": 6}'\n</code></pre>"}, {"location": "anki/#create-a-new-deck", "title": "Create a new deck", "text": "<pre><code>self.requests(\"createDeck\", {\"deck\": deck})\n</code></pre>"}, {"location": "anki/#references", "title": "References", "text": "<ul> <li>Homepage</li> <li>Anki-Connect reference</li> </ul>"}, {"location": "anonymous_feedback/", "title": "Anonymous Feedback", "text": "<p>Anonymous Feedback is a communication tool where people share feedback to teammates or other organizational members while protecting their identities.</p>"}, {"location": "anonymous_feedback/#why-would-you-need-anonymous-feedback", "title": "Why would you need anonymous feedback?", "text": "<p>Ideally, everyone in your company should be able to give feedback publicly and not anonymously. They should share constructive criticism and not shy away from direct feedback if they believe and trust that their opinions will be heard and addressed.</p> <p>However, to achieve this ideal, people need to feel that they are in a safe space, a place or environment in which they feel confident that they will not be exposed to discrimination, criticism, harassment, or any other emotional or physical harm. The work place is usually not considered a safe space by the employees because they may:</p> <ul> <li> <p>Fear of being judged: We want people to like us and not just in our     personal lives, but in our professional lives as well. It also seems to bear     a bigger importance that our supervisor likes us because he holds the power     over our career and financial security. So we live in a constant state of     anxiety of what might happen if our manager doesn't like us.</p> </li> <li> <p>Fear of losing their job: It\u2019s a form of self-preservation, abstaining from     saying something that may be perceived as wrong to someone in a position of     authority.</p> </li> <li> <p>Fear of being singled out: Giving direct feedback puts you in the spotlight.     Being highlighted against the rest of the employees might be seen as     a threat, especially by people belonging to a different race, gender,     national origin, or other identities than most of their coworkers.</p> </li> <li> <p>Feel insecure: People may distrust their colleagues, because they     just arrived at the organization or may have negative past experiences either with     them or with similar people. They may not have a solid stance on an issue,     be shy or have problems of self esteem.</p> </li> <li> <p>Distrust the open-door internal policies: Past experiences in other     companies may lead the employee not to trust open-doors policies until they     have seen them in practice.</p> </li> <li> <p>Not knowing the internal processes of the organization: As a Slack study     shows,     55 percent of business owners described their organization as very transparent,     but only 18 percent of their employees would agree.</p> </li> </ul> <p>For all these reasons, some employees may remain silent when asked for direct feedback, to speak up against an internal issue or in need to report a colleague or manager. These factors are further amplified if:</p> <ul> <li>The person belongs to a minority group inside the organization.</li> <li>The greater the difference in position between the talking parties. It's more     difficult to talk to the CEO than to the immediate manager.</li> </ul> <p>Until the safe space is built where direct feedback is viable, anonymous feedback gives these employees a mechanism to raise their concerns, practice their feedback-giving skills, test the waters, and understand how people perceive their constructive (and sometimes critical) opinions, thus building the needed trust.</p>"}, {"location": "anonymous_feedback/#pros-and-cons", "title": "Pros and cons", "text": "<p>Pros of Anonymous Feedback:</p> <ul> <li> <p>Employees can express themselves freely and provide valuable insights: On     topics that are considered sensitive, you\u2019ll often find employees who are     afraid to share their opinions. But when employees have the option to use     anonymous feedback, you will be offering a safe space for them to share     their honest, constructive feedback about sensitive workplace issues,     without fear of being judged, victimized, radicalized or labelled in any     way.</p> <p>A formal, non-anonymous feedback form will only reveal some of the superficial, non-threatening issues that affect the workplace, without mentioning the most important, underlying problems. The real problems that no one talks about because they know they are so important that they could stir things up.</p> <p>In fact, these controversial, important issues are the ones that need to be brought to the table as soon as possible. They should be addressed by the entire team before they become a source of unhappiness, conflict and lack of productivity.</p> <p>An anonymous feedback instrument gives you real power over those issues because it doesn't matter who brought it up, but that it\u2019s resolved. For a manager, that insight is invaluable.</p> </li> <li> <p>It builds trust: Anonymous feedback allows the employee see how management     reacts to feedback, understand how people perceive their constructive (and     sometimes critical) opinions, how are the open-door policies being applied     and build up self esteem.</p> </li> <li> <p>It offers a sense of security: Anonymity soothes the employee anxiety and     creates a greater willingness to share our ideas and opinions.</p> </li> <li> <p>It allows every voice to be heard and respected: In workplaces, where they     practice direct or attributed feedback, leaders may give preference to some     voices over others. Due to our unconscious biases, people of higher     authority, backgrounds, or eloquence tend to command respect and attention.     In such situations, the issues they raise are likely to get immediate     attention than those raised by the rest of the group. However, when feedback     is collected anonymously, it eliminates biases and allows leaders to focus     entirely on the feedback.</p> </li> <li> <p>It encourages new employees to share their opinions:     Research     has shown that new employees, who happen to be less senior or influential,     see anonymous feedback as more appropriate for formal and informal     evaluations than their older colleagues. Typically, the last thing a new     employee wants is to start on the wrong foot, so they maintain a neutral     stance. Using anonymous feedback can make new employees feel     more comfortable sharing their real opinions on workplace issues.</p> </li> </ul> <p>Cons of Anonymous Feedback:</p> <ul> <li> <p>It can breed hostility: According to this Harvard Business Review article,     anonymity often sets off a \u201cwitch hunt\u201d, where leaders seek to know the     source of a negative comment. On the one hand, employees can hide behind     anonymity to say personal and hurtful things about their colleagues or     leaders. On the other hand, leaders may take constructive feedback as     a personal attack and become suspicious and hostile to all their     employees.</p> </li> <li> <p>It can be less impactful than attributed feedback: When using attributed     feedback where responses carry the employees\u2019 names, information can be     analyzed for relevance and impact. However, with anonymous feedback, it can     be difficult to analyze information accurately. It is not uncommon for     companies who choose to practice anonymous feedback, to find less specific     responses since details may reveal respondents\u2019 identities. Vague feedback     from employees would have less power to influence behaviors or drive change     in the organization.</p> </li> <li> <p>It can be difficult to act on: Since anonymous feedback is often difficult     to trace, it can be challenging for the organization to get context or     follow up on important issues, especially when a problem is peculiar to an     individual.</p> </li> </ul>"}, {"location": "anonymous_feedback/#how-to-request-anonymous-feedback", "title": "How to request anonymous feedback", "text": "<p>When requesting for anonymous feedback on an organizational level, it is necessary to:</p> <ul> <li>Set expectations for employees: Let your colleagues know how important their     feedback is to the organization. Also, assure them that their responses will     be non-identifiable (no identifiable names, titles, or other demographic     details). According to a Harvard Business Review     article,     \u201crespondents are much more likely to participate if they are confident that     personal anonymity is guaranteed.\u201d Set those expectations to increase the     chances of response from them.</li> <li>Deploy a feedback platform: Use a trusted feedback platform to send feedback     requests to the rest of the employees.</li> </ul>"}, {"location": "anonymous_feedback/#how-to-act-on-anonymous-feedback", "title": "How to Act on Anonymous Feedback", "text": "<p>Once you have sent the anonymous feedback, be sure to:</p> <ul> <li>Gather and share the findings: A significant issue with employee feedback is     that the data often ends up unused. After collecting the results, share the     data\u2014the positives and negatives\u2014with everyone. Doing this shows     transparency and makes your colleagues develop a positive attitude toward     future requests for feedback.</li> <li>Get everyone involved: Engage employees, managers, and leaders in     discussing and analyzing the feedback findings. Doing this helps to build     trust and develop actionable ideas to move the organization forward.</li> <li>Identify the key issues: From the discussions and analysis, identify the key     issues and understand how they would impact the organization, once     addressed.</li> <li>Define and act on the next steps: The purpose of collecting feedback would     be pointless if the next steps aren't defined. Real improvement comes     from knowing and working on the next steps.</li> </ul>"}, {"location": "anonymous_feedback/#references", "title": "References", "text": "<ul> <li>Osasumwen Arigbe article on Diversity, Inclusion, and Anonymous Feedback</li> <li>Paula Clapon article Why anonymous employee feedback is the better alternative</li> <li>Julian Cook article.     I haven't used it's text, but it's written for managers in their language,     it may help someone there.</li> </ul>"}, {"location": "ansible_snippets/", "title": "Ansible Snippets", "text": ""}, {"location": "ansible_snippets/#ansible-add-a-sleep", "title": "Ansible add a sleep", "text": "<pre><code>- name: Pause for 5 minutes to build app cache\nansible.builtin.pause:\nminutes: 5\n</code></pre>"}, {"location": "ansible_snippets/#ansible-condition-that-uses-a-regexp", "title": "Ansible condition that uses a regexp", "text": "<pre><code>- name: Check if an instance name or hostname matches a regex pattern\nwhen: inventory_hostname is not match('molecule-.*')\nfail:\nmsg: \"not a molecule instance\"\n</code></pre>"}, {"location": "ansible_snippets/#ansible-lint-doesnt-find-requirements", "title": "Ansible-lint doesn't find requirements", "text": "<p>It may be because you're using <code>requirements.yaml</code> instead of <code>requirements.yml</code>. Create a temporal link from one file to the other, run the command and then remove the link.</p> <p>It will work from then on even if you remove the link. <code>\u00af\\(\u00b0_o)/\u00af</code></p>"}, {"location": "ansible_snippets/#run-task-only-once", "title": "Run task only once", "text": "<p>Add <code>run_once: true</code> on the task definition:</p> <pre><code>- name: Do a thing on the first host in a group.\ndebug: msg: \"Yay only prints once\"\nrun_once: true\n</code></pre>"}, {"location": "ansible_snippets/#run-command-on-a-working-directory", "title": "Run command on a working directory", "text": "<pre><code>- name: Change the working directory to somedir/ and run the command as db_owner ansible.builtin.command: /usr/bin/make_database.sh db_user db_name\nbecome: yes\nbecome_user: db_owner\nargs:\nchdir: somedir/\ncreates: /path/to/database\n</code></pre>"}, {"location": "ansible_snippets/#run-handlers-in-the-middle-of-the-tasks-file", "title": "Run handlers in the middle of the tasks file", "text": "<p>If you need handlers to run before the end of the play, add a task to flush them using the meta module, which executes Ansible actions:</p> <pre><code>tasks:\n- name: Some tasks go here\nansible.builtin.shell: ...\n\n- name: Flush handlers\nmeta: flush_handlers\n\n- name: Some other tasks\nansible.builtin.shell: ...\n</code></pre> <p>The <code>meta: flush_handlers</code> task triggers any handlers that have been notified at that point in the play.</p> <p>Once handlers are executed, either automatically after each mentioned section or manually by the <code>flush_handlers meta</code> task, they can be notified and run again in later sections of the play.</p>"}, {"location": "ansible_snippets/#run-command-idempotently", "title": "Run command idempotently", "text": "<pre><code>- name: Register the runner in gitea\nbecome: true\ncommand: act_runner register --config config.yaml --no-interactive --instance {{ gitea_url }} --token {{ gitea_docker_runner_token }}\nargs:\ncreates: /var/lib/gitea_docker_runner/.runner\n</code></pre>"}, {"location": "ansible_snippets/#get-the-correct-architecture-string", "title": "Get the correct architecture string", "text": "<p>If you have an <code>amd64</code> host you'll get <code>x86_64</code>, but sometimes you need the <code>amd64</code> string. On those cases you can use the next snippet:</p> <pre><code>---\n# vars/main.yaml\ndeb_architecture: aarch64: arm64\nx86_64: amd64\n\n---\n# tasks/main.yaml\n- name: Download the act runner binary\nbecome: True\nansible.builtin.get_url:\nurl: https://dl.gitea.com/act_runner/act_runner-linux-{{ deb_architecture[ansible_architecture] }}\ndest: /usr/bin/act_runner\nmode: '0755'\n</code></pre>"}, {"location": "ansible_snippets/#check-the-instances-that-are-going-to-be-affected-by-playbook-run", "title": "Check the instances that are going to be affected by playbook run", "text": "<p>Useful to list the instances of a dynamic inventory</p> <pre><code>ansible-inventory -i aws_ec2.yaml --list\n</code></pre>"}, {"location": "ansible_snippets/#check-if-variable-is-defined-or-empty", "title": "Check if variable is defined or empty", "text": "<p>In Ansible playbooks, it is often a good practice to test if a variable exists and what is its value.</p> <p>Particularity this helps to avoid different \u201cVARIABLE IS NOT DEFINED\u201d errors in Ansible playbooks.</p> <p>In this context there are several useful tests that you can apply using Jinja2 filters in Ansible.</p>"}, {"location": "ansible_snippets/#check-if-ansible-variable-is-defined-exists", "title": "Check if Ansible variable is defined (exists)", "text": "<pre><code>tasks:\n\n- shell: echo \"The variable 'foo' is defined: '{{ foo }}'\"\nwhen: foo is defined\n\n- fail: msg=\"The variable 'bar' is not defined\"\nwhen: bar is undefined\n</code></pre>"}, {"location": "ansible_snippets/#check-if-ansible-variable-is-empty", "title": "Check if Ansible variable is empty", "text": "<pre><code>tasks:\n\n- fail: msg=\"The variable 'bar' is empty\"\nwhen: bar|length == 0\n\n- shell: echo \"The variable 'foo' is not empty: '{{ foo }}'\"\nwhen: foo|length &gt; 0\n</code></pre>"}, {"location": "ansible_snippets/#check-if-ansible-variable-is-defined-and-not-empty", "title": "Check if Ansible variable is defined and not empty", "text": "<pre><code>tasks:\n\n- shell: echo \"The variable 'foo' is defined and not empty\"\nwhen: (foo is defined) and (foo|length &gt; 0)\n\n- fail: msg=\"The variable 'bar' is not defined or empty\"\nwhen: (bar is not defined) or (bar|length == 0)\n</code></pre>"}, {"location": "ansible_snippets/#start-and-enable-a-systemd-service", "title": "Start and enable a systemd service", "text": "<p>Typically defined in <code>handlers/main.yaml</code>:</p> <pre><code>- name: Restart the service\nbecome: true\nsystemd:\nname: zfs_exporter\nenabled: true\ndaemon_reload: true\nstate: started\n</code></pre> <p>And used in any task:</p> <pre><code>- name: Create the systemd service\nbecome: true\ntemplate:\nsrc: service.j2\ndest: /etc/systemd/system/zfs_exporter.service\nnotify: Restart the service\n</code></pre>"}, {"location": "ansible_snippets/#download-a-file", "title": "Download a file", "text": "<pre><code>- name: Download foo.conf\nansible.builtin.get_url:\nurl: http://example.com/path/file.conf\ndest: /etc/foo.conf\nmode: '0440'\n</code></pre>"}, {"location": "ansible_snippets/#download-an-decompress-a-targz", "title": "Download an decompress a tar.gz", "text": "<pre><code>- name: Unarchive a file that needs to be downloaded (added in 2.0)\nansible.builtin.unarchive:\nsrc: https://example.com/example.zip\ndest: /usr/local/bin\nremote_src: yes\n</code></pre> <p>If you want to only extract a file you can use the <code>includes</code> arg</p> <pre><code>- name: Download the zfs exporter\nbecome: true\nansible.builtin.unarchive:\nsrc: https://github.com/pdf/zfs_exporter/releases/download/v{{ zfs_exporter_version }}/zfs_exporter-{{ zfs_exporter_version }}.linux-amd64.tar.gz\ndest: /usr/local/bin\ninclude: zfs_exporter\nremote_src: yes\nmode: 0755\n</code></pre> <p>But that snippet sometimes fail, you can alternatively download it locally and <code>copy</code> it:</p> <pre><code>- name: Test if zfs_exporter binary exists\nstat:\npath: /usr/local/bin/zfs_exporter\nregister: zfs_exporter_binary\n\n- name: Install the zfs exporter\nblock:\n- name: Download the zfs exporter\ndelegate_to: localhost\nansible.builtin.unarchive:\nsrc: https://github.com/pdf/zfs_exporter/releases/download/v{{ zfs_exporter_version }}/zfs_exporter-{{ zfs_exporter_version }}.linux-amd64.tar.gz\ndest: /tmp/\nremote_src: yes\n\n- name: Upload the zfs exporter to the server\nbecome: true\ncopy:\nsrc: /tmp/zfs_exporter-{{ zfs_exporter_version }}.linux-amd64/zfs_exporter\ndest: /usr/local/bin\nmode: 0755\nwhen: not zfs_exporter_binary.stat.exists\n</code></pre>"}, {"location": "ansible_snippets/#skip-ansible-lint-for-some-tasks", "title": "Skip ansible-lint for some tasks", "text": "<pre><code>- name: Modify permissions\ncommand: &gt;\nchmod -R g-w /home/user\ntags:\n- skip_ansible_lint\nsudo: yes\n</code></pre>"}, {"location": "ansible_snippets/#authorize-an-ssh-key", "title": "Authorize an SSH key", "text": "<pre><code>- name: Authorize the sender ssh key\nauthorized_key:\nuser: syncoid\nstate: present\nkey: \"{{ syncoid_receive_ssh_key }}\"\n</code></pre>"}, {"location": "ansible_snippets/#create-a-user", "title": "Create a user", "text": "<p>The following snippet creates a user with password login disabled.</p> <pre><code>- name: Create the syncoid user\nansible.builtin.user:\nname: syncoid\nstate: present\npassword: !\nshell: /usr/sbin/nologin\n</code></pre> <p>If you don't set a password any user can do <code>su your_user</code> to set a random password use the next snippet:</p> <pre><code>- name: Create the syncoid user\nansible.builtin.user:\nname: syncoid\nstate: present\npassword: \"{{ lookup('password', '/dev/null', length=50, encrypt='sha512_crypt') }}\"\nshell: /bin/bash\n</code></pre> <p>This won't pass the idempotence tests as it doesn't save the password anywhere (<code>/dev/null</code>) in the controler machine.</p>"}, {"location": "ansible_snippets/#create-an-ssh-key", "title": "Create an ssh key", "text": "<pre><code>- name: Create .ssh directory\nbecome: true\nfile:\npath: /root/.ssh\nstate: directory\nmode: 700\n\n- name: Create the SSH key to directory\nbecome: true\nopenssh_keypair:\npath: /root/.ssh/id_ed25519\ntype: ed25519\nregister: ssh\n\n- name: Show public key\ndebug:\nvar: ssh.public_key\n</code></pre>"}, {"location": "ansible_snippets/#get-the-hosts-of-a-dynamic-ansible-inventory", "title": "Get the hosts of a dynamic ansible inventory", "text": "<pre><code>ansible-inventory -i environments/production --graph\n</code></pre> <p>You can also use the <code>--list</code> flag to get more info of the hosts.</p>"}, {"location": "ansible_snippets/#speed-up-the-stat-module", "title": "Speed up the stat module", "text": "<p>The <code>stat</code> module calculates the checksum and the md5 of the file in order to get the required data. If you just want to check if the file exists use:</p> <pre><code>- name: Verify swapfile status\nstat:\npath: \"{{ common_swapfile_location }}\"\nget_checksum: no\nget_md5: no\nget_mime: no\nget_attributes: no\nregister: swap_status\nchanged_when: not swap_status.stat.exists\n</code></pre>"}, {"location": "ansible_snippets/#stop-running-docker-containers", "title": "Stop running docker containers", "text": "<pre><code>- name: Get running containers\ndocker_host_info:\ncontainers: yes\nregister: docker_info\n\n- name: Stop running containers\ndocker_container:\nname: \"{{ item }}\"\nstate: stopped\nloop: \"{{ docker_info.containers | map(attribute='Id') | list }}\"\n</code></pre>"}, {"location": "ansible_snippets/#moving-a-file-remotely", "title": "Moving a file remotely", "text": "<p>Funnily enough, you can't without a <code>command</code>. You could use the <code>copy</code> module with:</p> <pre><code>- name: Copy files from foo to bar\ncopy:\nremote_src: True\nsrc: /path/to/foo\ndest: /path/to/bar\n\n- name: Remove old files foo\nfile: path=/path/to/foo state=absent\n</code></pre> <p>But that doesn't move, it copies and removes, which is not the same.</p> <p>To make the <code>command</code> idempotent you can use a <code>stat</code> task before.</p> <pre><code>- name: stat foo\nstat: path: /path/to/foo\nregister: foo_stat\n\n- name: Move foo to bar\ncommand: mv /path/to/foo /path/to/bar\nwhen: foo_stat.stat.exists\n</code></pre>"}, {"location": "antifascism/", "title": "Antifascism", "text": "<p>Antifascism is a method of politics, a locus of individual and group self-indentification, it's a transnational movement that adapted preexisting socialist, anarchist, and communist currents to a sudden need to react to the far right menace (Mark p. 11). It's based on the idea that any oppression form can't be allowed, and should be actively fought with whatever means are necessary. Usually sharing space and even blending with other politic stances that share the same principle, such as intersectional feminism.</p> <p>Read the references</p> <p>The articles under this section are the brushstrokes I use to learn how to become an efficient antifascist.</p> <p>It assumes that you identify yourself as an antifascist, so I'll go straight to the point, skipping much of the argumentation that is needed to sustain these ideas. I'll add links to Mark's and Pol's awesome books, which I strongly recommend you to buy, as they both are jewels that everyone should read.</p> <p>Despite the criminalization and stigmatization by the mainstream press and part of the society, antifascism is a rock solid organized movement with a lot of history, that has invested blood, tears and lives to prevent us from living in a yet more horrible world.</p> <p>The common stereotype is a small group of leftist young people that confront the nazis on the streets, preventing them from using the public space, and from further organizing through direct action and violence if needed. If you don't identify yourself with this stereotype, don't worry, they are only a small (but essential) part of antifascism, there are so many and diverse ways to be part of the antifascist movement that in fact, everyone can (and should) be an antifascist.</p>"}, {"location": "antifascism/#what-is-fascism", "title": "What is fascism", "text": "<p>Fascism in Paxton's words is:</p> <p>... a form of political behavior marked by obsessive preoccupation with community decline, humiliation, or victimhood and by compensatory cults of unity, energy, and purity, in which a mass-based party of commited nationalist militians, working in uneasy but effective collaboration with traditional elites, abandons democratic liberties and pursues with redemptive violence and without ethical or legal restrains goals of internal cleansing and external expansion.</p> <p>They are nourished by the people's weariness with the corruption and inoperability of the traditional political parties, and the growing fear and anguish of an uncertain economic situation.</p> <p>They continuously adapt, redefine and reappropriate concepts under an irreverent, politically incorrect and critical spirit, to spread the old discourse of the privileged against the oppressed.</p> <p>They dress themselves as antisystems, pursuing the liberty behind the authority, and accepting the democratic system introducing totalitarianism nuances (Pol p.20).</p>"}, {"location": "antifascism/#how-to-identify-fascism", "title": "How to identify fascism", "text": "<p>We need to make sure that we use the term well, otherwise we run into the risk of the word loosing meaning. But equally important is not to fall in a wording discussion that paralyzes us.</p> <p>One way to make it measurable is to use Kimberl\u00e9 Williams Crenshaw intersectionality theory , which states that individuals experience oppression or privilege based on a belonging to a plurality of social categories, to measure how close an action or discourse follows fascism principles (Pol p.26).</p> <p> Source</p> <p>Fascism has always been carried out by people with many privileges (the upper part of the diagram) against collectives under many oppressions (the lower part of the diagram). We can then state that the more the oppressions a discourse defends and perpetuates, the more probable it is to be fascist. If it also translates into physical or verbal aggressions, escalates into the will to transform that discourse into laws that backs up those aggressions, or tries to build a government under those ideas, then we clearly have a political roadmap towards fascism.</p> <p>The fact that they don't propose to abolish the democracy or try to send people to concentration camps doesn't mean they are not fascist. First, we don't need them to commit the exact same crimes that the fascists of last century made to put at risk some social collectives, and secondly, history tells us that classic fascism movements didn't show their true intentions in their early phases.</p> <p>Fascism shifts their form and particular characteristics based on place and time. Waiting to see it clear is risking being late to fight it. Therefore whenever we see a discourse that comes from a privileged person against a oppressed one, we should fight it immediately, once fought, you can analyze if it was fascist or not (Pol p.28)</p>"}, {"location": "antifascism/#how-to-fight-fascism", "title": "How to fight fascism", "text": "<p>There are many ways to fight it, the book Todo el mundo puede ser Antifa: Manual practico para destruir el fascismo of Pol Andi\u00f1ach gathers some of them.</p> <p>One way we've seen pisses them off quite much is when they are ridiculed and they evocate the image of incompetence. It's a fine line to go, because if it falls into a pity image then it may strengthen their victim role.</p>"}, {"location": "antifascism/#references", "title": "References", "text": "<ul> <li>Antifa: The anti-fascist handbook by Mark Bray</li> <li>Todo el mundo puede ser Antifa: Manual practico para destruir el fascismo de Pol     Andi\u00f1ach</li> </ul>"}, {"location": "antifascism/#magazines", "title": "Magazines", "text": "<ul> <li>Hope not Hate</li> <li>Searchlight</li> </ul>"}, {"location": "antifascism/#podcasts", "title": "Podcasts", "text": "<ul> <li>Hope not Hate</li> </ul>"}, {"location": "antifascist_actions/", "title": "Antifa Actions", "text": "<p>Collection of amazing and inspiring antifa actions.</p>"}, {"location": "antifascist_actions/#2022", "title": "2022", "text": ""}, {"location": "antifascist_actions/#an-open-data-initiative-to-map-spanish-hate-crimes", "title": "An open data initiative to map spanish hate crimes", "text": "<p>The project Crimenes de Odio have created an open database of the hate crimes registered in the spanish state.</p>"}, {"location": "antifascist_actions/#an-open-data-initiative-to-map-spanish-fascist-icons", "title": "An open data initiative to map spanish fascist icons", "text": "<p>The project Deber\u00edaDesaparecer have created an open database of the remains of the spanish fascist regime icons. The visualization they've created is astonishing, and they've provided a form so that anyone can contribute to the dataset.</p>"}, {"location": "antifascist_actions/#2021", "title": "2021", "text": ""}, {"location": "antifascist_actions/#a-fake-company-and-five-million-recycled-flyers", "title": "A fake company and five million recycled flyers", "text": "<p>A group of artists belonging to the Center for political beauty created a fake company Flyerservice Hahn and convinced more than 80 regional sections of the far right party AfD to hire them to deliver their electoral propaganda.</p> <p>They gathered five million flyers, with a total weight of 72 tons. They justify that they wouldn't be able to lie to the people, so they did nothing in the broader sense of the word. They declared that they are the \"world wide leader in the non-delivery of nazi propaganda\". At the start of the electoral campaign, they went to the AfD stands, and they let their members to give them flyers the throw them to the closest bin. \"It's something that any citizen can freely do, we have only industrialized the process\".</p> <p>They've done a crowdfunding to fund the legal process that may result.</p>"}, {"location": "antitransphobia/", "title": "Anti-transphobia", "text": "<p>Anti-transphobia being reductionist is the opposition to the collection of ideas and phenomena that encompass a range of negative attitudes, feelings or actions towards transgender people or transness in general. Transphobia can include fear, aversion, hatred, violence, anger, or discomfort felt or expressed towards people who do not conform to social gender expectations. It is often expressed alongside homophobic views and hence is often considered an aspect of homophobia.</p> <p>It's yet another clear case of privileged people oppressing even further already oppressed collectives. We can clearly see it if we use the ever useful Kimberl\u00e9 Williams Crenshaw intersectionality theory diagram.</p> <p> Source</p>"}, {"location": "antitransphobia/#terf", "title": "TERF", "text": "<p>TERF is an acronym for trans-exclusionary radical feminist. The term originally applied to the minority of feminists that expressed transphobic sentiments such as the rejection of the assertion that trans women are women, the exclusion of trans women from women's spaces, and opposition to transgender rights legislation. The meaning has since expanded to refer more broadly to people with trans-exclusionary views who may have no involvement with radical feminism.</p>"}, {"location": "antitransphobia/#arguments-against-theories-that-deny-the-reality-of-trans-people", "title": "Arguments against theories that deny the reality of trans people", "text": "<p>This section is a direct translation from Alana Portero's text called Definitions.</p>"}, {"location": "antitransphobia/#sex-is-a-medical-category-and-gender-a-social-category", "title": "Sex is a medical category and gender a social category", "text": "<p>Sex is a medical category, it's not a biological one. According to body features like the chromosome structure and genitalia appearance, medicine assigns the sex (hence gender) to the bodies. In the case of intersexual people, they are usually mutilated and hormonated so that their bodies fit into one of the two options contemplated by the medicine.</p> <p>Gender is the term we use to refer to how a person feels about themselves as a boy/man, a girl/woman or non-binary.</p> <p>Since birth, we're told what's appropriate (and what isn't) for each gender. These are the gender roles. It's not the same gender than gender role: the gender determines how you interact with the other roles. For example, a woman can take traditionally understood male roles gender roles, that doesn't mean that she is or isn't a woman.</p> <p>The problem arises when these two oppressions are mixed up: cissexism (the believe that bodies have an immutable gender defined by the sex assigned at birth) and misogyny (the base of feminine oppression). When you mixing them up you get the idea that the trans movement erases the feminine structural oppression, when in reality, it broadens the scope and makes it more precise, as they suffer the same misogyny than the cis women.</p> <p>Women are killed for being women. They are socially assigned the responsibility for care, they are prevented from having individual will and they are deterred from accessing resources. This structural violence is suffered by all women regardless of the sex assigned at birth.</p> <p>Questioning the adjudication of gender to the bodies and questioning the roles assigned to the genders are complementary paths for the feminism liberation.</p>"}, {"location": "antitransphobia/#avoid-the-interested-manipulation-of-the-sexual-or-gender-identity", "title": "Avoid the interested manipulation of the sexual or gender identity", "text": "<p>The sexual or gender identity determines whether there is correspondence with the gender assigned at birth. When there isn't, it concerns a trans person.</p> <p>The sex and gender terms represent the same reality, being sex the medical term, and gender the academic one. Equally transexual and transgender represent the same reality, although these last have a pathologizing undertone, the term trans is preferred.</p>"}, {"location": "antitransphobia/#avoid-the-fears-of-letting-trans-people-be", "title": "Avoid the fears of letting trans people be", "text": "<p>Some are afraid that the trans women negatively affect the statistics of unemployment, laboral inequality, feminization of the poverty and machist violence, and they contradict the problems of the cis women.</p> <p>Trans people usually have a greater unemployment rate (85% in Spain), so the glass ceiling is not yet even a concern, and they are also greatly affected by machist violence.</p> <p>The queer theory doesn't erase or blur women as a political subject. Thinking that it risks the rights and achievements earned through the feminist movement shows a complete misunderstanding of the theory.</p>"}, {"location": "antitransphobia/#women-are-not-an-entity", "title": "Women are not an entity", "text": "<p>Women are not an entity, they are a group of people that are placed below men in the social scale, each with her own unique experience. The woman identity belongs to any person that identifies herself with it.</p> <p>The fight against discrimination and towards inclusion politics should be mandatory for all society, and shouldn't be used against the trans people.</p>"}, {"location": "antitransphobia/#references", "title": "References", "text": "<ul> <li>Wikipedia page on Transphobia</li> </ul>"}, {"location": "argocd/", "title": "ArgoCD", "text": "<p>Argo CD is a declarative, GitOps continuous delivery tool for Kubernetes.</p> <p>Argo CD follows the GitOps pattern of using Git repositories as the source of truth for defining the desired application state. Kubernetes manifests can be specified in several ways:</p> <ul> <li>kustomize applications</li> <li>helm charts</li> <li>jsonnet files</li> <li>Plain directory of YAML/json manifests</li> <li>Any custom config management tool configured as a config management plugin, for example with helmfile</li> </ul> <p>Argo CD automates the deployment of the desired application states in the specified target environments. Application deployments can track updates to branches, tags, or pinned to a specific version of manifests at a Git commit. See tracking strategies for additional details about the different tracking strategies available.</p>"}, {"location": "argocd/#using-helmfile", "title": "Using helmfile", "text": "<p><code>helmfile</code> is not yet supported officially, but you can use it through this plugin.</p>"}, {"location": "argocd/#references", "title": "References", "text": "<ul> <li>Docs</li> </ul>"}, {"location": "asyncio/", "title": "Asyncio", "text": "<p>asyncio is a library to write concurrent code using the async/await syntax.</p> <p>asyncio is used as a foundation for multiple Python asynchronous frameworks that provide high-performance network and web-servers, database connection libraries, distributed task queues, etc.</p> <p>asyncio is often a perfect fit for IO-bound and high-level structured network code.</p> <p>Note<p>\"Asyncer looks very useful\"</p> </p>"}, {"location": "asyncio/#basic-concepts", "title": "Basic concepts", "text": ""}, {"location": "asyncio/#concurrency", "title": "Concurrency", "text": "<p>Concurrency is best explained by an example stolen from Miguel Grinberg.</p> <p>Chess master Judit Polg\u00e1r hosts a chess exhibition in which she plays multiple amateur players. She has two ways of conducting the exhibition: synchronously and asynchronously.</p> <p>Assumptions:</p> <ul> <li>24 opponents</li> <li>Judit makes each chess move in 5 seconds</li> <li>Opponents each take 55 seconds to make a move</li> <li>Games average 30 pair-moves (60 moves total)</li> </ul> <p>Synchronous version: Judit plays one game at a time, never two at the same time, until the game is complete. Each game takes (55 + 5) * 30 == 1800 seconds, or 30 minutes. The entire exhibition takes 24 * 30 == 720 minutes, or 12 hours.</p> <p>Asynchronous version: Judit moves from table to table, making one move at each table. She leaves the table and lets the opponent make their next move during the wait time. One move on all 24 games takes Judit 24 * 5 == 120 seconds, or 2 minutes. The entire exhibition is now cut down to 120 * 30 == 3600 seconds, or just 1 hour.</p> <p>Async IO takes long waiting periods in which functions would otherwise be blocking and allows other functions to run during that downtime. (A function that blocks effectively forbids others from running from the time that it starts until the time that it returns.)</p>"}, {"location": "asyncio/#asyncio-is-not-easy", "title": "AsyncIO is not easy", "text": "<p>You may have heard the phrase \u201cUse async IO when you can; use threading when you must.\u201d The truth is that building durable multithreaded code can be hard and error-prone. Async IO avoids some of the potential speedbumps that you might otherwise encounter with a threaded design.</p> <p>But that\u2019s not to say that async IO in Python is easy. Be warned: when you venture a bit below the surface level, async programming can be difficult too! Python\u2019s async model is built around concepts such as callbacks, events, transports, protocols, and futures\u2014just the terminology can be intimidating. The fact that its API has been changing continually makes it no easier.</p> <p>Luckily, asyncio has matured to a point where most of its features are no longer provisional, while its documentation has received a huge overhaul and some quality resources on the subject are starting to emerge as well.</p>"}, {"location": "asyncio/#the-asyncawait-syntax-and-native-coroutines", "title": "The async/await Syntax and Native Coroutines", "text": "<p>At the heart of async IO are coroutines. A coroutine is a specialized version of a Python generator function. A coroutine is a function that can suspend its execution before reaching return, and it can indirectly pass control to another coroutine for some time. For example look at this Hello World async IO example:</p> <pre><code>#!/usr/bin/env python3\n# countasync.py\n\nimport asyncio\n\nasync def count():\n    print(\"One\")\n    await asyncio.sleep(1)\n    print(\"Two\")\n\nasync def main():\n    await asyncio.gather(count(), count(), count())\n\nif __name__ == \"__main__\":\n    import time\n    s = time.perf_counter()\n    asyncio.run(main())\n    elapsed = time.perf_counter() - s\n    print(f\"{__file__} executed in {elapsed:0.2f} seconds.\")\n</code></pre> <p>When you execute this file, take note of what looks different than if you were to define the functions with just <code>def</code> and <code>time.sleep()</code>:</p> <pre><code>$ python3 countasync.py\nOne\nOne\nOne\nTwo\nTwo\nTwo\ncountasync.py executed in 1.01 seconds.\n</code></pre> <p>The order of this output is the heart of async IO. Talking to each of the calls to <code>count()</code> is a single event loop, or coordinator. When each task reaches <code>await asyncio.sleep(1)</code>, the function talks to the event loop and gives control back to it saying, \u201cI\u2019m going to be sleeping for 1 second. Go ahead and let something else meaningful be done in the meantime.\u201d</p> <p>Contrast this to the synchronous version:</p> <pre><code>#!/usr/bin/env python3\n# countsync.py\n\nimport time\n\ndef count():\n    print(\"One\")\n    time.sleep(1)\n    print(\"Two\")\n\ndef main():\n    for _ in range(3):\n        count()\n\nif __name__ == \"__main__\":\n    s = time.perf_counter()\n    main()\n    elapsed = time.perf_counter() - s\n    print(f\"{__file__} executed in {elapsed:0.2f} seconds.\")\n</code></pre> <p>When executed, there is a slight but critical change in order and execution time:</p> <pre><code>$ python3 countsync.py\nOne\nTwo\nOne\nTwo\nOne\nTwo\ncountsync.py executed in 3.01 seconds.\n</code></pre> <p>Here <code>time.sleep()</code> can represent any time-consuming blocking function call, while <code>asyncio.sleep()</code> is used to stand in for a non-blocking time-consuming call.</p> <p>Summing up the benefit of awaiting something, including <code>asyncio.sleep()</code>, is that the surrounding function can temporarily cede control to another function that\u2019s more readily able to do something immediately. In contrast, <code>time.sleep()</code> or any other blocking call is incompatible with asynchronous Python code, because it will stop everything in its tracks for the duration of the sleep time.</p>"}, {"location": "asyncio/#the-rules-of-async-io", "title": "The Rules of Async IO", "text": "<p>There are a strict set of rules around when and how you can and cannot use <code>async</code>/<code>await</code>:</p> <ul> <li> <p>A function that you introduce with <code>async def</code> is a coroutine. It may use <code>await</code>, <code>return</code>, or <code>yield</code>, but all of these are optional.</p> <ul> <li>Using <code>await</code> and/or <code>return</code> creates a coroutine function. To call a coroutine function, you must <code>await</code> it to get its results.</li> <li>Using <code>yield</code> in an <code>async def</code> block creates an asynchronous generator, which you iterate over with <code>async for</code>. </li> <li>Anything defined with <code>async def</code> may not use <code>yield from</code>, which will raise a <code>SyntaxError</code>. (Remember that <code>yield from x()</code> is just syntactic sugar to replace for <code>i in x(): yield i</code>)</li> </ul> </li> <li> <p>The keyword <code>await</code> passes function control back to the event loop. (It suspends the execution of the surrounding coroutine.) If Python encounters an <code>await f()</code> expression in the scope of <code>g()</code>, this is how <code>await</code> tells the event loop, \u201cSuspend execution of g() until whatever I\u2019m waiting on\u2014the result of f()\u2014is returned. In the meantime, go let something else run.\u201d. In pseudo code this would be:</p> <pre><code>async def g():\n    r = await f() # Pause here and come back to g() when f() is ready\n    return r\n</code></pre> </li> <li> <p>You can't use <code>await</code> outside of an <code>async def</code> coroutine.</p> </li> <li>When you use await <code>f()</code>, it\u2019s required that <code>f()</code> be an object that is awaitable. An awaitable object is either another coroutine or an object defining an <code>.__await__()</code> method that returns an iterator. </li> </ul> <p>Here are some examples that summarize the above rules:</p> <pre><code>async def f(x):\n    y = await z(x)  # OK - `await` and `return` allowed in coroutines\n    return y\n\nasync def g(x):\n    yield x  # OK - this is an async generator\n\nasync def m(x):\n    yield from gen(x)  # No - SyntaxError\n\ndef m(x):\n    y = await z(x)  # No - SyntaxError (no `async def` here)\n    return y\n</code></pre>"}, {"location": "asyncio/#async-io-design-patterns", "title": "Async IO Design Patterns", "text": "<p>Async IO comes with its own set of possible script designs.</p>"}, {"location": "asyncio/#chaining-coroutines", "title": "Chaining Coroutines", "text": "<p>This allows you to break programs into smaller, manageable, recyclable coroutines:</p> <pre><code>#!/usr/bin/env python3\n# chained.py\n\nimport asyncio\nimport random\nimport time\n\nasync def part1(n: int) -&gt; str:\n    i = random.randint(0, 10)\n    print(f\"part1({n}) sleeping for {i} seconds.\")\n    await asyncio.sleep(i)\n    result = f\"result{n}-1\"\n    print(f\"Returning part1({n}) == {result}.\")\n    return result\n\nasync def part2(n: int, arg: str) -&gt; str:\n    i = random.randint(0, 10)\n    print(f\"part2{n, arg} sleeping for {i} seconds.\")\n    await asyncio.sleep(i)\n    result = f\"result{n}-2 derived from {arg}\"\n    print(f\"Returning part2{n, arg} == {result}.\")\n    return result\n\nasync def chain(n: int) -&gt; None:\n    start = time.perf_counter()\n    p1 = await part1(n)\n    p2 = await part2(n, p1)\n    end = time.perf_counter() - start\n    print(f\"--&gt;Chained result{n} =&gt; {p2} (took {end:0.2f} seconds).\")\n\nasync def main(*args):\n    await asyncio.gather(*(chain(n) for n in args))\n\nif __name__ == \"__main__\":\n    import sys\n    random.seed(444)\n    args = [1, 2, 3] if len(sys.argv) == 1 else map(int, sys.argv[1:])\n    start = time.perf_counter()\n    asyncio.run(main(*args))\n    end = time.perf_counter() - start\n    print(f\"Program finished in {end:0.2f} seconds.\")\n</code></pre> <p>Pay careful attention to the output, where <code>part1()</code> sleeps for a variable amount of time, and <code>part2()</code> begins working with the results as they become available:</p> <pre><code>$ python3 chained.py 9 6 3\npart1(9) sleeping for 4 seconds.\npart1(6) sleeping for 4 seconds.\npart1(3) sleeping for 0 seconds.\nReturning part1(3) == result3-1.\npart2(3, 'result3-1') sleeping for 4 seconds.\nReturning part1(9) == result9-1.\npart2(9, 'result9-1') sleeping for 7 seconds.\nReturning part1(6) == result6-1.\npart2(6, 'result6-1') sleeping for 4 seconds.\nReturning part2(3, 'result3-1') == result3-2 derived from result3-1.\n--&gt;Chained result3 =&gt; result3-2 derived from result3-1 (took 4.00 seconds).\nReturning part2(6, 'result6-1') == result6-2 derived from result6-1.\n--&gt;Chained result6 =&gt; result6-2 derived from result6-1 (took 8.01 seconds).\nReturning part2(9, 'result9-1') == result9-2 derived from result9-1.\n--&gt;Chained result9 =&gt; result9-2 derived from result9-1 (took 11.01 seconds).\nProgram finished in 11.01 seconds.\n</code></pre> <p>In this setup, the runtime of <code>main()</code> will be equal to the maximum runtime of the tasks that it gathers together and schedules.</p>"}, {"location": "asyncio/#using-a-queue", "title": "Using a Queue", "text": "<p>The <code>asyncio</code> package provides queue classes that are designed to be similar to classes of the <code>queue</code> module.</p> <p>There is an alternative structure that can also work with async IO: a number of producers, which are not associated with each other, add items to a queue. Each producer may add multiple items to the queue at staggered, random, unannounced times. A group of consumers pull items from the queue as they show up, greedily and without waiting for any other signal.</p> <p>In this design, there is no chaining of any individual consumer to a producer. The consumers don\u2019t know the number of producers, or even the cumulative number of items that will be added to the queue, in advance.</p> <p>It takes an individual producer or consumer a variable amount of time to put and extract items from the queue, respectively. The queue serves as a throughput that can communicate with the producers and consumers without them talking to each other directly.</p> <p>One use-case for queues is for the queue to act as a transmitter for producers and consumers that aren\u2019t otherwise directly chained or associated with each other.</p> <p>For example:</p> <pre><code>#!/usr/bin/env python3\n# asyncq.py\n\nimport asyncio\nimport itertools \nimport os\nimport random\nimport time\n\nasync def makeitem(size: int = 5) -&gt; str:\n    return os.urandom(size).hex()\n\nasync def randsleep(caller=None) -&gt; None:\n    i = random.randint(0, 10)\n    if caller:\n        print(f\"{caller} sleeping for {i} seconds.\")\n    await asyncio.sleep(i)\n\nasync def produce(name: int, q: asyncio.Queue) -&gt; None:\n    n = random.randint(0, 10)\n    for _ in itertools.repeat(None, n):  # Synchronous loop for each single producer\n        await randsleep(caller=f\"Producer {name}\")\n        i = await makeitem()\n        t = time.perf_counter()\n        await q.put((i, t))\n        print(f\"Producer {name} added &lt;{i}&gt; to queue.\")\n\nasync def consume(name: int, q: asyncio.Queue) -&gt; None:\n    while True:\n        await randsleep(caller=f\"Consumer {name}\")\n        i, t = await q.get()\n        now = time.perf_counter()\n        print(f\"Consumer {name} got element &lt;{i}&gt;\"\n              f\" in {now-t:0.5f} seconds.\")\n        q.task_done()\n\nasync def main(nprod: int, ncon: int):\n    q = asyncio.Queue()\n    producers = [asyncio.create_task(produce(n, q)) for n in range(nprod)]\n    consumers = [asyncio.create_task(consume(n, q)) for n in range(ncon)]\n    await asyncio.gather(*producers)\n    await q.join()  # Implicitly awaits consumers, too\n    for c in consumers:\n        c.cancel()\n\nif __name__ == \"__main__\":\n    import argparse\n    random.seed(444)\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"-p\", \"--nprod\", type=int, default=5)\n    parser.add_argument(\"-c\", \"--ncon\", type=int, default=10)\n    ns = parser.parse_args()\n    start = time.perf_counter()\n    asyncio.run(main(**ns.__dict__))\n    elapsed = time.perf_counter() - start\n    print(f\"Program completed in {elapsed:0.5f} seconds.\")\n</code></pre> <p>The challenging part of this workflow is that there needs to be a signal to the consumers that production is done. Otherwise, <code>await q.get()</code> will hang indefinitely, because the queue will have been fully processed, but consumers won\u2019t have any idea that production is complete. The key is to <code>await q.join()</code>, which blocks until all items in the queue have been received and processed, and then to cancel the consumer tasks, which would otherwise hang up and wait endlessly for additional queue items to appear.</p> <p>The first few coroutines are helper functions that return a random string, a fractional-second performance counter, and a random integer. A producer puts anywhere from 1 to 10 items into the queue. Each item is a tuple of <code>(i, t)</code> where <code>i</code> is a random string and <code>t</code> is the time at which the producer attempts to put the tuple into the queue.</p> <p>When a consumer pulls an item out, it simply calculates the elapsed time that the item sat in the queue using the timestamp that the item was put in with.</p> <p>Here is a test run with two producers and five consumers:</p> <pre><code>$ python3 asyncq.py -p 2 -c 5\nProducer 0 sleeping for 3 seconds.\nProducer 1 sleeping for 3 seconds.\nConsumer 0 sleeping for 4 seconds.\nConsumer 1 sleeping for 3 seconds.\nConsumer 2 sleeping for 3 seconds.\nConsumer 3 sleeping for 5 seconds.\nConsumer 4 sleeping for 4 seconds.\nProducer 0 added &lt;377b1e8f82&gt; to queue.\nProducer 0 sleeping for 5 seconds.\nProducer 1 added &lt;413b8802f8&gt; to queue.\nConsumer 1 got element &lt;377b1e8f82&gt; in 0.00013 seconds.\nConsumer 1 sleeping for 3 seconds.\nConsumer 2 got element &lt;413b8802f8&gt; in 0.00009 seconds.\nConsumer 2 sleeping for 4 seconds.\nProducer 0 added &lt;06c055b3ab&gt; to queue.\nProducer 0 sleeping for 1 seconds.\nConsumer 0 got element &lt;06c055b3ab&gt; in 0.00021 seconds.\nConsumer 0 sleeping for 4 seconds.\nProducer 0 added &lt;17a8613276&gt; to queue.\nConsumer 4 got element &lt;17a8613276&gt; in 0.00022 seconds.\nConsumer 4 sleeping for 5 seconds.\nProgram completed in 9.00954 seconds.\n</code></pre> <p>In this case, the items process in fractions of a second. A delay can be due to two reasons:</p> <ul> <li>Standard, largely unavoidable overhead</li> <li>Situations where all consumers are sleeping when an item appears in the queue</li> </ul> <p>With regards to the second reason, luckily, it is perfectly normal to scale to hundreds or thousands of consumers. You should have no problem with <code>python3 asyncq.py -p 5 -c 100</code>. The point here is that, theoretically, you could have different users on different systems controlling the management of producers and consumers, with the queue serving as the central throughput.</p>"}, {"location": "asyncio/#async-for-and-list-comprehensions", "title": "<code>async for</code> and list comprehensions", "text": "<p>You can use <code>async for</code> to iterate over an asynchronous iterator. The purpose of an asynchronous iterator is for it to be able to call asynchronous code at each stage when it is iterated over.</p> <p>A natural extension of this concept is an asynchronous generator:</p> <pre><code>&gt;&gt;&gt; async def mygen(u: int = 10):\n...     \"\"\"Yield powers of 2.\"\"\"\n...     i = 0\n...     while i &lt; u:\n...         yield 2 ** i\n...         i += 1\n...         await asyncio.sleep(0.1)\n</code></pre> <p>You can also use asynchronous comprehension with <code>async for</code>:</p> <pre><code>&gt;&gt;&gt; async def main():\n...     # This does *not* introduce concurrent execution\n...     # It is meant to show syntax only\n...     g = [i async for i in mygen()]\n...     f = [j async for j in mygen() if not (j // 3 % 5)]\n...     return g, f\n...\n&gt;&gt;&gt; g, f = asyncio.run(main())\n&gt;&gt;&gt; g\n[1, 2, 4, 8, 16, 32, 64, 128, 256, 512]\n&gt;&gt;&gt; f\n[1, 2, 16, 32, 256, 512]\n</code></pre> <p>Asynchronous iterators and asynchronous generators are not designed to concurrently map some function over a sequence or iterator. They\u2019re merely designed to let the enclosing coroutine allow other tasks to take their turn. The <code>async for</code> and <code>async with</code> statements are only needed to the extent that using plain <code>for</code> or <code>with</code> would \u201cbreak\u201d the nature of <code>await</code> in the coroutine. </p>"}, {"location": "asyncio/#the-event-loop-and-asynciorun", "title": "The Event Loop and asyncio.run()", "text": "<p>You can think of an event loop as something like a <code>while True</code> loop that monitors coroutines, taking feedback on what\u2019s idle, and looking around for things that can be executed in the meantime. It is able to wake up an idle coroutine when whatever that coroutine is waiting on becomes available.</p> <p>Thus far, the entire management of the event loop has been implicitly handled by one function call:</p> <pre><code>asyncio.run(main())\n</code></pre> <p><code>asyncio.run()</code> is responsible for getting the event loop, running tasks until they are marked as complete, and then closing the event loop.</p> <p>If you do need to interact with the event loop within a Python program, <code>loop</code> (obtained through <code>loop = asyncio.get_event_loop()</code>) is a good-old-fashioned Python object that supports introspection with <code>loop.is_running()</code> and <code>loop.is_closed()</code>. You can manipulate it if you need to get more fine-tuned control, such as in scheduling a callback by passing the loop as an argument.</p> <p>Some important points regarding the event loop are:</p> <ul> <li> <p>Coroutines don\u2019t do much on their own until they are tied to the event loop. If you have a main coroutine that awaits others, simply calling it in isolation has little effect:</p> <pre><code>&gt;&gt;&gt; import asyncio\n\n&gt;&gt;&gt; async def main():\n...     print(\"Hello ...\")\n...     await asyncio.sleep(1)\n...     print(\"World!\")\n\n&gt;&gt;&gt; routine = main()\n&gt;&gt;&gt; routine\n&lt;coroutine object main at 0x1027a6150&gt;\n</code></pre> <p>Remember to use <code>asyncio.run()</code> to actually force execution by scheduling the <code>main()</code> coroutine (future object) for execution on the event loop:</p> <pre><code>&gt;&gt;&gt; asyncio.run(routine)\nHello ...\nWorld!\n</code></pre> </li> <li> <p>By default, an async IO event loop runs in a single thread and on a single CPU core. It is also possible to run event loops across multiple cores. Check out this talk by John Reese for more, and be warned that your laptop may spontaneously combust.</p> </li> </ul>"}, {"location": "asyncio/#creating-and-gathering-tasks", "title": "Creating and gathering tasks", "text": "<p>You can use <code>create_task()</code> to schedule the execution of a coroutine object, followed by <code>asyncio.run()</code>:</p> <pre><code>&gt;&gt;&gt; import asyncio\n\n&gt;&gt;&gt; async def coro(seq) -&gt; list:\n...     \"\"\"'IO' wait time is proportional to the max element.\"\"\"\n...     await asyncio.sleep(max(seq))\n...     return list(reversed(seq))\n...\n&gt;&gt;&gt; async def main():\n...     # This is a bit redundant in the case of one task\n...     # We could use `await coro([3, 2, 1])` on its own\n...     t = asyncio.create_task(coro([3, 2, 1])) \n...     await t\n...     print(f't: type {type(t)}')\n...     print(f't done: {t.done()}')\n...\n&gt;&gt;&gt; t = asyncio.run(main())\nt: type &lt;class '_asyncio.Task'&gt;\nt done: True\n</code></pre> <p>There\u2019s a subtlety to this pattern: if you don\u2019t <code>await t</code> within <code>main()</code>, it may finish before <code>main()</code> itself signals that it is complete. Because <code>asyncio.run(main())</code> calls <code>loop.run_until_complete(main())</code>, the event loop is only concerned (without <code>await t</code> present) that <code>main()</code> is done, not that the tasks that get created within <code>main()</code> are done, if this happens the loop\u2019s other tasks will be cancelled, possibly before they are completed. If you need to get a list of currently pending tasks, you can use <code>asyncio.Task.all_tasks()</code>.</p> <p>Separately, there\u2019s <code>asyncio.gather()</code> which is meant to neatly put a collection of coroutines (futures) into a single future. As a result, it returns a single future object, and, if you <code>await asyncio.gather()</code> and specify multiple tasks or coroutines, you\u2019re waiting for all of them to be completed. (This somewhat parallels <code>queue.join()</code> from our earlier example.) The result of <code>gather()</code> will be a list of the results across the inputs:</p> <pre><code>&gt;&gt;&gt; import time\n&gt;&gt;&gt; async def main():\n...     t = asyncio.create_task(coro([3, 2, 1]))\n...     t2 = asyncio.create_task(coro([10, 5, 0]))  # Python 3.7+\n...     print('Start:', time.strftime('%X'))\n...     a = await asyncio.gather(t, t2)\n...     print('End:', time.strftime('%X'))  # Should be 10 seconds\n...     print(f'Both tasks done: {all((t.done(), t2.done()))}')\n...     return a\n...\n&gt;&gt;&gt; a = asyncio.run(main())\nStart: 16:20:11\nEnd: 16:20:21\nBoth tasks done: True\n&gt;&gt;&gt; a\n[[1, 2, 3], [0, 5, 10]]\n</code></pre> <p>You can loop over <code>asyncio.as_completed()</code> to get tasks as they are completed, in the order of completion. The function returns an iterator that yields tasks as they finish. Below, the result of <code>coro([3, 2, 1])</code> will be available before <code>coro([10, 5, 0])</code> is complete, which is not the case with <code>gather()</code>:</p> <pre><code>&gt;&gt;&gt; async def main():\n...     t = asyncio.create_task(coro([3, 2, 1]))\n...     t2 = asyncio.create_task(coro([10, 5, 0]))\n...     print('Start:', time.strftime('%X'))\n...     for res in asyncio.as_completed((t, t2)):\n...         compl = await res\n...         print(f'res: {compl} completed at {time.strftime(\"%X\")}')\n...     print('End:', time.strftime('%X'))\n...     print(f'Both tasks done: {all((t.done(), t2.done()))}')\n...\n&gt;&gt;&gt; a = asyncio.run(main())\nStart: 09:49:07\nres: [1, 2, 3] completed at 09:49:10\nres: [0, 5, 10] completed at 09:49:17\nEnd: 09:49:17\nBoth tasks done: True\n</code></pre> <p>Lastly, you may also see <code>asyncio.ensure_future()</code>. You should rarely need it, because it\u2019s a lower-level plumbing API and largely replaced by <code>create_task()</code>, which was introduced later.</p>"}, {"location": "asyncio/#when-and-why-is-async-io-the-right-choice", "title": "When and Why Is Async IO the Right Choice?", "text": "<p>If you have multiple, fairly uniform CPU-bound tasks (a great example is a grid search in libraries such as scikit-learn or keras), multiprocessing should be an obvious choice.</p> <p>Simply putting <code>async</code> before every function is a bad idea if all of the functions use blocking calls. This can actually slow down your code.</p> <p>The contest between async IO and threading is a little bit more direct. Even in cases where threading seems easy to implement, it can still lead to infamous impossible-to-trace bugs due to race conditions and memory usage, among other things.</p> <p>Threading also tends to scale less elegantly than async IO, because threads are a system resource with a finite availability. Creating thousands of threads will fail on many machines. Creating thousands of async IO tasks is completely feasible.</p> <p>Async IO shines when you have multiple IO-bound tasks where the tasks would otherwise be dominated by blocking IO-bound wait time, such as:</p> <ul> <li>Network IO, whether your program is the server or the client side</li> <li>Serverless designs, such as a peer-to-peer, multi-user network like a group chatroom</li> <li>Read/write operations where you want to mimic a \u201cfire-and-forget\u201d style but worry less about holding a lock on whatever you\u2019re reading and writing to</li> </ul> <p>The biggest reason not to use it is that <code>await</code> only supports a specific set of objects that define a specific set of methods. If you want to do async read operations with a certain DBMS, you\u2019ll need to find not just a Python wrapper for that DBMS, but one that supports the async/await syntax. Coroutines that contain synchronous calls block other coroutines and tasks from running.</p>"}, {"location": "asyncio/#async-io-it-is-but-which-one", "title": "Async IO It Is, but Which One?", "text": "<p><code>asyncio</code> certainly isn\u2019t the only async IO library out there. The most popular are:</p> <ul> <li><code>anyio</code></li> <li><code>curio</code></li> <li><code>trio</code></li> </ul> <p>You might find that they get the same thing done in a way that\u2019s more intuitive for you as the user. Many of the package-agnostic concepts presented here should permeate to alternative async IO packages as well. But if you\u2019re building a moderately sized, straightforward program, just using <code>asyncio</code> is plenty sufficient and understandable, and lets you avoid adding yet another large dependency outside of Python\u2019s standard library.</p>"}, {"location": "asyncio/#snippets", "title": "Snippets", "text": ""}, {"location": "asyncio/#write-on-file", "title": "Write on file", "text": "<pre><code>import aiofiles\nasync with aiofiles.open(file, \"a\") as f:\n    for p in res:\n        await f.write(f\"{url}\\t{p}\\n\")\n</code></pre>"}, {"location": "asyncio/#do-http-requests", "title": "Do http requests", "text": "<p>Use the <code>aiohttp</code> library</p>"}, {"location": "asyncio/#tips", "title": "Tips", "text": ""}, {"location": "asyncio/#limit-concurrency", "title": "Limit concurrency", "text": "<p>Use <code>asyncio.Semaphore</code>.</p> <pre><code>sem = asyncio.Semaphore(10)\nasync with sem:\n    # work with shared resource\n</code></pre> <p>Note that this method is not thread-safe.</p>"}, {"location": "asyncio/#testing", "title": "Testing", "text": "<p>With the <code>pytest-asyncio</code> plugin you can test code that uses the asyncio library.</p> <p>Install it with <code>pip install pytest-asyncio</code></p> <p>Specifically, <code>pytest-asyncio</code> provides support for coroutines as test functions. This allows users to await code inside their tests. For example, the following code is executed as a test item by pytest:</p> <pre><code>@pytest.mark.asyncio\nasync def test_some_asyncio_code():\n    res = await library.do_something()\n    assert b\"expected result\" == res\n</code></pre> <p><code>pytest-asyncio</code> runs each test item in its own <code>asyncio</code> event loop. The loop can be accessed via the <code>event_loop</code> fixture, which is automatically requested by all async tests.</p> <pre><code>async def test_provided_loop_is_running_loop(event_loop):\n    assert event_loop is asyncio.get_running_loop()\n</code></pre> <p>You can think of <code>event_loop</code> as an <code>autouse</code> fixture for async tests.</p> <p>It has two discovery modes:</p> <ul> <li>Strict mode: will only run tests that have the <code>asyncio</code> marker and will only evaluate async fixtures decorated with <code>@pytest_asyncio.fixture</code></li> <li> <p>Auto mode: will automatically add the <code>asyncio</code> marker to all asynchronous test functions. It will also take ownership of all async fixtures, regardless of whether they are decorated with <code>@pytest.fixture</code> or <code>@pytest_asyncio.fixture</code>. </p> <p>This mode is intended for projects that use <code>asyncio</code> as their only asynchronous programming library. Auto mode makes for the simplest test and fixture configuration and is the recommended default.</p> </li> </ul> <p>To use the Auto mode you need to pass the <code>--asyncio-mode=auto</code> flag to <code>pytest</code>. If you use <code>pyproject.toml</code> you can set the next configuration:</p> <pre><code>[tool.pytest.ini_options]\naddopts = \"--asyncio-mode=auto\"\n</code></pre>"}, {"location": "asyncio/#references", "title": "References", "text": "<ul> <li>Docs</li> <li>Awesome Asyncio</li> <li>Real python tutorial</li> <li>Roguelynn tutorial</li> </ul>"}, {"location": "asyncio/#libraries-to-explore", "title": "Libraries to explore", "text": "<ul> <li>Asyncer</li> </ul>"}, {"location": "authentik/", "title": "Authentik", "text": "<p>Authentik is an open-source Identity Provider focused on flexibility and versatility.</p> <p>What I like:</p> <ul> <li>Is maintained and popular</li> <li>It has a clean interface</li> <li>They have their own terraform provider Oo!</li> </ul> <p>What I don't like:</p> <ul> <li> <p>It's heavy focused on GUI interaction, but you can export the configuration to YAML files to be applied without the GUI interaction.</p> </li> <li> <p>The documentation is oriented to developers and not users. It's a little difficult to get a grasp on how to do things in the platform without following blog posts.</p> </li> </ul>"}, {"location": "authentik/#installation", "title": "Installation", "text": "<p>You can install it with Kubernetes or with <code>docker-compose</code>. I'm going to do the second.</p> <p>Download the latest <code>docker-compose.yml</code> from here. Place it in a directory of your choice.</p> <p>If this is a fresh authentik install run the following commands to generate a password:</p> <pre><code># You can also use openssl instead: `openssl rand -base64 36`\nsudo apt-get install -y pwgen\n# Because of a PostgreSQL limitation, only passwords up to 99 chars are supported\n# See https://www.postgresql.org/message-id/09512C4F-8CB9-4021-B455-EF4C4F0D55A0@amazon.com\necho \"PG_PASS=$(pwgen -s 40 1)\" &gt;&gt; .env\necho \"AUTHENTIK_SECRET_KEY=$(pwgen -s 50 1)\" &gt;&gt; .env\n</code></pre> <p>It is also recommended to configure global email credentials. These are used by authentik to notify you about alerts and configuration issues. They can also be used by Email stages to send verification/recovery emails.</p> <p>Append this block to your .env file</p> <pre><code># SMTP Host Emails are sent to\nAUTHENTIK_EMAIL__HOST=localhost\nAUTHENTIK_EMAIL__PORT=25\n# Optionally authenticate (don't add quotation marks to your password)\nAUTHENTIK_EMAIL__USERNAME=\nAUTHENTIK_EMAIL__PASSWORD=\n# Use StartTLS\nAUTHENTIK_EMAIL__USE_TLS=false\n# Use SSL\nAUTHENTIK_EMAIL__USE_SSL=false\nAUTHENTIK_EMAIL__TIMEOUT=10\n# Email address authentik will send from, should have a correct @domain\nAUTHENTIK_EMAIL__FROM=authentik@localhost\n</code></pre> <p>By default, authentik listens on port 9000 for HTTP and 9443 for HTTPS. To change this, you can set the following variables in .env:</p> <pre><code>AUTHENTIK_PORT_HTTP=80\nAUTHENTIK_PORT_HTTPS=443\n</code></pre> <p>You may need to tweak the <code>volumes</code> and the <code>networks</code> sections of the <code>docker-compose.yml</code> to your liking.</p> <p>Once everything is set you can run <code>docker-compose up</code> to test everything is working.</p> <p>In your browser, navigate to authentik\u2019s initial setup page https://auth.home.yourdomain.com/if/flow/initial-setup/</p> <p>Set the email and password for the default admin user, <code>akadmin</code>. You\u2019re now logged in.</p>"}, {"location": "authentik/#configuration", "title": "Configuration", "text": ""}, {"location": "authentik/#terraform", "title": "Terraform", "text": "<p>You can use <code>terraform</code> to configure authentik! <code>&lt;3</code>.</p>"}, {"location": "authentik/#configure-the-provider", "title": "Configure the provider", "text": "<p>To configure the provider you need to specify the url and an Authentik API token, keeping in mind that whoever gets access to this information will have access and full permissions on your Authentik instance it's critical that you store this information well. We'll use <code>sops</code> to encrypt the token with GPG..</p> <p>First create an Authentik user under <code>Admin interface/Directory/Users</code> with the next attributes:</p> <ul> <li>Username: <code>terraform</code></li> <li>Name: <code>Terraform</code></li> <li>Path: <code>infra</code></li> <li>Groups: <code>Admin</code></li> </ul> <p>Then create a token with name <code>Terraform</code> under <code>Directory/Tokens &amp; App passwords</code>, copy it to your clipboard.</p> <p>Configure <code>sops</code> by defining the gpg keys in a <code>.sops.yaml</code> file at the top of your repository:</p> <pre><code>---\ncreation_rules:\n- pgp: &gt;-\n2829BASDFHWEGWG23WDSLKGL323534J35LKWERQS,\n2GEFDBW349YHEDOH2T0GE9RH0NEORIG342RFSLHH\n</code></pre> <p>Then create the secrets file with the command <code>sops secrets.enc.json</code> somewhere in your terraform repository. For example:</p> <pre><code>{\n\"authentik_token\": \"paste the token here\"\n}\n</code></pre> <pre><code>terraform {\nrequired_providers {\nauthentik = {\nsource = \"goauthentik/authentik\"\nversion = \"~&gt; 2023.1.1\"\n}\nsops = {\nsource = \"carlpett/sops\"\nversion = \"~&gt; 0.5\"\n}\n}\n}\n\nprovider \"authentik\" {\nurl   = \"https://oauth.your-domain.org\"\ntoken = data.sops_file.secrets.data[\"authentik_token\"]\n}\n</code></pre>"}, {"location": "authentik/#configure-some-common-applications", "title": "Configure some common applications", "text": "<p>You have some guides to connect some popular applications</p>"}, {"location": "authentik/#gitea", "title": "Gitea", "text": "<p>You can follow the Authentik Gitea docs or you can use the next terraform snippet:</p> <pre><code># ----------------\n# --    Data    --\n# ----------------\n\ndata \"authentik_flow\" \"default-authorization-flow\" {\nslug = \"default-provider-authorization-implicit-consent\"\n}\n\n# -----------------------\n# --    Application    --\n# -----------------------\n\nresource \"authentik_application\" \"gitea\" {\nname              = \"Gitea\"\nslug              = \"gitea\"\nprotocol_provider = authentik_provider_oauth2.gitea.id\nmeta_icon = \"application-icons/gitea.svg\"\nlifecycle {\nignore_changes = [\n      # The terraform provider is continuously changing the attribute even though it's set\nmeta_icon,\n]\n}\n}\n\n# --------------------------\n# --    Oauth provider    --\n# --------------------------\n\nresource \"authentik_provider_oauth2\" \"gitea\" {\nname               = \"Gitea\"\nclient_id = \"gitea\"\nauthorization_flow = data.authentik_flow.default-authorization-flow.id\nproperty_mappings = [\nauthentik_scope_mapping.gitea.id,\ndata.authentik_scope_mapping.email.id,\ndata.authentik_scope_mapping.openid.id,\ndata.authentik_scope_mapping.profile.id,\n]\nredirect_uris = [\n\"https://git.your-domain.org/user/oauth2/authentik/callback\",\n]\nsigning_key = data.authentik_certificate_key_pair.default.id\n}\n\ndata \"authentik_certificate_key_pair\" \"default\" {\nname = \"authentik Self-signed Certificate\"\n}\n\n# -------------------------\n# --    Scope mapping    --\n# -------------------------\n\nresource \"authentik_scope_mapping\" \"gitea\" {\nname       = \"Gitea\"\nscope_name = \"gitea\"\nexpression = &lt;&lt;EOF\ngitea_claims = {}\nif request.user.ak_groups.filter(name=\"Users\").exists():\n    gitea_claims[\"gitea\"]= \"user\"\nif request.user.ak_groups.filter(name=\"Admins\").exists():\n    gitea_claims[\"gitea\"]= \"admin\"\n\nreturn gitea_claims\nEOF\n}\n\ndata \"authentik_scope_mapping\" \"email\" {\nmanaged = \"goauthentik.io/providers/oauth2/scope-email\"\n}\n\ndata \"authentik_scope_mapping\" \"openid\" {\nmanaged = \"goauthentik.io/providers/oauth2/scope-openid\"\n}\n\ndata \"authentik_scope_mapping\" \"profile\" {\nmanaged = \"goauthentik.io/providers/oauth2/scope-profile\"\n}\n\n# -------------------\n# --    Outputs    --\n# -------------------\n\noutput \"gitea_oauth_id\" {\nvalue = authentik_provider_oauth2.gitea.client_id\n}\n\noutput \"gitea_oauth_secret\" {\nvalue = authentik_provider_oauth2.gitea.client_secret\n}\n</code></pre> <p>It assumes that:</p> <ul> <li>You've changed <code>git.your-domain.org</code> with your gitea domain.</li> <li>The gitea logo is mounted in the docker directory <code>/media/application-icons/gitea.svg</code>.</li> </ul> <p>Gitea can be configured through terraform too. There is an official provider that doesn't work, there's a [fork that does though[(https://registry.terraform.io/providers/Lerentis/gitea/latest/docs). Sadly it doesn't yet support configuring Oauth Authentication sources. So you'll need to configure it manually.</p> <p>Be careful <code>gitea_oauth2_app</code> looks to be the right resource to do that, but instead it configures Gitea to be the Oauth provider, not a consumer.</p>"}, {"location": "authentik/#configure-the-invitation-flow", "title": "Configure the invitation flow", "text": "<p>Let's assume that we have two groups (Admins and Users) created under <code>Directory/Groups</code> and that we want to configure an invitation link for a user to be added directly on the <code>Admins</code> group.</p> <p>Authentik works by defining Stages and Flows. Stages are the steps you need to follow to complete a procedure, and a flow is the procedure itself.</p> <p>You create Stages by: * Going to the Admin interface * Going to Flows &amp; Stages/Stages * Click on Create</p> <p>To be able to complete the invitation through link we need to define the next stages:</p> <ul> <li> <p>An Invitation Stage: This stage represents the moment an admin chooses to create an invitation for a user.    Graphically you would need to:</p> </li> <li> <p>Click on Create</p> </li> <li>Select Invitation Stage</li> <li>Fill the form with the next data:<ul> <li>Name: enrollment-invitation</li> <li>Uncheck the <code>Continue flow without invitation</code> as we don't want users to be able to register without the invitation.</li> </ul> </li> <li>Click Finish</li> </ul> <p>Or use the next terraform snippet:</p> <pre><code>resource \"authentik_stage_invitation\" \"default\" {\nname                             = \"enrollment-invitation\"\ncontinue_flow_without_invitation = false\n}\n</code></pre> <ul> <li> <p>An User Write Stage: This is when the user will be created but it won't show up as the username and password are not yet selected.   Graphically you would need to:</p> <ul> <li>Click on Create</li> <li>Select User Write Stage</li> <li>Click on Next</li> <li>Fill the form with the next data:</li> <li>Name: enrollment-invitation-admin-write</li> <li>Enable the <code>Can Create Users</code> flag.</li> <li>If you want users to validate their email leave \"Create users as inactive\" enabled, otherwise disable it.</li> <li>Select the group you want the user to be added to. I don't yet know how to select more than one group</li> <li>Click on Finish</li> </ul> </li> </ul> <p>Or use the next terraform snippet:</p> <pre><code>resource \"authentik_stage_user_write\" \"admin_write\" {\nname                     = \"enrollment-invitation-admin-write\"\ncreate_users_as_inactive = true\ncreate_users_group       = authentik_group.admins.id\n}\n</code></pre> <p>Where <code>authentik_group.admin</code> is defined as:</p> <pre><code>resource \"authentik_group\" \"admins\" {\nname         = \"Admins\"\nis_superuser = true\nusers = [\ndata.authentik_user.user_1.id,\ndata.authentik_user.user_2.id,\n]\n}\n\ndata \"authentik_user\" \"user_1\" {\nusername = \"user_1\"\n}\n\ndata \"authentik_user\" \"user_2\" {\nusername = \"user_2\"\n}\n</code></pre> <ul> <li>Email Confirmation Stage: This is when the user gets an email to confirm that it has access to it</li> </ul> <p>Graphically you would need to:</p> <ul> <li>Click on Create</li> <li>Select Email Stage</li> <li>Click on Next<ul> <li>Name: email-account-confirmation</li> <li>Subject: Account confirmation</li> <li>Template: Account confirmation</li> </ul> </li> <li>Click on Finish</li> </ul> <p>Or use the next terraform snippet:</p> <pre><code>resource \"authentik_stage_email\" \"account_confirmation\" {\nname                     = \"email-account-confirmation\"\nactivate_user_on_success = true\nsubject                  = \"Authentik Account Confirmation\"\ntemplate                 = \"email/account_confirmation.html\"\ntimeout                  = 10\n}\n</code></pre> <p>Create the invitation Flow:</p> <p>Graphically you would need to:</p> <ul> <li>Go to <code>Flows &amp; Stages/Flows</code></li> <li>Click on Create</li> <li>Fill the form with the next data:<ul> <li>Name: Enrollment Invitation Admin</li> <li>Title: Enrollment Invitation Admin</li> <li>Designation: Enrollment</li> <li>Unfold the Behavior settings to enable the Compatibility mode</li> </ul> </li> <li>Click Create</li> </ul> <p>Or use the next terraform snippet:</p> <pre><code>resource \"authentik_flow\" \"enrollment_admin\" {\nname        = \"Enrollment invitation admin\"\ntitle       = \"Enrollment invitation admin\"\nslug        = \"enrollment-invitation-admin\"\ndesignation = \"enrollment\"\n}\n</code></pre> <p>We need to define how the flow is going to behave by adding the different the stage bindings:</p> <ul> <li>Bind the Invitation admin stage:</li> </ul> <p>Graphically you would need to:</p> <ul> <li>Click on the flow we just created <code>enrollment-invitation-admin</code></li> <li>Click on <code>Stage Bindings</code></li> <li>Click on <code>Bind Stage</code></li> <li>Fill the form with the next data:<ul> <li>Stage: select <code>enrollment-invitation-admin</code></li> <li>Order: 10</li> </ul> </li> <li>Click Create</li> </ul> <p>Or use the next terraform snippet:</p> <pre><code>resource \"authentik_flow_stage_binding\" \"invitation_creation\" {\ntarget = authentik_flow.enrollment_admin.uuid\nstage  = authentik_stage_invitation.default.id\norder  = 10\n}\n</code></pre> <ul> <li>Bind the Enrollment prompt stage: This is a builtin stage where the user is asked for their login information</li> </ul> <p>Graphically you would need to:</p> <ul> <li>Click on <code>Bind Stage</code></li> <li>Fill the form with the next data:<ul> <li>Stage: select <code>default-source-enrollment-prompt</code></li> <li>Order: 20</li> </ul> </li> <li>Click Create</li> <li>Click Edit Stage and configure it wit:<ul> <li>On the fields select: </li> <li>username</li> <li>name</li> <li>email</li> <li>password</li> <li>password_repeat</li> <li>Select the validation policy you have one</li> </ul> </li> </ul> <p>Or use the next terraform snippet:</p> <pre><code>resource \"authentik_stage_prompt\" \"user_data\" {\nname = \"enrollment-user-data-prompt\"\nfields = [ authentik_stage_prompt_field.username.id,\nauthentik_stage_prompt_field.name.id,\nauthentik_stage_prompt_field.email.id,\nauthentik_stage_prompt_field.password.id,\nauthentik_stage_prompt_field.password_repeat.id,\n]\n}\n\nresource \"authentik_stage_prompt_field\" \"username\" {\nfield_key = \"username\"\nlabel     = \"Username\"\ntype      = \"text\"\norder = 200\nplaceholder = &lt;&lt;EOT\ntry:\n    return user.username\nexcept:\n    return ''\nEOT\nplaceholder_expression = true\nrequired = true\n}\n\nresource \"authentik_stage_prompt_field\" \"name\" {\nfield_key = \"name\"\nlabel     = \"Name\"\ntype      = \"text\"\norder = 201\nplaceholder = &lt;&lt;EOT\ntry:\n    return user.name\nexcept:\n    return ''\nEOT\nplaceholder_expression = true\nrequired = true\n}\n\nresource \"authentik_stage_prompt_field\" \"email\" {\nfield_key = \"email\"\nlabel     = \"Email\"\ntype      = \"email\"\norder = 202\nplaceholder = &lt;&lt;EOT\ntry:\n    return user.email\nexcept:\n    return ''\nEOT\nplaceholder_expression = true\nrequired = true\n}\n\nresource \"authentik_stage_prompt_field\" \"password\" {\nfield_key = \"password\"\nlabel     = \"Password\"\ntype      = \"password\"\norder = 300\nplaceholder = \"Password\"\nplaceholder_expression = false\nrequired = true\n}\n\nresource \"authentik_stage_prompt_field\" \"password_repeat\" {\nfield_key = \"password_repeat\"\nlabel     = \"Password (repeat)\"\ntype      = \"password\"\norder = 301\nplaceholder = \"Password (repeat)\"\nplaceholder_expression = false\nrequired = true\n}\n</code></pre> <p>We had to redefine all the <code>authentik_stage_prompt_field</code> because the terraform provider doesn't yet support the <code>data</code> resource of the <code>authentik_stage_prompt_field</code></p> <ul> <li>Bind the User write stage:</li> </ul> <p>Graphically you would need to:</p> <ul> <li>Click on <code>Bind Stage</code></li> <li>Fill the form with the next data:<ul> <li>Stage: select <code>enrollment-invitation-admin-write</code></li> <li>Order: 30</li> </ul> </li> <li>Click Create</li> </ul> <p>Or use the next terraform snippet:</p> <pre><code>resource \"authentik_flow_stage_binding\" \"invitation_user_write\" {\ntarget = authentik_flow.enrollment_admin.uuid\nstage  = authentik_stage_user_write.admin_write.id\norder  = 30\n}\n</code></pre> <ul> <li>Bind the email account confirmation stage: </li> </ul> <p>Graphically you would need to:</p> <ul> <li>Click on <code>Bind Stage</code></li> <li>Fill the form with the next data:<ul> <li>Stage: select <code>email-account-confirmation</code></li> <li>Order: 40</li> </ul> </li> <li>Click Create</li> <li>Edit the stage and make sure that you have enabled:<ul> <li>Activate pending user on success</li> <li>Use global settings</li> </ul> </li> <li>Click Update</li> </ul> <p>Or use the next terraform snippet:</p> <pre><code>resource \"authentik_flow_stage_binding\" \"invitation_account_confirmation\" {\ntarget = authentik_flow.enrollment_admin.uuid\nstage  = authentik_stage_email.account_confirmation.id\norder  = 40\n}\n</code></pre> <ul> <li>Bind the User login stage: This is a builtin stage where the user is asked to log in</li> </ul> <p>Graphically you would need to:</p> <ul> <li>Click on <code>Bind Stage</code></li> <li>Fill the form with the next data:<ul> <li>Stage: select <code>default-source-enrollment-login</code></li> <li>Order: 50</li> </ul> </li> <li>Click Create</li> </ul> <p>Or use the next terraform snippet:</p> <pre><code>resource \"authentik_flow_stage_binding\" \"invitation_login\" {\ntarget = authentik_flow.enrollment_admin.uuid\nstage  = data.authentik_stage.default_source_enrollment_login.id\norder  = 50\n}\n</code></pre>"}, {"location": "authentik/#configure-password-recovery", "title": "Configure password recovery", "text": "<p>Recovery of password is not enabled by default, to configure it you need to create two new stages:</p> <ul> <li>An identification stage:</li> </ul> <pre><code>data \"authentik_source\" \"built_in\" {\nmanaged = \"goauthentik.io/sources/inbuilt\"\n}\n\nresource \"authentik_stage_identification\" \"recovery\" {\nname           = \"recovery-authentication-identification\"\nuser_fields    = [\"username\", \"email\"]\nsources = [data.authentik_source.built_in.uuid]\ncase_insensitive_matching = true\n}\n</code></pre> <ul> <li>An Email recovery stage: </li> </ul> <pre><code>resource \"authentik_stage_email\" \"recovery\" {\nname                     = \"recovery-email\"\nactivate_user_on_success = true\nsubject                  = \"Password Recovery\"\ntemplate                 = \"email/password_reset.html\"\ntimeout                  = 10\n}\n</code></pre> <ul> <li>We will reuse two existing stages too:</li> </ul> <pre><code>data \"authentik_stage\" \"default_password_change_prompt\" {\nname = \"default-password-change-prompt\"\n}\n\ndata \"authentik_stage\" \"default_password_change_write\" {\nname = \"default-password-change-write\"\n}\n</code></pre> <p>Then we need to create the recovery flow and bind all the stages:</p> <pre><code>resource \"authentik_flow\" \"password_recovery\" {\nname        = \"Password Recovery\"\ntitle       = \"Password Recovery\"\nslug        = \"password-recovery\"\ndesignation = \"recovery\"\n}\n\nresource \"authentik_flow_stage_binding\" \"recovery_identification\" {\ntarget = authentik_flow.password_recovery.uuid\nstage  = authentik_stage_identification.recovery.id\norder  = 0\n}\n\nresource \"authentik_flow_stage_binding\" \"recovery_email\" {\ntarget = authentik_flow.password_recovery.uuid\nstage  = authentik_stage_email.recovery.id\norder  = 10\n}\n\nresource \"authentik_flow_stage_binding\" \"recovery_password_change\" {\ntarget = authentik_flow.password_recovery.uuid\nstage  = data.authentik_stage.default_password_change_prompt.id\norder  = 20\n}\n\nresource \"authentik_flow_stage_binding\" \"recovery_password_write\" {\ntarget = authentik_flow.password_recovery.uuid\nstage  = data.authentik_stage.default_password_change_write.id\norder  = 30\n}\n</code></pre> <p>Finally we need to enable it in the site's authentication flow. To be able to do change the default flow we'd need to do two manual steps, so to have all the code in terraform we will create a new tenancy for our site and a new authentication flow.</p> <p>Starting with the authentication flow we need to create the Flow, stages and stage bindings.</p> <pre><code># -----------\n# -- Flows --\n# -----------\n\nresource \"authentik_flow\" \"authentication\" {\nname        = \"Welcome to Authentik!\"\ntitle        = \"Welcome to Authentik!\"\nslug        = \"custom-authentication-flow\"\ndesignation = \"authentication\"\nauthentication = \"require_unauthenticated\"\ncompatibility_mode = false\n}\n\n# ------------\n# -- Stages --\n# ------------\n\nresource \"authentik_stage_identification\" \"authentication\" {\nname           = \"custom-authentication-identification\"\nuser_fields    = [\"username\", \"email\"]\npassword_stage = data.authentik_stage.default_authentication_password.id\ncase_insensitive_matching = true\nrecovery_flow = authentik_flow.password_recovery.uuid\n}\n\ndata \"authentik_stage\" \"default_authentication_mfa_validation\" {\nname = \"default-authentication-mfa-validation\"\n}\n\ndata \"authentik_stage\" \"default_authentication_login\" {\nname = \"default-authentication-login\"\n}\n\ndata \"authentik_stage\" \"default_authentication_password\" {\nname = \"default-authentication-password\"\n}\n\n# -------------------\n# -- Stage binding --\n# -------------------\n\nresource \"authentik_flow_stage_binding\" \"login_identification\" {\ntarget = authentik_flow.authentication.uuid\nstage  = authentik_stage_identification.authentication.id\norder  = 10\n}\n\nresource \"authentik_flow_stage_binding\" \"login_mfa\" {\ntarget = authentik_flow.authentication.uuid\nstage  = data.authentik_stage.default_authentication_mfa_validation.id\norder  = 20\n}\n\nresource \"authentik_flow_stage_binding\" \"login_login\" {\ntarget = authentik_flow.authentication.uuid\nstage  = data.authentik_stage.default_authentication_login.id\norder  = 30\n}\n</code></pre> <p>Now we can bind it to the new tenant for our site:</p> <pre><code># ------------\n# -- Tenant --\n# ------------\n\nresource \"authentik_tenant\" \"default\" {\ndomain         = \"your-domain.org\"\ndefault        = false\nbranding_title = \"Authentik\"\nbranding_logo = \"/static/dist/assets/icons/icon_left_brand.svg\"\nbranding_favicon = \"/static/dist/assets/icons/icon.png\"\nflow_authentication = authentik_flow.authentication.uuid\n  # We need to define id instead of uuid until \n  # https://github.com/goauthentik/terraform-provider-authentik/issues/305\n  # is fixed.\nflow_invalidation = data.authentik_flow.default_invalidation_flow.id\nflow_user_settings = data.authentik_flow.default_user_settings_flow.id\nflow_recovery = authentik_flow.password_recovery.uuid\n}\n\n# -----------\n# -- Flows --\n# -----------\n\ndata \"authentik_flow\" \"default_invalidation_flow\" {\nslug = \"default-invalidation-flow\"\n}\n\ndata \"authentik_flow\" \"default_user_settings_flow\" {\nslug = \"default-user-settings-flow\"\n}\n</code></pre>"}, {"location": "authentik/#hide-and-application-from-a-user", "title": "Hide and application from a user", "text": "<p>Application access can be configured using (Policy) Bindings. Click on an application in the applications list, and select the Policy / Group / User Bindings tab. There you can bind users/groups/policies to grant them access. When nothing is bound, everyone has access. You can use this to grant access to one or multiple users/groups, or dynamically give access using policies.</p> <p>With terraform you can use <code>authentik_policy_binding</code>, for example:</p> <pre><code>resource \"authentik_policy_binding\" \"admin\" {\ntarget = authentik_application.gitea.uuid\ngroup  = authentik_group.admins.id\norder  = 0\n}\n</code></pre>"}, {"location": "authentik/#protect-applications-that-dont-have-authentication", "title": "Protect applications that don't have authentication", "text": "<p>Some applications don't have authentication, for example prometheus. You can use Authentik in front of such applications to add the authentication and authorization layer.</p> <p>Authentik can be used as a (very) simple reverse proxy by using its Provider feature with the regular \"Proxy\" setting. This let's you wrap authentication around a sub-domain / app where it normally wouldn't have authentication (or not the type of auth that you would specifically want) and then have Authentik handle the proxy forwarding and Auth.</p> <p>In this mode, there is no domain level nor 'integrated' authentication into your desired app; Authentik becomes both your reverse proxy and auth for this one particular app or (sub) domain. This mode does not forward authentication nor let you log in into any app. It's just acts like an authentication wrapper.</p> <p>It's best to use a normal reverse proxy out front of Authentik. This adds a second layer of routing to deal with but Authentik is not NGINX or a reverse proxy system, so it does not have that many configuration options. </p> <p>We'll use the following fake domains in this example:</p> <ul> <li>Authentik domain: auth.yourdomain.com</li> <li>App domain: app.yourdomain.com</li> <li>Nginx: nginx.yourdomain.com</li> <li>Authentik's docker conter name: auth_server</li> </ul> <p>The steps are:</p> <ul> <li>Configure the proxy provider:</li> </ul> <pre><code># ---------------\n# -- Variables --\n# ---------------\n\nvariable \"prometheus_url\" {\ntype        = string\ndescription = \"The url to access the service.\"\n}\n\n# ----------\n# -- Data --\n# ----------\n\ndata \"authentik_flow\" \"default-authorization-flow\" {\nslug = \"default-provider-authorization-implicit-consent\"\n}\n\n# --------------------\n# --    Provider    --\n# --------------------\n\nresource \"authentik_provider_proxy\" \"prometheus\" {\nname               = \"Prometheus\"\ninternal_host      = \"http://prometheus:9090\"\nexternal_host      = var.prometheus_url\nauthorization_flow = data.authentik_flow.default-authorization-flow.id\ninternal_host_ssl_validation = false\n}\n</code></pre> <ul> <li>Configure the application:</li> </ul> <pre><code>variable \"prometheus_icon\" {\ntype        = string\ndescription = \"The icon shown in the application\"\ndefault     = \"/application-icons/prometheus.svg\"\n}\n\n# -----------------------\n# --    Application    --\n# -----------------------\n\nresource \"authentik_application\" \"prometheus\" {\nname              = \"Prometheus\"\nslug              = \"prometheus\"\nmeta_icon         = var.prometheus_icon\nprotocol_provider = authentik_provider_proxy.prometheus.id\nlifecycle {\nignore_changes = [\n      # The terraform provider is continuously changing the attribute even though it's set\nmeta_icon,\n]\n}\n}\n</code></pre> <ul> <li>Edit the default outpost. So far there is no way to load the default outpost automatically in terraform, so you need to import it manually with <code>terraform import authentik_outpost.default default_outpost_id</code>. You can get the <code>default_outpost_id</code> by going to the Admin interface/Applications/Outposts, open the browser network inspector and click on edit the <code>authentik Embedded Outpost</code>, the first request you see will be to an uri similar to <code>/api/v3/outposts/instances/eabbbc70-e411-48f9-95b6-29bh23ldghwc/</code>, then your <code>default_outpost_id</code> is <code>eabbbc70-e411-48f9-95b6-29bh23ldghwc</code>. Then run the next terraform code</li> </ul> <pre><code># ----------------\n# --- Outposts ---\n# ----------------\n\nresource \"authentik_outpost\" \"default\" {\nname = \"authentik Embedded Outpost\"\nservice_connection = authentik_service_connection_docker.local.id protocol_providers = [\nauthentik_provider_proxy.prometheus.id\n]\n}\n\n# ----------------------------\n# --- Outpost integrations ---\n# ----------------------------\n\nresource \"authentik_service_connection_docker\" \"local\" {\nname  = \"Local Docker connection\"\nlocal = true\n}\n</code></pre>"}, {"location": "authentik/#use-blueprints", "title": "Use blueprints", "text": "<p>WARNING: Use the <code>terraform</code> provider instead!!!</p> <p>Blueprints offer a new way to template, automate and distribute authentik configuration. Blueprints can be used to automatically configure instances, manage config as code without any external tools, and to distribute application configs.</p> <p>Blueprints are yaml files, whose format is described further in File structure and uses YAML tags to configure the objects. It can be complicated when you first look at it, reading this example may help.</p> <p>Blueprints can be applied in one of two ways:</p> <ul> <li>As a Blueprint instance, which is a YAML file mounted into the authentik (worker) container. This file is read and applied every time it changes. Multiple instances can be created for a single blueprint file, and instances can be given context key:value attributes to configure the blueprint.</li> <li>As a Flow import, which is a YAML file uploaded via the Browser/API. This file is validated and applied directly after being uploaded, but is not further monitored/applied.</li> </ul> <p>The authentik container by default looks for blueprints in <code>/blueprints</code>. Underneath this directory, there are a couple default subdirectories:</p> <ul> <li><code>/blueprints/default</code>: Default blueprints for default flows, tenants, etc</li> <li><code>/blueprints/example</code>: Example blueprints for common configurations and flows</li> <li><code>/blueprints/system</code>: System blueprints for authentik managed Property mappings, etc</li> </ul> <p>Any additional <code>.yaml</code> file in /blueprints will be discovered and automatically instantiated, depending on their labels.</p> <p>To disable existing blueprints, an empty file can be mounted over the existing blueprint.</p> <p>File-based blueprints are automatically removed once they become unavailable, however none of the objects created by those blueprints are affected by this.</p>"}, {"location": "authentik/#export-blueprints", "title": "Export blueprints", "text": "<p>Exports from either method will contain a (potentially) long list of objects, all with hardcoded primary keys and no ability for templating/instantiation. This is because currently, authentik does not check which primary keys are used where. It is assumed that for most exports, there'll be some manual changes done regardless, to filter out unwanted objects, adjust properties, etc. That's why it may be better to use the flow export for the resources you've created rather than the global export.</p>"}, {"location": "authentik/#global-export", "title": "Global export", "text": "<p>To migrate existing configurations to blueprints, run <code>ak export_blueprint</code> within any authentik Worker container. This will output a blueprint for most currently created objects. Some objects will not be exported as they might have dependencies on other things.</p> <p>Exported blueprints don't use any of the YAML Tags, they just contain a list of entries as they are in the database.</p> <p>Note that fields which are write-only (for example, OAuth Provider's Secret Key) will not be added to the blueprint, as the serialisation logic from the API is used for blueprints.</p> <p>Additionally, default values will be skipped and not added to the blueprint.</p>"}, {"location": "authentik/#flow-export", "title": "Flow export", "text": "<p>Instead of exporting everything from a single instance, there's also the option to export a single flow with it's attached stages, policies and other objects.</p> <p>This export can be triggered via the API or the Web UI by clicking the download button in the flow list.</p>"}, {"location": "authentik/#monitorization", "title": "Monitorization", "text": "<p>I've skimmed through the prometheus metrics exposed at <code>:9300/metrics</code> in the core and they aren't that useful :(</p>"}, {"location": "authentik/#references", "title": "References", "text": "<ul> <li>Source</li> <li>Docs</li> <li> <p>Home</p> </li> <li> <p>Terraform provider docs</p> </li> <li>Terraform provider source code</li> </ul>"}, {"location": "aws_savings_plan/", "title": "AWS Savings plan", "text": "<p>Saving plans offer a flexible pricing model that provides savings on AWS usage. You can save up to 72 percent on your AWS compute workloads.</p> <p>!!! note \"Please don't make Jeff Bezos even richer, try to pay as less money to AWS as you can.\"</p> <p>Savings Plans provide savings beyond On-Demand rates in exchange for a commitment of using a specified amount of compute power (measured per hour) for a one or three year period.</p> <p>When you sign up for Savings Plans, the prices you'll pay for usage stays the same through the plan term. You can pay for your commitment using All Upfront, Partial upfront, or No upfront payment options.</p> <p>Plan types:</p> <ul> <li> <p>Compute Savings Plans provide the most flexibility and prices that are up     to 66 percent off of On-Demand rates. These plans automatically apply to     your EC2 instance usage, regardless of instance family (for example, m5, c5,     etc.), instance sizes (for example, c5.large, c5.xlarge, etc.), Region (for     example, us-east-1, us-east-2, etc.), operating system (for example,     Windows, Linux, etc.), or tenancy (for example, Dedicated, default,     Dedicated Host). With Compute Savings Plans, you can move a workload from C5     to M5, shift your usage from EU (Ireland) to EU (London). You can continue     to benefit from the low prices provided by Compute Savings Plans as you make     these changes.</p> </li> <li> <p>EC2 Instance Savings Plans provide savings up to 72 percent off On-Demand,     in exchange for a commitment to a specific instance family in a chosen AWS     Region (for example, M5 in Virginia). These plans automatically apply to     usage regardless of size (for example, m5.xlarge, m5.2xlarge, etc.), OS (for     example, Windows, Linux, etc.), and tenancy (Host, Dedicated, Default)     within the specified family in a Region.</p> <p>With an EC2 Instance Savings Plan, you can change your instance size within the instance family (for example, from c5.xlarge to c5.2xlarge) or the operating system (for example, from Windows to Linux), or move from Dedicated tenancy to Default and continue to receive the discounted rate provided by your EC2 Instance Savings Plan.</p> </li> <li> <p>Standard Reserved Instances: The old reservation system, you reserve an     instance type and you can get up to 72 percent of discount. The lack of     flexibility makes them inferior to the new EC2 instance plans.</p> </li> <li> <p>Convertible Reserved Instances: Same as the Standard Reserved Instances but     with more flexibility. Discounts range up to 66%, similar to the new Compute     Savings Plan, which again gives more less the same discounts with more     flexibility, so I wouldn't use this plan either.</p> </li> </ul>"}, {"location": "aws_savings_plan/#understanding-how-savings-plans-apply-to-your-aws-usage", "title": "Understanding how Savings Plans apply to your AWS usage", "text": "<p>If you have active Savings Plans, they apply automatically to your eligible AWS usage to reduce your bill.</p> <p>Savings Plans apply to your usage after the Amazon EC2 Reserved Instances (RI) are applied. Then EC2 Instance Savings Plans are applied before Compute Savings Plans because Compute Savings Plans have broader applicability.</p> <p>They calculate your potential savings percentages of each combination of eligible usage. This percentage compares the Savings Plans rates with your current On-Demand rates. Your Savings Plans are applied to your highest savings percentage first. If there are multiple usages with equal savings percentages, Savings Plans are applied to the first usage with the lowest Savings Plans rate. Savings Plans continue to apply until there are no more remaining usages, or your commitment is exhausted. Any remaining usage is charged at the On-Demand rates.</p>"}, {"location": "aws_savings_plan/#savings-plan-example", "title": "Savings plan example", "text": "<p>In this example, you have the following usage in a single hour:</p> <ul> <li>4x r5.4xlarge Linux, shared tenancy instances in us-east-1, running for the     duration of a full hour.</li> <li>1x m5.24xlarge Linux, dedicated tenancy instance in us-east-1, running for the     duration of a full hour.</li> </ul> <p>Pricing example:</p> Type On-Demand rate Compute Savings Plans rate CSP Savings percentage EC2 Instance Savings Plans rate EC2IS percentage r5.4xlarge $1.00 $0.70 30% $0.60 40% m5.24xlarge $10.00 $8.20 18% $7.80 22% <p>They've included other products in the example but I've removed them for the sake of simplicity</p>"}, {"location": "aws_savings_plan/#scenario-1-savings-plan-apply-to-all-usage", "title": "Scenario 1: Savings Plan apply to all usage", "text": "<p>You purchase a one-year, partial upfront Compute Savings Plan with a $50.00/hour commitment.</p> <p>Your Savings Plan covers all of your usage because multiplying each of your usages by the equivalent Compute Savings Plans is $47.13. This is still less than the $50.00/hour commitment.</p> <p>Without Savings Plans, you would be charged at On-Demand rates in the amount of $59.10.</p>"}, {"location": "aws_savings_plan/#scenario-2-savings-plans-apply-to-some-usage", "title": "Scenario 2: Savings Plans apply to some usage", "text": "<p>You purchase a one-year, partial upfront Compute Savings Plan with a $2.00/hour commitment.</p> <p>In any hour, your Savings Plans apply to your usage starting with the highest discount percentage (30 percent).</p> <p>Your $2.00/hour commitment is used to cover approximately 2.9 units of this usage. The remaining 1.1 units are charged at On-Demand rates, resulting in $1.14 of On-Demand charges for r5.</p> <p>The rest of your usage are also charged at On-Demand rates, resulting in $55.10 of On-Demand charges. The total On-Demand charges for this usage are $56.24.</p>"}, {"location": "aws_savings_plan/#scenario-3-savings-plans-and-ec2-reserved-instances-apply-to-the-usage", "title": "Scenario 3: Savings Plans and EC2 reserved instances apply to the usage", "text": "<p>You purchase a one-year, partial upfront Compute Savings Plan with an $18.20/hour commitment. You have two EC2 Reserved Instances (RI) for r5.4xlarge Linux shared tenancy in us-east-1.</p> <p>First, the Reserve Instances covers two of the r5.4xlarge instances. Then, the Savings Plans rate is applied to the remaining r5.4xlarge and the rest of the usage, which exhausts the hourly commitment of $18.20.</p>"}, {"location": "aws_savings_plan/#scenario-4-multiple-savings-plans-apply-to-the-usage", "title": "Scenario 4: Multiple Savings Plans apply to the usage", "text": "<p>You purchase a one-year, partial upfront EC2 Instance Family Savings Plan for the r5 family in us-east-1 with a $3.00/hour commitment. You also have a one-year, partial upfront Compute Savings Plan with a $16.80/hour commitment.</p> <p>Your EC2 Instance Family Savings Plan (r5, us-east-1) covers all of the r5.4xlarge usage because multiplying the usage by the EC2 Instance Family Savings Plan rate is $2.40. This is less than the $3.00/hour commitment.</p> <p>Next, the Compute Savings Plan is applied to rest of the resource usage, if it doesn't cover the whole expense, then On demand rates will apply.</p>"}, {"location": "aws_savings_plan/#monitoring-the-savings-plan", "title": "Monitoring the savings plan", "text": "<p>Monitoring is an important part of your Savings Plans usage. Understanding the Savings Plan that you own, how they are applying to your usage, and what usage is being covered are important parts of optimizing your costs with Savings Plans. You can monitor your usage in multiple forms.</p> <ul> <li> <p>Using the     inventory:     The Savings Plans Inventory page shows a detailed overview of the Savings     Plans that you own, or have queued for future purchase.</p> <p>To view your Inventory page:</p> <ul> <li>Open the AWS Cost Management     console.</li> <li>In the navigation pane, under Savings Plans, choose Inventory.</li> </ul> </li> <li> <p>Using the utilization     report:     Savings Plans utilization shows you the percentage of your Savings Plans     commitment that you're using across your On-Demand usage. You can use your     Savings Plans utilization report to visually understand how your Savings     Plans apply to your usage over the configured time period. Along with     a visualized graph, the report shows high-level metrics based on your     selected Savings Plan, filters, and lookback periods. Utilization is     calculated based on how your Savings Plans applied to your usage over the     lookback period.</p> <p>For example, if you have a 10 $/hour commitment, and your usage billed with Savings Plans rates totals to $9.80 for the hour, your utilization for that hour is 98 percent.</p> <p>You can find high-level metrics in the Utilization report section:</p> <ul> <li>On-Demand Spend Equivalent: The amount you would have spent on the same     usage if you didn\u2019t commit to Savings Plans. This amount is the     equivalent On-Demand cost based on current On-Demand rates.</li> <li>Savings Plans spend: Your Savings Plans commitment spend over the     lookback period.</li> <li>Total Net Savings: The amount you saved using Savings Plans commitments     over the selected time period, compared to the On-Demand cost estimate.</li> </ul> <p>To access your utilization report:</p> <ul> <li>Open the AWS Cost Management     console.</li> <li>On the navigation pane, choose Savings Plans.</li> <li>In the left pane, choose Utilization report.</li> </ul> </li> <li> <p>Using the coverage     report:     The Savings Plans coverage report shows how much of your eligible spend was     covered by your Savings Plans and how much is not covered by either Savings plan or Reserved instances based on the selected time period. </p> <p>You can find the following high-level metrics in the Coverage report section:</p> <ul> <li>Average Coverage: The aggregated Savings Plans coverage percentage based     on the selected filters and look-back period.</li> <li>Additional potential savings: Your potential savings amount based on     your Savings Plans recommendations. This is shown as a monthly amount.</li> <li>On-Demand spend not covered: The amount of eligible savings spend that     was not covered by Savings Plans or Reserved Instances over the lookback     period.</li> </ul> <p>To access your utilization report:</p> <ul> <li>Open the AWS Cost Management     console.</li> <li>On the navigation pane, choose Savings Plans.</li> <li>In the left pane, choose Coverage report.</li> </ul> <p>The columns are a bit tricky:</p> <ul> <li>\"Spend covered by Savings Plan\": Refers to the on demand usage amount that you would have paid on demand that is being covered by the Savings Plans. Not the Savings Plan amount that is applied to on demand usage.</li> </ul> <p>The coverage report of the reserved instances has the same trick on the columns:</p> <ul> <li>\"Reservation covered hours\": the column does not refer to your RI hours. This column refers to your on demand hours that was covered by Reserved Instances.</li> </ul> </li> </ul>"}, {"location": "aws_savings_plan/#doing-your-savings-plan", "title": "Doing your savings plan", "text": "<p>Go to the AWS savings plan simulator and check the different instances you were evaluating.</p>"}, {"location": "aws_snippets/", "title": "AWS Snippets", "text": ""}, {"location": "aws_snippets/#invalidate-a-cloudfront-distribution", "title": "Invalidate a cloudfront distribution", "text": "<pre><code>aws cloudfront create-invalidation --paths \"/pages/about\" --distribution-id my-distribution-id\n</code></pre>"}, {"location": "aws_snippets/#get-ec2-metadata-from-within-the-instance", "title": "Get EC2 metadata from within the instance", "text": "<p>The quickest way to fetch or retrieve EC2 instance metadata from within a running EC2 instance is to log in and run the command:</p> <p>Fetch metadata from IPv4:</p> <pre><code>curl -s http://169.254.169.254/latest/dynamic/instance-identity/document\n</code></pre> <p>You can also download the <code>ec2-metadata</code> tool to get the info:</p> <pre><code># Download the ec2-metadata script\nwget http://s3.amazonaws.com/ec2metadata/ec2-metadata\n\n# Modify the permission to execute the bash script\nchmod +x ec2-metadata\n\n./ec2-metadata --all\n</code></pre>"}, {"location": "aws_snippets/#find-if-external-ip-belongs-to-you", "title": "Find if external IP belongs to you", "text": "<p>You can list the network interfaces that match the IP you're searching for</p> <pre><code>aws ec2 describe-network-interfaces --filters Name=association.public-ip,Values=\"{{ your_ip_address}}\"\n</code></pre>"}, {"location": "aws_waf/", "title": "AWS WAF", "text": "<p>AWS WAF is a web application firewall that helps protect your web applications or APIs against common web exploits and bots that may affect availability, compromise security, or consume excessive resources. AWS WAF gives you control over how traffic reaches your applications by enabling you to create security rules that control bot traffic and block common attack patterns, such as SQL injection or cross-site scripting. You can also customize rules that filter out specific traffic patterns.</p>"}, {"location": "aws_waf/#extracting-information", "title": "Extracting information", "text": "<p>You can configure the WAF to write it's logs into S3, Kinesis or a Cloudwatch log group. S3 saves the data in small compressed files which are difficult to analyze, Kinesis makes sense if you post-process the data on a log system such as graylog, the last one allows you to use the WAF's builtin cloudwatch log insights which has the next interesting reports:             . * Top 100 Ip addresses * Top 100 countries * Top 100 hosts * Top 100 terminating rules             . Nevertheless, it still lacks some needed reports to analyze the traffic. But it's quite easy to build them yourself in Cloudwatch Log Insights. If you have time I'd always suggest to avoid using proprietary AWS tools, but sadly it's the quickest way to get results.</p>"}, {"location": "aws_waf/#creating-log-insights-queries", "title": "Creating Log Insights queries", "text": "<p>Inside the Cloudwatch site, on the left menu you'll see the <code>Logs</code> tab, and under it <code>Log Insights</code>. There you can write the query you want to run. Once it returns the expected result, you can save it. Saved queries can be seen on the right menu, under <code>Queries</code>.</p> <p>If you later change the query, you'll see a blue dot beside the query you last run. The query will remain changed until you click on <code>Actions</code> and then <code>Reset</code>.</p>"}, {"location": "aws_waf/#useful-queries", "title": "Useful Queries", "text": ""}, {"location": "aws_waf/#top-ips", "title": "Top IPs", "text": "<p>Is a directory to save the queries to analyze a count of requests aggregated by ips.</p>"}, {"location": "aws_waf/#top-ips-query", "title": "Top IPs query", "text": "<pre><code>fields httpRequest.clientIp\n| stats count(*) as requestCount by httpRequest.clientIp\n| sort requestCount desc\n| limit 100\n</code></pre>"}, {"location": "aws_waf/#top-ips-by-uri", "title": "Top IPs by uri", "text": "<pre><code>fields httpRequest.clientIp\n| filter httpRequest.uri like \"/\"\n| stats count(*) as requestCount by httpRequest.clientIp\n| sort requestCount desc\n| limit 100\n</code></pre>"}, {"location": "aws_waf/#top-uris", "title": "Top URIs", "text": "<p>Is a directory to save the queries to analyze a count of requests aggregated by uris.</p>"}, {"location": "aws_waf/#top-uris-query", "title": "Top URIs query", "text": "<p>This report shows all the uris that are allowed to pass the WAF.</p> <pre><code>fields httpRequest.uri\n| filter action like \"ALLOW\"\n| stats count(*) as requestCount by httpRequest.uri\n| sort requestCount desc\n| limit 100\n</code></pre>"}, {"location": "aws_waf/#top-uris-of-a-termination-rule", "title": "Top URIs of a termination rule", "text": "<pre><code>fields httpRequest.uri\n| filter terminatingRuleId like \"AWS-AWSManagedRulesUnixRuleSet\"\n| stats count(*) as requestCount by httpRequest.uri\n| sort requestCount desc\n| limit 100\n</code></pre>"}, {"location": "aws_waf/#top-uris-of-an-ip", "title": "Top URIs of an IP", "text": "<pre><code>fields httpRequest.uri\n| filter @message like \"6.132.241.132\"\n| stats count(*) as requestCount by httpRequest.uri\n| sort requestCount desc\n| limit 100\n</code></pre>"}, {"location": "aws_waf/#top-uris-of-a-cloudfront-id", "title": "Top URIs of a Cloudfront ID", "text": "<pre><code>fields httpRequest.uri\n| filter httpSourceId like \"CLOUDFRONT_ID\"\n| stats count(*) as requestCount by httpRequest.uri\n| sort requestCount desc\n| limit 100\n</code></pre>"}, {"location": "aws_waf/#waf-top-terminating-rules", "title": "WAF Top terminating rules", "text": "<p>Report that shows the top rules that are blocking the content.</p> <pre><code>fields terminatingRuleId\n| filter terminatingRuleId not like \"Default_Action\"\n| stats count(*) as requestCount by terminatingRuleId\n| sort requestCount desc\n| limit 100\n</code></pre>"}, {"location": "aws_waf/#top-blocks-by-cloudfront-id", "title": "Top blocks by Cloudfront ID", "text": "<pre><code>fields httpSourceId\n| filter terminatingRuleId not like \"Default_Action\"\n| stats count(*) as requestCount by httpSourceId\n| sort requestCount desc\n| limit 100\n</code></pre>"}, {"location": "aws_waf/#top-allows-by-cloudfront-id", "title": "Top allows by Cloudfront ID", "text": "<pre><code>fields httpSourceId\n| filter terminatingRuleId like \"Default_Action\"\n| stats count(*) as requestCount by httpSourceId\n| sort requestCount desc\n| limit 100\n</code></pre>"}, {"location": "aws_waf/#waf-top-countries", "title": "WAF Top countries", "text": "<pre><code>fields httpRequest.country\n| stats count(*) as requestCount by httpRequest.country\n| sort requestCount desc\n| limit 100\n</code></pre>"}, {"location": "aws_waf/#requests-by", "title": "Requests by", "text": "<p>Is a directory to save the queries to show the requests filtered by a criteria.</p>"}, {"location": "aws_waf/#requests-by-ip", "title": "Requests by IP", "text": "<pre><code>fields @timestamp, httpRequest.uri, httpRequest.args, httpSourceId\n| sort @timestamp desc\n| filter @message like \"6.132.241.132\"\n| limit 100\n</code></pre>"}, {"location": "aws_waf/#requests-by-termination-rule", "title": "Requests by termination rule", "text": "<pre><code>fields @timestamp, httpRequest.uri, httpRequest.args, httpSourceId\n| sort @timestamp desc\n| filter terminatingRuleId like \"AWS-AWSManagedRulesUnixRuleSet\"\n| limit 100\n</code></pre>"}, {"location": "aws_waf/#requests-by-uri", "title": "Requests by URI", "text": "<pre><code>fields @timestamp, httpRequest.uri, httpRequest.args, httpSourceId\n| sort @timestamp desc\n| filter httpRequest.uri like \"wp-json\"\n| limit 100\n</code></pre>"}, {"location": "aws_waf/#waf-top-args-of-an-uri", "title": "WAF Top Args of an URI", "text": "<pre><code>fields httpRequest.args\n| filter httpRequest.uri like \"/\"\n| stats count(*) as requestCount by httpRequest.args\n| sort requestCount desc\n| limit 100\n</code></pre>"}, {"location": "aws_waf/#analysis-workflow", "title": "Analysis workflow", "text": "<p>To analyze the WAF insights you can:</p> <ul> <li>Analyze the traffic of the top IPs</li> <li>Analyze the top URIs</li> <li>Analyze the terminating rules</li> </ul>"}, {"location": "aws_waf/#analyze-the-traffic-of-the-top-ips", "title": "Analyze the traffic of the top IPS", "text": "<p>For IP in the WAF Top IPs report, do:</p> <ul> <li> <p>Analyze the top uris of that IP to see if they are     legit requests or if it contains malicious requests. If you want to get the     details of a particular request, you can use the requests by     uri report.</p> <p>For request in malicious requests:</p> <ul> <li>If it's not being filtered by the WAF update your WAF rules.</li> </ul> </li> <li> <p>If the IP is malicious mark it as problematic.</p> </li> </ul>"}, {"location": "aws_waf/#analyze-the-top-uris", "title": "Analyze the top uris", "text": "<p>For uri in the WAF Top URIs report, do:</p> <ul> <li> <p>For argument in the top arguments of that uri     report, see if they are legit requests or if it's malicious. If you want to     get the details of a particular request, you can use the requests by     uri report.</p> <p>For request in malicious requests:</p> <ul> <li>If it's not being filtered by the WAF update your WAF rules.</li> <li>For IP in top uris by IP report:<ul> <li>Mark IP as problematic.</li> </ul> </li> </ul> </li> </ul>"}, {"location": "aws_waf/#analyze-the-terminating-rules", "title": "Analyze the terminating rules", "text": "<p>For terminating rule in the WAF Top terminating rules report, do:</p> <ul> <li>For IP in the top ips by termination rule mark it as problematic.</li> </ul> <p>After some time you can see which rules are not being triggered and remove them. With the requests by termination rule you can see which requests are being blocked and try to block it in another rule set and merge both.</p>"}, {"location": "aws_waf/#mark-ip-as-problematic", "title": "Mark IP as problematic", "text": "<p>To process an problematic IP:</p> <ul> <li>Add it to the captcha list.</li> <li>If it is already in the captcha list and is still triggering problematic     requests, add it to the block list.</li> <li>Add a task to remove the IP from the block or captcha list X minutes or     hours in the future.</li> </ul>"}, {"location": "aws_waf/#references", "title": "References", "text": "<ul> <li>Homepage</li> </ul>"}, {"location": "bash_snippets/", "title": "Bash Snippets", "text": ""}, {"location": "bash_snippets/#remove-the-lock-screen-in-ubuntu", "title": "Remove the lock screen in ubuntu", "text": "<p>Create the <code>/usr/share/glib-2.0/schemas/90_ubuntu-settings.gschema.override</code> file with the next content:</p> <pre><code>[org.gnome.desktop.screensaver]\nlock-enabled = false\n[org.gnome.settings-daemon.plugins.power]\nidle-dim = false\n</code></pre> <p>Then reload the schemas with:</p> <pre><code>sudo glib-compile-schemas /usr/share/glib-2.0/schemas/\n</code></pre>"}, {"location": "bash_snippets/#how-to-deal-with-hostcontextswitching-alertmanager-alert", "title": "How to deal with HostContextSwitching alertmanager alert", "text": "<p>A context switch is described as the kernel suspending execution of one process on the CPU and resuming execution of some other process that had previously been suspended. A context switch is required for every interrupt and every task that the scheduler picks.</p> <p>Context switching can be due to multitasking, Interrupt handling , user &amp; kernel mode switching. The interrupt rate will naturally go high, if there is higher network traffic, or higher disk traffic. Also it is dependent on the application which every now and then invoking system calls.</p> <p>If the cores/CPU's are not sufficient to handle load of threads created by application will also result in context switching.</p> <p>It is not a cause of concern until performance breaks down. This is expected that CPU will do context switching. One shouldn't verify these data at first place since there are many statistical data which should be analyzed prior to looking into kernel activities. Verify the CPU, memory and network usage during this time. </p> <p>You can see which process is causing issue with the next command:</p> <pre><code># pidstat -w 3 10   &gt; /tmp/pidstat.out\n\n10:15:24 AM     UID     PID     cswch/s         nvcswch/s       Command 10:15:27 AM     0       1       162656.7        16656.7         systemd\n10:15:27 AM     0       9       165451.04       15451.04        ksoftirqd/0\n10:15:27 AM     0       10      158628.87       15828.87        rcu_sched\n10:15:27 AM     0       11      156147.47       15647.47        migration/0\n10:15:27 AM     0       17      150135.71       15035.71        ksoftirqd/1\n10:15:27 AM     0       23      129769.61       12979.61        ksoftirqd/2\n10:15:27 AM     0       29      2238.38         238.38          ksoftirqd/3\n10:15:27 AM     0       43      1753            753             khugepaged\n10:15:27 AM     0       443     1659            165             usb-storage\n10:15:27 AM     0       456     1956.12         156.12          i915/signal:0\n10:15:27 AM     0       465     29550           29550           kworker/3:1H-xfs-log/dm-3\n10:15:27 AM     0       490     164700          14700           kworker/0:1H-kblockd\n10:15:27 AM     0       506     163741.24       16741.24        kworker/1:1H-xfs-log/dm-3\n10:15:27 AM     0       594     154742          154742          dmcrypt_write/2\n10:15:27 AM     0       629     162021.65       16021.65        kworker/2:1H-kblockd\n10:15:27 AM     0       715     147852.48       14852.48        xfsaild/dm-1\n10:15:27 AM     0       886     150706.86       15706.86        irq/131-iwlwifi\n10:15:27 AM     0       966     135597.92       13597.92        xfsaild/dm-3\n10:15:27 AM     81      1037    2325.25         225.25          dbus-daemon\n10:15:27 AM     998     1052    118755.1        11755.1         polkitd\n10:15:27 AM     70      1056    158248.51       15848.51        avahi-daemon\n10:15:27 AM     0       1061    133512.12       455.12          rngd\n10:15:27 AM     0       1110    156230          16230           cupsd\n10:15:27 AM     0       1192    152298.02       1598.02         sssd_nss\n10:15:27 AM     0       1247    166132.99       16632.99        systemd-logind\n10:15:27 AM     0       1265    165311.34       16511.34        cups-browsed\n10:15:27 AM     0       1408    10556.57        1556.57         wpa_supplicant\n10:15:27 AM     0       1687    3835            3835            splunkd\n10:15:27 AM     42      1773    3728            3728            Xorg\n10:15:27 AM     42      1996    3266.67         266.67          gsd-color\n10:15:27 AM     0       3166    32036.36        3036.36         sssd_kcm\n10:15:27 AM     119349  3194    151763.64       11763.64        dbus-daemon\n10:15:27 AM     119349  3199    158306          18306           Xorg\n10:15:27 AM     119349  3242    15.28           5.8             gnome-shell\n\n# pidstat -wt 3 10  &gt; /tmp/pidstat-t.out\n\nLinux 4.18.0-80.11.2.el8_0.x86_64 (hostname)    09/08/2020  _x86_64_    (4 CPU)\n\n10:15:15 AM   UID      TGID       TID   cswch/s   nvcswch/s  Command\n10:15:19 AM     0         1         -   152656.7   16656.7   systemd\n10:15:19 AM     0         -         1   152656.7   16656.7   |__systemd\n10:15:19 AM     0         9         -   165451.04  15451.04  ksoftirqd/0\n10:15:19 AM     0         -         9   165451.04  15451.04  |__ksoftirqd/0\n10:15:19 AM     0        10         -   158628.87  15828.87  rcu_sched\n10:15:19 AM     0         -        10   158628.87  15828.87  |__rcu_sched\n10:15:19 AM     0        23         -   129769.61  12979.61  ksoftirqd/2\n10:15:19 AM     0         -        23   129769.61  12979.33  |__ksoftirqd/2\n10:15:19 AM     0        29         -   32424.5    2445      ksoftirqd/3\n10:15:19 AM     0         -        29   32424.5    2445      |__ksoftirqd/3\n10:15:19 AM     0        43         -   334        34        khugepaged\n10:15:19 AM     0         -        43   334        34        |__khugepaged\n10:15:19 AM     0       443         -   11465      566       usb-storage\n10:15:19 AM     0         -       443   6433       93        |__usb-storage\n10:15:19 AM     0       456         -   15.41      0.00      i915/signal:0\n10:15:19 AM     0         -       456   15.41      0.00      |__i915/signal:0\n10:15:19 AM     0       715         -   19.34      0.00      xfsaild/dm-1\n10:15:19 AM     0         -       715   19.34      0.00      |__xfsaild/dm-1\n10:15:19 AM     0       886         -   23.28      0.00      irq/131-iwlwifi\n10:15:19 AM     0         -       886   23.28      0.00      |__irq/131-iwlwifi\n10:15:19 AM     0       966         -   19.67      0.00      xfsaild/dm-3\n10:15:19 AM     0         -       966   19.67      0.00      |__xfsaild/dm-3\n10:15:19 AM    81      1037         -   6.89       0.33      dbus-daemon\n10:15:19 AM    81         -      1037   6.89       0.33      |__dbus-daemon\n10:15:19 AM     0      1038         -   11567.31   4436      NetworkManager\n10:15:19 AM     0         -      1038   1.31       0.00      |__NetworkManager\n10:15:19 AM     0         -      1088   0.33       0.00      |__gmain\n10:15:19 AM     0         -      1094   1340.66    0.00      |__gdbus\n10:15:19 AM   998      1052         -   118755.1   11755.1   polkitd\n10:15:19 AM   998         -      1052   32420.66   25545     |__polkitd\n10:15:19 AM   998         -      1132   0.66       0.00      |__gdbus\n</code></pre> <p>Then with help of PID which is causing issue, one can get all system calls details: Raw</p> <pre><code># strace -c -f -p &lt;pid of process/thread&gt;\n</code></pre> <p>Let this command run for a few minutes while the load/context switch rates are high. It is safe to run this on a production system so you could run it on a good system as well to provide a comparative baseline. Through strace, one can debug &amp; troubleshoot the issue, by looking at system calls the process has made.</p>"}, {"location": "bash_snippets/#redirect-stderr-of-all-subsequent-commands-of-a-script-to-a-file", "title": "Redirect stderr of all subsequent commands of a script to a file", "text": "<pre><code>{\nsomecommand somecommand2\n    somecommand3\n} 2&gt;&amp;1 | tee -a $DEBUGLOG\n</code></pre>"}, {"location": "bash_snippets/#get-the-root-path-of-a-git-repository", "title": "Get the root path of a git repository", "text": "<pre><code>git rev-parse --show-toplevel\n</code></pre>"}, {"location": "bash_snippets/#get-epoch-gmt-time", "title": "Get epoch gmt time", "text": "<pre><code>date -u '+%s'\n</code></pre>"}, {"location": "bash_snippets/#check-the-length-of-an-array-with-jq", "title": "Check the length of an array with jq", "text": "<pre><code>echo '[{\"username\":\"user1\"},{\"username\":\"user2\"}]' | jq '. | length'\n</code></pre>"}, {"location": "bash_snippets/#exit-the-script-if-there-is-an-error", "title": "Exit the script if there is an error", "text": "<pre><code>set -eu\n</code></pre>"}, {"location": "bash_snippets/#prompt-the-user-for-data", "title": "Prompt the user for data", "text": "<pre><code>read -p \"Ask whatever\" choice\n</code></pre>"}, {"location": "bash_snippets/#parse-csv-with-bash", "title": "Parse csv with bash", "text": ""}, {"location": "bash_snippets/#do-the-remainder-or-modulus-of-a-number", "title": "Do the remainder or modulus of a number", "text": "<pre><code>expr 5 % 3\n</code></pre>"}, {"location": "bash_snippets/#update-a-json-file-with-jq", "title": "Update a json file with jq", "text": "<p>Save the next snippet to a file, for example <code>jqr</code> and add it to your <code>$PATH</code>.</p> <pre><code>#!/bin/zsh\n\nquery=\"$1\"\nfile=$2\n\ntemp_file=\"$(mktemp)\"\n\n# Update the content\njq \"$query\" $file &gt; \"$temp_file\"\n\n# Check if the file has changed\ncmp -s \"$file\" \"$temp_file\"\nif [[ $? -eq 0 ]] ; then\n/bin/rm \"$temp_file\"\nelse\n/bin/mv \"$temp_file\" \"$file\"\nfi\n</code></pre> <p>Imagine you have the next json file:</p> <pre><code>{\n\"property\": true,\n\"other_property\": \"value\"\n}\n</code></pre> <p>Then you can run:</p> <pre><code>jqr '.property = false' status.json\n</code></pre> <p>And then you'll have:</p> <pre><code>{\n\"property\": false,\n\"other_property\": \"value\"\n}\n</code></pre>"}, {"location": "beancount/", "title": "Beancount", "text": "<p>Beancount is a Python double entry accounting command line tool similar to <code>ledger</code>.</p>"}, {"location": "beancount/#installation", "title": "Installation", "text": "<pre><code>pip3 install beancount\n</code></pre>"}, {"location": "beancount/#tools", "title": "Tools", "text": "<p><code>beancount</code> is the core component, it's a declarative language. It parses a text file, and produces reports from the resulting data structures.</p>"}, {"location": "beancount/#bean-check", "title": "bean-check", "text": "<p><code>bean-check</code> is the program you use to verify that your input syntax and transactions work correctly. All it does is load your input file and run the various plugins you configured in it, plus some extra validation checks.</p> <pre><code>bean-check /path/to/file.beancount\n</code></pre> <p>If there are no errors, there should be no output, it should exit quietly.</p>"}, {"location": "beancount/#bean-report", "title": "bean-report", "text": "<p>This is the main tool used to extract specialized reports to the console in text or one of the various other formats.</p> <p>For a graphic exploration of your data, use the fava web application instead.</p> <pre><code>bean-report /path/to/file.beancount {{ report_name }}\n</code></pre> <p>There are many reports available, to get a full list run <code>bean-report --help-reports</code></p> <p>Report names sometimes may accept arguments, if they do  so use <code>:</code></p> <pre><code>bean-report /path/to/file.beancount balances:Vanguard\n</code></pre>"}, {"location": "beancount/#to-get-the-balances", "title": "To get the balances", "text": "<pre><code>bean-report {{ path/to/file.beancount }} balances | treeify\n</code></pre>"}, {"location": "beancount/#to-get-the-journal", "title": "To get the journal", "text": "<pre><code>bean-report {{ path/to/file.beancount }} journal\n</code></pre>"}, {"location": "beancount/#to-get-the-holdings", "title": "To get the holdings", "text": "<p>To get the aggregations for the total list of holdings</p> <pre><code>bean-report {{ path/to/file.beancount }} holdings\n</code></pre>"}, {"location": "beancount/#to-get-the-accounts", "title": "To get the accounts", "text": "<pre><code>bean-report {{ path/to/file.beancount }} accounts\n</code></pre>"}, {"location": "beancount/#bean-query", "title": "bean-query", "text": "<p><code>bean-query</code> is a command-line tool that acts like a client to that in-memory database in which you can type queries in a variant of SQL. It has it's own document</p> <pre><code>bean-query /path/to/file.beancount\n</code></pre>"}, {"location": "beancount/#bean-web", "title": "bean-web", "text": "<p>Deprecated use fava instead</p> <p><code>bean-web</code> serves all the reports on a web server that runs on your computer</p> <pre><code>bean-web /path/to/file.beancount\n</code></pre> <p>It will serve on <code>localhost:8080</code></p>"}, {"location": "beancount/#bean-doctor", "title": "bean-doctor", "text": "<p>This is a debugging tool used to perform various diagnostics and run debugging commands, and to help provide information for reporting bugs.</p>"}, {"location": "beancount/#bean-format", "title": "bean-format", "text": "<p>Pure text processing tool will reformat Beancount input to right-align all the numbers at the same, minimal column.</p>"}, {"location": "beancount/#bean-example", "title": "bean-example", "text": "<p>Generates an example Beancount input file.</p>"}, {"location": "beancount/#bean-identify", "title": "bean-identify", "text": "<p>Given a messy list of downloaded files automatically identify which of your configured importers is able to handle them and print them out. This is to be used for debugging and figuring out if your configuration is properly associating a suitable importer for each of the files you downloaded.</p>"}, {"location": "beancount/#bean-extract", "title": "bean-extract", "text": "<p>Extracts transactions and statement date from each file, if at all possible. This produces some Beancount input text to be moved to your input file.</p> <pre><code>bean-extract {{ path/to/config.config }} {{ path/to/source/files }}\n</code></pre> <p>The tool calls methods on importer objects. You must provide a list of such importers; this list is the configuration for the importing process.</p> <p>For each file found, each of the importers is called to assert whether it can or cannot handle that file. If it deems that it can, methods can be called to produce a list of transactions extract a date, or produce a cleaned up filename for the downloaded file.</p> <p>The configuration should be a python3 module in which you instantiate the importers and assign the list to the module-level \"CONFIG\" variable</p> <pre><code>#!/usr/bin/env python3\nfrom myimporters.bank import acmebank\nfrom myimporters.bank import chase\n\u2026\nCONFIG = [\nacmebank.Importer(),\nchase.Importer(),\n\u2026\n]\n</code></pre>"}, {"location": "beancount/#writing-an-importer", "title": "Writing an importer", "text": "<p>Each of the importers must comply with a particular protocol and implement at least some of its methods. The full detail of the protocol is in the source of importer.py</p> <pre><code>\"\"\"Importer protocol.\n\nAll importers must comply with this interface and implement at least some of its\nmethods. A configuration consists in a simple list of such importer instances.\nThe importer processes run through the importers, calling some of its methods in\norder to identify, extract and file the downloaded files.\n\nEach of the methods accept a cache.FileMemo object which has a 'name' attribute\nwith the filename to process, but which also provides a place to cache\nconversions. Use its convert() method whenever possible to avoid carrying out\nthe same conversion multiple times. See beancount.ingest.cache for more details.\n\nSynopsis:\n\n name(): Return a unique identifier for the importer instance.\n identify(): Return true if the identifier is able to process the file.\n extract(): Extract directives from a file's contents and return of list of entries.\n file_account(): Return an account name associated with the given file for this importer.\n file_date(): Return a date associated with the downloaded file (e.g., the statement date).\n file_name(): Return a cleaned up filename for storage (optional).\n\nJust to be clear: Although this importer will not raise NotImplementedError\nexceptions (it returns default values for each method), you NEED to derive from\nit in order to do anything meaningful. Simply instantiating this importer will\nnot match not provide any useful information. It just defines the protocol for\nall importers.\n\"\"\"\n__copyright__ = \"Copyright (C) 2016  Martin Blais\"\n__license__ = \"GNU GPLv2\"\n\nfrom beancount.core import flags\n\n\nclass ImporterProtocol:\n    \"Interface that all source importers need to comply with.\"\n\n    # A flag to use on new transaction. Override this flag in derived classes if\n    # you prefer to create your imported transactions with a different flag.\n    FLAG = flags.FLAG_OKAY\n\n    def name(self):\n\"\"\"Return a unique id/name for this importer.\n\n        Returns:\n          A string which uniquely identifies this importer.\n        \"\"\"\n        cls = self.__class__\n        return '{}.{}'.format(cls.__module__, cls.__name__)\n\n    __str__ = name\n\n    def identify(self, file):\n\"\"\"Return true if this importer matches the given file.\n\n        Args:\n          file: A cache.FileMemo instance.\n        Returns:\n          A boolean, true if this importer can handle this file.\n        \"\"\"\n\n    def extract(self, file):\n\"\"\"Extract transactions from a file.\n\n        Args:\n          file: A cache.FileMemo instance.\n        Returns:\n          A list of new, imported directives (usually mostly Transactions)\n          extracted from the file.\n        \"\"\"\n\n    def file_account(self, file):\n\"\"\"Return an account associated with the given file.\n\n        Note: If you don't implement this method you won't be able to move the\n        files into its preservation hierarchy; the bean-file command won't work.\n\n        Also, normally the returned account is not a function of the input\n        file--just of the importer--but it is provided anyhow.\n\n        Args:\n          file: A cache.FileMemo instance.\n        Returns:\n          The name of the account that corresponds to this importer.\n        \"\"\"\n\n    def file_name(self, file):\n\"\"\"A filter that optionally renames a file before filing.\n\n        This is used to make tidy filenames for filed/stored document files. The\n        default implementation just returns the same filename. Note that a\n        simple RELATIVE filename must be returned, not an absolute filename.\n\n        Args:\n          file: A cache.FileMemo instance.\n        Returns:\n          The tidied up, new filename to store it as.\n        \"\"\"\n\n    def file_date(self, file):\n        \"\"\"Attempt to obtain a date that corresponds to the given file.\n\n        Args:\n          file: A cache.FileMemo instance.\n        Returns:\n          A date object, if successful, or None if a date could not be extracted.\n          (If no date is returned, the file creation time is used. This is the\n          default.)\n</code></pre> <p>A summary of the methods you need to, or may want to implement:</p> <ul> <li> <p>name(): Provides a unique id for each importer instance. It's convenient to   be able to refer to your importers with a unique name; it gets printed out by   the identification process.</p> </li> <li> <p>identify(): This method just returns true if this importer can handle the   given file. You must implement this method, and all the tools invoke it ot   figure out the list of (file, importer) pairs.</p> </li> <li> <p>extract(): This is called to attempt to extract some Beancount directives   from the file contents. It must create the directives by instatiating the   objects define in beancout.core.data and return them.</p> </li> </ul> <pre><code>from beancount.ingest import importer\n\nclass Importer(importer.ImporterProtocol):\n\n  def identify(self, file):\n  \u2026\n  # Override other methods\u2026\n</code></pre> <p>Some importer examples:</p> <ul> <li>mterwill gist</li> <li>wzyboy importers</li> </ul>"}, {"location": "beancount/#bean-file", "title": "bean-file", "text": "<ul> <li><code>bean-file</code> filing documents. It si able to identify which document belongs to   which account, it can move the downloaded file to the documents archive   automatically.</li> </ul>"}, {"location": "beancount/#basic-concepts", "title": "Basic concepts", "text": ""}, {"location": "beancount/#beancount-transaction", "title": "Beancount transaction", "text": "<pre><code>2014-05-23 * \"CAFE MOGADOR NEW YO\" \"Dinner with Caroline\"\n  Liabilities:US:BofA:CreditCard -98.32 USD\n  Expenses:Restaurant\n</code></pre> <ul> <li>Currencies must be entirely in capital letters.</li> <li>Account names do not admit spaces.</li> <li>Description strings must be quoted.</li> <li>Dates are only parsed in YYYY-MM-DD format.</li> <li>Tags must begin with <code>#</code> and links with <code>^</code>.</li> </ul>"}, {"location": "beancount/#beancount-operators", "title": "Beancount Operators", "text": ""}, {"location": "beancount/#open", "title": "Open", "text": "<p>All accounts need to be declared open in order to accept amounts posted to them.</p> <pre><code>YYYY-MM-DD open {{ account_name }} [{{ ConstrainCurrency }}]\n</code></pre>"}, {"location": "beancount/#close", "title": "Close", "text": "<pre><code>YYYY-MM-DD close {{ account_name }}\n</code></pre> <p>It's useful to insert a balance of 0 units just before closing an account, just to make sure its contents are empty as you close it.</p>"}, {"location": "beancount/#commodity", "title": "Commodity", "text": "<p>It can be used to declare currencies, financial instruments, commodities... It's optional</p> <pre><code>YYYY-MM-DD commodity {{ currency_name }}\n</code></pre>"}, {"location": "beancount/#transactions", "title": "Transactions", "text": "<pre><code>YYYY-MM-DD txn \"[{{ payee }}]\"  \"{{ Comment }}\"\n  {{ Account1 }} {{ value}}\n  [{{ Accountn-1 }} {{ value }}]\n  {{ Accountn }}\n</code></pre> <p>Payee is a string that represents an external entity that is involved in the transaction. Payees are sometimes useful on transactions that post amounts to Expense accounts, whereby the account accumulates a category of expenses from multiple business</p> <p>As transactions is the most common, you can substitute <code>txn</code> for a flag, by default : * <code>*</code>: Completed transaction, known amounts, \"this looks correct\" * <code>!</code>: Incomplete transaction, needs confirmation or revision, \"this looks   incorrect\"</p> <p>You can also attach flags to the postings themselves, if you want to flag one of the transaction's legs in particular:</p> <pre><code>2014-05-05 * \"Transfer from Savings account\"\n  Assets:MyBank:Checking     -400.00 USD\n  ! Assets:MyBank:Savings\n</code></pre> <p>This is useful in the intermediate stage of de-duping transactions</p>"}, {"location": "beancount/#tags-vs-payee", "title": "Tags vs Payee", "text": "<p>You can tag your transactions with <code>#{{tag_name}}</code>, so you can later filter or generate reports based on that tag. Therefore the Payee could be used as whom or who pays and the tag for the context. For example, for a trip I could use the tag #34C3</p> <p>To mark a series of transactions with tags use the following syntax <pre><code>pushtag #berlin-trip-2014\n\n2014-04-23 * \"Flight to Berlin\"\n  Expenses:Flights -1230.27 USD\n  Liabilities:CreditCard\n\n...\n\npoptag #berlin-trip-2014\n</code></pre></p>"}, {"location": "beancount/#links", "title": "Links", "text": "<p>Transactions can also be linked together. You may think of the link as a special kind of tag that can be used to group together a set of financially related transactions over time.</p> <pre><code>2014-02-05 * \"Invoice for January\" ^invoice-acme-studios-jan14\n  Income:Clients:ACMEStudios   -8450.00 USD\n  Assets:AccountsReceivable\n\n...\n\n2014-02-20 * \"Check deposit - payment from ACME\" ^invoice-acme-studios-jan14\n  Assets:BofA:Checking         8450.00 USD\n  Assets:AccountsReceivable\n</code></pre>"}, {"location": "beancount/#balance", "title": "Balance", "text": "<p>A balance assertion is a way for you to input your statement balance into the flow of transactions. It tells Beancount to verify that the number of units of a particular commodity in some account should equal some expected value at some point in time.</p> <p>If no error is reported, you should have some confidence that the list of transactions that precedes it in this account is highly likely to be correct. This is useful in practice because in many cases some transactions can get imported separately from the accounts of each of their postings.</p> <p>As all other non-transaction directives, it applies at the beginning of it's date. Just imagine that the balance checks occurs right after midnight on that day.</p> <pre><code>YYYY-MM-DD balance {{ account_name }} {{ amount }}\n</code></pre>"}, {"location": "beancount/#pad", "title": "Pad", "text": "<p>A padding directive automatically inserts a transaction that will make the subsequent balance assertion succeed, if it is needed. It inserts the difference needed to fulfill that balance assertion.</p> <p>Being subsequent in date order, not in the order of the declarations in the file.</p> <pre><code>YYYY-MM-DD pad {{ account_name }} {{ account_name_to_pad }}\n</code></pre> <p>The first account is the account to credit the automatically calculated amount to. This is the account that should have a balance assertion following it. The second leg is the source where the funds will come from, and this is almost always some Equity account.</p> <pre><code>1990-05-17 open Assets:Cash EUR\n1990-05-17 pad Assets:Cash Equity:Opening-Balances\n2017-12-26 balance Assets:Cash 250 EUR\n</code></pre> <p>You could also insert pad entries between balance assertions so as to fix un registered transactions</p>"}, {"location": "beancount/#notes", "title": "Notes", "text": "<p>A note directive is simply used to attach a dated comment to the journal of a particular account.</p> <p>this can be useful to record facts and claims associated with a financial event.</p> <pre><code>YYYY-MM-DD note {{ account_name }} {{ comment }}\n</code></pre>"}, {"location": "beancount/#document", "title": "Document", "text": "<p>A Document directive can be used to attach an external file to the journal of an account.</p> <p>The filename gets rendered as a browser link in the journals of the web interface for the corresponding account and you should be able to click on it to view the contents of the file itself.</p> <pre><code>YYYY-MM-DD {{ account_name }} {{ path/to/document }}\n</code></pre>"}, {"location": "beancount/#includes", "title": "Includes", "text": "<p>This allows you to split up large input files into multiple files.</p> <pre><code>include {{ path/to/file.beancount }}\n</code></pre> <p>The path could be relative or absolute.</p>"}, {"location": "beancount/#library-usage", "title": "Library usage", "text": "<p>Beancount can also be used as a Python library.</p> <p>There are some articles in the documentation where you can start seeing how to use it: scripting plugins , external contributions and the api reference. Although I found it more pleasant to read the source code itself as it's really well documented (both by docstrings and type hints).</p>"}, {"location": "beancount/#references", "title": "References", "text": "<ul> <li>Homepage</li> <li>Git</li> <li>Docs</li> <li>Awesome beancount</li> <li>Docs in google</li> <li>Vim plugin</li> </ul>"}, {"location": "beautifulsoup/", "title": "BeautifulSoup", "text": "<p>BeautifulSoup is a Python library for pulling data out of HTML and XML files. It works with your favorite parser to provide idiomatic ways of navigating, searching, and modifying the parse tree.</p> <pre><code>import requests\nfrom bs4 import BeautifulSoup\n\nrequest = requests.get('{{ url }}')\nsoup = BeautifulSoup(request.text, \"html.parser\")\n</code></pre> <p>Here are some simple ways to navigate that data structure:</p> <pre><code>soup.title\n# &lt;title&gt;The Dormouse's story&lt;/title&gt;\n\nsoup.title.name\n# u'title'\n\nsoup.title.string\n# u'The Dormouse's story'\n\nsoup.title.parent.name\n# u'head'\n\nsoup.p\n# &lt;p class=\"title\"&gt;&lt;b&gt;The Dormouse's story&lt;/b&gt;&lt;/p&gt;\n\nsoup.p['class']\n# u'title'\n\nsoup.a\n# &lt;a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\"&gt;Elsie&lt;/a&gt;\n\nsoup.find_all('a')\n# [&lt;a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\"&gt;Elsie&lt;/a&gt;,\n#  &lt;a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\"&gt;Lacie&lt;/a&gt;,\n#  &lt;a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\"&gt;Tillie&lt;/a&gt;]\n\nsoup.find(id=\"link3\")\n# &lt;a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\"&gt;Tillie&lt;/a&gt;\n</code></pre>"}, {"location": "beautifulsoup/#installation", "title": "Installation", "text": "<pre><code>pip install beautifulsoup4\n</code></pre> <p>The default parser <code>html.parser</code> doesn't work with HTML5, so you'll probably need to use the <code>html5lib</code> parser, it's not included by default, so you might need to install it as well</p> <pre><code>pip install html5lib\n</code></pre>"}, {"location": "beautifulsoup/#usage", "title": "Usage", "text": ""}, {"location": "beautifulsoup/#kinds-of-objects", "title": "Kinds of objects", "text": "<p>Beautiful Soup transforms a complex HTML document into a complex tree of Python objects. But you\u2019ll only ever have to deal with about four kinds of objects: <code>Tag</code>, <code>NavigableString</code>, <code>BeautifulSoup</code>, and <code>Comment</code>.</p>"}, {"location": "beautifulsoup/#tag", "title": "Tag", "text": "<p>A <code>Tag</code> object corresponds to an XML or HTML tag in the original document:</p> <pre><code>soup = BeautifulSoup('&lt;b class=\"boldest\"&gt;Extremely bold&lt;/b&gt;')\ntag = soup.b\ntype(tag)\n# &lt;class 'bs4.element.Tag'&gt;\n</code></pre> <p>The most important features of a tag are its <code>name</code> and <code>attributes</code>.</p>"}, {"location": "beautifulsoup/#name", "title": "Name", "text": "<p>Every tag has a <code>name</code>, accessible as <code>.name</code>:</p> <pre><code>tag.name\n# u'b'\n</code></pre> <p>If you change a tag\u2019s name, the change will be reflected in any HTML markup generated by Beautiful Soup:.</p> <pre><code>tag.name = \"blockquote\"\ntag\n# &lt;blockquote class=\"boldest\"&gt;Extremely bold&lt;/blockquote&gt;\n</code></pre>"}, {"location": "beautifulsoup/#attributes", "title": "Attributes", "text": "<p>A tag may have any number of attributes. The tag <code>&lt;b id=\"boldest\"&gt;</code> has an attribute <code>id</code> whose value is <code>boldest</code>. You can access a tag\u2019s attributes by treating the tag like a dictionary:</p> <pre><code>tag['id']\n# u'boldest'\n</code></pre> <p>You can access that dictionary directly as <code>.attrs</code>:</p> <pre><code>tag.attrs\n# {u'id': 'boldest'}\n</code></pre> <p>You can add, remove, and modify a tag\u2019s attributes. Again, this is done by treating the tag as a dictionary:</p> <pre><code>tag['id'] = 'verybold'\ntag['another-attribute'] = 1\ntag\n# &lt;b another-attribute=\"1\" id=\"verybold\"&gt;&lt;/b&gt;\n\ndel tag['id']\ndel tag['another-attribute']\ntag\n# &lt;b&gt;&lt;/b&gt;\n\ntag['id']\n# KeyError: 'id'\nprint(tag.get('id'))\n# None\n</code></pre>"}, {"location": "beautifulsoup/#multi-valued-attributes", "title": "Multi-valued attributes", "text": "<p>HTML 4 defines a few attributes that can have multiple values. HTML 5 removes a couple of them, but defines a few more. The most common multi-valued attribute is <code>class</code> (that is, a tag can have more than one CSS class). Others include <code>rel</code>, <code>rev</code>, <code>accept-charset</code>, <code>headers</code>, and <code>accesskey</code>. Beautiful Soup presents the value(s) of a multi-valued attribute as a list:</p> <pre><code>css_soup = BeautifulSoup('&lt;p class=\"body\"&gt;&lt;/p&gt;')\ncss_soup.p['class']\n# [\"body\"]\n\ncss_soup = BeautifulSoup('&lt;p class=\"body strikeout\"&gt;&lt;/p&gt;')\ncss_soup.p['class']\n# [\"body\", \"strikeout\"]\n</code></pre> <p>If an attribute looks like it has more than one value, but it\u2019s not a multi-valued attribute as defined by any version of the HTML standard, Beautiful Soup will leave the attribute alone:</p> <pre><code>id_soup = BeautifulSoup('&lt;p id=\"my id\"&gt;&lt;/p&gt;')\nid_soup.p['id']\n# 'my id'\n</code></pre> <p>When you turn a tag back into a string, multiple attribute values are consolidated:</p> <pre><code>rel_soup = BeautifulSoup('&lt;p&gt;Back to the &lt;a rel=\"index\"&gt;homepage&lt;/a&gt;&lt;/p&gt;')\nrel_soup.a['rel']\n# ['index']\nrel_soup.a['rel'] = ['index', 'contents']\nprint(rel_soup.p)\n# &lt;p&gt;Back to the &lt;a rel=\"index contents\"&gt;homepage&lt;/a&gt;&lt;/p&gt;\n</code></pre> <p>If you parse a document as XML, there are no multi-valued attributes:</p>"}, {"location": "beautifulsoup/#navigablestring", "title": "NavigableString", "text": "<p>A string corresponds to a bit of text within a tag. Beautiful Soup uses the <code>NavigableString</code> class to contain these bits of text:</p> <pre><code>tag.string\n# u'Extremely bold'\ntype(tag.string)\n# &lt;class 'bs4.element.NavigableString'&gt;\n</code></pre> <p>A <code>NavigableString</code> is just like a Python Unicode string, except that it also supports some of the features described in Navigating the tree and Searching the tree. You can convert a <code>NavigableString</code> to a Unicode string with <code>unicode()</code>:</p> <pre><code>unicode_string = unicode(tag.string)\nunicode_string\n# u'Extremely bold'\ntype(unicode_string)\n# &lt;type 'unicode'&gt;\n</code></pre> <p>You can\u2019t edit a string in place, but you can replace one string with another, using <code>replace_with()</code>:</p> <pre><code>tag.string.replace_with(\"No longer bold\")\ntag\n# &lt;blockquote&gt;No longer bold&lt;/blockquote&gt;\n</code></pre>"}, {"location": "beautifulsoup/#beautifulsoup", "title": "BeautifulSoup", "text": "<p>The <code>BeautifulSoup</code> object represents the parsed document as a whole. For most purposes, you can treat it as a <code>Tag</code> object. This means it supports most of the methods described in Navigating the tree and Searching the tree.</p>"}, {"location": "beautifulsoup/#navigating-the-tree", "title": "Navigating the tree", "text": ""}, {"location": "beautifulsoup/#going-down", "title": "Going down", "text": "<p>Tags may contain strings and other tags. These elements are the tag\u2019s children. Beautiful Soup provides a lot of different attributes for navigating and iterating over a tag\u2019s children.</p> <p>Note that Beautiful Soup strings don\u2019t support any of these attributes, because a string can\u2019t have children.</p>"}, {"location": "beautifulsoup/#navigating-using-tag-names", "title": "Navigating using tag names", "text": "<p>The simplest way to navigate the parse tree is to say the name of the tag you want. If you want the <code>&lt;head&gt;</code> tag, just say <code>soup.head</code>:</p> <pre><code>soup.head\n# &lt;head&gt;&lt;title&gt;The Dormouse's story&lt;/title&gt;&lt;/head&gt;\n\nsoup.title\n# &lt;title&gt;The Dormouse's story&lt;/title&gt;\n</code></pre> <p>You can do use this trick again and again to zoom in on a certain part of the parse tree. This code gets the first <code>&lt;b&gt;</code> tag beneath the <code>&lt;body&gt;</code> tag:</p> <pre><code>soup.body.b\n# &lt;b&gt;The Dormouse's story&lt;/b&gt;\n</code></pre> <p>Using a tag name as an attribute will give you only the first tag by that name:</p> <pre><code>soup.a\n# &lt;a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\"&gt;Elsie&lt;/a&gt;\n</code></pre> <p>If you need to get all the <code>&lt;a&gt;</code> tags, or anything more complicated than the first tag with a certain name, you\u2019ll need to use one of the methods described in Searching the tree, such as <code>find_all()</code>:</p> <pre><code>soup.find_all('a')\n# [&lt;a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\"&gt;Elsie&lt;/a&gt;,\n#  &lt;a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\"&gt;Lacie&lt;/a&gt;,\n#  &lt;a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\"&gt;Tillie&lt;/a&gt;]\n</code></pre>"}, {"location": "beautifulsoup/#contents-and-children", "title": "<code>.contents</code> and <code>.children</code>", "text": "<p>A tag\u2019s children are available in a list called <code>.contents</code>:</p> <pre><code>head_tag = soup.head\nhead_tag\n# &lt;head&gt;&lt;title&gt;The Dormouse's story&lt;/title&gt;&lt;/head&gt;\n\nhead_tag.contents\n[&lt;title&gt;The Dormouse's story&lt;/title&gt;]\n\ntitle_tag = head_tag.contents[0]\ntitle_tag\n# &lt;title&gt;The Dormouse's story&lt;/title&gt;\ntitle_tag.contents\n# [u'The Dormouse's story']\n</code></pre> <p>Instead of getting them as a list, you can iterate over a tag\u2019s children using the <code>.children</code> generator:</p> <pre><code>for child in title_tag.children:\n    print(child)\n# The Dormouse's story\n</code></pre>"}, {"location": "beautifulsoup/#descendants", "title": "<code>.descendants</code>", "text": "<p>The <code>.contents</code> and <code>.children</code> attributes only consider a tag\u2019s direct children. For instance, the <code>&lt;head&gt;</code> tag has a single direct child\u2013the <code>&lt;title&gt;</code> tag:</p> <pre><code>head_tag.contents\n# [&lt;title&gt;The Dormouse's story&lt;/title&gt;]\n</code></pre> <p>But the <code>&lt;title&gt;</code> tag itself has a child: the string <code>The Dormouse\u2019s story</code>. There\u2019s a sense in which that string is also a child of the <code>&lt;head&gt;</code> tag. The <code>.descendants</code> attribute lets you iterate over all of a tag\u2019s children, recursively: its direct children, the children of its direct children, and so on:.</p> <pre><code>for child in head_tag.descendants:\n    print(child)\n# &lt;title&gt;The Dormouse's story&lt;/title&gt;\n# The Dormouse's story\n</code></pre>"}, {"location": "beautifulsoup/#string", "title": "<code>.string</code>", "text": "<p>If a tag has only one child, and that child is a <code>NavigableString</code>, the child is made available as <code>.string</code>:</p> <pre><code>title_tag.string\n# u'The Dormouse's story'\n</code></pre> <p>If a tag\u2019s only child is another tag, and that tag has a <code>.string</code>, then the parent tag is considered to have the same <code>.string</code> as its child:</p> <pre><code>head_tag.contents\n# [&lt;title&gt;The Dormouse's story&lt;/title&gt;]\n\nhead_tag.string\n# u'The Dormouse's story'\n</code></pre> <p>If a tag contains more than one thing, then it\u2019s not clear what <code>.string</code> should refer to, so <code>.string</code> is defined to be <code>None</code>:</p> <pre><code>print(soup.html.string)\n# None\n</code></pre>"}, {"location": "beautifulsoup/#strings-and-stripped_strings", "title": "<code>.strings</code> and <code>.stripped_strings</code>", "text": "<p>If there\u2019s more than one thing inside a tag, you can still look at just the strings. Use the <code>.strings</code> generator:</p> <pre><code>for string in soup.strings:\n    print(repr(string))\n# u\"The Dormouse's story\"\n# u'\\n\\n'\n# u\"The Dormouse's story\"\n# u'\\n\\n'\n</code></pre> <p>These strings tend to have a lot of extra whitespace, which you can remove by using the <code>.stripped_strings</code> generator instead:</p> <pre><code>for string in soup.stripped_strings:\n    print(repr(string))\n# u\"The Dormouse's story\"\n# u\"The Dormouse's story\"\n# u'Once upon a time there were three little sisters; and their names were'\n# u'Elsie'\n</code></pre>"}, {"location": "beautifulsoup/#going-up", "title": "Going up", "text": "<p>Continuing the \u201cfamily tree\u201d analogy, every tag and every string has a parent: the tag that contains it.</p>"}, {"location": "beautifulsoup/#parent", "title": "<code>.parent</code>", "text": "<p>You can access an element\u2019s parent with the <code>.parent</code> attribute.</p> <pre><code>title_tag = soup.title\ntitle_tag\n# &lt;title&gt;The Dormouse's story&lt;/title&gt;\ntitle_tag.parent\n# &lt;head&gt;&lt;title&gt;The Dormouse's story&lt;/title&gt;&lt;/head&gt;\n</code></pre>"}, {"location": "beautifulsoup/#parents", "title": "<code>.parents</code>", "text": "<p>You can iterate over all of an element\u2019s parents with <code>.parents</code>.</p>"}, {"location": "beautifulsoup/#going-sideways", "title": "Going sideways", "text": "<p>When a document is pretty-printed, siblings show up at the same indentation level. You can also use this relationship in the code you write.</p>"}, {"location": "beautifulsoup/#next_sibling-and-previous_sibling", "title": "<code>.next_sibling</code> and <code>.previous_sibling</code>", "text": "<p>You can use <code>.next_sibling</code> and <code>.previous_sibling</code> to navigate between page elements that are on the same level of the parse tree:.</p> <pre><code>sibling_soup.b.next_sibling\n# &lt;c&gt;text2&lt;/c&gt;\n\nsibling_soup.c.previous_sibling\n# &lt;b&gt;text1&lt;/b&gt;\n</code></pre> <p>The <code>&lt;b&gt;</code> tag has a <code>.next_sibling</code>, but no <code>.previous_sibling</code>, because there\u2019s nothing before the <code>&lt;b&gt;</code> tag on the same level of the tree. For the same reason, the <code>&lt;c&gt;</code> tag has a <code>.previous_sibling</code> but no <code>.next_sibling</code>:</p> <pre><code>print(sibling_soup.b.previous_sibling)\n# None\nprint(sibling_soup.c.next_sibling)\n# None\n</code></pre> <p>In real documents, the <code>.next_sibling</code> or <code>.previous_sibling</code> of a tag will usually be a string containing whitespace.</p> <pre><code>&lt;a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\"&gt;Elsie&lt;/a&gt;\n&lt;a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\"&gt;Lacie&lt;/a&gt;\n&lt;a href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\"&gt;Tillie&lt;/a&gt;\n</code></pre> <p>You might think that the .next_sibling of the first <code>&lt;a&gt;</code> tag would be the second <code>&lt;a&gt;</code> tag. But actually, it\u2019s a string: the comma and newline that separate the first <code>&lt;a&gt;</code> tag from the second:</p> <pre><code>link = soup.a\nlink\n# &lt;a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\"&gt;Elsie&lt;/a&gt;\n\nlink.next_sibling\n# u',\\n'\n</code></pre> <p>The second <code>&lt;a&gt;</code> tag is actually the <code>.next_sibling</code> of the comma:</p> <pre><code>link.next_sibling.next_sibling\n# &lt;a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\"&gt;Lacie&lt;/a&gt;\n</code></pre>"}, {"location": "beautifulsoup/#next_siblings-and-previous_siblings", "title": "<code>.next_siblings</code> and <code>.previous_siblings</code>", "text": "<p>You can iterate over a tag\u2019s siblings with <code>.next_siblings</code> or <code>.previous_siblings</code>:</p> <pre><code>for sibling in soup.a.next_siblings:\n    print(repr(sibling))\n# u',\\n'\n# &lt;a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\"&gt;Lacie&lt;/a&gt;\n# u' and\\n'\n# &lt;a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\"&gt;Tillie&lt;/a&gt;\n# u'; and they lived at the bottom of a well.'\n# None\n\nfor sibling in soup.find(id=\"link3\").previous_siblings:\n    print(repr(sibling))\n# ' and\\n'\n# &lt;a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\"&gt;Lacie&lt;/a&gt;\n# u',\\n'\n# &lt;a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\"&gt;Elsie&lt;/a&gt;\n# u'Once upon a time there were three little sisters; and their names were\\n'\n# None\n</code></pre>"}, {"location": "beautifulsoup/#searching-the-tree", "title": "Searching the tree", "text": "<p>By passing in a filter to an argument like <code>find_all()</code>, you can zoom in on the parts of the document you\u2019re interested in.</p>"}, {"location": "beautifulsoup/#kinds-of-filters", "title": "Kinds of filters", "text": ""}, {"location": "beautifulsoup/#a-string", "title": "A string", "text": "<p>The simplest filter is a string. Pass a string to a search method and Beautiful Soup will perform a match against that exact string. This code finds all the <code>&lt;b&gt;</code> tags in the document:</p> <pre><code>soup.find_all('b')\n# [&lt;b&gt;The Dormouse's story&lt;/b&gt;]\n</code></pre>"}, {"location": "beautifulsoup/#a-regular-expression", "title": "A regular expression", "text": "<p>If you pass in a regular expression object, Beautiful Soup will filter against that regular expression using its <code>search()</code> method. This code finds all the tags whose names start with the letter <code>b</code>; in this case, the <code>&lt;body&gt;</code> tag and the <code>&lt;b&gt;</code> tag:</p> <pre><code>import re\nfor tag in soup.find_all(re.compile(\"^b\")):\n    print(tag.name)\n# body\n# b\n</code></pre>"}, {"location": "beautifulsoup/#a-list", "title": "A list", "text": "<p>If you pass in a list, Beautiful Soup will allow a string match against any item in that list. This code finds all the <code>&lt;a&gt;</code> tags and all the <code>&lt;b&gt;</code> tags:</p> <pre><code>soup.find_all([\"a\", \"b\"])\n# [&lt;b&gt;The Dormouse's story&lt;/b&gt;,\n#  &lt;a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\"&gt;Elsie&lt;/a&gt;,\n#  &lt;a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\"&gt;Lacie&lt;/a&gt;,\n#  &lt;a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\"&gt;Tillie&lt;/a&gt;]\n</code></pre>"}, {"location": "beautifulsoup/#a-function", "title": "A function", "text": "<p>If none of the other matches work for you, define a function that takes an element as its only argument. The function should return <code>True</code> if the argument matches, and <code>False</code> otherwise.</p> <p>Here\u2019s a function that returns <code>True</code> if a tag defines the <code>class</code> attribute but doesn\u2019t define the <code>id</code> attribute:</p> <pre><code>def has_class_but_no_id(tag):\n    return tag.has_attr('class') and not tag.has_attr('id')\n</code></pre> <p>Pass this function into <code>find_all()</code> and you\u2019ll pick up all the <code>&lt;p&gt;</code> tags:</p> <pre><code>soup.find_all(has_class_but_no_id)\n# [&lt;p class=\"title\"&gt;&lt;b&gt;The Dormouse's story&lt;/b&gt;&lt;/p&gt;,\n#  &lt;p class=\"story\"&gt;Once upon a time there were...&lt;/p&gt;,\n#  &lt;p class=\"story\"&gt;...&lt;/p&gt;]\n</code></pre>"}, {"location": "beautifulsoup/#find_all", "title": "find_all()", "text": "<p>The <code>find_all()</code> method looks through a tag\u2019s descendants and retrieves all descendants that match your filters.</p> <pre><code>soup.find_all(\"title\")\n# [&lt;title&gt;The Dormouse's story&lt;/title&gt;]\n\nsoup.find_all(\"p\", \"title\")\n# [&lt;p class=\"title\"&gt;&lt;b&gt;The Dormouse's story&lt;/b&gt;&lt;/p&gt;]\n\nsoup.find_all(\"a\")\n# [&lt;a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\"&gt;Elsie&lt;/a&gt;,\n#  &lt;a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\"&gt;Lacie&lt;/a&gt;,\n#  &lt;a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\"&gt;Tillie&lt;/a&gt;]\n\nsoup.find_all(id=\"link2\")\n# [&lt;a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\"&gt;Lacie&lt;/a&gt;]\n\nimport re\nsoup.find(string=re.compile(\"sisters\"))\n# u'Once upon a time there were three little sisters; and their names were\\n'\n</code></pre>"}, {"location": "beautifulsoup/#the-name-argument", "title": "The <code>name</code> argument", "text": "<p>Pass in a value for <code>name</code> and you\u2019ll tell Beautiful Soup to only consider tags with certain names. Text strings will be ignored, as will tags whose names that don\u2019t match.</p> <p>This is the simplest usage:</p> <pre><code>soup.find_all(\"title\")\n# [&lt;title&gt;The Dormouse's story&lt;/title&gt;]\n</code></pre>"}, {"location": "beautifulsoup/#the-keyword-arguments", "title": "The <code>keyword</code> arguments", "text": "<p>Any argument that\u2019s not recognized will be turned into a filter on one of a tag\u2019s attributes. If you pass in a value for an argument called <code>id</code>, Beautiful Soup will filter against each tag\u2019s <code>id</code> attribute:</p> <pre><code>soup.find_all(id='link2')\n# [&lt;a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\"&gt;Lacie&lt;/a&gt;]\n</code></pre> <p>You can filter an attribute based on a string, a regular expression, a list, a function, or the value True.</p> <p>You can filter multiple attributes at once by passing in more than one keyword argument:</p> <pre><code>soup.find_all(href=re.compile(\"elsie\"), id='link1')\n# [&lt;a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\"&gt;three&lt;/a&gt;]\n</code></pre>"}, {"location": "beautifulsoup/#searching-by-css-class", "title": "Searching by CSS class", "text": "<p>It\u2019s very useful to search for a tag that has a certain CSS class, but the name of the CSS attribute, <code>class</code>, is a reserved word in Python. Using class as a keyword argument will give you a syntax error. As of Beautiful Soup 4.1.2, you can search by CSS class using the keyword argument <code>class_</code>:</p> <pre><code>soup.find_all(\"a\", class_=\"sister\")\n# [&lt;a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\"&gt;Elsie&lt;/a&gt;,\n#  &lt;a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\"&gt;Lacie&lt;/a&gt;,\n#  &lt;a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\"&gt;Tillie&lt;/a&gt;]\n</code></pre>"}, {"location": "beautifulsoup/#the-string-argument", "title": "The string argument", "text": "<p>With <code>string</code> you can search for strings instead of tags.</p> <pre><code>soup.find_all(string=\"Elsie\")\n# [u'Elsie']\n\nsoup.find_all(string=[\"Tillie\", \"Elsie\", \"Lacie\"])\n# [u'Elsie', u'Lacie', u'Tillie']\n\nsoup.find_all(string=re.compile(\"Dormouse\"))\n[u\"The Dormouse's story\", u\"The Dormouse's story\"]\n\ndef is_the_only_string_within_a_tag(s):\n\"\"\"Return True if this string is the only child of its parent tag.\"\"\"\n    return (s == s.parent.string)\n\nsoup.find_all(string=is_the_only_string_within_a_tag)\n# [u\"The Dormouse's story\", u\"The Dormouse's story\", u'Elsie', u'Lacie', u'Tillie', u'...']\n</code></pre> <p>Although string is for finding strings, you can combine it with arguments that find tags: Beautiful Soup will find all tags whose <code>.string</code> matches your value for string.</p> <pre><code>soup.find_all(\"a\", string=\"Elsie\")\n# [&lt;a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\"&gt;Elsie&lt;/a&gt;]\n</code></pre>"}, {"location": "beautifulsoup/#searching-by-attribute-and-value", "title": "Searching by attribute and value", "text": "<pre><code>soup = BeautifulSoup(html)\nresults = soup.findAll(\"td\", {\"valign\" : \"top\"})\n</code></pre>"}, {"location": "beautifulsoup/#the-limit-argument", "title": "The limit argument", "text": "<p><code>find_all()</code> returns all the tags and strings that match your filters. This can take a while if the document is large. If you don\u2019t need all the results, you can pass in a number for <code>limit</code>.</p>"}, {"location": "beautifulsoup/#the-recursive-argument", "title": "The recursive argument", "text": "<p>If you call <code>mytag.find_all()</code>, Beautiful Soup will examine all the descendants of <code>mytag</code>. If you only want Beautiful Soup to consider direct children, you can pass in <code>recursive=False</code>.</p>"}, {"location": "beautifulsoup/#calling-a-tag-is-like-calling-find_all", "title": "Calling a tag is like calling find_all()", "text": "<p>Because <code>find_all()</code> is the most popular method in the Beautiful Soup search API, you can use a shortcut for it. If you treat the BeautifulSoup object or a Tag object as though it were a function, then it\u2019s the same as calling <code>find_all()</code> on that object. These two lines of code are equivalent:</p> <pre><code>soup.find_all(\"a\")\nsoup(\"a\")\n</code></pre>"}, {"location": "beautifulsoup/#find", "title": "<code>find()</code>", "text": "<p><code>find()</code> is like <code>find_all()</code> but returning just one result.</p>"}, {"location": "beautifulsoup/#find_parent-and-find_parents", "title": "<code>find_parent()</code> and <code>find_parents()</code>", "text": "<p>These methods work their way up the tree, looking at a tag\u2019s (or a string\u2019s) parents.</p>"}, {"location": "beautifulsoup/#find_next_siblings-and-find_next_sibling", "title": "<code>find_next_siblings()</code> and <code>find_next_sibling()</code>", "text": "<p>These methods use <code>.next_siblings</code> to iterate over the rest of an element\u2019s siblings in the tree. The <code>find_next_siblings()</code> method returns all the siblings that match, and <code>find_next_sibling()</code> only returns the first one:</p> <pre><code>first_link = soup.a\nfirst_link\n# &lt;a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\"&gt;Elsie&lt;/a&gt;\n\nfirst_link.find_next_siblings(\"a\")\n# [&lt;a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\"&gt;Lacie&lt;/a&gt;,\n#  &lt;a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\"&gt;Tillie&lt;/a&gt;]\n</code></pre> <p>To go in the other direction you can use <code>find_previous_siblings()</code> and <code>find_previous_sibling()</code></p>"}, {"location": "beautifulsoup/#modifying-the-tree", "title": "Modifying the tree", "text": ""}, {"location": "beautifulsoup/#replace_with", "title": "<code>replace_with</code>", "text": "<p><code>PageElement.replace_with()</code> removes a tag or string from the tree, and replaces it with the tag or string of your choice:</p> <pre><code>markup = '&lt;a href=\"http://example.com/\"&gt;I linked to &lt;i&gt;example.com&lt;/i&gt;&lt;/a&gt;'\nsoup = BeautifulSoup(markup)\na_tag = soup.a\n\nnew_tag = soup.new_tag(\"b\")\nnew_tag.string = \"example.net\"\na_tag.i.replace_with(new_tag)\n\na_tag\n# &lt;a href=\"http://example.com/\"&gt;I linked to &lt;b&gt;example.net&lt;/b&gt;&lt;/a&gt;\n</code></pre> <p>Sometimes it doesn't work. If it doesn't use:</p> <pre><code>a_tag.clear()\na_tag.append(new_tag)\n</code></pre>"}, {"location": "beautifulsoup/#tips", "title": "Tips", "text": ""}, {"location": "beautifulsoup/#show-content-beautified-prettified", "title": "Show content beautified / prettified", "text": "<p>Use <code>print(soup.prettify())</code>.</p>"}, {"location": "beautifulsoup/#cleaning-escaped-html-code", "title": "Cleaning escaped HTML code", "text": "<pre><code>soup = BeautifulSoup(s.replace(r\"\\\"\", '\"').replace(r\"\\/\", \"/\"), \"html.parser\")\n</code></pre>"}, {"location": "beautifulsoup/#references", "title": "References", "text": "<ul> <li>Docs</li> </ul>"}, {"location": "beets/", "title": "Beets", "text": "<p>Beets is a music management library used to get your music collection right once and for all. It catalogs your collection, automatically improving its metadata as it goes using the MusicBrainz database. Then it provides a set of tools for manipulating and accessing your music.</p> <p>Through plugins it supports:</p> <ul> <li>Fetch or calculate all the metadata you could possibly need: album art,     lyrics, genres, tempos,     ReplayGain     levels, or acoustic fingerprints.</li> <li>Get metadata from MusicBrainz, Discogs, or Beatport. Or guess metadata using     songs\u2019 filenames or their acoustic fingerprints.</li> <li>Transcode audio to any format you like.</li> <li>Check your library for duplicate tracks and albums or for albums that are     missing tracks.</li> <li>Browse your music library graphically through a Web browser and play it in any     browser that supports HTML5 Audio.</li> </ul> <p>Still, if beets doesn't do what you want yet, writing your own plugin is easy if you know a little Python. Or you can use it as a library.</p>"}, {"location": "beets/#installation", "title": "Installation", "text": "<pre><code>pipx install beets\n</code></pre> <p>You\u2019ll want to set a few basic options before you start using beets. The configuration is stored in a text file. You can show its location by running <code>beet config -p</code>, though it may not exist yet. Run <code>beet config -e</code> to edit the configuration in your favorite text editor. The file will start out empty, but here\u2019s good place to start:</p> <pre><code># Path to a directory where you\u2019d like to keep your music.\ndirectory: ~/music\n\n# Database file that keeps an index of your music.\nlibrary: ~/data/musiclibrary.db\n</code></pre> <p>The default configuration assumes you want to start a new organized music folder (that directory above) and that you\u2019ll copy cleaned-up music into that empty folder using beets\u2019 <code>import</code> command. But you can configure beets to behave many other ways:</p> <ul> <li> <p>Start with a new empty directory, but move new music in instead of copying it (saving disk space). Put this in your config file:</p> <pre><code>import:\nmove: yes\n</code></pre> </li> <li> <p>Keep your current directory structure; importing should never move or copy files but instead just correct the tags on music. Put the line <code>copy: no</code> under the <code>import:</code> heading in your config file to disable any copying or renaming. Make sure to point <code>directory</code> at the place where your music is currently stored.</p> </li> <li> <p>Keep your current directory structure and do not correct files\u2019 tags: leave files completely unmodified on your disk. (Corrected tags will still be stored in beets\u2019 database, and you can use them to do renaming or tag changes later.) Put this in your config file:</p> <pre><code>import:\ncopy: no\nwrite: no\n</code></pre> <p>to disable renaming and tag-writing.</p> </li> </ul>"}, {"location": "beets/#usage", "title": "Usage", "text": ""}, {"location": "beets/#importing-your-library", "title": "Importing your library", "text": "<p>The next step is to import your music files into the beets library database. Because this can involve modifying files and moving them around, data loss is always a possibility, so now would be a good time to make sure you have a recent backup of all your music. We\u2019ll wait.</p> <p>There are two good ways to bring your existing library into beets. You can either: (a) quickly bring all your files with all their current metadata into beets\u2019 database, or (b) use beets\u2019 highly-refined autotagger to find canonical metadata for every album you import. Option (a) is really fast, but option (b) makes sure all your songs\u2019 tags are exactly right from the get-go. The point about speed bears repeating: using the autotagger on a large library can take a very long time, and it\u2019s an interactive process. So set aside a good chunk of time if you\u2019re going to go that route.</p> <p>If you\u2019ve got time and want to tag all your music right once and for all, do this:</p> <pre><code>$ beet import /path/to/my/music\n</code></pre> <p>(Note that by default, this command will copy music into the directory you specified above. If you want to use your current directory structure, set the import.copy config option.) To take the fast, un-autotagged path, just say:</p> <pre><code>$ beet import -A /my/huge/mp3/library\n</code></pre> <p>Note that you just need to add <code>-A</code> for \u201cdon\u2019t autotag\u201d.</p>"}, {"location": "beets/#references", "title": "References", "text": "<ul> <li>Git</li> <li>Docs</li> <li>Homepage</li> </ul>"}, {"location": "book_binding/", "title": "Book binding", "text": "<p>Book binding is the process of physically assembling a book of codex format from an ordered stack of paper sheets that are folded together into sections called signatures or sometimes left as a stack of individual sheets. Several signatures are then bound together along one edge with a thick needle and sturdy thread.</p>"}, {"location": "book_binding/#references", "title": "References", "text": "<ul> <li>http://tuxgraphics.org/npa/book-binding/</li> <li>https://www.instructables.com/id/How-to-bind-your-own-Hardback-Book/</li> <li>https://www.instructables.com/id/Binding-a-Book-With-Common-Materials/</li> <li>https://www.instructables.com/id/Perfect-Bound-Paperback-Notebook/</li> </ul>"}, {"location": "book_binding/#videos", "title": "Videos", "text": "<ul> <li>https://www.youtube.com/watch?v=S2FRKbQI2kY</li> <li>https://www.youtube.com/watch?v=Av_rU-yOPd4</li> <li>https://www.youtube.com/watch?v=9O4kFTOEh6k</li> <li>https://www.youtube.com/watch?v=XGQ5P8QVHSg</li> </ul>"}, {"location": "book_management/", "title": "Book Management", "text": "<p>Book management is the set of systems and processes to get and categorize books so it's easy to browse and discover new content. It involves the next actions:</p> <ul> <li>Automatically index and download metadata of new books.</li> <li>Notify the user when a new book is added.</li> <li>Monitor the books of an author, and get them once they are released.</li> <li>Send books to the e-reader.</li> <li>A nice interface to browse the existent library, with the possibility of     filtering by author, genre, years, tags or series.</li> <li>An interface to preview or read the items.</li> <li>An interface to rate and review library items.</li> <li>An interface to discover new content based on the ratings and item metadata.</li> </ul> <p>I haven't yet found a single piece of software that fulfills all these needs, so we need to split it into subsystems.</p> <ul> <li>Downloader and indexer.</li> <li>Gallery browser.</li> <li>Review system.</li> <li>Content discovery.</li> </ul>"}, {"location": "book_management/#downloader-and-indexer", "title": "Downloader and indexer", "text": "<p>System that monitors the availability of books in a list of indexers, when they are available, they download it to a directory of the server. The best one that I've found is Readarr, it makes it easy to search for authors and books, supports a huge variety of indexers (such as Archive.org), and download clients (such as torrent clients).</p> <p>It can be used as a limited gallery browser, you can easily see the books of an author or series, but it doesn't yet support the automatic fetch of genres or tags.</p> <p>I haven't found an easy way of marking elements as read, prioritize the list of books to read, or add a user rating. Until these features are added (if they ever are), we need to use it in parallel with a better gallery browser.</p>"}, {"location": "book_management/#gallery-browser", "title": "Gallery browser", "text": "<p>System that shows the books in the library in a nice format, allowing the user to filter out the contents, prioritize them, mark them as read, rate them and optionally sync books with the ereader.</p> <p>Calibre-web is a beautiful solution, without trying it, it looks like it supports all of the required features, but it doesn't work well with Readarr. Readarr has support to interact with Calibre content server by defining a root folder to be managed by Calibre, but the books you want to have Readarr recognize on initial library import must already be in Calibre. Books within the folder and not in Calibre will be ignored. So you'll need to do the first import in Calibre, instead of Readarr (which is quite pleasant). Note also that you cannot add Calibre integration to a root folder after it's created.</p> <p>Calibre-web interacts directly with the Sqlite database of Calibre, so it doesn't expose the Calibre Content Server, therefore is not compatible with Readarr.</p> <p>To make it work, you'd need to have both the <code>calibre</code> server and the <code>calibre-web</code> running at the same time, which has led to database locks (1, and 2) that the <code>calibre-web</code> developer has tried to avoid by controlling the database writes, and said that:</p> <p>If you start Calibre first and afterwards Calibre-Web, Calibre indeed locks the database and doesn't allow Calibre-Web to access the database (metadata.db) file. Starting Calibre-Web and afterwards Calibre should work.</p> <p>The problem comes when Readarr writes in the database through calibre to add books, and calibre-web tries to write too to add user ratings or other metadata.</p> <p>Another option would be to only run calibre-web and automatically import the books once they are downloaded by Readarr. calibre-web is not going to support a watch directory feature, the author recommends to use a cron script to do it. I haven't tried this path yet.</p> <p>Another option would be to assume that calibre-web is not going to do any insert in the database, so it would become a read-only web interface, therefore we wouldn't be able to edit the books or rate them, one of the features we'd like to have in the gallery browser. To make sure that we don't get locks, instead of using the same file, a cron job could do an <code>rsync</code> between the database managed by <code>calibre</code> and the one used by <code>calibre-web</code>.</p> <p>Calibre implements genres with tags, behind the scenes it uses the <code>fetch-ebook-metadata</code> command line tool, that returns all the metadata in human readable form</p> <pre><code>$: fetch-ebook-metadata -i 9780061796883 -c cover.jpg\n\nTitle               : The Dispossessed\nAuthor(s)           : Ursula K. le Guin\nPublisher           : Harper Collins\nTags                : Fiction, Science Fiction, Space Exploration, Literary, Visionary &amp; Metaphysical\nLanguages           : eng\nPublished           : 2009-10-13T20:34:30.878865+00:00\nIdentifiers         : google:tlhFtmTixvwC, isbn:9780061796883\nComments            : \u201cOne of the greats\u2026.Not just a science fiction writer; a literary icon.\u201d \u2013 Stephen KingFrom the brilliant and award-winning author Ursula K. Le Guin comes a classic tale of two planets torn\napart by conflict and mistrust \u2014 and the man who risks everything to reunite them.A bleak moon settled by utopian anarchists, Anarres has long been isolated from other worlds, including its mother planet, Urras\u2014a\n civilization of warring nations, great poverty, and immense wealth. Now Shevek, a brilliant physicist, is determined to reunite the two planets, which have been divided by centuries of distrust. He will seek ans\nwers, question the unquestionable, and attempt to tear down the walls of hatred that have kept them apart.To visit Urras\u2014to learn, to teach, to share\u2014will require great sacrifice and risks, which Shevek willingly\n accepts. But the ambitious scientist's gift is soon seen as a threat, and in the profound conflict that ensues, he must reexamine his beliefs even as he ignites the fires of change.\nCover               : cover.jpg\n</code></pre> <p>Or in xml if you use the <code>-o</code> flag.</p> <p>I've checked if these tags could be automatically applied to Readarr, but their tags are meant only to be attached to Authors to apply metadata profiles. I've opened an issue to see if they plan to implement tags for books.</p> <p>It's a pity we are not going to use <code>calibre-web</code> as it also had support to sync the reading stats from Kobo.</p> <p>In the past I used gcstar and then polar bookshelf, but decided not to use them anymore for different reasons.</p> <p>In conclusion, the tools reviewed don't work as I need them to, some ugly patches could be applied and maybe it would work, but it clearly shows that they are not ready yet unless you want to invest time in it, and even if you did, it will be unstable. Until a better system shows up, I'm going to use Readarr to browse the books that I want to read, and add them to an ordered markdown file with sections as genres, not ideal, but robust as hell xD.</p>"}, {"location": "book_management/#review-system", "title": "Review system", "text": "<p>System to write reviews and rate books, if the gallery browser doesn't include it, we'll use an independent component.</p> <p>Until I find something better, I'm saving the title, author, genre, score, and review in a json file, so it's easy to import in the chosen component.</p>"}, {"location": "book_management/#content-discovery", "title": "Content discovery", "text": "<p>Recommendation system to analyze the user taste and suggest books that might like. Right now I'm monitoring the authors with Readarr to get notifications when they release a new book. I also manually go through goodreads and similar websites looking for similar books to the ones I liked.</p>"}, {"location": "book_management/#deprecated-components", "title": "Deprecated components", "text": ""}, {"location": "book_management/#polar-bookself", "title": "Polar bookself", "text": "<p>It was a very promising piece of software that went wrong :(. It had a nice interface built for incremental reading and studying with anki, and a nice tag system. It was a desktop application you installed in your computer, but since Polar 2.0 they moved into a cloud hosted service, with no possibility of self-hosting it, so you give them your books and all your data, a nasty turn of events.</p>"}, {"location": "book_management/#gcstar", "title": "GCStar", "text": "<p>The first free open source application for managing collections I used, it has an old looking desktop interface and is no longer maintained.</p>"}, {"location": "boto3/", "title": "Boto3", "text": "<p>Boto3 is the AWS SDK for Python to create, configure, and manage AWS services, such as Amazon Elastic Compute Cloud (Amazon EC2) and Amazon Simple Storage Service (Amazon S3). The SDK provides an object-oriented API as well as low-level access to AWS services.</p>"}, {"location": "boto3/#installation", "title": "Installation", "text": "<pre><code>pip install boto3\n</code></pre>"}, {"location": "boto3/#usage", "title": "Usage", "text": ""}, {"location": "boto3/#s3", "title": "S3", "text": ""}, {"location": "boto3/#list-the-files-of-a-bucket", "title": "List the files of a bucket", "text": "<pre><code>def list_s3_by_prefix(\n    bucket: str, key_prefix: str, max_results: int = 100\n) -&gt; List[str]:\n    next_token = \"\"\n    all_keys = []\n    while True:\n        if next_token:\n            res = s3.list_objects_v2(\n                Bucket=bucket, ContinuationToken=next_token, Prefix=key_prefix\n            )\n        else:\n            res = s3.list_objects_v2(Bucket=bucket, Prefix=key_prefix)\n\n        if \"Contents\" not in res:\n            break\n\n        if res[\"IsTruncated\"]:\n            next_token = res[\"NextContinuationToken\"]\n        else:\n            next_token = \"\"\n\n        keys = [item[\"Key\"] for item in res[\"Contents\"]]\n\n        all_keys.extend(keys)\n\n        if not next_token:\n            break\n    return all_keys[-1 * max_results :]\n</code></pre> <p>The <code>boto3</code> doesn't have any way to sort the outputs of the bucket, you need to do them once you've loaded all the objects :S.</p>"}, {"location": "boto3/#ec2", "title": "EC2", "text": ""}, {"location": "boto3/#run-ec2-instance", "title": "Run EC2 instance", "text": "<p>Use the <code>run_instances</code> method of the <code>ec2</code> client. Check their docs for the different configuration options. The required ones are <code>MinCount</code> and <code>MaxCount</code>.</p> <pre><code>import boto3\n\nec2 = boto3.client('ec2')\ninstance = ec2.run_instances(MinCount=1, MaxCount=1)\n</code></pre>"}, {"location": "boto3/#get-instance-types", "title": "Get instance types", "text": "<pre><code>from pydantic import BaseModel\nimport boto3\n\nclass InstanceType(BaseModel):\n\"\"\"Define model of the instance type.\n\n    Args:\n        id_: instance type name\n        cpu_vcores: Number of virtual cpus (cores * threads)\n        cpu_speed: Sustained clock speed in Ghz\n        ram: RAM memory in MiB\n        network_performance:\n        price: Hourly cost\n    \"\"\"\n\n    id_: str\n    cpu_vcores: int\n    cpu_speed: Optional[int] = None\n    ram: int\n    network_performance: str\n    price: Optional[float] = None\n\n    @property\n    def cpu(self) -&gt; int:\n\"\"\"Calculate the total Ghz available.\"\"\"\n        if self.cpu_speed is None:\n            return self.cpu_vcores\n        return self.cpu_vcores * self.cpu_speed\n\n\n\ndef get_instance_types() -&gt; InstanceTypes:\n\"\"\"Get the available instance types.\"\"\"\n    log.info(\"Retrieving instance types\")\n    instance_types: InstanceTypes = {}\n    for type_ in _ec2_instance_types(cpu_arch=\"x86_64\"):\n        instance = InstanceType(\n            id_=instance_type,\n            cpu_vcores=type_[\"VCpuInfo\"][\"DefaultVCpus\"],\n            ram=type_[\"MemoryInfo\"][\"SizeInMiB\"],\n            network_performance=type_[\"NetworkInfo\"][\"NetworkPerformance\"],\n            price=_ec2_price(instance_type),\n        )\n\n        with suppress(KeyError):\n            instance.cpu_speed = type_[\"ProcessorInfo\"][\"SustainedClockSpeedInGhz\"]\n\n        instance_types[type_[\"InstanceType\"]] = instance\n\n    return instance_types\n</code></pre>"}, {"location": "boto3/#get-instance-prices", "title": "Get instance prices", "text": "<pre><code>import json\nimport boto3\nfrom pkg_resources import resource_filename\n\ndef _ec2_price(\n    instance_type: str,\n    region_code: str = \"us-east-1\",\n    operating_system: str = \"Linux\",\n    preinstalled_software: str = \"NA\",\n    tenancy: str = \"Shared\",\n    is_byol: bool = False,\n) -&gt; Optional[float]:\n\"\"\"Get the price of an EC2 instance type.\"\"\"\n    log.debug(f\"Retrieving price of {instance_type}\")\n    region_name = _get_region_name(region_code)\n\n    if is_byol:\n        license_model = \"Bring your own license\"\n    else:\n        license_model = \"No License required\"\n\n    if tenancy == \"Host\":\n        capacity_status = \"AllocatedHost\"\n    else:\n        capacity_status = \"Used\"\n\n    filters = [\n        {\"Type\": \"TERM_MATCH\", \"Field\": \"termType\", \"Value\": \"OnDemand\"},\n        {\"Type\": \"TERM_MATCH\", \"Field\": \"capacitystatus\", \"Value\": capacity_status},\n        {\"Type\": \"TERM_MATCH\", \"Field\": \"location\", \"Value\": region_name},\n        {\"Type\": \"TERM_MATCH\", \"Field\": \"instanceType\", \"Value\": instance_type},\n        {\"Type\": \"TERM_MATCH\", \"Field\": \"tenancy\", \"Value\": tenancy},\n        {\"Type\": \"TERM_MATCH\", \"Field\": \"operatingSystem\", \"Value\": operating_system},\n        {\n            \"Type\": \"TERM_MATCH\",\n            \"Field\": \"preInstalledSw\",\n            \"Value\": preinstalled_software,\n        },\n        {\"Type\": \"TERM_MATCH\", \"Field\": \"licenseModel\", \"Value\": license_model},\n    ]\n\n    pricing_client = boto3.client(\"pricing\", region_name=\"us-east-1\")\n    response = pricing_client.get_products(ServiceCode=\"AmazonEC2\", Filters=filters)\n\n    for price in response[\"PriceList\"]:\n        price = json.loads(price)\n\n        for on_demand in price[\"terms\"][\"OnDemand\"].values():\n            for price_dimensions in on_demand[\"priceDimensions\"].values():\n                price_value = price_dimensions[\"pricePerUnit\"][\"USD\"]\n\n        return float(price_value)\n    return None\n\n\ndef _get_region_name(region_code: str) -&gt; str:\n\"\"\"Extract the region name from it's code.\"\"\"\n    endpoint_file = resource_filename(\"botocore\", \"data/endpoints.json\")\n\n    with open(endpoint_file, \"r\", encoding=\"UTF8\") as f:\n        endpoint_data = json.load(f)\n\n    region_name = endpoint_data[\"partitions\"][0][\"regions\"][region_code][\"description\"]\n    return region_name.replace(\"Europe\", \"EU\")\n</code></pre>"}, {"location": "boto3/#type-hints", "title": "Type hints", "text": "<p>AWS library doesn't have working type hints <code>-.-</code>, so you either use <code>Any</code> or dive into the myriad of packages that implement them. I've so far tried boto3_type_annotations, boto3-stubs, and mypy_boto3_builder without success. <code>Any</code> it is for now...</p>"}, {"location": "boto3/#testing", "title": "Testing", "text": "<p>Programs that interact with AWS through <code>boto3</code> create, change or get information on real AWS resources.</p> <p>When developing these programs, you don't want the testing framework to actually do those changes, as it might break things and cost you money. You need to find a way to intercept the calls to AWS and substitute them with the data their API would return. I've found three ways to achieve this:</p> <ul> <li>Manually mocking the <code>boto3</code> methods used by the program with <code>unittest.mock</code>.</li> <li>Using moto.</li> <li>Using Botocore's     Stubber.</li> </ul> <p>TL;DR</p> <p>Try to use moto, using the stubber as fallback option.</p> <p>Using <code>unittest.mock</code> forces you to know what the API is going to return and hardcode it in your tests. If the response changes, you need to update your tests, which is not good.</p> <p>moto is a library that allows you to easily mock out tests based on AWS infrastructure. It works well because it mocks out all calls to AWS automatically without requiring any dependency injection. The downside is that it goes behind <code>boto3</code> so some of the methods you need to test won't be still implemented, that leads us to the third option.</p> <p>Botocore's Stubber is a class that allows you to stub out requests so you don't have to hit an endpoint to write tests. Responses are returned first in, first out. If operations are called out of order, or are called with no remaining queued responses, an error will be raised. It's like the first option but cleaner. If you go down this path, check adamj's post on testing S3.</p>"}, {"location": "boto3/#moto", "title": "moto", "text": "<p>moto's library lets you fictitiously create and change AWS resources as you normally do with the <code>boto3</code> library. They mimic what the real methods do on fake objects.</p> <p>The Docs are awful though.</p>"}, {"location": "boto3/#install", "title": "Install", "text": "<pre><code>pip install moto\n</code></pre>"}, {"location": "boto3/#simple-usage", "title": "Simple usage", "text": "<p>To understand better how it works, I'm going to show you an understandable example, it's not the best way to use it though, go to the usage section for production ready usage.</p> <p>Imagine you have a function that you use to launch new ec2 instances:</p> <pre><code>import boto3\n\n\ndef add_servers(ami_id, count):\n    client = boto3.client('ec2', region_name='us-west-1')\n    client.run_instances(ImageId=ami_id, MinCount=count, MaxCount=count)\n</code></pre> <p>To test it we'd use:</p> <pre><code>from . import add_servers\nfrom moto import mock_ec2\n\n@mock_ec2\ndef test_add_servers():\n    add_servers('ami-1234abcd', 2)\n\n    client = boto3.client('ec2', region_name='us-west-1')\n    instances = client.describe_instances()['Reservations'][0]['Instances']\n    assert len(instances) == 2\n    instance1 = instances[0]\n    assert instance1['ImageId'] == 'ami-1234abcd'\n</code></pre> <p>The decorator <code>@mock_ec2</code> tells <code>moto</code> to capture all <code>boto3</code> calls to AWS. When we run the <code>add_servers</code> function to test, it will create the fake objects on the memory (without contacting AWS servers), and the <code>client.describe_instances</code> <code>boto3</code> method returns the data of that fake data. Isn't it awesome?</p>"}, {"location": "boto3/#usage_1", "title": "Usage", "text": "<p>You can use it with decorators, context managers, directly or with pytest fixtures.</p> <p>Being a pytest fan, the last option looks the cleaner to me.</p> <p>To make sure that you don't change the real infrastructure, ensure that your tests have dummy environmental variables.</p> <p>File: <code>tests/conftest.py</code></p> <pre><code>@pytest.fixture()\ndef _aws_credentials() -&gt; None:\n\"\"\"Mock the AWS Credentials for moto.\"\"\"\n    os.environ[\"AWS_ACCESS_KEY_ID\"] = \"testing\"\n    os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"testing\"\n    os.environ[\"AWS_SECURITY_TOKEN\"] = \"testing\"\n    os.environ[\"AWS_SESSION_TOKEN\"] = \"testing\"\n\n\n@pytest.fixture()\ndef ec2(_aws_credentials: None) -&gt; Any:\n\"\"\"Configure the boto3 EC2 client.\"\"\"\n    with mock_ec2():\n        yield boto3.client(\"ec2\", region_name=\"us-east-1\")\n</code></pre> <p>The <code>ec2</code> fixture can then be used in the tests to setup the environment or assert results.</p>"}, {"location": "boto3/#testing-ec2", "title": "Testing EC2", "text": "<p>If you want to add security groups to the tests, you need to create the resource first.</p> <pre><code>def test_ec2_with_security_groups(ec2: Any) -&gt; None:\n    security_group_id = ec2.create_security_group(\n        GroupName=\"TestSecurityGroup\", Description=\"SG description\"\n    )[\"GroupId\"]\n    instance = ec2.run_instances(\n        ImageId=\"ami-xxxx\",\n        MinCount=1,\n        MaxCount=1,\n        SecurityGroupIds=[security_group_id],\n    )[\"Instances\"][0]\n\n    # Test your code here\n</code></pre> <p>To add tags, use:</p> <pre><code>def test_ec2_with_security_groups(ec2: Any) -&gt; None:\n    instance = ec2.run_instances(\n        ImageId=\"ami-xxxx\",\n        MinCount=1,\n        MaxCount=1,\n        TagSpecifications=[\n            {\n                \"ResourceType\": \"instance\",\n                \"Tags\": [\n                    {\n                        \"Key\": \"Name\",\n                        \"Value\": \"instance name\",\n                    },\n                ],\n            }\n        ],\n    )[\"Instances\"][0]\n\n    # Test your code here\n</code></pre>"}, {"location": "boto3/#testing-rds", "title": "Testing RDS", "text": "<p>Use the <code>rds</code> fixture:</p> <pre><code>from moto import mock_rds2\n\n@pytest.fixture()\ndef rds(_aws_credentials: None) -&gt; Any:\n\"\"\"Configure the boto3 RDS client.\"\"\"\n    with mock_rds2():\n        yield boto3.client(\"rds\", region_name=\"us-east-1\")\n</code></pre> <p>To create an instance use:</p> <pre><code>instance = rds.create_db_instance(\n    DBInstanceIdentifier=\"db-xxxx\",\n    DBInstanceClass=\"db.m3.2xlarge\",\n    Engine=\"postgres\",\n)[\"DBInstance\"]\n</code></pre> <p>It won't have VPC information, if you need it, create the subnet group first (you'll need the <code>ec2</code> fixture too):</p> <pre><code>subnets = [subnet['SubnetId'] for subnet in ec2.describe_subnets()[\"Subnets\"]]\nrds.create_db_subnet_group(DBSubnetGroupName=\"dbsg\", SubnetIds=subnets, DBSubnetGroupDescription=\"Text\")\ninstance = rds.create_db_instance(\n    DBInstanceIdentifier=\"db-xxxx\",\n    DBInstanceClass=\"db.m3.2xlarge\",\n    Engine=\"postgres\",\n    DBSubnetGroupName=\"dbsg\",\n)[\"DBInstance\"]\n</code></pre>"}, {"location": "boto3/#testing-s3", "title": "Testing S3", "text": "<p>Use the <code>s3_mock</code> fixture:</p> <pre><code>from moto import mock_s3\n\n@pytest.fixture()\ndef s3_mock(_aws_credentials: None) -&gt; Any:\n\"\"\"Configure the boto3 S3 client.\"\"\"\n    with mock_s3():\n        yield boto3.client(\"s3\")\n</code></pre> <p>To create an instance use:</p> <pre><code>s3_mock.create_bucket(Bucket=\"mybucket\")\ninstance = s3_mock.list_buckets()[\"Buckets\"][0]\n</code></pre> <p>Check the official docs to check the <code>create_bucket</code> arguments.</p>"}, {"location": "boto3/#testing-route53", "title": "Testing Route53", "text": "<p>Use the <code>route53</code> fixture:</p> <pre><code>from moto import mock_route53\n\n@pytest.fixture(name='route53')\ndef route53_(_aws_credentials: None) -&gt; Any:\n\"\"\"Configure the boto3 Route53 client.\"\"\"\n    with mock_route53():\n        yield boto3.client(\"route53\")\n</code></pre> <p>To create an instance use:</p> <p><pre><code>hosted_zone = route53.create_hosted_zone(\n    Name=\"example.com\", CallerReference=\"Test\"\n)[\"HostedZone\"]\nhosted_zone_id = re.sub(\".hostedzone.\", \"\", hosted_zone[\"Id\"])\nroute53.change_resource_record_sets(\n    ChangeBatch={\n        \"Changes\": [\n            {\n                \"Action\": \"CREATE\",\n                \"ResourceRecordSet\": {\n                    \"Name\": \"example.com\",\n                    \"ResourceRecords\": [\n                        {\n                            \"Value\": \"192.0.2.44\",\n                        },\n                    ],\n                    \"TTL\": 60,\n                    \"Type\": \"A\",\n                },\n            },\n        ],\n        \"Comment\": \"Web server for example.com\",\n    },\n    HostedZoneId=hosted_zone_id,\n)\n</code></pre> You need to first create a hosted zone. The <code>change_resource_record_sets</code> order to create the instance doesn't return any data, so if you need to work on it, use the <code>list_resource_record_sets</code> method of the route53 client (you'll need to set the <code>HostedZoneId</code> argument). If you have more than 300 records, the endpoint gives you a paginated response, so if the <code>IsTruncated</code> attribute is <code>True</code>, you need to call the method again setting the <code>StartRecordName</code> and <code>StartRecordType</code> to the <code>NextRecordName</code> and <code>NextRecordType</code> response arguments. Not nice at all.</p> <p>Pagination is not yet supported by moto, so you won't be able to test that part of your code.</p> <p>Check the official docs to check the method arguments:</p> <ul> <li><code>create_hosted_zone</code>.</li> <li><code>change_resource_record_sets</code>.</li> </ul>"}, {"location": "boto3/#test-vpc", "title": "Test VPC", "text": "<p>Use the <code>ec2</code> fixture defined in the usage section.</p> <p>To create an instance use:</p> <pre><code>instance = ec2.create_vpc(\n    CidrBlock=\"172.16.0.0/16\",\n)[\"Vpc\"]\n</code></pre> <p>Check the official docs to check the method arguments:</p> <ul> <li><code>create_vpc</code>.</li> <li><code>create_subnet</code>.</li> </ul>"}, {"location": "boto3/#testing-autoscaling-groups", "title": "Testing autoscaling groups", "text": "<p>Use the <code>autoscaling</code> fixture:</p> <pre><code>from moto import mock_autoscaling\n\n@pytest.fixture(name='autoscaling')\ndef autoscaling_(_aws_credentials: None) -&gt; Any:\n\"\"\"Configure the boto3 Autoscaling Group client.\"\"\"\n    with mock_autoscaling():\n        yield boto3.client(\"autoscaling\")\n</code></pre> <p>They don't yet support LaunchTemplates, so you'll have to use LaunchConfigurations. To create an instance use:</p> <pre><code>autoscaling.create_launch_configuration(LaunchConfigurationName='LaunchConfiguration', ImageId='ami-xxxx', InstanceType='t2.medium')\nautoscaling.create_auto_scaling_group(AutoScalingGroupName='ASG name', MinSize=1, MaxSize=3, LaunchConfigurationName='LaunchConfiguration', AvailabilityZones=['us-east-1a'])\ninstance = autoscaling.describe_auto_scaling_groups()[\"AutoScalingGroups\"][0]\n</code></pre> <p>Check the official docs to check the method arguments:</p> <ul> <li><code>create_auto_scaling_group</code>.</li> <li><code>create_launch_configuration</code>.</li> <li><code>describe_auto_scaling_groups</code>.</li> </ul>"}, {"location": "boto3/#test-security-groups", "title": "Test Security Groups", "text": "<p>Use the <code>ec2</code> fixture defined in the usage section.</p> <p>To create an instance use:</p> <pre><code>instance_id = ec2.create_security_group(\n    GroupName=\"TestSecurityGroup\", Description=\"SG description\"\n)[\"GroupId\"]\ninstance = ec2.describe_security_groups(GroupIds=[instance_id])\n</code></pre> <p>To add permissions to the security group you need to use the <code>authorize_security_group_ingress</code> and <code>authorize_security_group_egress</code> methods.</p> <pre><code>ec2.authorize_security_group_ingress(\n    GroupId=instance_id,\n    IpPermissions=[\n        {\n            \"IpProtocol\": \"tcp\",\n            \"FromPort\": 80,\n            \"ToPort\": 80,\n            \"IpRanges\": [{\"CidrIp\": \"0.0.0.0/0\"}],\n        },\n    ],\n)\n</code></pre> <p>By default, the created security group comes with an egress rule to allow all traffic. To remove rules use the <code>revoke_security_group_egress</code> and <code>revoke_security_group_ingress</code> methods.</p> <pre><code>ec2.revoke_security_group_egress(\n    GroupId=instance_id,\n    IpPermissions=[\n        {\"IpProtocol\": \"-1\", \"IpRanges\": [{\"CidrIp\": \"0.0.0.0/0\"}]},\n    ],\n)\n</code></pre> <p>Check the official docs to check the method arguments:</p> <ul> <li><code>create_security_group</code>.</li> <li><code>describe_security_group</code>.</li> <li><code>authorize_security_group_ingress</code>.</li> <li><code>authorize_security_group_egress</code>.</li> <li><code>revoke_security_group_ingress</code>.</li> <li><code>revoke_security_group_egress</code>.</li> </ul>"}, {"location": "boto3/#test-iam-users", "title": "Test IAM users", "text": "<p>Use the <code>iam</code> fixture:</p> <pre><code>from moto import mock_iam\n\n@pytest.fixture(name='iam')\ndef iam_(_aws_credentials: None) -&gt; Any:\n\"\"\"Configure the boto3 IAM client.\"\"\"\n    with mock_iam():\n        yield boto3.client(\"iam\")\n</code></pre> <p>To create an instance use:</p> <pre><code>instance = iam.create_user(UserName=\"User\")[\"User\"]\n</code></pre> <p>Check the official docs to check the method arguments:</p> <ul> <li><code>create_user</code></li> <li><code>list_users</code></li> </ul>"}, {"location": "boto3/#test-iam-groups", "title": "Test IAM Groups", "text": "<p>Use the <code>iam</code> fixture defined in the test IAM users section:</p> <p>To create an instance use:</p> <pre><code>user = iam.create_user(UserName=\"User\")[\"User\"]\ninstance = iam.create_group(GroupName=\"UserGroup\")[\"Group\"]\niam.add_user_to_group(GroupName=instance[\"GroupName\"], UserName=user[\"UserName\"])\n</code></pre> <p>Check the official docs to check the method arguments:</p> <ul> <li><code>create_group</code></li> <li><code>add_user_to_group</code></li> </ul>"}, {"location": "boto3/#issues", "title": "Issues", "text": "<ul> <li>Support LaunchTemplates: Once     they are, test clinv autoscaling group     adapter support for launch templates.</li> <li>Support Route53 pagination: test     clinv route53 update and update the test route53 section.</li> <li><code>cn-north-1</code> rds and autoscaling     errors: increase the timeout of     clinv, and test if the coverage has changed.</li> </ul>"}, {"location": "boto3/#references", "title": "References", "text": "<ul> <li>Git</li> <li>Docs</li> </ul>"}, {"location": "calendar_management/", "title": "Calendar Management", "text": "<p>Since the break of my taskwarrior instance I've used a physical calendar to manage the tasks that have a specific date. </p> <p>The next factors made me search for a temporal solution:</p> <ul> <li>It's taking longer than expected.</li> <li>I've started using a nextcloud calendar with some friends.</li> <li>I frequently use Google calendar at work.</li> <li>I'm sick of having to log in Nexcloud and Google to get the day's     appointments.</li> </ul> <p>To fulfill my needs the solution needs to:</p> <ul> <li>Import calendar events from different sources, basically through     the CalDAV protocol.</li> <li>Have a usable terminal user interface</li> <li>Optionally have a command line interface or python library so it's easy to make scripts.</li> <li>Optionally it can be based in python so it's easy to contribute</li> <li>Support having a personal calendar mixed with the shared ones.</li> <li>Show all calendars in the same interface</li> </ul>"}, {"location": "calendar_management/#khal", "title": "Khal", "text": "<p>Looking at the available programs I found <code>khal</code>, which looks like it may be up to the task.</p> <p>Go through the installation steps and configure the instance to have a local calendar.</p> <p>If you want to sync your calendar events through CalDAV, you need to set vdirsyncer.</p>"}, {"location": "calendar_versioning/", "title": "Calendar Versioning", "text": "<p>Calendar Versioning is a versioning convention based on your project's release calendar, instead of arbitrary numbers.</p> <p>CalVer suggests version number to be in format of: <code>YEAR.MONTH.sequence</code>. For example, <code>20.1</code> indicates a release in 2020 January, while <code>20.5.2</code> indicates a release that occurred in 2020 May, while the <code>2</code> indicates this is the third release of the month.</p> <p>You can see it looks similar to semantic versioning and has the benefit that a later release qualifies as bigger than an earlier one within the semantic versioning world (which mandates that a version number must grow monotonically). This makes it easy to use in all places where semantic versioning can be used.</p> <p>The idea here is that if the only maintained version is the latest, then we might as well use the version number to indicate the release date to signify just how old of a version you\u2019re using. You also have the added benefit that you can make calendar-based promises. For example, Ubuntu offers five years of support, therefore given version <code>20.04</code> you can quickly determine that it will be supported up to April 2025.</p>"}, {"location": "calendar_versioning/#when-to-use-calver", "title": "When to use CalVer", "text": "<p>Check the Deciding what version system to use for your programs article section.</p>"}, {"location": "calendar_versioning/#references", "title": "References", "text": "<ul> <li>Home</li> </ul>"}, {"location": "changelog/", "title": "Changelog", "text": "<p>A changelog is a file which contains a curated, chronologically ordered list of notable changes for each version of a project.</p> <p>It's purpose is to make it easier for users and contributors to see precisely what notable changes have been made between each release (or version) of the project.</p>"}, {"location": "changelog/#types-of-changes", "title": "Types of changes", "text": "<ul> <li>Added for new features.</li> <li>Changed for changes in existing functionality.</li> <li>Deprecated for soon-to-be removed features.</li> <li>Removed for now removed features.</li> <li>Fixed for any bug fixes.</li> <li>Security in case of vulnerabilities.</li> </ul>"}, {"location": "changelog/#changelog-guidelines", "title": "Changelog Guidelines", "text": "<p>Good changelogs follow the next principles:</p> <ul> <li>Changelogs are for humans, not machines.</li> <li>There should be an entry for every single version.</li> <li>The same types of changes should be grouped.</li> <li>Versions and sections should be linkable.</li> <li>The latest version comes first.</li> <li>The release date of each version is displayed.</li> <li>Mention your versioning strategy.</li> <li>Call it     <code>CHANGELOG.md</code>.</li> </ul> <p>Some examples of bad changelogs are:</p> <ul> <li> <p>Commit log diffs: The purpose of a changelog entry is to document the     noteworthy difference, often across multiple commits, to communicate them     clearly to end users. If someone wants to see the commit log diffs they can     access it through the <code>git</code> command.</p> </li> <li> <p>Ignoring Deprecations: When people upgrade from one version to another, it     should be painfully clear when something will break. It should be possible     to upgrade to a version that lists deprecations, remove what's deprecated,     then upgrade to the version where the deprecations become removals.</p> </li> <li> <p>Confusing Dates: Regional date formats vary throughout the world and it's     often difficult to find a human-friendly date format that feels intuitive to     everyone. The advantage of dates formatted like 2017-07-17 is that they     follow the order of largest to smallest units: year, month, and day. This     format also doesn't overlap in ambiguous ways with other date formats,     unlike some regional formats that switch the position of month and day     numbers. These reasons, and the fact this date format is an ISO standard,     are why it is the recommended date format for changelog entries.</p> </li> </ul>"}, {"location": "changelog/#how-to-reduce-the-effort-required-to-maintain-a-changelog", "title": "How to reduce the effort required to maintain a changelog", "text": "<p>There are two ways to ease the burden of maintaining a changelog:</p>"}, {"location": "changelog/#build-it-automatically", "title": "Build it automatically", "text": "<p>If you use Semantic Versioning you can use the commitizen tool to automatically generate the changelog each time you cut a new release by running <code>cz bump --changelog --no-verify</code>.</p> <p>The <code>--no-verify</code> part is required if you use pre-commit hooks.</p>"}, {"location": "changelog/#use-the-unreleased-section", "title": "Use the <code>Unreleased</code> section", "text": "<p>Keep an Unreleased section at the top to track upcoming changes.</p> <p>This serves two purposes:</p> <ul> <li>People can see what changes they might expect in upcoming releases.</li> <li>At release time, you can move the Unreleased section changes into a new     release version section.</li> </ul>"}, {"location": "changelog/#references", "title": "References", "text": "<ul> <li>Keep a Changelog</li> </ul>"}, {"location": "chezmoi/", "title": "Chezmoi", "text": "<p>Chezmoi stores the desired state of your dotfiles in the directory <code>~/.local/share/chezmoi</code>. When you run <code>chezmoi apply</code>, <code>chezmoi</code> calculates the desired contents for each of your dotfiles and then makes the minimum changes required to make your dotfiles match your desired state.</p> <p>What I like:</p> <ul> <li>Supports <code>pass</code> to retrieve credentials.</li> <li>Popular</li> <li>Can remove directories on <code>apply</code></li> <li>It has a <code>diff</code></li> <li>It can include dotfiles from an URL</li> <li>Encrypt files with gpg</li> <li>There's a vim plugin</li> <li>Actively maintained</li> <li>Good documentation</li> </ul> <p>What I don't like:</p> <ul> <li>Go templates, although   it supports autotemplating   and it's   well explained</li> <li>Written in Go</li> </ul>"}, {"location": "chezmoi/#installation", "title": "Installation", "text": "<p>I've added some useful aliases:</p> <pre><code>alias ce='chezmoi edit'\nalias ca='chezmoi add'\nalias cdiff='chezmoi diff'\nalias cdata='chezmoi edit-config'\nalias capply='chezmoi apply'\nalias cexternal='nvim ~/.local/share/chezmoi/.chezmoiexternal.yaml'\n</code></pre>"}, {"location": "chezmoi/#basic-usage", "title": "Basic Usage", "text": "<p>Assuming that you have already installed <code>chezmoi</code>, initialize <code>chezmoi</code> with:</p> <pre><code>$ chezmoi init\n</code></pre> <p>This will create a new git local repository in <code>~/.local/share/chezmoi</code> where <code>chezmoi</code> will store its source state. By default, <code>chezmoi</code> only modifies files in the working copy.</p> <p>Manage your first file with chezmoi:</p> <pre><code>$ chezmoi add ~/.bashrc\n</code></pre> <p>This will copy <code>~/.bashrc</code> to <code>~/.local/share/chezmoi/dot_bashrc</code>.</p> <p>Edit the source state:</p> <pre><code>$ chezmoi edit ~/.bashrc\n</code></pre> <p>This will open <code>~/.local/share/chezmoi/dot_bashrc</code> in your <code>$EDITOR</code>. Make some changes and save the file.</p> <p>See what changes chezmoi would make:</p> <pre><code>$ chezmoi diff\n</code></pre> <p>Apply the changes:</p> <pre><code>$ chezmoi -v apply\n</code></pre> <p>Sometimes the <code>diff</code> is too big and you need to work with it chuck by chunk. For each change you can either:</p> <ul> <li><code>chezmoi add &lt;target&gt;</code> if you want to keep the changes you've manually made to the files that match the <code>&lt;target&gt;</code>.</li> <li><code>chezmoi apply &lt;target&gt;</code> if you want to apply the changes that chezmoi proposes for the <code>&lt;target&gt;</code>.</li> </ul> <p>Here <code>&lt;target&gt;</code> is any directory or file listed in the <code>diff</code>.</p> <p>All <code>chezmoi</code> commands accept the <code>-v</code> (verbose) flag to print out exactly what changes they will make to the file system, and the <code>-n</code> (dry run) flag to not make any actual changes. The combination <code>-n -v</code> is very useful if you want to see exactly what changes would be made.</p> <p>Next, open a shell in the source directory, to commit your changes:</p> <pre><code>$ chezmoi cd\n$ git add .\n$ git commit -m \"Initial commit\"\n</code></pre> <p>Create a new repository on your desired git server called <code>dotfiles</code> and then push your repo:</p> <pre><code>$ git remote add origin https://your_git_server.com/$GIT_USERNAME/dotfiles.git\n$ git branch -M main\n$ git push -u origin main\n</code></pre> <p>Hint: <code>chezmoi</code> can be configured to automatically add, commit, and push changes to your repo.</p> <p>Finally, exit the shell in the source directory to return to where you were:</p> <pre><code>$ exit\n</code></pre>"}, {"location": "chezmoi/#install-a-binary-from-an-external-url", "title": "Install a binary from an external url", "text": "<p>Sometimes you may want to install some binaries from external urls, for example <code>velero</code> a backup tool for kubernetes. And you may want to be able to define what version you want to have and be able to update it at will. </p> <p>To do that we can define the version in the configuration with <code>chezmoi edit-config</code></p> <pre><code>data:\nvelero_version: 1.9.5\n</code></pre> <p>All the variables you define under the <code>data</code> field are globally available on all your templates.</p> <p>Then we can set the external configuration of chezmoi by editing the file <code>~/.config/chezmoi/.chezmoiexternal.yaml</code> and add the next snippet:</p> <pre><code>.local/bin/velero:\ntype: \"file\"\nurl: https://github.com/vmware-tanzu/velero/releases/download/v{{ .velero_version }}/velero-v{{ .velero_version }}-{{ .chezmoi.os }}-{{ .chezmoi.arch }}.tar.gz\nexecutable: true\nrefreshPeriod: 168h\nfilter:\ncommand: tar\nargs:\n- --extract\n- --file\n- /dev/stdin\n- --gzip\n- --to-stdout\n- velero-v{{ .velero_version }}-{{ .chezmoi.os }}-{{ .chezmoi.arch }}/velero\n</code></pre> <p>This will download the binary of version <code>1.9.5</code> from the source, unpack it and extract the <code>velero</code> binary and save it to <code>~/.local/bin/velero</code>.</p>"}, {"location": "chezmoi/#references", "title": "References", "text": "<ul> <li>Home</li> </ul>"}, {"location": "chromium/", "title": "Chromium", "text": "<pre><code>apt-get install chromium\n</code></pre>"}, {"location": "code_learning/", "title": "Learning to code", "text": "<p>Learning to code is a never ending, rewarding, frustrating, enlightening task. In this article you can see what is the generic roadmap (in my personal opinion) of a developer. As each of us is different, probably a generic roadmap won't suit your needs perfectly, if you are new to coding, I suggest you find a mentor so you can both tweak it to your case.</p>"}, {"location": "code_learning/#learning-methods", "title": "Learning methods", "text": "<p>Not all of us like to learn in the same way, first you need to choose how do you want to learn, for example through:</p> <ul> <li>Attendance-based courses.</li> <li>Online courses.</li> <li>Video courses.</li> <li>Reading books.</li> </ul> <p>Whichever you choose make sure you have regular feedback from other humans such as:</p> <ul> <li>Mentors.</li> <li>Learning communities.</li> <li>Friends.</li> </ul>"}, {"location": "code_learning/#roadmap", "title": "Roadmap", "text": "<p>The roadmap is divided in these main phases:</p> <ul> <li> <p>Beginner: You start from scratch and need to get the basic     knowledge, skills and tool set to set the learning ball rolling. At the end     of this phase you will be able to develop simple pieces of code and     collaborate with other projects with the help of a mentor.</p> </li> <li> <p>Junior: In this phase you'll learn how to:</p> <ul> <li>Improve the quality of your code through the use of testing,     documentation, linters, fixers and other techniques.</li> <li>Be more proficient with your development environment.</li> </ul> <p>At the end of the phase you'll become a senior developer that's able to code autonomously by:</p> <ul> <li>Creating a whole project from start to finish.</li> <li>Contribute to other open source projects by yourself.</li> </ul> </li> <li> <p>Senior: In this never ending phase you     keep on improving your development skills, knowledge and tools.</p> </li> </ul> <p>Don't try to rush, this is a lifetime roadmap, depending on how much time you put into learning the first steps may take from months to one or two years, the refinement phase from 2 to 8 years, and the enhancement phase never ends.</p>"}, {"location": "code_learning/#beginner", "title": "Beginner", "text": "<p>First steps are hard, you're entering a whole new world that mostly looks like magic to you. Probably you'll feel overwhelmed by the path ahead but don't fret, as every path, it's doable one step at a time.</p> <p>First steps are also exciting so try to channel all that energy into the learning process to overcome the obstacles you find in your way.</p> <p>In this section you'll learn how to start walking in the development world by:</p> <ul> <li>Setting up the development environment.</li> <li>Learning the basics</li> </ul>"}, {"location": "code_learning/#setup-your-development-environment", "title": "Setup your development environment", "text": ""}, {"location": "code_learning/#editor", "title": "Editor", "text": "<p>TBD</p>"}, {"location": "code_learning/#git", "title": "Git", "text": "<p>Git is a software for tracking changes in any set of files, usually used for coordinating work among programmers collaboratively developing source code during software development. Its goals include speed, data integrity, and support for distributed, non-linear workflows (thousands of parallel branches running on different systems).</p> <p>Git is a tough nut to crack, no matter how experience you are you'll frequently get surprised. Sadly it's one of the main tools to develop your code, so you must master it as soon as possible.</p> <p>I've listed you some resources here on how to start. From that article I think it's also interesting that you read about:</p> <ul> <li>Pull Request process</li> <li>Git workflow</li> </ul>"}, {"location": "code_learning/#language-specific-environment", "title": "Language specific environment", "text": ""}, {"location": "code_learning/#learn-the-basics", "title": "Learn the basics", "text": "<p>Now it's the time to study, choose your desired learning method and follow them until you get the basics of the type of developer you want to become, for example:</p> <ul> <li>Frontend developer.</li> </ul> <p>In parallel it's crucial to learn Git as soon as you can, it's the main tool to collaborate with other developers and your safety net in the development workflow.</p>"}, {"location": "code_learning/#searching-for-information", "title": "Searching for information", "text": "<ul> <li>Search engines</li> <li>Github</li> </ul>"}, {"location": "code_learning/#junior", "title": "Junior", "text": "<p>TBD</p>"}, {"location": "code_learning/#senior", "title": "Senior", "text": "<p>TBD</p>"}, {"location": "cone/", "title": "Cone", "text": "<p>Cone is a mobile ledger application compatible with beancount. I use it as part of my accounting automation workflow.</p>"}, {"location": "cone/#installation", "title": "Installation", "text": "<ul> <li>Download the application from F-droid.</li> <li>It assumes that you have a txt file to store the     information. As it doesn't yet     support the edition or deletion of     transactions, I suggest you     create the <code>ledger.txt</code> file with your favorite mobile editor such as     Markor.</li> <li>Open the application and load the <code>ledger.txt</code> file.</li> </ul>"}, {"location": "cone/#usage", "title": "Usage", "text": "<p>To be compliant with my beancount ledger:</p> <ul> <li>I've initialized the <code>ledger.txt</code> file with the <code>open</code> statements of the     beancount accounts, so the transaction UI autocompletes them.</li> <li>Cone doesn't still support the beancount     format by default, so in the     description of the transaction I also introduce the payee. For example:     <code>* \"payee1\" \"Bought X</code> instead of just <code>Bought X</code>.</li> </ul> <p>If I need to edit or delete a transaction, I change it with the Markor editor.</p> <p>To send the ledger file to the computer, I use either Share via HTTP or Termux through ssh.</p>"}, {"location": "cone/#references", "title": "References", "text": "<ul> <li>Git</li> <li>Docs</li> </ul>"}, {"location": "configuration_management/", "title": "Configuration management", "text": "<p>Configuring your devices is boring, disgusting and complex. Specially when your device dies and you need to reinstall. You usually don't have the time or energy to deal with it, you just want it to work.</p> <p>To have a system that allows you to recover from a disaster it's expensive in both time and knowledge, and many people have different solutions.</p> <p>This article shows the latest step of how I'm doing it.</p>"}, {"location": "configuration_management/#linux-devices", "title": "Linux devices", "text": ""}, {"location": "configuration_management/#operating-system-installation", "title": "Operating System installation", "text": "<p>I don't use PEX or anything, just burn the ISO in an USB and manually install it.</p>"}, {"location": "configuration_management/#main-skeleton", "title": "Main skeleton", "text": "<p>I use the same Ansible playbook to configure all my devices. Using a monolith playbook is usually not a good idea in enterprise environments, you'd better use a <code>cruft</code> template and a playbook for each server. At your home you'd probably have a limited number of devices and maintaining many playbooks and a cruft template is not worth your time. Ansible is flexible enough to let you manage the differences of the devices while letting you reuse common parts.</p> <p>The playbook configures the skeleton of the device namely:</p> <ul> <li>Configure the disks with OpenZFS.</li> <li>Install the packages.</li> <li>Configure the firewall.</li> </ul> <p>In that playbook I use roles in these ways:</p> <ul> <li>The ones that can be cloned without configuration from a clean environment are   defined in the <code>requirements.yml</code> file and installed with <code>ansible-galaxy</code>.</li> <li>The ones that can't be cloned but are developed by other people are cloned   using git submodules in the <code>roles</code> directory.</li> <li>The ones I make live in the <code>roles</code> directory of the playbook and don't have   tests. Again, managing your own roles this way is a bad idea in enterprise   environments, you should use <code>cruft</code>, a separate repository for each role,   test them with <code>molecule</code> and version it in the <code>requirements.yml</code> of the   playbook pinning it to a specific version. This of course is an overkill for a   home deployment. You'd spend much more time writing the tests and debugging   issues than using the <code>roles</code> directory directly.</li> </ul>"}, {"location": "configuration_management/#home-configuration", "title": "Home configuration", "text": ""}, {"location": "contact/", "title": "Contact", "text": "<p>I'm available through:</p> <ul> <li>Email: <code>lyz</code> at <code>riseup.net</code></li> </ul> PGP Key: <code>6ADA882386CDF9BD1884534C6C7D7C1612CDE02F</code> <pre><code>-----BEGIN PGP PUBLIC KEY BLOCK-----\n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\n=QNxk\n-----END PGP PUBLIC KEY BLOCK-----\n</code></pre> <ul> <li>XMPP at <code>lyz</code> at <code>disroot.org</code></li> <li>Through Github by opening an issue.</li> </ul>"}, {"location": "cooking/", "title": "Cooking", "text": "<p>Cooking as defined in Wikipedia, is the art, science, and craft of using heat to prepare food for consumption. It sounds like an enlightening experience that brings you joy. Reality then slaps you in the face yet once again. It's very different to cook because you want to, than cooking because you need to. I love to eat, but have always hated to cook, mainly because I've always seen cooking with that second view.</p> <p>Being something disgusting that I had to do, pushed me to batch cook once a week as quickly as possible, and to buy prepared food from the local squat center's tavern. Now I aim to shift my point of view to enjoy the time invested preparing food. Two are the main reasons: I'm going to spend a great amount of my life in front of the stove, so I'd better enjoy it, and probably the end result would be better for my well being.</p> <p>One way that can help me with the switch, is to understand the science behind it and be precise with the process. Thus this section was born, I'll start with the very basics and build from there on.</p>"}, {"location": "cooking_basics/", "title": "Cooking Basics", "text": "<p>All great recipes are based on the same basic principles and processes, these are the cooking snippets that I've needed.</p>"}, {"location": "cooking_basics/#boiling-an-egg", "title": "Boiling an egg", "text": "<p>Cooking an egg well is a matter of time.</p> <ul> <li>Put enough water in the pot so that the eggs are completely covered.</li> <li>Add a pinch of salt and a dash of vinegar.</li> <li>Let the water boil. Use a kettle to heat it if you have one.</li> <li>Add the eggs.</li> <li>Depending on the type of egg you want, you need to wait more or less time:<ul> <li>5-6 minutes: You'll get soft boiled eggs, whose yolk is liquid and the     white is semi-liquid.</li> <li>7 minutes: mollet egs, with semi-liquid yolk and curdled white.</li> <li>10-12 minutes: boiled eggs, compact white and curdled yolk.</li> </ul> </li> <li>Pour off the hot water, shake it gently to crack the eggs, and add cold water,     with even a few ice cubes.</li> <li>Wait 5 minutes if you want to serve them warm, or 15 otherwise, and then peel     them under the same water.</li> </ul> <p>Here are some tips to improve your chances to get the perfect egg:</p> <ul> <li>Use fresh eggs, when they've been in the fridge for a while, they get     dehydrated and the air that's inside gets expanded. That's why, when you put     an egg into a glass of water, if it doesn't stay at the bottom you'd better     not use it.</li> <li>Take them out of the fridge an hour before cooking them. (Yeah, like you're     going to remember to do it :P).</li> </ul>"}, {"location": "cooking_basics/#cooking-legumes", "title": "Cooking legumes", "text": "<p>Legumes are wonderful, to squeeze their value remember to:</p> <ul> <li>Don't use old ones: If you're legumes are older than a year, they can be     old. They loose water with the time up to a point that they can be     impossible to cook.</li> <li> <p>Soak them: Some legumes like the lentils don't need to be soaked, but for most     of them it's better to be hydrated before putting them in the pot.</p> <p>For chickpeas and beans, the best is to soak them for 10 to 12 hours. They'll drink the water, so add it until you double the volume of the legumes. Once done, discard that water, rinse them and use new one for the pot. That way you'll prevent the acids and oligosaccharides that promotes a heavy digestion. * Don't scare them: Don't cut the cooking with cold water, they won't help you avoid farting and for some great chefs it's one of the worst errors you can do, at least with the chickpeas. Remember always to have enough water from the start to avoid this situation. * Know when to add them in the pot: chickpeas need to be added when the water is already boiling</p> </li> </ul> <p>When you're using boiled legumes in your recipes, be careful, after the hydration, they weight the double!</p>"}, {"location": "cooking_basics/#boil-chickpeas-when-youve-forgotten-to-soak-them", "title": "Boil chickpeas when you've forgotten to soak them", "text": "<p>Soaked chickpeas take one or two hours to cook in a normal pot, 20 to 30 in a fast pot, which saves a ton of energy.</p> <p>If you forgot to soak them, add a level teaspoon of baking soda to the pot and cook them as usual. When not using a fast pot, you'll need to periodically remove the foam that will be created. The problem with this method is that you don't discard the first water round, and they can be more indigestible. Another option is to cook them for an hour, change the water and then cook them again.</p>"}, {"location": "cooking_software/", "title": "Cooking software", "text": "<p>While using grocy to manage my recipes I've found what I want from a recipe manager:</p> <ul> <li>Write recipes in plaintext files.</li> <li>Import recipes from urls.</li> <li>Import other recipes in a recipe. To avoid code repetition. For example if     I want to do raviolis carbonara, I want to have a basic recipe to create raviolis     and another for carbonara, then a normal recipe that imports both.</li> <li>Define processes that can be imported as steps in recipes, for example boil     water</li> <li>Change the number the servings</li> <li>Annotate lessons learned</li> <li>Translate common cooking units (spoonful, a piece) into metric system units of     volume and weight.</li> <li>Helper to follow the recipe while cooking</li> <li>Keep track of the evolution of the recipe each time you cook it with     deviations from plan, evaluation of the result dish, lessons learned and     changes made to the recipe.</li> <li>Attach image to the recipe</li> <li>Specify the storage size (a ration of lentils uses 2cm of a tupper).</li> <li>Give rating to recipes</li> <li>Track the number of times you do a recipe</li> <li>Grade the maturity level of a recipe by the number of times done and the     number of changes.</li> <li>Browse all available recipes with the possibility of:<ul> <li>Search by name</li> <li>Search by ingredient</li> <li>Filter by season</li> <li>Filter by meal type (lunch, dinner, dessert...)</li> <li>Sort by rating</li> <li>Sort by number of times cooked</li> <li>Sort by last time cooked</li> <li>Show never cooked recipes</li> </ul> </li> <li>Do a meal plan</li> <li>Create reusable meal plans</li> <li>Be able to talk to the inventory management tool to:<ul> <li>See if there are enough ingredients.</li> <li>Fill up the shopping list.</li> </ul> </li> <li>Select which recipes I want to cook this month, the program should show me the     available ones taking into account the season.</li> <li>Keep track of the season of the ingredients, so you can mark an ingredient as     in season or out of season, and that will allow you to select which recipes     to add, or tell you which recipes are no longer going to be available.</li> <li>Define variations of a recipe, imagine that instead of red pepper you have     green</li> <li>Be able to tell the brand of an ingredient</li> <li>Define the cooking tools to use, which will be shown when preparing a recipe</li> <li>Define the preparing and clean steps of a cooking tool.</li> <li>Show the number of inactivity interruptions, their time and the total amount     of inactive time.</li> <li>Calculate the recipe time from the times of the processes involved and     subsequent recipes</li> <li>Be able to define where each process is carried out, what is the distance     between the places</li> <li>Suggest optimizations in the cooking process:<ul> <li>Reorder of steps to reduce waiting time and movement</li> </ul> </li> <li>Be able to start from the basic features and incrementally add on it</li> </ul>"}, {"location": "cooking_software/#software-review", "title": "Software review", "text": ""}, {"location": "cooking_software/#grocy", "title": "Grocy", "text": "<p>Grocy is an awesome software to manage your inventory, it's also a good one to manage your recipes, with an awesome integration with the inventory management. In fact, I've been using it for some years.</p> <p>Nevertheless, you can't:</p> <ul> <li>Write them in plaintext.</li> <li>Reuse recipes with other recipes in a pleasant way.</li> <li>Do variations of a recipe.</li> <li>Track the variations of a recipe.</li> </ul> <p>So I think whatever I choose to manage recipes needs to be able to speak to a Grocy instance at least to get an idea of the stock available and to complete the shopping list.</p>"}, {"location": "cooking_software/#cooklang", "title": "Cooklang", "text": "<p><code>Cooklang</code> looks real good, you write the recipes in plaintext but the syntax is not as smooth as I'd like:</p> <pre><code>Then add @salt and @ground black pepper{} to taste.\n</code></pre> <p>I'd like the parser to be able to detect the ingredient <code>salt</code> without the need of the <code>@</code>, and I don't either like how to specify the measurements:</p> <pre><code>Place @bacon strips{1%kg} on a baking sheet and glaze with @syrup{1/2%tbsp}.\n</code></pre> <p>On the other side there is more less popular:</p> <ul> <li>Their spec has their spec 400 stars.</li> <li>It has a cli, and an Android and ios apps</li> <li>There is a vim plugin for the     syntax.</li> <li>You can import recipes from urls.</li> <li>There are some recipe     books, some of     them     look nice with a <code>mkdocs</code> frontend.</li> </ul> <p>It could be used as part of the system, but it falls short in many of my desired features.</p>"}, {"location": "cooking_software/#kookbook", "title": "KookBook", "text": "<p><code>KookBook</code> is KDE solution for plaintext recipe management. Their documentation is sparse and not popular at all. I don't feel like using it.</p>"}, {"location": "cooking_software/#recipesage", "title": "RecipeSage", "text": "<p>RecipeSage is free personal recipe keeper, meal planner, and shopping list manager for Web, IOS, and Android.</p> <p>Quickly capture and save recipes from any website simply by entering the website URL. Sync your recipes, meal plans, and shopping lists between all of your devices. Share your recipes, shopping lists, and meal plans with family and friends.</p> <p>It looks good, but I'd use <code>grocy</code> instead.</p>"}, {"location": "cooking_software/#mealie", "title": "Mealie", "text": "<p>Mealie is a self hosted recipe manager and meal planner with a RestAPI backend and a reactive frontend application built in Vue for a pleasant user experience for the whole family. Easily add recipes into your database by providing the url and mealie will automatically import the relevant data or add a family recipe with the UI editor.</p> <p>It does have an API, but it looks too complex. Maybe to be used as a backend to retrieve recipes from the internet, but no plaintext recipes.</p>"}, {"location": "cooking_software/#chowdown", "title": "Chowdown", "text": "<p><code>Chowdown</code> is a simple, plaintext recipe database for hackers. It has nice features:</p> <ul> <li>You write your recipes in Markdown.</li> <li>You can easily import recipes in other recipes.</li> </ul> <p>An example would be:</p> <pre><code>---\n\nlayout: recipe\ntitle:  \"Broccoli Beer Cheese Soup\"\nimage: broccoli-beer-cheese-soup.jpg\ntags: sides, soups\n\ningredients:\n- 4 tablespoons butter\n- 1 cup diced onion\n- 1/2 cup shredded carrot\n- 1/2 cup diced celery\n- 1 tablespoon garlic\n- 1/4 cup flour\n- 1 quart chicken broth\n- 1 cup heavy cream\n- 10 ounces muenster cheese\n- 1 cup white white wine\n- 1 cup pale beer\n- 1 teaspoon Worcestershire sauce\n- 1/2 teaspoon hot sauce\n\ndirections:\n- Start with butter, onions, carrots, celery, garlic until cooked down\n- Add flour, stir well, cook for 4-5 mins\n- Add chicken broth, bring to a boil\n- Add wine and reduce to a simmer\n- Add cream, cheese, Worcestershire, and hot sauce\n- Serve with croutons\n\n---\n\nThis recipe is inspired by one of my favorites, Gourmand's Beer Cheese Soup, which uses Shiner Bock. Feel free to use whatever you want, then go to [Gourmand's](http://lovethysandwich.com) to have the real thing.\n</code></pre> <p>Or using other recipes:</p> <pre><code>---\n\nlayout: recipe\ntitle:  \"Red Berry Tart\"\nimage: red-berry-tart.jpg\ntags: desserts\n\ndirections:\n- Bake the crust and let it cool\n- Make the custard, pour into crust\n- Make the red berry topping, spread over the top\n\ncomponents:\n- Graham Cracker Crust\n- Vanilla Custard Filling\n- Red Berry Dessert Topping\n\n---\n\nA favorite when I go to BBQs (parties, hackathons, your folks' place), this red berry tart is fairly easy to make and packs a huge wow factor.\n</code></pre> <p>Where <code>Graham Cracker Crust</code> is another recipe. The outcome is nice too.</p> <p>Downsides are:</p> <ul> <li>It's written in HTML and javascript.</li> <li>They don't answer issues so it     looks unmaintained.</li> </ul> <p>It redirects to an interesting schema of a recipe.</p> <p>https://github.com/clarklab/chowdown https://raw.githubusercontent.com/clarklab/chowdown/gh-pages/_recipes/broccoli-cheese-soup.md https://www.paprikaapp.com/</p>"}, {"location": "cooking_software/#recipes", "title": "Recipes", "text": "<p>https://docs.tandoor.dev/features/authentication/</p>"}, {"location": "cooking_software/#chef", "title": "Chef", "text": ""}, {"location": "copier/", "title": "copier", "text": "<p>Copier is a library and CLI app for rendering project templates.</p> <ul> <li>Works with local paths and Git URLs.</li> <li>Your project can include any file and Copier can dynamically replace values in any kind of text file.</li> <li>It generates a beautiful output and takes care of not overwriting existing files unless instructed to do so.</li> </ul>"}, {"location": "copier/#installation", "title": "Installation", "text": "<pre><code>pipx install copier\n</code></pre> <p>Until this issue is solved you also need to downgrade <code>pydantic</code></p> <pre><code>pipx inject copier 'pydantic&lt;2'\n</code></pre>"}, {"location": "copier/#basic-concepts", "title": "Basic concepts", "text": "<p>Copier is composed of these main concepts:</p> <ul> <li>Templates: They lay out how to generate the subproject.</li> <li>Questionaries: They are configured in the template. Answers are used to generate projects.</li> <li>Projects: This is where your real program lives. But it is usually generated and/or updated from a template.</li> </ul> <p>Copier targets these main human audiences:</p> <ul> <li> <p>Template creators: Programmers that repeat code too much and prefer a tool to do it for them.     This quote on their docs made my day:</p> <p>Copier doesn't replace the DRY principle... but sometimes you simply can't be DRY and you need a DRYing machine...</p> </li> <li> <p>Template consumers: Programmers that want to start a new project quickly, or that want to evolve it comfortably.</p> </li> </ul> <p>Non-humans should be happy also by using Copier's CLI or API, as long as their expectations are the same as for those humans... and as long as they have feelings.</p> <p>Templates have these goals:</p> <ul> <li>Code scaffolding: Help consumers have a working source code tree as quickly as possible. All templates allow scaffolding.</li> <li>Code lifecycle management. When the template evolves, let consumers update their projects. Not all templates allow updating.</li> </ul> <p>Copier tries to have a smooth learning curve that lets you create simple templates that can evolve into complex ones as needed.</p>"}, {"location": "copier/#usage", "title": "Usage", "text": ""}, {"location": "copier/#creating-a-template", "title": "Creating a template", "text": "<p>A template is a directory: usually the root folder of a Git repository.</p> <p>The content of the files inside the project template is copied to the destination without changes, unless they end with <code>.jinja</code>. In that case, the templating engine will be used to render them.</p> <p>Jinja2 templating is used. Learn more about it by reading Jinja2 documentation.</p> <p>If a YAML file named <code>copier.yml</code> or <code>copier.yaml</code> is found in the root of the project, the user will be prompted to fill in or confirm the default values.</p> <p>Minimal example:</p> <pre><code>\ud83d\udcc1 my_copier_template                            # your template project\n\u251c\u2500\u2500 \ud83d\udcc4 copier.yml                                # your template configuration\n\u251c\u2500\u2500 \ud83d\udcc1 .git/                                     # your template is a Git repository\n\u251c\u2500\u2500 \ud83d\udcc1 {{project_name}}                          # a folder with a templated name\n\u2502   \u2514\u2500\u2500 \ud83d\udcc4 {{module_name}}.py.jinja              # a file with a templated name\n\u2514\u2500\u2500 \ud83d\udcc4 {{_copier_conf.answers_file}}.jinja       # answers are recorded here\n</code></pre> <p>Where: </p> <ul> <li> <p>copier.yml</p> <pre><code>```yaml\n</code></pre> </li> <li> <p><code>{{project_name}}/{{module_name}}.py.jinja</code></p> <pre><code>print(\"Hello from {{module_name}}!\")\n</code></pre> </li> <li> <p><code>{{_copier_conf.answers_file}}.jinja</code></p> <pre><code># Changes here will be overwritten by Copier\n{{ _copier_answers|to_nice_yaml -}}\n</code></pre> </li> </ul> <p>Generating a project from this template using <code>copier copy my_copier_template generated_project</code> answering <code>super_project</code> and <code>world</code> for the <code>project_name</code> and <code>module_name</code> questions respectively would create in the following directory and files:</p> <pre><code>\ud83d\udcc1 generated_project\n\u251c\u2500\u2500 \ud83d\udcc1 super_project\n\u2502   \u2514\u2500\u2500 \ud83d\udcc4 world.py\n\u2514\u2500\u2500 \ud83d\udcc4 .copier-answers.yml\n</code></pre> <p>Where:</p> <ul> <li> <p><code>super_project/world.py</code></p> <pre><code>print(\"Hello from world!\")\n</code></pre> </li> <li> <p><code>.copier-answers.yml</code></p> <pre><code># Changes here will be overwritten by Copier\n_commit: 0.1.0\n_src_path: gh:your_account/your_template\nproject_name: super_project\nmodule_name: world\n</code></pre> </li> </ul>"}, {"location": "copier/#questions", "title": "questions", "text": "<p>project_name:     type: str     help: What is your project name?</p> <p>module_name:     type: str     help: What is your Python module name? ```</p>"}, {"location": "copier/#template-helpers", "title": "Template helpers", "text": "<p>In addition to all the features Jinja supports, Copier includes:</p> <ul> <li> <p>All functions and filters from jinja2-ansible-filters. This includes the <code>to_nice_yaml</code> filter, which is used extensively in our context.</p> </li> <li> <p><code>_copier_answers</code> includes the current answers dict, but slightly modified to make it suitable to autoupdate your project safely:</p> <ul> <li>It doesn't contain secret answers.</li> <li>It doesn't contain any data that is not easy to render to JSON or YAML.</li> <li>It contains special keys like <code>_commit</code> and <code>_src_path</code>, indicating how the last template update was done.</li> </ul> </li> <li><code>_copier_conf</code> includes a representation of the current Copier Worker object, also slightly modified:<ul> <li>It only contains JSON-serializable data.</li> <li>You can serialize it with <code>{{ _copier_conf|to_json }}</code>.</li> <li>\u26a0\ufe0f It contains secret answers inside its <code>.data</code> key.</li> <li>Modifying it doesn't alter the current rendering configuration.</li> <li>It contains the current commit hash from the template in <code>{{ _copier_conf.vcs_ref_hash }}</code>.</li> <li>Contains Operating System-specific directory separator under <code>sep</code> key.</li> </ul> </li> </ul>"}, {"location": "copier/#configuring-a-template", "title": "Configuring a template", "text": ""}, {"location": "copier/#the-copieryaml-file", "title": "The <code>copier.yaml</code> file", "text": "<p>The <code>copier.yml</code> (or <code>copier.yaml</code>) file is found in the root of the template, and it is the main entrypoint for managing your template configuration.</p> <p>For each key found, Copier will prompt the user to fill or confirm the values before they become available to the project template.</p> <p>This <code>copier.yml</code> file:</p> <pre><code>name_of_the_project: My awesome project\nnumber_of_eels: 1234\nyour_email: \"\"\n</code></pre> <p>Will result in a questionary similar to:</p> <pre><code>\ud83c\udfa4 name_of_the_project\n  My awesome project\n\ud83c\udfa4 number_of_eels (int)\n  1234\n\ud83c\udfa4 your_email\n</code></pre> <p>Apart from the simplified format, as seen above, Copier supports a more advanced format to ask users for data. To use it, the value must be a dict.</p> <p>Supported keys:</p> <ul> <li>type: User input must match this type. Options are: <code>bool</code>, <code>float</code>, <code>int</code>, <code>json</code>, <code>str</code>, <code>yaml</code> (default).</li> <li>help: Additional text to help the user know what's this question for.</li> <li> <p>choices: To restrict possible values.</p> <p>A choice can be validated by using the extended syntax with dict-style and tuple-style choices. For example:</p> <pre><code>cloud:\ntype: str\nhelp: Which cloud provider do you use?\nchoices:\n- Any\n- AWS\n- Azure\n- GCP\n\niac:\ntype: str\nhelp: Which IaC tool do you use?\nchoices:\nTerraform: tf\nCloud Formation:\nvalue: cf\nvalidator: \"{% if cloud != 'AWS' %}Requires AWS{% endif %}\"\nAzure Resource Manager:\nvalue: arm\nvalidator: \"{% if cloud != 'Azure' %}Requires Azure{% endif %}\"\nDeployment Manager:\nvalue: dm\nvalidator: \"{% if cloud != 'GCP' %}Requires GCP{% endif %}\"\n</code></pre> <p>When the rendered validator is a non-empty string, the choice is disabled and the message is shown. Choice validation is useful when the validity of a choice depends on the answer to a previous question.</p> </li> <li> <p>default: Leave empty to force the user to answer. Provide a default to save them from typing it if it's quite common. When using choices, the default must be the choice value, not its key, and it must match its type. If values are quite long, you can use YAML anchors.</p> </li> <li>secret: When true, it hides the prompt displaying asterisks (*****) and doesn't save the answer in the answers file</li> <li>placeholder: To provide a visual example for what would be a good value. It is only shown while the answer is empty, so maybe it doesn't make much sense to provide both default and placeholder.</li> <li>multiline: When set to <code>true</code>, it allows multiline input. This is especially useful when type is json or yaml.</li> <li>validator: Jinja template with which to validate the user input. This template will be rendered with the combined answers as variables; it should render nothing if the value is valid, and an error message to show to the user otherwise.</li> <li> <p>when: Condition that, if false, skips the question.</p> <p>If it is a boolean, it is used directly, but it's a bit absurd in that case.</p> <p>If it is a string, it is converted to boolean using a parser similar to YAML, but only for boolean values.</p> <p>This is most useful when templated.</p> <p>If a question is skipped, its answer will be:</p> <ul> <li>The default value, if you're generating the project for the first time.</li> <li>The last answer recorded, if you're updating the project.</li> </ul> <pre><code>project_creator:\ntype: str\n\nproject_license:\ntype: str\nchoices:\n- GPLv3\n- Public domain\n\ncopyright_holder:\ntype: str\ndefault: |-\n{% if project_license == 'Public domain' -%}\n{#- Nobody owns public projects -#}\nnobody\n{%- else -%}\n{#- By default, project creator is the owner -#}\n{{ project_creator }}\n{%- endif %}\n# Only ask for copyright if project is not in the public domain\nwhen: \"{{ project_license != 'Public domain' }}\"\n</code></pre> <pre><code>love_copier:\ntype: bool # This makes Copier ask for y/n\nhelp: Do you love Copier?\ndefault: yes # Without a default, you force the user to answer\n\nproject_name:\ntype: str # Any value will be treated raw as a string\nhelp: An awesome project needs an awesome name. Tell me yours.\ndefault: paradox-specifier\nvalidator: &gt;-\n{% if not (project_name | regex_search('^[a-z][a-z0-9\\-]+$')) %}\nproject_name must start with a letter, followed one or more letters, digits or dashes all lowercase.\n{% endif %}\n\nrocket_launch_password:\ntype: str\nsecret: true # This value will not be logged into .copier-answers.yml\nplaceholder: my top secret password\n\n# I'll avoid default and help here, but you can use them too\nage:\ntype: int\nvalidator: \"{% if age &lt;= 0 %}Must be positive{% endif %}\"\n\nheight:\ntype: float\n\nany_json:\nhelp: Tell me anything, but format it as a one-line JSON string\ntype: json\nmultiline: true\n\nany_yaml:\nhelp: Tell me anything, but format it as a one-line YAML string\ntype: yaml # This is the default type, also for short syntax questions\nmultiline: true\n\nyour_favorite_book:\n# User will choose one of these and your template will get the value\nchoices:\n- The Bible\n- The Hitchhiker's Guide to the Galaxy\n\nproject_license:\n# User will see only the dict key and choose one, but you will\n# get the dict value in your template\nchoices:\nMIT: &amp;mit_text |\nHere I can write the full text of the MIT license.\nThis will be a long text, shortened here for example purposes.\nApache2: |\nFull text of Apache2 license.\n# When using choices, the default value is the value, **not** the key;\n# that's why I'm using the YAML anchor declared above to avoid retyping the\n# whole license\ndefault: *mit_text\n# You can still define the type, to make sure answers that come from --data\n# CLI argument match the type that your template expects\ntype: str\n\nclose_to_work:\nhelp: Do you live close to your work?\n# This format works just like the dict one\nchoices:\n- [at home, I work at home]\n- [less than 10km, quite close]\n- [more than 10km, not so close]\n- [more than 100km, quite far away]\n</code></pre> </li> </ul>"}, {"location": "copier/#include-other-yaml-files", "title": "Include other YAML files", "text": "<p>The <code>copier.yml</code> file supports multiple documents as well as using the <code>!include</code> tag to include settings and questions from other YAML files. This allows you to split up a larger <code>copier.yml</code> and enables you to reuse common partial sections from your templates. When multiple documents are used, care has to be taken with questions and settings that are defined in more than one document:</p> <ul> <li>A question with the same name overwrites definitions from an earlier document.</li> <li>Settings given in multiple documents for <code>exclude</code>, <code>skip_if_exists</code>, <code>jinja_extensions</code> and <code>secret_questions</code> are concatenated.</li> <li>Other settings (such as <code>tasks</code> or <code>migrations</code>) overwrite previous definitions for these settings.</li> </ul> <p>You can use Git submodules to sanely include shared code into templates!</p> <pre><code>---\n# Copier will load all these files\n!include shared-conf/common.*.yml\n\n# These 3 lines split the several YAML documents\n---\n# These two documents include common questions for these kind of projects\n!include common-questions/web-app.yml\n---\n!include common-questions/python-project.yml\n---\n\n# Here you can specify any settings or questions specific for your template\n_skip_if_exists:\n- .password.txt\ncustom_question: default answer\n</code></pre> <p>that includes questions and settings from <code>common-questions/python-project.yml</code></p> <pre><code>version:\ntype: str\nhelp: What is the version of your Python project?\n\n# Settings like `_skip_if_exists` are merged\n_skip_if_exists:\n- \"pyproject.toml\"\n</code></pre>"}, {"location": "copier/#conditional-files-and-directories", "title": "Conditional files and directories", "text": "<p>You can take advantage of the ability to template file and directory names to make them \"conditional\", i.e. to only generate them based on the answers given by a user.</p> <p>For example, you can ask users if they want to use pre-commit:</p> <pre><code>use_precommit:\ntype: bool\ndefault: false\nhelp: Do you want to use pre-commit?\n</code></pre> <p>And then, you can generate a <code>.pre-commit-config.yaml</code> file only if they answered \"yes\":</p> <pre><code>\ud83d\udcc1 your_template\n\u251c\u2500\u2500 \ud83d\udcc4 copier.yml\n\u2514\u2500\u2500 \ud83d\udcc4 {% if use_precommit %}.pre-commit-config.yaml{% endif %}.jinja\n</code></pre> <p>Note that the chosen template suffix must appear outside of the Jinja condition, otherwise the whole file won't be considered a template and will be copied as such in generated projects.</p> <p>You can even use the answers of questions with choices:</p> <pre><code>ci:\ntype: str\nhelp: What Continuous Integration service do you want to use?\nchoices:\nGitHub CI: github\nGitLab CI: gitlab\ndefault: github\n</code></pre> <pre><code>\ud83d\udcc1 your_template\n\u251c\u2500\u2500 \ud83d\udcc4 copier.yml\n\u251c\u2500\u2500 \ud83d\udcc1 {% if ci == 'github' %}.github{% endif %}\n\u2502   \u2514\u2500\u2500 \ud83d\udcc1 workflows\n\u2502       \u2514\u2500\u2500 \ud83d\udcc4 ci.yml\n\u2514\u2500\u2500 \ud83d\udcc4 {% if ci == 'gitlab' %}.gitlab-ci.yml{% endif %}.jinja\n</code></pre> <p>Contrary to files, directories must not end with the template suffix.</p>"}, {"location": "copier/#generating-a-directory-structure", "title": "Generating a directory structure", "text": "<p>You can use answers to generate file names as well as whole directory structures.</p> <pre><code>package:\ntype: str\nhelp: Package name\n</code></pre> <pre><code>\ud83d\udcc1 your_template\n\u251c\u2500\u2500 \ud83d\udcc4 copier.yml\n\u2514\u2500\u2500 \ud83d\udcc4 {{ package.replace('.', _copier_conf.sep) }}{{ _copier_conf.sep }}__main__.py.jinja\n</code></pre> <p>If you answer <code>your_package.cli.main</code> Copier will generate this structure:</p> <pre><code>\ud83d\udcc1 your_project\n\u2514\u2500\u2500 \ud83d\udcc1 your_package\n    \u2514\u2500\u2500 \ud83d\udcc1 cli\n        \u2514\u2500\u2500 \ud83d\udcc1 main\n            \u2514\u2500\u2500 \ud83d\udcc4 __main__.py\n</code></pre> <p>You can either use any separator, like <code>.</code>, and replace it with <code>_copier_conf.sep</code>, like in the example above, or just use <code>/</code>.</p>"}, {"location": "copier/#importing-jinja-templates-and-macros", "title": "Importing Jinja templates and macros", "text": "<p>You can include templates and import macros to reduce code duplication. A common scenario is the derivation of new values from answers, e.g. computing the slug of a human-readable name:</p> <ul> <li> <p><code>copier.yaml</code>:     <pre><code>_exclude:\n- name-slug\n\nname:\ntype: str\nhelp: A nice human-readable name\n\nslug:\ntype: str\nhelp: A slug of the name\ndefault: \"{% include 'name-slug.jinja' %}\"\n</code></pre></p> </li> <li> <p><code>name-slug.jinja</code></p> <pre><code>{# For simplicity ... -#}\n{{ name|lower|replace(' ', '-') }}\n</code></pre> </li> </ul> <pre><code>\ud83d\udcc1 your_template\n\u251c\u2500\u2500 \ud83d\udcc4 copier.yml\n\u2514\u2500\u2500 \ud83d\udcc4 name-slug.jinja\n</code></pre> <p>It is also possible to include a template in a templated folder name</p> <pre><code>\ud83d\udcc1 your_template\n\u251c\u2500\u2500 \ud83d\udcc4 copier.yml\n\u251c\u2500\u2500 \ud83d\udcc4 name-slug.jinja\n\u2514\u2500\u2500 \ud83d\udcc1 {% include 'name-slug.jinja' %}\n    \u2514\u2500\u2500 \ud83d\udcc4 __init__.py\n</code></pre> <p>or in a templated file name</p> <pre><code>\ud83d\udcc1 your_template\n\u251c\u2500\u2500 \ud83d\udcc4 copier.yml\n\u251c\u2500\u2500 \ud83d\udcc4 name-slug.jinja\n\u2514\u2500\u2500 \ud83d\udcc4 {% include 'name-slug.jinja' %}.py\n</code></pre> <p>or in the templated content of a text file:</p> <pre><code># pyproject.toml.jinja\n\n[project]\nname = \"{% include 'name-slug.jinja' %}\"\n</code></pre> <p>Similarly, a Jinja macro can be defined and imported, e.g. in copier.yml.</p> <pre><code>slugify.jinja\n\n{# For simplicity ... -#}\n{% macro slugify(value) -%}\n{{ value|lower|replace(' ', '-') }}\n{%- endmacro %}\n</code></pre> <pre><code># copier.yml\n\n_exclude:\n- slugify\n\nname:\ntype: str\nhelp: A nice human-readable name\n\nslug:\ntype: str\nhelp: A slug of the name\ndefault: \"{% from 'slugify.jinja' import slugify %}{{ slugify(name) }}\"\n</code></pre> <p>or in a templated folder name, in a templated file name, or in the templated content of a text file.</p> <p>As the number of imported templates and macros grows, you may want to place them in a dedicated directory such as <code>includes</code>:</p> <pre><code>\ud83d\udcc1 your_template\n\u251c\u2500\u2500 \ud83d\udcc4 copier.yml\n\u2514\u2500\u2500 \ud83d\udcc1 includes\n    \u251c\u2500\u2500 \ud83d\udcc4 name-slug.jinja\n    \u251c\u2500\u2500 \ud83d\udcc4 slugify.jinja\n    \u2514\u2500\u2500 \ud83d\udcc4 ...\n</code></pre> <p>Then, make sure to exclude this folder in <code>copier.yml</code></p> <pre><code>_exclude:\n- includes\n</code></pre> <p>or use a subdirectory, e.g.:</p> <pre><code>_subdirectory: template\n</code></pre> <p>To import it you can use either:</p> <pre><code>{% include pathjoin('includes', 'name-slug.jinja') %}\n</code></pre> <p>or</p> <pre><code>{% from pathjoin('includes', 'slugify.jinja') import slugify %}\n</code></pre>"}, {"location": "copier/#available-settings", "title": "Available settings", "text": "<p>Remember that the key must be prefixed with an underscore if you use it in the <code>copier.yml</code> file.</p> <p>Check the source for a complete list of settings</p>"}, {"location": "copier/#the-copieranswersyml-file", "title": "The <code>.copier.answers.yml</code> file", "text": "<p>If the destination path exists and a <code>.copier-answers.yml</code> file is present there, it will be used to load the last user's answers to the questions made in the <code>copier.yml</code> file.</p> <p>This makes projects easier to update because when the user is asked, the default answers will be the last ones they used.</p> <p>The file must be called exactly <code>{{ _copier_conf.answers_file }}.jinja</code> in your template's root folder to allow applying multiple templates to the same subproject.</p> <p>The file must have this content:</p> <pre><code># Changes here will be overwritten by Copier; NEVER EDIT MANUALLY\n{{ _copier_answers|to_nice_yaml -}}\n</code></pre>"}, {"location": "copier/#apply-multiple-templates-to-the-same-subproject", "title": "Apply multiple templates to the same subproject", "text": "<p>Imagine this scenario:</p> <ul> <li>You use one framework that has a public template to generate a project. It's available at https://github.com/example-framework/framework-template.git.</li> <li>You have a generic template that you apply to all your projects to use the same pre-commit configuration (formatters, linters, static type checkers...). You have published that in https://gitlab.com/my-stuff/pre-commit-template.git.</li> <li>You have a private template that configures your subproject to run in your internal CI. It's found in git@gitlab.example.com:my-company/ci-template.git.</li> </ul> <p>All 3 templates are completely independent:</p> <ul> <li>Anybody can generate a project for the specific framework, no matter if they want to use pre-commit or not.</li> <li>You want to share the same pre-commit configurations, no matter if the subproject is for one or another framework.</li> <li>You want to have a centralized CI configuration for all your company projects, no matter their pre-commit configuration or the framework they rely on.</li> </ul> <p>You need to use a different answers file for each one. All of them contain a <code>{{ _copier_conf.answers_file }}.jinja</code> file as specified above. Then you apply all the templates to the same project:</p> <pre><code>mkdir my-project\ncd my-project\ngit init\n# Apply framework template\ncopier copy -a .copier-answers.main.yml https://github.com/example-framework/framework-template.git .\ngit add .\ngit commit -m 'Start project based on framework template'\n# Apply pre-commit template\ncopier copy -a .copier-answers.pre-commit.yml https://gitlab.com/my-stuff/pre-commit-template.git .\ngit add .\npre-commit run -a  # Just in case \ud83d\ude09\ngit commit -am 'Apply pre-commit template'\n# Apply internal CI template\ncopier copy -a .copier-answers.ci.yml git@gitlab.example.com:my-company/ci-template.git .\ngit add .\ngit commit -m 'Apply internal CI template'\n</code></pre> <p>Done!</p> <p>After a while, when templates get new releases, updates are handled separately for each template:</p> <pre><code>copier update -a .copier-answers.main.yml\ncopier update -a .copier-answers.pre-commit.yml\ncopier update -a .copier-answers.ci.yml\n</code></pre>"}, {"location": "copier/#generating-a-template", "title": "Generating a template", "text": "<p>You can generate a project from a template using the copier command-line tool:</p> <pre><code>copier copy path/to/project/template path/to/destination\n</code></pre> <p>Or within Python code:</p> <pre><code>copier.run_copy(\"path/to/project/template\", \"path/to/destination\")\n</code></pre> <p>The \"template\" parameter can be a local path, an URL, or a shortcut URL:</p> <ul> <li>GitHub: <code>gh:namespace/project</code></li> <li>GitLab: <code>gl:namespace/project</code></li> </ul> <p>If Copier doesn't detect your remote URL as a Git repository, make sure it starts with one of <code>git+https://</code>, <code>git+ssh://</code>, <code>git@</code> or <code>git://</code>, or it ends with <code>.git</code>.</p> <p>Use the <code>--data</code> command-line argument or the <code>data</code> parameter of the <code>copier.run_copy()</code> function to pass whatever extra context you want to be available in the templates. The arguments can be any valid Python value, even a function.</p> <p>Use the <code>--vcs-ref</code> command-line argument to checkout a particular Git ref before generating the project.</p> <p>All the available options are described with the <code>--help-all</code> option.</p>"}, {"location": "copier/#updating-a-project", "title": "Updating a project", "text": "<p>The best way to update a project from its template is when all of these conditions are true:</p> <ul> <li>The destination folder includes a valid <code>.copier-answers.yml</code> file.</li> <li>The template is versioned with Git (with tags).</li> <li>The destination folder is versioned with Git.</li> </ul> <p>If that's your case, then just enter the destination folder, make sure <code>git status</code> shows it clean, and run:</p> <pre><code>copier update\n</code></pre> <p>This will read all available Git tags, will compare them using PEP 440, and will check out the latest one before updating. To update to the latest commit, add <code>--vcs-ref=HEAD</code>. You can use any other Git ref you want.</p> <p>When updating, Copier will do its best to respect your project evolution by using the answers you provided when copied last time. However, sometimes it's impossible for Copier to know what to do with a diff code hunk. In those cases, copier handles the conflict in one of two ways, controlled with the <code>--conflict</code> option:</p> <ul> <li><code>--conflict rej</code>: Creates a separate <code>.rej</code> file for each file with conflicts. These files contain the unresolved diffs.</li> <li><code>--conflict inline</code> (default): Updates the file with conflict markers. This is quite similar to the conflict markers created when a git merge command encounters a conflict.</li> </ul> <p>If the update results in conflicts, you should review those manually before committing.</p> <p>You probably don't want to lose important changes or to include merge conflicts in your Git history, but if you aren't careful, it's easy to make mistakes.</p> <p>That's why the recommended way to prevent these mistakes is to add a pre-commit (or equivalent) hook that forbids committing conflict files or markers. The recommended hook configuration depends on the <code>conflict</code> setting you use.</p> <p>Never update <code>.copier-answers.yml</code> manually!!!</p> <p>If you want to just reuse all previous answers use <code>copier update --force</code>.</p>"}, {"location": "copier/#migration-across-copier-major-versions", "title": "Migration across Copier major versions", "text": "<p>When there's a new major release of Copier (for example from Copier 5.x to 6.x), there are chances that there's something that changed. Maybe your template will not work as it did before.</p> <p>Copier needs to make a copy of the template in its old state with its old answers so it can actually produce a diff with the new state and answers and apply the smart update to the project. To overcome this situation you can:</p> <ul> <li>Write good migrations.</li> <li>Then you can test them on your template's CI on a matrix against several Copier versions.</li> <li>Or you can just recopy the project when you update to a newer Copier major release.</li> </ul>"}, {"location": "copier/#tasks-and-migrations", "title": "Tasks and migrations", "text": "<p>tasks are commands to execute after generating or updating a project from your template. They run ordered, and with the <code>$STAGE=task</code> variable in their environment.</p> <pre><code># copier.yml\n\n_tasks:\n# Strings get executed under system's default shell\n- \"git init\"\n- \"rm {{ name_of_the_project }}/README.md\"\n# Arrays are executed without shell, saving you the work of escaping arguments\n- [invoke, \"--search-root={{ _copier_conf.src_path }}\", after-copy]\n# You are able to output the full conf to JSON, to be parsed by your script\n- [invoke, end-process, \"--full-conf={{ _copier_conf|to_json }}\"]\n# Your script can be run by the same Python environment used to run Copier\n- [\"{{ _copier_python }}\", task.py]\n# OS-specific task (supported values are \"linux\", \"macos\", \"windows\" and `None`)\n- &gt;-\n{% if _copier_conf.os in ['linux', 'macos'] %}\nrm {{ name_of_the_project }}/README.md\n{% elif _copier_conf.os == 'windows' %}\nRemove-Item {{ name_of_the_project }}/README.md\n{% endif %}\n</code></pre> <p>Note: the example assumes you use Invoke as your task manager. But it's just an example. The point is that we're showing how to build and call commands.</p> <p>Migrations are like tasks, but each item in the list is a dict with these keys:</p> <ul> <li><code>version</code>: Indicates the version that the template update has to go through to trigger this migration. It is evaluated using PEP 440.</li> <li><code>before</code> (optional): Commands to execute before performing the update. The answers file is reloaded after running migrations in this stage, to let you migrate answer values.</li> <li><code>after</code> (optional): Commands to execute after performing the update.</li> </ul> <p>Migrations will run in the same order as declared in the file (so you could even run a migration for a higher version before running a migration for a lower version if the higher one is declared before and the update passes through both).</p> <p>They will only run when new <code>version &gt;= declared version &gt; old version</code>. And only when updating (not when copying for the 1<sup>st</sup> time).</p> <p>If the migrations definition contains Jinja code, it will be rendered with the same context as the rest of the template.</p> <p>Migration processes will receive these environment variables:</p> <ul> <li><code>$STAGE</code>: Either before or after.</li> <li><code>$VERSION_FROM</code>: Git commit description of the template as it was before updating.</li> <li><code>$VERSION_TO</code>: Git commit description of the template as it will be after updating.</li> <li><code>$VERSION_CURRENT</code>: The version detector as you indicated it when describing migration tasks.</li> <li><code>$VERSION_PEP440_FROM</code>, <code>$VERSION_PEP440_TO</code>, <code>$VERSION_PEP440_CURRENT</code>: Same as the above, but normalized into a standard PEP 440 version string indicator. If your scripts use these environment variables to perform migrations, you probably will prefer to use these variables.</li> </ul> <pre><code># copier.yml\n\n_migrations:\n- version: v1.0.0\nbefore:\n- rm ./old-folder\nafter:\n# {{ _copier_conf.src_path }} points to the path where the template was\n# cloned, so it can be helpful to run migration scripts stored there.\n- invoke -r {{ _copier_conf.src_path }} -c migrations migrate $VERSION_CURRENT\n</code></pre>"}, {"location": "copier/#developing-a-copier-template", "title": "Developing a copier template", "text": ""}, {"location": "copier/#avoid-doing-commits-when-developing", "title": "Avoid doing commits when developing", "text": "<p>While you're developing it's useful to see the changes before making a commit, to do so you can use <code>copier copy -r HEAD ./src ./dst</code>. Keep in mind that you won't be able to use <code>copier update</code> so the changes will be applied incrementally, not declaratively. So if you make a file in an old run that has been deleted in the source, it won't be removed in the destination. It's a good idea then to remove the destination directory often.</p>"}, {"location": "copier/#apply-migrations-only-once", "title": "Apply migrations only once", "text": "<p>Currently <code>copier</code> allows you to run two kind of commands:</p> <ul> <li>Tasks: that run each time you either <code>copy</code> or <code>update</code></li> <li>Migrations: That run only on <code>update</code>s if you're coming from a previous version</li> </ul> <p>But there isn't yet a way to run a task only on the <code>copy</code> of a project. Until there is you can embed inside the generated project's Makefile an <code>init</code> target that runs the init script. The user will then need to:</p> <pre><code>copier copy src dest\ncd dest\nmake init\n</code></pre> <p>Not ideal but it can be a workaround until we have the <code>pre-copy</code> tasks.</p> <p>Another solution I thought of is to:</p> <ul> <li>Create a tag <code>0.0.0</code> on the first valid commit of the template</li> <li>Create an initial migration script for version <code>0.1.0</code>. </li> </ul> <p>That way instead of doing <code>copier copy src dest</code> you can do:</p> <pre><code>copier copy -r 0.0.0 src dest\ncopier update\n</code></pre> <p>It will run over all the migrations steps you make in the future. A way to tackle this is to eventually release a <code>1.0.0</code> and move the <code>0.1.0</code> migration script to <code>1.1.0</code> using <code>copier copy -r 1.0.0 src dest</code>. </p> <p>However, @pawamoy thinks that this can eventually backfire because all the versions of the template will not be backward compatible with 0.0.0. If they are now, they probably won't be in the future. This might be because of the template itself, or because of the extensions it uses, or because of the version of Copier it required at the time of each version release. So this can be OK for existing projects, but not when trying to generate new ones.</p>"}, {"location": "copier/#create-your-own-jinja-extensions", "title": "Create your own jinja extensions", "text": "<p>You can create your own jinja filters. For example creating an <code>extensions.py</code> file with the contents:</p> <pre><code>import re\nimport subprocess\nimport unicodedata\nfrom datetime import date\n\nfrom jinja2.ext import Extension\n\n\ndef git_user_name(default: str) -&gt; str:\n    return subprocess.getoutput(\"git config user.name\").strip() or default\n\n\ndef git_user_email(default: str) -&gt; str:\n    return subprocess.getoutput(\"git config user.email\").strip() or default\n\n\ndef slugify(value, separator=\"-\"):\n    value = unicodedata.normalize(\"NFKD\", str(value)).encode(\"ascii\", \"ignore\").decode(\"ascii\")\n    value = re.sub(r\"[^\\w\\s-]\", \"\", value.lower())\n    return re.sub(r\"[-_\\s]+\", separator, value).strip(\"-_\")\n\n\nclass GitExtension(Extension):\n    def __init__(self, environment):\n        super().__init__(environment)\n        environment.filters[\"git_user_name\"] = git_user_name\n        environment.filters[\"git_user_email\"] = git_user_email\n\n\nclass SlugifyExtension(Extension):\n    def __init__(self, environment):\n        super().__init__(environment)\n        environment.filters[\"slugify\"] = slugify\n\n\nclass CurrentYearExtension(Extension):\n    def __init__(self, environment):\n        super().__init__(environment)\n        environment.globals[\"current_year\"] = date.today().year\n</code></pre> <p>Then you can import it in your <code>copier.yaml</code> file:</p> <pre><code>_jinja_extensions:\n- copier_templates_extensions.TemplateExtensionLoader\n- extensions.py:CurrentYearExtension\n- extensions.py:GitExtension\n- extensions.py:SlugifyExtension\n\n\nauthor_fullname:\ntype: str\nhelp: Your full name\ndefault: \"{{ 'Timoth\u00e9e Mazzucotelli' | git_user_name }}\"\n\nauthor_email:\ntype: str\nhelp: Your email\ndefault: \"{{ 'pawamoy@pm.me' | git_user_email }}\"\n\nrepository_name:\ntype: str\nhelp: Your repository name\ndefault: \"{{ project_name | slugify }}\"\n</code></pre> <p>You'll need to install <code>copier-templates-extensions</code>, if you've installed <code>copier</code> with pipx you can:</p> <pre><code>pipx inject copier copier-templates-extensions\n</code></pre>"}, {"location": "copier/#references", "title": "References", "text": "<ul> <li>Source</li> <li>Docs</li> <li>Example templates</li> </ul>"}, {"location": "cpu/", "title": "CPU", "text": "<p>A central processing unit or CPU, also known as the brain of the server, is the electronic circuitry that executes instructions comprising a computer program. The CPU performs basic arithmetic, logic, controlling, and input/output (I/O) operations specified by the instructions in the program.</p> <p>The main processor factors you should consider the most while buying a server are:</p> <ul> <li>Clock speed: Is measured in gigahertz (GHz). The higher the clock speed,     the faster the processor will calculate. The     clock speed along with the bit width conveys us that in a single second how     much data can flow. If a processor has a speed of 2.92 GHz and the bit width     of 32 bits, then it means that it can process almost 3 billion units of 32     bits of data per second.</li> <li>Cores: Every processor core handles individual processing tasks. A multi-core     processor can process multiple computing instructions in parallel..</li> <li>Threads: Threads refer to the number of processors a chip can handle     simultaneously. The difference between threads and cores is that cores are     physical processing units in charge of computing processes. In contrast,     threads are virtual components that run tasks as software would.</li> <li> <p>Cache: Used by the processor to speed up the access to instructions and data     between the processor and the RAM. There are three types of cache you will     come across while looking at the specification sheet:</p> <ul> <li>L1: Is the fastest, but cramped.</li> <li>L2: Is roomier, but slower.</li> <li>L3: Is spacious, but comparatively slower than above two.</li> </ul> </li> </ul>"}, {"location": "cpu/#speed", "title": "Speed", "text": "<p>To achieve the same speed you can play with the speed of a core and the number of cores.</p> <p>Having more cores and lesser clock speed has the next advantages:</p> <ul> <li>Processors with higher cores deliver higher performance and are cost-effective.</li> <li>Applications supporting multi-threading would benefit from a higher number of cores.</li> <li>Multi-threading support for applications will continue to enhance over time.</li> <li>Easily run more applications without experiencing the performance drop.</li> <li>Great for virtualization and running multiple virtual machines.</li> </ul> <p>With the disadvantages that it offers lower single threaded performance.</p> <p>On the other hand, using fewer cores and higher clock speed gives the next advantages:</p> <ul> <li>Offers better single threaded performance.</li> <li>Lower cost.</li> </ul> <p>And the next disadvantages</p> <ul> <li>Due to the lower number of cores, it becomes difficult to split between     applications.</li> <li>Not so strong as multi-threading performance.</li> </ul>"}, {"location": "cpu/#providers", "title": "Providers", "text": ""}, {"location": "cpu/#amd", "title": "AMD", "text": "<p>The Ryzen family is broken down into four distinct branches:</p> <ul> <li>Ryzen 3: entry-level.</li> <li>Ryzen 5: mainstream.</li> <li>Ryzen 7: performance</li> <li>Ryzen 9:  high-end</li> </ul> <p>They're all great chips in their own ways, but some certainly offer more value than others, and for many, the most powerful chips will be complete overkill.</p>"}, {"location": "cpu/#market-analysis", "title": "Market analysis", "text": "Property Ryzen 7 5800x Ryzen 5 5600x Ryzen 7 5700x Ryzen 5 5600G Cores 8 6 8 6 Threads 16 12 16 12 Clock 3.8 3.7 3.4 3.9 Socket AM4 AM4 AM4 AM4 PCI 4.0 4.0 4.0 3.0 Thermal Not included Wraith Stealth Not included Wraith Stealth Default TDP 105W 65W 65W 65W System Mem spec &gt;= 3200 MHz &gt;= 3200 MHz &gt;= 3200 MHz &gt;= 3200 MHz Mem type DDR4 DDR4 DDR4 DDR4 Price 315 232 279 179 <p>The data was extracted from AMD's official page.</p> <p>They all support the chosen RAM and the motherboard.</p> <p>I'm ruling out Ryzen 7 5800x because it's too expensive both on monetary and power consumption terms. Also ruling out Ryzen 5 5600G because it has comparatively bad properties.</p> <p>Between Ryzen 5 5600x and Ryzen 7 5700x, after checking these comparisons (1, 2) it looks like:</p> <ul> <li>Single core performance is similar.</li> <li>7 wins when all cores are involved.</li> <li>7 is more power efficient.</li> <li>7 is better rated.</li> <li>7 is newer (1.5 years).</li> <li>7 has around 3.52 GB/s (7%) higher theoretical RAM memory bandwidth</li> <li>They have the same cache</li> <li>7 has 5 degrees less of max temperature</li> <li>They both support ECC</li> <li>5 has a greater market share</li> <li>5 is 47$ cheaper</li> </ul> <p>I think that for 47$ it's work the increase on cores and theoretical RAM memory bandwidth. Therefore I'd go with the Ryzen 7 5700x.</p>"}, {"location": "cpu/#cpu-coolers", "title": "CPU coolers", "text": "<p>One of the most important decisions when building your PC, especially if you plan on overclocking, is choosing the best CPU cooler. The cooler is often a limiting factor to your overclocking potential, especially under sustained loads. Your cooler choice can also make a substantial difference in noise output. So buying a cooler that can handle your best CPU\u2019s thermal output/heat, (be it at stock settings or when overclocked) is critical to avoiding throttling and achieving your system\u2019s full potential, while keeping the whole system quiet.</p> <p>CPU Coolers come in dozens of shapes and sizes, but most fall into these categories:</p> <ul> <li>Air: made of some combination of metal heatsinks and fans, come in all shapes     and sizes and varying thermal dissipation capacities (sometimes listed as     TDP). High-end air coolers these days rival many all-in-one (AIO) liquid     coolers that have become popular in the market over the past several years.</li> <li>Closed-loop</li> <li>All-in one (AIO)custom</li> </ul> <p>AIO or closed-loop coolers can be (but aren\u2019t always) quieter than air coolers, without requiring the complications of cutting and fitting custom tubes and maintaining coolant levels after setup. AIOs have also become increasingly resistant to leaks over the years, and are easier to install. But they require room for a radiator, so may require a larger case than some air coolers.</p> <p>Here\u2019s a quick comparison of some of the pros and cons of air and liquid cooling.</p> <p>Liquid Cooling Pros:</p> <ul> <li>Highest cooling potential .</li> <li>Fewer clearance issues around the socket.</li> </ul> <p>Liquid Cooling Cons:</p> <ul> <li>Price is generally higher (and price to performance ratio is typically lower     as well).</li> <li>(Slim) possibility of component-damaging leaks.</li> </ul> <p>Air Cooling Pros:</p> <ul> <li>Price is generally lower (better price to performance ratio).</li> <li>No maintenance required.</li> <li>Zero chance for leaks.</li> </ul> <p>Air Cooling Cons:</p> <ul> <li>Limited cooling potential.</li> <li>Increased fitment issues around the socket with memory, fans, etc).</li> <li>Can be heavy/difficult to mount.</li> </ul>"}, {"location": "cpu/#quick-shopping-tips", "title": "Quick shopping tips", "text": "<ul> <li> <p>Own a recent Ryzen CPU?: You may not need to buy a cooler, even for     overclocking. All Ryzen 300- and 2000-series processors and some older Ryzen     models ship with coolers, and many of them can handle moderate overclocks.     If you want the best CPU clock speed possible, you\u2019ll still want to buy an     aftermarket cooler, but for many Ryzen owners, that won\u2019t be necessary.</p> </li> <li> <p>Check clearances before buying: Big air coolers and low-profile models can     bump up against tall RAM and even VRM heat sinks sometimes. And tall coolers     can butt up against your case door or window. Be sure to check the     dimensions and advertised clearances of any cooler and your case before     buying.</p> </li> <li> <p>More fans=better cooling, but more noise: The coolers that do the absolute     best job of moving warm air away from your CPU and out of your case are also     often the loudest. If fan noise is a problem for you, you\u2019ll want a cooler     that does a good job of balancing noise and cooling.</p> </li> <li> <p>Make sure you can turn off RGB: Many coolers these days include RGB fans and     / or lighting. This can be a fun way to customize the look of your PC. But     be sure there\u2019s a way, either via a built-in controller or when plugging the     cooler into a compatible RGB motherboard header, to turn the lights off     without turning off the PC.</p> </li> <li> <p>Check that the CPU has GPU if you don't want to use an external graphic card.     Otherwise the BIOS won't start.</p> </li> </ul>"}, {"location": "cpu/#market-analysis_1", "title": "Market analysis", "text": "<p>After a quick review I'm deciding between the Dark Rock 4 and the Enermax ETS-T50 Axe. The analysis is done after reading Tomshardware reviews (1, 2).</p> <p>They are equal in:</p> <ul> <li>CPU core and motherboard temperatures.</li> <li>Have installation videos.</li> </ul> <p>The Enermax has the advantages:</p> <ul> <li>Is much more silent (between 2 and 4 dB).</li> <li>It has better acoustic efficiency (Relative temperature against Relative     Noise) between 15% and 29%.</li> <li>More TDP</li> <li>Much cheaper (25 EUR)</li> <li>If you have a Phillips screwdriver (normal cross screwdriver) you don't get     another one.</li> </ul> <p>All in all the Enermax ETS-T50 Axe is a better one, but after checking the sizes, my case limit on the height of the CPU cooler is 160mm and the Enermax is 163mm... The Cooler Master Masterair ma610p has 166mm, so it's out of the question too. The Dark Rock 4 max height is 159mm. I don't know if I should bargain.</p> <p>To be in the safe side I'll go with the Dark Rock 4</p>"}, {"location": "cpu/#ryzen-recommended-coolers", "title": "Ryzen recommended coolers", "text": ""}, {"location": "cpu/#cpu-thermal-paste", "title": "CPU Thermal paste", "text": "<p>Thermal paste is designed to minimize microscopic air gaps and irregularities between the surface of the cooler and the CPU's IHS (integrated heat spreader), the piece of metal which is built into the top of the processor.</p> <p>Good thermal paste can have a profound impact on your performance, because it will allow your processor to transfer more of its waste heat to your cooler, keeping your processor running cool.</p> <p>Most pastes are comprised of ceramic or metallic materials suspended within a proprietary binder which allows for easy application and spread as well as simple cleanup.</p> <p>These thermal pastes can be electrically conductive or non-conductive, depending on their specific formula. Electrically conductive thermal pastes can carry current between two points, meaning that if the paste squeezes out onto other components, it can cause damage to motherboards and CPUs when you switch on the power. A single drop out of place can lead to a dead PC, so extra care is imperative.</p> <p>Liquid metal compounds are almost always electrically conductive, so while these compounds provide better performance than their paste counterparts, they require more focus and attention during application. They are very hard to remove if you get some in the wrong place, which would fry your system.</p> <p>In contrast, traditional thermal paste compounds are relatively simple for every experience level. Most, but not all, traditional pastes are electrically non-conductive.</p> <p>Most cpu coolers come with their own thermal paste, so check yours before buying another one.</p>"}, {"location": "cpu/#market-analysis_2", "title": "Market analysis", "text": "Model ProlimaTech PK-3 Thermal Grizzly Kryonaut Cooler Master MasterGel Pro v2 Electrical conductive No No No Thermal Conductivity 11.2 W/mk 12.5 W/mk 9 W/mk Ease of Use 4.5 4.5 4.5 Relative Performance 4.0 4.0 3.5 Price per gram 6.22 9.48 2.57 <p>The best choice would be the ProlimaTech but the package sold are expensive because it has many grams.</p> <p>In my case, my cooler comes with the thermal paste so I'd start with that before spending 20$ more.</p>"}, {"location": "cpu/#installation", "title": "Installation", "text": "<p>When installing an AM4 CPU in the motherboard, rotate the CPU so that the small arrow on one of the corners of the chip matches the arrow on the corner of the motherboard socket.</p>"}, {"location": "cpu/#references", "title": "References", "text": "<ul> <li>Tom's hardware CPU guide</li> <li>Tom's hardware CPU cooling guide</li> <li>How to select the best processor for your server</li> <li>Cloudzy best server processor</li> </ul>"}, {"location": "css/", "title": "CSS", "text": "<p>CSS stands for Cascading Style Sheets and is used to format the layout of a webpage.</p> <p>With CSS, you can control the color, font, the size of text, the spacing between elements, how elements are positioned and laid out, what background images or background colors are to be used, different displays for different devices and screen sizes, and much more!</p>"}, {"location": "css/#using-css-in-html", "title": "Using CSS in HTML", "text": "<p>CSS can be added to HTML documents in 3 ways:</p> <ul> <li>Inline: by using the style attribute inside HTML elements.</li> <li>Internal: by using a"}, {"location": "cypress/", "title": "Cypress", "text": "<p>Cypress is a next generation front end testing tool built for the modern web.</p> <p>Cypress enables you to write all types of tests:</p> <ul> <li>End-to-end tests</li> <li>Integration tests</li> <li>Unit tests</li> </ul> <p>Cypress can test anything that runs in a browser.</p>"}, {"location": "cypress/#features", "title": "Features", "text": "<ul> <li>Time Travel: Cypress takes snapshots as your tests run. Hover over commands     in the Command Log to see exactly what happened at each step.</li> <li>Debuggability: Stop guessing why your tests are failing. Debug directly from     familiar tools like Developer Tools. Our readable errors and stack traces     make debugging lightning fast.</li> <li>Automatic Waiting: Never add waits or sleeps to your tests. Cypress     automatically waits for commands and assertions before moving on. No more     async hell.</li> <li>Spies, Stubs, and Clocks: Verify and control the behavior of functions,     server responses, or timers. The same functionality you love from unit     testing is right at your fingertips.</li> <li>Network Traffic Control: Easily control, stub, and test edge cases without     involving your server. You can stub network traffic however you like.</li> <li>Consistent Results: Our architecture doesn\u2019t use Selenium or WebDriver. Say     hello to fast, consistent and reliable tests that are flake-free.</li> <li>Screenshots and Videos: View screenshots taken automatically on failure, or     videos of your entire test suite when run from the CLI.</li> <li>Cross browser Testing: Run tests within Firefox and Chrome-family browsers     (including Edge and Electron) locally and optimally in a Continuous     Integration pipeline.</li> </ul> <p>Check the key differences page to see more benefits of using the tool.</p>"}, {"location": "cypress/#installation", "title": "Installation", "text": "<pre><code>npm install cypress --save-dev\n</code></pre>"}, {"location": "cypress/#usage", "title": "Usage", "text": "<p>You first need to open cypress with <code>npx cypress open</code>.</p> <p>To get an overview of cypress' workflow follow the Writing your first test tutorial</p> <p>Tests live in the <code>cypress</code> directory, if you create a new file in the <code>cypress/integration</code> directory it will automatically show up in the UI. Cypress monitors your spec files for any changes and automatically displays any changes.</p> <p>Writing tests is meant to be simple, for example:</p> <pre><code>describe('My First Test', () =&gt; {\nit('Does not do much!', () =&gt; {\nexpect(true).to.equal(true)\n})\n})\n</code></pre>"}, {"location": "cypress/#test-structure", "title": "Test structure", "text": "<p>The test interface, borrowed from Mocha, provides <code>describe()</code>, <code>context()</code>, <code>it()</code> and <code>specify()</code>. <code>context()</code> is identical to <code>describe()</code> and <code>specify()</code> is identical to <code>it()</code>.</p> <pre><code>describe('Unit test our math functions', () =&gt; {\ncontext('math', () =&gt; {\nit('can add numbers', () =&gt; {\nexpect(add(1, 2)).to.eq(3)\n})\n\nit('can subtract numbers', () =&gt; {\nexpect(subtract(5, 12)).to.eq(-7)\n})\n\nspecify('can divide numbers', () =&gt; {\nexpect(divide(27, 9)).to.eq(3)\n})\n\nspecify('can multiply numbers', () =&gt; {\nexpect(multiply(5, 4)).to.eq(20)\n})\n})\n})\n</code></pre>"}, {"location": "cypress/#hooks", "title": "Hooks", "text": "<p>Hooks are helpful to set conditions that you want to run before a set of tests or before each test. They're also helpful to clean up conditions after a set of tests or after each test.</p> <pre><code>before(() =&gt; {\n// root-level hook\n// runs once before all tests\n})\n\nbeforeEach(() =&gt; {\n// root-level hook\n// runs before every test block\n})\n\nafterEach(() =&gt; {\n// runs after each test block\n})\n\nafter(() =&gt; {\n// runs once all tests are done\n})\n\ndescribe('Hooks', () =&gt; {\nbefore(() =&gt; {\n// runs once before all tests in the block\n})\n\nbeforeEach(() =&gt; {\n// runs before each test in the block\n})\n\nafterEach(() =&gt; {\n// runs after each test in the block\n})\n\nafter(() =&gt; {\n// runs once after all tests in the block\n})\n})\n</code></pre> <p>!!! warning \"Before writing <code>after()</code> or <code>afterEach()</code> hooks, read the anti-pattern of cleaning up state with <code>after()</code> or <code>afterEach()</code>\"</p>"}, {"location": "cypress/#skipping-tests", "title": "Skipping tests", "text": "<p>You can skip tests in the next ways:</p> <pre><code>describe('TodoMVC', () =&gt; {\nit('is not written yet')\n\nit.skip('adds 2 todos', function () {\ncy.visit('/')\ncy.get('.new-todo').type('learn testing{enter}').type('be cool{enter}')\ncy.get('.todo-list li').should('have.length', 100)\n})\n\nxit('another test', () =&gt; {\nexpect(false).to.true\n})\n})\n</code></pre>"}, {"location": "cypress/#querying-elements", "title": "Querying elements", "text": "<p>Cypress automatically retries the query until either the element is found or a set timeout is reached. This makes Cypress robust and immune to dozens of common problems that occur in other testing tools.</p>"}, {"location": "cypress/#query-by-html-properties", "title": "Query by HTML properties", "text": "<p>You need to find the elements to act upon, usually you do it with the <code>cy.get()</code> function. For example:</p> <pre><code>cy.get('.my-selector')\n</code></pre> <p>Cypress leverages jQuery's powerful selector engine and exposes many of its DOM traversal methods to you so you can work with complex HTML structures. For example:</p> <pre><code>cy.get('#main-content').find('.article').children('img[src^=\"/static\"]').first()\n</code></pre> <p>If you follow the Write testable code guide, you'll select elements by the <code>data-cy</code> element.</p> <pre><code>cy.get('[data-cy=submit]')\n</code></pre> <p>You'll probably write that a lot, that's why it's useful to define the next commands in <code>/cypress/support/commands.ts</code>.</p> <pre><code>Cypress.Commands.add('getById', (selector, ...args) =&gt; {\nreturn cy.get(`[data-cy=${selector}]`, ...args)\n})\n\nCypress.Commands.add('getByIdLike', (selector, ...args) =&gt; {\nreturn cy.get(`[data-cy*=${selector}]`, ...args)\n})\n\nCypress.Commands.add('findById', {prevSubject: true}, (subject, selector, ...args) =&gt; {\nreturn subject.find(`[data-cy=${selector}]`, ...args)\n})\n</code></pre> <p>So you can now do <pre><code>cy.getById('submit')\n</code></pre></p>"}, {"location": "cypress/#query-by-content", "title": "Query by content", "text": "<p>Another way to locate things -- a more human way -- is to look them up by their content, by what the user would see on the page. For this, there's the handy <code>cy.contains()</code> command, for example:</p> <pre><code>// Find an element in the document containing the text 'New Post'\ncy.contains('New Post')\n\n// Find an element within '.main' containing the text 'New Post'\ncy.get('.main').contains('New Post')\n</code></pre> <p>This is helpful when writing tests from the perspective of a user interacting with your app. They only know that they want to click the button labeled \"Submit\". They have no idea that it has a type attribute of submit, or a CSS class of <code>my-submit-button</code>.</p>"}, {"location": "cypress/#changing-the-timeout", "title": "Changing the timeout", "text": "<p>The querying methods accept the <code>timeout</code> argument to change the default timeout.</p> <pre><code>// Give this element 10 seconds to appear\ncy.get('.my-slow-selector', { timeout: 10000 })\n</code></pre>"}, {"location": "cypress/#select-by-position-in-list", "title": "Select by position in list", "text": "<p>Inside our list, we can select elements based on their position in the list, using <code>.first()</code>, <code>.last()</code> or <code>.eq()</code> selector.</p> <pre><code>cy\n.get('li')\n.first(); // select \"red\"\n\ncy\n.get('li')\n.last(); // select \"violet\"\n\ncy\n.get('li')\n.eq(2); // select \"yellow\"\n</code></pre> <p>You can also use <code>.next()</code> and <code>.prev()</code> to navigate through the elements.</p>"}, {"location": "cypress/#select-elements-by-filtering", "title": "Select elements by filtering", "text": "<p>Once you select multiple elements, you can filter within these based on another selector.</p> <pre><code>cy\n.get('li')\n.filter('.primary') // select all elements with the class .primary\n</code></pre> <p>To do the exact opposite, you can use <code>.not()</code> command.</p> <p>cy   .get('li')   .not('.primary') // select all elements without the class .primary</p>"}, {"location": "cypress/#finding-elements", "title": "Finding elements", "text": "<p>You can specify your selector by first selecting an element you want to search within, and then look down the DOM structure to find a specific element you are looking for.</p> <pre><code>cy\n.get('.list')\n.find('.violet') // finds an element with class .violet inside .list element\n</code></pre> <p>Instead of looking down the DOM structure and finding an element within another element, we can look up. In this example, we first select our list item, and then try to find an element with a <code>.list</code> class.</p> <pre><code>cy\n.get('.violet')\n.parent('.list') // finds an element with class .list that is above our .violet element\n</code></pre>"}, {"location": "cypress/#interacting-with-elements", "title": "Interacting with elements", "text": "<p>Cypress allows you to click on and type into elements on the page by using <code>.click()</code> and <code>.type()</code> commands with a <code>cy.get()</code> or <code>cy.contains()</code> command. This is a great example of chaining in action.</p> <pre><code>cy.get('textarea.post-body').type('This is an excellent post.')\n</code></pre> <p>We're chaining the <code>.type()</code> onto the <code>cy.get()</code>, telling it to type into the subject yielded from the <code>cy.get()</code> command, which will be a DOM element.</p> <p>Here are even more action commands Cypress provides to interact with your app:</p> <ul> <li><code>.blur()</code>: Make a focused DOM element blur.</li> <li><code>.focus()</code>: Focus on a DOM element.</li> <li><code>.clear()</code>: Clear the value of an input or <code>textarea</code>.</li> <li><code>.check()</code>: Check checkbox(es) or radio(s).</li> <li><code>.uncheck()</code>: Uncheck checkbox(es).</li> <li><code>.select()</code>: Select an <code>&lt;option&gt;</code> within a <code>&lt;select&gt;</code>.</li> <li><code>.dblclick()</code>: Double-click a DOM element.</li> <li><code>.rightclick()</code>: Right-click a DOM element.</li> </ul> <p>These commands ensure some guarantees about what the state of the elements should be prior to performing their actions.</p> <p>For example, when writing a <code>.click()</code> command, Cypress ensures that the element is able to be interacted with (like a real user would). It will automatically wait until the element reaches an \"actionable\" state by:</p> <ul> <li>Not being hidden</li> <li>Not being covered</li> <li>Not being disabled</li> <li>Not animating</li> </ul> <p>This also helps prevent flake when interacting with your application in tests.</p> <p>If you want to jump into the command flow and use a custom function use <code>.then()</code>. When the previous command resolves, it will call your callback function with the yielded subject as the first argument.</p> <p>If you wish to continue chaining commands after your <code>.then()</code>, you'll need to specify the subject you want to yield to those commands, which you can achieve with a return value other than <code>null</code> or <code>undefined</code>. Cypress will yield that to the next command for you.</p> <pre><code>cy\n// Find the el with id 'some-link'\n.get('#some-link')\n\n.then(($myElement) =&gt; {\n// ...massage the subject with some arbitrary code\n\n// grab its href property\nconst href = $myElement.prop('href')\n\n// strip out the 'hash' character and everything after it\nreturn href.replace(/(#.*)/, '')\n})\n.then((href) =&gt; {\n// href is now the new subject\n// which we can work with now\n})\n</code></pre>"}, {"location": "cypress/#setting-aliases", "title": "Setting aliases", "text": "<p>Cypress has some added functionality for quickly referring back to past subjects called Aliases.</p> <p>It looks something like this:</p> <pre><code>cy.get('.my-selector')\n.as('myElement') // sets the alias\n.click()\n\n/* many more actions */\n\ncy.get('@myElement') // re-queries the DOM as before (only if necessary)\n.click()\n</code></pre> <p>This lets us reuse our DOM queries for faster tests when the element is still in the DOM, and it automatically handles re-querying the DOM for us when it is not immediately found in the DOM. This is particularly helpful when dealing with front end frameworks that do a lot of re-rendering.</p> <p>It can be used to share context between tests, for example with fixtures:</p> <pre><code>beforeEach(() =&gt; {\n// alias the users fixtures\ncy.fixture('users.json').as('users')\n})\n\nit('utilize users in some way', function () {\n// access the users property\nconst user = this.users[0]\n\n// make sure the header contains the first\n// user's name\ncy.get('header').should('contain', user.name)\n})\n</code></pre>"}, {"location": "cypress/#asserting-about-elements", "title": "Asserting about elements", "text": "<p>Assertions let you do things like ensuring an element is visible or has a particular attribute, CSS class, or state. Assertions are commands that enable you to describe the desired state of your application. Cypress will automatically wait until your elements reach this state, or fail the test if the assertions don't pass. For example:</p> <pre><code>cy.get(':checkbox').should('be.disabled')\n\ncy.get('form').should('have.class', 'form-horizontal')\n\ncy.get('input').should('not.have.value', 'US')\n</code></pre> <p>Cypress bundles Chai, Chai-jQuery, and Sinon-Chai to provide built-in assertions. You can see a comprehensive list of them in the list of assertions reference. You can also write your own assertions as Chai plugins and use them in Cypress.</p>"}, {"location": "cypress/#default-assertions", "title": "Default assertions", "text": "<p>Many commands have a default, built-in assertion, or rather have requirements that may cause it to fail without needing an explicit assertion you've added.</p> <ul> <li><code>cy.visit()</code>: Expects the page to send text/html content with a 200 status     code.</li> <li><code>cy.request()</code>: Expects the remote server to exist and provide a response.</li> <li><code>cy.contains()</code>: Expects the element with content to eventually exist in the     DOM.</li> <li><code>cy.get()</code>: Expects the element to eventually exist in the DOM.</li> <li><code>.find()</code>: Also expects the element to eventually exist in the DOM.</li> <li><code>.type()</code>: Expects the element to eventually be in a typeable state.</li> <li><code>.click()</code>: Expects the element to eventually be in an actionable state.</li> <li><code>.its()</code>: Expects to eventually find a property on the current subject.</li> </ul> <p>Certain commands may have a specific requirement that causes them to immediately fail without retrying: such as <code>cy.request()</code>. Others, such as DOM based commands will automatically retry and wait for their corresponding elements to exist before failing.</p>"}, {"location": "cypress/#writing-assertions", "title": "Writing assertions", "text": "<p>There are two ways to write assertions in Cypress:</p> <ul> <li>Implicit Subjects: Using <code>.should()</code> or <code>.and()</code>.</li> <li>Explicit Subjects: Using <code>expect</code>.</li> </ul> <p>The implicit form is much shorter, so only use the explicit form in the next cases:</p> <ul> <li>Assert multiple things about the same subject.</li> <li>Massage the subject in some way prior to making the assertion.</li> </ul>"}, {"location": "cypress/#implicit-subjects", "title": "Implicit Subjects", "text": "<p>Using <code>.should()</code> or <code>.and()</code> commands is the preferred way of making assertions in Cypress.</p> <pre><code>// the implicit subject here is the first &lt;tr&gt;\n// this asserts that the &lt;tr&gt; has an .active class\ncy.get('tbody tr:first').should('have.class', 'active')\n</code></pre> <p>You can chain multiple assertions together using <code>.and()</code>, which is another name for <code>.should()</code> that makes things more readable:</p> <pre><code>cy.get('#header a')\n.should('have.class', 'active')\n.and('have.attr', 'href', '/users')\n</code></pre> <p>Because <code>.should('have.class')</code> does not change the subject, <code>.and('have.attr')</code> is executed against the same element. This is handy when you need to assert multiple things against a single subject quickly.</p>"}, {"location": "cypress/#explicit-subjects", "title": "Explicit Subjects", "text": "<p>Using <code>expect</code> allows you to pass in a specific subject and make an assertion about it.</p> <pre><code>// the explicit subject here is the boolean: true\nexpect(true).to.be.true\n</code></pre>"}, {"location": "cypress/#common-assertions", "title": "Common Assertions", "text": "<ul> <li> <p>Length:</p> <pre><code>// retry until we find 3 matching &lt;li.selected&gt;\ncy.get('li.selected').should('have.length', 3)\n</code></pre> </li> <li> <p>Attribute: For example to test links     <pre><code>// check the content of an attribute\ncy\n.get('a')\n.should('have.attr', 'href', 'https://docs.cypress.io')\n.and('have.attr', 'target', '_blank') // Test it's meant to be opened\n// another tab\n</code></pre></p> </li> <li> <p>Class:</p> <pre><code>// retry until this input does not have class disabled\ncy.get('form').find('input').should('not.have.class', 'disabled')\n</code></pre> </li> <li> <p>Value:</p> <pre><code>// retry until this textarea has the correct value\ncy.get('textarea').should('have.value', 'foo bar baz')\n</code></pre> </li> <li> <p>Text Content:</p> <pre><code>// assert the element's text content is exactly the given text\ncy.get('#user-name').should('have.text', 'Joe Smith')\n// assert the element's text includes the given substring\ncy.get('#address').should('include.text', 'Atlanta')\n// retry until this span does not contain 'click me'\ncy.get('a').parent('span.help').should('not.contain', 'click me')\n// the element's text should start with \"Hello\"\ncy.get('#greeting')\n.invoke('text')\n.should('match', /^Hello/)\n// tip: use cy.contains to find element with its text\n// matching the given regular expression\ncy.contains('#a-greeting', /^Hello/)\n</code></pre> </li> <li> <p>Visibility:</p> <pre><code>// retry until the button with id \"form-submit\" is visible\ncy.get('button#form-submit').should('be.visible')\n// retry until the list item with text \"write tests\" is visible\ncy.contains('.todo li', 'write tests').should('be.visible')\n</code></pre> <p>Note: if there are multiple elements, the assertions <code>be.visible</code> and <code>not.be.visible</code> act differently:</p> <pre><code>// retry until SOME elements are visible\ncy.get('li').should('be.visible')\n// retry until EVERY element is invisible\ncy.get('li.hidden').should('not.be.visible')\n</code></pre> </li> <li> <p>Existence:</p> <pre><code>// retry until loading spinner no longer exists\ncy.get('#loading').should('not.exist')\n</code></pre> </li> <li> <p>State:</p> <pre><code>// retry until our radio is checked\ncy.get(':radio').should('be.checked')\n</code></pre> </li> <li> <p>CSS:</p> <pre><code>// retry until .completed has matching css\ncy.get('.completed').should('have.css', 'text-decoration', 'line-through')\n\n// retry while .accordion css has the \"display: none\" property\ncy.get('#accordion').should('not.have.css', 'display', 'none')\n</code></pre> </li> <li> <p>Disabled property:</p> <pre><code>&lt;input type=\"text\" id=\"example-input\" disabled /&gt;\n</code></pre> <pre><code>cy.get('#example-input')\n.should('be.disabled')\n// let's enable this element from the test\n.invoke('prop', 'disabled', false)\n\ncy.get('#example-input')\n// we can use \"enabled\" assertion\n.should('be.enabled')\n// or negate the \"disabled\" assertion\n.and('not.be.disabled')\n</code></pre> </li> </ul>"}, {"location": "cypress/#negative-assertions", "title": "Negative assertions", "text": "<p>There are positive and negative assertions. Examples of positive assertions are:</p> <pre><code>cy.get('.todo-item').should('have.length', 2).and('have.class', 'completed')\n</code></pre> <p>The negative assertions have the <code>not</code> chainer prefixed to the assertion. For example:</p> <pre><code>cy.contains('first todo').should('not.have.class', 'completed')\ncy.get('#loading').should('not.be.visible')\n</code></pre> <p>We recommend using negative assertions to verify that a specific condition is no longer present after the application performs an action. For example, when a previously completed item is unchecked, we might verify that a CSS class is removed.</p> <pre><code>// at first the item is marked completed\ncy.contains('li.todo', 'Write tests')\n.should('have.class', 'completed')\n.find('.toggle')\n.click()\n\n// the CSS class has been removed\ncy.contains('li.todo', 'Write tests').should('not.have.class', 'completed')\n</code></pre> <p>Read more on the topic in the blog post Be Careful With Negative Assertions.</p>"}, {"location": "cypress/#custom-assertions", "title": "Custom assertions", "text": "<p>You can write your own assertion function and pass it as a callback to the <code>.should()</code> command.</p> <pre><code>cy.get('div').should(($div) =&gt; {\nexpect($div).to.have.length(1)\n\nconst className = $div[0].className\n\n// className will be a string like \"main-abc123 heading-xyz987\"\nexpect(className).to.match(/heading-/)\n})\n</code></pre>"}, {"location": "cypress/#setting-up-the-tests", "title": "Setting up the tests", "text": "<p>Depending on how your application is built - it's likely that your web application is going to be affected and controlled by the server.</p> <p>Traditionally when writing e2e tests using Selenium, before you automate the browser you do some kind of set up and tear down on the server.</p> <p>You generally have three ways to facilitate this with Cypress:</p> <ul> <li><code>cy.exec()</code>: To run system commands.</li> <li><code>cy.task()</code>: To run code in Node via the <code>pluginsFile</code>.</li> <li><code>cy.request()</code>: To make HTTP requests.</li> </ul> <p>If you're running node.js on your server, you might add a <code>before</code> or <code>beforeEach</code> hook that executes an npm task.</p> <pre><code>describe('The Home Page', () =&gt; {\nbeforeEach(() =&gt; {\n// reset and seed the database prior to every test\ncy.exec('npm run db:reset &amp;&amp; npm run db:seed')\n})\n\nit('successfully loads', () =&gt; {\ncy.visit('/')\n})\n})\n</code></pre> <p>Instead of just executing a system command, you may want more flexibility and could expose a series of routes only when running in a test environment.</p> <p>For instance, you could compose several requests together to tell your server exactly the state you want to create.</p> <pre><code>describe('The Home Page', () =&gt; {\nbeforeEach(() =&gt; {\n// reset and seed the database prior to every test\ncy.exec('npm run db:reset &amp;&amp; npm run db:seed')\n\n// seed a post in the DB that we control from our tests\ncy.request('POST', '/test/seed/post', {\ntitle: 'First Post',\nauthorId: 1,\nbody: '...',\n})\n\n// seed a user in the DB that we can control from our tests\ncy.request('POST', '/test/seed/user', { name: 'Jane' })\n.its('body')\n.as('currentUser')\n})\n\nit('successfully loads', () =&gt; {\n// this.currentUser will now point to the response\n// body of the cy.request() that we could use\n// to log in or work with in some way\n\ncy.visit('/')\n})\n})\n</code></pre> <p>While there's nothing really wrong with this approach, it does add a lot of complexity. You will be battling synchronizing the state between your server and your browser - and you'll always need to set up / tear down this state before tests (which is slow).</p> <p>The good news is that we aren't Selenium, nor are we a traditional e2e testing tool. That means we're not bound to the same restrictions.</p> <p>With Cypress, there are several other approaches that can offer an arguably better and faster experience.</p>"}, {"location": "cypress/#stubbing-the-server", "title": "Stubbing the server", "text": "<p>Another valid approach opposed to seeding and talking to your server is to bypass it altogether.</p> <p>While you'll still receive all of the regular HTML / JS / CSS assets from your server and you'll continue to <code>cy.visit()</code> it in the same way - you can instead stub the JSON responses coming from it.</p> <p>This means that instead of resetting the database, or seeding it with the state we want, you can force the server to respond with whatever you want it to. In this way, we not only prevent needing to synchronize the state between the server and browser, but we also prevent mutating state from our tests. That means tests won't build up state that may affect other tests.</p> <p>Another upside is that this enables you to build out your application without needing the contract of the server to exist. You can build it the way you want the data to be structured, and even test all of the edge cases, without needing a server.</p> <p>However - there is likely still a balance here where both strategies are valid (and you should likely do them).</p> <p>While stubbing is great, it means that you don't have the guarantees that these response payloads actually match what the server will send. However, there are still many valid ways to get around this:</p> <ul> <li> <p>Generate the fixture stubs ahead of time: You could have the server generate     all of the fixture stubs for you ahead of time. This means their data will     reflect what the server will actually send.</p> </li> <li> <p>Write a single e2e test without stubs, and then stub the rest: Another more     balanced approach is to integrate both strategies. You likely want to have     a single test that takes a true e2e approach and stubs nothing. It'll use     the feature for real - including seeding the database and setting up state.</p> <p>Once you've established it's working you can then use stubs to test all of the edge cases and additional scenarios. There are no benefits to using real data in the vast majority of cases. We recommend that the vast majority of tests use stub data. They will be orders of magnitude faster, and much less complex.</p> </li> </ul> <p><code>cy.intercept()</code> is used to control the behavior of HTTP requests. You can statically define the body, HTTP status code, headers, and other response characteristics.</p> <pre><code>cy.intercept(\n{\nmethod: 'GET', // Route all GET requests\nurl: '/users/*', // that have a URL that matches '/users/*'\n},\n[] // and force the response to be: []\n).as('getUsers') // and assign an alias\n</code></pre>"}, {"location": "cypress/#fixtures", "title": "Fixtures", "text": "<p>A fixture is a fixed set of data located in a file that is used in your tests. The purpose of a test fixture is to ensure that there is a well known and fixed environment in which tests are run so that results are repeatable. Fixtures are accessed within tests by calling the <code>cy.fixture()</code> command.</p> <p>When stubbing a response, you typically need to manage potentially large and complex JSON objects. Cypress allows you to integrate fixture syntax directly into responses.</p> <pre><code>// we set the response to be the activites.json fixture\ncy.intercept('GET', '/activities/*', { fixture: 'activities.json' })\n</code></pre> <p>Fixtures live in <code>/cypress/fixtures/</code> and can be further organized within additional directories. For instance, you could create another folder called images and add images:</p> <pre><code>/cypress/fixtures/images/cats.png\n/cypress/fixtures/images/dogs.png\n/cypress/fixtures/images/birds.png\n</code></pre> <p>To access the fixtures nested within the images folder, include the folder in your <code>cy.fixture()</code> command.</p> <pre><code>cy.fixture('images/dogs.png') // yields dogs.png as Base64\n</code></pre>"}, {"location": "cypress/#use-the-content-of-a-fixture-set-in-a-hook-in-a-test", "title": "Use the content of a fixture set in a hook in a test", "text": "<p>If you store and access the fixture data using this test context object, make sure to use <code>function () { ... }</code> callbacks both for the hook and the test. Otherwise the test engine will NOT have this pointing at the test context.</p> <pre><code>describe('User page', () =&gt; {\nbeforeEach(function () {\n// \"this\" points at the test context object\ncy.fixture('user').then((user) =&gt; {\n// \"this\" is still the test context object\nthis.user = user\n})\n})\n\n// the test callback is in \"function () { ... }\" form\nit('has user', function () {\n// this.user exists\nexpect(this.user.firstName).to.equal('Jane')\n})\n})\n</code></pre>"}, {"location": "cypress/#logging-in", "title": "Logging in", "text": "<p>One of the first (and arguably one of the hardest) hurdles you'll have to overcome in testing is logging into your application.</p> <p>It's a great idea to get your signup and login flow under test coverage since it is very important to all of your users and you never want it to break.</p> <p>Logging in is one of those features that are mission critical and should likely involve your server. We recommend you test signup and login using your UI as a real user would. For example:</p> <pre><code>describe('The Login Page', () =&gt; {\nbeforeEach(() =&gt; {\n// reset and seed the database prior to every test\ncy.exec('npm run db:reset &amp;&amp; npm run db:seed')\n\n// seed a user in the DB that we can control from our tests\n// assuming it generates a random password for us\ncy.request('POST', '/test/seed/user', { username: 'jane.lane' })\n.its('body')\n.as('currentUser')\n})\n\nit('sets auth cookie when logging in via form submission', function () {\n// destructuring assignment of the this.currentUser object\nconst { username, password } = this.currentUser\n\ncy.visit('/login')\n\ncy.get('input[name=username]').type(username)\n\n// {enter} causes the form to submit\ncy.get('input[name=password]').type(`${password}{enter}`)\n\n// we should be redirected to /dashboard\ncy.url().should('include', '/dashboard')\n\n// our auth cookie should be present\ncy.getCookie('your-session-cookie').should('exist')\n\n// UI should reflect this user being logged in\ncy.get('h1').should('contain', 'jane.lane')\n})\n})\n</code></pre> <p>You'll likely also want to test your login UI for:</p> <ul> <li>Invalid username / password.</li> <li>Username taken.</li> <li>Password complexity requirements.</li> <li>Edge cases like locked / deleted accounts.</li> </ul> <p>Each of these likely requires a full blown e2e test, and it makes sense to go through the login process. But when you're testing another area of the system that relies on a state from a previous feature: do not use your UI to set up this state. So for these cases you'd do:</p> <p><pre><code>describe('The Dashboard Page', () =&gt; {\nbeforeEach(() =&gt; {\n// reset and seed the database prior to every test\ncy.exec('npm run db:reset &amp;&amp; npm run db:seed')\n\n// seed a user in the DB that we can control from our tests\n// assuming it generates a random password for us\ncy.request('POST', '/test/seed/user', { username: 'jane.lane' })\n.its('body')\n.as('currentUser')\n})\n\nit('logs in programmatically without using the UI', function () {\n// destructuring assignment of the this.currentUser object\nconst { username, password } = this.currentUser\n\n// programmatically log us in without needing the UI\ncy.request('POST', '/login', {\nusername,\npassword,\n})\n\n// now that we're logged in, we can visit\n// any kind of restricted route!\ncy.visit('/dashboard')\n\n// our auth cookie should be present\ncy.getCookie('your-session-cookie').should('exist')\n\n// UI should reflect this user being logged in\ncy.get('h1').should('contain', 'jane.lane')\n})\n})\n</code></pre> This saves an enormous amount of time visiting the login page, filling out the username, password, and waiting for the server to redirect us before every test.</p> <p>Because we previously tested the login system end-to-end without using any shortcuts, we already have 100% confidence it's working correctly.</p> <p>Here are other login recipes.</p>"}, {"location": "cypress/#setting-up-backend-servers-for-e2e-tests", "title": "Setting up backend servers for E2E tests", "text": "<p>Cypress team does NOT recommend trying to start your back end web server from within Cypress.</p> <p>Any command run by <code>cy.exec()</code> or <code>cy.task()</code> has to exit eventually. Otherwise, Cypress will not continue running any other commands.</p> <p>Trying to start a web server from <code>cy.exec()</code> or <code>cy.task()</code> causes all kinds of problems because:</p> <ul> <li>You have to background the process.</li> <li>You lose access to it via terminal.</li> <li>You don't have access to its stdout or logs.</li> <li>Every time your tests run, you'd have to work out the complexity around     starting an already running web server.</li> <li>You would likely encounter constant port conflicts.</li> </ul> <p>Therefore you should start your web server before running Cypress and kill it after it completes. They have examples showing you how to start and stop your web server in a CI environment.</p>"}, {"location": "cypress/#waiting", "title": "Waiting", "text": "<p>Cypress enables you to declaratively <code>cy.wait()</code> for requests and their responses.</p> <pre><code>cy.intercept('/activities/*', { fixture: 'activities' }).as('getActivities')\ncy.intercept('/messages/*', { fixture: 'messages' }).as('getMessages')\n\n// visit the dashboard, which should make requests that match\n// the two routes above\ncy.visit('http://localhost:8888/dashboard')\n\n// pass an array of Route Aliases that forces Cypress to wait\n// until it sees a response for each request that matches\n// each of these aliases\ncy.wait(['@getActivities', '@getMessages'])\n\n// these commands will not run until the wait command resolves above\ncy.get('h1').should('contain', 'Dashboard')\n</code></pre> <p>If you would like to check the response data of each response of an aliased route, you can use several <code>cy.wait()</code> calls.</p> <pre><code>cy.intercept({\nmethod: 'POST',\nurl: '/myApi',\n}).as('apiCheck')\n\ncy.visit('/')\ncy.wait('@apiCheck').then((interception) =&gt; {\nassert.isNotNull(interception.response.body, '1st API call has data')\n})\n\ncy.wait('@apiCheck').then((interception) =&gt; {\nassert.isNotNull(interception.response.body, '2nd API call has data')\n})\n\ncy.wait('@apiCheck').then((interception) =&gt; {\nassert.isNotNull(interception.response.body, '3rd API call has data')\n})\n</code></pre> <p>Waiting on an aliased route has big advantages:</p> <ul> <li>Tests are more robust with much less flake.</li> <li>Failure messages are much more precise.</li> <li>You can assert about the underlying request object.</li> </ul>"}, {"location": "cypress/#avoiding-flake-tests", "title": "Avoiding Flake tests", "text": "<p>One advantage of declaratively waiting for responses is that it decreases test flake. You can think of <code>cy.wait()</code> as a guard that indicates to Cypress when you expect a request to be made that matches a specific routing alias. This prevents the next commands from running until responses come back and it guards against situations where your requests are initially delayed.</p> <pre><code>cy.intercept('/search*', [{ item: 'Book 1' }, { item: 'Book 2' }]).as(\n'getSearch'\n)\n\n// our autocomplete field is throttled\n// meaning it only makes a request after\n// 500ms from the last keyPress\ncy.get('#autocomplete').type('Book')\n\n// wait for the request + response\n// thus insulating us from the\n// throttled request\ncy.wait('@getSearch')\n\ncy.get('#results').should('contain', 'Book 1').and('contain', 'Book 2')\n</code></pre>"}, {"location": "cypress/#assert-on-wait-content", "title": "Assert on wait content", "text": "<p>Another benefit of using <code>cy.wait()</code> on requests is that it allows you to access the actual request object. This is useful when you want to make assertions about this object.</p> <p>In our example above we can assert about the request object to verify that it sent data as a query string in the URL. Although we're mocking the response, we can still verify that our application sends the correct request.</p> <pre><code>// any request to \"/search/*\" endpoint will automatically receive\n// an array with two book objects\ncy.intercept('/search/*', [{ item: 'Book 1' }, { item: 'Book 2' }]).as(\n'getSearch'\n)\n\ncy.get('#autocomplete').type('Book')\n\n// this yields us the interception cycle object which includes\n// fields for the request and response\ncy.wait('@getSearch').its('request.url').should('include', '/search?query=Book')\n\ncy.get('#results').should('contain', 'Book 1').and('contain', 'Book 2')\n</code></pre> <p>Of the intercepted object you can check:</p> <ul> <li>URL.</li> <li>Method.</li> <li>Status Code.</li> <li>Request Body.</li> <li>Request Headers.</li> <li>Response Body.</li> <li>Response Headers.</li> </ul> <pre><code>// spy on POST requests to /users endpoint\ncy.intercept('POST', '/users').as('new-user')\n// trigger network calls by manipulating web app's user interface, then\ncy.wait('@new-user').should('have.property', 'response.statusCode', 201)\n\n// we can grab the completed interception object again to run more assertions\n// using cy.get(&lt;alias&gt;)\ncy.get('@new-user') // yields the same interception object\n.its('request.body')\n.should(\n'deep.equal',\nJSON.stringify({\nid: '101',\nfirstName: 'Joe',\nlastName: 'Black',\n})\n)\n\n// and we can place multiple assertions in a single \"should\" callback\ncy.get('@new-user').should(({ request, response }) =&gt; {\nexpect(request.url).to.match(/\\/users$/)\nexpect(request.method).to.equal('POST')\n// it is a good practice to add assertion messages\n// as the 2nd argument to expect()\nexpect(response.headers, 'response headers').to.include({\n'cache-control': 'no-cache',\nexpires: '-1',\n'content-type': 'application/json; charset=utf-8',\nlocation: '&lt;domain&gt;/users/101',\n})\n})\n</code></pre> <p>You can inspect the full request cycle object by logging it to the console</p> <pre><code>cy.wait('@new-user').then(console.log)\n</code></pre>"}, {"location": "cypress/#dont-repeat-yourself", "title": "Don't repeat yourself", "text": ""}, {"location": "cypress/#share-code-before-each-test", "title": "Share code before each test", "text": "<pre><code>describe('my form', () =&gt; {\nbeforeEach(() =&gt; {\ncy.visit('/users/new')\ncy.get('#first').type('Johnny')\ncy.get('#last').type('Appleseed')\n})\n\nit('displays form validation', () =&gt; {\ncy.get('#first').clear() // clear out first name\ncy.get('form').submit()\ncy.get('#errors').should('contain', 'First name is required')\n})\n\nit('can submit a valid form', () =&gt; {\ncy.get('form').submit()\n})\n})\n</code></pre>"}, {"location": "cypress/#parametrization", "title": "Parametrization", "text": "<p>If you want to run similar tests with different data, you can use parametrization. For example to test the same pages for different screen sizes use:</p> <pre><code>const sizes = ['iphone-6', 'ipad-2', [1024, 768]]\n\ndescribe('Logo', () =&gt; {\nsizes.forEach((size) =&gt; {\n// make assertions on the logo using\n// an array of different viewports\nit(`Should display logo on ${size} screen`, () =&gt; {\nif (Cypress._.isArray(size)) {\ncy.viewport(size[0], size[1])\n} else {\ncy.viewport(size)\n}\n\ncy.visit('https://www.cypress.io')\ncy.get('#logo').should('be.visible')\n})\n})\n})\n</code></pre>"}, {"location": "cypress/#use-functions", "title": "Use functions", "text": "<p>Sometimes, the piece of code is redundant and we don't we don't require it in all the test cases. We can create utility functions and move such code there.</p> <p>We can create a separate folder as utils in support folder and store our functions in a file in that folder.</p> <p>Consider the following example of utility function for login.</p> <pre><code>//cypress/support/utils/common.js\n\nexport const loginViaUI = (username, password) =&gt; {\ncy.get(\"[data-cy='login-email-field']\").type(username);\ncy.get(\"[data-cy='login-password-field']\").type(password);\ncy.get(\"[data-cy='submit-button']\").submit()\n}\n</code></pre> <p>This is how we can use utility function in our test case:</p> <pre><code>import {\nloginViaUI\n} from '../support/utils/common.js';\n\ndescribe(\"Login\", () =&gt; {\nit('should allow user to log in', () =&gt; {\ncy.visit('/login');\nloginViaUI('username', 'password');\n});\n});\n</code></pre> <p>Utility functions are similar to Cypress commands. If the code being used in almost every test suite, we can create a custom command for it. The benefit of this is that we don't have to import the js file to use the command, it is available directly on cy object i.e. <code>cy.loginViaUI()</code>.</p> <p>But, this doesn't mean that we should use commands for everything. If the code is used in only some of the test suite, we can create a utility function and import it whenever needed.</p>"}, {"location": "cypress/#setting-up-time-of-the-tests", "title": "Setting up time of the tests", "text": "<p>Specify a <code>now</code> timestamp</p> <pre><code>// your app code\n$('#date').text(new Date().toJSON())\n\nconst now = new Date(2017, 3, 14).getTime() // April 14, 2017 timestamp\n\ncy.clock(now)\ncy.visit('/index.html')\ncy.get('#date').contains('2017-04-14')\n</code></pre>"}, {"location": "cypress/#simulate-errors", "title": "Simulate errors", "text": "<p>End-to-end tests are excellent for testing \u201chappy path\u201d scenarios and the most important application features.</p> <p>However, there are unexpected situations, and when they occur, the application cannot completely \"break\".</p> <p>Such situations can occur due to errors on the server or the network, to name a few.</p> <p>With Cypress, we can easily simulate error situations.</p> <p>Below are examples of tests for server and network errors.</p> <pre><code>context('Errors', () =&gt; {\nconst errorMsg = 'Oops! Try again later'\n\nit('simulates a server error', () =&gt; {\ncy.intercept(\n'GET',\n'**/search?query=cypress',\n{ statusCode: 500 }\n).as('getServerFailure')\n\ncy.visit('https://example.com/search')\n\ncy.get('[data-cy=\"search-field\"]')\n.should('be.visible')\n.type('cypress{enter}')\ncy.wait('@getServerFailure')\n\ncy.contains(errorMsg)\n.should('be.visible')\n})\n\nit('simulates a network failure', () =&gt; {\ncy.intercept(\n'GET',\n'**/search?query=cypressio',\n{ forceNetworkError: true }\n).as('getNetworkFailure')\n\ncy.visit('https://example.com/search')\n\ncy.get('[data-cy=\"search-field\"]')\n.should('be.visible')\n.type('cypressio{enter}')\ncy.wait('@getNetworkFailure')\n\ncy.contais(errorMsg)\n.should('be.visible')\n})\n})\n</code></pre> <p>In the above tests, the HTTP request of type GET to the search endpoint is intercepted. In the first test, we use the <code>statusCode</code> option with the value <code>500</code>. In the second test, we use the <code>forceNewtworkError</code> option with the value of <code>true</code>. After that, you can test that the correct message is visible to the user.</p>"}, {"location": "cypress/#sending-different-responses", "title": "Sending different responses", "text": "<p>To return different responses from a single <code>GET /todos</code> intercept, you can place all prepared responses into an array, and then use Array.prototype.shift to return and remove the first item.</p> <pre><code>it('returns list with more items on page reload', () =&gt; {\nconst replies = [{ fixture: 'articles.json' }, { statusCode: 404 }]\ncy.intercept('GET', '/api/inbox', req =&gt; req.reply(replies.shift()))\n})\n</code></pre>"}, {"location": "cypress/#component-testing", "title": "Component testing", "text": "<p>Component testing in Cypress is similar to end-to-end testing. The notable differences are:</p> <ul> <li>There's no need to navigate to a URL. You don't need to call <code>cy.visit()</code> in your test.</li> <li>Cypress provides a blank canvas where we can <code>mount</code> components in isolation.</li> </ul> <p>For example:</p> <pre><code>import { mount } from '@cypress/vue'\nimport TodoList from './components/TodoList'\n\ndescribe('TodoList', () =&gt; {\nit('renders the todo list', () =&gt; {\nmount(&lt;TodoList /&gt;)\ncy.get('[data-testid=todo-list]').should('exist')\n})\n\nit('contains the correct number of todos', () =&gt; {\nconst todos = [\n{ text: 'Buy milk', id: 1 },\n{ text: 'Learn Component Testing', id: 2 },\n]\n\nmount(&lt;TodoList todos={todos} /&gt;)\n\ncy.get('[data-testid=todos]').should('have.length', todos.length)\n})\n})\n</code></pre> <p>If you are using Cypress Component Testing in a project that also has tests written with the Cypress End-to-End test runner, you may want to configure some Component Testing specific defaults.</p> <p>It doesn't yet work with vuetify</p>"}, {"location": "cypress/#install", "title": "Install", "text": "<p>Run:</p> <pre><code>npm install --save-dev cypress @cypress/vue @cypress/webpack-dev-server webpack-dev-server\n</code></pre> <p>You will also need to configure the component testing framework of your choice by installing the corresponding component testing plugin.</p> <pre><code>// cypress/plugins/index.js\n\nmodule.exports = (on, config) =&gt; {\nif (config.testingType === 'component') {\nconst { startDevServer } = require('@cypress/webpack-dev-server')\n\n// Vue's Webpack configuration\nconst webpackConfig = require('@vue/cli-service/webpack.config.js')\n\non('dev-server:start', (options) =&gt;\nstartDevServer({ options, webpackConfig })\n)\n}\n}\n</code></pre>"}, {"location": "cypress/#usage_1", "title": "Usage", "text": "<p><pre><code>// components/HelloWorld.spec.js\nimport { mount } from '@cypress/vue'\nimport { HelloWorld } from './HelloWorld.vue'\ndescribe('HelloWorld component', () =&gt; {\nit('works', () =&gt; {\nmount(HelloWorld)\n// now use standard Cypress commands\ncy.contains('Hello World!').should('be.visible')\n})\n})\n</code></pre> You can pass additional styles, css files and external stylesheets to load, see docs/styles.md for full list.</p> <pre><code>import Todo from './Todo.vue'\nconst todo = {\nid: '123',\ntitle: 'Write more tests',\n}\n\nmount(Todo, {\npropsData: { todo },\nstylesheets: [\n'https://cdnjs.cloudflare.com/ajax/libs/bulma/0.7.2/css/bulma.css',\n],\n})\n</code></pre>"}, {"location": "cypress/#visual-testing", "title": "Visual testing", "text": "<p>Cypress is a functional Test Runner. It drives the web application the way a user would, and checks if the app functions as expected: if the expected message appears, an element is removed, or a CSS class is added after the appropriate user action. Cypress does NOT see how the page actually looks though.</p> <p>You could technically write a functional test asserting the CSS properties using the have.css assertion, but these may quickly become cumbersome to write and maintain, especially when visual styles rely on a lot of CSS styles.</p> <p>Visual testing can be done through plugins that do visual regression testing, which is to take an image snapshot of the entire application under test or a specific element, and then compare the image to a previously approved baseline image. If the images are the same (within a set pixel tolerance), it is determined that the web application looks the same to the user. If there are differences, then there has been some change to the DOM layout, fonts, colors or other visual properties that needs to be investigated.</p> <p>If you want to test if your app is responsive use parametrization to have maintainable tests.</p> <p>For more information on how to do visual regression testing read this article.</p> <p>As of 2022-04-23 the most popular tools that don't depend on third party servers are:</p> <ul> <li>cypress-plugin-snapshots:     It looks to be the best plugin as it allows you to update the screenshots     directly through the Cypress interface, but it is unmaintained</li> <li>cypress-visual-regression:     Maintained but it doesn't show the differences in the cypress interface and     you have to interact with them through the command line.</li> <li>cypress-image-snapshot:     Most popular but it looks unmaintained     (1,     2)</li> </ul> <p>Check the Visual testing plugins list to see all available solutions. Beware of the third party solutions like  Percy and Applitools as they send your pictures to their servers on each test.</p>"}, {"location": "cypress/#cypress-visual-regression", "title": "<code>cypress-visual-regression</code>", "text": ""}, {"location": "cypress/#installation_1", "title": "Installation", "text": "<pre><code>npm install --save-dev cypress-visual-regression\n</code></pre> <p>Add the following config to your <code>cypress.json</code> file:</p> <pre><code>{\n\"screenshotsFolder\": \"./cypress/snapshots/actual\",\n\"trashAssetsBeforeRuns\": true\n}\n</code></pre> <p>Add the plugin to <code>cypress/plugins/index.js</code>:</p> <pre><code>const getCompareSnapshotsPlugin = require('cypress-visual-regression/dist/plugin');\n\nmodule.exports = (on, config) =&gt; {\ngetCompareSnapshotsPlugin(on, config);\n};\n</code></pre> <p>Add the command to <code>cypress/support/commands.js</code>:</p> <pre><code>const compareSnapshotCommand = require('cypress-visual-regression/dist/command');\n\ncompareSnapshotCommand();\n</code></pre> <p>Make sure you import <code>commands.js</code> in <code>cypress/support/index.js</code>:</p> <pre><code>import './commands'\n</code></pre>"}, {"location": "cypress/#use", "title": "Use", "text": "<p>Add <code>cy.compareSnapshot('home')</code> in your tests specs whenever you want to test for visual regressions, making sure to replace home with a relevant name. You can also add an optional error threshold: Value can range from 0.00 (no difference) to 1.00 (every pixel is different). So, if you enter an error threshold of 0.51, the test would fail only if &gt; 51% of pixels are different. For example:</p> <pre><code>it('should display the login page correctly', () =&gt; {\ncy.visit('/03.html');\ncy.get('H1').contains('Login');\ncy.compareSnapshot('login', 0.0);\ncy.compareSnapshot('login', 0.1);\n});\n</code></pre> <p>You can target a single HTML element as well:</p> <pre><code>cy.get('#my-header').compareSnapshot('just-header')\n</code></pre> <p>Check more examples here</p> <p>You need to take or update the base images, do it with:</p> <pre><code>npx cypress run \\\n--env type=base \\\n--config screenshotsFolder=cypress/snapshots/base,testFiles=\\\"**/*regression-tests.js\\\"\n</code></pre> <p>To find regressions run:</p> <pre><code>npx cypress run --env type=actual\n</code></pre> <p>Or if you want to just check a subset of tests use:</p> <pre><code>npx cypress run --env type=actual --spec \"cypress\\integration\\visual-tests.spec.js\"\nnpx cypress run --env type=actual --spec \"cypress\\integration\\test1.spec.js\",\"cypress\\integration\\test2.spec.js\"\nnpx cypress run --env type=actual --spec \"cypress\\integration\\**\\*.spec.js\n</code></pre>"}, {"location": "cypress/#third-party-component-testing", "title": "Third party component testing", "text": "<p>Other examples of testing third party components</p> <ul> <li>Testing HTML emails</li> </ul>"}, {"location": "cypress/#configuration", "title": "Configuration", "text": "<p>Cypress saves it's configuration in the <code>cypress.json</code> file.</p> <pre><code>{\n\"baseUrl\": \"http://localhost:8080\"\n}\n</code></pre> <p>Where:</p> <ul> <li><code>baseUrl</code>: Will be prefixed on <code>cy.visit()</code> and <code>cy.requests()</code>.</li> </ul>"}, {"location": "cypress/#environment-variables", "title": "Environment variables", "text": "<p>Environment variables are useful when:</p> <ul> <li>Values are different across developer machines.</li> <li>Values are different across multiple environments: (dev, staging, qa, prod).</li> <li>Values change frequently and are highly dynamic.</li> </ul> <p>Instead of hard coding this in your tests:</p> <pre><code>cy.request('https://api.acme.corp') // this will break on other environments\n</code></pre> <p>We can move this into a Cypress environment variable:</p> <pre><code>cy.request(Cypress.env('EXTERNAL_API')) // points to a dynamic env var\n</code></pre> <p>Any key/value you set in your configuration file under the <code>env</code> key will become an environment variable.</p> <pre><code>{\n\"projectId\": \"128076ed-9868-4e98-9cef-98dd8b705d75\",\n\"env\": {\n\"login_url\": \"/login\",\n\"products_url\": \"/products\"\n}\n}\n</code></pre> <p>To access it use:</p> <pre><code>Cypress.env() // {login_url: '/login', products_url: '/products'}\nCypress.env('login_url') // '/login'\nCypress.env('products_url') // '/products'\n</code></pre>"}, {"location": "cypress/#configure-component-testing", "title": "Configure component testing", "text": "<p>You can configure or override Component Testing defaults in your configuration file using the <code>component</code> key.</p> <pre><code>{\n\"testFiles\": \"cypress/integration/*.spec.js\",\n\"component\": {\n\"componentFolder\": \"src\",\n\"testFiles\": \".*/__tests__/.*spec.tsx\",\n\"viewportHeight\": 500,\n\"viewportWidth\": 700\n}\n}\n</code></pre>"}, {"location": "cypress/#debugging", "title": "Debugging", "text": ""}, {"location": "cypress/#using-the-debugger", "title": "Using the debugger", "text": "<p>Use the <code>.debug()</code> command directly BEFORE the action.</p> <pre><code>// break on a debugger before the action command\ncy.get('button').debug().click()\n</code></pre>"}, {"location": "cypress/#step-through-test-commands", "title": "Step through test commands", "text": "<p>You can run the test command by command using the <code>.pause()</code> command.</p> <pre><code>it('adds items', () =&gt; {\ncy.pause()\ncy.get('.new-todo')\n// more commands\n})\n</code></pre> <p>This allows you to inspect the web application, the DOM, the network, and any storage after each command to make sure everything happens as expected.</p>"}, {"location": "cypress/#issues", "title": "Issues", "text": "<ul> <li>Allow rerun only failed     tests: Until it's ready     use <code>it.only</code> on the test you want to run.</li> </ul>"}, {"location": "cypress/#references", "title": "References", "text": "<ul> <li>Home</li> <li>Git</li> <li>Examples of usage</li> <li>Cypress API</li> <li>Real World Application Cypress testing example</li> <li>Tutorial on writing     tests</li> <li>Video tutorials</li> </ul>"}, {"location": "diffview/", "title": "Diffview", "text": "<p>Diffview is a single tabpage interface for easily cycling through diffs for all modified files for any git rev.</p>"}, {"location": "diffview/#installation", "title": "Installation", "text": "<p>If you're using it with NeoGit and Packer use:</p> <pre><code>  use {\n    'NeogitOrg/neogit',\n    requires = {\n      'nvim-lua/plenary.nvim',\n      'sindrets/diffview.nvim',\n      'nvim-tree/nvim-web-devicons'\n    }\n  }\n</code></pre>"}, {"location": "diffview/#usage", "title": "Usage", "text": ""}, {"location": "diffview/#diffviewopen", "title": "DiffviewOpen", "text": "<p>Calling <code>:DiffviewOpen</code> with no args opens a new <code>Diffview</code> that compares against the current index. You can also provide any valid git rev to view only changes for that rev.</p> <p>Examples:</p> <ul> <li><code>:DiffviewOpen</code></li> <li><code>:DiffviewOpen HEAD~2</code></li> <li><code>:DiffviewOpen HEAD~4..HEAD~2</code></li> <li><code>:DiffviewOpen d4a7b0d</code></li> <li><code>:DiffviewOpen d4a7b0d^!</code></li> <li><code>:DiffviewOpen d4a7b0d..519b30e</code></li> <li><code>:DiffviewOpen origin/main...HEAD</code></li> </ul> <p>You can also provide additional paths to narrow down what files are shown <code>:DiffviewOpen HEAD~2 -- lua/diffview plugin</code>.</p> <p>Additional commands for convenience:</p> <ul> <li><code>:DiffviewClose</code>: Close the current diffview. You can also use <code>:tabclose</code>.</li> <li><code>:DiffviewToggleFiles</code>: Toggle the file panel.</li> <li><code>:DiffviewFocusFiles</code>: Bring focus to the file panel.</li> <li><code>:DiffviewRefresh</code>: Update stats and entries in the file list of the current Diffview.</li> </ul> <p>With a Diffview open and the default key bindings, you can:</p> <ul> <li>Cycle through changed files with <code>&lt;tab&gt;</code> and <code>&lt;s-tab&gt;</code></li> <li>You can stage changes with <code>-</code></li> <li>Restore a file with <code>X</code></li> <li>Refresh the diffs with <code>R</code></li> <li>Go to the file panel with <code>&lt;leader&gt;e</code></li> </ul>"}, {"location": "diffview/#tips", "title": "Tips", "text": ""}, {"location": "diffview/#use-the-same-binding-to-open-and-close-the-diffview-windows", "title": "Use the same binding to open and close the diffview windows", "text": "<pre><code>vim.keymap.set('n', 'dv', function()\n  if next(require('diffview.lib').views) == nil then\n    vim.cmd('DiffviewOpen')\n  else\n    vim.cmd('DiffviewClose')\n  end\nend)\n</code></pre>"}, {"location": "diffview/#troubleshooting", "title": "Troubleshooting", "text": ""}, {"location": "diffview/#no-valid-vcs-tool-found", "title": "No valid VCS tool found", "text": "<p>It may be because you have an outdated version of git. To fix it update to the latest one, if it's still not enough, install it from the backports repo</p>"}, {"location": "diffview/#references", "title": "References", "text": "<ul> <li>Source</li> </ul>"}, {"location": "digital_garden/", "title": "Digital Garden", "text": "<p>Digital Garden is a method of storing  and maintaining knowledge in an maintainable, scalable and searchable way. They  are also known as second brains.</p> <p>Unlike in common blogging where you write an article and forget about it, posts are treated as plants in various stages of growth and nurturing. Some might wither and die, and others will flourish and provide a source of continued knowledge for the gardener and folks in the community that visit.</p> <p>The content is diverse, you can find ideas, articles, investigations, snippets, resources, thoughts, collections, and other bits and pieces that I find interesting and useful.</p> <p>It's my personal Stock, the content that\u2019s as interesting in two months (or two years) as it is today. It\u2019s what people discover via search. It\u2019s what spreads slowly but surely, building fans over time.</p> <p>They are a metaphor for thinking about writing and creating that focuses less on the resulting \"showpiece\" and more on the process, care, and craft it takes to get there.</p>"}, {"location": "digital_garden/#existing-digital-gardens", "title": "Existing digital gardens", "text": "<p>If you look for inspiration check my favourite digital gardens:</p> <ul> <li>Nikita's Everything I know: It's awesome     both in content quality and length, as the way he presents it.</li> <li>Gwern's site: The way he presents content is     unique and gorgeous. I've found myself not very hooked to the content, but     when you find something you like it's awesome, such as the about     page, his article on     spaced repetition or the essay     of Death Note: L, Anonymity &amp; Eluding     Entropy.</li> </ul> <p>Or browse the following lists:</p> <ul> <li>Best-of Digital gardens</li> <li>Maggie Appleton's compilation</li> <li>Nikita's     compilation</li> <li>Richard Litt's compilation</li> <li>KasperZutterman's compilation</li> </ul> <p>Or the digital garden's reddit.</p>"}, {"location": "digital_garden/#references", "title": "References", "text": "<ul> <li>Joel Hooks article on Digital Gardens</li> <li>Tom Critchlow article on Digital Gardens</li> </ul>"}, {"location": "diversity/", "title": "Diversity", "text": "<p>Diversity, equity, and inclusion (DEI) can be defined as:</p> <ul> <li> <p>Diversity is the representation and acknowledgement of the multitudes of     identities, experiences, and ways of moving through the world. This     includes\u2014but is not limited to\u2014ability, age, citizenship status, criminal     record and/or incarceration, educational attainment, ethnicity, gender,     geographical location, language, nationality, political affiliation,     religion, race, sexuality, socioeconomic status, and veteran status.     Further, we recognize that each individual's experience is informed by     intersections across multiple identities.</p> </li> <li> <p>Equity  seeks to ensure respect and equal opportunity for all, using all     resources and tools to elevate the voices of under-represented and/or     disadvantaged groups.</p> </li> <li> <p>Inclusion is fostering an environment in which people of all identities are     welcome, valued, and supported. An inclusive organization solicits, listens     to, learns from, and acts on the contributions of all its stakeholders.</p> </li> </ul>"}, {"location": "diversity/#references", "title": "References", "text": "<ul> <li>Pulitzer center DEI     page</li> <li>Journalist's Toolbox DEI links</li> </ul>"}, {"location": "docker/", "title": "Docker", "text": "<p>Docker is a set of platform as a service (PaaS) products that use OS-level virtualization to deliver software in packages called containers. Containers are isolated from one another and bundle their own software, libraries and configuration files; they can communicate with each other through well-defined channels. Because all of the containers share the services of a single operating system kernel, they use fewer resources than virtual machines.</p>"}, {"location": "docker/#installation", "title": "Installation", "text": "<p>Follow these instructions</p> <p>If that doesn't install the version of <code>docker-compose</code> that you want use the next snippet:</p> <pre><code>VERSION=$(curl --silent https://api.github.com/repos/docker/compose/releases/latest | grep -Po '\"tag_name\": \"\\K.*\\d')\nDESTINATION=/usr/local/bin/docker-compose\nsudo curl -L https://github.com/docker/compose/releases/download/${VERSION}/docker-compose-$(uname -s)-$(uname -m) -o $DESTINATION\nsudo chmod 755 $DESTINATION\n</code></pre> <p>If you don't want the latest version set the <code>VERSION</code> variable.</p>"}, {"location": "docker/#how-to-keep-containers-updated", "title": "How to keep containers updated", "text": ""}, {"location": "docker/#with-renovate", "title": "With Renovate", "text": "<p>Renovate is a program that does automated dependency updates. Multi-platform and multi-language.</p>"}, {"location": "docker/#with-watchtower", "title": "With Watchtower", "text": "<p>With watchtower you can update the running version of your containerized app simply by pushing a new image to the Docker Hub or your own image registry. Watchtower will pull down your new image, gracefully shut down your existing container and restart it with the same options that were used when it was deployed initially.</p> <p>Run the watchtower container with the next command:</p> <pre><code>docker run -d \\\n--name watchtower \\\n-v /var/run/docker.sock:/var/run/docker.sock \\\n-v /etc/localtime:/etc/localtime:ro \\\n-e WATCHTOWER_NOTIFICATIONS=email \\\n-e WATCHTOWER_NOTIFICATION_EMAIL_FROM={{ email.from }} \\\n-e WATCHTOWER_NOTIFICATION_EMAIL_TO={{ email.to }} \\\\\n-e WATCHTOWER_NOTIFICATION_EMAIL_SERVER=mail.riseup.net \\\n-e WATCHTOWER_NOTIFICATION_EMAIL_SERVER_PORT=587 \\\n-e WATCHTOWER_NOTIFICATION_EMAIL_SERVER_USER={{ email.user }} \\\n-e WATCHTOWER_NOTIFICATION_EMAIL_SERVER_PASSWORD={{ email.password }} \\\n-e WATCHTOWER_NOTIFICATION_EMAIL_DELAY=2 \\\ncontainrrr/watchtower:latest --no-restart --no-startup-message\n</code></pre> <p>Use the <code>--no-restart</code> flag if you use systemd to manage the dockers, and <code>--no-startup-message</code> if you don't want watchtower to send you an email each time it starts the update process.</p> <p>Keep in mind that if the containers don't have good migration scripts, upgrading may break the service. To enable this feature, make sure you have frequent backups and a tested rollback process. If you're not sure one of the containers is going to behave well, you can only monitor it or disable it by using docker labels.</p> <p>The first check will be done by default in the next 24 hours, to check that everything works use the <code>--run-once</code> flag.</p> <p>Another alternative is Diun, which is a CLI application written in Go and delivered as a single executable (and a Docker image) to receive notifications when a Docker image is updated on a Docker registry.</p> <p>They don't yet support Prometheus metrics but it surely looks promising.</p>"}, {"location": "docker/#logging-in-automatically", "title": "Logging in automatically", "text": "<p>To log in automatically without entering the password, you need to have the password stored in your personal password store (not in root's!), imagine it's in the <code>dockerhub</code> entry. Then you can use:</p> <pre><code>pass show dockerhub | docker login --username foo --password-stdin\n</code></pre>"}, {"location": "docker/#override-entrypoint", "title": "Override entrypoint", "text": "<pre><code>sudo docker run -it --entrypoint /bin/bash [docker_image]\n</code></pre>"}, {"location": "docker/#snippets", "title": "Snippets", "text": ""}, {"location": "docker/#add-healthcheck-to-your-dockers", "title": "Add healthcheck to your dockers", "text": "<p>Health checks allow a container to expose its workload\u2019s availability. This stands apart from whether the container is running. If your database goes down, your API server won\u2019t be able to handle requests, even though its Docker container is still running.</p> <p>This makes for unhelpful experiences during troubleshooting. A simple <code>docker ps</code> would report the container as available. Adding a health check extends the <code>docker ps</code> output to include the container\u2019s true state.</p> <p>You configure container health checks in your Dockerfile. This accepts a command which the Docker daemon will execute every 30 seconds. Docker uses the command\u2019s exit code to determine your container\u2019s healthiness:</p> <ul> <li><code>0</code>: The container is healthy and working normally.</li> <li><code>1</code>: The container is unhealthy; the workload may not be functioning.</li> </ul> <p>Healthiness isn\u2019t checked straightaway when containers are created. The status will show as starting before the first check runs. This gives the container time to execute any startup tasks. A container with a passing health check will show as healthy; an unhealthy container displays unhealthy.</p> <p>In docker-compose you can write the healthchecks like the next snippet:</p> <pre><code>---\nversion: '3.4'\n\nservices:\njellyfin:\nimage: linuxserver/jellyfin:latest\ncontainer_name: jellyfin\nrestart: unless-stopped\nhealthcheck:\ntest: curl http://localhost:8096/health || exit 1\ninterval: 10s\nretries: 5\nstart_period: 5s\ntimeout: 10s\n</code></pre>"}, {"location": "docker/#list-the-dockers-of-a-registry", "title": "List the dockers of a registry", "text": "<p>List all repositories (effectively images):</p> <pre><code>$: curl -X GET https://myregistry:5000/v2/_catalog\n&gt; {\"repositories\":[\"redis\",\"ubuntu\"]}\n</code></pre> <p>List all tags for a repository:</p> <pre><code>$: curl -X GET https://myregistry:5000/v2/ubuntu/tags/list\n&gt; {\"name\":\"ubuntu\",\"tags\":[\"14.04\"]}\n</code></pre> <p>If the registry needs authentication you have to specify username and password in the curl command</p> <pre><code>curl -X GET -u &lt;user&gt;:&lt;pass&gt; https://myregistry:5000/v2/_catalog\ncurl -X GET -u &lt;user&gt;:&lt;pass&gt; https://myregistry:5000/v2/ubuntu/tags/list\n</code></pre>"}, {"location": "docker/#attach-a-docker-to-many-networks", "title": "Attach a docker to many networks", "text": "<p>You can't do it through the <code>docker run</code> command, there you can only specify one network. However, you can attach a docker to a network with the command:</p> <pre><code>docker network attach network-name docker-name\n</code></pre>"}, {"location": "docker/#get-the-output-of-docker-ps-as-a-json", "title": "Get the output of <code>docker ps</code> as a json", "text": "<p>To get the complete json for reference.</p> <pre><code>docker ps -a --format \"{{json .}}\" | jq -s\n</code></pre> <p>To get only the required columns in the output with tab separated version</p> <pre><code>docker ps -a --format \"{{json .}}\" | jq -r -c '[.ID, .State, .Names, .Image]'\n</code></pre> <p>To get also the image's ID you can use:</p> <pre><code>docker inspect --format='{{json .}}' $(docker ps -aq) | jq -r -c '[.Id, .Name, .Config.Image, .Image]'\n</code></pre>"}, {"location": "docker/#connect-multiple-docker-compose-files", "title": "Connect multiple docker compose files", "text": "<p>You can connect services defined across multiple docker-compose.yml files.</p> <p>In order to do this you\u2019ll need to:</p> <ul> <li>Create an external network with <code>docker network create &lt;network name&gt;</code></li> <li>In each of your <code>docker-compose.yml</code> configure the default network to use your     externally created network with the networks top-level key.</li> <li>You can use either the service name or container name to connect between containers.</li> </ul> <p>Let's do it with an example:</p> <ul> <li> <p>Creating the network</p> <pre><code>$ docker network create external-example\n2af4d92c2054e9deb86edaea8bb55ecb74f84a62aec7614c9f09fee386f248a6\n</code></pre> </li> <li> <p>Create the first docker-compose file</p> <pre><code>version: '3'\nservices:\nservice1:\nimage: busybox\ncommand: sleep infinity\n\nnetworks:\ndefault:\nexternal:\nname: external-example\n</code></pre> </li> <li> <p>Bring the service up</p> <pre><code>$ docker-compose up -d\nCreating compose1_service1_1 ... done\n</code></pre> </li> <li> <p>Create the second docker-compose file with network configured</p> <pre><code>version: '3'\nservices:\nservice2:\nimage: busybox\ncommand: sleep infinity\n\nnetworks:\ndefault:\nexternal:\nname: external-example\n</code></pre> </li> <li> <p>Bring the service up</p> <pre><code>$ docker-compose up -d\nCreating compose2_service2_1 ... done\n</code></pre> </li> </ul> <p>After running <code>docker-compose up -d</code> on both docker-compose.yml files, we see that no new networks were created.</p> <pre><code>$ docker network ls\nNETWORK ID          NAME                DRIVER              SCOPE\n25e0c599d5e5        bridge              bridge              local\n2af4d92c2054        external-example    bridge              local\n7df4631e9cff        host                host                local\n194d4156d7ab        none                null                local\n</code></pre> <p>With the containers using the external-example network, they are able to ping one another.</p> <pre><code># By service name\n$ docker exec -it compose1_service1_1 ping service2\nPING service2 (172.24.0.3): 56 data bytes\n64 bytes from 172.24.0.3: seq=0 ttl=64 time=0.054 ms\n^C\n--- service2 ping statistics ---\n1 packets transmitted, 1 packets received, 0% packet loss\nround-trip min/avg/max = 0.054/0.054/0.054 ms\n\n# By container name\n$ docker exec -it compose1_service1_1 ping compose2_service2_1\nPING compose2_service2_1 (172.24.0.2): 56 data bytes\n64 bytes from 172.24.0.2: seq=0 ttl=64 time=0.042 ms\n^C\n--- compose2_service2_1 ping statistics ---\n1 packets transmitted, 1 packets received, 0% packet loss\nround-trip min/avg/max = 0.042/0.042/0.042 ms\n</code></pre> <p>The other way around works too.</p>"}, {"location": "docker/#remove-the-apt-cache-after-installing-a-package", "title": "Remove the apt cache after installing a package", "text": "<pre><code>RUN apt-get update &amp;&amp; apt-get install -y \\\n  python3 \\\n  python3-pip \\\n  &amp;&amp; rm -rf /var/lib/apt/lists/*\n</code></pre>"}, {"location": "docker/#add-the-contents-of-a-directory-to-the-docker", "title": "Add the contents of a directory to the docker", "text": "<pre><code>ADD ./path/to/directory /path/to/destination\n</code></pre>"}, {"location": "docker/#append-a-new-path-to-path", "title": "Append a new path to PATH", "text": "<pre><code>ENV PATH=\"${PATH}:/opt/gtk/bin\"\n</code></pre>"}, {"location": "docker/#troubleshooting", "title": "Troubleshooting", "text": "<p>If you are using a VPN and docker, you're going to have a hard time.</p> <p>The <code>docker</code> systemd service logs <code>systemctl status docker.service</code> usually doesn't give much information. Try to start the daemon directly with <code>sudo /usr/bin/dockerd</code>.</p>"}, {"location": "docker/#dont-store-credentials-in-plaintext", "title": "Don't store credentials in plaintext", "text": "<p>It doesn't work, don't go this painful road and assume that docker is broken.</p> <p>The official steps are horrible, and once you've spent two hours debugging it, you won't be able to push or pull images with your user.</p> <p>When you use <code>docker login</code> and introduce the user and password you get the next warning:</p> <pre><code>WARNING! Your password will be stored unencrypted in /root/.docker/config.json.\nConfigure a credential helper to remove this warning. See\nhttps://docs.docker.com/engine/reference/commandline/login/#credentials-store\n</code></pre> <p>I got a nice surprise when I saw that <code>pass</code> was suggested in the link of the warning, to be used as a backend to store the password. But that feeling soon faded.</p> <p>To make docker understand that you want to use <code>pass</code> you need to use the <code>docker-credential-pass</code> script. A Go script \"maintained\" by docker, whose last commit was two years ago , has the CI broken and many old unanswered issues. Setting it up it's not easy either and it's ill documented.</p> <p>Furthermore, the script doesn't do what I expected, which is to store the password of your registry user in a pass entry. Instead, you need to create an empty pass entry in <code>docker-credential-helpers/docker-pass-initialized-check</code>, and when you use <code>docker login</code>, manually introducing your data, it creates another entry, as you can see in the next <code>pass</code> output:</p> <pre><code>Password Store\n\u2514\u2500\u2500 docker-credential-helpers\n    \u251c\u2500\u2500 aHR0cHM6Ly9pbmRleC5kb2NrZXIuaW8vdjEv\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 lyz\n    \u2514\u2500\u2500 docker-pass-initialized-check\n</code></pre> <p>That entry is removed when you use <code>docker logout</code> so the next time you log in you need to introduce the user and password <code>(\u256f\u00b0\u25a1\u00b0)\u256f \u253b\u2501\u253b</code>.</p>"}, {"location": "docker/#installing-docker-credential-pass", "title": "Installing docker-credential-pass", "text": "<p>You first need to install the script:</p> <pre><code># Check for later releases at https://github.com/docker/docker-credential-helpers/releases\nversion=\"v0.6.3\"\narchive=\"docker-credential-pass-$version-amd64.tar.gz\"\nurl=\"https://github.com/docker/docker-credential-helpers/releases/download/$version/$archive\"\n\n# Download cred helper, unpack, make executable, and move it where Docker will find it.\nwget $url \\\n&amp;&amp; tar -xf $archive \\\n&amp;&amp; chmod +x docker-credential-pass \\\n&amp;&amp; mv -f docker-credential-pass /usr/local/bin/\n</code></pre> <p>Another tricky issue is that even if you use a non-root user who's part of the <code>docker</code> group, the script is not aware of that, so it will look in the password store of root instead of the user's. This means that additionally to your own, you need to create a new password store for root. Follow the next steps with the root user:</p> <ul> <li>Create the password with <code>gpg --full-gen</code>, and copy the key id. Use a non     empty password, otherwise you are getting the same security as with the     password in cleartext.</li> <li>Initialize the password store <code>pass init gpg_id</code>, changing <code>gpg_id</code> for the     one of the last step.</li> <li> <p>Create the empty <code>docker-credential-helpers/docker-pass-initialized-check</code>     entry:</p> <pre><code>pass insert docker-credential-helpers/docker-pass-initialized-check\n</code></pre> <p>And press enter twice.</p> </li> </ul> <p>Finally we need to specify in the root's docker configuration that we want to use the <code>pass</code> credential storage.</p> <p>File: /root/.docker/config.json</p> <pre><code>{\n\"credsStore\": \"pass\"\n}\n</code></pre>"}, {"location": "docker/#testing-it-works", "title": "Testing it works", "text": "<p>To test that docker is able to use pass as backend to store the credentials, run <code>docker login</code> and introduce the user and password. You should see the <code>Login Succeeded</code> message without any warning.</p> <pre><code>Login with your Docker ID to push and pull images from Docker Hub. If you don't have a Docker ID, head over to https://hub.docker.com to create one.\nUsername: lyz\nPassword:\nLogin Succeeded\n</code></pre> <p>Awful experience, wasn't it? Don't worry it gets worse.</p> <p>Now that you're logged in, whenever you try to push an image you're probably going to get an <code>denied: requested access to the resource is denied</code> error. That's because docker is not able to use the password it has stored in the root's password store. If you're using <code>root</code> to push the image (bad idea anyway), you will need to <code>export GPG_TTY=$(tty)</code> so that docker can ask you for your password to unlock root's <code>pass</code> entry. If you're like me that uses a non-root user belonging to the <code>docker</code> group, not even that works, so you've spent all this time reading and trying to fix everything for nothing... Thank you Docker <code>-.-</code>.</p>"}, {"location": "docker/#start-request-repeated-too-quickly", "title": "Start request repeated too quickly", "text": "<p>Shutdown the VPN and it will work. If it doesn't inspect the output of <code>journalctl -eu docker</code>.</p>"}, {"location": "docker/#disable-ipv6", "title": "Disable ipv6", "text": "<pre><code>sysctl net.ipv6.conf.all.disable_ipv6=1\nsysctl net.ipv6.conf.default.disable_ipv6=1\n</code></pre>"}, {"location": "documentation/", "title": "Write good documentation", "text": "<p>It doesn't matter how good your program is, because if its documentation is not good enough, people will not use it. Even if they have to use it because they have no choice, without good documentation, they won\u2019t use it effectively or the way you\u2019d like them to.</p> <p>People working with software need different kinds of documentation at different times, in different circumstances, so good software documentation needs them all.</p> <p>They first need to get started and see how to solve specific problems. Then they need a way to search the software possibilities in a reference. Finally when they hit a road block, they need to understand how everything works so they can solve it.</p> <p>Each of these sections must be clearly differenced and the writing style must be adapted. The five types of documentation are:</p> <ul> <li>Introduction: A short description with optional pictures or screen casts,     that catches the user's attention and makes them want to use it. Like     the advertisement of your program.</li> <li>Get started: Lessons that allows the newcomer learn how to     start using the software. Like teaching a small child how to cook.</li> <li>How-to guides: Series of steps that show how to solve a specific problem. Like     a recipe in a cookery book.</li> <li>Technical reference: Searchable and organized dry     description of the software's machinery. Like a reference encyclopedia     article.</li> <li>Background information: Discursive explanations     that makes the user understand how the software works and how has it     evolved. Like an article on culinary social history.</li> </ul> <p>This division makes it obvious to both author and reader what material, and what kind of material, goes where. It tells the author how to write, what to write, and where to write it.</p>"}, {"location": "documentation/#introduction", "title": "Introduction", "text": "<p>The introduction is the first gateway for the users to your program, as such, it needs to be eye-catching, otherwise they will walk pass it to one of the other thousand programs or libraries out there.</p> <p>It needs to start with a short phrase that defines the whole project in a way that catches the user's attention.</p> <p>If the short phrase doesn't give enough context, you can add a small paragraph with further information. But don't make it too long, human's attention is weak.</p> <p>It's also a good idea to add a screenshot or screencast showing the usage of the program.</p> <p>Optionally, you can also add a list of features that differentiate your solution from the rest.</p>"}, {"location": "documentation/#get-started", "title": "Get started", "text": "<p>Made of tutorials that take the reader by the hand through a series of steps to complete a meaningful project achievable for a complete beginner. They are what your project needs in order to show a beginner that they can achieve something with it.</p> <p>Tutorials are what will turn your learners into users. A bad or missing tutorial will prevent your project from acquiring new users.</p> <p>They need to be useful for the beginner, easy to follow, meaningful, extremely robust, and kept up-to-date. You might well find that writing and maintaining your tutorials can occupy as much time and energy as the other four parts put together.</p>"}, {"location": "documentation/#how-to-write-good-tutorials", "title": "How to write good tutorials", "text": ""}, {"location": "documentation/#allow-the-user-to-learn-by-doing", "title": "Allow the user to learn by doing", "text": "<p>Your learner needs to do things. The different things that they do while following your tutorial need to cover a wide range of tools and operations, building up from the simplest ones at the start to more complex ones.</p>"}, {"location": "documentation/#get-the-user-started", "title": "Get the user started", "text": "<p>It\u2019s perfectly acceptable if your beginner\u2019s first steps are hand-held baby steps. It\u2019s also good if what you get the beginner to do is not the way an experienced person would, or even if it\u2019s not the \u2018correct\u2019 way.</p> <p>The point of a tutorial is to get your learner started on their journey, not to get them to a final destination.</p>"}, {"location": "documentation/#make-sure-that-your-tutorial-works", "title": "Make sure that your tutorial works", "text": "<p>One of your jobs as a tutor is to inspire the beginner\u2019s confidence: in the software, in the tutorial, in the tutor, and in their own ability to achieve what\u2019s being asked of them.</p> <p>There are many things that contribute to this. A friendly tone helps, as does consistent use of language, and a logical progression through the material. But the single most important thing is that what you ask the beginner to do must work.</p> <p>If the learner\u2019s actions produce an error or unexpected results, your tutorial has failed. When your students are there with you, you can rescue them; if they\u2019re reading your documentation on their own you can\u2019t. So you have to prevent that from happening in advance.</p> <p>One way of achieving this is by adding the snippets in your documentation to the test suite.</p>"}, {"location": "documentation/#ensure-the-user-sees-results-immediately", "title": "Ensure the user sees results immediately", "text": "<p>Everything the learner does should accomplish something comprehensible, however small. If your student has to do strange and incomprehensible things for two pages before they even see a result, that\u2019s much too long. The effect of every action should be visible and evident as soon as possible, and the connection to the action should be clear.</p> <p>The conclusion of each section of a tutorial, or the tutorial as a whole, must be a meaningful accomplishment.</p>"}, {"location": "documentation/#focus-on-concrete-steps-not-abstract-concepts", "title": "Focus on concrete steps, not abstract concepts", "text": "<p>Tutorials need to be concrete, built around specific, particular actions and outcomes.</p> <p>The temptation to introduce abstraction is huge; it is after all how most computing derives its power. But all learning proceeds from the particular and concrete to the general and abstract, and asking the learner to appreciate levels of abstraction before they have even had a chance to grasp the concrete is poor teaching.</p>"}, {"location": "documentation/#provide-the-minimum-necessary-explanation", "title": "Provide the minimum necessary explanation", "text": "<p>Don\u2019t explain anything the learner doesn\u2019t need to know in order to complete the tutorial. Extended discussion is important, just not in a tutorial. In a tutorial, it is an obstruction and a distraction. Only the bare minimum is appropriate. Instead, link to explanations elsewhere in the documentation.</p>"}, {"location": "documentation/#focus-only-on-the-steps-the-user-needs-to-take", "title": "Focus only on the steps the user needs to take", "text": "<p>Your tutorial needs to be focused on the task in hand. Maybe the command you\u2019re introducing has many other options, or maybe there are different ways to access a certain API. It doesn\u2019t matter: right now, your learner does not need to know about those in order to make progress.</p>"}, {"location": "documentation/#how-to-guides", "title": "How-to guides", "text": ""}, {"location": "documentation/#technical-reference", "title": "Technical reference", "text": ""}, {"location": "documentation/#background-information", "title": "Background information", "text": ""}, {"location": "documentation/#references", "title": "References", "text": "<ul> <li>divio's documentation wiki</li> <li>Vue's guidelines</li> <li>FastAPI awesome docs</li> </ul>"}, {"location": "dotdrop/", "title": "Dotdrop", "text": "<p>The main idea of Dotdropis to have the ability to store each dotfile only once and deploy them with a different content on different hosts/setups. To achieve this, it uses a templating engine that allows to specify, during the dotfile installation with dotdrop, based on a selected profile, how (with what content) each dotfile will be installed.</p> <p>What I like:</p> <ul> <li>Popular</li> <li>Actively maintained</li> <li>Written in Python</li> <li>Uses jinja2</li> <li>Has a nice to read config file</li> </ul> <p>What I don't like:</p> <ul> <li>Updating dotfiles doesn't look as smooth as with chezmoi</li> <li>Uses <code>{{@@ @@}}</code> instead of <code>{{ }}</code> :S</li> <li>Doesn't support <code>pass</code>.</li> <li>Not easy way to edit the files.</li> </ul>"}, {"location": "dotdrop/#references", "title": "References", "text": "<ul> <li>Source</li> <li>Docs</li> <li>Blog post</li> </ul>"}, {"location": "dotfiles/", "title": "Dotfiles", "text": "<p>User-specific application configuration is traditionally stored in so called dotfiles (files whose filename starts with a dot). It is common practice to track dotfiles with a version control system such as Git to keep track of changes and synchronize dotfiles across various hosts. There are various approaches to managing your dotfiles (e.g. directly tracking dotfiles in the home directory v.s. storing them in a subdirectory and symlinking/copying/generating files with a shell script or a dedicated tool).</p> <p>Note: this is not meant to configure files that are outside your home directory, use Ansible for that use case.</p>"}, {"location": "dotfiles/#tracking-dotfiles-directly-with-git", "title": "Tracking dotfiles directly with Git", "text": "<p>The benefit of tracking dotfiles directly with Git is that it only requires Git and does not involve symlinks. The disadvantage is that host-specific configuration generally requires merging changes into multiple branches.</p> <pre><code>$ git init --bare ~/.dotfiles\n$ alias config='/usr/bin/git --git-dir=$HOME/.dotfiles/ --work-tree=$HOME'\n$ config config status.showUntrackedFiles no\n</code></pre>"}, {"location": "dotfiles/#host-specific-configuration", "title": "Host-specific configuration", "text": "<p>A common problem with synchronizing dotfiles across various machines is host-specific configuration.</p> <p>With Git this can be solved by maintaining a main branch for all shared configuration, while each individual machine has a machine-specific branch checked out. Host-specific configuration can be committed to the machine-specific branch; when shared configuration is modified in the master branch, the per-machine branches need to be rebased on top of the updated master.</p> <p>If you find rebasing Git branches too cumbersome, you may want to use a tool that supports file grouping, or if even greater flexibility is desired, a tool that does processing.</p>"}, {"location": "dotfiles/#using-ansible-to-manage-the-dotfiles", "title": "Using Ansible to manage the dotfiles", "text": "<p>Ansible gives you a lot of freedom on how to configure complex devices, and I've use it for a while, creating my own roles for each application with tests, it was beautiful to see.</p> <p>It wasn't so pleasant to use or maintain because:</p> <ul> <li> <p>Every time you update something you need to:</p> </li> <li> <p>Change the files manually until you get the new state of the files</p> </li> <li>Copy the files to the ansible-playbook repo</li> <li>Apply the changes</li> </ul> <p>Alternatively you can do the changes directly on the playbook repo, but then   you'd need to run the <code>apply</code> many more times, and it's slow, so in the end   you don't do it.</p> <ul> <li> <p>If you want to try a new tool but you're not sure you want it either you add   it to the playbook and then remove it (waste of time), or play with the tool   and then once your finished add it to the playbook. This last approach didn't   work for me. It's like writing the docs after you've finished coding, you just   don't do it, you don't have energy left and go to the next thing.</p> </li> <li> <p>Most of the applications that use dotfiles are similarly configured, so   ansible is an overkill for them. dotfiles tools are much better because you'd   spend less time configuring it and the result is the same.</p> </li> </ul>"}, {"location": "dotfiles/#tools", "title": "Tools", "text": "Name Written in File grouping Processing Stars chezmoi Go directory-based Go templates 8.2k dot-templater Rust directory-based custom syntax dotdrop Python configuration file Jinja2 1.5k dotfiles Python No No 555 Dots Python directory-based custom append points 264 Mackup Python automatic  per application No 12.8k dotter Rust configuration file Handlebars dt-cli Rust configuration file Handlebars mir.qualia Python No custom blocks <p>Where:</p> <ul> <li>File grouping: How configuration files can be grouped to configuration groups   (also called profiles or packages).</li> <li>Processing: Some tools process configuration files to allow them to be   customized depending on the host.</li> </ul> <p>A quick look up shows that:</p> <ul> <li>chezmoi looks like the best option.</li> <li>dotdrop looks like the second best option.</li> <li>dotfiles is unmaintained.</li> <li>dots: is maintained but migrated to go</li> <li>mackup: Looks like it's built for the cloud and it needs to support your   application?</li> </ul> <p>I think I'll give <code>chezmoi</code> a try.</p>"}, {"location": "drone/", "title": "Drone", "text": "<p>Drone is a modern Continuous Integration platform that empowers busy teams to automate their build, test and release workflows using a powerful, cloud native pipeline engine.</p>"}, {"location": "drone/#installation", "title": "Installation", "text": "<p>This section explains how to install the Drone server for Gitea.</p> <p>Note</p> <p>They explicitly recommend not to use Gitea and Drone in the same instance, and even less using <code>docker-compose</code> due to network complications. But if you have only a small instance as I do, you'll have to try :P.</p> <ul> <li>Log in with an admin Gitea user. </li> <li>Go to the administration configuration, then Applications.</li> <li> <p>Create a Gitea OAuth application. The Consumer Key and Consumer Secret are     used to authorize access to Gitea resources. Use the next data:</p> <ul> <li>Name: Drone</li> <li>Redirect URI: https://drone.your-domain.com/login</li> </ul> <p>Even though it looks like you could use terraform to create the resource, you can't as it creates an application but at user level, not at gitea level. So the next snippet won't work</p> <pre><code>resource \"gitea_oauth2_app\" \"drone\" {\nname = \"drone\"\nredirect_uris = [var.drone_redirect_uri]\n}\n\noutput \"drone_oauth2_id\" {\nvalue = gitea_oauth2_app.drone.client_id\n}\n\noutput \"drone_oauth2_secret\" {\nvalue = gitea_oauth2_app.drone.client_secret\n}\n</code></pre> </li> <li> <p>Create a shared secret to authenticate communication between runners and your     central Drone server.</p> <pre><code>openssl rand -hex 16\n</code></pre> </li> <li> <p>Create the required docker networks:     <pre><code>docker network create continuous-delivery\ndocker network create drone\ndocker network create swag\n</code></pre></p> </li> <li> <p>Create the docker-compose file for the server</p> <pre><code>---\nversion: '3'\n\nservices:\nserver:\nimage: drone/drone:2\nenvironment:\n- DRONE_GITEA_SERVER=https://try.gitea.io\n- DRONE_GITEA_CLIENT_ID=05136e57d80189bef462\n- DRONE_GITEA_CLIENT_SECRET=7c229228a77d2cbddaa61ddc78d45e\n- DRONE_RPC_SECRET=super-duper-secret\n- DRONE_SERVER_HOST=drone.company.com\n- DRONE_SERVER_PROTO=https\ncontainer_name: drone\nrestart: always\nnetworks:\n- swag\n- drone\n- continuous-delivery\nvolumes:\n- drone-data:/data\n\nnetworks:\ncontinuous-delivery:\nexternal:\nname: continuous-delivery\ndrone:\nexternal:\nname: drone\nswag:\nexternal:\nname: swag\n\nvolumes:\ndrone-data:\ndriver: local\ndriver_opts:\ntype: none\no: bind\ndevice: /data/drone\n</code></pre> <p>Where we specify where we want the data to be stored at, and the networks to use. We're assuming that you're going to use the linuxserver swag proxy to end the ssl connection (which is accessible through the <code>swag</code> network), and that <code>gitea</code> is in the <code>continuous-delivery</code> network.</p> </li> <li> <p>Add the runners you want to install.</p> </li> <li>Configure your proxy to forward the requests to the correct dockers.</li> <li>Run <code>docker-compose up</code> from the file where your <code>docker-compose.yaml</code> file is     to test everything works. If it does, run <code>docker-compose down</code>.</li> <li>Create a systemd service to start and stop the whole service. For example     create the <code>/etc/systemd/system/drone.service</code> file with the content:     <pre><code>Description=drone\n[Unit]\nDescription=drone\nRequires=gitea.service\nAfter=gitea.service\n\n[Service]\nRestart=always\nUser=root\nGroup=docker\nWorkingDirectory=/data/config/continuous-delivery/drone\n# Shutdown container (if running) when unit is started\nTimeoutStartSec=100\nRestartSec=2s\n# Start container when unit is started\nExecStart=/usr/bin/docker-compose -f docker-compose.yml up\n# Stop container when unit is stopped\nExecStop=/usr/bin/docker-compose -f docker-compose.yml down\n\n[Install]\nWantedBy=multi-user.target\n</code></pre></li> </ul>"}, {"location": "drone/#drone-runners", "title": "Drone Runners", "text": ""}, {"location": "drone/#docker-runner", "title": "Docker Runner", "text": "<p>Merge the next docker-compose with the one of the server above:</p> <pre><code>---\nversion: '3'\n\nservices:\ndocker_runner:\nimage: drone/drone-runner-docker:1\nenvironment:\n- DRONE_RPC_PROTO=https\n- DRONE_RPC_HOST=drone.company.com\n- DRONE_RPC_SECRET=super-duper-secret\n- DRONE_RUNNER_CAPACITY=2\n- DRONE_RUNNER_NAME=docker-runner\ncontainer_name: drone-docker-runner\nrestart: always\nnetworks:\n- drone\nvolumes:\n- /var/run/docker.sock:/var/run/docker.sock\nexpose:\n- \"3000\"\n\nnetworks:\ndrone:\nexternal:\nname: drone\n</code></pre> <p>Use the <code>docker logs</code> command to view the logs and verify the runner successfully established a connection with the Drone server.</p> <pre><code>$ docker logs runner\n\nINFO[0000] starting the server\nINFO[0000] successfully pinged the remote server\n</code></pre>"}, {"location": "drone/#ssh-runner", "title": "SSH Runner", "text": "<p>Merge the next docker-compose with the one of the server above:</p> <pre><code>---\nversion: '3'\n\nservices:\nssh_runner:\nimage: drone/drone-runner-ssh:latest\nenvironment:\n- DRONE_RPC_PROTO=https\n- DRONE_RPC_HOST=drone.company.com\n- DRONE_RPC_SECRET=super-duper-secret\ncontainer_name: drone-ssh-runner\nrestart: always\nnetworks:\n- drone\nexpose:\n- \"3000\"\n\nnetworks:\ndrone:\nexternal:\nname: drone\n</code></pre> <p>Use the <code>docker logs</code> command to view the logs and verify the runner successfully established a connection with the Drone server.</p> <pre><code>$ docker logs runner\n\nINFO[0000] starting the server\nINFO[0000] successfully pinged the remote server\n</code></pre>"}, {"location": "drone/#troubleshooting", "title": "Troubleshooting", "text": ""}, {"location": "drone/#create-the-administrators", "title": "Create the administrators", "text": "<p>When you configure the Drone server you can create the initial administrative account by passing the below environment variable, which defines the account username (e.g. github handle) and admin flag set to true.</p> <pre><code>DRONE_USER_CREATE=username:octocat,admin:true\n</code></pre> <p>If you need to grant the primary administrative role to an existing user, you can provide an existing username. Drone will update the account and grant administrator role on server restart.</p> <p>You can create administrator accounts using the command line tools. Please see the command line tools documentation for installation instructions.</p> <p>Create a new administrator account:</p> <pre><code>$ drone user add octocat --admin\n</code></pre> <p>Or grant the administrator role to existing accounts:</p> <pre><code>$ drone user update octocat --admin\n</code></pre>"}, {"location": "drone/#linter-untrusted-repositories-cannot-mount-host-volumes", "title": "linter: untrusted repositories cannot mount host volumes", "text": "<p>Thats because the repository is not trusted.</p> <p>You have to set the trust as an admin of drone through the GUI or through the CLI with</p> <pre><code>drone repo update --trusted &lt;your/repo&gt;\n</code></pre> <p>If you're not an admin the above command returns a success but you'll see that the trust has not changed if you run</p> <pre><code>drone repo info &lt;your/repo&gt;\n</code></pre>"}, {"location": "drone/#references", "title": "References", "text": "<ul> <li>Docs</li> <li>Home</li> </ul>"}, {"location": "dunst/", "title": "Dunst", "text": "<p>Dunst is a lightweight replacement for the notification daemons provided by most desktop environments. It\u2019s very customizable, isn\u2019t dependent on any toolkits, and therefore fits into those window manager centric setups we all love to customize to perfection.</p>"}, {"location": "dunst/#installation", "title": "Installation", "text": "<pre><code>sudo apt-get install dunst\n</code></pre> <p>Test it's working with:</p> <pre><code>notify-send \"Notification Title\" \"Notification Messages\"\n</code></pre> <p>If your distro version is too old that doesn't have <code>dunstctl</code> or <code>dunstify</code>, you can install it manually:</p> <pre><code>git clone https://github.com/dunst-project/dunst.git\ncd dunst\n\n# Install dependencies\nsudo apt-get install libgdk-pixbuf2.0-0 libnotify-dev librust-pangocairo-dev\n\n# Build the program and install\nmake WAYLAND=0 SYSTEMD=1\nsudo make WAYLAND=0 SYSTEMD=1 install\n</code></pre> <p>Read and tweak the <code>~/.dunst/dunstrc</code> file to your liking.</p>"}, {"location": "dunst/#references", "title": "References", "text": "<ul> <li>Git</li> <li>Home</li> <li>Archwiki page on dunst</li> </ul>"}, {"location": "dynamicdns/", "title": "Dynamic DNS notes", "text": "<p>Dynamic DNS (DDNS) is a method of automatically updating a name server in the Domain Name Server (DNS), often in real time, with the active DDNS configuration of its configured hostnames, addresses or other information.</p> <p>There are different DDNS providers, I use Duckdns as it is easy to setup and the Linuxserver people have a docker that makes it work.</p>"}, {"location": "elasticsearch_exporter/", "title": "Blackbox Exporter", "text": "<p>The elasticsearch exporter allows monitoring Elasticsearch clusters with Prometheus.</p>"}, {"location": "elasticsearch_exporter/#installation", "title": "Installation", "text": "<p>To install the exporter we'll use helmfile to install the prometheus-elasticsearch-exporter chart.</p> <p>Add the following lines to your <code>helmfile.yaml</code>.</p> <pre><code>- name: prometheus-elasticsearch-exporter\nnamespace: monitoring\nchart: prometheus-community/prometheus-elasticsearch-exporter\nvalues:\n- prometheus-elasticsearch-exporter/values.yaml\n</code></pre> <p>Edit the chart values. <pre><code>mkdir prometheus-elasticsearch-exporter\nhelm inspect values prometheus-community/prometheus-elasticsearch-exporter &gt; prometheus-elasticsearch-exporter/values.yaml\nvi prometheus-elasticsearch-exporter/values.yaml\n</code></pre></p> <p>Comment out all the values you don't edit, so that the chart doesn't break when you upgrade it.</p> <p>Make sure that the <code>serviceMonitor</code> labels match your Prometheus <code>serviceMonitorSelector</code> otherwise they won't be added to the configuration.</p> <pre><code>es:\n## Address (host and port) of the Elasticsearch node we should connect to.\n## This could be a local node (localhost:9200, for instance), or the address\n## of a remote Elasticsearch server. When basic auth is needed,\n## specify as: &lt;proto&gt;://&lt;user&gt;:&lt;password&gt;@&lt;host&gt;:&lt;port&gt;. e.g., http://admin:pass@localhost:9200.\n##\nuri: http://localhost:9200\n\nserviceMonitor:\n## If true, a ServiceMonitor CRD is created for a prometheus operator\n## https://github.com/coreos/prometheus-operator\n##\nenabled: true\n#  namespace: monitoring\nlabels:\nrelease: prometheus-operator\ninterval: 30s\n# scrapeTimeout: 10s\n# scheme: http\n# relabelings: []\n# targetLabels: []\nmetricRelabelings:\n- sourceLabels: [cluster]\ntargetLabel: cluster_name\nregex: '.*:(.*)'\n# sampleLimit: 0\n</code></pre> <p>You can build the <code>cluster</code> label following this instructions, I didn't find the required meta tags, so I've built the <code>cluster_name</code> label for alerting purposes.</p> <p>The grafana dashboard I chose is <code>2322</code>. Taking as reference the grafana helm chart values, add the next yaml under the <code>grafana</code> key in the <code>prometheus-operator</code> <code>values.yaml</code>.</p> <pre><code>grafana:\nenabled: true\ndefaultDashboardsEnabled: true\ndashboardProviders:\ndashboardproviders.yaml:\napiVersion: 1\nproviders:\n- name: 'default'\norgId: 1\nfolder: ''\ntype: file\ndisableDeletion: false\neditable: true\noptions:\npath: /var/lib/grafana/dashboards/default\ndashboards:\ndefault:\nelasticsearch:\n# Ref: https://grafana.com/dashboards/2322\ngnetId: 2322\nrevision: 4\ndatasource: Prometheus\n</code></pre> <p>And install.</p> <pre><code>helmfile diff\nhelmfile apply\n</code></pre>"}, {"location": "elasticsearch_exporter/#elasticsearch-exporter-alerts", "title": "Elasticsearch exporter alerts", "text": "<p>Now that we've got the metrics, we can define the alert rules. Most have been tweaked from the Awesome prometheus alert rules collection.</p>"}, {"location": "elasticsearch_exporter/#availability-alerts", "title": "Availability alerts", "text": "<p>The most basic probes, test if the service is healthy</p> <pre><code>- alert: ElasticsearchClusterRed\nexpr: elasticsearch_cluster_health_status{color=\"red\"} == 1\nfor: 0m\nlabels:\nseverity: critical\nannotations:\nsummary: &gt;\nElasticsearch Cluster Red\n(cluster {{ $labels.cluster_name }})\ndescription: |\nElastic Cluster Red status\nVALUE = {{ $value }}\nLABELS = {{ $labels }}\n- alert: ElasticsearchClusterYellow\nexpr: elasticsearch_cluster_health_status{color=\"yellow\"} == 1\nfor: 0m\nlabels:\nseverity: warning\nannotations:\nsummary: &gt;\nElasticsearch Cluster Yellow\n(cluster {{ $labels.cluster_name }})\ndescription: |\nElastic Cluster Yellow status\nVALUE = {{ $value }}\nLABELS = {{ $labels }}\n- alert: ElasticsearchHealthyNodes\nexpr: elasticsearch_cluster_health_number_of_nodes &lt; 3\nfor: 0m\nlabels:\nseverity: critical\nannotations:\nsummary: &gt;\nElasticsearch Healthy Nodes\n(cluster {{ $labels.cluster_name }})\ndescription: |\nMissing node in Elasticsearch cluster\nVALUE = {{ $value }}\nLABELS = {{ $labels }}\n- alert: ElasticsearchHealthyMasterNodes\nexpr: &gt;\nelasticsearch_cluster_health_number_of_nodes\n- elasticsearch_cluster_health_number_of_data_nodes &gt; 0 &lt; 3\nfor: 0m\nlabels:\nseverity: critical\nannotations:\nsummary: &gt;\nElasticsearch Healthy Master Nodes &lt; 3\n(cluster {{ $labels.cluster_name }})\ndescription: |\nMissing master node in Elasticsearch cluster\nVALUE = {{ $value }}\nLABELS = {{ $labels }}\n- alert: ElasticsearchHealthyDataNodes\nexpr: elasticsearch_cluster_health_number_of_data_nodes &lt; 3\nfor: 0m\nlabels:\nseverity: critical\nannotations:\nsummary: &gt;\nElasticsearch Healthy Data Nodes\n(cluster {{ $labels.cluster_name }})\ndescription: |\nMissing data node in Elasticsearch cluster\nVALUE = {{ $value }}\nLABELS = {{ $labels }}\n</code></pre>"}, {"location": "elasticsearch_exporter/#performance-alerts", "title": "Performance alerts", "text": "<pre><code>- alert: ElasticsearchCPUUsageTooHigh\nexpr: elasticsearch_os_cpu_percent &gt; 90\nfor: 2m\nlabels:\nseverity: critical\nannotations:\nsummary: &gt;\nElasticsearch Node CPU Usage Too High\n(cluster {{ $labels.cluster_name }} node {{ $labels.name }})\ndescription: |\nThe CPU usage of node {{ $labels.name }} is over 90%\nVALUE = {{ $value }}\nLABELS = {{ $labels }}\n- alert: ElasticsearchCPUUsageWarning\nexpr: elasticsearch_os_cpu_percent &gt; 80\nfor: 2m\nlabels:\nseverity: warning\nannotations:\nsummary: &gt;\nElasticsearch Node CPU Usage Too High\n(cluster {{ $labels.cluster_name }} node {{ $labels.name }})\ndescription: |\nThe CPU usage of node {{ $labels.name }} is over 90%\nVALUE = {{ $value }}\nLABELS = {{ $labels }}\n- alert: ElasticsearchHeapUsageTooHigh\nexpr: &gt;\n(\nelasticsearch_jvm_memory_used_bytes{area=\"heap\"}\n/ elasticsearch_jvm_memory_max_bytes{area=\"heap\"}\n) * 100 &gt; 90\nfor: 2m\nlabels:\nseverity: critical\nannotations:\nsummary: &gt;\nElasticsearch Node Heap Usage Critical\n(cluster {{ $labels.cluster_name }} node {{ $labels.name }})\ndescription: |\nThe heap usage of node {{ $labels.name }} is over 90%\nVALUE = {{ $value }}\nLABELS = {{ $labels }}\n- alert: ElasticsearchHeapUsageWarning\nexpr: &gt;\n(\nelasticsearch_jvm_memory_used_bytes{area=\"heap\"}\n/ elasticsearch_jvm_memory_max_bytes{area=\"heap\"}\n) * 100 &gt; 80\nfor: 2m\nlabels:\nseverity: warning\nannotations:\nsummary: &gt;\nElasticsearch Node Heap Usage Warning\n(cluster {{ $labels.cluster_name }} node {{ $labels.name }})\ndescription: |\nThe heap usage of node {{ $labels.name }} is over 80%\nVALUE = {{ $value }}\nLABELS = {{ $labels }}\n- alert: ElasticsearchDiskOutOfSpace\nexpr: &gt;\nelasticsearch_filesystem_data_available_bytes\n/ elasticsearch_filesystem_data_size_bytes * 100 &lt; 10\nfor: 0m\nlabels:\nseverity: critical\nannotations:\nsummary: &gt;\nElasticsearch disk out of space\n(cluster {{ $labels.cluster_name }})\ndescription: |\nThe disk usage is over 90%\nVALUE = {{ $value }}\nLABELS = {{ $labels }}\n- alert: ElasticsearchDiskSpaceLow\nexpr: &gt;\nelasticsearch_filesystem_data_available_bytes\n/ elasticsearch_filesystem_data_size_bytes * 100 &lt; 20\nfor: 2m\nlabels:\nseverity: warning\nannotations:\nsummary: &gt;\nElasticsearch disk space low\n(cluster {{ $labels.cluster_name }})\ndescription: |\nThe disk usage is over 80%\nVALUE = {{ $value }}\nLABELS = {{ $labels }}\n- alert: ElasticsearchRelocatingShardsTooLong\nexpr: elasticsearch_cluster_health_relocating_shards &gt; 0\nfor: 15m\nlabels:\nseverity: warning\nannotations:\nsummary: &gt;\nElasticsearch relocating shards too long\n(cluster {{ $labels.cluster_name }})\ndescription: |\nElasticsearch has been relocating shards for 15min\nVALUE = {{ $value }}\nLABELS = {{ $labels }}\n- alert: ElasticsearchInitializingShardsTooLong\nexpr: elasticsearch_cluster_health_initializing_shards &gt; 0\nfor: 15m\nlabels:\nseverity: warning\nannotations:\nsummary: &gt;\nElasticsearch initializing shards too long\n(cluster_name {{ $labels.cluster }})\ndescription: |\nElasticsearch has been initializing shards for 15 min\nVALUE = {{ $value }}\nLABELS = {{ $labels }}\n- alert: ElasticsearchUnassignedShards\nexpr: elasticsearch_cluster_health_unassigned_shards &gt; 0\nfor: 0m\nlabels:\nseverity: critical\nannotations:\nsummary: &gt;\nElasticsearch unassigned shards\n(cluster {{ $labels.cluster_name }})\ndescription: |\nElasticsearch has unassigned shards\nVALUE = {{ $value }}\nLABELS = {{ $labels }}\n- alert: ElasticsearchPendingTasks\nexpr: elasticsearch_cluster_health_number_of_pending_tasks &gt; 0\nfor: 15m\nlabels:\nseverity: warning\nannotations:\nsummary: &gt;\nElasticsearch pending tasks\n(cluster {{ $labels.cluster_name }})\ndescription: |\nElasticsearch has pending tasks. Cluster works slowly.\nVALUE = {{ $value }}\nLABELS = {{ $labels }}\n\n- alert: ElasticsearchCountOfJVMGarbageCollectorRuns\nexpr: rate(elasticsearch_jvm_gc_collection_seconds_count{}[5m]) &gt; 5\nfor: 1m\nlabels:\nseverity: warning\nannotations:\nsummary: &gt;\nElasticsearch JVM Garbage Collector runs &gt; 5\n(cluster {{ $labels.cluster_name }})\ndescription: |\nElastic Cluster JVM Garbage Collector runs &gt; 5\nVALUE = {{ $value }}\nLABELS = {{ $labels }}\n\n- alert: ElasticsearchCountOfJVMGarbageCollectorTime\nexpr: rate(elasticsearch_jvm_gc_collection_seconds_sum[5m]) &gt; 0.3\nfor: 1m\nlabels:\nseverity: warning\nannotations:\nsummary: &gt;\nElasticsearch JVM Garbage Collector time &gt; 0.3\n(cluster {{ $labels.cluster_name }})\ndescription: |\nElastic Cluster JVM Garbage Collector runs &gt; 0.3\nVALUE = {{ $value }}\nLABELS = {{ $labels }}\n- alert: ElasticsearchJSONParseErrors\nexpr: elasticsearch_cluster_health_json_parse_failures &gt; 0\nfor: 1m\nlabels:\nseverity: warning\nannotations:\nsummary: &gt;\nElasticsearch json parse error\n(cluster {{ $labels.cluster_name }})\ndescription: |\nElasticsearch json parse error\nVALUE = {{ $value }}\nLABELS = {{ $labels }}\n- alert: ElasticsearchCircuitBreakerTripped\nexpr: rate(elasticsearch_breakers_tripped{}[5m])&gt;0\nfor: 1m\nlabels:\nseverity: warning\nannotations:\nsummary: &gt;\nElasticsearch breaker {{ $labels.breaker }} tripped\n(cluster {{ $labels.cluster_name }}, node {{ $labels.name }})\ndescription: |\nElasticsearch breaker {{ $labels.breaker }} tripped\n(cluster {{ $labels.cluster_name }}, node {{ $labels.name }})\nVALUE = {{ $value }}\nLABELS = {{ $labels }}\n</code></pre>"}, {"location": "elasticsearch_exporter/#snapshot-alerts", "title": "Snapshot alerts", "text": "<pre><code>- alert: ElasticsearchMonthlySnapshot\nexpr: &gt;\ntime() -\nelasticsearch_snapshot_stats_snapshot_end_time_timestamp{state=\"SUCCESS\"}\n&gt; (3600 * 24 * 32)\nfor: 15m\nlabels:\nseverity: warning\nannotations:\nsummary: &gt;\nElasticsearch monthly snapshot failed\n(cluster {{ $labels.cluster_name }},\nsnapshot {{ $labels.repository }})\ndescription: |\nLast successful elasticsearch snapshot\nof repository {{ $labels.repository}} is older than 32 days.\nVALUE = {{ $value }}\nLABELS = {{ $labels }}\n\n- record: elasticsearch_indices_search_latency:rate1m\nexpr: |\nincrease(elasticsearch_indices_search_query_time_seconds[1m])/\nincrease(elasticsearch_indices_search_query_total[1m])\n- record: elasticsearch_indices_search_rate:rate1m\nexpr: increase(elasticsearch_indices_search_query_total[1m])/60\n- alert: ElasticsearchSlowSearchLatency\nexpr: elasticsearch_indices_search_latency:rate1m &gt; 1\nfor: 2m\nlabels:\nseverity: warning\nannotations:\nsummary: &gt;\nElasticsearch search latency is greater than 1 s\n(cluster {{ $labels.cluster_name }}, node {{ $labels.name }})\ndescription: |\nElasticsearch search latency is greater than 1 s\n(cluster {{ $labels.cluster_name }}, node {{ $labels.name }})\nVALUE = {{ $value }}\nLABELS = {{ $labels }}\n</code></pre>"}, {"location": "elasticsearch_exporter/#links", "title": "Links", "text": "<ul> <li>Git</li> </ul>"}, {"location": "email_automation/", "title": "Email automation", "text": "<p>Most of the received emails require repetitive actions that can be automated, and you may also want to access your emails through a command line interface and be able to search through them.</p> <p>One of the ways to achieve that goals is to use a combination of tools to synchronize the mailboxes, tag them, and run scripts automatically based on the tags.</p>"}, {"location": "email_automation/#installation", "title": "Installation", "text": "<p>First you need a program that syncs your mailboxes, following pazz's advice , I'll use mbsync. Follow the steps under installation to configure your accounts, taking as an example an account called <code>lyz</code> you should be able to sync all your emails with:</p> <pre><code>mbsync -V lyz\n</code></pre> <p>Now we need to install <code>notmuch</code> a tool to index, search, read, and tag large collections of email messages. Follow the steps under installation under you have created the database that indexes your emails.</p> <p>Once we have that, we need a tool to tag the emails following our desired rules. afew is one way to go. Follow the steps under installation.</p> <p>The remaining step to keep the inboxes synced and tagged is to run all the steps above in a cron. Particularize pazz's script for your usecase:</p> <pre><code>#!/bin/bash\n#\n# Download and index new mail.\n#\n# Copyright (c) 2017 Patrick Totzke\n# Dependencies: flock, nm-online, mbsync, notmuch, afew\n# Example crontab entry:\n#\n#   */2 * * * * /usr/bin/flock -n /home/pazz/.pullmail.lock /home/pazz/bin/pullmail.sh &gt; /home/pazz/.pullmail.log\n#\n\nPATH=/home/pazz/.local/bin:/usr/local/bin/:$PATH\nACCOUNTDIR=/home/pazz/.pullmail/\n\n# this makes the keyring daemon accessible\nfunction keyring-control() {\nlocal -a vars=( \\\nDBUS_SESSION_BUS_ADDRESS \\\nGNOME_KEYRING_CONTROL \\\nGNOME_KEYRING_PID \\\nXDG_SESSION_COOKIE \\\nGPG_AGENT_INFO \\\nSSH_AUTH_SOCK \\\n)\nlocal pid=$(ps -C i3 -o pid --no-heading)\neval \"unset ${vars[@]}; $(printf \"export %s;\" $(sed 's/\\x00/\\n/g' /proc/${pid//[^0-9]/}/environ | grep $(printf -- \"-e ^%s= \" \"${vars[@]}\")) )\"\n}\n\nfunction log() {\nnotify-send -t 2000  'mail sync:' \"$@\"\n}\n\nfunction die() {\nnotify-send -t 2000 -u critical 'mail sync:' \"$@\"\nexit 1\n}\n\n# Let's Do stuff\nkeyring-control\n\n# abort as soon as something fails\nset -e\n\n# abort if not online\nnm-online -x -t 0\n\necho ---------------------------------------------------------\ndate\nfor accfile in `ls $ACCOUNTDIR`;\ndo\nACC=$(basename $accfile)\necho ------------------------  $ACC   ------------------------\n    mbsync -V $ACC || log \"$ACC failed\"\ndone\n\n# index and tag new mails\necho ------------------------ NOTMUCH ------------------------\nnotmuch new 2&gt;/dev/null || die \"NOTMUCH new failed\"\n\necho ------------------------  AFEW   ------------------------\nafew -v --tag --new || die \"AFEW died\"\n\necho ---------------------------------------------------------\necho \"all done, goodbye.\"\n</code></pre> <p>Where <code>flock</code> is a tool to manage locks from shell scripts.</p> <p>And add the entry in your <code>crontab -e</code>.</p> <p>If you want to process your emails with this system through a command line interface, you can configure alot.</p>"}, {"location": "email_management/", "title": "Email management", "text": "<p>Email can be one of the main aggregators of interruptions as it's supported by almost everything. I use it as the notification backend of services that don't need to be acted upon immediately or when more powerful mechanisms are not available.</p> <p>If not used wisely, it can be a sink of productivity.</p>"}, {"location": "email_management/#analyze-how-often-you-need-to-check-it", "title": "Analyze how often you need to check it", "text": "<p>Follow the interruption analysis to discover how often you need to check it and if you need the notifications. Once you've decided the frequency, try to respect it!. If you want an example, check my work or personal analysis.</p>"}, {"location": "email_management/#workflow", "title": "Workflow", "text": "<p>Each time I decide to go through my emails I follow the inbox processing guidelines. I understand the email inbox are items that need to be taken care of. If an email doesn't fall in that category I either archive or delete it. That way the inbox has the smallest number of items, and if everything went well, it is empty. Having an empty inbox helps you a lot to reduce the mental load for many reasons:</p> <ul> <li>When you look at it and don't see any mail, you get the small satisfaction     that you have done everything.</li> <li>When there is something new, it stands out, without the distraction of other     email subjects that can drift your attention.</li> </ul>"}, {"location": "email_management/#accounts-shared-by-many-people", "title": "Accounts shared by many people", "text": "<p>On email accounts managed by many people, I delete/archive emails that I know that need no interaction by any of them. If there is nothing for me to do, I mark them as read and wait for them to archive/delete them. If an email is left unread for 3 or 4 days I ask by other channels what should we do with that event.</p>"}, {"location": "email_management/#use-email-to-transport-information-not-to-store-it", "title": "Use email to transport information, not to store it", "text": "<p>Email was envisioned as a protocol for person A to send information to person B. The fact that the \"free email providers\" such as Google allow users to have almost no limit on their inbox has driven people to store all their emails and use it as a knowledge repository. This approach has many problems:</p> <ul> <li>As most people don't use end to end encryption (GPG), the data of their emails     is available for the email provider to read. This is a privacy violation     that leads to scary behaviours, such as targeted adds or google suggestions     based on the content of recent emails. You could improve the situation by     using POP3 instead of IMAP, but that'll force you to only use one device to     check your email, something that's becoming uncommon.</li> <li>The decent email providers that respect you, such as RiseUp,     Autistici or     Disroot, are maintained by communities and can     only offer a limited storage, so you're forced to empty your emails     periodically to be able to receive new ones.</li> <li>If you don't spend time and effort classifying your emails, searching between     them is a nightmare. It is even if you classify them. There are more     efficient knowledge repositories to store your information.</li> </ul> <p>On my personal emails, I forward the information to my archive, task manager or knowledge manager, deleting the email afterwards. At work, they use an indecent provider, encrypts most of emails with GPG and trust the provider to hold the rest of the data. I try to leak the least amount of personal information and I archive every email because you don't know when you're going to need them.</p>"}, {"location": "email_management/#use-key-bindings", "title": "Use key bindings", "text": "<p>Using the mouse to interact with the email client graphical interface is not efficient, try to learn the key bindings and use them as much as possible.</p>"}, {"location": "email_management/#environment-setup", "title": "Environment setup", "text": ""}, {"location": "email_management/#account-management", "title": "Account management", "text": "<p>It's common to have more than one account to check. For example, at work, I have my own account and another for each team I'm part of, the last ones are managed by all the team members. On the personal level, I've got many accounts for the different OpSec profiles or identities.</p> <p>For efficiency reasons, you need to be able to check all of them on one place. You can use an email manager such as Thunderbird. Once you choose one, try to master it.</p>"}, {"location": "email_management/#isolate-your-work-and-personal-environments", "title": "Isolate your work and personal environments", "text": "<p>Make sure that you set your environment so that you can't check your personal email when you're working and the other way around. For example, you could set two Thunderbird profiles, or you could avoid configuring the work email in your personal phone.</p>"}, {"location": "email_management/#automatic-filtering-and-processing", "title": "Automatic filtering and processing", "text": "<p>Inbox management is time consuming, so you want to reduce the number of emails to process. From the interruption analysis you'll know which ones don't give you any value, our goal is to make them disappear before we open our inbox.</p> <p>You can get rid of them by:</p> <ul> <li>Preventing the sender to send them: Unsubscribe from the newsletters you no     longer read or fix the configuration of the services that send you     notifications that don't want.</li> <li>Tweak your spam filter: If you have no control on the source, tweak your spam     filter so that it filters them out for you.</li> <li>Use your email client filtering and processing features: If you want to     receive the emails for archival purposes, configure your email client to     match them by regular expressions on the sender or subject, mark them as     read and move them to the desired directory.</li> <li>Use email automation software: If you want to run automatic processes     triggered by emails, use email automation     solutions.</li> </ul>"}, {"location": "email_management/#use-your-preferred-editor-to-write-the-emails", "title": "Use your preferred editor to write the emails", "text": "<p>You'll probably be less efficient with the email client's editor in comparison with your own. If you use vim or emacs, there's a good chance that the email client has a plugin that allows you to use it. Or you can always migrate to a command line client. I'll probably do that once I set up the email automation system.</p>"}, {"location": "emojis/", "title": "Emojis", "text": "<p>Curated list of emojis to copy paste.</p>"}, {"location": "emojis/#angry", "title": "Angry", "text": "<pre><code>(\u0482\u2323\u0300_\u2323\u0301)\n\n( &gt;\u0434&lt;)\n\n\u0295\u2022\u0300o\u2022\u0301\u0294\n\n\u30fd(\u2267\u0414\u2266)\u30ce\n\n\u1559(\u21c0\u2038\u21bc\u2036)\u1557\n\n\u0669(\u256c\u0298\u76ca\u0298\u256c)\u06f6\n</code></pre>"}, {"location": "emojis/#annoyed", "title": "Annoyed", "text": "<pre><code>(\u2256\u035e_\u2256\u0325)\n(&gt;_&lt;)\n</code></pre>"}, {"location": "emojis/#awesome", "title": "Awesome", "text": "<pre><code>( \u00b7_\u00b7)\n( \u00b7_\u00b7)       --\u25a0-\u25a0\n( \u00b7_\u00b7)--\u25a0-\u25a0\n(-\u25a0_\u25a0) YEAAAAAAAAAAAAAAAAAAAAAHHHHHHHHHHHH\n</code></pre>"}, {"location": "emojis/#conforting", "title": "Conforting", "text": "<pre><code>(\uff4f\u30fb_\u30fb)\u30ce\u201d(\u1d17_ \u1d17\u3002)\n</code></pre>"}, {"location": "emojis/#congratulations", "title": "Congratulations", "text": "<pre><code>( \u141b )\u0648\n\n\uff3c\\ \u0669( \u141b )\u0648 /\uff0f\n</code></pre>"}, {"location": "emojis/#crying", "title": "Crying", "text": "<pre><code>(\u2565\ufe4f\u2565)\n(\u0ca5\ufe4f\u0ca5)\n</code></pre>"}, {"location": "emojis/#excited", "title": "Excited", "text": "<pre><code>(((o(*\uff9f\u25bd\uff9f*)o)))\n\no(\u2267\u2207\u2266o)\n</code></pre>"}, {"location": "emojis/#dance", "title": "Dance", "text": "<pre><code>(~\u203e\u25bf\u203e)~   ~(\u203e\u25bf\u203e)~   ~(\u203e\u25bf\u203e~)\n\n\u250c(\u30fb\u3002\u30fb)\u2518 \u266a \u2514(\u30fb\u3002\u30fb)\u2510 \u266a \u250c(\u30fb\u3002\u30fb)\u2518\n\n\u01aa(\u02d8\u2323\u02d8)\u2510    \u01aa(\u02d8\u2323\u02d8)\u0283    \u250c(\u02d8\u2323\u02d8)\u0283\n\n(&gt;'-')&gt;\n&lt;('-'&lt;)\n^('-')^\nv('-')v\n(&gt;'-')&gt;\n (^-^)\n</code></pre>"}, {"location": "emojis/#happy", "title": "Happy", "text": "<pre><code>\u1555( \u141b )\u1557\n\n\u0295\u2022\u1d25\u2022\u0294\n\n(\u2022\u203f\u2022)\n\n(\u25e1\u203f\u25e1\u273f)\n\n(\u273f\u25e0\u203f\u25e0)\n\n\u266a(\u0e51\u1d16\u25e1\u1d16\u0e51)\u266a\n</code></pre>"}, {"location": "emojis/#kisses", "title": "Kisses", "text": "<pre><code>(\u3065\uffe3 \u00b3\uffe3)\u3065\n\n( \u02d8 \u00b3\u02d8)\u2665\n</code></pre>"}, {"location": "emojis/#love", "title": "Love", "text": "<pre><code>\u2764\n</code></pre>"}, {"location": "emojis/#pride", "title": "Pride", "text": "<pre><code>&lt;(\uffe3\uff3e\uffe3)&gt;\n</code></pre>"}, {"location": "emojis/#relax", "title": "Relax", "text": "<pre><code>_\u3078__(\u203e\u25e1\u25dd )&gt;\n</code></pre>"}, {"location": "emojis/#sad", "title": "Sad", "text": "<pre><code>\uff61\uff9f(*\u00b4\u25a1`)\uff9f\uff61\n\n(\u25de\u2038\u25df\uff1b)\n</code></pre>"}, {"location": "emojis/#scared", "title": "Scared", "text": "<pre><code>\u30fd(\uff9f\u0414\uff9f)\uff89\n\n\u30fd\u3014\uff9f\u0414\uff9f\u3015\u4e3f\n</code></pre>"}, {"location": "emojis/#sleepy", "title": "Sleepy", "text": "<pre><code>(\u1d17\u02f3\u1d17)\n</code></pre>"}, {"location": "emojis/#smug", "title": "Smug", "text": "<pre><code>\uff08\uffe3\uff5e\uffe3\uff09\n</code></pre>"}, {"location": "emojis/#whyyyy", "title": "Whyyyy?", "text": "<pre><code>(/\uff9f\u0414\uff9f)/\n</code></pre>"}, {"location": "emojis/#surprised", "title": "Surprised", "text": "<pre><code>(\\_/)\n(O.o)\n(&gt; &lt;)\n\n(\u2299_\u2609)\n\n(\u00ac\u00ba-\u00b0)\u00ac\n\n(\u2609_\u2609)\n\n(\u2022 \u0325\u0306\u2006\u2022)\n\n\u00af\\(\u00b0_o)/\u00af\n\n(\u30fb0\u30fb\u3002(\u30fb-\u30fb\u3002(\u30fb0\u30fb\u3002(\u30fb-\u30fb\u3002)\n\n(*\uff9f\u25ef\uff9f*)\n</code></pre>"}, {"location": "emojis/#who-cares", "title": "Who cares", "text": "<pre><code>\u00af\\_(\u30c4)_/\u00af\n</code></pre>"}, {"location": "emojis/#wtf", "title": "WTF", "text": "<pre><code>(\u256f\u00b0\u25a1\u00b0)\u256f \u253b\u2501\u253b\n\n\u30d8\uff08\u3002\u25a1\u00b0\uff09\u30d8\n</code></pre>"}, {"location": "emojis/#links", "title": "Links", "text": "<ul> <li>Japanese Emoticons</li> </ul>"}, {"location": "environmentalism/", "title": "Environmentalism", "text": ""}, {"location": "environmentalism/#measure-the-carbon-footprint-of-your-travels", "title": "Measure the carbon footprint of your travels", "text": "<p>https://www.carbonfootprint.com/</p> <p>There are also some calculators for events itself:</p> <p>https://co2.myclimate.org/en/event_calculators/new https://psci.princeton.edu/events-emissions-calculator</p>"}, {"location": "environmentalism/#saving-water", "title": "Saving water", "text": "<p>Here are some small things I'm doing to save some water each day:</p> <ul> <li>Use the watering can or a bucket to gather the shower water until it's warm   enough. I use this water to flush the toilet. It would be best if it were   possible to fill up the toilet's deposit, but it's not easy.</li> <li>Use a glass of water to wet the toothbrush and rinse my mouth instead of using   running water.</li> </ul>"}, {"location": "fastapi/", "title": "FastAPI", "text": "<p>FastAPI is a modern, fast (high-performance), web framework for building APIs with Python 3.6+ based on standard Python type hints.</p> <p>The key features are:</p> <ul> <li>Fast: Very high performance, on par with NodeJS and Go (thanks to Starlette   and Pydantic). One of the fastest Python frameworks available.</li> <li>Fast to code: Increase the speed to develop features by about 200% to 300%.</li> <li>Fewer bugs: Reduce about 40% of human (developer) induced errors.</li> <li>Intuitive: Great editor support. Completion everywhere. Less time debugging.</li> <li>Easy: Designed to be easy to use and learn. Less time reading docs.</li> <li>Short: Minimize code duplication. Multiple features from each parameter   declaration. Fewer bugs.</li> <li>Robust: Get production-ready code. With automatic interactive documentation.</li> <li>Standards-based: Based on (and fully compatible with) the open standards for   APIs: OpenAPI (previously known as Swagger) and JSON Schema.</li> <li>Authentication with JWT:   with a super nice tutorial on how to set it up.</li> </ul>"}, {"location": "fastapi/#installation", "title": "Installation", "text": "<pre><code>pip install fastapi\n</code></pre> <p>You will also need an ASGI server, for production such as Uvicorn or Hypercorn.</p> <pre><code>pip install uvicorn[standard]\n</code></pre>"}, {"location": "fastapi/#simple-example", "title": "Simple example", "text": "<ul> <li>Create a file <code>main.py</code> with:</li> </ul> <pre><code>from typing import Optional\n\nfrom fastapi import FastAPI\n\napp = FastAPI()\n\n\n@app.get(\"/\")\ndef read_root():\n    return {\"Hello\": \"World\"}\n\n\n@app.get(\"/items/{item_id}\")\ndef read_item(item_id: int, q: Optional[str] = None):\n    return {\"item_id\": item_id, \"q\": q}\n</code></pre> <ul> <li>Run the server:</li> </ul> <pre><code>uvicorn main:app --reload\n</code></pre> <ul> <li>Open your browser at http://127.0.0.1:8000/items/5?q=somequery. You will see   the JSON response as:</li> </ul> <pre><code>{\n\"item_id\": 5,\n\"q\": \"somequery\"\n}\n</code></pre> <p>You already created an API that:</p> <ul> <li>Receives HTTP requests in the paths <code>/</code> and <code>/items/{item_id}</code>.</li> <li>Both paths take GET operations (also known as HTTP methods).</li> <li>The path <code>/items/{item_id}</code> has a path parameter <code>item_id</code> that should be an   <code>int</code>.</li> <li>The path <code>/items/{item_id}</code> has an optional <code>str</code> query parameter <code>q</code>.</li> <li>Has interactive API docs made for you:</li> <li>Swagger: http://127.0.0.1:8000/docs.</li> <li>Redoc: http://127.0.0.1:8000/redoc.</li> </ul> <p>You will see the automatic interactive API documentation (provided by Swagger UI):</p>"}, {"location": "fastapi/#sending-data-to-the-server", "title": "Sending data to the server", "text": "<p>When you need to send data from a client (let's say, a browser) to your API, you have three basic options:</p> <ul> <li>As path parameters in the URL (<code>/items/2</code>).</li> <li>As query parameters in the URL (<code>/items/2?skip=true</code>).</li> <li>In the body of a POST request.</li> </ul> <p>To send simple data use the first two, to send complex or sensitive data, use the last.</p> <p>It also supports sending data through cookies and headers.</p>"}, {"location": "fastapi/#path-parameters", "title": "Path Parameters", "text": "<p>You can declare path \"parameters\" or \"variables\" with the same syntax used by Python format strings:</p> <pre><code>@app.get(\"/items/{item_id}\")\ndef read_item(item_id: int):\n    return {\"item_id\": item_id}\n</code></pre> <p>If you define the type hints of the function arguments, FastAPI will use pydantic data validation.</p> <p>If you need to use a Linux path as an argument, check this workaround, but be aware that it's not supported by OpenAPI.</p>"}, {"location": "fastapi/#order-matters", "title": "Order matters", "text": "<p>Because path operations are evaluated in order, you need to make sure that the path for the fixed endpoint <code>/users/me</code> is declared before the variable one <code>/users/{user_id}</code>:</p> <pre><code>@app.get(\"/users/me\")\nasync def read_user_me():\n    return {\"user_id\": \"the current user\"}\n\n\n@app.get(\"/users/{user_id}\")\nasync def read_user(user_id: str):\n    return {\"user_id\": user_id}\n</code></pre> <p>Otherwise, the path for <code>/users/{user_id}</code> would match also for <code>/users/me</code>, \"thinking\" that it's receiving a parameter user_id with a value of \"me\".</p>"}, {"location": "fastapi/#predefined-values", "title": "Predefined values", "text": "<p>If you want the possible valid path parameter values to be predefined, you can use a standard Python <code>Enum</code>.</p> <pre><code>from enum import Enum\n\n\nclass ModelName(str, Enum):\n    alexnet = \"alexnet\"\n    resnet = \"resnet\"\n    lenet = \"lenet\"\n\n\n@app.get(\"/models/{model_name}\")\ndef get_model(model_name: ModelName):\n    if model_name == ModelName.alexnet:\n        return {\"model_name\": model_name, \"message\": \"Deep Learning FTW!\"}\n\n    if model_name.value == \"lenet\":\n        return {\"model_name\": model_name, \"message\": \"LeCNN all the images\"}\n\n    return {\"model_name\": model_name, \"message\": \"Have some residuals\"}\n</code></pre> <p>These are the basics, FastAPI supports more complex path parameters and string validations.</p>"}, {"location": "fastapi/#query-parameters", "title": "Query Parameters", "text": "<p>When you declare other function parameters that are not part of the path parameters, they are automatically interpreted as \"query\" parameters.</p> <pre><code>fake_items_db = [{\"item_name\": \"Foo\"}, {\"item_name\": \"Bar\"}, {\"item_name\": \"Baz\"}]\n\n\n@app.get(\"/items/\")\nasync def read_item(skip: int = 0, limit: int = 10):\n    return fake_items_db[skip : skip + limit]\n</code></pre> <p>The query is the set of key-value pairs that go after the <code>?</code> in a URL, separated by <code>&amp;</code> characters.</p> <p>For example, in the URL: http://127.0.0.1:8000/items/?skip=0&amp;limit=10</p> <p>These are the basics, FastAPI supports more complex query parameters and string validations.</p>"}, {"location": "fastapi/#request-body", "title": "Request Body", "text": "<p>To declare a request body, you use Pydantic models with all their power and benefits.</p> <pre><code>from typing import Optional\nfrom pydantic import BaseModel\n\n\nclass Item(BaseModel):\n    name: str\n    description: Optional[str] = None\n    price: float\n    tax: Optional[float] = None\n\n\n@app.post(\"/items/\")\nasync def create_item(item: Item):\n    return item\n</code></pre> <p>With just that Python type declaration, FastAPI will:</p> <ul> <li>Read the body of the request as JSON.</li> <li>Convert the corresponding types (if needed).</li> <li>Validate the data: If the data is invalid, it will return a nice and clear   error, indicating exactly where and what was the incorrect data.</li> <li>Give you the received data in the parameter <code>item</code>.</li> <li>Generate JSON Schema definitions for your model.</li> <li>Those schemas will be part of the generated OpenAPI schema, and used by the   automatic documentation UIs.</li> </ul> <p>These are the basics, FastAPI supports more complex patterns such as:</p> <ul> <li>Using multiple models in the same query.</li> <li>Additional validations of the pydantic models.</li> <li>Nested models.</li> </ul>"}, {"location": "fastapi/#sending-data-to-the-client", "title": "Sending data to the client", "text": "<p>When you create a FastAPI path operation you can normally return any data from it: a <code>dict</code>, a <code>list</code>, a Pydantic model, a database model, etc.</p> <p>By default, FastAPI would automatically convert that return value to JSON using the <code>jsonable_encoder</code>.</p> <p>To return custom responses such as a direct string, xml or html use <code>Response</code>:</p> <pre><code>from fastapi import FastAPI, Response\n\napp = FastAPI()\n\n\n@app.get(\"/legacy/\")\ndef get_legacy_data():\n    data = \"\"\"&lt;?xml version=\"1.0\"?&gt;\n    &lt;shampoo&gt;\n    &lt;Header&gt;\n        Apply shampoo here.\n    &lt;/Header&gt;\n    &lt;Body&gt;\n        You'll have to use soap here.\n    &lt;/Body&gt;\n    &lt;/shampoo&gt;\n    \"\"\"\n    return Response(content=data, media_type=\"application/xml\")\n</code></pre>"}, {"location": "fastapi/#handling-errors", "title": "Handling errors", "text": "<p>There are many situations in where you need to notify an error to a client that is using your API.</p> <p>In these cases, you would normally return an HTTP status code in the range of 400 (from 400 to 499).</p> <p>This is similar to the 200 HTTP status codes (from 200 to 299). Those \"200\" status codes mean that somehow there was a \"success\" in the request.</p> <p>To return HTTP responses with errors to the client you use <code>HTTPException</code>.</p> <pre><code>from fastapi import HTTPException\n\nitems = {\"foo\": \"The Foo Wrestlers\"}\n\n\n@app.get(\"/items/{item_id}\")\nasync def read_item(item_id: str):\n    if item_id not in items:\n        raise HTTPException(status_code=404, detail=\"Item not found\")\n    return {\"item\": items[item_id]}\n</code></pre>"}, {"location": "fastapi/#updating-data", "title": "Updating data", "text": ""}, {"location": "fastapi/#update-replacing-with-put", "title": "Update replacing with PUT", "text": "<p>To update an item you can use the HTTP PUT operation.</p> <p>You can use the <code>jsonable_encoder</code> to convert the input data to data that can be stored as JSON (e.g. with a NoSQL database). For example, converting datetime to str.</p> <pre><code>from typing import List, Optional\n\nfrom fastapi.encoders import jsonable_encoder\nfrom pydantic import BaseModel\n\n\nclass Item(BaseModel):\n    name: Optional[str] = None\n    description: Optional[str] = None\n    price: Optional[float] = None\n    tax: float = 10.5\n    tags: List[str] = []\n\n\nitems = {\n    \"foo\": {\"name\": \"Foo\", \"price\": 50.2},\n    \"bar\": {\"name\": \"Bar\", \"description\": \"The bartenders\", \"price\": 62, \"tax\": 20.2},\n    \"baz\": {\"name\": \"Baz\", \"description\": None, \"price\": 50.2, \"tax\": 10.5, \"tags\": []},\n}\n\n\n@app.get(\"/items/{item_id}\", response_model=Item)\nasync def read_item(item_id: str):\n    return items[item_id]\n\n\n@app.put(\"/items/{item_id}\", response_model=Item)\nasync def update_item(item_id: str, item: Item):\n    update_item_encoded = jsonable_encoder(item)\n    items[item_id] = update_item_encoded\n    return update_item_encoded\n</code></pre>"}, {"location": "fastapi/#partial-updates-with-patch", "title": "Partial updates with PATCH", "text": "<p>You can also use the HTTP PATCH operation to partially update data.</p> <p>This means that you can send only the data that you want to update, leaving the rest intact.</p>"}, {"location": "fastapi/#configuration", "title": "Configuration", "text": ""}, {"location": "fastapi/#application-configuration", "title": "Application configuration", "text": "<p>In many cases your application could need some external settings or configurations, for example secret keys, database credentials, credentials for email services, etc.</p> <p>You can load these configurations through environmental variables, or you can use the awesome Pydantic settings management, whose advantages are:</p> <ul> <li>Do Pydantic's type validation on the fields.</li> <li>Automatically reads the missing values from environmental variables.</li> <li>Supports reading variables from   Dotenv files.</li> <li>Supports secrets.</li> </ul> <p>First you define the <code>Settings</code> class with all the fields:</p> <p>File: <code>config.py</code>:</p> <pre><code>from pydantic import BaseSettings\n\n\nclass Settings(BaseSettings):\n    verbose: bool = True\n    database_url: str = \"tinydb://~/.local/share/pyscrobbler/database.tinydb\"\n</code></pre> <p>Then in the api definition, set the dependency.</p> <p>File: <code>api.py</code>:</p> <pre><code>from functools import lru_cache\nfrom fastapi import Depends, FastAPI\n\n\napp = FastAPI()\n\n\n@lru_cache()\ndef get_settings() -&gt; Settings:\n\"\"\"Configure the program settings.\"\"\"\n    return Settings()\n\n\n@app.get(\"/verbose\")\ndef verbose(settings: Settings = Depends(get_settings)) -&gt; bool:\n    return settings.verbose\n</code></pre> <p>Where:</p> <ul> <li> <p><code>get_settings</code> is the dependency function that configures the <code>Settings</code>   object. The endpoint <code>verbose</code> is   dependant of <code>get_settings</code>.</p> </li> <li> <p>The <code>@lru_cache</code> decorator   changes the function it decorates to return the same value that was returned   the first time, instead of computing it again, executing the code of the   function every time.</p> </li> </ul> <p>So, the function will be executed once for each combination of arguments. And   then the values returned by each of those combinations of arguments will be   used again and again whenever the function is called with exactly the same   combination of arguments.</p> <p>Creating the <code>Settings</code> object is a costly operation as it needs to check the   environment variables or read a file, so we want to do it just once, not on   each request.</p> <p>This setup makes it easy to inject testing configuration so as not to break production code.</p>"}, {"location": "fastapi/#openapi-configuration", "title": "OpenAPI configuration", "text": ""}, {"location": "fastapi/#define-title-description-and-version", "title": "Define title, description and version", "text": "<pre><code>from fastapi import FastAPI\n\napp = FastAPI(\n    title=\"My Super Project\",\n    description=\"This is a very fancy project, with auto docs for the API and everything\",\n    version=\"2.5.0\",\n)\n</code></pre>"}, {"location": "fastapi/#define-path-tags", "title": "Define path tags", "text": "<p>You can add tags to your path operation, pass the parameter tags with a list of <code>str</code> (commonly just one <code>str</code>):</p> <pre><code>from typing import Optional, Set\n\nfrom pydantic import BaseModel\n\n\nclass Item(BaseModel):\n    name: str\n    description: Optional[str] = None\n    price: float\n    tax: Optional[float] = None\n    tags: Set[str] = []\n\n\n@app.post(\"/items/\", response_model=Item, tags=[\"items\"])\nasync def create_item(item: Item):\n    return item\n\n\n@app.get(\"/items/\", tags=[\"items\"])\nasync def read_items():\n    return [{\"name\": \"Foo\", \"price\": 42}]\n\n\n@app.get(\"/users/\", tags=[\"users\"])\nasync def read_users():\n    return [{\"username\": \"johndoe\"}]\n</code></pre> <p>They will be added to the OpenAPI schema and used by the automatic documentation interfaces.</p>"}, {"location": "fastapi/#add-metadata-to-the-tags", "title": "Add metadata to the tags", "text": "<pre><code>tags_metadata = [\n    {\n        \"name\": \"users\",\n        \"description\": \"Operations with users. The **login** logic is also here.\",\n    },\n    {\n        \"name\": \"items\",\n        \"description\": \"Manage items. So _fancy_ they have their own docs.\",\n        \"externalDocs\": {\n            \"description\": \"Items external docs\",\n            \"url\": \"https://fastapi.tiangolo.com/\",\n        },\n    },\n]\n</code></pre> <p>app = FastAPI(openapi_tags=tags_metadata)</p>"}, {"location": "fastapi/#add-a-summary-and-description", "title": "Add a summary and description", "text": "<pre><code>@app.post(\"/items/\", response_model=Item, summary=\"Create an item\")\nasync def create_item(item: Item):\n\"\"\"\n    Create an item with all the information:\n\n    - **name**: each item must have a name\n    - **description**: a long description\n    - **price**: required\n    - **tax**: if the item doesn't have tax, you can omit this\n    - **tags**: a set of unique tag strings for this item\n    \"\"\"\n    return item\n</code></pre>"}, {"location": "fastapi/#response-description", "title": "Response description", "text": "<pre><code>@app.post(\n    \"/items/\",\n    response_description=\"The created item\",\n)\nasync def create_item(item: Item):\n    return item\n</code></pre>"}, {"location": "fastapi/#deprecate-a-path-operation", "title": "Deprecate a path operation", "text": "<p>When you need to mark a path operation as deprecated, but without removing it</p> <pre><code>@app.get(\"/elements/\", tags=[\"items\"], deprecated=True)\nasync def read_elements():\n    return [{\"item_id\": \"Foo\"}]\n</code></pre>"}, {"location": "fastapi/#deploy-with-docker", "title": "Deploy with Docker.", "text": "<p>FastAPI has it's own optimized docker, which makes the deployment of your applications really easy.</p> <ul> <li>In your project directory create the <code>Dockerfile</code> file:</li> </ul> <pre><code>FROM tiangolo/uvicorn-gunicorn-fastapi:python3.7\n\nCOPY ./app /app\n</code></pre> <ul> <li> <p>Go to the project directory (in where your Dockerfile is, containing your app   directory).</p> </li> <li> <p>Build your FastAPI image:</p> </li> </ul> <pre><code>docker build -t myimage .\n</code></pre> <ul> <li>Run a container based on your image:</li> </ul> <pre><code>docker run -d --name mycontainer -p 80:80 myimage\n</code></pre> <p>Now you have an optimized FastAPI server in a Docker container. Auto-tuned for your current server (and number of CPU cores).</p>"}, {"location": "fastapi/#installing-dependencies", "title": "Installing dependencies", "text": "<p>If your program needs other dependencies, use the next dockerfile:</p> <pre><code>FROM tiangolo/uvicorn-gunicorn-fastapi:python3.7\n\nCOPY ./requirements.txt /app\nRUN pip install -r requirements.txt\n\nCOPY ./app /app\n</code></pre>"}, {"location": "fastapi/#other-project-structures", "title": "Other project structures", "text": "<p>The previous examples assume that you have followed the FastAPI project structure. If instead you've used mine your application will be defined in the <code>app</code> variable in the <code>src/program_name/entrypoints/api.py</code> file.</p> <p>To make things simpler make the <code>app</code> variable available on the root of your package, so you can do <code>from program_name import app</code> instead of <code>from program_name.entrypoints.api import app</code>. To do that we need to add <code>app</code> to the <code>__all__</code> internal python variable of the <code>__init__.py</code> file of our package.</p> <p>File: <code>src/program_name/__init__.py</code>:</p> <pre><code>from .entrypoints.ap\nimport app\n\n__all__: List[str] = ['app']\n</code></pre> <p>The image is configured through environmental variables</p> <p>So we will need to use:</p> <pre><code>FROM tiangolo/uvicorn-gunicorn-fastapi:python3.7\n\nENV MODULE_NAME=\"program_name\"\n\nCOPY ./src/program_name /app/program_name\n</code></pre>"}, {"location": "fastapi/#testing", "title": "Testing", "text": "<p>FastAPI gives a <code>TestClient</code> object borrowed from Starlette to do the integration tests on your application.</p> <pre><code>from fastapi import FastAPI\nfrom fastapi.testclient import TestClient\n\napp = FastAPI()\n\n\n@app.get(\"/\")\nasync def read_main():\n    return {\"msg\": \"Hello World\"}\n\n\n@pytest.fixture(name=\"client\")\ndef client_() -&gt; TestClient:\n\"\"\"Configure FastAPI TestClient.\"\"\"\n    return TestClient(app)\n\n\ndef test_read_main(client: TestClient):\n    response = client.get(\"/\")\n    assert response.status_code == 200\n    assert response.json() == {\"msg\": \"Hello World\"}\n</code></pre>"}, {"location": "fastapi/#test-a-post-request", "title": "Test a POST request", "text": "<pre><code>result = client.post(\n    \"/items/\",\n    headers={\"X-Token\": \"coneofsilence\"},\n    json={\"id\": \"foobar\", \"title\": \"Foo Bar\", \"description\": \"The Foo Barters\"},\n)\n</code></pre>"}, {"location": "fastapi/#inject-testing-configuration", "title": "Inject testing configuration", "text": "<p>If your application follows the application configuration section, injecting testing configuration is easy with dependency injection.</p> <p>Imagine you have a <code>db_tinydb</code> fixture that sets up the testing database:</p> <pre><code>@pytest.fixture(name=\"db_tinydb\")\ndef db_tinydb_(tmp_path: Path) -&gt; str:\n\"\"\"Create an TinyDB database engine.\n\n    Returns:\n        database_url: Url used to connect to the database.\n    \"\"\"\n    tinydb_file_path = str(tmp_path / \"tinydb.db\")\n    return f\"tinydb:///{tinydb_file_path}\"\n</code></pre> <p>You can override the default <code>database_url</code> with:</p> <pre><code>@pytest.fixture(name=\"client\")\ndef client_(db_tinydb: str) -&gt; TestClient:\n\"\"\"Configure FastAPI TestClient.\"\"\"\n\n    def override_settings() -&gt; Settings:\n\"\"\"Inject the testing database in the application settings.\"\"\"\n        return Settings(database_url=db_tinydb)\n\n    app.dependency_overrides[get_settings] = override_settings\n    return TestClient(app)\n</code></pre>"}, {"location": "fastapi/#add-endpoints-only-on-testing-environment", "title": "Add endpoints only on testing environment", "text": "<p>Sometimes you want to have some API endpoints to populate the database for end to end testing the frontend. If your <code>app</code> config has the <code>environment</code> attribute, you could try to do:</p> <pre><code>app = FastAPI()\n\n\n@lru_cache()\ndef get_config() -&gt; Config:\n\"\"\"Configure the program settings.\"\"\"\n    # no cover: the dependency are injected in the tests\n    log.info(\"Loading the config\")\n    return Config()  # pragma: no cover\n\n\nif get_config().environment == \"testing\":\n\n    @app.get(\"/seed\", status_code=201)\n    def seed_data(\n        repo: Repository = Depends(get_repo),\n        empty: bool = True,\n        num_articles: int = 3,\n        num_sources: int = 2,\n    ) -&gt; None:\n\"\"\"Add seed data for the end to end tests.\n\n        Args:\n            repo: Repository to store the data.\n        \"\"\"\n        services.seed(\n            repo=repo, empty=empty, num_articles=num_articles, num_sources=num_sources\n        )\n        repo.close()\n</code></pre> <p>But the injection of the dependencies is only done inside the functions, so <code>get_config().environment</code> will always be the default value. I ended up doing that check inside the endpoint, which is not ideal.</p> <pre><code>@app.get(\"/seed\", status_code=201)\ndef seed_data(\n    config: Config = Depends(get_config),\n    repo: Repository = Depends(get_repo),\n    empty: bool = True,\n    num_articles: int = 3,\n    num_sources: int = 2,\n) -&gt; None:\n\"\"\"Add seed data for the end to end tests.\n\n    Args:\n        repo: Repository to store the data.\n    \"\"\"\n    if config.environment != \"testing\":\n        repo.close()\n        raise HTTPException(status_code=404)\n    ...\n</code></pre>"}, {"location": "fastapi/#tips-and-tricks", "title": "Tips and tricks", "text": ""}, {"location": "fastapi/#create-redirections", "title": "Create redirections", "text": "<p>Returns an HTTP redirect. Uses a 307 status code (Temporary Redirect) by default.</p> <pre><code>from fastapi import FastAPI\nfrom fastapi.responses import RedirectResponse\n\napp = FastAPI()\n\n\n@app.get(\"/typer\")\nasync def read_typer():\n    return RedirectResponse(\"https://typer.tiangolo.com\")\n</code></pre>"}, {"location": "fastapi/#test-that-your-application-works-locally", "title": "Test that your application works locally", "text": "<p>Once you have your application built and tested, everything should work right? well, sometimes it don't. If you need to use <code>pdb</code> to debug what's going on, you can't use the docker as you won't be able to interact with the debugger.</p> <p>Instead, launch an uvicorn application directly with:</p> <pre><code>uvicorn program_name:app --reload\n</code></pre> <p>Note: The command is assuming that your <code>app</code> is available at the root of your package, look at the deploy section if you feel lost.</p>"}, {"location": "fastapi/#resolve-the-307-error", "title": "Resolve the 307 error", "text": "<p>Probably you've introduced an ending <code>/</code> to the endpoint, so instead of asking for <code>/my/endpoint</code> you tried to do <code>/my/endpoint/</code>.</p>"}, {"location": "fastapi/#resolve-the-409-error", "title": "Resolve the 409 error", "text": "<p>Probably an exception was raised in the backend, use <code>pdb</code> to follow the trace and catch where it happened.</p>"}, {"location": "fastapi/#resolve-the-422-error", "title": "Resolve the 422 error", "text": "<p>You're probably passing the wrong arguments to the POST request, to solve it see the <code>text</code> attribute of the result. For example:</p> <pre><code># client: TestClient\nresult = client.post(\n    \"/source/add\",\n    json={\"body\": body},\n)\n\nresult.text\n# '{\"detail\":[{\"loc\":[\"query\",\"url\"],\"msg\":\"field required\",\"type\":\"value_error.missing\"}]}'\n</code></pre> <p>The error is telling us that the required <code>url</code> parameter is missing.</p>"}, {"location": "fastapi/#logging", "title": "Logging", "text": "<p>By default the application log messages are not shown in the uvicorn log, you need to add the next lines to the file where your app is defined:</p> <p>File: <code>src/program_name/entrypoints/api.py</code>:</p> <pre><code>from fastapi import FastAPI\nfrom fastapi.logger import logger\nimport logging\n\nlog = logging.getLogger(\"gunicorn.error\")\nlogger.handlers = log.handlers\nif __name__ != \"main\":\n    logger.setLevel(log.level)\nelse:\n    logger.setLevel(logging.DEBUG)\n\napp = FastAPI()\n\n# rest of the application...\n</code></pre>"}, {"location": "fastapi/#logging-to-sentry", "title": "Logging to Sentry", "text": "<p>FastAPI can integrate with Sentry or similar application loggers through the ASGI middleware.</p>"}, {"location": "fastapi/#run-a-fastapi-server-in-the-background-for-testing-purposes", "title": "Run a FastAPI server in the background for testing purposes", "text": "<p>Sometimes you want to launch a web server with a simple API to test a program that can't use the testing client. First define the API to launch with:</p> <p>File: <code>tests/api_server.py</code>:</p> <pre><code>from fastapi import FastAPI, HTTPException\n\napp = FastAPI()\n\n\n@app.get(\"/existent\")\nasync def existent():\n    return {\"msg\": \"exists!\"}\n\n\n@app.get(\"/inexistent\")\nasync def inexistent():\n    raise HTTPException(status_code=404, detail=\"It doesn't exist\")\n</code></pre> <p>Then create the fixture:</p> <p>File: <code>tests/conftest.py</code>:</p> <pre><code>from multiprocessing import Process\n\nfrom typing import Generator\nimport pytest\nimport uvicorn\n\nfrom .api_server import app\n\n\ndef run_server() -&gt; None:\n\"\"\"Command to run the fake api server.\"\"\"\n    uvicorn.run(app)\n\n\n@pytest.fixture()\ndef _server() -&gt; Generator[None, None, None]:\n\"\"\"Start the fake api server.\"\"\"\n    proc = Process(target=run_server, args=(), daemon=True)\n    proc.start()\n    yield\n    proc.kill()  # Cleanup after test\n</code></pre> <p>Now you can use the <code>server: None</code> fixture in your tests and run your queries against <code>http://localhost:8000</code>.</p>"}, {"location": "fastapi/#interesting-features-to-explore", "title": "Interesting features to explore", "text": "<ul> <li>Structure big applications.</li> <li>Dependency injection.</li> <li>Running background tasks after the request is finished.</li> <li>Return a different response model.</li> <li>Upload files.</li> <li>Set authentication.</li> <li>Host behind a proxy.</li> <li>Static files.</li> </ul>"}, {"location": "fastapi/#issues", "title": "Issues", "text": "<ul> <li>FastAPI does not log messages:   update <code>pyscrobbler</code> and any other maintained applications and remove the   snippet defined in the logging section.</li> </ul>"}, {"location": "fastapi/#references", "title": "References", "text": "<ul> <li> <p>Docs</p> </li> <li> <p>Git</p> </li> <li> <p>Awesome FastAPI</p> </li> <li> <p>Testdriven.io course: suggested   by the developer.</p> </li> </ul>"}, {"location": "ferdium/", "title": "Ferdium", "text": "<p>Ferdium is a desktop application to have all your services in one place. It's similar to Rambox, Franz or Ferdi only that it's maintained by the community and respects your privacy.</p>"}, {"location": "ferdium/#installation", "title": "Installation", "text": "<p>Download the deb package and run</p> <pre><code>sudo dpkg -i /path/to/your/file.deb\n</code></pre>"}, {"location": "ferdium/#security", "title": "Security", "text": "<p>In terms of security the Ferdium master password lock will only prevent an attacker from accessing your passwords if it has very few time to do the attack. They encrypt the password and save it in the config file along with a property <code>lockingFeatureEnabled</code> which is set to <code>true</code> when you activate this feature. Nevertheless if an attacker were to change this value to <code>false</code>, then they'll be able to access your Ferdium instance.</p> <p>Therefore I think that it's better to rely on locking your computer when leaving it and encrypting your hard drive. Adding the master password will only make the life harder for you for no substantial increase in security.</p> <p>Keep in mind that Ferdium stores the cookies to automatically log in the sites and that the information is accessible by an attacker that has access to your device. So only add services that are not critical to you.</p>"}, {"location": "ferdium/#references", "title": "References", "text": "<ul> <li>Homepage</li> </ul>"}, {"location": "ffmpeg/", "title": "ffmpeg", "text": "<p>ffmpeg is a complete, cross-platform solution to record, convert and stream audio and video</p> <p>You can run <code>ffmpeg -formats</code> to get a list of every format that is supported.</p>"}, {"location": "ffmpeg/#cut", "title": "Cut", "text": ""}, {"location": "ffmpeg/#cut-video-file-into-a-shorter-clip", "title": "Cut video file into a shorter clip", "text": "<p>You can use the time offset parameter <code>-ss</code> to specify the start time stamp in <code>HH:MM:SS.ms</code> format while the <code>-t</code> parameter is for specifying the actual duration of the clip in seconds.</p> <pre><code>ffmpeg -i input.mp4 -ss 00:00:50.0 -codec copy -t 20 output.mp4\n</code></pre>"}, {"location": "ffmpeg/#split-a-video-into-multiple-parts", "title": "Split a video into multiple parts", "text": "<p>The next command will split the source video into 2 parts. One ending at 50s from the start and the other beginning at 50s and ending at the end of the input video.</p> <pre><code>ffmpeg -i video.mp4 -t 00:00:50 -c copy small-1.mp4 -ss 00:00:50 -codec copy small-2.mp4\n</code></pre>"}, {"location": "ffmpeg/#crop-an-audio-file", "title": "Crop an audio file", "text": "<p>To create a 30 second audio file starting at 90 seconds from the original audio file without transcoding use:</p> <pre><code>ffmpeg -ss 00:01:30 -t 30 -acodec copy -i inputfile.mp3 outputfile.mp3\n</code></pre>"}, {"location": "ffmpeg/#join", "title": "Join", "text": ""}, {"location": "ffmpeg/#join-concatenate-video-files", "title": "Join (concatenate) video files", "text": "<p>If you have multiple audio or video files encoded with the same codecs, you can join them into a single file. Create a input file with a list of all source files that you wish to concatenate and then run this command.</p> <p>Create first the file list with a Bash for loop:</p> <pre><code>for f in ./*.wav; do echo \"file '$f'\" &gt;&gt; mylist.txt; done\n</code></pre> <p>Then convert</p> <pre><code>ffmpeg -f concat -safe 0 -i mylist.txt -c copy output.mp4\n</code></pre>"}, {"location": "ffmpeg/#merge-an-audio-and-video-file", "title": "Merge an audio and video file", "text": "<p>You can also specify the -shortest switch to finish the encoding when the shortest clip ends.</p> <pre><code>ffmpeg -i video.mp4 -i audio.mp3 -c:v copy -c:a aac -strict experimental output.mp4\nffmpeg -i video.mp4 -i audio.mp3 -c:v copy -c:a aac -strict experimental -shortest output.mp4\n</code></pre>"}, {"location": "ffmpeg/#mute", "title": "Mute", "text": "<p>Use the <code>-an</code> parameter to disable the audio portion of a video stream.</p> <pre><code>ffmpeg -i video.mp4 -an mute-video.mp4\n</code></pre>"}, {"location": "ffmpeg/#convert", "title": "Convert", "text": ""}, {"location": "ffmpeg/#convert-video-from-one-format-to-another", "title": "Convert video from one format to another", "text": "<p>You can use the <code>-vcodec</code> parameter to specify the encoding format to be used for the output video. Encoding a video takes time but you can speed up the process by forcing a preset though it would degrade the quality of the output video.</p> <pre><code>ffmpeg -i youtube.flv -c:v libx264 filename.mp4\nffmpeg -i video.wmv -c:v libx264 -preset ultrafast video.mp4\n</code></pre>"}, {"location": "ffmpeg/#convert-a-x265-file-into-x264", "title": "Convert a x265 file into x264", "text": "<pre><code>for i in *.mkv ; do\nffmpeg -i \"$i\" -bsf:v h264_mp4toannexb -vcodec libx264 \"$i.x264.mkv\"\ndone\n</code></pre> <ul> <li><code>ffmpeg -i \"$i\"</code>: Executes the program ffmpeg and calls for files to be     processed.</li> <li><code>-bsf:v</code>: Activates the video bit stream filter to be used.</li> <li> <p><code>h264_mp4toannexb</code>: Is the bit stream filter that is activated.     Convert an H.264 bitstream from length prefixed mode to start code prefixed     mode (as defined in the Annex B of the ITU-T H.264 specification).</p> <p>This is required by some streaming formats, typically the MPEG-2 transport stream format (mpegts) processing MKV h.264 (currently)requires this, if is not included you will get an error in the terminal window instructing you to use it. * <code>-vcodec libx264</code> This tells ffmpeg to encode the output to H.264. * <code>\"$i.ts\"</code> Saves the output to .ts format, this is useful so as not to overwrite your source files.</p> </li> </ul>"}, {"location": "ffmpeg/#convert-vob-to-mkv", "title": "Convert VOB to mkv", "text": "<ul> <li> <p>Unify your VOBs     <pre><code>cat *.VOB &gt; output.vob\n</code></pre></p> </li> <li> <p>Identify the streams</p> <pre><code>ffmpeg -analyzeduration 100M -probesize 100M -i output.vob\n</code></pre> <p>Select the streams that you are interested in, imagine that is 1, 3, 4, 5 and 6.</p> </li> <li> <p>Encoding</p> <pre><code>ffmpeg \\\n-analyzeduration 100M -probesize 100M \\\n-i output.vob \\\n-map 0:1 -map 0:3 -map 0:4 -map 0:5 -map 0:6 \\\n-metadata:s:a:0 language=ita -metadata:s:a:0 title=\"Italian stereo\" \\\n-metadata:s:a:1 language=eng -metadata:s:a:1 title=\"English stereo\" \\\n-metadata:s:s:0 language=ita -metadata:s:s:0 title=\"Italian\" \\\n-metadata:s:s:1 language=eng -metadata:s:s:1 title=\"English\" \\\n-codec:v libx264 -crf 21 \\\n-codec:a libmp3lame -qscale:a 2 \\\n-codec:s copy \\\noutput.mkv\n</code></pre> </li> </ul>"}, {"location": "ffmpeg/#convert-a-video-into-animated-gif", "title": "Convert a video into animated GIF", "text": "<pre><code>ffmpeg -ss 30 -t 3 -i input.mp4 -vf \"fps=10,scale=480:-1:flags=lanczos,split[s0][s1];[s0]palettegen[p];[s1][p]paletteuse\" -loop 0 output.gif\n</code></pre> <ul> <li>This example will skip the first 30 seconds (-ss 30) of the input and create     a 3 second output (-t 3).</li> <li>fps filter sets the frame rate. A rate of 10 frames per second is used in the     example.</li> <li>Scale filter will resize the output to 320 pixels wide and automatically     determine the height while preserving the aspect ratio. The lanczos scaling     algorithm is used in this example.</li> <li>Palettegen and paletteuse filters will generate and use a custom palette     generated from your input. These filters have many options, so refer to the     links for a list of all available options and values. Also see the Advanced     options section below.</li> <li>Split filter will allow everything to be done in one command and avoids having     to create a temporary PNG file of the palette.</li> <li>Control looping with -loop output option but the values are confusing. A value     of 0 is infinite looping, -1 is no looping, and 1 will loop once meaning it     will play twice. So a value of 10 will cause the GIF to play 11 times.</li> </ul>"}, {"location": "ffmpeg/#convert-video-into-images", "title": "Convert video into images", "text": "<p>You can use FFmpeg to automatically extract image frames from a video every <code>n</code> seconds and the images are saved in a sequence. This command saves image frame after every 4 seconds.</p> <pre><code>ffmpeg -i movie.mp4 -r 0.25 frames_%04d.png\n</code></pre>"}, {"location": "ffmpeg/#convert-a-single-image-into-a-video", "title": "Convert a single image into a video", "text": "<p>Use the <code>-t</code> parameter to specify the duration of the video.</p> <pre><code>ffmpeg -loop 1 -i image.png -c:v libx264 -t 30 -pix_fmt yuv420p video.mp4\n</code></pre>"}, {"location": "ffmpeg/#convert-opus-or-wav-to-mp3", "title": "Convert opus or wav to mp3", "text": "<pre><code>ffmpeg -i input.wav -vn -ar 44100 -ac 2 -b:a 320k output.mp3\n</code></pre> <ul> <li><code>-i</code>: input file.</li> <li><code>-vn</code>: Disable video, to make sure no video (including album cover image) is     included if the source would be a video file.</li> <li><code>-ar</code>: Set the audio sampling frequency. For output streams it is set by     default to the frequency of the corresponding input stream. For input     streams this option only makes sense for audio grabbing devices and raw     demuxers and is mapped to the corresponding demuxer options.</li> <li><code>-ac</code>: Set the number of audio channels. For output streams it is set by     default to the number of input audio channels. For input streams this option     only makes sense for audio grabbing devices and raw demuxers and is mapped     to the corresponding demuxer options. So used here to make sure it is stereo     (2 channels).</li> <li><code>-b:a</code>: Converts the audio bitrate to be exact 320kbit per second.</li> </ul>"}, {"location": "ffmpeg/#extract", "title": "Extract", "text": ""}, {"location": "ffmpeg/#extract-the-audio-from-video", "title": "Extract the audio from video", "text": "<p>The <code>-vn</code> switch extracts the audio portion from a video and we are using the <code>-ab</code> switch to save the audio as a 256kbps MP3 audio file.</p> <pre><code>ffmpeg -i video.mp4 -vn -ab 256 audio.mp3\n</code></pre>"}, {"location": "ffmpeg/#extract-image-frames-from-a-video", "title": "Extract image frames from a video", "text": "<p>This command will extract the video frame at the 15s mark and saves it as a 800px wide JPEG image. You can also use the -s switch (like -s 400\u00d7300) to specify the exact dimensions of the image file though it will probably create a stretched image if the image size doesn\u2019t follow the aspect ratio of the original video file.</p> <pre><code>ffmpeg -ss 00:00:15 -i video.mp4 -vf scale=800:-1 -vframes 1 image.jpg\n</code></pre>"}, {"location": "ffmpeg/#extract-metadata-of-video", "title": "Extract metadata of video", "text": "<pre><code>ffprobe {{ file }}\n</code></pre>"}, {"location": "ffmpeg/#resize", "title": "Resize", "text": ""}, {"location": "ffmpeg/#resize-a-video", "title": "Resize a video", "text": ""}, {"location": "ffmpeg/#change-the-constat-rate-factor", "title": "Change the Constat Rate Factor", "text": "<p>Setting the Constant Rate Factor, which lowers the average bit rate, but retains better quality. Vary the CRF between around 18 and 24 \u2014 the lower, the higher the bitrate.</p> <pre><code>ffmpeg -i input.mp4 -vcodec libx265 -crf 24 output.mp4\n</code></pre> <p>Change the codec as needed - libx264 may be available if libx265 is not, at the cost of a slightly larger resultant file size.</p>"}, {"location": "ffmpeg/#change-video-resolution", "title": "Change video resolution", "text": "<p>Use the size <code>-s</code> switch with ffmpeg to resize a video while maintaining the aspect ratio.</p> <pre><code>ffmpeg -i input.mp4 -s 480x320 -c:a copy output.mp4\n</code></pre>"}, {"location": "ffmpeg/#presentation", "title": "Presentation", "text": ""}, {"location": "ffmpeg/#create-video-slideshow-from-images", "title": "Create video slideshow from images", "text": "<p>This command creates a video slideshow using a series of images that are named as <code>img001.png</code>, <code>img002.png</code>, etc. Each image will have a duration of 5 seconds (-r \u2155).</p> <pre><code>ffmpeg -r 1/5 -i img%03d.png -c:v libx264 -r 30 -pix_fmt yuv420p slideshow.mp4\n</code></pre>"}, {"location": "ffmpeg/#add-a-poster-image-to-audio", "title": "Add a poster image to audio", "text": "<p>You can add a cover image to an audio file and the length of the output video will be the same as that of the input audio stream. This may come handy for uploading MP3s to YouTube.</p> <pre><code>ffmpeg -loop 1 -i image.jpg -i audio.mp3 -c:v libx264 -c:a aac -strict experimental -b:a 192k -shortest output.mp4\n</code></pre>"}, {"location": "ffmpeg/#add-subtitles-to-a-movie", "title": "Add subtitles to a movie", "text": "<p>This will take the subtitles from the <code>.srt</code> file. FFmpeg can decode most common subtitle formats.</p> <pre><code>ffmpeg -i movie.mp4 -i subtitles.srt -map 0 -map 1 -c copy -c:v libx264 -crf 23 -preset veryfast output.mkv\n</code></pre>"}, {"location": "ffmpeg/#change-the-audio-volume", "title": "Change the audio volume", "text": "<p>You can use the volume filter to alter the volume of a media file using FFmpeg. This command will half the volume of the audio file.</p> <pre><code>ffmpeg -i input.wav -af 'volume=0.5' output.wav\n</code></pre>"}, {"location": "ffmpeg/#rotate-a-video", "title": "Rotate a video", "text": "<p>This command will rotate a video clip 90\u00b0 clockwise. You can set transpose to 2 to rotate the video 90\u00b0 anti-clockwise.</p> <pre><code>ffmpeg -i input.mp4 -filter:v 'transpose=1' rotated-video.mp4\n</code></pre> <p>This will rotate the video 180\u00b0 counter-clockwise.</p> <pre><code>ffmpeg -i input.mp4 -filter:v 'transpose=2,transpose=2' rotated-video.mp4\n</code></pre>"}, {"location": "ffmpeg/#speed-up-or-slow-down-the-video", "title": "Speed up or Slow down the video", "text": "<p>You can change the speed of your video using the setpts (set presentation time stamp) filter of FFmpeg. This command will make the video 8x (\u215b) faster or use setpts=4*PTS to make the video 4x slower.</p> <pre><code>ffmpeg -i input.mp4 -filter:v \"setpts=0.125*PTS\" output.mp4\n</code></pre>"}, {"location": "ffmpeg/#speed-up-or-slow-down-the-audio", "title": "Speed up or Slow down the audio", "text": "<p>For changing the speed of audio, use the atempo audio filter. This command will double the speed of audio. You can use any value between 0.5 and 2.0 for audio.</p> <p><code>bash ffmpeg -i input.mkv -filter:a \"atempo=2.0\" -vn output.mkv</code></p> <p>Stack Exchange has a good overview to get you started with FFmpeg. You should also check out the official documentation at ffmpeg.org or the wiki at trac.ffmpeg.org to know about all the possible things you can do with FFmpeg.</p>"}, {"location": "ffmpeg/#references", "title": "References", "text": "<ul> <li>Home</li> </ul>"}, {"location": "finnix/", "title": "Finnix", "text": "<p>Finnix is a live Linux distribution specialized in the recovery, maintenance, testing of systems.</p>"}, {"location": "finnix/#installation", "title": "Installation", "text": "<ul> <li>Download the latest version from the web</li> <li>Load it into a usb:    <pre><code>dd if=/path/to/your/finnix.iso of=/dev/path/to/your/disk\n</code></pre></li> </ul> <p>!!! warning \"Be sure that the <code>/dev/path/to/your/disk</code> is the one you want to overwrite, you may end up fucking your device hard drive instead!\"</p>"}, {"location": "finnix/#references", "title": "References", "text": "<ul> <li>Home</li> </ul>"}, {"location": "fitness_band/", "title": "Fitness tracker", "text": "<p>Fitness tracker or activity trackers are devices or applications for monitoring and tracking fitness-related metrics such as distance walked or run, calorie consumption, and in some cases heartbeat. It is a type of wearable computer.</p> <p>As with anything that can be bought, I usually first try a cheap model to see if I need the advanced features that the expensive ones offer. After a quick model review, I went for the Amazfit band 5.</p> <p>I've now discovered wasp-os an open source firmware for smart watches that are based on the nRF52 family of microcontrollers. Fully supported by gadgetbridge, Wasp-os features full heart rate monitoring and step counting support together with multiple clock faces, a stopwatch, an alarm clock, a countdown timer, a calculator and lots of other games and utilities. All of this, and still with access to the MicroPython REPL for interactive tweaking, development and testing.</p> <p>Currently it support the following devices:</p> <ul> <li>Colmi P8</li> <li>Senbono K9</li> <li>Pine64 PineTime</li> </ul> <p>Pinetime seems to be a work in progress, Colmi P8 looks awesome, and the Senbono K9 looks good too but wasp-os is lacking touch screen support.</p> <p>So if I had to choose now, I'd try the Colmi P8, the only thing that I'd miss is the possible voice assistant support. They say that you can take one for 18$ in aliexpress.</p>"}, {"location": "flakeheaven/", "title": "Flakeheaven", "text": "<p>Flakeheaven is a Flake8 wrapper to make it cool.</p> <p>Some of it's features are:</p> <ul> <li>Lint md, rst, ipynb, and     more.</li> <li>Shareable and remote     configs.</li> <li>Legacy-friendly:     ability to get report only about new errors.</li> <li>Caching for much better performance.</li> <li>Use only specified     plugins, not     everything installed.</li> <li>Make output beautiful.</li> <li>pyproject.toml support.</li> <li>Check that all required plugins are     installed.</li> <li>Syntax highlighting in messages and code     snippets.</li> <li>PyLint integration.</li> <li>Remove unused noqa.</li> <li>Powerful GitLab support.</li> <li>Codes management:<ul> <li>Manage codes per plugin.</li> <li>Enable and disable plugins and codes by wildcard.</li> <li>Show codes for installed plugins.</li> <li>Show all messages and codes for a plugin.</li> <li>Allow codes intersection for different plugins.</li> </ul> </li> </ul> <p>You can use this cookiecutter template to create a python project with <code>flakeheaven</code> already configured.</p>"}, {"location": "flakeheaven/#installation", "title": "Installation", "text": "<pre><code>pip install flakeheaven\n</code></pre>"}, {"location": "flakeheaven/#configuration", "title": "Configuration", "text": "<p>Flakeheaven can be configured in pyproject.toml. You can specify any Flake8 options and Flakeheaven-specific parameters.</p>"}, {"location": "flakeheaven/#plugins", "title": "Plugins", "text": "<p>In <code>pyproject.toml</code> you can specify <code>[tool.flakeheaven.plugins]</code> table. It's a list of flake8 plugins and associated to them rules.</p> <p>Key can be exact plugin name or wildcard template. For example <code>\"flake8-commas\"</code> or <code>\"flake8-*\"</code>. Flakeheaven will choose the longest match for every plugin if possible. In the previous example, <code>flake8-commas</code> will match to the first pattern, <code>flake8-bandit</code> and <code>flake8-bugbear</code> to the second, and <code>pycodestyle</code> will not match to any pattern.</p> <p>Value is a list of templates for error codes for this plugin. First symbol in every template must be <code>+</code> (include) or <code>-</code> (exclude). The latest matched pattern wins. For example, <code>[\"+*\", \"-F*\", \"-E30?\", \"-E401\"]</code> means \"Include everything except all checks that starts with <code>F</code>, check from <code>E301</code> to <code>E310</code>, and <code>E401</code>\".</p> <p>Example: pyproject.toml</p> <pre><code>[tool.flakeheaven]\n# specify any flake8 options. For example, exclude \"example.py\":\nexclude = [\"example.py\"]\n# make output nice\nformat = \"grouped\"\n# don't limit yourself\nmax_line_length = 120\n# show line of source code in output\nshow_source = true\n\n# list of plugins and rules for them\n[tool.flakeheaven.plugins]\n# include everything in pyflakes except F401\npyflakes = [\"+*\", \"-F401\"]\n# enable only codes from S100 to S199\nflake8-bandit = [\"-*\", \"+S1??\"]\n# enable everything that starts from `flake8-`\n\"flake8-*\" = [\"+*\"]\n# explicitly disable plugin\nflake8-docstrings = [\"-*\"]\n\n# disable some checks for tests\n[tool.flakeheaven.exceptions.\"tests/\"]\npycodestyle = [\"-F401\"]     # disable a check\npyflakes = [\"-*\"]           # disable a plugin\n\n# do not disable `pyflakes` for one file in tests\n[tool.flakeheaven.exceptions.\"tests/test_example.py\"]\npyflakes = [\"+*\"]           # enable a plugin\n</code></pre> <p>Check a complete list of flake8 extensions.</p> <ul> <li>flake8-bugbear: Finding likely bugs     and design problems in your program. Contains warnings that don't belong in     pyflakes and pycodestyle.</li> <li>flake8-fixme: Check for FIXME,     TODO and other temporary developer notes.</li> <li>flake8-debugger: Check for     <code>pdb</code> or <code>idbp</code> imports and set traces.</li> <li>flake8-mutable: Checks for     mutable default     arguments anti-pattern.</li> <li>flake8-pytest: Check for uses of     Django-style assert-statements in tests. So no more <code>self.assertEqual(a, b)</code>,     but instead <code>assert a == b</code>.</li> <li>flake8-pytest-style: Checks     common style issues or inconsistencies with pytest-based tests.</li> <li>flake8-simplify: Helps you     to simplify code.</li> <li>flake8-variables-names:     Helps to make more readable variables names.</li> <li>pep8-naming: Check your code against     PEP 8 naming conventions.</li> <li>flake8-expression-complexity:     Check expression complexity.</li> <li>flake8-use-fstring:     Checks you're using f-strings.</li> <li>flake8-docstrings: adds an     extension for the fantastic     pydocstyle tool to Flake8.</li> <li>flake8-markdown: lints     GitHub-style Python code blocks in Markdown files using flake8.</li> <li>pylint is a Python static code analysis     tool which looks for programming errors, helps enforcing a coding standard,     sniffs for code smells and offers simple refactoring suggestions.</li> <li>dlint: Encourage best coding practices     and helping ensure Python code is secure.</li> <li>flake8-aaa: Checks Python tests     follow the Arrange-Act-Assert     pattern.</li> <li>flake8-annotations-complexity:     Report on too complex type annotations.</li> <li>flake8-annotations: Detects the     absence of PEP 3107-style function annotations and PEP 484-style type     comments.</li> <li>flake8-typing-imports:     Checks that typing imports are properly guarded.</li> <li>flake8-comprehensions:     Help you write better list/set/dict comprehensions.</li> <li>flake8-eradicate:  find     commented out (or so called \"dead\") code.</li> </ul>"}, {"location": "flakeheaven/#usage", "title": "Usage", "text": "<p>When using Flakeheaven, I frequently use the following commands:</p> <code>flakeheaven lint</code> Runs the linter, similar to the flake8 command. <code>flakeheaven plugins</code> Lists all the plugins used, and their configuration status. <code>flakeheaven missed</code> Shows any plugins that are in the configuration but not installed properly. <code>flakeheaven code S322</code> (or any other code) Shows the explanation for that specific warning code. <code>flakeheaven yesqa</code> Removes unused codes from <code># noqa</code> and removes bare noqa that says \u201cignore everything on this line\u201d as is a bad practice."}, {"location": "flakeheaven/#integrations", "title": "Integrations", "text": "<p>Flakeheaven checks can be run in:</p> <ul> <li> <p>In Vim though the ALE plugin.</p> </li> <li> <p>Through a pre-commit:</p> <pre><code>  - repo: https://github.com/flakeheaven/flakeheaven\nrev: master\nhooks:\n- name: Run flakeheaven static analysis tool\nid: flakeheaven\n</code></pre> </li> <li> <p>In the CI:     <pre><code>  - name: Test linters\nrun: make lint\n</code></pre></p> <p>Assuming you're using a Makefile like the one in my cookiecutter-python-project.</p> </li> </ul>"}, {"location": "flakeheaven/#issues", "title": "Issues", "text": "<ul> <li>ImportError: cannot import name 'MergedConfigParser' from     'flake8.options.config':     remove the dependency pin in cookiecutter template and propagate to all     projects.</li> <li> <p>'Namespace' object has no attribute 'extended_default_ignore'     error:     Until it's fixed either use a version below or equal to 3.9.0, or add to     your <code>pyproject.toml</code>:</p> <pre><code>[tool.flakeheaven]\nextended_default_ignore=[]  # add this\n</code></pre> <p>Once it's fixed, remove the patch from the maintained projects.</p> </li> </ul>"}, {"location": "flakeheaven/#troubleshooting", "title": "Troubleshooting", "text": ""}, {"location": "flakeheaven/#namespace-object-has-no-attribute", "title": "['Namespace' object has no attribute", "text": "<p>'extended_default_ignore'](https://githubmemory.com/repo/flakeheaven/flakeheaven/issues/10)</p> <p>Add to your <code>pyproject.toml</code>:</p> <pre><code>[tool.flakeheaven]\nextended_default_ignore=[]\n</code></pre>"}, {"location": "flakeheaven/#references", "title": "References", "text": "<ul> <li>Git</li> <li> <p>Docs</p> </li> <li> <p>Using Flake8 and pyproject.toml with Flakeheaven article by Jonathan     Bowman</p> </li> </ul>"}, {"location": "food_management/", "title": "Food management", "text": "<p>As humans diet is an important factor in our health, we need to eat daily around three times a day, as such, each week we need to invest time into managing how to get food in front of us. Tasks like thinking what do you want to eat, buying the ingredients and cooking them make use a non negligible amount of time. Also something to keep in mind, is that eating is one of the great pleasures in our lives, so doing it poorly is a waste. The last part of the equation is that to eat good you either need time or money.</p> <p>This article explores my thoughts and findings on how to optimize the use of time, money and mental load in food management while keeping the desired level of quality to enjoy each meal, being healthy and following the principles of ecology and sustainability. I'm no expert at all on either of these topics. I'm learning and making my mind while writing these lines.</p>"}, {"location": "food_management/#choosing-your-diet", "title": "Choosing your diet", "text": ""}, {"location": "food_management/#the-broad-picture-of-diet", "title": "The broad picture of diet", "text": "<p>Seasonal, vegetarian, proximity, responsible</p>"}, {"location": "food_management/#the-details-of-diet", "title": "The details of diet", "text": "<p>How to choose what to eat this week</p>"}, {"location": "food_management/#buying-the-products", "title": "Buying the products", "text": "<p>Check grocy management for more details.</p>"}, {"location": "food_management/#cooking", "title": "Cooking", "text": "<p>Week batch cooking, yearly batch cooking</p>"}, {"location": "forgejo/", "title": "Forgejo", "text": "<p>Forgejo is a self-hosted lightweight software forge. Easy to install and low maintenance, it just does the job. The awful name comes from <code>for\u011dejo</code>, the Esperanto word for forge. I kind of like though the concept of forge for the repositories.</p> <p>Brought to you by an inclusive community under the umbrella of Codeberg e.V., a democratic non-profit organization, Forgejo can be trusted to be exclusively Free Software. It is a \"soft\" fork of Gitea with a focus on scaling, federation and privacy. </p>"}, {"location": "forgejo/#history", "title": "History", "text": "<p>In October 2022 the domains and trademark of Gitea were transferred to a for-profit company without knowledge or approval of the community. Despite writing an open letter, the takeover was later confirmed. The goal of Forgejo is to continue developing the code with a healthy democratic governance.</p> <p>On the 15<sup>th</sup> of December of 2022 the project was born with these major objectives:</p> <ul> <li>The community is in control, and ensures we develop to address community needs.</li> <li>We will help liberate software development from the shackles of proprietary tools.</li> </ul> <p>One of the approaches to achieve the last point is through pushing for the Forgejo federation a much needed feature in the git web application ecosystem.</p> <p>On the 29<sup>th</sup> of December of 2022 they released the first stable release and they have released several security releases between then and now.</p>"}, {"location": "forgejo/#pros-and-cons", "title": "Pros and cons", "text": "<p>Despite what you choose, the good thing is that as long as it's a soft fork migrating between these software should be straight forward.</p> <p>Forgejo outshines Gitea in:</p> <ul> <li>Being built up by the people for the people. The project may die but it's not likely it will follow Gitea's path.</li> <li>They are transparent regarding the gobernance of the project which is created through open community discussions. </li> <li>It's a political project that fights for the people's rights, for example through federation and freely incorporating the new additions of Gitea</li> <li>They'll eventually have a better license</li> <li>They get all the features and fixes of Gitea plus the contributions of the developers of the community that run out of Gitea.</li> </ul> <p>Gitea on the other hand has the next advantages:</p> <ul> <li>It's a more stable project, it's been alive for much more time and now has the back up of a company trying to make profit out of it. Forgejo's community and structure is still evolving to a stable state though, although it looks promising!</li> <li>Quicker releases. As Forgejo needs to review and incorporate Gitea's contributions, it takes longer to do a release.</li> </ul> <p>Being a soft-fork has it's disadvantages too, for example deciding where to open the issues and pull requests, they haven't yet decided which is their policy around this topic.</p>"}, {"location": "forgejo/#references", "title": "References", "text": "<ul> <li>Source</li> <li>Issues</li> <li>Docs</li> <li>Home</li> <li>News</li> </ul>"}, {"location": "forking_this_wiki/", "title": "How to create your own wiki from this one", "text": "<p>Follow the next steps.</p>", "tags": ["wiki"]}, {"location": "forking_this_wiki/#download-the-repository", "title": "Download the repository", "text": "<p>On your terminal, clone this repository with:</p> <pre><code>git clone https://github.com/lyz-code/blue-book.git\n</code></pre> <p>I recommend against forking the repository via Github. If you do that, you'll have all the history of my repository, which will make your repository more heavy than it should (as I have a lot of images), and it will make it hard for me to make pull requests to your digital garden.</p> <p>Furthermore, you'll always see a message in your repo similar to <code>This branch is 909 commits ahead, 1030 commits behind lyz-code:master.</code> like you can see in this fork. Also if you don't want to keep all the content I've made so far and want to start from scratch then the only thing that is useful for you is the skeleton I've made, and I don't need any attribution or credit for that :P.</p> <p>If on the other hand you do want to keep all my content, then wouldn't it be better to just make contributions to this repository instead?</p> <p>Therefore the best way to give credit and attribution is by building your garden (the more we are writing the merrier :) ), and then if you want to spread the word that my garden exists within your content then that would be awesome.</p> <p>If you end up building your own, remember to add yourself to the digital garden's list.</p>", "tags": ["wiki"]}, {"location": "forking_this_wiki/#adaptations", "title": "Adaptations", "text": "", "tags": ["wiki"]}, {"location": "forking_this_wiki/#project-name-and-repository-url", "title": "Project name and repository URL", "text": "<p>There are several files that contain references to this repository's name and URL, which is different to the new forked repository URL, since the user name and the repository name might have changed. As of now, the files where you should replace the references are:</p> <ul> <li><code>README.md</code></li> <li><code>mkdocs.yml</code></li> <li><code>theme/main.html</code></li> </ul> <p>Blue book is the name of my personal digital garden, try to find a different name for your project that is meaningful to you.</p>", "tags": ["wiki"]}, {"location": "forking_this_wiki/#documents-and-structure", "title": "Documents and structure", "text": "<p>You can either use the documents of this wiki and extend them, or change the structure by editing the  <code>nav</code> section of the <code>mkdocs.yml</code> file. If you want to start from scratch, remove everything on the <code>docs</code> directory.</p> <p>If you're worried about giving credit to the blue-book then you can link each section of a document from wherever you took the information, or add a link to the bottom of the documents under a References section. Take for example the Python snippets document, each section has a link to where I took the info from. You could do the same on the articles you got from my book by linking to them.</p>", "tags": ["wiki"]}, {"location": "forking_this_wiki/#remove-the-newsletter-feature", "title": "Remove the newsletter feature", "text": "<p>The newsletter feature allows your readers to keep updated on the changes of your garden. If you don't want them:</p> <ul> <li>Remove the plugin <code>mkdocs-newsletter</code> from the <code>requirements.in</code> and     <code>mkdocs.yaml</code> files.</li> <li>Remove the references both to header and footer. To do that, undo the steps described     here.</li> <li> <p>Remove the cron configuration of the <code>.github/workflows/gh-pages.yml</code>     pipeline:</p> <pre><code>schedule:\n- cron: 11 06 * * *\n</code></pre> </li> </ul>", "tags": ["wiki"]}, {"location": "forking_this_wiki/#dependencies", "title": "Dependencies", "text": "<p>In order to be able to build your site, some Python dependencies are needed. You can install them by running</p> <pre><code>make update\n</code></pre>", "tags": ["wiki"]}, {"location": "forking_this_wiki/#checking-how-it-looks", "title": "Checking how it looks", "text": "<p>First, clean the old generated site with</p> <pre><code>make clean\n</code></pre> <p>Then, you can preview the site on your local machine by running</p> <pre><code>make\n</code></pre> <p>and then opening the link in your web browser.</p>", "tags": ["wiki"]}, {"location": "forking_this_wiki/#set-up-the-github-repository", "title": "Set up the Github repository", "text": "<p>On GitHub create a new repository by clicking on the <code>+</code> symbol on the top right and then <code>New Repository</code>.</p>", "tags": ["wiki"]}, {"location": "forking_this_wiki/#removing-the-old-commits", "title": "Removing the old commits", "text": "<p>The mkdocs-newsletter plugin uses the commit history to generate the newsletter articles, so if you want to start the newsletter from scratch, a way of doing so is removing the commit history.</p> <p>A way of doing so is removing the .git folder and re-initializing the repository. Within the repository directory do</p> <pre><code>rm -rf .git\ngit init\ngit remote add origin git@github.com:your_username/your_project_name.git\ngit add .\ngit commit -m \"Initial commit\"\ngit push --set-upstream origin master\n</code></pre> <p>Remember to change <code>your_username</code> and <code>your_project_name</code> to your real values.</p>", "tags": ["wiki"]}, {"location": "forking_this_wiki/#setting-up-github-pages", "title": "Setting up GitHub Pages", "text": "<p>To enable the Github Pages website associated with your repo, follow these steps:</p> <ul> <li>Create SSH Deploy Key.</li> <li>Activate the GitHub Pages repository configuration with the <code>gh-pages</code> branch.</li> </ul> <p>Now, the site will be built whenever you push new commits and periodically, according to the <code>cron</code> configuration from .github/workflows/gh-pages.yml.</p>", "tags": ["wiki"]}, {"location": "free_knowledge/", "title": "Free Knowledge", "text": "<p>One of the early principles of the internet has been to make knowledge free to everyone. Alexandra Elbakyan of Sci-Hub, bookwarrior of Library Genesis, Aaron Swartz, and countless unnamed others have fought to free science from the grips of for-profit publishers. Today, they do it working in hiding, alone, without acknowledgment, in fear of imprisonment, and even now wiretapped by the FBI. They sacrifice everything for one vision: Open Science.</p> <p>Some days ago, a post appeared on reddit to rescue Sci-Hub by increasing the seeders of the 850 scihub torrents. The plan is to follow the steps done last year to move Libgen to IPFS to make it more difficult for the states to bring down this marvelous collection.</p> <p>A good way to start is to look at the most ill torrents and fix their state. If you follow this path, take care of IP leaking, they're surely monitoring who's sharing.</p> <p>Another way to contribute is by following the guidelines of freeread.org and contribute to the IPFS free library. Beware though, the guidelines don't explain how to install IPFS behind a VPN or Tor. This could be contributed to the site.</p> <p>Something that is needed is a command line tool that reads the list of ill torrents, and downloads the torrents that have a low number of seeders and DHT peers. The number of torrents to download could be limited by the amount the user wants to share. A second version could have an interaction with the torrent client so that when a torrent is no longer ill, it's automatically replaced with one that is.</p>"}, {"location": "free_knowledge/#references", "title": "References", "text": "<ul> <li>FreeRead.org</li> <li>Libgen reddit</li> <li>Sci-Hub reddit</li> <li>DataHoarder reddit</li> </ul>"}, {"location": "frontend_development/", "title": "Frontend Development", "text": "<p>I've recently started learning how to make web frontends, two years ago I learned a bit of HTML, CSS, Javascript and React, but it didn't stick that much.</p> <p>This time I'm full in with Vue which in my opinion is by far prettier than React.</p>"}, {"location": "frontend_development/#newbie-tips", "title": "Newbie tips", "text": "<p>I feel completely lost xD, I don't know even how to search well what I need, it's like going back to programming 101. Funnily though, it's bringing me closer to the people I mentor on Python, I'm getting frustrated with similar things that they do, those things that you don't see when you already are proficient in a language, so here are some tips.</p>"}, {"location": "frontend_development/#dont-resize-your-browser-window", "title": "Don't resize your browser window", "text": "<p>As I use i3wm, I've caught myself resizing the browser by adding terminals above and besides the browser to see how does the site react to different screen sizes. I needed the facepalm of a work colleague, which kindly suggested to spawn the Developer Tools and move that around. If you need to resize in the other direction, change the position of the Developer tools and grow it in that direction.</p> <p>A better feature yet is to use the  Responsive Design mode, which lets you select screen sizes of existent devices, and it's easy to resize the screen.</p>"}, {"location": "frontend_development/#your-frontend-probably-doesnt-talk-to-your-backend", "title": "Your frontend probably doesn't talk to your backend", "text": "<p>If you're using Vue or a similar framework, your frontend is just a webserver (like nginx) that has some html, js and css static files whose only purpose is to serve those static files to the user. It is the user's browser the one that does all the queries, even to the backend.</p> <p>Imagine that we have a frontend application that uses a backend API behind the scenes. In the front application you'll do the queries on <code>/api</code> and depending on the environment two different things will happen:</p> <ul> <li> <p>In your development environment, when you run the development server to     manually interact with it with the browser, you configure it so that     whatever request you do to <code>/api</code> is redirected to the backend endpoint,     which usually is listening on another port on <code>localhost</code>.</p> <p>If you are doing unit or integration tests, you'll probably use your test runner to intercept those calls and mock the result.</p> <p>If you are doing E2E tests, your test runner will probably understand your development configuration and forward the requests to the backend service.</p> </li> <li> <p>In production you'll have an SSL proxy, for example linuxserver's     swag, that will forward <code>/api</code>     to the backend and the rest to the frontend.</p> </li> </ul>"}, {"location": "frontend_development/#ux-design", "title": "UX design", "text": "<p>The most popular tool out there is <code>Figma</code> but it's closed sourced, the alternative (quite popular in github) is <code>penpot</code>.</p>"}, {"location": "frontend_development/#testing", "title": "Testing", "text": ""}, {"location": "frontend_development/#write-testable-code", "title": "Write testable code", "text": "<p>Every test you write will include selectors for elements. To save yourself a lot of headaches, you should write selectors that are resilient to changes.</p> <p>Oftentimes we see users run into problems targeting their elements because:</p> <ul> <li>Your application may use dynamic classes or ID's that change.</li> <li>Your selectors break from development changes to CSS styles or JS behavior.</li> </ul> <p>Luckily, it is possible to avoid both of these problems.</p> <ul> <li>Don't target elements based on CSS attributes such as: id, class, tag.</li> <li>Don't target elements that may change their <code>textContent</code>.</li> <li>Add <code>data-*</code> attributes to make it easier to target elements.</li> </ul> <p>Given a button that we want to interact with:</p> <pre><code>&lt;button\n  id=\"main\"\n  class=\"btn btn-large\"\n  name=\"submission\"\n  role=\"button\"\n  data-cy=\"submit\"\n&gt;\n  Submit\n&lt;/button&gt;\n</code></pre> <p>Let's investigate how we could target it: | Selector                         | Recommended | Notes                                              | | cy.get('button').click()         | Never       | Worst - too generic, no context.                   | | cy.get('.btn.btn-large').click() | Never       | Bad. Coupled to styling. Highly subject to change. | | cy.get('#main').click() | Sparingly | Better. But still coupled to styling or JS event listeners. | | cy.get('[name=submission]').click() | Sparingly | Coupled to the name attribute which has HTML semantics. | | cy.contains('Submit').click() |   Depends |   Much better. But still coupled to text content that may change. | | cy.get('[data-cy=submit]').click() |  Always |    Best. Isolated from all changes. |</p>"}, {"location": "frontend_development/#conditional-testing", "title": "Conditional testing", "text": "<p>Conditional testing refers to the common programming pattern:</p> <pre><code>If X, then Y, else Z\n</code></pre> <p>Here are some examples:</p> <ul> <li>How do I do something different whether an element does or doesn't exist?</li> <li>My application does A/B testing, how do I account for that?</li> <li>My users receive a \"welcome wizard\", but existing ones don't. Can I always     close the wizard in case it's shown, and ignore it when it's not?</li> <li>I want to automatically find all  elements and based on which ones I find, I want to check that each link works. <p>The problem is - while first appearing simple, writing tests in this fashion often leads to flaky tests, random failures, and difficult to track down edge cases.</p> <p>Some interesting cases and their solutions:</p> <ul> <li>Welcome wizard</li> <li>A/B Campaign</li> </ul>"}, {"location": "frontend_learning/", "title": "Frontend learning", "text": "<p>This section is the particularization of the Development learning article for a frontend developer, in particular a Vue developer.</p>"}, {"location": "frontend_learning/#what-is-a-frontend-developer", "title": "What is a Frontend developer?", "text": "<p>A Front-End Developer is someone who creates websites and web applications. It's main responsibility is to create what the user sees.</p> <p>The basic languages for Front-End Development are HTML, CSS, and JavaScript. Nowadays writing interfaces with only the basic languages makes no sense as there are other languages and frameworks that make better and quicker solutions. One of them is Vue, which is the one I learnt, so the whole document will be focused on this path, nevertheless there are others popular ones like: Bootstrap, React, jQuery or Angular.</p> <p>The difference between Front-End and Back-End is that Front-End refers to how a web page looks, while back-end refers to how it works.</p>"}, {"location": "frontend_learning/#roadmap", "title": "Roadmap", "text": ""}, {"location": "frontend_learning/#first-steps", "title": "First steps", "text": ""}, {"location": "frontend_learning/#setup-your-development-environment", "title": "Setup your development environment", "text": ""}, {"location": "frontend_learning/#learn-the-basics", "title": "Learn the basics", "text": "<p>In order to write Vue code you first need to understand the foundations it's built upon, that means learning the basics of:</p> <ul> <li>HTML: For example following the W3     tutorial, at least until the     HTML Forms section.</li> <li>CSS: For example following the W3     tutorial until CSS Advanced.</li> <li>Javascript: For example using the W3     tutorial until JS Versions.</li> </ul> <p>If you follow other learning methods, make sure that they cover more less the same concepts.</p>"}, {"location": "fun/", "title": "Fun Stuff", "text": ""}, {"location": "fun/#coding", "title": "Coding", "text": "<p> (source)</p>"}, {"location": "gadgetbridge/", "title": "Gadgetbridge", "text": "<p>Gadgetbridge is an Android (4.4+) application which will allow you to use your Pebble, Mi Band, Amazfit Bip and HPlus device (and more) without the vendor's closed source application and without the need to create an account and transmit any of your data to the vendor's servers.</p> <p>It wont be ever be as good as the proprietary application, but it supports a good range of features, and supports a huge range of bands.</p>"}, {"location": "gadgetbridge/#data-extraction", "title": "Data extraction", "text": "<p>You can use the Data export or Data Auto export to get copy of your data.</p> <p>Here is an example of a Python program that post processes the data. Also is this post explaining how to reverse engineer the miband2 with this or this scripts. If you start the path of reverse engineering the Bluetooth protocol look at gadgetbridge guidelines.</p> <p>If you start to think on how to avoid the connection with an android phone and directly extract or interact from a linux device through python, I'd go with pybluez for the bluetooth interface, understand the band code of Gadgetbridge porting the logic to the python module, and reverse engineering the call you want to process. There isn't much in the internet following this approach, I've found an implementation for the Mi Band 4 though, which can be a good start.</p>"}, {"location": "gadgetbridge/#heartrate-measurement", "title": "Heartrate measurement", "text": "<p>Follow the official instructions.</p>"}, {"location": "gadgetbridge/#sleep", "title": "Sleep", "text": "<p>It looks that they don't yet support smart alarms.</p>"}, {"location": "gadgetbridge/#weather", "title": "Weather", "text": "<p>Follow the official instructions</p>"}, {"location": "gadgetbridge/#events", "title": "Events", "text": "<p>I haven't figured out yet how to let the events show in the \"events\" tab. Mobile calendar events show up as notifications, but you can't see the list of the next ones.</p> <p>For the Amazfit band 5, there is a bug that prevents events from showing in the reminders tab. Notifications work well though.</p>"}, {"location": "gadgetbridge/#setup", "title": "Setup", "text": "<p>In the case of the Amazfit band 5, we need to use the Huami server pairing:</p> <ul> <li>Install the Zepp application</li> <li>Create an account through the application.</li> <li>Pair your band and wait for the firmware update</li> <li>Use the python script to extract     the credentials<ul> <li><code>git clone https://github.com/argrento/huami-token.git</code></li> <li><code>pip install -r requirements.txt</code></li> <li>Run script with your credentials: <code>python huami_token.py --method amazfit --email youemail@example.com --password your_password --bt_keys</code></li> </ul> </li> <li>Do not unpair the band/watch from MiFit/Amazfit/Zepp app</li> <li>Kill or uninstall the MiFit/Amazfit/Zepp app</li> <li>Ensure GPS/location services are enabled</li> <li>The official instructions tell you to unpair the band/watch from your phone's     bluetooth but I didn't have to do it.</li> <li>Add the band in gadgetbridge.</li> <li>Under Auth key add your key.</li> </ul>"}, {"location": "gadgetbridge/#references", "title": "References", "text": "<ul> <li>Git</li> <li>Home</li> <li>Issue tracker</li> <li>Blog, although the     RSS is not     working.</li> </ul>"}, {"location": "gajim/", "title": "Gajim", "text": "<p>Gajim is the best Linux XMPP client in terms of end-to-end encryption support as it's able to speak OMEMO.</p>"}, {"location": "gajim/#installation", "title": "Installation", "text": "<pre><code>sudo apt-get install gajim gajim-omemo\n</code></pre> <p>Once you open it, you need to enable the plugin in the main program dropdown.</p> <p>The only problem I've encountered so far is that OMEMO is not enabled by default, they made a PR but closed it because it encountered some errors that was not able to solve. It's a crucial feature, so if you have some spare time and know a bit of Python please try to fix it!</p>"}, {"location": "gajim/#developing", "title": "Developing", "text": "<p>I've found the Developing section in the wiki to get started.</p>"}, {"location": "gajim/#issues", "title": "Issues", "text": "<ul> <li>Enable encryption by     default: Nothing to     do.</li> </ul>"}, {"location": "gajim/#references", "title": "References", "text": "<ul> <li>Homepage</li> </ul>"}, {"location": "gancio/", "title": "Gancio", "text": "<p>Gancio is a shared agenda for local communities.</p>"}, {"location": "gancio/#plugins", "title": "Plugins", "text": ""}, {"location": "gancio/#telegram-bridge", "title": "Telegram bridge", "text": "<p>Telegram bridge to republish Gancio events</p>"}, {"location": "gancio/#references", "title": "References", "text": "<ul> <li>Docs</li> <li>Source</li> <li>Home</li> </ul>"}, {"location": "gardening/", "title": "Gardening", "text": ""}, {"location": "gardening/#fertilizing-with-manure", "title": "Fertilizing with manure", "text": "<p>Manure is one of the best organic fertilizers for plants. It's made by the accumulation of excrements of bats, sea birds and seals and it usually doesn't contain additives or synthetic chemical components. </p> <p>This fertilizer is rich in nitrogen, phosphorus and potassium, which are key minerals for the growth of plants. These components help the regeneration of the soil, the enrichment in terms of nutrients and also acts as fungicide preventing plagues. </p> <p>Manure is a fertilizer of slow absorption, which means that it's released to the plants in an efficient, controlled and slow pace. That way the plants take the nutrients when they need them.</p>"}, {"location": "gardening/#when-to-fertilize-with-manure", "title": "When to fertilize with manure", "text": "<p>The best moment to use it is at spring and depending on the type of plant you should apply it between each month and a half and three months. It's use in winter is not recommended, as it may burn the plant's roots.</p>"}, {"location": "gardening/#how-to-fertilize-with-manure", "title": "How to fertilize with manure", "text": "<p>Manure can be obtained in dust or liquid state. The first is perfect to scatter directly over the earth, while the second is better used on plant pots. You don't need to use much, in fact, with just a pair of spoons per pot is enough. Apply it around the base of the plant, avoiding it's touch with leaves, stem or exposed roots, as it may burn them. After you apply them remember to water them often, keep in mind that it's like a heavy greasy sandwich for the plants, and they need water to digest it.</p> <p>For my indoor plants I'm going to apply a small dose (one spoon per plant) at the start of Autumn (first days of September), and two spoons at the start of spring (first days of March). </p>"}, {"location": "gettext/", "title": "Gettext", "text": "<p>Gettext is the defacto universal solution for internationalization (I18N) and localization (L10N), offering a set of tools that provides a framework to help other packages produce multi-lingual messages. It gives an opinionated way of how programs should be written to support translated message strings and a directory and file naming organisation for the messages that need to be translated.</p> <p>In regards to directory conventions, we need to have a place to put our localised translations based on the specified locale language. For example, let\u2019s say we need to support 2 languages English and Greek. Their language codes are <code>en</code> and <code>el</code> respectively.</p> <p>We can create a directory named <code>locales</code> and inside we need to create directories for each language code and each folder will contain another directory named each <code>LC_MESSAGES</code>  with one or multiple <code>.po</code> files.</p> <p>So, the file structure should look like this:</p> <pre><code>locales/\n\u251c\u2500\u2500 el\n\u2502   \u2514\u2500\u2500 LC_MESSAGES\n\u2502       \u2514\u2500\u2500 base.po\n\u2514\u2500\u2500 en\n    \u2514\u2500\u2500 LC_MESSAGES\n        \u2514\u2500\u2500 base.po\n</code></pre> <p>A PO file contains a number of messages, partly independent text segments to be translated, which have been grouped into one file according to some logical division of what is being translated. Those groups are called domains. In the example above, we have only one domain named as <code>base</code>. The PO files themselves are also called message catalogs. The PO format is a plain text format.</p> <p>Apart from PO files, you might sometimes encounter <code>.mo</code> files. MO, or Machine Object is a binary data file that contains object data referenced by a program. It is typically used to translate program code, and can be loaded or imported into the GNU gettext program.</p> <p>In addition, there are also <code>.pot</code> files. These are the template files for PO files. They will have all the translation strings left empty. A POT file is essentially an empty PO file without the translations, with just the original strings. In practice we have the <code>.pot</code> files be generated from some tools and we should not modify them directly.</p>"}, {"location": "gettext/#usage", "title": "Usage", "text": "<p>The <code>gettext</code> module comes shipped with Python. It exposes two APIs. The first one is the basic API that supports the GNU gettext catalog API. The second one is the higher level one, class-based API that may be more appropriate for Python files. The class bases API offers more flexibility and greater convenience than the GNU gettext API and it is the recommended way of localizing your Python applications and modules.</p> <p>In order to provide multilingual messages for your Python programs, you need to take the next steps:</p> <ul> <li>Mark all translatable strings in your program with a wrapper function.</li> <li>Run a suite of tools over your marked files to generate raw messages catalogs     or POT files.</li> <li>Duplicate the POT files into specific locale folders and write the     translations.</li> <li>Import and use the gettext module so that message strings are properly     translated.</li> </ul> <p>Let\u2019s start with a function that prints some strings.</p> <pre><code># main.py\ndef print_some_strings():\n    print(\"Hello world\")\n    print(\"This is a translatable string\")\n\nif __name__ == '__main__':\n    print_some_strings()\n</code></pre> <p>Now as it is you cannot provide localization options using <code>gettext</code>.</p> <p>The first step is to specially mark all translatable strings in the program. To do that we need to wrap all the translatable strings inside <code>_()</code>.</p> <pre><code># main.py\nimport gettext\n_ = gettext.gettext\ndef print_some_strings():\n    print(_(\"Hello world\"))\n    print(_(\"This is a translatable string\"))\nif __name__=='__main__':\n    print_some_strings()\n</code></pre> <p>Notice that we imported <code>gettext</code> and assigned <code>_</code>  as <code>gettext.gettext</code>. This is to ensure that our program compiles as well.</p> <p>If you run the program, you will see that nothing has changed:</p> <pre><code>$: python main.py\nHello world\nThis is a translatable string\n</code></pre> <p>However, now we are able to proceed to the next steps which are extracting the translatable messages in a POT file.</p>"}, {"location": "gettext/#create-the-pot-files", "title": "Create the POT files", "text": "<p>For the purpose of automating the process of generating raw translatable messages from wrapped strings throughout the applications, the <code>gettext</code> library authors have provided a set to tools that help to parse the source files and to extract the messages in a general message catalog.</p> <p>The Python distribution includes some specific programs called <code>pygettext.py</code> and <code>msgfmt.py</code> that recognize only python source code and not other languages.</p> <p>Call it specifying the file you want to parse the strings for:</p> <pre><code>$: pygettext -d base -o locales/base.pot src/main.py\n</code></pre> <p>If you want to search for other strings than <code>_</code>, use the <code>-k</code> flag, for example <code>-k gettext</code>.</p> <p>That will generate a <code>base.pot</code> file in the <code>locales</code> directory taken from our <code>main.py</code> program. Remember that POT files are just templates and we should not touch them. Let us inspect the contents of the <code>base.pot</code> file:</p> <pre><code># SOME DESCRIPTIVE TITLE.\n# Copyright (C) YEAR ORGANIZATION\n# FIRST AUTHOR &lt;EMAIL@ADDRESS&gt;, YEAR.\n#\nmsgid \"\"\nmsgstr \"\"\n\"Project-Id-Version: PACKAGE VERSION\\n\"\n\"POT-Creation-Date: 2018-01-28 16:47+0000\\n\"\n\"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\\n\"\n\"Last-Translator: FULL NAME &lt;EMAIL@ADDRESS&gt;\\n\"\n\"Language-Team: LANGUAGE &lt;LL@li.org&gt;\\n\"\n\"MIME-Version: 1.0\\n\"\n\"Content-Type: text/plain; charset=UTF-8\\n\"\n\"Content-Transfer-Encoding: 8bit\\n\"\n\"Generated-By: pygettext.py 1.5\\n\"\n#: src/main.py:5\nmsgid \"Hello world\"\nmsgstr \"\"\n#: src/main.py:6\nmsgid \"This is a translatable string\"\nmsgstr \"\"\n</code></pre> <p>In a bigger program, we would have many translatable strings following.  Here we specified a domain called base because the application is only one file. In bigger ones, I would use multiple domains in order to logically separate the different messages based on the application scope.</p> <p>Notice that we have a simple convention for our translatable strings. <code>msgid</code> is the original string wrapped in <code>_()</code> . <code>msgstr</code> is the translation we need to provide.</p>"}, {"location": "gettext/#create-the-po-files", "title": "Create the PO files", "text": "<p>Now we are ready to create our translations. Because we have the template generated for us, the next step is to create the required directory structure and copy the template into the right spot. We\u2019ve seen the recommended file structure before. We are going to create 2 additional directories inside <code>locales</code> with the structure <code>locales/$language/LC_MESSAGES/$domain.po</code></p> <p>Where:</p> <ul> <li><code>$language</code> is the language identifier such as <code>en</code> or <code>el</code></li> <li><code>$domain</code> is <code>base</code>.</li> </ul> <p>Copy and rename the <code>base.pot</code> into the following directories <code>locales/en/LC_MESSAGES/base.po</code> and <code>locales/el/LC_MESSAGES/base.po</code>. Then modify their headers to include more information about the locale. For example, this is the Greek translation.</p> <pre><code># My App.\n# Copyright (C) 2018\n#\nmsgid \"\"\nmsgstr \"\"\n\"Project-Id-Version: 1.0\\n\"\n\"POT-Creation-Date: 2018-01-28 16:47+0000\\n\"\n\"PO-Revision-Date: 2018-01-28 16:48+0000\\n\"\n\"Last-Translator: me &lt;johndoe@example.com&gt;\\n\"\n\"Language-Team: Greek &lt;yourteam@example.com&gt;\\n\"\n\"MIME-Version: 1.0\\n\"\n\"Content-Type: text/plain; charset=UTF-8\\n\"\n\"Content-Transfer-Encoding: 8bit\\n\"\n\"Generated-By: pygettext.py 1.5\\n\"\n#: main.py:5\nmsgid \"Hello world\"\nmsgstr \"\u03a7\u03ad\u03c1\u03b5 \u039a\u03cc\u03c3\u03bc\u03b5\"\n#: main.py:6\nmsgid \"This is a translatable string\"\nmsgstr \"\u0391\u03c5\u03c4\u03cc \u03b5\u03af\u03bd\u03b1\u03b9 \u03ad\u03bd\u03b1 \u03bc\u03b5\u03c4\u03b1\u03c6\u03c1\u03b1\u03b6\u03cc\u03bc\u03b5\u03bd\u03bf \u03ba\u03b5\u03af\u03bc\u03b5\u03bd\u03bf\"\n</code></pre>"}, {"location": "gettext/#updating-pot-and-po-files", "title": "Updating POT and PO files", "text": "<p>Once you add more strings or change some strings in your program, you execute again <code>pygettext</code> which regenerates the template file:</p> <pre><code>pygettext main.py -o po/hello.pot\n</code></pre> <p>Then you can update individual translation files to match newly created templates (this includes reordering the strings to match new template) with <code>msgmerge</code>:</p> <pre><code>msgmerge --previous --update po/cs.po po/hello.pot\n</code></pre>"}, {"location": "gettext/#create-the-mo-files", "title": "Create the MO files", "text": "<p>The catalog is built from the <code>.po</code> file using a tool called <code>msgformat.py</code>. This tool will parse the <code>.po</code> file and generate an equivalent <code>.mo</code> file.</p> <pre><code>$: msgfmt -o base.mo base\n</code></pre> <p>This command will generate a <code>base.mo</code> file in the same folder as the <code>base.po</code> file.</p> <p>So, the final file structure should look like this:</p> <pre><code>locales\n\u251c\u2500\u2500 el\n\u2502   \u2514\u2500\u2500 LC_MESSAGES\n\u2502       \u251c\u2500\u2500 base.mo\n\u2502       \u2514\u2500\u2500 base.po\n\u251c\u2500\u2500 en\n\u2502   \u2514\u2500\u2500 LC_MESSAGES\n\u2502       \u251c\u2500\u2500 base.mo\n\u2502       \u2514\u2500\u2500 base.po\n\u2514\u2500\u2500 base.pot\n</code></pre>"}, {"location": "gettext/#switching-locale", "title": "Switching Locale", "text": "<p>To have the ability to switch locales in our program we need to actually use the Class based <code>gettext</code> API. One of it's methods is <code>gettext.translation</code>, it accepts some parameters that can be used to load the associated <code>.mo</code> files of a particular language. If no <code>.mo</code> file is found, it raises an error.</p> <p>Add the following code to the program:</p> <pre><code>import gettext\nel = gettext.translation('base', localedir='locales', languages=['el'])\nel.install()\n_ = el.gettext # Greek\n</code></pre> <p>The first argument base is the domain and the method will look for a <code>.po</code>  file with the same name in our locale directory. If you don\u2019t specify a domain it will fallback to the messages domain. The <code>localedir</code> parameter is the directory location of the <code>locales</code> directory you created. The <code>languages</code> parameter is a hint for the searching mechanism to load particular language code more resiliently.</p> <p>If you run the program again you will see the translations happening:</p> <pre><code>$ python main.py\n\u03a7\u03b1\u03af\u03c1\u03b5 \u039a\u03cc\u03c3\u03bc\u03b5\n\u0391\u03c5\u03c4\u03cc \u03b5\u03af\u03bd\u03b1\u03b9 \u03ad\u03bd\u03b1 \u03bc\u03b5\u03c4\u03b1\u03c6\u03c1\u03b1\u03b6\u03cc\u03bc\u03b5\u03bd\u03bf \u03ba\u03b5\u03af\u03bc\u03b5\u03bd\u03bf\n</code></pre> <p>The install method will cause all the <code>_()</code> calls to return the Greek translated strings globally into the built-in namespace. This is because we assigned <code>_</code> to point to the Greek dictionary of translations. To go back to the English just assign <code>_</code> to be the original <code>gettext</code> object.</p> <pre><code>_ = gettext.gettext\n</code></pre>"}, {"location": "gettext/#finding-message-catalogs", "title": "Finding Message Catalogs", "text": "<p>When there are cases where you need to locate all translation files at runtime, you can use the <code>find</code> function as provided by the class-based API. This function takes a few parameters in order to retrieve from the disk a list of <code>.mo</code> files available.</p> <p>You can pass a <code>localedir</code>, a <code>domain</code> and a list of <code>languages</code>. If you don\u2019t, the library module will use the respective defaults, which is not what you intended to do in most cases. For example, if you don\u2019t specify a <code>localdir</code> parameter, it will fallback to <code>sys.prefix + \u2018/share/locale\u2019</code>  which is a global locale dir that can contain a lot of random files.</p> <p>The <code>language</code> portion of the path is taken from one of several environment variables that can be used to configure localization features (LANGUAGE, LC_ALL, LC_MESSAGES, and LANG). The first variable found to be set is used. Multiple languages can be selected by separating the values with a colon :.</p> <pre><code>&gt;&gt;&gt; os.environ['LANGUAGE']='el:en'\n&gt;&gt;&gt; gettext.find('base', 'locales')\n'locales/el/LC_MESSAGES/base.mo'\n&gt;&gt;&gt; gettext.find('base', 'locales', all=True)\n ['locales/el/LC_MESSAGES/base.mo', 'locales/en/LC_MESSAGES/base.mo']\n</code></pre>"}, {"location": "gettext/#using-f-strings", "title": "Using f-strings", "text": "<p>You can't use f-strings inside <code>gettext</code>, you'll get an <code>Seen unexpected token \"f\"</code> error, you need to use the old <code>format</code> method:</p> <pre><code>_('Hey {},').format(username)\n</code></pre>"}, {"location": "gettext/#integrations", "title": "Integrations", "text": "<p>You can use it with weblate.</p>"}, {"location": "gettext/#references", "title": "References", "text": "<ul> <li>Homepage</li> <li>Reference</li> <li>Phrase blog on Localizing with GNU gettext</li> </ul>"}, {"location": "gh/", "title": "Github cli client", "text": "<p><code>gh</code> is GitHub\u2019s official command line tool.</p>"}, {"location": "gh/#installation", "title": "Installation", "text": "<p>Get the <code>deb</code> file from the releases page and install it with <code>sudo dpkg -i</code></p> <p>Authenticate the command following the steps of:</p> <pre><code>gh auth login\n</code></pre>"}, {"location": "gh/#usage", "title": "Usage", "text": ""}, {"location": "gh/#create-a-pull-request", "title": "Create a pull request", "text": "<p>Whenever you do a git push it will show you the link to create the pull request. To avoid going into the browser, you can use <code>gh pr create</code>.</p> <p>You can also merge a ready pull request with <code>gh pr merge</code></p>"}, {"location": "gh/#workflow-runs", "title": "Workflow runs", "text": "<p>With <code>gh run list</code> you can get the list of the last workflow runs. If you want to see the logs of one of the runs, get the id and run <code>gh run view {{ run_id }}</code>.</p> <p>To see what failed, run <code>gh run view {{ run_id }} --log-failed</code>.</p>"}, {"location": "gh/#pull-request-checks", "title": "Pull request checks", "text": "<p><code>gh</code> allows you to check the status of the checks of a pull requests, this is useful to get an alert once the checks are done. For example you can use the next bash/zsh function:</p> <pre><code>function checks(){\nwhile true; do\ngh pr checks\n        if [[ -z \"$(gh pr status --json statusCheckRollup | grep IN_PROGRESS)\" ]]; then\nbreak\nfi\nsleep 1\necho\ndone\ngh pr checks\n    echo -e '\\a'\n}\n</code></pre>"}, {"location": "gh/#trigger-a-workflow-run", "title": "Trigger a workflow run", "text": "<p>To manually trigger a workflow you need to first configure it to allow <code>workflow_dispatch</code> events.</p> <pre><code>on:\nworkflow_dispatch:\n</code></pre> <p>Then you can trigger the workflow with <code>gh workflow run {{ workflow_name }}</code>, where you can get the <code>workflow_name</code> with <code>gh workflow list</code></p>"}, {"location": "gh/#references", "title": "References", "text": "<ul> <li>Git</li> </ul>"}, {"location": "git/", "title": "Git", "text": "<p>Git is a software for tracking changes in any set of files, usually used for coordinating work among programmers collaboratively developing source code during software development. Its goals include speed, data integrity, and support for distributed, non-linear workflows (thousands of parallel branches running on different systems).</p>"}, {"location": "git/#learning-git", "title": "Learning git", "text": "<p>Git is a tough nut to crack, no matter how experience you are you'll frequently get surprised. Sadly it's one of the main tools to develop your code, so you must master it as soon as possible.</p> <p>Depending on how you like to learn I've found these options:</p> <ul> <li>Written courses: W3 git course</li> <li>Interactive tutorials:   Learngitbranching interactive tutorial</li> <li>Written article:   Freecode camp article</li> <li>Video courses: Code academy and   Udemy</li> </ul>"}, {"location": "git/#pull-request-process", "title": "Pull Request Process", "text": "<p>This part of the doc is shamefully edited from the source. It was for the k8s project but they are good practices that work for all the projects. It explains the process and best practices for submitting a PR It should serve as a reference for all contributors, and be useful especially to new and infrequent submitters.</p>"}, {"location": "git/#before-you-submit-a-pr", "title": "Before You Submit a PR", "text": "<p>This guide is for contributors who already have a PR to submit. If you're looking for information on setting up your developer environment and creating code to contribute to the project, search the development guide.</p> <p>Make sure your PR adheres to the projects best practices. These include following project conventions, making small PRs, and commenting thoroughly.</p>"}, {"location": "git/#run-local-verifications", "title": "Run Local Verifications", "text": "<p>You can run the tests in local before you submit your PR to predict the pass or fail of continuous integration.</p>"}, {"location": "git/#why-is-my-pr-not-getting-reviewed", "title": "Why is my PR not getting reviewed?", "text": "<p>A few factors affect how long your PR might wait for review.</p> <p>If it's the last few weeks of a milestone, we need to reduce churn and stabilize.</p> <p>Or, it could be related to best practices. One common issue is that the PR is too big to review. Let's say you've touched 39 files and have 8657 insertions. When your would-be reviewers pull up the diffs, they run away - this PR is going to take 4 hours to review and they don't have 4 hours right now. They'll get to it later, just as soon as they have more free time (ha!).</p> <p>There is a detailed rundown of best practices, including how to avoid too-lengthy PRs, in the next section.</p> <p>But, if you've already followed the best practices and you still aren't getting any PR love, here are some things you can do to move the process along:</p> <ul> <li> <p>Make sure that your PR has an assigned reviewer (assignee in GitHub). If not,   reply to the PR comment stream asking for a reviewer to be assigned.</p> </li> <li> <p>Ping the assignee (@username) on the PR comment stream, and ask for an   estimate of when they can get to the review.</p> </li> <li> <p>Ping the assignee by email (many of us have publicly available email   addresses).</p> </li> <li> <p>If you're a member of the organization ping the team (via @team-name) that   works in the area you're submitting code.</p> </li> <li> <p>If you have fixed all the issues from a review, and you haven't heard back,   you should ping the assignee on the comment stream with a \"please take another   look\" (<code>PTAL</code>) or similar comment indicating that you are ready for another   review.</p> </li> </ul> <p>Read on to learn more about how to get faster reviews by following best practices.</p>"}, {"location": "git/#best-practices-for-faster-reviews", "title": "Best Practices for Faster Reviews", "text": "<p>You've just had a brilliant idea on how to make a project better. Let's call that idea Feature-X. Feature-X is not even that complicated. You have a pretty good idea of how to implement it. You jump in and implement it, fixing a bunch of stuff along the way. You send your PR - this is awesome! And it sits. And sits. A week goes by and nobody reviews it. Finally, someone offers a few comments, which you fix up and wait for more review. And you wait. Another week or two go by. This is horrible.</p> <p>Let's talk about best practices so your PR gets reviewed quickly.</p>"}, {"location": "git/#familiarize-yourself-with-project-conventions", "title": "Familiarize yourself with project conventions", "text": "<ul> <li>Search for the Development guide</li> <li>Search for the Coding conventions</li> <li>Search for the API conventions</li> </ul>"}, {"location": "git/#is-the-feature-wanted-make-a-design-doc-or-sketch-pr", "title": "Is the feature wanted? Make a Design Doc or Sketch PR", "text": "<p>Are you sure Feature-X is something the project team wants or will accept? Is it implemented to fit with other changes in flight? Are you willing to bet a few days or weeks of work on it?</p> <p>It's better to get confirmation beforehand. There are two ways to do this:</p> <ul> <li>Make a proposal doc (in docs/proposals; for example   the QoS proposal), or reach out to the affected   special interest group (SIG). Some projects have that</li> <li>Coordinate your effort with SIG Docs ahead of time</li> <li>Make a sketch PR (e.g., just the API or Go interface). Write or code up just   enough to express the idea and the design and why you made those choices</li> </ul> <p>Or, do all of the above.</p> <p>Be clear about what type of feedback you are asking for when you submit a proposal doc or sketch PR.</p> <p>Now, if we ask you to change the design, you won't have to re-write it all.</p>"}, {"location": "git/#smaller-is-better-small-commits-small-prs", "title": "Smaller Is Better: Small Commits, Small PRs", "text": "<p>Small commits and small PRs get reviewed faster and are more likely to be correct than big ones.</p> <p>Attention is a scarce resource. If your PR takes 60 minutes to review, the reviewer's eye for detail is not as keen in the last 30 minutes as it was in the first. It might not get reviewed at all if it requires a large continuous block of time from the reviewer.</p> <p>Breaking up commits</p> <p>Break up your PR into multiple commits, at logical break points.</p> <p>Making a series of discrete commits is a powerful way to express the evolution of an idea or the different ideas that make up a single feature. Strive to group logically distinct ideas into separate commits.</p> <p>For example, if you found that Feature-X needed some prefactoring to fit in, make a commit that JUST does that prefactoring. Then make a new commit for Feature-X.</p> <p>Strike a balance with the number of commits. A PR with 25 commits is still very cumbersome to review, so use judgment.</p> <p>Breaking up PRs</p> <p>Or, going back to our prefactoring example, you could also fork a new branch, do the prefactoring there and send a PR for that. If you can extract whole ideas from your PR and send those as PRs of their own, you can avoid the painful problem of continually rebasing.</p> <p>Multiple small PRs are often better than multiple commits. Don't worry about flooding us with PRs. We'd rather have 100 small, obvious PRs than 10 unreviewable monoliths.</p> <p>We want every PR to be useful on its own, so use your best judgment on what should be a PR vs. a commit.</p> <p>As a rule of thumb, if your PR is directly related to Feature-X and nothing else, it should probably be part of the Feature-X PR. If you can explain why you are doing seemingly no-op work (\"it makes the Feature-X change easier, I promise\") we'll probably be OK with it. If you can imagine someone finding value independently of Feature-X, try it as a PR. (Do not link pull requests by <code>#</code> in a commit description, because GitHub creates lots of spam. Instead, reference other PRs via the PR your commit is in.)</p>"}, {"location": "git/#open-a-different-pr-for-fixes-and-generic-features", "title": "Open a Different PR for Fixes and Generic Features", "text": "<p>Put changes that are unrelated to your feature into a different PR.</p> <p>Often, as you are implementing Feature-X, you will find bad comments, poorly named functions, bad structure, weak type-safety, etc.</p> <p>You absolutely should fix those things (or at least file issues, please) - but not in the same PR as your feature. Otherwise, your diff will have way too many changes, and your reviewer won't see the forest for the trees.</p> <p>Look for opportunities to pull out generic features.</p> <p>For example, if you find yourself touching a lot of modules, think about the dependencies you are introducing between packages. Can some of what you're doing be made more generic and moved up and out of the Feature-X package? Do you need to use a function or type from an otherwise unrelated package? If so, promote! We have places for hosting more generic code.</p> <p>Likewise, if Feature-X is similar in form to Feature-W which was checked in last month, and you're duplicating some tricky stuff from Feature-W, consider prefactoring the core logic out and using it in both Feature-W and Feature-X. (Do that in its own commit or PR, please.)</p>"}, {"location": "git/#comments-matter", "title": "Comments Matter", "text": "<p>In your code, if someone might not understand why you did something (or you won't remember why later), comment it. Many code-review comments are about this exact issue.</p> <p>If you think there's something pretty obvious that we could follow up on, add a TODO.</p>"}, {"location": "git/#test", "title": "Test", "text": "<p>Nothing is more frustrating than starting a review, only to find that the tests are inadequate or absent. Very few PRs can touch code and NOT touch tests.</p> <p>If you don't know how to test Feature-X, please ask! We'll be happy to help you design things for easy testing or to suggest appropriate test cases.</p>"}, {"location": "git/#squashing-and-commit-titles", "title": "Squashing and Commit Titles", "text": "<p>Your reviewer has finally sent you feedback on Feature-X.</p> <p>Make the fixups, and don't squash yet. Put them in a new commit, and re-push. That way your reviewer can look at the new commit on its own, which is much faster than starting over.</p> <p>We might still ask you to clean up your commits at the very end for the sake of a more readable history, but don't do this until asked: typically at the point where the PR would otherwise be tagged <code>LGTM</code>.</p> <p>Each commit should have a good title line (<code>&lt;70</code> characters) and include an additional description paragraph describing in more detail the change intended.</p> <p>General squashing guidelines:</p> <ul> <li>Sausage =&gt; squash</li> </ul> <p>Do squash when there are several commits to fix bugs in the original commit(s), address reviewer feedback, etc. Really we only want to see the end state and commit message for the whole PR.</p> <ul> <li>Layers =&gt; don't squash</li> </ul> <p>Don't squash when there are independent changes layered to achieve a single goal. For instance, writing a code munger could be one commit, applying it could be another, and adding a precommit check could be a third. One could argue they should be separate PRs, but there's really no way to test/review the munger without seeing it applied, and there needs to be a precommit check to ensure the munged output doesn't immediately get out of date.</p> <p>A commit, as much as possible, should be a single logical change.</p>"}, {"location": "git/#kiss-yagni-mvp-etc", "title": "KISS, YAGNI, MVP, etc.", "text": "<p>Sometimes we need to remind each other of core tenets of software design - Keep It Simple, You Aren't Gonna Need It, Minimum Viable Product, and so on. Adding a feature \"because we might need it later\" is antithetical to software that ships. Add the things you need NOW and (ideally) leave room for things you might need later - but don't implement them now.</p>"}, {"location": "git/#its-ok-to-push-back", "title": "It's OK to Push Back", "text": "<p>Sometimes reviewers make mistakes. It's OK to push back on changes your reviewer requested. If you have a good reason for doing something a certain way, you are absolutely allowed to debate the merits of a requested change. Both the reviewer and reviewee should strive to discuss these issues in a polite and respectful manner.</p> <p>You might be overruled, but you might also prevail. We're pretty reasonable people. Mostly.</p> <p>Another phenomenon of open-source projects (where anyone can comment on any issue) is the dog-pile - your PR gets so many comments from so many people it becomes hard to follow. In this situation, you can ask the primary reviewer (assignee) whether they want you to fork a new PR to clear out all the comments. You don't HAVE to fix every issue raised by every person who feels like commenting, but you should answer reasonable comments with an explanation.</p>"}, {"location": "git/#common-sense-and-courtesy", "title": "Common Sense and Courtesy", "text": "<p>No document can take the place of common sense and good taste. Use your best judgment, while you put a bit of thought into how your work can be made easier to review. If you do these things your PRs will get merged with less friction.</p>"}, {"location": "git/#split-long-pr-into-smaller-ones", "title": "Split long PR into smaller ones", "text": "<ul> <li> <p>Start a new branch from where you want to merge.</p> </li> <li> <p>Start an interactive rebase on HEAD:</p> </li> </ul> <pre><code>git rebase -i HEAD\n</code></pre> <ul> <li>Get the commits you want: Now comes the clever part, we are going to pick out   all the commits we care about from 112-new-feature-branch using the following   command:</li> </ul> <pre><code>git log --oneline --reverse HEAD..112-new-feature-branch -- app/models/ spec/models\n</code></pre> <p>Woah thats quite the line! Let\u2019s dissect it first:</p> <ul> <li><code>git log</code> shows a log of what you have done in your project.</li> <li><code>--online</code> formats the output from a few lines (including author and time of     commit), to just \u201c[sha-hash-of-commit] [description-of-commit]\u201d</li> <li><code>--reverse</code> reverses the log output chronologically (so oldest commit first,     newest last).</li> <li><code>112-new-feature-branch..HEAD</code> shows the difference in commits from your     current branch (HEAD) and the branch you are interested in     112-new-feature-branch.</li> <li><code>-- app/models/ spec/models</code> Only show commits that changed files in     app/models/ or spec/models So that we confine the changes to our model and     its tests.</li> </ul> <p>Now if you are using vim (or vi or neovim) you can put the results of this   command directly into your rebase-todo (which was opened when starting the   rebase) using the :r command like so:</p> <pre><code>:r !git log --oneline --reverse HEAD..112-new-feature-branch -- app/models/\n</code></pre> <ul> <li>Review the commits you want: Now you have a chance to go though your todo once   again. First you should remove the noop from above, since you actually do   something now. Second you should check the diffs of the sha-hashes.</li> </ul> <p>Note: If you are using vim, you might already have the fugitive plug-in. If   you haven\u2019t changed the standard configuration, you can just move your cursor   over the sha-hashes and press K (note that its capitalized) to see the diff of   that commit.</p> <p>If you don\u2019t have fugitive or don\u2019t use vim, you can check the diff using git   show SHA-HASH (for example <code>git show c4f74d0</code>), which shows the commits data.</p> <p>Now you can prepend and even rearrange the commits (Be careful rearranging or   leaving out commits, you might have to fix conflicts later).</p> <ul> <li>Execute the rebase: Now you can save and exit the editor and git will try to   execute the rebase. If you have conflicts you can fix them just like you do   with merges and then continue using git rebase --continue.</li> </ul> <p>If you feel like something is going terribly wrong (for example you have a   bunch of conflicts in just a few commits), you can abort the rebase using git   rebase --abort and it will be like nothing ever happened.</p>"}, {"location": "git/#git-workflow", "title": "Git workflow", "text": "<p>There are many ways of using git, one of the most popular is git flow, please read this article to understand it before going on.</p> <p>Unless you are part of a big team that delivers software that needs to maintain many versions, it's not worth using git flow as it's too complex and cumbersome. Instead I suggest a variation of the Github workflow.</p> <p>To carry out a reliable continuous delivery we must work to comply with the following list of best practices:</p> <ul> <li>Everything must be in the git server: source code, tests, pipelines, scripts,   templates and documentation.</li> <li>There is only a main branch (main) whose key is that everything is in this   branch must be always stable and deployable into production at any time.</li> <li>New branches are created from main in order to develop new features that   should be merged into main branch in short development cycles.</li> <li>It is highly recommended to do small commits to have more control over what is   being done and to avoid discarding many lines of code if a rollback has to be   done.</li> <li>A commit message policy should be set so that they are clear and conform the   same pattern, for example semantic versioning.</li> <li><code>main</code> is blocked to reject direct pushes as well as to protect it of   catastrophic deletion. Only pre-validated merge requests are accepted.</li> <li>When a feature is ready, we will open a merge request to merge changes into   <code>main</code> branch.</li> <li>Use webhooks to automate the execution of tests and validation tasks in the CI   server before/after adding changes in main.</li> <li>It is not needed to discard a merge request if any of the validation tasks   failed. We check the code and when the changes are pushed, the CI server will   relaunch the validation tasks.</li> <li>If all validation tasks pass, we will assign the merge request to two team   developers to review the feature code.</li> <li>After both reviewers validate the code, the merge request can be accepted and   the feature branch may be deleted.</li> <li>A clear versioning policy must be adopted for all generated artifacts.</li> <li>Each artifact must be generated once and be promoted to the different   environments in different stages.</li> </ul> <p>When a developer wants to add code to main should proceed as follows:</p> <ul> <li>Wait until the pipeline execution ends if it exists. If that process fails,   then the developer must help to other team members to fix the issue before   requesting a new merge request.</li> <li>Pull the changes from main and resolve the conflicts locally before pushing   the code to the new feature branch.</li> <li>Run a local script that compiles and executes the tests before committing   changes. This task can be done executing it manually by the developer or using   a git precommit.</li> <li>Open a new merge request setting the feature branch as source branch and main   as target branch.</li> <li>The CI server is notified of the new merge request and executes the pipeline   which compiles the source code, executes the tests, deploys the artifact, etc.</li> <li>If there are errors in the previous step, the developer must fix the code and   push it to the git server as soon as possible so that the CI server validate   once again the merge request.</li> <li>If no errors, the CI server will mark the merge request as OK and the   developer can assign it to two other team members to review the feature code.</li> <li>At this point, the developer can start with other task.</li> </ul> <p>Considerations</p> <p>The build process and the execution of the tests have to be pretty fast. It should not exceed about 10 minutes.</p> <p>Unit tests must be guarantee that they are completely unitary; they must be executed without starting the context of the application, they must not access to the DDBB, external systems, file system, etc.</p>"}, {"location": "git/#naming-conventions", "title": "Naming conventions", "text": "<p>The best idea is to use Semantic Versioning to define the names of the branches, for example: <code>feat/add-command-line-support</code> or <code>fix/correct-security-issue</code>, and also for the commit messages.</p>"}, {"location": "git/#tag-versioning-policy", "title": "Tag versioning policy", "text": "<p>We will also adopt semantic versioning policy on version management.</p>"}, {"location": "git/#versioning-control", "title": "Versioning control", "text": "<p>When a branch is merged into main, the CI server launches a job which generates a new artifact release as follow:</p> <ul> <li>The new version number is calculated taken into account the above   considerations.</li> <li>Generates a new artifact named as appname-major.minor.patch.build</li> <li>Upload the previous artifact to the artifact repository manager.</li> <li>Create a git tag on the repository with the same version identifier,   major.minor.patch.build</li> <li>Automatically deploy the artifact on the desired environment (dev, pre, etc)</li> </ul>"}, {"location": "git/#hotfixing", "title": "Hotfixing", "text": "<p>Hotfix should be developed and fixed using one of the next cases, which has been defined by preference order:</p>"}, {"location": "git/#case-1", "title": "Case 1", "text": "<p>In this case, we have pushed new code to \"main\" branch since the last deploy on production and we want to deploy the new code with the fix code.</p> <p>We have to follow the next steps:</p> <ul> <li>Create a branch \"Hotfix\" from commit/tag of the last deploy</li> <li>Fix the bug in \"hotfix\" branch</li> <li>Merge the new branch to \"main\"</li> <li>Deploy main branch</li> </ul>"}, {"location": "git/#case-2", "title": "Case 2", "text": "<p>In this case, we have pushed new code to \"main\" branch since the last deploy on production but we don't want to deploy the new code with the fix code.</p> <p>We have to follow the next steps:</p> <ul> <li>Create a branch \"Hotfix\" from commit/tag of the last deploy</li> <li>Fix the bug in \"hotfix\" branch</li> <li>Deploy main branch</li> <li>Merge the new branch to \"main.</li> </ul>"}, {"location": "git/#case-3", "title": "Case 3", "text": "<p>In this case, we have pushed new code to \"main\" branch since the last deploy on production but we don't want to deploy the new code with the fix code.</p> <p>We have to follow the next steps:</p> <ul> <li>Create a branch \"Hotfix\" from commit/tag of the last deploy</li> <li>Fix the bug in \"hotfix\" branch</li> <li>Deploy main branch</li> <li>Merge the new branch to \"main.</li> </ul>"}, {"location": "git/#git-housekeeping", "title": "Git housekeeping", "text": "<p>The best option is to:</p> <pre><code>git fetch --prune\ngit-sweep cleanup\n</code></pre> <p>To remove the local branches you can:</p> <pre><code>cd myrepo\ngit remote add local $(pwd)\ngit-sweep cleanup --origin=local\n</code></pre> <ul> <li>git-sweep: For local branches</li> <li>archaeologit: Tool to search   strings in the history of a github user</li> <li>jessfraz made a tool ghb0t: For github</li> </ul>"}, {"location": "git/#submodules", "title": "Submodules", "text": "<p>Shamefully edited from the docs</p> <p>It often happens that while working on one project, you need to use another project from within it. Perhaps it\u2019s a library that a third party developed or that you\u2019re developing separately and using in multiple parent projects. A common issue arises in these scenarios: you want to be able to treat the two projects as separate yet still be able to use one from within the other.</p> <p>Here\u2019s an example. Suppose you\u2019re developing a website and creating Atom feeds. Instead of writing your own Atom-generating code, you decide to use a library. You\u2019re likely to have to either include this code from a shared library like a CPAN install or Ruby gem, or copy the source code into your own project tree. The issue with including the library is that it\u2019s difficult to customize the library in any way and often more difficult to deploy it, because you need to make sure every client has that library available. The issue with copying the code into your own project is that any custom changes you make are difficult to merge when upstream changes become available.</p> <p>Git addresses this issue using submodules. Submodules allow you to keep a Git repository as a subdirectory of another Git repository. This lets you clone another repository into your project and keep your commits separate.</p> <p>It often happens that while working on one project, you need to use another project from within it. Perhaps it\u2019s a library that a third party developed or that you\u2019re developing separately and using in multiple parent projects. A common issue arises in these scenarios: you want to be able to treat the two projects as separate yet still be able to use one from within the other.</p> <p>Here\u2019s an example. Suppose you\u2019re developing a website and creating Atom feeds. Instead of writing your own Atom-generating code, you decide to use a library. You\u2019re likely to have to either include this code from a shared library like a CPAN install or Ruby gem, or copy the source code into your own project tree. The issue with including the library is that it\u2019s difficult to customize the library in any way and often more difficult to deploy it, because you need to make sure every client has that library available. The issue with copying the code into your own project is that any custom changes you make are difficult to merge when upstream changes become available.</p> <p>Git addresses this issue using submodules. Submodules allow you to keep a Git repository as a subdirectory of another Git repository. This lets you clone another repository into your project and keep your commits separate.</p>"}, {"location": "git/#submodule-tips", "title": "Submodule tips", "text": ""}, {"location": "git/#submodule-foreach", "title": "Submodule Foreach", "text": "<p>There is a foreach submodule command to run some arbitrary command in each submodule. This can be really helpful if you have a number of submodules in the same project.</p> <p>For example, let\u2019s say we want to start a new feature or do a bugfix and we have work going on in several submodules. We can easily stash all the work in all our submodules.</p> <pre><code>git submodule foreach 'git stash'\n</code></pre> <p>Then we can create a new branch and switch to it in all our submodules.</p> <pre><code>git submodule foreach 'git checkout -b featureA'\n</code></pre> <p>You get the idea. One really useful thing you can do is produce a nice unified diff of what is changed in your main project and all your subprojects as well.</p> <pre><code>git diff; git submodule foreach 'git diff'\n</code></pre>"}, {"location": "git/#useful-aliases", "title": "Useful Aliases", "text": "<p>You may want to set up some aliases for some of these commands as they can be quite long and you can\u2019t set configuration options for most of them to make them defaults. We covered setting up Git aliases in Git Aliases, but here is an example of what you may want to set up if you plan on working with submodules in Git a lot.</p> <pre><code>git config alias.sdiff '!'\"git diff &amp;&amp; git submodule foreach 'git diff'\"\ngit config alias.spush 'push --recurse-submodules=on-demand'\ngit config alias.supdate 'submodule update --remote --merge'\n</code></pre> <p>This way you can simply run git supdate when you want to update your submodules, or git spush to push with submodule dependency checking.</p>"}, {"location": "git/#encrypt-sensitive-information", "title": "Encrypt sensitive information", "text": "<p>Use git-crypt.</p>"}, {"location": "git/#use-different-git-configs", "title": "Use different git configs", "text": "<p>Include in your <code>~/.gitconfig</code></p> <pre><code>[includeIf \"gitdir:~/company_A/\"]\n  path = ~/.config/git/company_A.config\n</code></pre> <p>Every repository you create under that directory it will append the other configuration</p>"}, {"location": "git/#renaming-from-master-to-main", "title": "Renaming from master to main", "text": "<p>There's been a movement to migrate from <code>master</code> to <code>main</code>, the reason behind it is that the initial branch name, <code>master</code>, is offensive to some people and we empathize with those hurt by the use of that term.</p> <p>Existing versions of Git are capable of working with any branch name; there's nothing special about <code>master</code> except that it has historically been the name used for the first branch when creating a new repository from scratch (with the <code>git init</code> command). Thus many projects use it to represent the primary line of development. We support and encourage projects to switch to branch names that are meaningful and inclusive.</p> <p>To configure <code>git</code> to use <code>main</code> by default run:</p> <pre><code>git config --global init.defaultBranch main\n</code></pre> <p>It only works on since git version 2.28.0, so you're stuck with manually changing it if you have an earlier version.</p>"}, {"location": "git/#changes-controversy", "title": "Change's Controversy", "text": "<p>The change is not free of controversy, for example in the PDM project some people are not sure that it's needed for many reasons. Let's see each of them:</p> <ul> <li>The reason people are implementing the change is because other people are   doing it: After a quick search I found that the first one to do the change   was   the software freedom conservancy with the Git project.   You can also see Python,   Django,   Redis,   Drupal,   CouchDB and   Github's   statements.</li> </ul> <p>As we're not part of the deciding organisms of the collectives doing the   changes, all we can use are their statements and discussions to guess what are   the reasons behind their support of the change. Despite that some of them do   use the argument that other communities do support the change to emphasize the   need of the change, all of them mention that the main reason is that the term   is offensive to some people.</p> <ul> <li>I don't see an issue using the term master: If you relate to this statement   it can be because you're not part of the communities that suffer the   oppression tied to the term, and that makes you blind to the issue. It's a   lesson I learned on my own skin throughout the years. There are thousand of   situations, gestures, double meaning words and sentences that went unnoticed   by me until I started discussing it with the people that are suffering them   (women, racialized people, LGTBQI+, ...). Throughout my experience I've seen   that the more privileged you are, the blinder you become. You can read more on   privileged blindness   here,   here or   here   (I've skimmed through the articles, and are the first articles I've found,   there are probably better references).</li> </ul> <p>I'm not saying that privileged people are not aware of the issues or that they   can even raise them. We can do so and more we read, discuss and train   ourselves, the better we'll detect them. All I'm saying is that a non   privileged person will always detect more because they suffer them daily.</p> <p>I understand that for you there is no issue using the word master, there   wasn't an issue for me either until I saw these projects doing the change,   again I was blinded to the issue as I'm not suffering it. That's because   change is not meant for us, as we're not triggered by it. The change is   targeted to the people that do perceive that <code>master</code> is an offensive term.   What we can do is empathize with them and follow this tiny tiny tiny gesture.   It's the least we can do.</p> <p>Think of a term that triggers you, such as heil hitler, imagine that those   words were being used to define the main branch of your code, and that   everyday you sit in front of your computer you see them. You'll probably be   reminded of the historic events, concepts, feelings that are tied to that term   each time you see it, and being them quite negative it can slowly mine you.   Therefore it's legit that you wouldn't want to be exposed to that negative   effects.</p> <ul> <li>I don't see who will benefit from this change: Probably the people that   belongs to communities that are and have been under constant oppression for a   very long time, in this case, specially the racialized ones which have   suffered slavery.</li> </ul> <p>Sadly you will probably won't see many the affected people speak in these   discussions, first because there are not that many, sadly the IT world is   dominated by middle aged, economically comfortable, white, cis, hetero, males.   Small changes like this are meant to foster diversity in the community by   allowing them being more comfortable. Secondly because when they see these   debates they move on as they are so fed up on teaching privileged people of   their privileges. They not only have to suffer the oppression, we also put the   burden on their shoulders to teach us.</p> <p>As and ending thought, if you see yourself being specially troubled by the change, having a discomfort feeling and strong reactions. In my experience these signs are characteristic of privileged people that feel that their privileges are being threatened, I've felt them myself countless times. When I feel it, I usually do two things, fight them as strong as I can, or embrace them, analyze them, and go to the root of them. Depending on how much energy I have I go with the easy or the hard one. I'm not saying that it's you're case, but it could be.</p>"}, {"location": "git/#configuration", "title": "Configuration", "text": ""}, {"location": "git/#set-the-upstream-remote-by-default", "title": "Set the upstream remote by default", "text": "<pre><code>git config --global --add push.default current\ngit config --global --add push.autoSetupRemote true\n</code></pre>"}, {"location": "git/#snippets", "title": "Snippets", "text": ""}, {"location": "git/#remove-tags", "title": "Remove tags", "text": "<p>To delete a tag you can run:</p> <pre><code>git tag -d {{tag_name}}\n</code></pre> <p>To remove them remotely do</p> <pre><code>git push --delete origin {{ tag_name }}\n</code></pre>"}, {"location": "git/#revert-a-commit", "title": "Revert a commit", "text": "<pre><code>git revert commit_id\n</code></pre>"}, {"location": "git/#get-interesting-stats-of-the-repo", "title": "Get interesting stats of the repo", "text": "<p>Number of commits of the last year per user:</p> <pre><code>git shortlog -sne --since=\"31 Dec 2020\" --before=\"31 Dec 2021\"\n</code></pre> <p>You can also use <code>git-fame</code> to extract a more detailed report:</p> <pre><code>$: git-fame --since 1.year --cost hour --loc ins -w -M -C\n\n| Author          |   hrs |   loc |   coms |   fils |  distribution   |\n|:----------------|------:|------:|-------:|-------:|:----------------|\n| Lyz             |    10 | 28933 |    112 |    238 | 64.1/33.3/75.8  |\n| GitHub Action   |     2 | 16194 |    220 |     73 | 35.9/65.5/23.2  |\n| Alexander Gil   |     2 |     9 |      1 |      1 | 0.0/ 0.3/ 0.3   |\n| n0rt3y5ur       |     2 |     1 |      1 |      1 | 0.0/ 0.3/ 0.3   |\n| Guilherme Danno |     2 |     1 |      1 |      1 | 0.0/ 0.3/ 0.3   |\n| lyz-code        |     2 |     0 |      1 |      0 | 0.0/ 0.3/ 0.0   |\n</code></pre> <p>You can use <code>pipx install git-fame</code> to install it.</p>"}, {"location": "git/#references", "title": "References", "text": "<ul> <li>FAQ</li> <li>Funny FAQ</li> <li>Nvie post on branching model</li> </ul>"}, {"location": "git/#courses", "title": "Courses", "text": "<ul> <li>W3 git course</li> <li>Learngitbranching interactive tutorial</li> <li>katakoda</li> <li>Code academy</li> <li>Udemy</li> <li>Freecode camp article</li> </ul>"}, {"location": "git/#tools", "title": "Tools", "text": "<ul> <li>git-extras</li> </ul>"}, {"location": "gitea/", "title": "Gitea", "text": "<p>Gitea is a community managed lightweight code hosting solution written in Go. It's the best self hosted Github alternative in my opinion.</p>"}, {"location": "gitea/#installation", "title": "Installation", "text": "<p>Gitea provides automatically updated Docker images within its Docker Hub organisation.</p>"}, {"location": "gitea/#configure-gitea-actions", "title": "Configure gitea actions", "text": "<p>We've been using Drone as CI runner for some years now as Gitea didn't have their native runner. On Mar 20, 2023 however Gitea released the version 1.19.0 which promoted to stable the Gitea Actions which is a built-in CI system like GitHub Actions. With Gitea Actions, you can reuse your familiar workflows and Github Actions in your self-hosted Gitea instance. While it is not currently fully compatible with GitHub Actions, they intend to become as compatible as possible in future versions. The typical procedure is as follows:</p> <ul> <li>Register a runner (at the moment, act runners are the only option). This can be done on the following scopes:</li> <li>site-wide (by site admins)</li> <li>organization-wide (by organization owners)</li> <li>repository-wide (by repository owners)</li> <li>Create workflow files under <code>.gitea/workflows/&lt;workflow name&gt;.yaml</code> or <code>.github/workflows/&lt;workflow name&gt;.yaml</code>. The syntax is the same as the GitHub workflow syntax where supported. </li> </ul> <p>Gitea Actions advantages are:</p> <ul> <li>Uses the same pipeline syntax as Github Actions, so it's easier to use for new developers</li> <li>You can reuse existent Github actions.</li> <li>Migration from Github repositories to Gitea is easier.</li> <li>You see the results of the workflows in the same gitea webpage, which is much cleaner than needing to go to drone</li> <li>Define the secrets in the repository configuration.</li> </ul> <p>Drone advantages are:</p> <ul> <li>They have the promote event. Not critical as we can use other git events such as creating a tag.</li> <li>They can be run as a service by default. The gitea runners will need some work to run on instance restart.</li> <li>Has support for running kubernetes pipelines. Gitea actions doesn't yet support this</li> </ul>"}, {"location": "gitea/#setup-gitea-actions", "title": "Setup Gitea actions", "text": "<p>You need a Gitea instance with a version of 1.19.0 or higher. Actions are disabled by default (as they are still an feature-in-progress), so you need to add the following to the configuration file to enable it:</p> <pre><code>[actions]\nENABLED=true\n</code></pre> <p>Even if you enable at configuration level you need to manually enable the actions on each repository until this issue is solved.</p> <p>So far there is only one possible runner which is based on docker and <code>act</code>. Currently, the only way to install act runner is by compiling it yourself, or by using one of the pre-built binaries. There is no Docker image or other type of package management yet. At the moment, act runner should be run from the command line. Of course, you can also wrap this binary in something like a system service, supervisord, or Docker container.</p> <p>You can create the default configuration of the runner with:</p> <pre><code>./act_runner generate-config &gt; config.yaml\n</code></pre> <p>You can tweak there for example the <code>capacity</code> so you are able to run more than one workflow in parallel.</p> <p>Before running a runner, you should first register it to your Gitea instance using the following command:</p> <pre><code>./act_runner register --config config.yaml --no-interactive --instance &lt;instance&gt; --token &lt;token&gt;\n</code></pre> <p>There are two arguments required, <code>instance</code> and <code>token</code>.</p> <p><code>instance</code> refers to the address of your Gitea instance, like <code>http://192.168.8.8:3000</code>. The runner and job containers (which are started by the runner to execute jobs) will connect to this address. This means that it could be different from the <code>ROOT_URL</code> of your Gitea instance, which is configured for web access. It is always a bad idea to use a loopback address such as <code>127.0.0.1</code> or <code>localhost</code>, as we will discuss later. If you are unsure which address to use, the LAN address is usually the right choice.</p> <p><code>token</code> is used for authentication and identification, such as <code>P2U1U0oB4XaRCi8azcngmPCLbRpUGapalhmddh23</code>. It is one-time use only and cannot be used to register multiple runners. You can obtain tokens from <code>your_gitea.com/admin/runners</code>.</p> <p>After registering, a new file named <code>.runner</code> will appear in the current directory. This file stores the registration information. Please do not edit it manually. If this file is missing or corrupted, you can simply remove it and register again.</p> <p>Finally, it\u2019s time to start the runner.</p> <pre><code>./act_runner --config config.yaml daemon\n</code></pre> <p>You can also create a systemd service so that it starts when the server boots. For example in `/etc/systemd/system/gitea_actions_runner.service:</p> <pre><code>[Unit]\nDescription=Gitea Actions Runner\nAfter=network.target\n\n[Service]\nWorkingDirectory=/var/gitea/gitea/act_runner/main\nExecStart=/var/gitea/gitea/act_runner/main/act_runner-main-linux-amd64 daemon\n\n[Install]\nWantedBy=default.target\n</code></pre>"}, {"location": "gitea/#use-the-gitea-actions", "title": "Use the gitea actions", "text": "<p>Even if Actions is enabled for the Gitea instance, repositories still disable Actions by default. Enable it on the settings page of your repository.</p> <p>You will need to study the workflow syntax for Actions and write the workflow files you want.</p> <p>However, we can just start from a simple demo:</p> <pre><code>name: Gitea Actions Demo\nrun-name: ${{ gitea.actor }} is testing out Gitea Actions\non: [push]\njobs:\nExplore-Gitea-Actions:\nruns-on: ubuntu-latest\nsteps:\n- run: echo \"The job was automatically triggered by a ${{ gitea.event_name }} event.\"\n- run: echo \"This job is now running on a ${{ runner.os }} server hosted by Gitea!\"\n- run: echo \"The name of your branch is ${{ gitea.ref }} and your repository is ${{ gitea.repository }}.\"\n- name: Check out repository code\nuses: actions/checkout@v3\n- run: echo \"The ${{ gitea.repository }} repository has been cloned to the runner.\"\n- run: echo \"The workflow is now ready to test your code on the runner.\"\n- name: List files in the repository\nrun: |\nls ${{ gitea.workspace }}          \n- run: echo \"This job's status is ${{ gitea.status }}.\"\n</code></pre> <p>You can upload it as a file with the extension <code>.yaml</code> in the directory <code>.gitea/workflows/</code> or <code>.github/workflows</code> of the repository, for example <code>.gitea/workflows/demo.yaml</code>. </p> <p>You may be aware that there are tens of thousands of marketplace actions in GitHub. However, when you write <code>uses: actions/checkout@v3</code>, it actually downloads the scripts from gitea.com/actions/checkout by default (not GitHub). This is a mirror of github.com/actions/checkout, but it\u2019s impossible to mirror all of them. That\u2019s why you may encounter failures when trying to use some actions that haven\u2019t been mirrored.</p> <p>The good news is that you can specify the URL prefix to use actions from anywhere. This is an extra syntax in Gitea Actions. For example:</p> <ul> <li><code>uses: https://github.com/xxx/xxx@xxx</code></li> <li><code>uses: https://gitea.com/xxx/xxx@xxx</code></li> <li><code>uses: https://your_gitea_instance.com/xxx@xxx</code></li> </ul> <p>Be careful, the <code>https://</code> or <code>http://</code> prefix is necessary!</p>"}, {"location": "gitea/#tweak-the-runner-image", "title": "Tweak the runner image", "text": "<p>The gitea runner uses the <code>node:16-bullseye</code> image by default, in that image the <code>setup-python</code> action doesn't work. You can tweak the docker image that the runner runs by editing the <code>.runner</code> file that is in the directory where you registered the runner (probably close to the <code>act_runner</code> executable).</p> <p>If you open that up, you\u2019ll see that there is a section called labels, and it (most likely) looks like this:</p> <pre><code>\"labels\": [\n\"ubuntu-latest:docker://node:16-bullseye\",\n\"ubuntu-22.04:docker://node:16-bullseye\",\n\"ubuntu-20.04:docker://node:16-bullseye\",\n\"ubuntu-18.04:docker://node:16-buster\"\n]\n</code></pre> <p>You can specify any other docker image. Adding new labels doesn't work yet.</p> <p>You can start with this dockerfile:</p> <pre><code>FROM node:16-bullseye\n\n# Configure the labels\nLABEL prune=false\n\n# Configure the AWS credentials\nRUN mkdir /root/.aws\nCOPY files/config /root/.aws/config\nCOPY files/credentials /root/.aws/credentials\n\n# Install dependencies\nRUN apt-get update &amp;&amp; apt-get install -y \\\npython3 \\\npython3-pip \\\npython3-venv \\\nscreen \\\nvim \\\n&amp;&amp; python3 -m pip install --upgrade pip \\\n&amp;&amp; rm -rf /var/lib/apt/lists/*\n\nRUN pip install \\\nmolecule==5.0.1 \\\nansible==8.0.0 \\\nansible-lint \\\nyamllint \\ \nmolecule-plugins[ec2,docker,vagrant] \\\nboto3 \\ \nbotocore \\\ntestinfra \\\npytest\n\nRUN wget https://download.docker.com/linux/static/stable/x86_64/docker-24.0.2.tgz \\\n&amp;&amp; tar xvzf docker-24.0.2.tgz \\\n&amp;&amp; cp docker/* /usr/bin \\\n&amp;&amp; rm -r docker docker-*\n</code></pre> <p>It's prepared for:</p> <ul> <li>Working within an AWS environment</li> <li>Run Ansible and molecule</li> <li>Build dockers</li> </ul>"}, {"location": "gitea/#things-that-are-not-ready-yet", "title": "Things that are not ready yet", "text": "<ul> <li>Enable actions by default</li> <li>Kubernetes act runner</li> <li>Support cron jobs</li> <li>Badge for the CI jobs</li> </ul>"}, {"location": "gitea/#build-a-docker-within-a-gitea-action", "title": "Build a docker within a gitea action", "text": "<p>Assuming you're using the custom gitea_runner docker proposed above you can build and upload a docker to a registry with this action:</p> <pre><code>---\nname: Publish Docker image\n\n\"on\": [push]\n\njobs:\nbuild-and-push:\nruns-on: ubuntu-latest\nsteps:\n- name: Checkout code\nuses: https://github.com/actions/checkout@v3\n\n- name: Login to Docker Registry\nuses: https://github.com/docker/login-action@v2\nwith:\nregistry: my_registry.org\nusername: ${{ secrets.REGISTRY_USERNAME }}\npassword: ${{ secrets.REGISTRY_PASSWORD }}\n\n- name: Set up QEMU\nuses: https://github.com/docker/setup-qemu-action@v2\n\n- name: Set up Docker Buildx\nuses: https://github.com/docker/setup-buildx-action@v2\n\n- name: Extract metadata (tags, labels) for Docker\nid: meta\nuses: https://github.com/docker/metadata-action@v4\nwith:\nimages: my_registry.org/the_name_of_the_docker_to_build\n\n- name: Build and push\nuses: docker/build-push-action@v2\nwith:\ncontext: .\nplatforms: linux/amd64,linux/arm64\npush: true\ncache-from: type=registry,ref=my_registry.org/the_name_of_the_docker_to_build:buildcache\ncache-to: type=registry,ref=my_registry.org/the_name_of_the_docker_to_build:buildcache,mode=max\ntags: ${{ steps.meta.outputs.tags }}\nlabels: ${{ steps.meta.outputs.labels }}\n</code></pre> <p>It uses a pair of nice features:</p> <ul> <li>Multi-arch builds</li> <li>Cache to speed up the builds</li> </ul> <p>As it reacts to all events it will build and push:</p> <ul> <li>A tag with the branch name on each push to that branch</li> <li>a tag with the tag on tag push</li> </ul>"}, {"location": "gitea/#bump-the-version-of-a-repository-on-commits-on-master", "title": "Bump the version of a repository on commits on master", "text": "<ul> <li>Create a SSH key for the CI to send commits to protected branches. </li> <li>Upload the private key to a repo or organization secret called <code>DEPLOY_SSH_KEY</code>.</li> <li>Upload the public key to the repo configuration deploy keys</li> <li> <p>Create the <code>bump.yaml</code> file with the next contents:</p> <pre><code>---\nname: Bump version\n\n\"on\":\npush:\nbranches:\n- main\n\njobs:\nbump_version:\nif: \"!startsWith(github.event.head_commit.message, 'bump:')\"\nruns-on: ubuntu-latest\nname: \"Bump version and create changelog\"\nsteps:\n- name: Check out\nuses: actions/checkout@v3\nwith:\nfetch-depth: 0  # Fetch all history\n\n- name: Configure SSH\nrun: |\necho \"${{ secrets.DEPLOY_SSH_KEY }}\" &gt; ~/.ssh/deploy_key\nchmod 600 ~/.ssh/deploy_key\ndos2unix ~/.ssh/deploy_key\nssh-agent -a $SSH_AUTH_SOCK &gt; /dev/null\nssh-add ~/.ssh/deploy_key\n\n- name: Bump the version\nrun: cz bump --changelog --no-verify\n\n- name: Push changes\nrun: |\ngit remote add ssh git@gitea-production.cloud.icij.org:templates/ansible-role.git\ngit pull ssh main\ngit push ssh main\ngit push ssh --tags\n</code></pre> <p>It assumes that you have <code>cz</code> (commitizen) and <code>dos2unix</code> installed in your runner.</p> </li> </ul>"}, {"location": "gitea/#skip-gitea-actions-job-on-changes-of-some-files", "title": "Skip gitea actions job on changes of some files", "text": ""}, {"location": "gitea/#using-paths-filter-custom-action", "title": "Using <code>paths-filter</code> custom action", "text": "<pre><code>jobs:\n  test:\n    if: \"!startsWith(github.event.head_commit.message, 'bump:')\"\n    name: Test\n    runs-on: ubuntu-latest\n    steps:\n      - name: Checkout the codebase\n        uses: https://github.com/actions/checkout@v3\n\n      - name: Check if we need to run the molecule tests\n        uses: https://github.com/dorny/paths-filter@v2\n        id: filter\n        with:\n          filters: |\n            molecule:\n              - 'defaults/**'\n              - 'tasks/**'\n              - 'handlers/**'\n              - 'tasks/**'\n              - 'templates/**'\n              - 'molecule/**'\n              - 'requirements.yaml'\n              - '.github/workflows/tests.yaml'\n\n      - name: Run Molecule tests\n        if: steps.filter.outputs.molecule == 'true'\n        run: make molecule\n</code></pre> <p>You can find more examples on how to use <code>paths-filter</code> here.</p>"}, {"location": "gitea/#using-paths-ignore-gitea-actions-built-in-feature", "title": "Using <code>paths-ignore</code> gitea actions built-in feature", "text": "<p>Note: at least till 2023-09-04 this path lead to some errors such as pipeline not being triggered on the first commit of a pull request even if the files that should trigger it were modified.</p> <p>There are some expensive CI pipelines that don't need to be run for example if you changed a line in the <code>README.md</code>, to skip a pipeline on changes of certain files you can use the <code>paths-ignore</code> directive:</p> <pre><code>---\nname: Ansible Testing\n\n\"on\":\npush:\npaths-ignore:\n- 'meta/**'\n- Makefile\n- README.md\n- renovate.json\n- CHANGELOG.md\n- .cz.toml\n- '.gitea/workflows/**'\n\njobs:\ntest:\nname: Test\nruns-on: ubuntu-latest\nsteps:\n...\n</code></pre> <p>The only downside is that if you set this pipeline as required in the branch protection, the merge button will look yellow instead of green when the pipeline is skipped.</p>"}, {"location": "gitea/#disable-the-regular-login-use-only-oauth", "title": "Disable the regular login, use only Oauth", "text": "<p>Inside your <code>custom</code> directory which may be <code>/var/lib/gitea/custom</code>:</p> <ul> <li>Create the directories <code>templates/user/auth</code>, </li> <li>Create the <code>signin_inner.tmpl</code> file with the next contents:   <pre><code>                {{if or (not .LinkAccountMode) (and .LinkAccountMode .LinkAccountModeSignIn)}}\n              {{template \"base/alert\" .}}\n              {{end}}\n              &lt;h4 class=\"ui top attached header center\"&gt;\n                      {{if .LinkAccountMode}}\n                              {{.locale.Tr \"auth.oauth_signin_title\"}}\n                      {{else}}\n                              {{.locale.Tr \"auth.login_userpass\"}}\n                      {{end}}\n              &lt;/h4&gt;\n              &lt;div class=\"ui attached segment\"&gt;\n                      &lt;form class=\"ui form\" action=\"{{.SignInLink}}\" method=\"post\"&gt;\n                      {{.CsrfTokenHtml}}\n                      {{if and .OrderedOAuth2Names .OAuth2Providers}}\n                      &lt;div class=\"ui attached segment\"&gt;\n                              &lt;div class=\"oauth2 center\"&gt;\n                                      &lt;div id=\"oauth2-login-loader\" class=\"ui disabled centered loader\"&gt;&lt;/div&gt;\n                                      &lt;div&gt;\n                                              &lt;div id=\"oauth2-login-navigator\"&gt;\n                                                      &lt;p&gt;Sign in with &lt;/p&gt;\n                                                      {{range $key := .OrderedOAuth2Names}}\n                                                              {{$provider := index $.OAuth2Providers $key}}\n                                                              &lt;a href=\"{{AppSubUrl}}/user/oauth2/{{$key}}\"&gt;\n                                                                      &lt;img\n                                                                              alt=\"{{$provider.DisplayName}}{{if eq $provider.Name \"openidConnect\"}} ({{$key}}){{end}}\"\n                                                                              title=\"{{$provider.DisplayName}}{{if eq $provider.Name \"openidConnect\"}} ({{$key}}){{end}}\"\n                                                                              class=\"{{$provider.Name}} oauth-login-image\"\n                                                                              src=\"{{AppSubUrl}}{{$provider.Image}}\"\n                                                                      &gt;&lt;/a&gt;\n                                                      {{end}}\n                                              &lt;/div&gt;\n                                      &lt;/div&gt;\n                              &lt;/div&gt;\n                      &lt;/div&gt;\n                      {{end}}\n                      &lt;/form&gt;\n              &lt;/div&gt;\n</code></pre></li> <li>Download the <code>signin_inner.tmpl</code></li> </ul>"}, {"location": "gitea/#configure-it-with-terraform", "title": "Configure it with terraform", "text": "<p>Gitea can be configured through terraform too. There is an official provider that doesn't work, there's a fork that does though. Sadly it doesn't yet support configuring Oauth Authentication sources. Be careful <code>gitea_oauth2_app</code> looks to be the right resource to do that, but instead it configures Gitea to be the Oauth provider, not a consumer.</p> <p>To configure the provider you need to specify the url and a Gitea API token, keeping in mind that whoever gets access to this information will have access and full permissions on your Gitea instance it's critical that you store this information well. We'll use <code>sops</code> to encrypt the token with GPG..</p> <p>First create a Gitea user under <code>Site Administration/User Accounts/</code> with the <code>terraform</code> name (use your Oauth2 provider if you have one!).</p> <p>Then log in with that user and create a token with name <code>Terraform</code> under <code>Settings/Applications</code>, copy it to your clipboard.</p> <p>Configure <code>sops</code> by defining the gpg keys in a <code>.sops.yaml</code> file at the top of your repository:</p> <pre><code>---\ncreation_rules:\n- pgp: &gt;-\n2829BASDFHWEGWG23WDSLKGL323534J35LKWERQS,\n2GEFDBW349YHEDOH2T0GE9RH0NEORIG342RFSLHH\n</code></pre> <p>Then create the secrets file with the command <code>sops secrets.enc.json</code> somewhere in your terraform repository. For example:</p> <pre><code>{\n\"gitea_token\": \"paste the token here\"\n}\n</code></pre> <pre><code>terraform {\nrequired_providers {\ngitea = {\nsource  = \"Lerentis/gitea\"\nversion = \"~&gt; 0.12.1\"\n}\nsops = {\nsource = \"carlpett/sops\"\nversion = \"~&gt; 0.5\"\n}\n}\n}\n\nprovider \"gitea\" {\nbase_url   = \"https://gitea.your-domain.org\"\ntoken = data.sops_file.secrets.data[\"gitea_token\"]\n}\n</code></pre>"}, {"location": "gitea/#create-an-organization", "title": "Create an organization", "text": "<p>If you manage your users externally for example with an Oauth2 provider like Authentik you don't need to create a resource for the users, use a <code>data</code> instead:</p> <pre><code>resource \"gitea_org\" \"docker_compose\" {\nname = \"docker-compose\"\n}\n\nresource \"gitea_team\" \"docker_compose\" {\nname         = \"Developers\"\norganisation = gitea_org.docker_compose.name\npermission   = \"owner\"\nmembers      = [\ndata.gitea_user.lyz.username,\n]\n}\n</code></pre> <p>If you have many organizations that share the same users you can use variables.</p> <pre><code>resource \"gitea_org\" \"docker_compose\" {\nname = \"docker-compose\"\n}\n\nresource \"gitea_team\" \"docker_compose\" {\nname         = \"Developers\"\norganisation = gitea_org.docker_compose.name\npermission   = \"owner\"\nmembers      = [\ndata.gitea_user.lyz.username,\n]\n}\n</code></pre> <p>To import organisations and teams you need to use their <code>ID</code>. You can see the ID of the organisations in the Administration panel. To get the Teams ID you need to use the API. Go to https://your.gitea.com/api/swagger#/organization/orgListTeams and enter the organisation name.</p>"}, {"location": "gitea/#create-an-admin-user-through-the-command-line", "title": "Create an admin user through the command line", "text": "<pre><code>gitea --config /etc/gitea/app.ini admin user create --admin --email email --username user_name --password password\n</code></pre> <p>Or you can change the admin's password:</p> <pre><code>gitea --config /etc/gitea/app.ini admin user change-password -u username -p password\n</code></pre>"}, {"location": "gitea/#gitea-client-command-line-tool", "title": "Gitea client command line tool", "text": "<p><code>tea</code> is a command line tool to interact with Gitea servers. It still lacks some features but is usable.</p>"}, {"location": "gitea/#installation_1", "title": "Installation", "text": "<ul> <li>Download the precompiled binary from https://dl.gitea.com/tea/</li> <li>Until #542 is fixed manually create a token with all the permissions</li> <li>Run <code>tea login add</code> to set your credentials.</li> </ul>"}, {"location": "gitea/#references", "title": "References", "text": "<ul> <li>Home</li> <li> <p>Docs</p> </li> <li> <p>Terraform provider docs</p> </li> <li>Terraform provider source code</li> </ul>"}, {"location": "gitsigns/", "title": "gitsigns", "text": "<p>Gitsigns is a neovim plugin to create git decorations similar to the vim plugin gitgutter but written purely in Lua.</p>"}, {"location": "gitsigns/#installation", "title": "Installation", "text": "<p>Add to your <code>plugins.lua</code> file:</p> <pre><code>  use {'lewis6991/gitsigns.nvim'}\n</code></pre> <p>Install it with <code>:PackerInstall</code>.</p> <p>Configure it in your <code>init.lua</code> with:</p> <pre><code>-- Configure gitsigns\nrequire('gitsigns').setup({\n  on_attach = function(bufnr)\n    local gs = package.loaded.gitsigns\n\n    local function map(mode, l, r, opts)\n      opts = opts or {}\n      opts.buffer = bufnr\n      vim.keymap.set(mode, l, r, opts)\n    end\n\n    -- Navigation\n    map('n', ']c', function()\n      if vim.wo.diff then return ']c' end\n      vim.schedule(function() gs.next_hunk() end)\n      return '&lt;Ignore&gt;'\n    end, {expr=true})\n\n    map('n', '[c', function()\n      if vim.wo.diff then return '[c' end\n      vim.schedule(function() gs.prev_hunk() end)\n      return '&lt;Ignore&gt;'\n    end, {expr=true})\n\n    -- Actions\n    map('n', '&lt;leader&gt;gs', gs.stage_hunk)\n    map('n', '&lt;leader&gt;gr', gs.reset_hunk)\n    map('v', '&lt;leader&gt;gs', function() gs.stage_hunk {vim.fn.line('.'), vim.fn.line('v')} end)\n    map('v', '&lt;leader&gt;gr', function() gs.reset_hunk {vim.fn.line('.'), vim.fn.line('v')} end)\n    map('n', '&lt;leader&gt;gS', gs.stage_buffer)\n    map('n', '&lt;leader&gt;gu', gs.undo_stage_hunk)\n    map('n', '&lt;leader&gt;gR', gs.reset_buffer)\n    map('n', '&lt;leader&gt;gp', gs.preview_hunk)\n    map('n', '&lt;leader&gt;gb', function() gs.blame_line{full=true} end)\n    map('n', '&lt;leader&gt;gb', gs.toggle_current_line_blame)\n    map('n', '&lt;leader&gt;gd', gs.diffthis)\n    map('n', '&lt;leader&gt;gD', function() gs.diffthis('~') end)\n    map('n', '&lt;leader&gt;ge', gs.toggle_deleted)\n\n    -- Text object\n    map({'o', 'x'}, 'ih', ':&lt;C-U&gt;Gitsigns select_hunk&lt;CR&gt;')\n  end\n})\n</code></pre>"}, {"location": "gitsigns/#usage", "title": "Usage", "text": "<p>Some interesting bindings:</p> <ul> <li><code>]c</code>: Go to next diff chunk</li> <li><code>[c</code>: Go to previous diff chunk</li> <li><code>&lt;leader&gt;gs</code>: Stage chunk, it works both in normal and visual mode</li> <li><code>&lt;leader&gt;gr</code>: Restore chunk from index, it works both in normal and visual mode</li> <li><code>&lt;leader&gt;gp</code>: Preview diff, you can use it with <code>]c</code> and <code>[c</code> to see all the chunk diffs</li> <li><code>&lt;leader&gt;gb</code>: Show the git blame of the line as a shadowed comment</li> </ul>"}, {"location": "gitsigns/#references", "title": "References", "text": "<ul> <li>Source</li> </ul>"}, {"location": "goaccess/", "title": "goaccess", "text": "<p>goaccess is a fast terminal-based log analyzer.</p> <p>Its core idea is to quickly analyze and view web server statistics in real time without needing to use your browser (great if you want to do a quick analysis of your access log via SSH, or if you simply love working in the terminal).</p> <p>While the terminal output is the default output, it has the capability to generate a complete, self-contained real-time HTML report (great for analytics, monitoring and data visualization), as well as a JSON, and CSV report.</p>"}, {"location": "goaccess/#installation", "title": "Installation", "text": "<pre><code>apt-get install goaccess\n</code></pre>"}, {"location": "goaccess/#usage", "title": "Usage", "text": ""}, {"location": "goaccess/#custom-log-format", "title": "Custom log format", "text": "<p>Sometimes the log format isn't supported, then you'll have to specify the log format. For example:</p> <pre><code>goaccess \\\n--log-format='%^,%^,%^: %h:%^ %^ [%d:%t.%^] %^ %^/%^ %^/%^/%^/%^/%^ %s %b - - ---- %^/%^/%^/%^/%^ %^/%^ {%v|} %^ %m \"\"%U\"\" \"%q\"' \\\n--date-format '%d/%b/%Y' \\\n--time-format '%H:%M:%S' \\\nfile.log\n</code></pre>"}, {"location": "goaccess/#references", "title": "References", "text": "<ul> <li>Home</li> <li>Git</li> <li> <p>Docs</p> </li> <li> <p>Tweaking goaccess for analytics post</p> </li> </ul>"}, {"location": "goodconf/", "title": "GoodConf", "text": "<p>goodconf is a thin wrapper over Pydantic's settings management. Allows you to define configuration variables and load them from environment or JSON/YAML file. Also generates initial configuration files and documentation for your defined configuration.</p>"}, {"location": "goodconf/#installation", "title": "Installation", "text": "<p><code>pip install goodconf</code> or <code>pip install goodconf[yaml]</code> if parsing/generating YAML files is required.</p>"}, {"location": "goodconf/#basic-usage", "title": "Basic Usage", "text": "<p>Define the configuration object in <code>config.py</code>:</p> <pre><code>import base64\nimport os\n\nfrom goodconf import GoodConf, Field\nfrom pydantic import PostgresDsn\n\n\nclass AppConfig(GoodConf):  # type: ignore\n\"\"\"Configure my application.\"\"\"\n\n    debug: bool\n    database_url: PostgresDsn = \"postgres://localhost:5432/mydb\"\n    secret_key: str = Field(\n        initial=lambda: base64.b64encode(os.urandom(60)).decode(),\n        description=\"Used for cryptographic signing. \"\n        \"https://docs.djangoproject.com/en/2.0/ref/settings/#secret-key\",\n    )\n\n    class Config:\n\"\"\"Define the default files to check.\"\"\"\n\n        default_files = [\n            os.path.expanduser(\"~/.local/share/your_program/config.yaml\"),\n            \"config.yaml\",\n        ]\n\n\nconfig = AppConfig()\n</code></pre> <p>To load the configuration use <code>config.load()</code>. If you don't pass any file to <code>load()</code>, then the <code>default_files</code> will be read in order.</p> <p>Remember that environment variables always take precedence over variables in the configuration files.</p> <p>For more details see Pydantic's docs for examples of loading:</p> <ul> <li>Dotenv (.env) files.</li> <li>Docker secrets.</li> </ul>"}, {"location": "goodconf/#references", "title": "References", "text": "<ul> <li>Git</li> </ul>"}, {"location": "gotify/", "title": "Gotify", "text": "<p>Gotify is a simple server for sending and receiving messages in real-time per WebSocket. </p>"}, {"location": "gotify/#not-there-yet", "title": "Not there yet", "text": "<ul> <li>Reactions on the notifications</li> </ul>"}, {"location": "gotify/#references", "title": "References", "text": "<ul> <li>Source</li> </ul>"}, {"location": "grafana/", "title": "Grafana", "text": "<p>Grafana is a web application to create dashboards.</p>"}, {"location": "grafana/#installation", "title": "Installation", "text": "<p>We're going to install it with docker-compose and connect it to Authentik.</p>"}, {"location": "grafana/#create-the-authentik-connection", "title": "Create the Authentik connection", "text": "<p>Assuming that you have the terraform authentik provider configured, use the next terraform code:</p> <pre><code># ---------------\n# -- Variables --\n# ---------------\n\nvariable \"grafana_name\" {\ntype        = string\ndescription = \"The name shown in the Grafana application.\"\ndefault     = \"Grafana\"\n}\n\nvariable \"grafana_redirect_uri\" {\ntype        = string\ndescription = \"The redirect url configured on Grafana.\"\n}\n\nvariable \"grafana_icon\" {\ntype        = string\ndescription = \"The icon shown in the Grafana application\"\ndefault     = \"/application-icons/grafana.svg\"\n}\n\n# -----------------------\n# --    Application    --\n# -----------------------\n\nresource \"authentik_application\" \"grafana\" {\nname              = var.grafana_name\nslug              = \"grafana\"\nprotocol_provider = authentik_provider_oauth2.grafana.id\nmeta_icon         = var.grafana_icon\nlifecycle {\nignore_changes = [\n      # The terraform provider is continuously changing the attribute even though it's set\nmeta_icon,\n]\n}\n}\n\n# --------------------------\n# --    Oauth provider    --\n# --------------------------\n\nresource \"authentik_provider_oauth2\" \"grafana\" {\nname               = var.grafana_name\nclient_id          = \"grafana\"\nauthorization_flow = data.authentik_flow.default-authorization-flow.id\nproperty_mappings = [\ndata.authentik_scope_mapping.email.id,\ndata.authentik_scope_mapping.openid.id,\ndata.authentik_scope_mapping.profile.id,\n]\nredirect_uris = [\nvar.grafana_redirect_uri,\n]\nsigning_key = data.authentik_certificate_key_pair.default.id\naccess_token_validity = \"minutes=120\"\n}\n\ndata \"authentik_certificate_key_pair\" \"default\" {\nname = \"authentik Self-signed Certificate\"\n}\n\ndata \"authentik_flow\" \"default-authorization-flow\" {\nslug = \"default-provider-authorization-implicit-consent\"\n}\n\n# -------------------\n# --    Outputs    --\n# -------------------\n\noutput \"grafana_oauth_id\" {\nvalue = authentik_provider_oauth2.grafana.client_id\n}\n\noutput \"grafana_oauth_secret\" {\nvalue = authentik_provider_oauth2.grafana.client_secret\n}\n</code></pre> <p>You'll need to upload the <code>grafana.svg</code> to your authentik application you can use the next docker-compose file.</p> <pre><code>---\nversion: \"3.3\"\nservices:\ngrafana:\nimage: grafana/grafana-oss:${GRAFANA_VERSION:-latest}\ncontainer_name: grafana\nrestart: unless-stopped\nvolumes:\n- config:/etc/grafana\n- data:/var/lib/grafana\nnetworks:\n- grafana\n- monitorization\n- swag\nenv_file:\n- .env\ndepends_on:\n- db\ndb:\nimage: postgres:${DATABASE_VERSION:-15}\nrestart: unless-stopped\ncontainer_name: grafana-db\nenvironment:\n- POSTGRES_DB=${GF_DATABASE_NAME:-grafana}\n- POSTGRES_USER=${GF_DATABASE_USER:-grafana}\n- POSTGRES_PASSWORD=${GF_DATABASE_PASSWORD:?database password required}\nnetworks:\n- grafana\nvolumes:\n- db-data:/var/lib/postgresql/data\nenv_file:\n- .env\n\nnetworks:\ngrafana:\nexternal:\nname: grafana\nmonitorization:\nexternal:\nname: monitorization\nswag:\nexternal:\nname: swag\n\nvolumes:\nconfig:\ndriver: local\ndriver_opts:\ntype: none\no: bind\ndevice: /data/grafana/app/config\ndata:\ndriver: local\ndriver_opts:\ntype: none\no: bind\ndevice: /data/grafana/app/data\ndb-data:\ndriver: local\ndriver_opts:\ntype: none\no: bind\ndevice: /data/grafana/database\n</code></pre> <p>Where the <code>monitorization</code> network is where prometheus and the rest of the stack listens, and <code>swag</code> the network to the gateway proxy.</p> <p>It uses the <code>.env</code> file to store the required configuration, to connect grafana with authentik you need to add the next variables:</p> <pre><code># --------------------------\n# --- Auth configuration ---\n# --------------------------\n\nGF_AUTH_GENERIC_OAUTH_ENABLED=\"true\"\nGF_AUTH_GENERIC_OAUTH_NAME=\"authentik\"\nGF_AUTH_GENERIC_OAUTH_CLIENT_ID=\"&lt;Client ID from above&gt;\"\nGF_AUTH_GENERIC_OAUTH_CLIENT_SECRET=\"&lt;Client Secret from above&gt;\"\nGF_AUTH_GENERIC_OAUTH_SCOPES=\"openid profile email\"\nGF_AUTH_GENERIC_OAUTH_AUTH_URL=\"https://authentik.company/application/o/authorize/\"\nGF_AUTH_GENERIC_OAUTH_TOKEN_URL=\"https://authentik.company/application/o/token/\"\nGF_AUTH_GENERIC_OAUTH_API_URL=\"https://authentik.company/application/o/userinfo/\"\nGF_AUTH_SIGNOUT_REDIRECT_URL=\"https://authentik.company/application/o/&lt;Slug of the application from above&gt;/end-session/\"\n# Optionally enable auto-login (bypasses Grafana login screen)\nGF_AUTH_OAUTH_AUTO_LOGIN=\"true\"\n# Optionally map user groups to Grafana roles\nGF_AUTH_GENERIC_OAUTH_ROLE_ATTRIBUTE_PATH=\"contains(groups[*], 'Grafana Admins') &amp;&amp; 'Admin' || contains(groups[*], 'Grafana Editors') &amp;&amp; 'Editor' || 'Viewer'\"\n</code></pre> <p>In the configuration above you can see an example of a role mapping. Upon login, this configuration looks at the groups of which the current user is a member. If any of the specified group names are found, the user will be granted the resulting role in Grafana.</p> <p>In the example shown above, one of the specified group names is \"Grafana Admins\". If the user is a member of this group, they will be granted the \"Admin\" role in Grafana. If the user is not a member of the \"Grafana Admins\" group, it moves on to see if the user is a member of the \"Grafana Editors\" group. If they are, they are granted the \"Editor\" role. Finally, if the user is not found to be a member of either of these groups, it fails back to granting the \"Viewer\" role.</p> <p>Also make sure in your configuration that <code>root_url</code> is set correctly, otherwise your redirect url might get processed incorrectly. For example, if your grafana instance is running on the default configuration and is accessible behind a reverse proxy at https://grafana.company, your redirect url will end up looking like this, https://grafana.company/. If you get <code>user does not belong to org</code> error when trying to log into grafana for the first time via OAuth, check if you have an organization with the ID of 1, if not, then you have to add the following to your grafana config: w</p> <pre><code>[users]\nauto_assign_org = true\nauto_assign_org_id = &lt;id-of-your-default-organization&gt;\n</code></pre> <p>Once you've made sure that the oauth works, go to <code>/admin/users</code> and remove the <code>admin</code> user.</p>"}, {"location": "grafana/#configure-grafana", "title": "Configure grafana", "text": "<p>Grafana has default and custom configuration files. You can customize your Grafana instance by modifying the custom configuration file or by using environment variables. To see the list of settings for a Grafana instance, refer to View server settings.</p> <p>To override an option use <code>GF_&lt;SectionName&gt;_&lt;KeyName&gt;</code>. Where the <code>section name</code> is the text within the brackets. Everything should be uppercase, <code>.</code> and <code>-</code> should be replaced by <code>_</code>. For example, if you have these configuration settings:</p> <pre><code># default section\ninstance_name = ${HOSTNAME}\n\n[security]\nadmin_user = admin\n\n[auth.google]\nclient_secret = 0ldS3cretKey\n\n[plugin.grafana-image-renderer]\nrendering_ignore_https_errors = true\n\n[feature_toggles]\nenable = newNavigation\n</code></pre> <p>You can override variables on Linux machines with:</p> <pre><code>export GF_DEFAULT_INSTANCE_NAME=my-instance\nexport GF_SECURITY_ADMIN_USER=owner\nexport GF_AUTH_GOOGLE_CLIENT_SECRET=newS3cretKey\nexport GF_PLUGIN_GRAFANA_IMAGE_RENDERER_RENDERING_IGNORE_HTTPS_ERRORS=true\nexport GF_FEATURE_TOGGLES_ENABLE=newNavigation\n</code></pre> <p>And in the docker compose you can edit the <code>.env</code> file. Mine looks similar to:</p> <pre><code># Check all configuration options at:\n# https://grafana.com/docs/grafana/latest/setup-grafana/configure-grafana\n\n# -----------------------------\n# --- General configuration ---\n# -----------------------------\n\nGRAFANA_VERSION=latest\nGF_DEFAULT_INSTANCE_NAME=\"production\"\nGF_SERVER_ROOT_URL=\"https://your.domain.org\"\n\n# Set this option to true to enable HTTP compression, this can improve transfer\n# speed and bandwidth utilization. It is recommended that most users set it to\n# true. By default it is set to false for compatibility reasons.\nGF_SERVER_ENABLE_GZIP=\"true\"\n\n# ------------------------------\n# --- Database configuration ---\n# ------------------------------\n\nDATABASE_VERSION=15\nGF_DATABASE_TYPE=postgres\nDATABASE_VERSION=15\nGF_DATABASE_HOST=grafana-db:5432\nGF_DATABASE_NAME=grafana\nGF_DATABASE_USER=grafana\nGF_DATABASE_PASSWORD=\"change-for-a-long-password\"\nGF_DATABASE_SSL_MODE=disable\n\n# --------------------------\n# --- Auth configuration ---\n# --------------------------\n\nGF_AUTH_GENERIC_OAUTH_ENABLED=\"true\"\nGF_AUTH_GENERIC_OAUTH_NAME=\"authentik\"\nGF_AUTH_GENERIC_OAUTH_CLIENT_ID=\"&lt;Client ID from above&gt;\"\nGF_AUTH_GENERIC_OAUTH_CLIENT_SECRET=\"&lt;Client Secret from above&gt;\"\nGF_AUTH_GENERIC_OAUTH_SCOPES=\"openid profile email\"\nGF_AUTH_GENERIC_OAUTH_AUTH_URL=\"https://authentik.company/application/o/authorize/\"\nGF_AUTH_GENERIC_OAUTH_TOKEN_URL=\"https://authentik.company/application/o/token/\"\nGF_AUTH_GENERIC_OAUTH_API_URL=\"https://authentik.company/application/o/userinfo/\"\nGF_AUTH_SIGNOUT_REDIRECT_URL=\"https://authentik.company/application/o/&lt;Slug of the application from above&gt;/end-session/\"\n# Optionally enable auto-login (bypasses Grafana login screen)\nGF_AUTH_OAUTH_AUTO_LOGIN=\"true\"\n# Set to true to enable automatic sync of the Grafana server administrator\n# role. If this option is set to true and the result of evaluating\n# role_attribute_path for a user is GrafanaAdmin, Grafana grants the user the\n# server administrator privileges and organization administrator role. If this\n# option is set to false and the result of evaluating role_attribute_path for a\n# user is GrafanaAdmin, Grafana grants the user only organization administrator\n# role.\nGF_AUTH_GENERIC_OAUTH_ALLOW_ASSIGN_GRAFANA_ADMIN=\"true\"\n# Optionally enable auto-login (bypasses Grafana login screen)\n# Optionally map user groups to Grafana roles\n# Optionally map user groups to Grafana roles\nGF_AUTH_GENERIC_OAUTH_ROLE_ATTRIBUTE_PATH=\"contains(groups[*], 'Grafana Admins') &amp;&amp; 'Admin' || contains(groups[*], 'Grafana Editors') &amp;&amp; 'Editor' || 'Viewer'\"\n# Set to true to disable (hide) the login form, useful if you use OAuth. Default is false.\nGF_AUTH_DISABLE_LOGIN_FORM=\"true\"\n\n# -------------------------\n# --- Log configuration ---\n# -------------------------\n\n# Options are \u201cconsole\u201d, \u201cfile\u201d, and \u201csyslog\u201d. Default is \u201cconsole\u201d and \u201cfile\u201d. Use spaces to separate multiple modes, e.g. console file.\nGF_LOG_MODE=\"console file\"\n# Options are \u201cdebug\u201d, \u201cinfo\u201d, \u201cwarn\u201d, \u201cerror\u201d, and \u201ccritical\u201d. Default is info.\nGF_LOG_LEVEL=\"info\"\n</code></pre>"}, {"location": "grafana/#configure-datasources", "title": "Configure datasources", "text": "<p>You can manage data sources in Grafana by adding YAML configuration files in the <code>provisioning/datasources</code> directory. Each config file can contain a list of datasources to add or update during startup. If the data source already exists, Grafana reconfigures it to match the provisioned configuration file.</p> <p>The configuration file can also list data sources to automatically delete, called <code>deleteDatasources</code>. Grafana deletes the data sources listed in <code>deleteDatasources</code> before adding or updating those in the datasources list.</p> <p>For example to configure a Prometheus datasource use:</p> <pre><code>apiVersion: 1\n\ndatasources:\n- name: Prometheus\ntype: prometheus\naccess: proxy\n# Access mode - proxy (server in the UI) or direct (browser in the UI).\nurl: http://prometheus:9090\njsonData:\nhttpMethod: POST\nmanageAlerts: true\ntimeInterval: 30s\nprometheusType: Prometheus\nprometheusVersion: 2.44.0\ncacheLevel: 'High'\ndisableRecordingRules: false\nincrementalQueryOverlapWindow: 10m\nexemplarTraceIdDestinations: []\n</code></pre> <p>Be careful to set the <code>timeInterval</code> variable to the value of how often you scrape the data from the node exporter to avoid this issue.</p>"}, {"location": "grafana/#configure-dashboards", "title": "Configure dashboards", "text": "<p>You can manage dashboards in Grafana by adding one or more YAML config files in the <code>provisioning/dashboards</code> directory. Each config file can contain a list of dashboards providers that load dashboards into Grafana from the local filesystem.</p> <p>Create one file called <code>dashboards.yaml</code> with the next contents:</p> <pre><code>---\napiVersion: 1\nproviders:\n- name: default # A uniquely identifiable name for the provider\ntype: file\noptions:\npath: /etc/grafana/provisioning/dashboards/definitions\n</code></pre> <p>Then inside the config directory of your docker compose create the directory <code>provisioning/dashboards/definitions</code> and add the json of the dashboards themselves. You can download them from the dashboard pages. For example:</p> <ul> <li>Node Exporter</li> <li>Blackbox Exporter</li> <li>Alertmanager</li> </ul>"}, {"location": "grafana/#configure-the-plugins", "title": "Configure the plugins", "text": "<p>To install plugins in the Docker container, complete the following steps:</p> <ul> <li>Pass the plugins you want to be installed to Docker with the <code>GF_INSTALL_PLUGINS</code> environment variable as a comma-separated list.</li> <li>This sends each plugin name to <code>grafana-cli plugins install ${plugin}</code> and installs them when Grafana starts.</li> </ul> <p>For example:</p> <pre><code>docker run -d -p 3000:3000 --name=grafana \\\n-e \"GF_INSTALL_PLUGINS=grafana-clock-panel, grafana-simple-json-datasource\" \\\ngrafana/grafana-oss\n</code></pre> <p>To specify the version of a plugin, add the version number to the <code>GF_INSTALL_PLUGINS</code> environment variable. For example: <code>GF_INSTALL_PLUGINS=grafana-clock-panel 1.0.1</code>. </p> <p>To install a plugin from a custom URL, use the following convention to specify the URL: <code>&lt;url to plugin zip&gt;;&lt;plugin install folder name&gt;</code>. For example: <code>GF_INSTALL_PLUGINS=https://github.com/VolkovLabs/custom-plugin.zip;custom-plugin</code>.</p>"}, {"location": "grafana/#references", "title": "References", "text": "<ul> <li>Home</li> </ul>"}, {"location": "grapheneos/", "title": "GrapheneOS", "text": "<p>GrapheneOS is a private and secure mobile operating system with Android app compatibility. Developed as a non-profit open source project.</p> <p>GrapheneOS is a private and secure mobile operating system with great functionality and usability. It starts from the strong baseline of the Android Open Source Project (AOSP) and takes great care to avoid increasing attack surface or hurting the strong security model. GrapheneOS makes substantial improvements to both privacy and security through many carefully designed features built to function against real adversaries. The project cares a lot about usability and app compatibility so those are taken into account for all of our features.</p> <p>GrapheneOS is also hard at work on filling in gaps from not bundling Google apps and services into the OS. We aren't against users using Google services but it doesn't belong integrated into the OS in an invasive way. GrapheneOS won't take the shortcut of simply bundling a very incomplete and poorly secured third party reimplementation of Google services into the OS. That wouldn't ever be something users could rely upon. It will also always be chasing a moving target while offering poorer security than the real thing if the focus is on simply getting things working without great care for doing it robustly and securely.</p>"}, {"location": "grapheneos/#features", "title": "Features", "text": "<p>These are a subset some of the features of GrapheneOS beyond what's provided by version 13 of the Android Open Source Project. It only covers our improvements to AOSP and not baseline features. This section doesn't list features like the standard app sandbox, verified boot, exploit mitigations (ASLR, SSP, Shadow Call Stack, Control Flow Integrity, etc.), permission system (foreground-only and one-time permission grants, scoped file access control, etc.) and so on but rather only our improvements to modern Android.</p>"}, {"location": "grapheneos/#defending-against-exploitation-of-unknown-vulnerabilities", "title": "Defending against exploitation of unknown vulnerabilities", "text": "<p>The first line of defense is attack surface reduction. Removing unnecessary code or exposed attack surface eliminates many vulnerabilities completely. GrapheneOS avoids removing any useful functionality for end users, but we can still disable lots of functionality by default and require that users opt-in to using it to eliminate it for most of them. An example we landed upstream in Android is disallowing using the kernel's profiling support by default, since it was and still is a major source of Linux kernel vulnerabilities.</p> <p>The next line of defense is preventing an attacker from exploiting a vulnerability, either by making it impossible, unreliable or at least meaningfully harder to develop. The vast majority of vulnerabilities are well understood classes of bugs and exploitation can be prevented by avoiding the bugs via languages/tooling or preventing exploitation with strong exploit mitigations. In many cases, vulnerability classes can be completely wiped out while in many others they can at least be made meaningfully harder to exploit. Android does a lot of work in this area and GrapheneOS has helped to advance this in Android and the Linux kernel.</p> <p>The final line of defense is containment through sandboxing at various levels: fine-grained sandboxes around a specific context like per site browser renderers, sandboxes around a specific component like Android's media codec sandbox and app / workspace sandboxes like the Android app sandbox used to sandbox each app which is also the basis for user/work profiles. GrapheneOS improves all of these sandboxes through fortifying the kernel and other base OS components along with improving the sandboxing policies.</p> <p>Preventing an attacker from persisting their control of a component or the OS / firmware through verified boot and avoiding trust in persistent state also helps to mitigate the damage after a compromise has occurred.</p>"}, {"location": "grapheneos/#attack-surface-reduction", "title": "Attack surface reduction", "text": "<ul> <li>Greatly reduced remote, local and proximity-based attack surface by stripping   out unnecessary code, making more features optional and disabling optional   features by default (NFC, Bluetooth, etc.), when the screen is locked   (connecting new USB peripherals, camera access) and optionally after a timeout   (Bluetooth, Wi-Fi)</li> <li>Option to disable native debugging (ptrace) to reduce local attack surface   (still enabled by default for compatibility)</li> </ul>"}, {"location": "grapheneos/#downsides", "title": "Downsides", "text": "<p>It looks that the community behind GrapheneOS is not the kindest one, they are sometimes harsh and when they are questioned they enter a defensive position. This can be seen in the discussions regarding whether or not to use the screen pattern lock (1, 2).</p>"}, {"location": "grapheneos/#recommended-devices", "title": "Recommended devices", "text": "<p>They strongly recommend only purchasing one of the following devices for GrapheneOS due to better security and a minimum 5 year guarantee from launch for full security updates and other improvements:</p> <ul> <li>Pixel 7 Pro</li> <li>Pixel 7</li> <li>Pixel 6a</li> <li>Pixel 6 Pro</li> <li>Pixel 6</li> </ul> <p>!!! note \"Check the source as this section is probably outdated\"</p> <p>Newer devices have more of their 5 year minimum guarantee remaining but the actual support time may be longer than the minimum guarantee.</p> <p>The Pixel 7 and Pixel 7 Pro are all around improvements over the Pixel 6 and Pixel 6 Pro with a significantly better GPU and cellular radio along with an incremental CPU upgrade. The 7<sup>th</sup> generation Pixels are far more similar to the previous generation than any prior Pixels.</p> <p>The Pixel 6 and Pixel 6 Pro are flagship phones with much nicer hardware than previous generation devices (cameras, CPU, GPU, display, battery).</p> <p>The cheaper Pixel 6 has extremely competitive pricing for the flagship level hardware especially with the guaranteed long term support. Pixel 6 Pro has 50% more memory (12GB instead of 8GB), a higher end screen, a 3<sup>rd</sup> rear camera with 4x optical zoom and a higher end front camera. Both devices have the same SoC (CPU, GPU, etc.) and the same main + ultrawide rear cameras. The Pixel 6 is quite large and the Pixel 6 Pro is larger.</p> <p>The Pixel 6a is a budget device with the same 5 years of guaranteed full security support from launch as the flagship 6<sup>th</sup> generation Pixels. It also has the same flagship SoC as the higher end devices, the same main rear and front cameras as the Pixel 5 and a rear wide angle lens matching the flagship 6<sup>th</sup> generation Pixels. Compared to the 5<sup>th</sup> generation Pixels, it has 5 years of full security support remaining instead of less than 2 years and the CPU is 2x faster. We strongly recommend buying the Pixel 6a rather than trying to get a deal with older generation devices. You'll be able to use the Pixel 6a much longer before it needs to be replaced due to lack of support.</p> <p>It's funny though that in the search for security and privacy you end up buying a Google device. If you also reached this thought, you're not alone. Summing up, the Pixel's are in fact the devices that are more secure and that potentially respect your privacy.</p>"}, {"location": "grapheneos/#installation", "title": "Installation", "text": "<p>I was not able to follow the web instructions so I had to follow the cli ones.</p> <p>Whenever I run a <code>fastboot</code> command it got stuck in <code>&lt; waiting for devices &gt;</code>, so I added the next rules on the <code>udev</code> configuration at <code>/etc/udev/rules.d/51-android.rules</code></p> <pre><code>SUBSYSTEM==\"usb\", ATTR{idVendor}==\"18d1\", ATTR{idProduct}==\"4ee7\", MODE=\"0600\", OWNER=\"myuser\"\n</code></pre> <p>The <code>idProduct</code> and <code>idVendor</code> were deduced from <code>lsusb</code>. Then after a restart everything worked fine.</p>"}, {"location": "grapheneos/#setup-auditor", "title": "Setup Auditor", "text": "<p>Auditor provides attestation for GrapheneOS phones and the stock operating systems on a number of devices. It uses hardware security features to make sure that the firmware and operating system have not been downgraded or tampered with.</p> <p>Attestation can be done locally by pairing with another Android 8+ device or remotely using the remote attestation service. To make sure that your hardware and operating system is genuine, perform local attestation immediately after the device has been setup and prior to any internet connection.</p>"}, {"location": "grapheneos/#references", "title": "References", "text": "<ul> <li> <p>Home</p> </li> <li> <p>Articles</p> </li> <li>Features</li> </ul>"}, {"location": "graylog/", "title": "Graylog", "text": "<p>Graylog is a log management tool</p>"}, {"location": "graylog/#tips", "title": "Tips", "text": ""}, {"location": "graylog/#send-a-test-message-to-check-an-input", "title": "Send a test message to check an input", "text": "<p>The next line will send a test message to the TCP 12201 port of the graylog server, if you use UDP, add the <code>-u</code> flag to the <code>nc</code> command.</p> <pre><code>echo -e '{\"version\": \"1.1\",\"host\":\"example.org\",\"short_message\":\"Short message\",\"full_message\":\"Backtrace here\\n\\nmore stuff\",\"level\":1,\"_user_id\":9001,\"_some_info\":\"foo\",\"_some_env_var\":\"bar\"}\\0' | nc -w 1 my.graylog.server 12201\n</code></pre> <p>To see if it arrives, you can check the <code>Input</code> you're trying to access, or at a lower level, you can <code>ngrep</code> with:</p> <pre><code>ngrep -d any port 12201\n</code></pre> <p>Or if you're using UDP:</p> <pre><code>ngrep -d any '' udp port 12201\n</code></pre>"}, {"location": "graylog/#references", "title": "References", "text": "<ul> <li>Homepage</li> </ul>"}, {"location": "grocy_management/", "title": "Grocy Management", "text": "<p>Buying stuff is an unpleasant activity that drains your energy and time, it's the main perpetrator of the broken capitalist system, but sadly we have to yield to survive.</p> <p>This article explores my thoughts and findings on how to optimize the use of time, money and mental load in grocy management to have enough stuff stored to live, while following the principles of ecology and sustainability. I'm no expert at all on either of these topics. I'm learning and making my mind while writing these lines.</p> <p>grocy is a web-based self-hosted groceries &amp; household management solution for your home.</p> <p>My chosen way to deploy grocy has been using Docker. The hard part comes when you do the initial load, as you have to add all the:</p> <ul> <li>User attributes.</li> <li>Product locations.</li> <li>Product groups.</li> <li>Quantity conversions.</li> <li>Products.</li> </ul>"}, {"location": "grocy_management/#tips", "title": "Tips", "text": "<p>Note</p> <p>Very recommended to use the android app</p> <ul> <li>Add first the products with less letters, so add first <code>Toothpaste</code> and then   <code>Toothpaste roommate</code>.</li> <li>Do the filling in iterations:</li> <li>Add the common products: this can be done with the ticket of the last     groceries, or manually inspecting all the elements in your home.</li> <li>Incrementally add the recipes that you use</li> <li>Add the barcodes in the products that make sense.</li> <li>Add the <code>score</code> and <code>shop</code> userfields for the products, so you can evaluate   how much you like the product and where to buy it. If you show them in the   columns, you can also filter the shopping list by shop.</li> </ul>"}, {"location": "grocy_management/#minimum-quantities", "title": "Minimum quantities", "text": "<p>The minimum quantity defines when does the product is going to be added to the shopping list, it must be enough so we have time to go to the shop to buy more, so it has to follow:</p> <pre><code>minimum quantity = max_shop_frequency * average_consumption_rate * security_factor\n</code></pre> <p>Where: * <code>max_shop_frequency</code>: is the maximum number of days between I visit the   shop where I can obtain that product. If the product can be obtained in   several shops we'll take the smallest number of days. * <code>average_consumption_rate</code>: is the average number of units consumed per day.   It can be calculated by the following equation:</p> <pre><code>average_consumption_rate = total_units_consumed / days_since_first_unit_bought\n</code></pre> <p>The calculation could be improved giving more weight to the recent consumption   against the overall trend.</p> <ul> <li><code>security_factor</code>: Is an amount to add to take into account the imprecisions   on the measures. A starting <code>security_factor</code> could be 1.2.</li> </ul> <p>But we won't have most of the required data when we start from scratch, therefore I've followed the next criteria:</p> <ul> <li>If the product is critical, I want to always have at least a spare one, so the   minimum quantity will be 2.</li> <li>I roughly evaluate the relationship between the <code>average_consumption_rate</code> and   the <code>max_shop_frequency</code>.</li> </ul> <p>Also, I usually have a recipient for the critical products, so I mark the product as consumed once I transfer it from the original recipient to my recipient. Therefore I always have a security factor. This also helps to reduce the management time. For example, for the fruit, instead of marking as consumed each time I eat a piece, I mark them as consumed when I move them from the fridge to a recipient I've got in the living room.</p>"}, {"location": "grocy_management/#parent-products", "title": "Parent products", "text": "<p>Parent products let you group different kind of products under the same roof. The idea is to set the minimum quantity in the parent product and it will inherit all the quantities of it's children.</p> <p>I've used parent products for example to set a minimum amount of red tea, while storing the different red teas in different products.</p> <p>The advantage of this approach is that you have a detailed product page for each kind of product. This allows you to have different purchase - storage ratio, price evolution, set score and description for the different kinds, set different store...</p> <p>The disadvantage is that you have to add and maintain additional products.</p> <p>So if you expect that the difference between products is relevant split them, if you don't start with one product that aggregates all, like chocolate bar for all kinds of chocolate bars, and maybe in the future refactor it to a parent and child products.</p> <p>Another good use case is if the different brands of a product sell different sizes, so the conversion from buy unit to storage unit is different. Then I'll use a parent product that specifies the minimum and the different sub products with the different conversion rate.</p>"}, {"location": "grocy_management/#on-the-units", "title": "On the units", "text": "<p>I've been uncertain on what units use on some products.</p> <p>Imagine you buy a jar of peas, should you use jar or grams? or a bottle of wine should be in bottles or milliliters?</p> <p>The rule of thumb I've been using is:</p> <ul> <li>If the product is going to be used in a recipe, use whatever measure the   recipe is going to use. For example, grams for the peas.</li> <li>If not, use whatever will cost you less management time. For example,   milliliters for the wine (so I only have to update the inventory when the   bottle is gone).</li> </ul>"}, {"location": "grocy_management/#future-ideas", "title": "Future ideas", "text": "<p>I could monitor the ratio of rotting and when a product gets below the minimum stock to optimize the units to buy above the minimum quantity so as to minimize the shopping frequency. It can be saved in the <code>max_amount</code> user field.</p> <p>To calculate it's use I can use the average shelf life, last purchased and last used specified in the product information</p>"}, {"location": "grocy_management/#todo", "title": "TODO", "text": "<ul> <li>Define the userfields I've used</li> <li>Define the workflow for :</li> <li>initial upload</li> <li>purchase</li> <li>consumption</li> <li>cooking</li> <li>How to interact with humans that don't use the system but live in the same   space</li> </ul>"}, {"location": "grocy_management/#unclassified", "title": "Unclassified", "text": "<ul> <li>When creating a child product, copy the parent buy and stock units and     conversion, also the expiration till it's solved the child creation or     duplication (search issue)</li> <li>Use of pieza de fruta to monitor the amount instead of per product</li> <li>Caja de pa\u00f1uelos solo se cuentan los que est\u00e1n encima de la nevera</li> <li>La avena he apuntado lo que implica el rellenar el bote para consumir solo     cuando lo rellene.</li> <li>Locations are going to be used when you review the inventory so make sure you     don't have to walk far</li> <li> <p>Tare weight not supported with transfer</p> </li> <li> <p>it makes no sense to ask for the Location sheet to be editable, you've got the     stock overview for that. If you want to consume do so, if you want to add     you need to enter information one by one so you can't do it in a batch.</p> </li> <li>If you want only to check if an ingredient exist but don't want to consume it     select <code>Only check if a single unit is in stock (a different quantity can     then be used above)</code>.</li> <li>Marcar bayetas como abiertas para recordarte que tienes que cambiarla</li> <li>Common userfields should go together</li> <li>Acelga o lechuga dificil de medir por raciones o piezas, tirar de gramos</li> <li>Use stock units accordingly on how you consume them. 1 ration = \u00bd lemon, and   adjust the recipes accordingly.</li> <li>For example the acelgas are saved as pieces, lettuce, as two rations per   piece, spinach bought as kg and saved as rations</li> <li>Important to specify the location, as you'll use it later for the inventory   review</li> <li>IF you don't know the rations per kilogram, use kilograms till you know it.</li> <li>Buy unit the one you are going to encounter in the supermarket, both to input   in purchase and to see the evolution of price.</li> <li>In the shops only put the ones you want to buy to, even if in others the   product is available</li> <li>Things like the spices add them to recipes without consuming stock, and once   you see you are low on the spice consume the rations</li> <li>In the things that are so light that 0.01 means a lot, change the buying unit   to the equivalent x1000, even if you have to use other unit that is not the   buying unit (species case)</li> <li>When you don't still have the complete inventory and you are cooking with   someone, annotate in a paper the recipe or at least the elements it needs and   afterwards transfer them to grocy.</li> <li>Evaluate the use of sublocations in grocy, like Freezer:Drawer 1.</li> <li>For products that are in two places, (fregadero and stock), consume the stock     one instead of consuming the other and transfering the product.</li> <li>Adapt the due days of the fresh products that don't have it.</li> <li>If you hit enter in any field it commits the product (product description,     purchase)</li> </ul>"}, {"location": "grocy_management/#issues", "title": "Issues", "text": "<ul> <li>Standard consumption location:     Change it in the products that get consumed elsewhere.</li> <li>Allow stock modifications from the location content sheet     page: Nothing to do, start     using it. He closed them as duplicate of     1,     2 and     3.</li> </ul>"}, {"location": "grocy_management/#resources", "title": "Resources", "text": "<ul> <li>Homepage</li> </ul>"}, {"location": "gtd/", "title": "Getting Things Done", "text": ""}, {"location": "gtd/#icijetting-things-donehttpsenwikipediaorgwikigetting_things_done-commonly-known-as-gtd-is-a-task-management-method-created-by-david-allen-and-published-in-a-book-of-the-same-name", "title": "icij/etting things done](https://en.wikipedia.org/wiki/Getting_Things_Done), commonly known as GTD is a task management method created by David Allen and published in a book of the same name.", "text": "<p>The GTD method rests on the idea of moving all items of interest, relevant information, issues, tasks and projects out of one's mind by recording them externally and then breaking them into actionable work items with known time limits. This allows one's attention to focus on taking action on each task listed in an external record, instead of recalling them intuitively.</p> <p>The method was described by David Allen in a book with the same name. It's clear that the book is the corner stone of David's business. He is selling his method on every word, some times to the point of tiresome. It's also repeats the same ideas on different parts of the book, I guess that's good in terms of sticking an idea in the people's mind, but if you're already convinced and are trying to sum up the book it's like, hey, I have 90% of the valuable contents of this chapter already in my summary. It's obvious too the context of the writer, that the book was written a while ago and who does it write to. It talks quite often about assistants, bosses of high firm companies he's helped, preferring low-tech physical solutions over digital ones, a lot of references about parenting... If you're able to ignore all the above, it's actually a very good book. The guy has been polishing the method for more than 30 years, and has pretty nice ideas that can change how you manage your life.</p> <p>My idea of this summary is to try to extract the useful ideas removing all those old-fashioned capitalist values from it.</p>"}, {"location": "gtd/#theory-principles", "title": "Theory principles", "text": "<p>The theory is based on three key objectives: </p> <ul> <li>Capturing all the things that might need to get done or have usefulness for you in a logical and trusted system outside your head and off your mind.</li> <li>Directing yourself to make front-end decisions about all of the \u201cinputs\u201d you let into your life so that you will always have a workable inventory of \u201cnext actions\u201d that you can implement or renegotiate in the moment.</li> <li>Curating and coordinating all of that content, utilizing the recognition of the multiple levels of commitments with yourself and others you will have at play, at any point in time.</li> </ul> <p>The idea is to have a system with a coherent set of behaviors and tools that:</p> <ul> <li>Functions effectively at the level at which work really happens. </li> <li>Incorporates the results of big-picture thinking as well as the smallest of open details. </li> <li>Manage multiple tiers of priorities. </li> <li>Maintains control over hundreds of new inputs daily. </li> <li>Save a lot more time and effort than are needed to maintain it. </li> <li>Allows you to maximize your time \"in the zone\" or \"flowing\".</li> </ul>"}, {"location": "gtd/#managing-commitments", "title": "Managing commitments", "text": "<p>Most stress comes from inappropriately managed commitments, every one of them, big or little, is being tracked by a less-than-conscious part of you. These are the \u201cincompletes,\u201d or \u201copen loops,\u201d which are anything pulling at your attention that doesn\u2019t belong where it is, the way it is.</p> <p>In order to deal effectively with all of that, you must first identify and capture all those things that are \u201cringing your bell\u201d in some way, clarify what, exactly, they mean to you, and then make a decision about how to move on them.</p> <p>Managing commitments well requires the implementation of some basic activities and behaviors:</p> <ul> <li>If it\u2019s on your mind, your mind isn\u2019t clear. Anything you consider unfinished in any way must be captured in a trusted system outside your mind that you know you\u2019ll come back to regularly and sort through.</li> <li>You must clarify exactly what your commitment is and decide what you have to do, if anything, to make progress toward fulfilling it.</li> <li>Once you\u2019ve decided on all the actions you need to take, you must keep reminders of them organized in a system you review regularly.</li> </ul> <p>These \"open loops\" make you unreliably think many times on the topic without making any progress. This is a waste of time and energy and only adds to your anxiety about what you should be doing and aren't. There is no reason to ever have the same thought twice, unless you like having that thought.</p>"}, {"location": "gtd/#managing-stuff", "title": "Managing stuff", "text": "<p>\"stuff\" is anything you have allowed into your psychological or physical world that doesn\u2019t belong where it is, but for which you haven\u2019t yet determined what, exactly, it means to you, with the desired outcome and the next action step. The reason most organizing systems haven\u2019t worked for most people is that they haven\u2019t yet transformed all the stuff they\u2019re trying to organize. As long as it\u2019s still stuff, it\u2019s not controllable.</p> <p>Almost all of the to-do lists are merely listings of \"stuff\", not inventories of the resultant real work that needed to be done. They are partial reminders of a lot of things that are unresolved and as yet untranslated into outcomes and actions. Looking at these often creates more stress than relief, because, though it is a valuable trigger for something that you\u2019ve committed to do or decide something about, it still calls out psychologically, \u201cDecide about me!\u201d And if you do not have the energy or focus at the moment to think and decide, it will simply remind you that you are overwhelmed.</p> <p>The key to managing all of your stuff is managing your actions.</p> <p>What you do with your time, what you do with information, and what you do with your body and your focus relative to your priorities. Those are the real options to which you must allocate your limited resources. The substantive issue is how to make appropriate choices about what to do at any point in time. The real work is to manage our actions.</p>"}, {"location": "gtd/#managing-actions", "title": "Managing actions", "text": "<p>You need to control commitments, projects, and actions in two ways:</p> <ul> <li>Horizontally: Horizontal control maintains coherence across all the activities in which you are involved. You need a good system that can keep track of as many of them as possible, supply required information about them on demand, and allow you to shift your focus from one thing to the next quickly and easily.</li> <li>Vertically: Vertical control, in contrast, manages thinking, development, and coordination of individual topics and projects. This is \u201cproject planning\u201d in the broad sense. It\u2019s focusing in on a single endeavor, situation, or person and fleshing out whatever ideas, details, priorities, and sequences of events may be required for you to handle it, at least for the moment.</li> </ul>"}, {"location": "gtd/#bottom-up-approach", "title": "Bottom-Up approach", "text": "<p>Intellectually, the most appropriate way ought to be to work from the top down, first uncovering personal and organizational purpose and vision, then defining critical objectives, and finally focusing on the details of implementation. The trouble is, however, that most people are so embroiled in commitments on a day-to-day level that their ability to focus successfully on the larger horizon is seriously impaired. Consequently, a bottom-up approach is usually more effective.</p>"}, {"location": "gtd/#workflow-steps", "title": "Workflow steps", "text": "<ul> <li>Capture what has our attention.</li> <li>Clarify what each item means and what to do about it.</li> <li>Organize the results.</li> <li>Reflect on the options.</li> <li>Engage the chosen options.</li> </ul>"}, {"location": "gtd/#capture", "title": "Capture", "text": "<p>It\u2019s important to know what needs to be captured and how to do that most effectively so you can process it appropriately. In order for your mind to let go of the lower-level task of trying to hang on to everything, you have to know that you have truly captured everything that might represent something you have to do or at least decide about, and that at some point in the near future you will process and review all of it.</p> <p>In order to eliminate \u201choles in your bucket,\u201d you need to collect and gather placeholders for, or representations of, all the things you consider incomplete in your world: anything personal or professional, big or little, of urgent or minor importance, that you think ought to be different than it currently is and that you have any level of internal commitment to changing.</p> <p>To manage this inventory of open loops appropriately, you need to capture it into \u201ccontainers\u201d that hold items in abeyance until you have a few moments to decide what they are and what, if anything, you\u2019re going to do about them. Then you must empty these containers regularly to ensure that they remain viable capture tools.</p> <p>There are several types of tools, both low and high tech, that can be used to collect your open loops. The following can all serve as versions of an in-tray, capturing self-generated input as well as information from external sources:</p> <ul> <li>Physical in-tray</li> <li>Paper-based note-taking devices</li> <li>Digital/audio note-taking devices</li> <li>E-mail and text messaging</li> </ul> <p>You'll probably need to choose a variety of tools to fulfill the different inputs of your life, these should become part of your lifestyle. Keep them close by so no matter where you are you can collect a potentially valuable thought. You should have as many in-trays as you need and as few as you can get by with. If you have too many collection zones, however, you won\u2019t be able to process them easily or consistently.</p> <p>That's one of the key points of your capture tools, if you don\u2019t empty and process the stuff you\u2019ve collected, your tools aren\u2019t serving any function other than the storage of amorphous material. Emptying the contents does not mean that you have to finish what\u2019s there; it just means that you have to decide more specifically what it is and what should be done with it, and if it\u2019s still unfinished, organize it into your system. You must get it out of the container. </p> <p>In order to get your \u201cinbox\u201d to empty, however, an integrated life-management system must be in place. Too much stuff is left piled in in-trays (physical and digital) because of a lack of effective systems downstream.</p>"}, {"location": "gtd/#clarify-and-organize", "title": "Clarify and organize", "text": "<p>This is the component of input management that forms the basis for your personal organisation. It's basically thinking about the item and following the next diagram:</p> <p></p> <p>Remember to follow the next rules while processing the items:</p> <ul> <li>Process the top item first: that way you treat each element equally, so the \"least\" important ones are not left dangling forever in your inbox thus thwarting it's purpose. </li> <li>Process one item at a time.</li> <li>Never put anything back into \u201cin.\u201d</li> </ul> <p>For each element you need to ask yourself: \"What's the next action?\".</p> <p>If you don't see any then refile the element to:</p> <ul> <li> <p>Trash: Throw away, shred, or recycle anything that has no potential future action or reference value. If you leave this stuff mixed in with other categories, it will seriously undermine the system and your clarity in the environment.</p> </li> <li> <p>Someday/Maybe: Here goes the elements that don't have next actions now but may have them in the future. This is the \u201cparking lot\u201d for projects that would be impossible to move on at present but that you don\u2019t want to forget about entirely. You\u2019d like to be reminded of the possibility at regular intervals (probably on your weekly reviews). You can also think this as the backlog. Here also belongs other types of information that only needs to be reviewed when you have an urge to engage in a particular kind of activity such as lists like books to read, web sites to surf, trips to do, things to do, recipes to try... Only if you want to review them often to expand your options for creative exploration, otherwise they belong to the References.</p> </li> <li> <p>References are the rest of information that that requires no action but have intrinsic value as information such as your digital garden or technical books.</p> </li> </ul> <p>Reference systems generally take two forms: </p> <ul> <li> <p>Topic and area specific storage: </p> </li> <li> <p>General reference files: Stores ad hoc information that doesn\u2019t belong in some predesigned larger category. </p> </li> </ul> <p>If you can do something about the element, you need to think which is the next physical, visible activity that would be required to move the situation towards closure. It's tricky, something like \"set meeting\" won't do because it's not descriptive of physical behaviour. There is still stuff to decide how, when, with whom, if you don't do it now you won't empty your head and the uncertainty will create a psychological gap that will make you procrastinate, so define the next action now. \"Decide what to do about X\" doesn't work either, you may need to gather more information on the topic, but deciding doesn't take time. </p> <p>Once you have the next action, if it can be done in two minutes or less, do it when you first pick the item up. Even if it is not a high-priority one, do it now if you\u2019re ever going to do it at all. The rationale for the two-minute rule is that it\u2019s more or less the point where it starts taking longer to store and track an item than to deal with it the first time it\u2019s in your hands. Two minutes is just a guideline. If you have a long open window of time in which to process your in-tray, you can extend the cutoff for each item to five or ten minutes. If you\u2019ve got to get to the bottom of all your input rapidly, then you may want to shorten the time to one minute, or even thirty seconds, so you can get through everything a little faster. </p> <p>There\u2019s nothing you really need to track about your two-minute actions. Just do them. If, however, you take an action and don\u2019t finish the project with that one action, you\u2019ll need to clarify what\u2019s next on it, and manage that according to the same criteria. </p> <p>If the next action is going to take longer than two minutes, ask yourself, \u201cAm I the best person to be doing it?\u201d If not, hand it off to the appropriate person, in order of priority:</p> <ul> <li>Send an e-mail.</li> <li>Write a note or an over-note on paper and route it to that person.</li> <li>Send it a instant message.</li> <li>Add it as an agenda item on a list for your next real-time conversation with that person.</li> <li>Talk with her directly, either face-to-face or by phone.</li> </ul> <p>When you hand it off to someone else, and if you care at all whether something happens as a result, you\u2019ll need to track it. Depending on how active you need to be it can go to your Waiting list or to your tickler.</p> <p>You need to determine what to do with the rest of the next actions once you do, they will end up in one of the next containers:</p> <ul> <li>Todo list</li> <li>Calendar</li> <li>Tickler</li> </ul> <p>I's critical that all of these containers be kept distinct from one another. They each represent a discrete type of agreement we make with ourselves, to be reminded of at a specific time and in a specific way, and if they lose their edges and begin to blend, much of the value of organizing will be lost. That's why capturing and clarifying what your relationship to them is primary to getting organized.</p>"}, {"location": "gtd/#todo-list", "title": "Todo list", "text": "<p>This list contains all the next actions and projects you are going to actively work on. Projects are any desired result that can be accomplished within a year that requires more than one action step. This means that some rather small things you might not normally call projects are going to be on your Projects list, as well as some big ones. If one step won\u2019t complete something, some kind of goalpost needs to be set up to remind you that there\u2019s something still left to do. If you don\u2019t have a placeholder to remind you about it, it will slip back into your head. The reason for the one-year time frame is that anything you are committed to finish within that scope needs to be reviewed weekly to feel comfortable about its status. Another way to think of this is as a list of open loops, no matter what the size. This is going to be one of the lists that you'll review more often, and it needs to be manageable, if the items start to grow you may want to track the elements you want to do in the semester, or trimester.</p> <p>Projects do not initially need to be listed in any particular order, by size, or by priority. They just need to be on a master list so you can review them regularly enough to ensure that appropriate next actions have been defined for each of them. That being said, I like to order them a little bit so that I don't need to read the whole list to choose what to do.</p> <p>There may be reasons to sort your projects into different subcategories, based upon different areas of your focus, but initially creating a single list of all of them will make it easier to customize your system appropriately as you get more comfortable with its usage. To sort them use tags instead of hierarchical structures, they are more flexible. For example you could use tags for:</p> <ul> <li>Context: Where can you do the element: <code>home</code>, <code>computer</code>, <code>mobile</code>, ...</li> <li>Area: Broad categories where the element falls in: <code>activism</code>, <code>caring</code>, <code>self-caring</code>, <code>home</code>, <code>digital services</code>, ...</li> <li>Type: I like to separate the tasks that are meant to survive (<code>maintenance</code>) from the ones that are meant to improve things (<code>improvement</code>)</li> <li>Mood, energy level, time: It's useful to have a quick way to see the tasks you can work on when you don't have that much time (<code>small</code>), you don't have that much mental energy (<code>brainless</code>), when you're <code>sad</code>, ...</li> </ul> <p>For many of your projects, you will accumulate relevant information that you will want to organize by theme or topic or project name. Your Projects list will be merely an index. All of the details, plans, and supporting information that you may need as you work on your various projects should be contained in your References system.</p>"}, {"location": "gtd/#calendar", "title": "Calendar", "text": "<p>The calendar holds reminders of actions you need to take fall into two categories: those about things that have to happen on a specific day or time, and those about things that just need to get done as soon as possible. Your calendar handles the first type of reminder.</p> <p>These things go on your calendar:</p> <ul> <li>Time-Specific actions or appointments.</li> <li>Day-Specific actions: These are things that you need to do sometime on a certain day, but not necessarily at a specific time. </li> <li>Day-Specific information: Information that may be useful on a certain date. This might include directions for appointments, activities that other people will be involved in then, or events of interest. It\u2019s helpful to put short-term tickler information here, too, such as a reminder to call someone after he or she returns from vacation. This is also where you would want to park important reminders about when something might be due, or when something needs to be started, given a determined lead time.</li> </ul> <p>Daily to-do lists don't belong to the calendar because:</p> <ul> <li> <p>Constant new input and shifting tactical priorities reconfigure daily work so consistently that it\u2019s virtually impossible to nail down to-do items ahead of time. Having a working game plan as a reference point is always useful, but it must be able to be renegotiated at any moment. Trying to keep a list on the calendar, which must then be reentered on another day if items don\u2019t get done, is demoralizing and a waste of time. The Next Actions lists will hold all of those action reminders, even the most time-sensitive ones. And they won\u2019t have to be rewritten daily.</p> </li> <li> <p>If there\u2019s something on a daily to-do list that doesn\u2019t absolutely have to get done that day, it will dilute the emphasis on the things that truly do. The calendar should be sacred territory. If you write something there, it must get done that day or not at all. </p> </li> </ul> <p>That said, there\u2019s absolutely nothing wrong with creating a quick, informal, short list of \u201cif I have time, I\u2019d really like to\u00a0.\u00a0.\u00a0.\u201d kinds of things, picked from your Next Actions inventory. It just should not be confused with your \u201chave-tos,\u201d and it should be treated lightly enough to discard or change quickly as the inevitable surprises of the day unfold.</p>"}, {"location": "gtd/#reflect", "title": "Reflect", "text": "<p>This is where you take a look at all your outstanding projects and open loops. It\u2019s your chance to scan all the defined actions and options before you, thus radically increasing the efficacy of the choices you make about what you\u2019re doing at any point in time.</p> <p>The item you\u2019ll probably review most frequently is your calendar, which will remind you about what things truly have to be handled that day. This doesn\u2019t mean that the contents there are the most \u201cimportant\u201d in some grand sense, only that they must get done. At any point in time, knowing what has to get done and when creates a terrain for maneuvering. It\u2019s a good habit, as soon as you conclude an action on your calendar, to check and see what else remains to be done.</p> <p>Review whatever lists, overviews, and orientation maps you need to, as often as you need to, to get their contents off your mind.</p> <p>After checking your calendar, you\u2019ll most often turn to your Next Action lists. These hold the inventory of predefined actions that you can take if you have any discretionary time during the day. If you\u2019ve organized them by context (At Home; At Computer; In Meeting with George) they\u2019ll come into play only when those contexts are available.</p> <p>Projects, Waiting For, and Someday/Maybe lists need to be reviewed only as often as you think they have to be in order to stop you from wondering about them with a minimum recurrence of once each week.</p>"}, {"location": "gtd/#engage", "title": "Engage", "text": "<p>The basic purpose of this workflow management process is to facilitate good choices about what you\u2019re doing at any point in time and feel good about it. The idea is to use your intuition on the information you have captured, clarified, organized, and reflected regarding all your current commitments. </p> <p>First you need to get an awareness of your state. At any point in time you're limited to do the tasks that meet your:</p> <ul> <li>Context: Most actions need to be done on a specific location or having some tool at hand. You can use tags to filter the actions available to your context.</li> <li>Time available.</li> <li>Energy available.</li> </ul> <p>Then you need to decide what type of task you want to do:</p> <ul> <li>Predefined tasks: Dealing with tasks that you have previously determined need to be done, items of your calendar and Next actions list.</li> <li>Unplanned tasks: Often things come up ad hoc, unsuspected and unforeseen that you either have to or choose to engage in as they occur. When you follow these leads, you\u2019re deciding by default that these things are more important than anything else you have to do at those times.</li> <li>Managing the task system: This entails following the earlier steps of the workflow: capturing stuff, clarifying, organizing and reflecting. As you process your inputs, you\u2019ll no doubt be taking care of some less-than-two-minute actions and tossing and filing numerous things. A good portion of this activity will consist of identifying things that need to get done sometime, but not right away. You\u2019ll be adding to all of your lists as you go along.</li> </ul> <p>And finally decide which one to do based on your priority. Defining priority is a hard task that often involves looking towards the \"big picture\". The idea is to scale up the abstraction tree below to help you decide:</p> <ul> <li>Task level: This is the accumulated list of all the actions you need to take.</li> <li>Project level: Group of tasks that share a relatively short-term outcome you want to achieve.</li> <li>Area level: These are the key areas of your life within which you want to achieve results and maintain standards such as health, social, learning, home, recreation, finances, etc. These are not things to finish but rather to use as criteria for assessing our experiences and our engagements, to maintain balance and sustainability as we live. Listing and reviewing these responsibilities gives a more comprehensive framework for evaluating your inventory of projects.</li> <li>Goal level: What specific achievements you want to meet in the different areas of your life one to two years from now will add another dimension to prioritize your tasks. </li> <li>Vision level: Projecting three to five years into the future generates thinking about bigger categories: organization strategies, environmental trends, career and lifestyle transition circumstances. Internal factors include longer-term career, family, financial, and quality-of-life aspirations and considerations. </li> <li>Purpose and Principles: This is the big-picture view. The primary purpose for anything provides the core definition of what it really is. All goals, visions, objectives, projects, and actions derive from this, and lead toward it.</li> </ul> <p>In real life the important conversations you will have about your focus and your priorities may not exactly fit one level or another. They can provide a useful framework, however, to remind you of the multilayered nature of your commitments and tasks.</p> <p>Setting priorities in the traditional sense of focusing on your long-term goals and values, though obviously a necessary core focus, does not provide a practical framework for a vast majority of the decisions and tasks you must engage in day to day. Mastering the flow of your work at all the levels you experience that work provides a much more holistic way to get things done and feel good about it.</p>"}, {"location": "gtd/#setting-up-the-system", "title": "Setting up the system", "text": "<p>To apply the workflow you need to first set the system up, then you'll be able to use and maintain it. To be able to do it save block of time to initialize this process and prepare a workstation with the appropriate space, furniture, and tools. If your space is properly set up and streamlined, it can reduce your unconscious resistance to dealing with your stuff and even make it attractive for you to sit down and crank through your input and your work. An ideal time frame for most people is two whole days, back to back.</p>"}, {"location": "gtd/#setting-up-the-space", "title": "Setting up the space", "text": "<p>Choose a physical location to serve as as your central cockpit of control. If you already have a desk and office space set up where you work, that\u2019s probably the best place to start. If you work from a home office, obviously that will be your prime location. If you already have both, you\u2019ll want to establish identical, even interchangeable systems in both places, though one will probably be primary.</p> <p>The basics for a workspace are just a writing surface and room for an in-tray.</p>"}, {"location": "gtd/#design-your-system", "title": "Design your system", "text": "<p>Before you start moving stuff around it's a good idea to get the first design of your whole system, in my case I'm going to heavily rely on org-mode to track most of the stuff with a repository with the next structure:</p> <pre><code>.\n\u251c\u2500\u2500 calendar\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 personal.org\n\u2502   \u2502\u00a0\u00a0 \u251c\u2500\u2500 One time events\n\u2502   \u2502\u00a0\u00a0 \u251c\u2500\u2500 Recurring events\n\u2502   \u2502\u00a0\u00a0 \u251c\u2500\u2500 Birthdays\n\u2502   \u2502\u00a0\u00a0 \u2514\u2500\u2500 Deathdays\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 day.org\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 tickler.org\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 work_1.org\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 work_2.org\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 partner.org\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 family.org\n\u251c\u2500\u2500 inbox\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 computer.org\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 mobile.org\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 tablet.org\n\u251c\u2500\u2500 reference\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 blue\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 reference.org\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 health.org\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 red \n\u2514\u2500\u2500 todo\n    \u251c\u2500\u2500 personal.org\n    \u251c\u2500\u2500 work_1.org\n    \u251c\u2500\u2500 work_2.org\n    \u251c\u2500\u2500 recurrent.org\n    \u2514\u2500\u2500 someday.org\n</code></pre> <p>Where:</p> <ul> <li>The subtrees behind the <code>.org</code> files are the heading trees.</li> <li>All <code>org</code> files go with their respective <code>org_archive</code> ones, they're not shown in the above diagram to keep it simple.</li> <li><code>calendar/personal.org</code> is my personal calendar.</li> <li><code>calendar/day.org</code> is my day planner.</li> </ul>"}, {"location": "gtd/#inbox-management", "title": "Inbox management", "text": "<p>Inbox is the container where you capture your stuff. I've found myself capturing stuff in each of my devices: computer, mobile phone and tablet. Each of them has their own org file under the <code>inbox</code> directory. Each of these files has the <code>#+FILETAGS: :inbox:</code> heading so that all elements share the tag.</p> <p>Part of the daily planning is to check the computer and mobile inboxes to see if there is anything that needs to be processed on the day. I don't check the tablet inbox as there's usually no urgent stuff there. The rest of the elements will be processed on the weekly review leaving all the inbox files empty.</p>"}, {"location": "gtd/#computer-inbox-management", "title": "Computer inbox management", "text": "<p><code>nvim-orgmode</code> has an awesome feature called capture which lets you capture thoughts with a keystroke. This is awesome as no matter what are you doing inside <code>neovim</code> you can quickly record your thought, action or idea and keep on doing whatever you were doing. It's a very efficient way to record your stuff at the same time as you keep your focus. </p> <p>You can use the next capture template:</p> <pre><code>  org_capture_templates = {\n    i = {\n      description = \"Inbox\",\n      template = \"* TODO %?\\n %U\",\n      target = \"~/org/inbox/computer.org\",\n    },\n  }\n</code></pre>"}, {"location": "gtd/#mobile-and-tablet-inbox-management", "title": "Mobile and tablet inbox management", "text": "<p>To capture the content on the go you can use the orgzly and then sync them with your computer through syncthing.</p>"}, {"location": "gtd/#calendar-management", "title": "Calendar management", "text": "<p>You need to trust your calendar as sacred territory, reflecting the exact hard edges of your day's commitments, which should be noticeable at a glance while you're on the run.</p> <p>So for each element you encounter in the calendar ask yourself, does this element need to be done on this hard date? If the answer is no, then the calendar is not the correct place for the element to be. </p> <p>Using dates to order your tasks it's a waste of time, because there will always be a thousand of reasons why you can't do all the things you allocate to that day. As these not done issues start piling up, you'll start to get stressed with a lot of things that you were not able to do on the dates you deceived yourself you were supposed to do at and then you need to spend time defining a new date. Use next actions in your <code>todo</code> instead.</p>"}, {"location": "gtd/#personal-calendar", "title": "Personal calendar", "text": "<p>The <code>calendar/personal.org</code> file holds:</p> <ul> <li>Appointments: Meant to be used for elements of the org file that have a defined date to occur. You whether do it that date or not do it at all.  If you need to act on it use a <code>TODO</code> element, otherwise a headline is enough An example would be.</li> </ul> <pre><code>* TODO Meet with Marie\n&lt;2023-02-24 Fri&gt;\n\n* Internet's birthday\n&lt;2023-03-13 Mon&gt;\n</code></pre> <ul> <li>Recurring events: Events that not only happen on the given date, but again and again after a certain interval of N hours (h), days (d), weeks (w), months (m), or years (y). The following shows up in the agenda every Wednesday:</li> </ul> <pre><code>* TODO Go to pilates\n  &lt;2007-05-16 Wed 12:30 ++1w&gt;\n</code></pre> <ul> <li>Tickler events: Check the Tickler management section for more details.</li> </ul> <p>Each section has it's own tag: <code>:recurring:</code>, <code>:day:</code>, <code>:birthday:</code>, <code>:deathday:</code>, and the whole file has the <code>:event:</code> tag for easy filtering.</p> <p>In rare cases you may want to use the <code>DEADLINE</code> property if you want to be warned in the agenda some days before the date arrives or the <code>SCHEDULED</code> one in case you want to see in the agenda when you start working on the task. Again, don't waste time postponing these dates, if you do, you're using the system wrong.</p>"}, {"location": "gtd/#day-planner", "title": "Day planner", "text": "<p>Some of my day events are semi-regular, meaning that the recurrence options are not powerful enough. For example, I usually go to pilates on Tuesdays, but some weeks I go at 18:00 and others at 19:00. In the past I used a script that interacts with <code>ikhal</code> to create the elements of the day based on some questionary. The idea is to migrate the tool to create appointments in the day under the <code>diary.org</code> file using a <code>datetree</code> structure:</p> <pre><code>* 2010\n** 2010-12 December\n*** 2010-12-20 Tuesday\n**** TODO Go to pilates\n    &lt;2010-12-20 Tue 19:00-20:00&gt;\n</code></pre> <p>I also use this file to add any diary annotations for my life log. Once this issue is solved it will be really easy to add diary thoughts through the capture feature</p>"}, {"location": "gtd/#todo-files", "title": "Todo files", "text": "<p>The <code>todo</code> files are where you track the todo list, which holds your projects and their next steps to work on. The <code>todo/personal.org</code>, <code>todo/work_1.org</code> and <code>todo/work_2.org</code> files of the above schema will be divided into these level 1 headings:</p> <ul> <li><code>* Necessary</code>: These projects need to be dealt with immediately and finished as soon as possible</li> <li><code>* Desirable</code>: Here is where most of your elements will be, these are the ones that you think it's important to work on but there is no hard pressure.</li> <li><code>* Optional</code>: These are the projects that it would be nice to work on, but if you don't it's fine.</li> </ul> <p>Projects are any the second level headings with TODO keywords. To see the list of your projects just fold all the items in the <code>todo.org</code> file. </p> <p>Inside each section the elements are more less ordered by what I want to work on first. But all projects are actionable, so if I'm not in the mood to do the first ones, I tackle the rest. As such, I try not to spend too much time ordering them.</p> <p>I find useful to split the tasks between my life silos, so that I don't even have a chance to think of anything of <code>work_1</code> when I'm doing my <code>personal</code> stuff or <code>work_2</code> stuff.</p>"}, {"location": "gtd/#project-structure", "title": "Project structure", "text": "<p>Given the broad definition of what we consider a project and how they are usually cooked, the system that represents it must be equally flexible, quick to interact with and easy to evolve. </p> <p>Every project starts with the title:</p> <pre><code>* TODO Improve task management system\n</code></pre> <p>Optionally you can add a description</p> <pre><code>* TODO Improve task management system\n  Using Openprojects is uncomfortable, I need to find a better system.\n</code></pre> <p>You may have noticed that the description doesn't follow the rules we defined for next actions, that's fine as you don't act on projects, but on their underlying actions. Nevertheless I like to start them with a verb. It may even make sense not to use TODO items but simple headings to define your projects. On one side you don't act on projects so it would make sense to use headings, on the other, it's also interesting to know the project state, which can be easily tracked with the TODO keywords. If you could tell apart headings from TODO items in the agenda views it would make sense to use them. Right now <code>nvim-orgmode</code> let's you select in the agenda views only TODO items or TODO and headings, but you can't select only headings, so at the moment I don't see any good reason not to use TODO items for the projects.</p> <p>To define the next actions of a project you can use checklists</p> <pre><code>* TODO Improve task management system\n  - [-] Read David Allen's GTD book\n    - [x] Read chapters 6 and 7\n    - [ ] Read chapters 8 and 9\n  - [ ] Sum up the book in the blue book\n</code></pre> <p>As your checklists grow they may start to be uncomfortable, for example if it has:</p> <ul> <li>More than two levels of indentation: It may be hard to follow the logic of the task structure.</li> <li>A lot of elements: You can't archive parts of checklists, so as you complete elements, they will still be shown diverting your attention from the things you can actually act upon or making you loose time scrolling to find where they are.</li> </ul> <p>In these cases it makes sense to promote the first level of headings to subprojects:</p> <pre><code>* TODO Improve task management system\n  * DOING Read David Allen's GTD book\n    - [x] Read chapters 6 and 7\n    - [ ] Read chapters 8 and 9\n  * TODO up the book in the blue book\n</code></pre> <p>That way when <code>Read David Allen's GTD book</code> is done, you can archive it and forget about it. </p> <p>If the project starts having many subprojects, it may help to have a section \"Outcomes\" to define what do you want to achieve with the project. It can be accompanied with a \"Next Steps\" section to add any subproject or action that doesn't match the defined outcomes, and once you finish the project, you can refile them into new projects.</p>"}, {"location": "gtd/#the-next-state", "title": "The NEXT state", "text": "<p>It's useful to have a <code>NEXT</code> state to track the first next action you need to deal with for each project. That way when you open the file, you can go to the top of it and search for <code>NEXT</code> and it will lead you directly to where you need to work on.</p>"}, {"location": "gtd/#tag-management", "title": "Tag management", "text": "<p>As explained in the todo list section, you can use tags to filter your tasks. I'm using the next ones:</p> <ul> <li>Area: Broad categories where the element falls in: <code>activism</code>, <code>caring</code>, <code>self-caring</code>, <code>home</code>, <code>digital services</code>, ...</li> <li>Type: I like to separate the tasks that are meant to survive (<code>maintenance</code>) from the ones that are meant to improve things (<code>improvement</code>). I use these only in the big projects.</li> <li><code>:long_break:</code>: I'm using this tag to track the small projects that can be done in the long pomodoro breaks. Depending on the kind of long break that I need I then filter for the next tags:</li> <li><code>brainless</code>: If I want to keep on thinking on what I was doing, an example could be emptying the dishwasher, watering the plants, ...</li> <li><code>call</code>: If I want to completely change context and want some social interaction. For example call mom. </li> <li><code>:thinking:</code>: Used to track the elements where you just need to think about them. For example I like to have this list to have a prioritized list to deal with when I'm in the shower, while biking, hiking...</li> <li><code>:relax:</code>: Used to track the things you can do when you just want to chill: really listen the music of a group, summarize a book, clean your music library...</li> <li>People involved: <code>:marie:</code>, <code>:billy:</code>, ...</li> </ul> <p>Always use lowercase tags, it will save you some key strokes.</p>"}, {"location": "gtd/#priority-management", "title": "Priority management", "text": "<p>You shouldn\u2019t bother to create some external structuring of the priorities on your lists that you\u2019ll then have to rearrange or rewrite as things change. Attempting to impose such scaffolding has been a big source of frustration in many people\u2019s organizing. You\u2019ll be prioritizing more intuitively as you see the whole list against quite a number of shifting variables. The list is just a way for you to keep track of the total inventory of active things to which you have made a commitment, and to have that inventory available for review.</p> <p>Therefore I'm going to try not to use orgmode's priorities for the tasks.</p>"}, {"location": "gtd/#waiting-tasks", "title": "Waiting tasks", "text": "<p>Waiting actions are elements that are blocked for any reason. I use the <code>WAITING</code> TODO keyword to track this state. Under each element you should add that reason and optionally the process you want to follow to unblock it.</p> <p>If you need to actively track the evolution of the WAITING status, leave it on the top of your <code>todo</code>. Otherwise set the date you want to check its status and move it to the <code>ticker.org</code> file. If you don't even want to track it, move it to the <code>someday.org</code> file.</p>"}, {"location": "gtd/#tickler-management", "title": "Tickler management", "text": "<p>The tickler is a system where you postpone actions to a specific date, but not with a calendar mindset where the action needs to be done at that date. With the tickler you schedule the action to enter your inbox that day to decide what are you going to do with it.</p> <p>To implement this in orgmode you can add the <code>:tickler:</code> tag to any element that is tracked in the agenda files and once a day you can look at the day's agenda and decide what to do with the action. It's important though that whatever you do with it, you have to remove it from the agenda view in order to only keep the elements that you need to do in the day. You can follow this workflow by:</p> <ul> <li>Opening the agenda view <code>gaa</code> </li> <li>Go to the view of the day <code>vd</code></li> <li>Go to today <code>.</code></li> <li>Search by tickler <code>/tickler</code></li> </ul> <p>It can also help to review in the weeklies all the ticklers of the week to avoid surprises.</p> <p>If you want to make the project go away from your <code>todo</code> or <code>someday</code> until the tickler date, move it to the <code>tickler.org</code> file.</p>"}, {"location": "gtd/#soft-recurrent-tasks", "title": "Soft recurrent tasks", "text": "<p>There are some tasks that have a soft recurrence meaning that once you do them you don't want them to show up in your list of actions until a specific time has passed. You could use a recurrent <code>DEADLINE</code> or <code>SCHEDULED</code> but as we've seen earlier that will clutter your calendar pretty soon. Try following the next workflow with these tasks:</p> <ul> <li>Add the <code>:soft_recurrence:</code> tag to keep them tracked.</li> <li>Add them to the tickler file with a recurrent appointment date <code>&lt;2023-03-13 Mon ++1w&gt;</code> and the <code>:tickler:</code> tag so that it doesn't show up in the agenda view even if you move it to another file.</li> <li>When the appointed day comes you'll review the tickler elements as part of your day's routine. If you think it's time to do it, refile it to the <code>todo.org</code> file, if not, adjust the recurrence period and set the next date. Even though this workflow is reproducing the \"kick the can forward\" that we want to avoid, the idea is that once you get the period right you'll never have to do it again. If you see that after some iterations the period keeps on changing, maybe this workflow is not working for that kind of task and you may need to think of a better system <code>\u00af\\(\u00b0_o)/\u00af</code>.</li> <li>Once you complete the item, the new one will be spawned, once it has refile it to the tickler file again.</li> </ul> <p>We use appointments instead of <code>DEADLINE</code> or <code>SCHEDULED</code> so that they don't clutter the tickler view if you don't do them on the appointment date. </p> <p>Another option is not to archive the DONE tasks and in the weekly reset them to TODO the ones that you want to do the next week.</p>"}, {"location": "gtd/#setting-up-your-filing-system", "title": "Setting up your filing system", "text": "<p>You will resist the whole process of capturing information if your reference systems are not fast, functional, and fun. Thus you need to envision a system at hand that supports both physical and digital content. Without a streamlined system for both, you will resist keeping potentially valuable information, or what you do keep will accumulate in inappropriate places. </p> <p>We\u2019re concerned here mostly with general-reference filing, anything that you want to keep for its interesting or useful data or purpose and that doesn\u2019t fit into your specialized filing systems and won\u2019t stand up by itself on a shelf. For example articles, brochures, pieces of paper, notes, printouts, documents, and even physical things like tickets, keys, buyers-club membership cards, and flash drives.</p> <p>It should take you less than one minute to pick something up out of your in-tray, decide it needs no next action but has some potential future value, and finish storing it in a trusted system. If it takes longer, you\u2019ll likely stack it or stuff it somewhere instead. Besides being fast, the system needs to be fun and easy, current and complete. Otherwise you\u2019ll unconsciously resist emptying your in-tray because you know there\u2019s likely to be something in there that ought to get filed, and you won\u2019t even want to look at the papers or your clogged e-mail. If you have to get up every time you have some ad hoc piece of paper you want to file, or you have to search multiple places on your computer for an appropriate location for a piece of information you want to keep, you\u2019ll tend to stack it or leave it in its original place instead of filing it.</p> <p>You must feel equally comfortable about filing a single piece of paper on a new topic, even a scribbled note, in its own file as you would about filing a more formal, larger document.</p> <p>Whatever you need to do to get your reference system to that quick and easy standard for everything it has to hold, do it. For example purge your files at least once a year, that keeps it from going stale and becoming the dreaded black hole- .</p>"}, {"location": "gtd/#digital-general-reference", "title": "Digital general reference", "text": "<p>It is very helpful to have a visual map sorted in ways that make sense, either by indexes or data groups organized effectively, usually in an alphabetic order.</p> <p>The biggest issue for digitally oriented people is that the ease of capturing and storing has generated a write-only syndrome: all they\u2019re doing is capturing information\u2014not actually accessing and using it intelligently. Some consciousness needs to be applied to keep one\u2019s potentially huge digital library functional, versus a black hole of data easily dumped in there with a couple of keystrokes.</p> <p>You need to consistently check how much room to give yourself so that the content remains meaningfully and easily accessible, without creating a black hole of an inordinate amount of information amorphously organized. </p>"}, {"location": "gtd/#physical-general-reference", "title": "Physical general reference", "text": "<p>One idea is to have one system/place where you order the content alphabetically, not multiple ones. People have a tendency to want to use their files as a personal management system, and therefore they attempt to organize them in groupings by projects or areas of focus. This magnifies geometrically the number of places something isn\u2019t when you forget where you filed it.</p>"}, {"location": "gtd/#capture-all-your-stuff", "title": "Capture all your stuff", "text": "<p>The focus of this process is to capture everything that has your attention, otherwise some part of you will still not totally trust that you're working with the whole picture. While you're doing it, create a list of all the sources of inputs in your world. </p> <p>What you're going to do is methodically go through each piece of your life and search for anything that doesn\u2019t permanently belong where it is, the way it is, and put it into your in-tray. You\u2019ll be gathering things that are incomplete or things that have some decision about potential action tied to them. They all go into your \u201cinbox\u201d, so they\u2019ll be available for later processing. If it's immediately evident that you don't need the stuff trash it.</p> <p>Be patient, this process may take between 1 and 6 hours, and at the end you'll have a huge pile of stuff in your inbox. You might be scared and get the thought of \"what am I doing with my life?\", but don't worry you'll get everything in order soon :).</p>"}, {"location": "gtd/#define-what-is-going-to-be-your-in-tray", "title": "Define what is going to be your in-tray", "text": "<p>To be able to store all the \"stuff\" that needs to be dealt with you need to define what is your in-tray going to be, some suggestions are:</p> <ul> <li>A clear room: Make some space somewhere at the location you're going to do the gathering to pile physical stuff</li> <li>A physical or digital list.</li> </ul>"}, {"location": "gtd/#physical-gathering", "title": "Physical gathering", "text": "<p>The first activity is to search your physical environment. The best way to create a clean decision about whether something should go into the in-tray is to understand clearly what shouldn\u2019t go in. Here are the four categories of things that can remain where they are, the way they are, with no action tied to them:</p> <ul> <li>Supplies: Anything you need to keep because you use it regularly. </li> <li>Reference Material: Anything you simply keep for information as needed. This category includes manuals, all your telephone and address information, any material relevant to projects, themes, and topics. It also includes books and magazines that you may be keeping as a library.</li> <li>Decoration.</li> <li>Equipment: Your phone, computer, printer, wastebasket, furniture, clock, chargers, pens, and notepads.</li> </ul> <p>Everything else goes into your inbox. But many of the things you might initially interpret as supplies, reference, decoration, or equipment could also have action associated with them because they still aren\u2019t exactly the way they need to be.</p> <p>As you engage in the capturing step, you may run into one or more of the following problems:</p> <ul> <li>An item is too big to go in the in-tray: create a post it that represents it or add it as an entry in your digital inbox. If you can, add the date too</li> <li>The pile is too big to fit the in-tray: Create visually distinct stacks around the in-tray, even on the floor.</li> <li>Doubts whether to trash something: When in doubt keep it, you'll decide about it later when you process the in-tray. What you need to avoid is to get caught up in deciding what to do with the element. That's going to be the next step in the process, let's go one at a time.</li> <li>Getting caught up in cleaning and organizing: If it doesn't take that long it's fine but remember the purpose of this process and the fact that we want to finish it as soon as possible. If you discover things you want to change, add them to the in-tray.</li> <li>If you encounter stuff that is already on lists and organizers, treat them as everything else in the \"in\".</li> </ul> <p>Now that the process it's clear let's start.</p> <p>Start with the space where you actually do stuff, scan the place centimeter by centimeter with the mindset defined above, check your desk, drawers, floors, walls, shelves, equipment, furniture, fixtures...Then repeat the process with each room of your home.</p>"}, {"location": "gtd/#mental-gathering", "title": "Mental gathering", "text": "<p>Once you already have a nice pile of stuff, think of what has your attention that isn\u2019t represented by something already in your in-tray and record each thought, each idea, each project or thing that occurs you and add it to the inbox.</p> <p>To assist in clearing your head, you may want to review the following the next trigger list, item by item, to see if you\u2019ve forgotten anything. </p>"}, {"location": "gtd/#personal", "title": "Personal", "text": "<ul> <li>Projects started, not completed</li> <li>Projects that need to be started</li> <li>Projects\u2014other organizations</li> <li>Activism</li> <li>Community</li> <li>Volunteer</li> <li>Spiritual organization</li> <li>Commitments/promises to others</li> <li>Friends</li> <li>Partner</li> <li>Family</li> <li>Parents</li> <li>Children</li> <li>Professionals</li> <li>Returnable items</li> <li>Debts</li> <li>Communications to make/get</li> <li>Calls</li> <li>Instant messages</li> <li>Voice messages</li> <li>E-mails</li> <li>Cards and letters</li> <li>Thank-yous</li> <li>Texts</li> <li>Social media postings</li> <li>Upcoming events</li> <li>Birthdays</li> <li>Anniversaries</li> <li>Holidays</li> <li>Vacation</li> <li>Travel</li> <li>Dinners</li> <li>Parties</li> <li>Cultural events</li> <li>Sporting events</li> <li>Weddings</li> <li>Graduations</li> <li>Receptions</li> <li>Outings</li> <li>Administration</li> <li>Home office supplies</li> <li>Equipment</li> <li>Phones</li> <li>Mobile devices</li> <li>Audio/video media</li> <li>Voice mail</li> <li>Computers</li> <li>Software</li> <li>Internet</li> <li>Filing and records</li> <li>Data storage/backup</li> <li>Health</li> <li>Public health system</li> <li>Doctors</li> <li>Dentist</li> <li>Optometrist</li> <li>Healthcare specialists</li> <li>Checkups</li> <li>Diet</li> <li>Food</li> <li>Exercise</li> <li>Leisure</li> <li>Books</li> <li>Music</li> <li>Video</li> <li>Movies</li> <li>TV shows</li> <li>Hiking routes</li> <li>Travel</li> <li>Places to visit</li> <li>People to visit</li> <li>Web browsing</li> <li>Photography</li> <li>Sports equipment</li> <li>Hobbies</li> <li>Cooking</li> <li>Recreation</li> <li>Errands</li> <li>Shopping</li> <li>Stores</li> <li>Hardware</li> <li>Supplies</li> <li>Groceries</li> <li>Gifts</li> <li>Pharmacy</li> <li>Bank</li> <li>Cleaners</li> <li>Repairs</li> <li>Financial</li> <li>Worries</li> <li>Taxes</li> <li>Bills</li> <li>Banks</li> <li>Loans</li> <li>Budget</li> <li>Insurance</li> <li>Mortgage</li> <li>Bookkeeping</li> <li>Investments</li> <li>Accountants</li> <li>Pets</li> <li>Health</li> <li>Supplies</li> <li>Training</li> <li>Legal</li> <li>Wills</li> <li>Trusts</li> <li>Estate</li> <li>Legal affairs</li> <li>Friend/Family projects/activities</li> <li>Friends</li> <li>Partner</li> <li>Parents</li> <li>Relatives</li> <li>Children</li> <li>Home/household</li> <li>Worries</li> <li>Rent</li> <li>Real estate</li> <li>Repairs</li> <li>Construction</li> <li>Remodeling</li> <li>Landlords</li> <li>Heating and air conditioning</li> <li>Plumbing</li> <li>Utilities</li> <li>Roof</li> <li>Landscaping</li> <li>Driveway</li> <li>Garage</li> <li>Walls</li> <li>Floors</li> <li>Ceilings</li> <li>Decor</li> <li>Furniture</li> <li>Appliances</li> <li>Lights and wiring</li> <li>Kitchen supplies/equipment</li> <li>Laundry</li> <li>Purging, organizing, cleaning</li> <li>Storage</li> <li>Service providers</li> <li>Personal development</li> <li>Classes</li> <li>Seminars</li> <li>Education</li> <li>Coaching/counseling</li> <li>Career</li> <li>Creative expressions</li> <li>Transportation</li> <li>Bicycles</li> <li>Maintenance</li> <li>Repair</li> <li>Commuting</li> <li>Motor vehicles</li> <li>Clothes</li> <li>Casual</li> <li>Formal</li> <li>Sports</li> <li>Accessories</li> <li>Luggage</li> <li>Repairs</li> <li>Tailoring</li> <li>Professional</li> <li>Community</li> <li>Activism</li> <li>Neighborhood</li> <li>Neighbors</li> <li>Voting</li> <li>Waiting for</li> <li>Product orders</li> <li>Repairs</li> <li>Reimbursements</li> <li>Loaned items</li> <li>Information</li> <li>Projects/tasks completed by family/friends</li> </ul>"}, {"location": "gtd/#professional", "title": "Professional", "text": "<ul> <li>Projects started, not completed</li> <li>Projects that need to be started</li> <li>\u201cLook into\u00a0.\u00a0.\u00a0.\u201d projects</li> <li>Commitments/promises to others</li> <li>Colleagues</li> <li>Boss/partners</li> <li>Others in organization</li> <li>\u201cOutside\u201d people<ul> <li>Customers</li> <li>Other organizations</li> <li>Professionals</li> <li>Vendors</li> </ul> </li> <li>Communications to make/get</li> <li>Internal/external</li> <li>Initiate or respond to:<ul> <li>Phone calls</li> <li>Voice notes</li> <li>E-mails</li> <li>Text messages</li> <li>Letters</li> <li>Social media postings</li> </ul> </li> <li>Other writing to finish/submit</li> <li>Reports</li> <li>Evaluations/reviews</li> <li>Proposals</li> <li>Articles</li> <li>Manuals/instructions</li> <li>Summaries</li> <li>Rewrites and edits</li> <li>Status reporting</li> <li>Conversation and communication tracking</li> <li>Meetings that need to be set/requested</li> <li>Who needs to know about what decisions?</li> <li>Significant read/review</li> <li>Professional development</li> <li>Training/seminars</li> <li>Things to learn</li> <li>Things to find out</li> <li>Skills to practice/develop</li> <li>Books to read/study</li> <li>Research</li> <li>Formal education (licensing, degrees)</li> <li>Career research</li> <li>R\u00e9sum\u00e9</li> <li>Performance objectives</li> <li>Financial</li> <li>Forecasts/projections</li> <li>Credit line</li> <li>Stability</li> <li>Planning/organizing</li> <li>Formal planning (goals, targets, objectives)</li> <li>Current projects (next stages)</li> <li>Organizational initiatives</li> <li>Upcoming events</li> <li>Meetings</li> <li>Presentations</li> <li>Conferences</li> <li>Organizational structuring</li> <li>Changes in facilities</li> <li>Installation of new systems/equipment</li> <li>Travel</li> <li>Vacation</li> <li>Business trips</li> <li>Waiting for\u00a0.\u00a0.\u00a0.</li> <li>Information</li> <li>Delegated tasks/projects</li> <li>Completions critical to projects</li> <li>Answers to questions</li> <li>Replies to:<ul> <li>E-mails</li> <li>Letters</li> <li>Proposals</li> <li>Calls</li> <li>Invitations</li> </ul> </li> <li>Requisitions</li> <li>Reimbursements</li> <li>Insurance claims</li> <li>Ordered items</li> <li>Repairs</li> <li>Tickets</li> <li>Decisions of others</li> <li>Organization Development</li> <li>Organization chart</li> <li>Restructuring</li> <li>Roles</li> <li>Job descriptions</li> <li>Facilities</li> <li>New systems</li> <li>Leadership</li> <li>Change initiatives</li> <li>Succession planning</li> <li>Organization culture</li> <li>Administration</li> <li>Legal</li> <li>Insurance</li> <li>Personnel</li> <li>Staffing</li> <li>Policies/procedures</li> <li>Training</li> <li>Staff</li> <li>Hiring/firing/promoting</li> <li>Reviews</li> <li>Communication</li> <li>Staff development</li> <li>Compensation</li> <li>Feedback</li> <li>Morale</li> <li>Systems</li> <li>Mobile devices</li> <li>Phones</li> <li>Computers</li> <li>Software</li> <li>Databases</li> <li>Telecommunications</li> <li>Internet</li> <li>Filing and reference</li> <li>Inventories</li> <li>Storage</li> <li>Office/site</li> <li>Space/arrangements</li> <li>Furniture</li> <li>Equipment</li> <li>Decorations</li> <li>Utilities</li> <li>Supplies</li> <li>Maintenance/cleaning</li> <li>Security</li> </ul> <p>Now that you know where do your inputs come from you need to think how do you want to manage them from now on to ensure that you're able to be able to continuously capture items in a frictionless way.</p>"}, {"location": "gtd/#empty-the-inbox", "title": "Empty the inbox", "text": "<p>Now that we have collected everything that has your attention, you need to get to the bottom of your inbox. To be able to do it in a reasonable amount of time you are not meant to actually do the items themselves, instead analyze each item and decide what it is, what it means and what are you going to do with it.</p> <p>Follow the steps of Clarify and organize until you've reached the bottom of your inbox. You\u2019ll dump a mess of things, file a bunch, do a lot of two-minute actions, and hand off a number of items to other people. You\u2019ll also wind up with a stack of items that have actions associated with them that you still need to do soon, someday, or on a specific date, and reminders of things you\u2019re waiting on from other people. Now that you have everything at the next action level, we need to scale up in the abstraction ladder so that we can prioritize better what to do next.</p>"}, {"location": "gtd/#unclassified-thoughts", "title": "Unclassified thoughts", "text": "<ul> <li>There must be zero resistance to using the systems we have. Having to continually reinvent our in-tray, our filing system, and how and where we process our stuff can only be a source of incessant distraction.</li> <li>One of the best tricks for enhancing your productivity is having organizing tools you love to use.</li> <li>Being organized means nothing more or less than where something is matches what it means to you.</li> <li>Your organisation system is not something that you'll create all at once. It will evolve as you process yuor stuff and test out whether you have put everything in the best place for you. It won't remain static, it will evolve as you do.</li> </ul>"}, {"location": "gtd/#the-weekly-review", "title": "The weekly review", "text": "<p>The Weekly Review is the time to:</p> <ul> <li>Gather and process all your stuff.</li> <li>Review your system.</li> <li>Update your lists.</li> <li>Get clean, clear, current, and complete.</li> </ul>"}, {"location": "hard_drive_health/", "title": "Hard Drive Health", "text": "<p>Hard drives die, so we must be ready for that to happen. There are several solutions, such as using RAID to minimize the impact of a disk loss, but even then, we should monitor the bad sectors to see when are our disks dying.</p> <p>S.M.A.R.T (Self-Monitoring, Analysis and Reporting Technology; often written as SMART) is a monitoring system included in computer hard disk drives (HDDs), solid-state drives (SSDs), and eMMC drives. Its primary function is to detect and report various indicators of drive reliability with the intent of anticipating imminent hardware failures.</p> <p>Between all the SMART attributes, some that define define the health status of the hard drive, such as:</p> <ul> <li>Reallocated Sectors Count:  Count of reallocated sectors. The raw value   represents a count of the bad sectors that have been found and remapped.   Thus, the higher the attribute value, the more sectors the drive has had to   reallocate. This value is primarily used as a metric of the life expectancy of   the drive; a drive which has had any reallocations at all is significantly   more likely to fail in the immediate months.</li> <li>Spin Retry Count: Count of retry of spin start attempts. This attribute   stores a total count of the spin start attempts to reach the fully operational   speed (under the condition that the first attempt was unsuccessful). An   increase of this attribute value is a sign of problems in the hard disk   mechanical subsystem.</li> <li>Reallocate Event Count: Count of remap operations. The raw value of this   attribute shows the total count of attempts to transfer data from reallocated   sectors to a spare area. Both successful and unsuccessful attempts are   counted.</li> <li> <p>Current Pending Sector Count: Count of \"unstable\" sectors (waiting to be     remapped, because of unrecoverable read errors). If an unstable sector is     subsequently read successfully, the sector is remapped and this value is     decreased. Read errors on a sector will not remap the sector immediately     (since the correct value cannot be read and so the value to remap is not     known, and also it might become readable later); instead, the drive     firmware remembers that the sector needs to be remapped, and will remap it     the next time it's written.</p> <p>However, some drives will not immediately remap such sectors when written; instead the drive will first attempt to write to the problem sector and if the write operation is successful then the sector will be marked good (in this case, the \"Reallocation Event Count\" (0xC4) will not be increased). This is a serious shortcoming, for if such a drive contains marginal sectors that consistently fail only after some time has passed following a successful write operation, then the drive will never remap these problem sectors. * Offline Uncorrectable Sector Count: The total count of uncorrectable errors   when reading/writing a sector. A rise in the value of this attribute indicates   defects of the disk surface and/or problems in the mechanical subsystem.</p> </li> </ul>"}, {"location": "hard_drive_health/#check-the-warranty-status", "title": "Check the warranty status", "text": "<p>If your drive is still under warranty from the manufacturer you may consider RMA\u2019ing the drive (initiating a warranty return process).</p> <ul> <li>Seagate Warranty Check</li> <li>Western Digital (WD) Warranty Check</li> <li>HGST Warranty Check</li> <li>Toshiba Warranty Check</li> </ul>"}, {"location": "hard_drive_health/#wipe-all-the-disk", "title": "Wipe all the disk", "text": "<p>Sometimes the <code>CurrentPendingSector</code> doesn't get reallocated, if you don't mind about the data in the disk, you can wipe it all with:</p> <pre><code>dd if=/dev/zero of=/dev/{{ disk_id }} bs=4096 status=progress\n</code></pre>"}, {"location": "hard_drive_health/#troubleshooting", "title": "Troubleshooting", "text": ""}, {"location": "hard_drive_health/#smart-error-currentpendingsector-detected-on-host", "title": "SMART error (CurrentPendingSector) detected on host", "text": "<p>As stated above, this means that at some point, the drive was unable to successfully read the data from X different sectors, and hence have flagged them for possible reallocation. The sector will be marked as reallocated if a subsequent write fails. If the write succeeds, it is removed from current pending sectors and assumed to be OK.</p> <p>Start with a long self test with <code>smartctl</code>. Assuming the disk to test is <code>/dev/sdd</code>:</p> <pre><code>smartctl -t long /dev/sdd\n</code></pre> <p>The command will respond with an estimate of how long it thinks the test will take to complete.  (But this assumes that no errors will be found!)</p> <p>To check progress use:</p> <pre><code>smartctl -A /dev/sdd | grep remaining\n# or\nsmartctl -c /dev/sdd | grep remaining\n</code></pre> <p>Don't check too often because it can abort the test with some drives. If you receive an empty output, examine the reported status with:</p> <pre><code>smartctl -l selftest /dev/sdd\n</code></pre> <p>You will see something like this:</p> <pre><code>=== START OF READ SMART DATA SECTION ===\nSMART Self-test log structure revision number 1\nNum  Test_Description    Status                  Remaining  LifeTime(hours)  LBA_of_first_error\n# 1  Extended offline    Completed: read failure       20%      1596         44724966\n</code></pre> <p>So take that 'LBA' of 44724966 and multiply by (512/4096) which is the equivalent of 'divide by 8'</p> <pre><code>44724966 / 8 = 5590620.75\n</code></pre> <p>The sector to test then is <code>5590620</code>. If it is in the middle of a file, overwritting it will corrupt the file. If you are not cool with that, check the following posts to check if that sector belongs to a file:</p> <ul> <li>Smartmontools and fixing Unreadable Disk     Sectors.</li> <li>Smartmontools Bad Block how to</li> <li>Archlinux Identify damaged files page</li> <li>Archlinux badblocks page</li> </ul> <p>If you don't care to corrupt the file, use the following command to 'zero-out' the sector:</p> <pre><code>dd if=/dev/zero of=/dev/sda conv=sync bs=4096 count=1 seek=5590620\n1+0 records in\n1+0 records out\n\nsync\n</code></pre> <p>Now retry the <code>smartctl -t short</code> (or <code>smartctl -t long</code> if <code>short</code> fails) and see if the test is able to finish the test without errors:</p> <pre><code>=== START OF READ SMART DATA SECTION ===\nSMART Self-test log structure revision number 1\nNum  Test_Description    Status                  Remaining  LifeTime(hours)  LBA_of_first_error\n# 1  Short offline       Completed without error       00%     11699         -\n# 2  Extended offline    Completed: read failure       90%     11680         65344288\n# 3  Extended offline    Completed: read failure       90%     11675         65344288\n</code></pre> <p>If reading errors remain, repeat the steps above until they don't or skip to the bad block analysis step.</p> <p><code>Current_Pending_Sector</code> should be <code>0</code> now and the drive will probably be fine. As long as <code>Reallocated_Sector_Ct</code> is zero, you should be fine. Even a few reallocated sectors seems OK, but if that count starts to increment frequently, then that is a danger sign. To regularly keep a close eye on the counters use <code>smartd</code> to schedule daily tests.</p> <p>If <code>Current_Pending_Sector</code> is still not <code>0</code>, we need to do a deeper analysis on the bad blocks.</p>"}, {"location": "hard_drive_health/#bad-block-analysis", "title": "Bad block analysis", "text": "<p>The SMART <code>long</code> test gives no guarantee to find every error. To find them, we're going to use the <code>badblocks</code> tool instead.</p> <p>There is read-only mode (default) which is the least accurate. There is the destructive write-mode (-w option) which is the most accurate but takes longer and will (obviously) destroy all data on the drive, thus making it quite useless for matching sectors up to files. There is finally the non-destructive read-write mode which is probably as accurate as the destructive mode, with the only real downside that it is probably the slowest. However, if a drive is known to be failing then read-only mode is probably still the safest.</p>"}, {"location": "hard_drive_health/#links", "title": "Links", "text": "<ul> <li>S.M.A.R.T Wikipedia article.</li> <li>linux-hardware SMART disk probes.</li> </ul>"}, {"location": "hard_drive_health/#bad-blocks", "title": "Bad blocks", "text": "<ul> <li>Smartmontools and fixing Unreadable Disk     Sectors.</li> <li>Smartmontools Bad Block how to</li> <li>Archlinux Identify damaged files page</li> <li>Archlinux badblocks page</li> <li>Hard drive geek guide on reducing the current pending sector     count.</li> <li>Hiddencode guide on how to check bad sectors</li> <li>Hiddencode guide on how to fix bad sectors</li> </ul>"}, {"location": "helm_git/", "title": "helm-git", "text": "<p>helm-git is a helm downloader plugin that provides GIT protocol support.</p> <p>This fits the following use cases:</p> <ul> <li>Need to keep charts private.</li> <li>Doesn't want to package charts before installing.</li> <li>Charts in a sub-path, or with another ref than master.</li> <li>Pull values files directly from (private) Git repository.</li> </ul>"}, {"location": "helm_git/#installation", "title": "Installation", "text": "<pre><code>helm plugin install https://github.com/aslafy-z/helm-git --version 0.11.1\n</code></pre>"}, {"location": "helm_git/#usage", "title": "Usage", "text": "<p><code>helm-git</code> will package any chart that is not so you can directly reference paths to original charts.</p> <p>Here's the Git urls format, followed by examples:</p> <pre><code>git+https://[provider.com]/[user]/[repo]@[path/to/charts][?[ref=git-ref][&amp;sparse=0][&amp;depupdate=0]]\ngit+ssh://git@[provider.com]/[user]/[repo]@[path/to/charts][?[ref=git-ref][&amp;sparse=0][&amp;depupdate=0]]\ngit+file://[path/to/repo]@[path/to/charts][?[ref=git-ref][&amp;sparse=0][&amp;depupdate=0]]\n\ngit+https://github.com/jetstack/cert-manager@deploy/charts?ref=v0.6.2&amp;sparse=0\ngit+ssh://git@github.com/jetstack/cert-manager@deploy/charts?ref=v0.6.2&amp;sparse=1\ngit+ssh://git@github.com/jetstack/cert-manager@deploy/charts?ref=v0.6.2\ngit+https://github.com/istio/istio@install/kubernetes/helm?ref=1.5.4&amp;sparse=0&amp;depupdate=0\n</code></pre> <p>Add your repository:</p> <pre><code>helm repo add cert-manager git+https://github.com/jetstack/cert-manager@deploy/charts?ref=v0.6.2\n</code></pre> <p>You can use it as any other Helm chart repository. Try:</p> <pre><code>$ helm search cert-manager\nNAME                                    CHART VERSION   APP VERSION     DESCRIPTION\ncert-manager/cert-manager               v0.6.6          v0.6.2          A Helm chart for cert-manager\n\n$ helm install cert-manager/cert-manager --version \"0.6.6\"\n</code></pre> <p>Fetching also works:</p> <pre><code>helm fetch cert-manager/cert-manager --version \"0.6.6\"\nhelm fetch git+https://github.com/jetstack/cert-manager@deploy/charts/cert-manager-v0.6.2.tgz?ref=v0.6.2\n</code></pre>"}, {"location": "helm_git/#references", "title": "References", "text": "<ul> <li>Git</li> </ul>"}, {"location": "html/", "title": "HTML", "text": "<p>HTML is the standard markup language for Web pages. With HTML you can create your own Website.</p>"}, {"location": "html/#document-structure", "title": "Document structure", "text": "<p>All HTML documents must start with a document type declaration: <code>&lt;!DOCTYPE html&gt;</code>.</p> <p>The HTML document itself begins with <code>&lt;html&gt;</code> and ends with <code>&lt;/html&gt;</code>.</p> <p>The visible part of the HTML document is between <code>&lt;body&gt;</code> and <code>&lt;/body&gt;</code>.</p> <pre><code>&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;body&gt;\n\n&lt;h1&gt;My First Heading&lt;/h1&gt;\n&lt;p&gt;My first paragraph.&lt;/p&gt;\n\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre>"}, {"location": "html/#html-elements", "title": "HTML elements", "text": "<ul> <li>Headings: <code>&lt;h1&gt;</code> to <code>&lt;h6&gt;</code></li> <li>Paragraphs: <code>&lt;p&gt;This is a paragraph.&lt;/p&gt;</code>.</li> <li>Links: <code>&lt;a href=\"https://www.w3schools.com\"&gt;This is a link&lt;/a&gt;</code></li> <li>Images: <code>&lt;img src=\"w3schools.jpg\" alt=\"W3Schools.com\" width=\"104\" height=\"142\"&gt;</code></li> <li>Line breaks: <code>&lt;br&gt;</code>, <code>&lt;hr&gt;</code></li> <li>Comments: <code>&lt;!-- Write your comments here --&gt;</code></li> <li>Code: <code>&lt;code&gt; x = 5&lt;/code&gt;</code></li> </ul>"}, {"location": "html/#links", "title": "Links", "text": "<p>HTML links are hyperlinks. You can click on a link and jump to another document.</p> <p>The HTML <code>&lt;a&gt;</code> tag defines a hyperlink. It has the following syntax:</p> <pre><code>&lt;a href=\"url\"&gt;link text&lt;/a&gt;\n</code></pre> <p>The link text is the part that will be visible to the reader.</p> <p>Link attributes:</p> <ul> <li><code>href</code>: indicates the link's destination.</li> <li><code>target</code>: specifies where to open the linked document. It can have one of the following values:<ul> <li><code>_self</code>: (Default) Opens the document in the same window/tab as it was     clicked.</li> <li><code>_blank</code>: Opens the document in a new window or tab.</li> <li><code>_parent</code>: Opens the document in the parent frame.</li> <li><code>_top</code>: Opens the document in the full body of the window.</li> </ul> </li> </ul>"}, {"location": "html/#images", "title": "Images", "text": "<p>The HTML <code>&lt;img&gt;</code> tag is used to embed an image in a web page.</p> <p>Images are not technically inserted into a web page; images are linked to web pages. The <code>&lt;img&gt;</code> tag creates a holding space for the referenced image.</p> <p>The <code>&lt;img&gt;</code> tag is empty, it contains attributes only, and does not have a closing tag.</p> <p>The <code>&lt;img&gt;</code> tag has two required attributes:</p> <ul> <li><code>src</code>: Specifies the path to the image.</li> <li><code>alt</code>: Specifies an alternate text for the image shown if the user for some     reason cannot view it.</li> </ul> <pre><code>&lt;img src=\"url\" alt=\"alternatetext\"&gt;\n</code></pre> <p>Other <code>&lt;img&gt;</code> attributes are:</p> <ul> <li> <p><code>&lt;style&gt;</code>: specify the width and height of an image.</p> <pre><code>&lt;img src=\"img_1.jpg\" alt=\"img_1\" style=\"width:500px;height:600px;\"&gt;\n</code></pre> <p>Even though you could use <code>width</code> and <code>height</code>, if you use the <code>style</code> attribute you prevent style sheets to change the size of images.</p> </li> <li> <p><code>&lt;float&gt;</code>: let the image float to the right or to the left of a text.</p> <pre><code>&lt;p&gt;&lt;img src=\"smiley.gif\" alt=\"Smiley face\" style=\"float:right;width:42px;height:42px;\"&gt;\nThe image will float to the right of the text.&lt;/p&gt;\n\n&lt;p&gt;&lt;img src=\"smiley.gif\" alt=\"Smiley face\" style=\"float:left;width:42px;height:42px;\"&gt;\nThe image will float to the left of the text.&lt;/p&gt;\n</code></pre> </li> </ul> <p>If you want to use an image as a link use:</p> <pre><code> &lt;a href=\"default.asp\"&gt;\n  &lt;img src=\"smiley.gif\" alt=\"HTML tutorial\" style=\"width:42px;height:42px;\"&gt;\n&lt;/a&gt;\n</code></pre>"}, {"location": "html/#lists", "title": "Lists", "text": "<p>HTML lists allow web developers to group a set of related items in lists.</p> <ul> <li> <p>Unordered lists: starts with the <code>&lt;ul&gt;</code> tag. Each list item starts with the     <code>&lt;li&gt;</code> tag. The list items will be marked with bullets (small black circles)     by default:</p> <p><pre><code>&lt;ul&gt;\n  &lt;li&gt;Coffee&lt;/li&gt;\n  &lt;li&gt;Tea&lt;/li&gt;\n  &lt;li&gt;Milk&lt;/li&gt;\n&lt;/ul&gt;\n</code></pre> * Ordered list: Starts with the <code>&lt;ol&gt;</code> tag. Each list item starts with the <code>&lt;li&gt;</code> tag. The list items will be marked with numbers by default:</p> <pre><code>&lt;ol&gt;\n  &lt;li&gt;Coffee&lt;/li&gt;\n  &lt;li&gt;Tea&lt;/li&gt;\n  &lt;li&gt;Milk&lt;/li&gt;\n&lt;/ol&gt;\n</code></pre> </li> </ul>"}, {"location": "html/#tables", "title": "Tables", "text": "<p>HTML tables allow web developers to arrange data into rows and columns.</p> <pre><code> &lt;table&gt;\n  &lt;tr&gt;\n    &lt;th&gt;Company&lt;/th&gt;\n    &lt;th&gt;Contact&lt;/th&gt;\n    &lt;th&gt;Country&lt;/th&gt;\n  &lt;/tr&gt;\n  &lt;tr&gt;\n    &lt;td&gt;Alfreds Futterkiste&lt;/td&gt;\n    &lt;td&gt;Maria Anders&lt;/td&gt;\n    &lt;td&gt;Germany&lt;/td&gt;\n  &lt;/tr&gt;\n  &lt;tr&gt;\n    &lt;td&gt;Centro comercial Moctezuma&lt;/td&gt;\n    &lt;td&gt;Francisco Chang&lt;/td&gt;\n    &lt;td&gt;Mexico&lt;/td&gt;\n  &lt;/tr&gt;\n&lt;/table&gt;\n</code></pre> <p>Where:</p> <ul> <li><code>&lt;th&gt;</code>: Defines the table headers</li> <li><code>&lt;tr&gt;</code>: Defines the table rows</li> <li><code>&lt;td&gt;</code>: Defines the table cells</li> </ul>"}, {"location": "html/#blocks", "title": "Blocks", "text": "<p>A block-level element always starts on a new line, and the browsers automatically add some space (a margin) before and after the element.</p> <p>A block-level element always takes up the full width available (stretches out to the left and right as far as it can).</p> <p>An inline element does not start on a new line and only takes up as much width as necessary.</p> <ul> <li> <p><code>&lt;p&gt;</code>: defines a paragraph in an HTML document.</p> </li> <li> <p><code>&lt;div&gt;</code>: defines a division or a section in an HTML document. It has     no required attributes, but style, class and id are common. When used together     with CSS, the <code>&lt;div&gt;</code> element can be used to style blocks of content:</p> <p><pre><code>&lt;div style=\"background-color:black;color:white;padding:20px;\"&gt;\n  &lt;h2&gt;London&lt;/h2&gt;\n  &lt;p&gt;London is the capital city of England. It is the most populous city in the United Kingdom, with a metropolitan area of over 13 million inhabitants.&lt;/p&gt;\n&lt;/div&gt;\n</code></pre> * <code>&lt;span&gt;</code>: Is an inline container used to mark up a part of a text, or a part of a document.</p> <p>The <code>&lt;span&gt;</code> element has no required attributes, but style, class and id are common. When used together with CSS, the <code>&lt;span&gt;</code> element can be used to style parts of the text:</p> <pre><code>&lt;p&gt;My mother has &lt;span style=\"color:blue;font-weight:bold\"&gt;blue&lt;/span&gt; eyes and my father has &lt;span style=\"color:darkolivegreen;font-weight:bold\"&gt;dark green&lt;/span&gt; eyes.&lt;/p&gt;\n</code></pre> </li> </ul>"}, {"location": "html/#classes", "title": "Classes", "text": "<p>The <code>class</code> attribute is often used to point to a class name in a style sheet. It can also be used by a JavaScript to access and manipulate elements with the specific class name.</p> <p>In the following example we have three <code>&lt;div&gt;</code> elements with a class attribute with the value of \"city\". All of the three <code>&lt;div&gt;</code> elements will be styled equally according to the <code>.city</code> style definition in the head section:</p> <pre><code> &lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;head&gt;\n&lt;style&gt;\n.city {\nbackground-color: tomato;\ncolor: white;\nborder: 2px solid black;\nmargin: 20px;\npadding: 20px;\n}\n&lt;/style&gt;\n&lt;/head&gt;\n&lt;body&gt;\n\n&lt;div class=\"city\"&gt;\n  &lt;h2&gt;London&lt;/h2&gt;\n  &lt;p&gt;London is the capital of England.&lt;/p&gt;\n&lt;/div&gt;\n\n&lt;div class=\"city\"&gt;\n  &lt;h2&gt;Paris&lt;/h2&gt;\n  &lt;p&gt;Paris is the capital of France.&lt;/p&gt;\n&lt;/div&gt;\n\n&lt;div class=\"city\"&gt;\n  &lt;h2&gt;Tokyo&lt;/h2&gt;\n  &lt;p&gt;Tokyo is the capital of Japan.&lt;/p&gt;\n&lt;/div&gt;\n\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre> <p>HTML elements can belong to more than one class. To define multiple classes, separate the class names with a space, e.g. <code>&lt;div class=\"city main\"&gt;</code>. The element will be styled according to all the classes specified.</p>"}, {"location": "html/#javascript", "title": "Javascript", "text": "<p>The HTML <code>&lt;script&gt;</code> tag is used to define a client-side script (JavaScript).</p> <p>The <code>&lt;script&gt;</code> element either contains script statements, or it points to an external script file through the src attribute.</p> <p>Common uses for JavaScript are image manipulation, form validation, and dynamic changes of content.</p> <p>This JavaScript example writes \"Hello JavaScript!\" into an HTML element with <code>id=\"demo\"</code>:</p> <pre><code>&lt;script&gt;\ndocument.getElementById(\"demo\").innerHTML = \"Hello JavaScript!\";\n&lt;/script&gt;\n</code></pre> <p>The HTML <code>&lt;noscript&gt;</code> tag defines an alternate content to be displayed to users that have disabled scripts in their browser or have a browser that doesn't support scripts:</p> <pre><code>&lt;script&gt;\ndocument.getElementById(\"demo\").innerHTML = \"Hello JavaScript!\";\n&lt;/script&gt;\n&lt;noscript&gt;Sorry, your browser does not support JavaScript!&lt;/noscript&gt;\n</code></pre>"}, {"location": "html/#head", "title": "Head", "text": "<p>The <code>&lt;head&gt;</code> element is a container for metadata (data about data) and is placed between the <code>&lt;html&gt;</code> tag and the <code>&lt;body&gt;</code> tag.</p> <p>HTML metadata is data about the HTML document. Metadata is not displayed.</p> <p>Metadata typically define the document title, character set, styles, scripts, and other meta information.</p> <p>It contains the next sections:</p> <ul> <li> <p><code>&lt;title&gt;</code>: defines the title of the document. The title must be text-only, and     it is used to:</p> <ul> <li>define the title in the browser toolbar</li> <li>provide a title for the page when it is added to favorites</li> <li>display a title for the page in search engine-results</li> </ul> <pre><code>&lt;title&gt;A Meaningful Page Title&lt;/title&gt;\n</code></pre> </li> <li> <p><code>&lt;style&gt;</code>: define style information for a single HTML page.</p> <pre><code>&lt;style&gt;\nbody {background-color: powderblue;}\nh1 {color: red;}\np {color: blue;}\n&lt;/style&gt;\n</code></pre> </li> <li> <p><code>&lt;link&gt;</code>: defines the relationship between the current document and an external resource.</p> <pre><code> &lt;link rel=\"stylesheet\" href=\"mystyle.css\"&gt;\n</code></pre> </li> <li> <p><code>&lt;meta&gt;</code>: specify the character set, page description, keywords, author of the     document, and viewport settings. It won't be displayed on the page, but are     used by browsers (how to display content or reload page), by search engines     (keywords), and other web services. For example:</p> <ul> <li>Define the character set used: <code>&lt;meta charset=\"UTF-8\"&gt;</code>.</li> <li>Define keywords for search engines: <code>&lt;meta name=\"keywords\" content=\"HTML,     CSS, JavaScript\"&gt;</code>.</li> <li>Define a description of your web page: <code>&lt;meta name=\"description\"     content=\"Free Web tutorials\"&gt;</code>.</li> <li>Define the author of a page: <code>&lt;meta name=\"author\" content=\"John Doe\"&gt;</code>.</li> <li>Refresh document every 30 seconds: <code>&lt;meta http-equiv=\"refresh\" content=\"30\"&gt;</code>.</li> <li>Setting the viewport.</li> </ul> </li> <li> <p><code>&lt;script&gt;</code>: define client-side JavaScripts.</p> <pre><code>&lt;script&gt;\nfunction myFunction() {\ndocument.getElementById(\"demo\").innerHTML = \"Hello JavaScript!\";\n}\n&lt;/script&gt;\n</code></pre> </li> <li> <p><code>&lt;base&gt;</code>: specifies the base URL and/or target for all relative URLs in     a page. The <code>&lt;base&gt;</code> tag must have either an href or a target attribute     present, or both.</p> <pre><code>&lt;base href=\"https://www.w3schools.com/\" target=\"_blank\"&gt;\n</code></pre> </li> </ul>"}, {"location": "html/#favicon", "title": "Favicon", "text": "<p>A favicon image is displayed to the left of the page title in the browser tab.</p> <p>To add a favicon to your website, either save your favicon image to the root directory of your webserver, or create a folder in the root directory called images, and save your favicon image in this folder. A common name for a favicon image is \"favicon.ico\".</p> <p>Next, add a <code>&lt;link&gt;</code> element to your \"index.html\" file, after the <code>&lt;title&gt;</code> element, like this:</p> <pre><code>&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;head&gt;\n  &lt;title&gt;My Page Title&lt;/title&gt;\n  &lt;link rel=\"icon\" type=\"image/x-icon\" href=\"/images/favicon.ico\"&gt;\n&lt;/head&gt;\n&lt;body&gt;\n</code></pre>"}, {"location": "html/#styles", "title": "Styles", "text": "<p>The HTML <code>style</code> attribute is used to add styles to an element, such as color, font, size, and more.</p> <pre><code>&lt;tagname style=\"property:value;\"&gt;\n</code></pre> <p>The property is a CSS property. The value is a CSS value.</p>"}, {"location": "html/#formatting", "title": "Formatting", "text": "<p>Formatting elements were designed to display special types of text:</p> <ul> <li><code>&lt;b&gt;</code>: Bold text.</li> <li><code>&lt;strong&gt;</code>: Important text.</li> <li><code>&lt;i&gt;</code>: Italic text.</li> <li><code>&lt;em&gt;</code>: Emphasized text.</li> <li><code>&lt;mark&gt;</code>: Marked text.</li> <li><code>&lt;small&gt;</code>: Smaller text.</li> <li><code>&lt;del&gt;</code>: Deleted text.</li> <li><code>&lt;ins&gt;</code>: Inserted text.</li> <li><code>&lt;sub&gt;</code>: Subscript text.</li> <li><code>&lt;sup&gt;</code>: Superscript text.</li> </ul>"}, {"location": "html/#layout", "title": "Layout", "text": "<p>Websites often display content in multiple columns (like a magazine or a newspaper).</p> <p>HTML has several semantic elements that define the different parts of a web page: HTML5 Semantic Elements</p> <ul> <li><code>&lt;header&gt;</code>: Defines a header for a document or a section.</li> <li><code>&lt;nav&gt;</code>: Defines a set of navigation links.</li> <li><code>&lt;section&gt;</code>: Defines a section in a document.</li> <li><code>&lt;article&gt;</code>: Defines an independent, self-contained content.</li> <li><code>&lt;aside&gt;</code>: Defines content aside from the content (like a sidebar).</li> <li><code>&lt;footer&gt;</code>: Defines a footer for a document or a section.</li> <li><code>&lt;details&gt;</code>: Defines additional details that the user can open and close on     demand.</li> <li><code>&lt;summary&gt;</code>: Defines a heading for the  element."}, {"location": "html/#layout-elements", "title": "Layout elements", "text": ""}, {"location": "html/#section", "title": "Section", "text": "<p>A section is a thematic grouping of content, typically with a heading.</p> <p>Examples of where a <code>&lt;section&gt;</code> element can be used:</p> <ul> <li>Chapters</li> <li>Introduction</li> <li>News items</li> <li>Contact information</li> </ul> <pre><code> &lt;section&gt;\n&lt;h1&gt;WWF&lt;/h1&gt;\n&lt;p&gt;The World Wide Fund for Nature (WWF) is an international organization working on issues regarding the conservation, research and restoration of the environment, formerly named the World Wildlife Fund. WWF was founded in 1961.&lt;/p&gt;\n&lt;/section&gt;\n\n&lt;section&gt;\n&lt;h1&gt;WWF's Panda symbol&lt;/h1&gt;\n&lt;p&gt;The Panda has become the symbol of WWF. The well-known panda logo of WWF originated from a panda named Chi Chi that was transferred from the Beijing Zoo to the London Zoo in the same year of the establishment of WWF.&lt;/p&gt;\n&lt;/section&gt;\n</code></pre>"}, {"location": "html/#article", "title": "article", "text": "<p>The <code>&lt;article&gt;</code> element specifies independent, self-contained content.</p> <p>An article should make sense on its own, and it should be possible to distribute it independently from the rest of the web site.</p> <p>Examples of where the <code>&lt;article&gt;</code> element can be used:</p> <ul> <li>Forum posts</li> <li>Blog posts</li> <li>User comments</li> <li>Product cards</li> <li>Newspaper articles</li> </ul> <pre><code>&lt;article&gt;\n&lt;h2&gt;Google Chrome&lt;/h2&gt;\n&lt;p&gt;Google Chrome is a web browser developed by Google, released in 2008. Chrome is the world's most popular web browser today!&lt;/p&gt;\n&lt;/article&gt;\n\n&lt;article&gt;\n&lt;h2&gt;Mozilla Firefox&lt;/h2&gt;\n&lt;p&gt;Mozilla Firefox is an open-source web browser developed by Mozilla. Firefox has been the second most popular web browser since January, 2018.&lt;/p&gt;\n&lt;/article&gt;\n\n&lt;article&gt;\n&lt;h2&gt;Microsoft Edge&lt;/h2&gt;\n&lt;p&gt;Microsoft Edge is a web browser developed by Microsoft, released in 2015. Microsoft Edge replaced Internet Explorer.&lt;/p&gt;\n&lt;/article&gt;\n</code></pre>"}, {"location": "html/#header", "title": "header", "text": "<p>The <code>&lt;header&gt;</code> element represents a container for introductory content or a set of navigational links.</p> <p>A <code>&lt;header&gt;</code> element typically contains:</p> <ul> <li>one or more heading elements (<code>&lt;h1&gt;</code> - <code>&lt;h6&gt;</code>)</li> <li>logo or icon</li> <li>authorship information</li> </ul> <pre><code> &lt;article&gt;\n  &lt;header&gt;\n    &lt;h1&gt;What Does WWF Do?&lt;/h1&gt;\n    &lt;p&gt;WWF's mission:&lt;/p&gt;\n  &lt;/header&gt;\n  &lt;p&gt;WWF's mission is to stop the degradation of our planet's natural environment,\n  and build a future in which humans live in harmony with nature.&lt;/p&gt;\n&lt;/article&gt;\n</code></pre>"}, {"location": "html/#footer", "title": "footer", "text": "<p>The <code>&lt;footer&gt;</code> element defines a footer for a document or section.</p> <p>A <code>&lt;footer&gt;</code> element typically contains:</p> <ul> <li>authorship information</li> <li>copyright information</li> <li>contact information</li> <li>sitemap</li> <li>back to top links</li> <li>related documents</li> </ul> <pre><code> &lt;footer&gt;\n  &lt;p&gt;Author: Hege Refsnes&lt;/p&gt;\n  &lt;p&gt;&lt;a href=\"mailto:hege@example.com\"&gt;hege@example.com&lt;/a&gt;&lt;/p&gt;\n&lt;/footer&gt;\n</code></pre>"}, {"location": "html/#layout-techniques", "title": "Layout Techniques", "text": "<p>There are four different techniques to create multicolumn layouts. Each technique has its pros and cons:</p> <ul> <li>CSS framework</li> <li>CSS float property</li> <li>CSS flexbox</li> <li>CSS grid</li> </ul>"}, {"location": "html/#frameworks", "title": "Frameworks", "text": "<p>If you want to create your layout fast, you can use a CSS framework, like W3.CSS or Bootstrap.</p>"}, {"location": "html/#float-layout", "title": "Float layout", "text": "<p>It is common to do entire web layouts using the CSS <code>float</code> property. Float is easy to learn - you just need to remember how the <code>float</code> and <code>clear</code> properties work.</p> <p>Disadvantages: Floating elements are tied to the document flow, which may harm the flexibility.</p>"}, {"location": "html/#flexbox-layout", "title": "Flexbox layout", "text": "<p>Use of flexbox ensures that elements behave predictably when the page layout must accommodate different screen sizes and different display devices.</p>"}, {"location": "html/#grid-layout", "title": "Grid layout", "text": "<p>The CSS Grid Layout Module offers a grid-based layout system, with rows and columns, making it easier to design web pages without having to use floats and positioning.</p>"}, {"location": "html/#responsive", "title": "Responsive", "text": "<p>Responsive web design is about creating web pages that look good on all devices.</p> <p>A responsive web design will automatically adjust for different screen sizes and viewports.</p>"}, {"location": "html/#setting-the-viewport", "title": "Setting the viewport", "text": "<p>To create a responsive website, add the following <code>&lt;meta&gt;</code> tag to all your web pages:</p> <p><pre><code>&lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"&gt;\n</code></pre> This gives the browser instructions on how to control the page's dimensions and scaling.</p> <p>The <code>width=device-width</code> part sets the width of the page to follow the screen-width of the device (which will vary depending on the device).</p> <p>The <code>initial-scale=1.0</code> part sets the initial zoom level when the page is first loaded by the browser.</p>"}, {"location": "html/#responsive-images", "title": "Responsive images", "text": "<p>Using the <code>max-width</code> property: If the CSS <code>max-width</code> property is set to <code>100%</code>, the image will be responsive and scale up and down, but never scale up to be larger than its original size:</p> <pre><code>&lt;img src=\"img_girl.jpg\" style=\"max-width:100%;height:auto;\"&gt;\n</code></pre>"}, {"location": "html/#responsive-text-size", "title": "Responsive text size", "text": "<p>The text size can be set with a \"vw\" unit, which means the \"viewport width\".</p> <p>That way the text size will follow the size of the browser window:</p> <pre><code>&lt;h1 style=\"font-size:10vw\"&gt;Hello World&lt;/h1&gt;\n</code></pre> <p>Viewport is the browser window size. <code>1vw = 1%</code> of viewport width. If the viewport is 50cm wide, 1vw is 0.5cm.</p>"}, {"location": "html/#media-queries", "title": "Media queries", "text": "<p>In addition to resize text and images, it is also common to use media queries in responsive web pages.</p> <p>With media queries you can define completely different styles for different browser sizes.</p> <p>The next example will make the three div elements display horizontally on large screens and stacked vertically on small screens:</p> <pre><code> &lt;style&gt;\n.left, .right {\nfloat: left;\nwidth: 20%; /* The width is 20%, by default */\n}\n\n.main {\nfloat: left;\nwidth: 60%; /* The width is 60%, by default */\n}\n\n/* Use a media query to add a breakpoint at 800px: */\n@media screen and (max-width: 800px) {\n.left, .main, .right {\nwidth: 100%; /* The width is 100%, when the viewport is 800px or smaller */\n}\n}\n&lt;/style&gt;\n</code></pre>"}, {"location": "html/#code-style", "title": "Code Style", "text": "<ul> <li>Always declare the document type as the first line in your document.     <pre><code>&lt;!DOCTYPE html&gt;\n</code></pre></li> <li>Use lowercase element names:     <pre><code>&lt;body&gt;\n&lt;p&gt;This is a paragraph.&lt;/p&gt;\n&lt;/body&gt;\n</code></pre></li> <li>Close all HTML elements.</li> <li>Use lowercase attribute names</li> <li>Always quote attribute values</li> <li>Always Specify alt, width, and height for Images.</li> <li>Don't add spaces between equal signs: <code>&lt;link rel=\"stylesheet\"     href=\"styles.css\"&gt;</code></li> <li>Avoid Long Code Lines</li> <li>Do not add blank lines, spaces, or indentations without a reason.</li> <li>Use two spaces for indentation instead of tab</li> <li>Never Skip the <code>&lt;title&gt;</code> Element</li> <li>Always add the <code>&lt;html&gt;</code>, <code>&lt;head&gt;</code> and <code>&lt;body&gt;</code> tags.</li> <li> <p>Always include the <code>lang</code> attribute inside the <code>&lt;html&gt;</code> tag</p> <p><pre><code>&lt;!DOCTYPE html&gt;\n&lt;html lang=\"en-us\"&gt;\n&lt;/html&gt;\n</code></pre> * Set the character encoding: <code>&lt;meta charset=\"UTF-8\"&gt;</code> * Set the viewport.</p> </li> </ul>"}, {"location": "html/#tips", "title": "Tips", "text": ""}, {"location": "html/#html-beautifier", "title": "HTML beautifier", "text": "<p>If you encounter html code that it's not well indented  you can use html beautify.</p>"}, {"location": "html/#references", "title": "References", "text": "<ul> <li>W3 tutorial</li> </ul>"}, {"location": "husboard/", "title": "Hushboard", "text": "<p>Hushboard is an utility that mutes your microphone while you\u2019re typing.</p>"}, {"location": "husboard/#installation", "title": "Installation", "text": "<p>They recommend using the Snap Store package but you can also install it manually as follows:</p> <pre><code>sudo apt install libgirepository1.0-dev libcairo2-dev\nmkvirtualenv hushboard\ngit clone https://github.com/stuartlangridge/hushboard\ncd hushboard\npip install pycairo PyGObject six xlib\npip install .\ndeactivate\n</code></pre>"}, {"location": "husboard/#running-the-application", "title": "Running the application", "text": "<p>You can run it manually as follows</p> <pre><code>workon hushboard\npython -m hushboard\ndeactivate\n</code></pre> <p>Or if you use i3wm, create the following script.</p> <pre><code>#!/usr/bin/env bash\n\nsource {WORKON_PATH}/hushboard/bin/activate\npython -m hushboard\ndeactivate\n</code></pre> <p>You should replace <code>{WORKON_PATH}</code> with your virtual environments path. Then add this line to your <code>i3wm</code> configuration file to start it automatically.</p> <pre><code>exec --no-startup-id ~/scripts/hushboard.sh\n</code></pre>"}, {"location": "husboard/#reference", "title": "Reference", "text": "<ul> <li>M0wer Husboard article</li> </ul>"}, {"location": "i3wm/", "title": "i3", "text": "<p>i3 is a tiling window manager.</p>"}, {"location": "i3wm/#layout-saving", "title": "Layout saving", "text": "<p>Layout saving/restoring allows you to load a JSON layout file so that you can have a base layout to start working with after powering on your computer.</p> <p>First of all arrange the windows in the workspace, then you can save the layout of either a single workspace or an entire output:</p> <pre><code>i3-save-tree --workspace \"1: terminal\" &gt; ~/.i3/workspace-1.json\n</code></pre> <p>You need to open the created file and remove the comments that match the desired windows under the <code>swallows</code> keys, so transform the next snippet:</p> <pre><code>    ...\n\"swallows\": [\n{\n//  \"class\": \"^URxvt$\",\n//  \"instance\": \"^irssi$\"\n}\n]\n...\n</code></pre> <p>Into:</p> <pre><code>    ...\n\"swallows\": [\n{\n\"class\": \"^URxvt$\",\n\"instance\": \"^irssi$\"\n}\n]\n...\n</code></pre> <p>Once is ready close all the windows of the workspace you want to restore (moving them away is not enough!).</p> <p>Then on a terminal you can restore the layout with:</p> <pre><code>i3-msg 'workspace \"1: terminal\"; append_layout ~/.i3/workspace-1.json'\n</code></pre> <p>It's important that you don't use a relative path</p> <p>Even if you're in <code>~/.i3/</code> you have to use <code>i3-msg append_layout ~/.i3/workspace-1.json</code>.</p> <p>This command will create some fake windows (called placeholders) with the layout you had before, <code>i3</code> will then wait for you to create the windows that match the selection criteria. Once they are, it will put them in their respective placeholders.</p> <p>If you wish to create the layouts at startup you can add the next snippet to your i3 config.</p> <pre><code>exec --no-startup-id \"i3-msg 'workspace \\\"1: terminal\\\"; append_layout ~/.i3/workspace-1.json'\"\n</code></pre>"}, {"location": "i3wm/#move-the-focus-to-a-container", "title": "Move the focus to a container", "text": "<p>Get the container identifier with <code>xprop</code> and then:</p> <pre><code>i3-msg '[title=\"khime\"]' focus\ni3-msg '[class=\"Firefox\"]' focus\n</code></pre>"}, {"location": "i3wm/#interact-with-python", "title": "Interact with Python", "text": "<p>Install the <code>i3ipc</code> library:</p> <pre><code>pip install i3ipc\n</code></pre> <p>Create the connection object:</p> <pre><code>from i3ipc import Connection, Event\n\n# Create the Connection object that can be used to send commands and subscribe\n# to events.\ni3 = Connection()\n</code></pre> <p>Interact with i3:</p> <pre><code># Print the name of the focused window\nfocused = i3.get_tree().find_focused()\nprint('Focused window %s is on workspace %s' %\n      (focused.name, focused.workspace().name))\n\n# Query the ipc for outputs. The result is a list that represents the parsed\n# reply of a command like `i3-msg -t get_outputs`.\noutputs = i3.get_outputs()\n\nprint('Active outputs:')\n\nfor output in filter(lambda o: o.active, outputs):\n    print(output.name)\n\n# Send a command to be executed synchronously.\ni3.command('focus left')\n\n# Take all fullscreen windows out of fullscreen\nfor container in i3.get_tree().find_fullscreen():\n    container.command('fullscreen')\n\n# Print the names of all the containers in the tree\nroot = i3.get_tree()\nprint(root.name)\nfor con in root:\n    print(con.name)\n\n# Define a callback to be called when you switch workspaces.\ndef on_workspace_focus(self, e):\n    # The first parameter is the connection to the ipc and the second is an object\n    # with the data of the event sent from i3.\n    if e.current:\n        print('Windows on this workspace:')\n        for w in e.current.leaves():\n            print(w.name)\n\n# Dynamically name your workspaces after the current window class\ndef on_window_focus(i3, e):\n    focused = i3.get_tree().find_focused()\n    ws_name = \"%s:%s\" % (focused.workspace().num, focused.window_class)\n    i3.command('rename workspace to \"%s\"' % ws_name)\n\n# Subscribe to events\ni3.on(Event.WORKSPACE_FOCUS, on_workspace_focus)\ni3.on(Event.WINDOW_FOCUS, on_window_focus)\n\n# Start the main loop and wait for events to come in.\ni3.main()\n</code></pre>"}, {"location": "i3wm/#references", "title": "References", "text": "<ul> <li>Home</li> <li><code>i3ipc</code> Docs</li> <li><code>i3ipc</code> Source</li> </ul>"}, {"location": "ics/", "title": "ics", "text": "<p>ics is a pythonic iCalendar library. Its goals are to read and write ics data in a developer-friendly way.</p>"}, {"location": "ics/#installation", "title": "Installation", "text": "<p>Install using pip:</p> <pre><code>pip install ics\n</code></pre>"}, {"location": "ics/#usage", "title": "Usage", "text": "<p><code>ics</code> will delete all data that it doesn't understand. Maybe it's better for your case to build a parse for ics.</p>"}, {"location": "ics/#import-a-calendar-from-a-file", "title": "Import a calendar from a file", "text": "<pre><code>file = '/tmp/event.ics'\n\nfrom ics import Calendar\n\nwith open(file, 'r') as fd:\n    calendar = Calendar(fd.read())\n\n# &lt;Calendar with 118 events and 0 todo&gt;\ncalendar.events\n\n# {&lt;Event 'Visite de \"Fab Bike\"' begin:2016-06-21T15:00:00+00:00 end:2016-06-21T17:00:00+00:00&gt;,\n# &lt;Event 'Le lundi de l'embarqu\u00e9: Adventure in Espressif Non OS SDK edition' begin:2018-02-19T17:00:00+00:00 end:2018-02-19T22:00:00+00:00&gt;,\n#  ...}\nevent = list(calendar.timeline)[0]\n</code></pre>"}, {"location": "ics/#export-a-calendar-to-a-file", "title": "Export a Calendar to a file", "text": "<pre><code>with open('my.ics', 'w') as f:\n    f.writelines(calendar.serialize_iter())\n# And it's done !\n\n# iCalendar-formatted data is also available in a string\ncalendar.serialize()\n# 'BEGIN:VCALENDAR\\nPRODID:...\n</code></pre>"}, {"location": "ics/#references", "title": "References", "text": "<ul> <li>Docs</li> </ul>"}, {"location": "instant_messages_management/", "title": "Instant messages management", "text": "<p>Instant messaging in all it's forms is becoming the main communication channel.</p> <p>As any other input system, if not used wisely, it can be a sink of productivity.</p>"}, {"location": "instant_messages_management/#analyze-how-often-you-need-to-check-it", "title": "Analyze how often you need to check it", "text": "<p>Follow the interruption analysis to discover how often you need to check it and if you need the notifications or fine grain them to the sources that have higher priority. Once you've decided the frequency, try to respect it!. If you want an example, check my work or personal analysis.</p>"}, {"location": "instant_messages_management/#workflow", "title": "Workflow", "text": "<p>I interact with messaging applications in two ways:</p> <ul> <li>To read the new items and answer questions.</li> <li>To start a conversation.</li> </ul> <p>The passively reading for new items works perfectly with the interruption management processes. Each time you decide to check for new messages, follow the inbox processing guidelines to extract the information to the appropriate system (task manager, calendar or knowledge manager). If you answer someone or if you start a new conversation, assume that any work done in the next 5 to 10 minutes will probably be interrupted, so choose small or mindless tasks. If the person doesn't answer in that time, start a new pomodoro and go back when the next interruption event comes.</p>"}, {"location": "instant_messages_management/#use-calls-for-non-short-conversations", "title": "Use calls for non short conversations", "text": "<p>Chats are good for short conversations that don't require long or quick responses. Even though people may have forgotten it, they are an asynchronous communication channel.</p> <p>They're not suited for long conversations though as:</p> <ul> <li>Typing on a keyboard (or a mobile <code>\u1559(\u21c0\u2038\u21bc\u2036)\u1557</code>) is slower than talking directly.</li> <li>It's difficult to transmit the conversation tone by message, and each reader     can interpret it differently, leading to misunderstandings.</li> <li>If the conversation topic is complex, graphical aids such as screen sharing or     doodling can make the conversation more efficient.</li> <li>Unless everyone involved is fully focused on the conversation, the delays     between messages can be high, and all that time, the attendees need to     manage the interruptions.</li> <li>If you fully focus on the conversation, you're loosing your time while you     wait for the other to answer.</li> </ul> <p>For all these reasons, whenever a conversation looks not to be short or trivial, arrange a quick call or video call.</p>"}, {"location": "instant_messages_management/#at-work-or-collectives-use-group-rooms-over-direct-messages", "title": "At work or collectives, use group rooms over direct messages", "text": "<p>Asking for help through direct messages should be avoided whenever possible, instead of interrupting one person, it's better to ask in the group rooms because:</p> <ul> <li>More people are reading, so you'll probably get answered sooner.</li> <li>Knowledge is spread throughout the group instead of isolated on specific     people. Even if I don't answer a question, I read what others have     said thus learning in the process.</li> <li>The responsibility of answering is shared between the group members, making     it easier to define the interruptions role.</li> </ul>"}, {"location": "instant_messages_management/#use-threads-or-replies-if-the-client-allows-it", "title": "Use threads or replies if the client allows it", "text": "<p>Threads are a feature that allows people to have parallel conversations in the same room in a way that the messages aren't mixed. This makes it easier to maintain the focus and follow past messages. It also allows users that are not interested, to silence the thread, so they won't get application or/and desktop notifications on that particular topic.</p> <p>Replies can be used when the conversation is not lengthy enough to open a thread. They give the benefit of giving context to the user you're replying to.</p>"}, {"location": "instant_messages_management/#use-chats-to-transport-information-not-to-store-it", "title": "Use chats to transport information, not to store it", "text": "<p>Chat applications were envisioned as a protocol for person A to send information to person B. The fact that the message providers allow users to have almost no limit on their message history has driven people to use them as a knowledge repository. This approach has many problems:</p> <ul> <li>As most people don't use end to end encryption (OMEMO/OTR/Signal), the data of     their messages is available for the service provider to read. This is     a privacy violation that should be avoided. Most providers don't allow you     to set a message limit, so you'd have to delete them manually.</li> <li>Searching information in the chats is a nightmare. There are more     efficient knowledge repositories to store your information.</li> </ul>"}, {"location": "instant_messages_management/#use-key-bindings", "title": "Use key bindings", "text": "<p>Using the mouse to interact with the chat client graphical interfaces is not efficient, try to learn the key bindings and use them as much as possible.</p>"}, {"location": "instant_messages_management/#environment-setup", "title": "Environment setup", "text": ""}, {"location": "instant_messages_management/#account-management", "title": "Account management", "text": "<p>It's common to have more than one account or application to check. There are many instant messaging solutions, such as XMPP, Signal, IRC, Telegram, Slack, Whatssap or Facebook. It would be ideal to have a client that could act as a bridge to all the solutions, but at least I don't know it, so you're forced to install the different applications to interact with them.</p> <p>The obvious suggestion would be to reduce the number of platforms in use, but we all know that it's asking too much as it will probably isolate you from specific people.</p> <p>Once you have the minimum clients chosen, put them all on the same workspace, for example an i3 window manager workspace, and only check them following the workflow rules.</p>"}, {"location": "instant_messages_management/#isolate-your-work-and-personal-environments", "title": "Isolate your work and personal environments", "text": "<p>Make sure that you set your environment so that you can't check your personal chats when you're working and the other way around. For example, you could configure different instances of the chat clients and only open the ones that you need to. Or you could avoid configuring the work clients in your personal phone.</p> <p>For example, at work, I have my own account and another for each team I'm part of, the last ones are managed by all the team members. On the personal level, I've got many accounts for the different OpSec profiles or identities.</p> <p>For efficiency reasons, you need to be able to check all of them on one place. You can use an email manager such as Thunderbird. Once you choose one, try to master it.</p>"}, {"location": "instant_messages_management/#fine-grain-configure-the-notifications", "title": "Fine grain configure the notifications", "text": "<p>Modern client applications allow you to define the notifications at room or people level. I usually:</p> <ul> <li>Use notifications on all messages on high priority channels. For example the     infrastructure monitorization one. Agree with your team to     write as less as possible.</li> <li>Use notifications when mentioned on group rooms: Don't get notified on any     message unless they add your name on it.</li> <li>Use notifications on direct messages: Decide which people are important enough     to activate the notifications.</li> </ul> <p>Sometimes the client applications don't give enough granularity, or you would like to show notifications based on more complex conditions, that's why I created the seed project to improve the notification management in Linux.</p>"}, {"location": "interruption_management/", "title": "Interruption Management", "text": "<p>Interruption management is the life management area that gathers the processes to minimize the time and willpower toll consumed by interruptions.</p> <p>We've come to accept that we need to be available 24/7 and answer immediately, that makes us slaves of the interruptions, it drives our work and personal relations. I feel that out of respect of ourselves and the other's time, we need to change that perspective. Most of the times interruptions can wait 20 or 60 minutes, and many of them can be avoided with better task and time planning.</p> <p>Interruptions are one of the main productivity killers. Not only they unexpectedly break your workflow, they also add undesired mental load as you are waiting for them to happen, and need to check them often. As we've seen previously, to be productive you need to be able to work on a task for 20 minutes without checking the interruption channels.</p>"}, {"location": "interruption_management/#interruption-analysis", "title": "Interruption analysis", "text": "<p>The interruption analysis is the main input to do interruption management. With it you consider what are the sources of the interruptions, and for each of them you classify the different source events in categories evaluating:</p> <ul> <li>How many interruption events does the source or category create.</li> <li>How many of the events require an action, and if it can be automated.</li> <li>How many hold information that don't need any action, and what do you want to     do with that information.</li> <li>How many could be automatically filtered out.</li> <li>What priority do the events have, and if it's the same for all events.</li> <li>How long can the associated action be delayed.</li> </ul> <p>Once you have that list, think if you can reduce it. Can you merge or directly remove one of the sources? The less channels to check, the better.</p> <p>Then think which of them you have no control over and think of ways to regain it. If you decide when to address the interruptions, your mind will have less load and will perform better when you're actually working.</p> <p>The ultimate goal of the analysis is to safely define the maximum amount of time you can spend without looking at the channels. Checking them continuously makes no sense, you're breaking your workflow for no good reason, as most times there is nothing new, and if there is, you feel the urge to act upon them, even though they could wait.</p> <p>In some teams, the situation doesn't allow you not to check them frequently. In those cases you can define the interruption manager role. A figure that is rotated by the team's members so that only one human needs to be monitoring the interruption channels, while the rest of them are able to work continuously on their tasks.</p> <p>If you want to see the analysis in action, check my work analysis or my personal one.</p>"}, {"location": "interruption_management/#workflow", "title": "Workflow", "text": "<p>Once you have all the interruption sources identified, classified, and defined the checking periodicity, you need to decide how to handle them.</p>"}, {"location": "interruption_management/#define-your-interruption-events", "title": "Define your interruption events", "text": "<p>To minimize the times you interrupt your workflow, aggregate the different sources and schedule when are you want to check them. For example, if the analysis gave the next sources:</p> <ul> <li>Source A: check each 4 hours.</li> <li>Source B: check each 5 hours.</li> <li>Source C: check each 20 minutes.</li> </ul> <p>You can schedule the next interruption events:</p> <ul> <li>Check sources A, B and C: when you start working, before lunch and before the     end of the day.</li> <li>Check C: after each Pomodoro     iteration.</li> </ul>"}, {"location": "interruption_management/#process-the-interruption-event-information", "title": "Process the interruption event information", "text": "<p>When an interruption event arrives, process sequentially each source following the inbox emptying guidelines.</p>"}, {"location": "issues/", "title": "Issue tracking", "text": "<p>I haven't found a tool to monitor the context it made me track certain software issues, so I get lost when updates come. Until a tool shows up, I'll use the good old markdown to keep track.</p>"}, {"location": "issues/#pydantic-errors", "title": "Pydantic errors", "text": "<ul> <li>No name 'BaseModel' in module 'pydantic'     (no-name-in-module),     you can find a patch in the pydantic article,     the pydantic developers took that as a solution as it lays in pylint's     roof, once that last issue is     solved try to find a better way to improve the patch solution.</li> </ul>"}, {"location": "issues/#vim-workflow-improvements", "title": "Vim workflow improvements", "text": "<p>Manually formatting paragraphs is an unproductive pain in the ass, Vim-pencil looks promising but there are still some usability issues that need to be fixed first:</p> <ul> <li>Wrong list management: #93     linked to #31 and     #95.</li> <li>Disable wrap of document     headers (less important).</li> </ul>"}, {"location": "issues/#gitea-improvements", "title": "Gitea improvements", "text": "<ul> <li>Replying discussion comments redirects to mail pull request     page: Notify the people     that it's fixed.</li> </ul>"}, {"location": "issues/#gitea-kanban-board-improvements", "title": "Gitea Kanban board improvements", "text": "<ul> <li>Remove the Default issue template:     #14383. When it's solved     apply it in the work's issue tracker.</li> </ul>"}, {"location": "issues/#docker-monitorization", "title": "Docker monitorization", "text": "<ul> <li>Integrate diun in the CI pipelines when they support prometheus     metrics. Update the     docker article too.</li> </ul>"}, {"location": "issues/#gadgetbridge-improvements", "title": "Gadgetbridge improvements", "text": "<ul> <li>Smart alarm     support: Use     it whenever it's available.</li> <li>GET Sp02 real time data, or at least export     it: See how     to use this data once it's available.</li> <li>export heart rate for activities without a GPX     track: See if     I can export the heart rate for post processing. Maybe it's covered     here.</li> <li>Add UI and logic for more complex database import, export and     merging:     Monitor to see if there are new ways or improvements of exporting data.</li> <li>Blog's RSS is not     working: Add     it to the feed reader once it does, and remove the warning from the     gadgetbridge article</li> <li>Integrate with home     assistant:     Check if the integration with kalliope is easy.</li> <li>Issues with zoom, swipe, interact with     graphs:     enable back disable swipe between tabs in the chart settings.</li> <li>PAI     implementation:     Check it once it's ready.</li> <li>Calendar synchronization     issue, could     be related with notifications work after     restart: try     it when it's solved</li> <li>Change snooze time     span: Change     the timespan from 10 to 5 minutes.</li> </ul>"}, {"location": "issues/#ombi-improvements", "title": "Ombi improvements", "text": "<ul> <li>Ebook     requests:     Configure it in the service, notify the people and start using it.</li> <li>Add working links to the details     pages:     nothing to do, just start using it.</li> <li>Allow search by     genre: Notify     the people and start using it.</li> </ul>"}, {"location": "javascript_snippets/", "title": "javascript snippets", "text": ""}, {"location": "javascript_snippets/#set-variable-if-its-undefined", "title": "Set variable if it's undefined", "text": "<pre><code>var x = (x === undefined) ? your_default_value : x;\n</code></pre>"}, {"location": "javascript_snippets/#concatenate-two-arrays", "title": "Concatenate two arrays", "text": "<pre><code>const arr1 = [\"Cecilie\", \"Lone\"];\nconst arr2 = [\"Emil\", \"Tobias\", \"Linus\"];\nconst children = arr1.concat(arr2);\n</code></pre> <p>To join more arrays you can use:</p> <pre><code>const arr1 = [\"Cecilie\", \"Lone\"];\nconst arr2 = [\"Emil\", \"Tobias\", \"Linus\"];\nconst arr3 = [\"Robin\"];\nconst children = arr1.concat(arr2,arr3);\n</code></pre>"}, {"location": "javascript_snippets/#check-if-a-variable-is-not-undefined", "title": "Check if a variable is not undefined", "text": "<pre><code>if(typeof lastname !== \"undefined\")\n{\nalert(\"Hi. Variable is defined.\");\n}\n</code></pre>"}, {"location": "javascript_snippets/#select-a-substring", "title": "Select a substring", "text": "<pre><code>'long string'.substring(startIndex, endIndex)\n</code></pre>"}, {"location": "javascript_snippets/#round-a-number", "title": "Round a number", "text": "<pre><code>Math.round(2.5)\n</code></pre>"}, {"location": "javascript_snippets/#remove-focus-from-element", "title": "Remove focus from element", "text": "<pre><code>document.activeElement.blur();\n</code></pre>"}, {"location": "jellyfin/", "title": "Jellyfin", "text": "<p>Jellyfin is a Free Software Media System that puts you in control of managing and streaming your media. It is an alternative to the proprietary Emby and Plex, to provide media from a dedicated server to end-user devices via multiple apps. Jellyfin is descended from Emby's 3.5.2 release and ported to the .NET Core framework to enable full cross-platform support. There are no strings attached, no premium licenses or features, and no hidden agendas: just a team who want to build something better and work together to achieve it.</p>"}, {"location": "jellyfin/#clients", "title": "Clients", "text": ""}, {"location": "jellyfin/#jellyfin-desktop", "title": "Jellyfin Desktop", "text": ""}, {"location": "jellyfin/#installation", "title": "Installation", "text": "<ul> <li>Download the latest deb package from the releases page</li> <li>Install the dependencies</li> <li>Run <code>dpkg -i</code></li> </ul> <p>If you're on a TV you may want to enable the TV mode so that the remote keys work as expected. The play/pause/next/prev won't work until this issue is solved, but it's not that bad to use the \"Ok\" and then navigate with the arrow keys.</p>"}, {"location": "jellyfin/#jellycon", "title": "Jellycon", "text": "<p>JellyCon is a lightweight Kodi add-on that lets you browse and play media files directly from your Jellyfin server within the Kodi interface. It can be thought of as a thin frontend for a Jellyfin server.</p> <p>It's not very pleasant to use though.</p>"}, {"location": "jellyfin/#installation_1", "title": "Installation", "text": "<ul> <li>Add the Jellyfin kodi addon repository     <pre><code>wget https://kodi.jellyfin.org/repository.jellyfin.kodi.zip\n</code></pre></li> <li>Open Kodi, go to the settings menu, and navigate to \"Add-on Browser\"</li> <li>Select \"Install from Zip File\"</li> <li>From within Kodi, navigate to \"Add-on Browser\"</li> <li>Select \"Install from Repository\"</li> <li>Choose \"Kodi Jellyfin Add-ons\", followed by \"Video Add-ons\"</li> <li>Select the JellyCon add-on and choose install</li> </ul>"}, {"location": "jellyfin/#missing-features", "title": "Missing features", "text": "<ul> <li>Hide movie or tv show from my gallery: Tracked by these feature requests 1 and 2</li> </ul>"}, {"location": "jellyfin/#troubleshooting", "title": "Troubleshooting", "text": ""}, {"location": "jellyfin/#forgot-password-please-try-again-within-your-home-network-to-initiate-the-password-reset-process", "title": "Forgot Password. Please try again within your home network to initiate the password reset process.", "text": "<p>If you're an external jellyfin user you can't reset your password unless you are part of the LAN. This is done because the reset password process is simple and insecure.</p> <p>If you don't care about that and still think that the internet is a happy and safe place here and here are some instructions on how to bypass the security measure.</p> <p>For more information also read 1 and 2.</p>"}, {"location": "jellyfin/#transcode-files-are-cleared-frequently", "title": "Transcode files are cleared frequently", "text": "<p>By default they are cleared each day. If you want to keep them you can go to Admin/Scheduled Tasks/Clean Transcode Directory and remove the scheduled task.</p>"}, {"location": "jellyfin/#deceptive-site-ahead", "title": "Deceptive site ahead", "text": "<p>It seems that Google is marking the domains that host Jellyfin as deceptive. If it happens to you, your users won't be able to access your instance with Firefox, Chrome nor the Android app. Nice uh? It's kind of scary how google is able to control who can access what in the internet without you signing for it. </p> <p>If you search the problem online they suggest that you log in with your google account into the Search Console and see the reasons behind it. Many people did this and reported in the issue that they didn't get any useful information through this process. It's a privacy violation though, as now google is able to tie your identity (as your google account is linked to your phone number) with your Jellyfin domain. Completely disgusting.</p> <p>To solve this issue you need to file a case with google and wait for them to unban you. It's like asking them for permission so that they let your users access your system. The disgust levels keep on growing. Don't waste your time being creative in the Comments of the request either, it looks like they don't even read them.</p> <p>The problem is that until the people from Jellyfin finds a solution, after following this ugly process, you may be flagged again any time in the future (ranging from days to months). </p> <p>A mitigation of the problem is to have an alternative domain that your users can use (for example in duckdns.org). You may be lucky that google doesn't block both domains at the same time.</p> <p>For more information follow the Jellyfin issue or the Jellyfin reddit thread.</p>"}, {"location": "jellyfin/#corrupt-sqlitepclprettysqliteexception-database-disk-image-is-malformed", "title": "Corrupt: SQLitePCL.pretty.SQLiteException: database disk image is malformed", "text": "<p>If your server log file shows SQLite errors like the following example your jellyfin.db file needs attention.</p> <pre><code>'SQLitePCL.pretty.SQLiteException'\n</code></pre> <p>Typical causes of this are sudden and abrupt terminations of the Emby server process, such as a power loss, operating system crash, force killing the server process, etc.</p>"}, {"location": "jellyfin/#solutions-to-try-in-this-order", "title": "Solutions to Try in This Order", "text": ""}, {"location": "jellyfin/#remove-database-locks", "title": "Remove Database Locks", "text": "<ol> <li>Shutdown Jellyfin</li> <li>Navigate to the folder containing your database file</li> <li>Delete <code>library.db-shm</code> and <code>library.db-wal</code></li> <li>Restart Jellyfin</li> </ol> <p>Check you server log for SQLite errors and only continue to the next step if needed.</p>"}, {"location": "jellyfin/#check-database-integrity-and-recover-database", "title": "Check Database Integrity and Recover Database", "text": "<p>This step will require the use of a SQLite editor, I recommend <code>litecli</code> installable with <code>pip</code>.</p>"}, {"location": "jellyfin/#run-integrity-check", "title": "Run Integrity Check", "text": "<p>Open the library.db database and run the following SQL command:</p> <pre><code>PRAGMA integrity_check\n</code></pre> <p>This should return an <code>integrity_check</code> back of <code>OK</code> with no errors reported. If errors are reported we need to recover the database.</p>"}, {"location": "jellyfin/#recover-librarydb", "title": "Recover library.db", "text": "<p>What we need to do is: </p> <ul> <li>Dump all data from the database to a text file and then reload this back to another freshly created database. Run the following command line:</li> </ul> <pre><code>sqlite3 library.db \".recover\" | sqlite3 library-recovered.db\n</code></pre> <p><code>sqlite3</code> can be installed with <code>apt-get install sqlite3</code>.</p> <ul> <li>We will now check the integrity of our recovered database (as above) using:</li> </ul> <pre><code>sqlite3 library-recovered.db \"PRAGMA integrity_check\"\n</code></pre> <p>This should return an <code>integrity_check</code> back of \"OK\" with no errors reported. If errors are reported please report this in the jellyfin issues before proceeding to Reset the Library Database. If OK and no errors are reported continue with the next step.</p> <ul> <li>Make a copy of both <code>library.db</code> and <code>library-recovered.db</code></li> </ul> <pre><code>mkdir broken-dbs\ncp library* broken-dbs\n</code></pre> <ul> <li>Rename <code>library.db</code> to library.old</li> </ul> <pre><code>mv library.db library.old\n</code></pre> <ul> <li>Rename library-recoved.db to library.db</li> </ul> <pre><code>mv library-recovered.db library.db\n</code></pre> <ul> <li>Restart Jellyfin Server</li> </ul> <pre><code>service jellyfin stop\nservice jellyfin start\n</code></pre> <p>Check you server log for SQLite errors and only continue to the next step if needed.</p>"}, {"location": "jellyfin/#reset-library-database-load-fresh", "title": "Reset Library Database &amp; Load Fresh", "text": "<ul> <li>Shutdown Jellyfin</li> <li>Do a copy of all your databases, copy the parent directory where your <code>.db</code> files are to <code>bk.data</code></li> <li>Rename <code>library.db</code> to <code>library.corrupt</code></li> <li>Restart Jellyfin</li> <li>Run a Full Library Scan</li> </ul>"}, {"location": "jellyfin/#move-all-the-journal-databases-away", "title": "Move all the journal databases away", "text": "<p>Finally I moved all the '*-journal' files to a directory, copied again the <code>library-recovered.db</code> to <code>library.db</code>, started the server, do a full scan.</p>"}, {"location": "jellyfin/#check-the-watched-history", "title": "Check the watched history", "text": "<p>Last time I followed these steps I lost part of the visualization history for the users (yikes!). So check that everything is alright.</p> <p>If it's not follow these steps</p>"}, {"location": "jellyfin/#restore-watched-history", "title": "Restore watched history", "text": "<p>Jellyfin stores the watched information in one of the <code>.db</code> files, there are two ways to restore it:</p> <ul> <li>Using scripts that interact with the API like <code>jelly-jar</code> or <code>jellyfin-backup-watched</code></li> <li>Running sqlite queries on the database itself.</li> </ul> <p>The user data is stored in the table <code>UserDatas</code> table in the <code>library.db</code> database file. The media data is stored in the <code>TypedBaseItems</code> table of the same database. </p> <p>Comparing the contents of the tables of the broken database (lost watched content) and a backup database, I've seen that the media content is the same after a full library rescan, so the issue was fixed after injecting the missing user data from the backup to the working database through the importing a table from another database sqlite operation.</p>"}, {"location": "jellyfin/#readonly-sqlitepclprettysqliteexception-attempt-to-write-a-readonly-database", "title": "ReadOnly: SQLitePCL.pretty.SQLiteException: attempt to write a readonly database", "text": "<p>Some of the database files of Jellyfin is not writable by the jellyfin user, check if you changed the ownership of the files, for example in the process of restoring a database file from backup.</p>"}, {"location": "jellyfin/#wrong-image-covers", "title": "Wrong image covers", "text": "<p>Remove all the <code>jpg</code> files of the directory and then fetch again the data from your favourite media management software.</p>"}, {"location": "jellyfin/#green-bars-in-the-reproduction", "title": "Green bars in the reproduction", "text": "<p>It's related to some hardware transcoding issue related to some video codecs, the solution is to either get a file with other codec, or convert it yourself without the hardware transcoding with:</p> <pre><code>ffmpeg -i input.avi -c:v libx264 out.mp4\n</code></pre>"}, {"location": "jellyfin/#stuck-at-login-page", "title": "Stuck at login page", "text": "<p>Sometimes Jellyfin gets stuck at the login screen when trying to log in with an endlessly spinning loading wheel. It looks like it's already fixed, so first try to update to the latest version. If the error remains, follow the next steps:</p> <p>To fix it run the next snippet:</p> <pre><code>systemctl stop jellyfin.service\nmv /var/lib/jellyfin/data/jellyfin.db{,.bak}\nsystemctl start jellyfin.service\n# Go to JF URL, get asked to log in even though\n# there are no Users in the JF DB now\nsystemctl stop jellyfin.service\nmv /var/lib/jellyfin/data/jellyfin.db{.bak,}\nsystemctl start jellyfin.service\n</code></pre> <p>If you use jfa-go for the invites, you may need to regenerate all the user profiles, so that the problem is not introduced again.</p>"}, {"location": "jellyfin/#issues", "title": "Issues", "text": "<ul> <li> <p>Subtitles get delayed from the video on some devices:     1,     2,     3. There is     a feature     request for a fix. Once it's solved notify the users     once it's solved.</p> </li> <li> <p>Trailers not     working:     No solution until it's fixed</p> </li> <li> <p>Unnecessary transcoding:     nothing to do</p> </li> <li>Local social     features:     test it and see how to share rating between users.</li> <li>Skip     intro/outro/credits:     try it.</li> <li>Music star rating:     try it and plan to migrate everything to Jellyfin.</li> <li>Remove pagination/use lazy     loading:     try it.</li> <li>Support     2FA:     try it.</li> <li>Mysql server     backend:     implement it to add robustness.</li> <li>Watched history:     try it.</li> <li>A richer ePub     reader:     migrate from Polar and add jellyfin to the awesome selfhosted list.</li> <li>Prometheus     exporter:     monitor it. There is a plugin that can be used but don't know how.</li> <li>Easy Import/Export Jellyfin     settings:     add to the backup process.</li> <li>Temporary direct file sharing     links:     try it.</li> <li>Remember subtitle and audio track choice between     episodes:     try it.</li> <li>IMBD Rating and Rotten Tomatoes Audiance Rating and Fresh rating on Movies and TV Shows:     try the new ratings.</li> <li>Trailers     Plugin:     Once it's merged to the core, remove the plugin.</li> <li>Jellyfin for apple     tv: tell     the people that use the shitty device.</li> <li> <p>Pressing play on a tv show doesn't reproduce the Next Up</p> </li> <li> <p>Federation between servers: Similar to Share Libraries between servers</p> </li> </ul>"}, {"location": "jellyfin/#references", "title": "References", "text": "<ul> <li>Home</li> <li>Git</li> <li>Blog(RSS)</li> </ul>"}, {"location": "kag/", "title": "Kag", "text": "<p>King Arthur Gold, also known as KAG, is a free Medieval Build n'Kill Multiplayer Game with Destructible Environments.</p> <p>Construct freeform forts as a medieval Builder, fight in sword duels as a Knight or snipe with your bow as an Archer. KAG blends the cooperative aspects of Lost Vikings, mashes them with the full destructibility of Worms and the visual style and action of Metal Slug, brought to you by the creators of Soldat.</p>"}, {"location": "kag/#guides", "title": "Guides", "text": ""}, {"location": "kag/#archer-guides", "title": "Archer guides", "text": "<ul> <li>Turtlebutt and Bunnie</li> <li>Coroz and RedOTheWisp</li> </ul>"}, {"location": "kag/#builder-guides", "title": "Builder guides", "text": "<ul> <li>Turtlebutt and Bunnie</li> </ul>"}, {"location": "khal/", "title": "Khal", "text": "<p><code>khal</code> is a standards based Python CLI (console) calendar program, able to synchronize with CalDAV servers through <code>vdirsyncer</code>.</p> <p>Features:</p> <ul> <li>Can read and write events/icalendars to vdir, so <code>vdirsyncer</code>   can be used to synchronize calendars with a variety of other programs, for   example CalDAV servers.</li> <li>Fast and easy way to add new events</li> <li><code>ikhal</code> (interactive <code>khal</code>) lets you browse and edit calendars and events.</li> </ul> <p>Limitations:</p> <ul> <li>It's not easy to get an idea of what you need to do in the week. At least not   as comfortable as a graphical interface.</li> <li> <p>Editing events with <code>ikhal</code> is a little bit cumbersome.</p> </li> <li> <p>Only rudimentary support for creating and editing recursion rules.</p> </li> <li>You cannot edit the timezones of events.</li> </ul>"}, {"location": "khal/#installation", "title": "Installation", "text": "<p>Although it's available in the major package managers, you can get a more bleeding edge version with <code>pip</code>.</p> <pre><code>pipx install khal\n</code></pre> <p>If you don't have <code>pipx</code> you can use <code>pip</code>.</p>"}, {"location": "khal/#configuration", "title": "Configuration", "text": "<p><code>khal</code> reads configuration files in the ini syntax. If you do not have a configuration file yet, running <code>khal configure</code> will launch a small, interactive tool that should help you with initial configuration of khal.</p> <p><code>khal</code> is looking for configuration files in the following places and order:</p> <ul> <li><code>$XDG_CONFIG_HOME/khal/config</code>: (on most systems this is   <code>~/.config/khal/config</code>),</li> <li><code>~/.khal/khal.conf</code> (deprecated)</li> <li>A <code>khal.conf</code> file in the current directory (deprecated).</li> </ul> <p>Alternatively you can specify which configuration file to use with <code>-c path/to/config</code> at runtime.</p>"}, {"location": "khal/#the-calendars-section", "title": "The calendars section", "text": "<p>The <code>[calendars]</code> section is mandatory and must contain at least one subsection. Every subsection must have a unique name (enclosed by two square brackets). Each subsection needs exactly one path setting, everything else is optional. Here is a small example:</p> <pre><code>[calendars]\n\n[[home]]\npath = ~/.calendars/home/\ncolor = dark green\npriority = 20\n\n[[work]]\npath = ~/.calendars/work/\nreadonly = True\n</code></pre> <p>Some properties are:</p> <ul> <li><code>path</code>: The path to an existing directory where this calendar is saved as a   vdir.</li> <li><code>color</code>: <code>khal</code> will use this color for coloring this calendar\u2019s event. The   following color names are supported: <code>black</code>, <code>white</code>, <code>brown</code>, <code>yellow</code>,   <code>dark gray</code>, <code>dark green</code>, <code>dark blue</code>, <code>light gray</code>, <code>light green</code>,   <code>light   blue</code>, <code>dark magenta</code>, <code>dark cyan</code>, <code>dark red</code>, <code>light magenta</code>,   <code>light   cyan</code>, <code>light red</code>.</li> <li><code>priority</code>: When coloring days, the color will be determined based on the   calendar with the highest priority. If the priorities are equal, then the   \u201cmultiple\u201d color will be used.</li> <li><code>readonly</code>: Setting this to True, will keep <code>khal</code> from making any changes to   this calendar.</li> </ul>"}, {"location": "khal/#the-default-section", "title": "The default section", "text": "<p>Some of this configurations do not affect <code>ikhal</code>.</p> <ul> <li><code>default_calendar</code>: The calendar to use if none is specified for some   operation (e.g. if adding a new event). If this is not set, such operations   require an explicit value.</li> <li><code>default_dayevent_duration</code>: Define the default duration for an event   (<code>khal   new</code> only). <code>1h</code> by default.</li> <li><code>default_event_duration</code>: Define the default duration for a day-long event   (<code>khal  new</code> only). <code>1d</code> by default.</li> <li><code>highlight_event_days</code>: If true, <code>khal</code> will highlight days with events.   Options for highlighting are in   highlight_days   section.</li> </ul>"}, {"location": "khal/#the-key-bindings-section", "title": "The key bindings section", "text": "<p>Key bindings for <code>ikhal</code> are set here. You can bind more than one key (combination) to a command by supplying a comma-separated list of keys. For binding key combinations concatenate them keys (with a space in between), for example <code>ctrl n</code>.</p> Action Default Description down down, j Move the cursor down (in the calendar browser). up up, k Move the cursor up (in the calendar browser). left left, h, backspace Move the cursor left (in the calendar browser). right right, l, space Move the cursor right (in the calendar browser). view enter Show details or edit (if details are already shown) the currently selected event. save meta enter Save the currently edited event and leave the event editor. quit q, Q Quit. new n Create a new event on the selected date. delete d Delete the currently selected event. search / Open a text field to start a search for events. mark v Go into highlight (visual) mode to choose a date range. other o In highlight mode go to the other end of the highlighted date range. today t Focus the calendar browser on today. duplicate p Duplicate the currently selected event. export e Export event as a .ics file. log L Show logged messages. external_edit meta E Edit the currently selected events\u2019 raw .ics file with $EDITOR <p>Use the <code>external_edit</code> with caution, the icalendar library we use doesn't do a lot of validation, it silently disregards most invalid data.</p>"}, {"location": "khal/#syncing", "title": "Syncing", "text": "<p>To get <code>khal</code> working with CalDAV you will first need to setup <code>vdirsyncer</code>. After each start <code>khal</code> will automatically check if anything has changed and automatically update its caching db (this may take some time after the initial sync, especially for large calendar collections). Therefore, you might want to execute <code>khal</code> automatically after syncing with <code>vdirsyncer</code> (for example via <code>cron</code>).</p>"}, {"location": "khal/#usage", "title": "Usage", "text": "<p><code>khal</code> offers a set of commands, most importantly:</p> <ul> <li><code>list</code>: Shows all events scheduled for a given date (or datetime) range, with   custom formatting.</li> <li><code>calendar</code>: Shows a calendar (similar to cal(1)) and list.</li> <li><code>new</code>: Allows for adding new events.</li> <li><code>search</code>: Search for events matching a search string and print them.</li> <li><code>at</code>: shows all events scheduled for a given datetime.</li> <li><code>edit</code>: An   interactive command for editing and deleting events using a search string.</li> <li><code>interactive</code>: Invokes the interactive version of <code>khal</code>, can also be invoked   by calling <code>ikhal</code>.</li> <li><code>printcalendars</code>:</li> <li><code>printformats</code></li> </ul>"}, {"location": "khal/#new", "title": "new", "text": "<pre><code>khal new [-a CALENDAR] [OPTIONS] [START [END | DELTA] [TIMEZONE] SUMMARY\n[:: DESCRIPTION]]\n</code></pre> <p>Where <code>start</code> and <code>end</code> are either datetimes, times, or keywords and times in the formats defined in the config file.</p> <p>If no calendar is given via <code>-a</code>, the default calendar is used.</p> <p>For example:</p> <pre><code>khal new 18:00 Awesome Event\n</code></pre> <p>Adds a new event starting today at 18:00 with summary <code>Awesome event</code> (lasting for the default time of one hour) to the default calendar.</p> <pre><code>khal new tomorrow 16:30 Coffee Break\n</code></pre> <p>Adds a new event tomorrow at 16:30.</p> <pre><code>khal new 25.10. 18:00 24:00 Another Event :: with Alice and Bob\n</code></pre> <p>Adds a new event on 25<sup>th</sup> of October lasting from 18:00 to 24:00 with an additional description.</p> <pre><code>khal new -a work 26.07. Great Event -g meeting -r weekly\n</code></pre> <p>Adds a new all day event on 26<sup>th</sup> of July to the calendar work in the meeting category, which recurs every week.</p>"}, {"location": "khal/#interactive", "title": "Interactive", "text": "<p>When the calendar on the left is in focus, you can:</p> <ul> <li> <p>Move through the calendar (default keybindings are the arrow keys, space and   backspace, those keybindings are configurable in the config file).</p> </li> <li> <p>Focus on the right column by pressing <code>tab</code> or <code>enter</code>.</p> </li> <li> <p>Focus on the current date, default keybinding <code>t</code> as in today.</p> </li> <li> <p>Marking a date range, default keybinding <code>v</code>, as in visual, think visual mode   in Vim, pressing <code>esc</code> escapes this visual mode.</p> </li> </ul> <p>If in visual mode, you can select the other end of the currently marked range,   default keybinding <code>o</code> as in other (again as in Vim).</p> <ul> <li> <p>Create a new event on the currently focused day (or date range if a range is   selected), default keybinding <code>n</code>.</p> </li> <li> <p>Search for events, default keybinding <code>/</code>, a pop-up will ask for your search   term.</p> </li> </ul> <p>When an event list is in focus, you can:</p> <ul> <li>View an event\u2019s details with pressing enter (or tab) and edit it with pressing   enter (or tab) again (if [view] <code>event_view_always_visible</code> is set to   <code>True</code>, the event in focus will always be shown in detail).</li> <li>Toggle an event\u2019s deletion status, default keybinding <code>d</code>, events marked for   deletion will appear with a <code>D</code> in front and will be deleted when <code>khal</code>   exits.</li> <li>Duplicate the selected event, default keybinding <code>p</code></li> <li>Export the selected event, default keybinding <code>e</code>.</li> </ul> <p>In the event editor, you can:</p> <ul> <li>Jump to the next (previous) selectable element with pressing <code>tab</code> (shift+tab)</li> <li>Quick save, default keybinding <code>meta+enter</code> (meta will probably be alt).</li> <li>Use some common editing short cuts in most text fields (ctrl+w deletes word   before cursor, ctrl+u (ctrl+k) deletes till the beginning (end) of the line,   ctrl+a (ctrl+e) will jump to the beginning (end) of the line.</li> <li>In the date and time fields you can increment and decrement the number under   the cursor with <code>ctrl+a</code> and <code>ctrl+x</code> (time in 15 minute steps)</li> <li>In the date fields you can access a miniature calendar by pressing enter.</li> <li>Activate actions by pressing enter on text enclosed by angled brackets, e.g.   \\&lt; Save &gt; (sometimes this might open a pop up).</li> </ul> <p>Pressing <code>esc</code> will cancel the current action and/or take you back to the previously shown pane (i.e. what you see when you open ikhal), if you are at the start pane, ikhal will quit on pressing esc again.</p>"}, {"location": "khal/#tricks", "title": "Tricks", "text": ""}, {"location": "khal/#edit-the-events-in-a-more-pleasant-way", "title": "Edit the events in a more pleasant way", "text": "<p>The <code>ikhal</code> event editor is not comfortable for me. I usually only change the title or the start date and in the default interface you need to press many keystrokes to make it happen.</p> <p>A patch solution is to pass a custom script on the <code>EDITOR</code> environmental variable. Assuming you have <code>questionary</code> and <code>ics</code> installed you can save the next snippet into an <code>edit_event</code> file in your <code>PATH</code>:</p> <pre><code>#!/usr/bin/python3\n\n\"\"\"Edit an ics calendar event.\"\"\"\n\nimport sys\n\nimport questionary\nfrom ics import Calendar\n\n# Load the event\nfile = sys.argv[1]\nwith open(file, \"r\") as fd:\n    calendar = Calendar(fd.read())\nevent = list(calendar.timeline)[0]\n\n# Modify the event\nevent.name = questionary.text(\"Title: \", default=event.name).ask()\nstart = questionary.text(\n    \"Start: \",\n    default=f\"{str(event.begin.hour).zfill(2)}:{str(event.begin.minute).zfill(2)}\",\n).ask()\nevent.begin = event.begin.replace(\n    hour=int(start.split(\":\")[0]), minute=int(start.split(\":\")[1])\n)\n\n# Save the event\nwith open(file, \"w\") as fd:\n    fd.writelines(calendar.serialize_iter())\n</code></pre> <p>Now if you open <code>ikhal</code> as <code>EDITOR=edit_event ikhal</code>, whenever you edit one event you'll get a better interface. Add to your <code>.zshrc</code> or <code>.bashrc</code>:</p> <pre><code>alias ikhal='EDITOR=edit_event ikhal'\n</code></pre> <p>The default keybinding for the edition is not very comfortable either, add the next snippet on your config:</p> <pre><code>[keybindings]\nexternal_edit = e\nexport = meta e\n</code></pre>"}, {"location": "khal/#references", "title": "References", "text": "<ul> <li>Docs</li> <li>Git</li> </ul>"}, {"location": "kitty/", "title": "kitty", "text": "<p>kitty is a fast, feature-rich, GPU based terminal emulator written in C and Python with nice features for the keyboard driven humans like me.</p>"}, {"location": "kitty/#installation", "title": "Installation", "text": "<p>Although it's in the official repos, the version of Debian is quite old, instead you can install it for the current user with:</p> <pre><code>curl -L https://sw.kovidgoyal.net/kitty/installer.sh | sh /dev/stdin\n</code></pre> <p>You'll need to add the next alias too to your <code>.zshrc</code> or <code>.bashrc</code></p> <pre><code>alias kitty=\"~/.local/kitty.app/bin/kitty\"\n</code></pre>"}, {"location": "kitty/#configuration", "title": "Configuration", "text": "<p>It's configuration is a simple, human editable, single file for easy reproducibility stored at <code>~/.config/kitty/kitty.conf</code></p>"}, {"location": "kitty/#print-images-in-the-terminal", "title": "Print images in the terminal", "text": "<p>Create an alias in your <code>.zshrc</code>:</p> <pre><code>alias icat=\"kitty +kitten icat\"\n</code></pre> <p></p>"}, {"location": "kitty/#colors", "title": "Colors", "text": "<p>The themes kitten allows you to easily change color themes, from a collection of almost two hundred pre-built themes available at kitty-themes. To use it run:</p> <pre><code>kitty +kitten themes\n</code></pre> <p>The kitten allows you to pick a theme, with live previews of the colors. You can choose between light and dark themes and search by theme name by just typing a few characters from the name.</p> <p>If you want to tweak some colors once you select a theme, you can use terminal sexy.</p>"}, {"location": "kitty/#make-the-background-transparent", "title": "Make the background transparent", "text": "<p>File: <code>~/.config/kitty/kitty.conf</code></p> <pre><code>background_opacity 0.85\n</code></pre> <p>A number between 0 and 1, where 1 is opaque and 0 is fully transparent. This will only work if supported by the OS (for instance, when using a compositor under X11).</p> <p>If you're using i3wm you need to configure compton</p> <p>Install it with <code>sudo apt-get install compton</code>, and configure i3 to start it in the background adding <code>exec --no-startup-id compton</code> to your i3 config.</p>"}, {"location": "kitty/#terminal-bell", "title": "Terminal bell", "text": "<p>I hate the auditive terminal bell, disable it with:</p> <pre><code>enable_audio_bell no\n</code></pre>"}, {"location": "kitty/#movement", "title": "Movement", "text": "<p>By default the movement is not vim friendly because if you use the same keystrokes, they will be captured by kitty and not forwarded to the application. The closest I got is:</p> <pre><code># Movement\n\nmap ctrl+shift+k scroll_line_up\nmap ctrl+shift+j scroll_line_down\nmap ctrl+shift+u scroll_page_up\nmap ctrl+shift+d scroll_page_down\n</code></pre> <p>If you need more fine grained movement, use the scrollback buffer.</p>"}, {"location": "kitty/#the-scrollback-buffer", "title": "The scrollback buffer", "text": "<p><code>kitty</code> supports scrolling back to view history, just like most terminals. You can use either keyboard shortcuts or the mouse scroll wheel to do so. However, kitty has an extra, neat feature. Sometimes you need to explore the scrollback buffer in more detail, maybe search for some text or refer to it side-by-side while typing in a follow-up command. kitty allows you to do this by pressing the <code>ctrl+shift+h</code> key-combination, which will open the scrollback buffer in your favorite pager program (which is less by default). Colors and text formatting are preserved. You can explore the scrollback buffer comfortably within the pager.</p> <p>To use <code>nvim</code> as the pager follow this discussion, the latest working snippet was:</p> <pre><code># Scrollback buffer\n# https://sw.kovidgoyal.net/kitty/overview/#the-scrollback-buffer\n# `bash -c '...'` Run everything in a shell taking the scrollback content on stdin\n# `-u NORC` Load plugins but not initialization files\n# `-c \"map q :qa!&lt;CR&gt;\"` Close with `q` key\n# `-c \"autocmd TermOpen * normal G\"` On opening of the embedded terminal go to last line\n# `-c \"terminal cat /proc/$$/fd/0 -\"` Open the embedded terminal and read stdin of the shell\n# `-c \"set clipboard+=unnamedplus\"` Always use clipboard to yank/put instead of having to specify +\nscrollback_pager bash -c 'nvim &lt;/dev/null -u NORC -c \"map q :qa!&lt;CR&gt;\" -c \"autocmd TermOpen * normal G\" -c \"terminal cat /proc/$$/fd/0 -\" -c \"set clipboard+=unnamedplus\" -c \"call cursor(CURSOR_LINE, CURSOR_COLUMN)\"'\n</code></pre> <p>To make the history scrollback infinite add the next lines:</p> <pre><code>scrollback_lines -1\nscrollback_pager_history_size 0\n</code></pre>"}, {"location": "kitty/#clipboard-management", "title": "Clipboard management", "text": "<pre><code># Clipboard\nmap ctrl+v        paste_from_clipboard\n</code></pre>"}, {"location": "kitty/#fonts", "title": "Fonts", "text": "<ul> <li>Add your fonts to the <code>~/.local/share/fonts</code> directory</li> <li>Check they are available when you run <code>kitty +list-fonts</code></li> <li>Add them to your config:</li> </ul> <pre><code>font_family      Operator Mono Book\nbold_font        Operator Mono Medium\nitalic_font      Operator Mono Book Italic\nbold_italic_font Operator Mono Medium Italic\n</code></pre>"}, {"location": "kitty/#troubleshooting", "title": "Troubleshooting", "text": ""}, {"location": "kitty/#scrollback-when-ssh-into-a-machine-doesnt-work", "title": "Scrollback when ssh into a machine doesn't work", "text": "<p>This happens because the kitty terminfo files are not available on the server. You can ssh in using the following command which will automatically copy the terminfo files to the server:</p> <pre><code>kitty +kitten ssh myserver\n</code></pre> <p>This ssh kitten takes all the same command line arguments as ssh, you can alias it to ssh in your shell\u2019s rc files to avoid having to type it each time:</p> <pre><code>alias ssh=\"kitty +kitten ssh\"\n</code></pre>"}, {"location": "kitty/#screen-not-working-on-server-with-sudo", "title": "Screen not working on server with sudo", "text": "<p>Make sure you're using the ssh alias below</p> <pre><code>alias ssh=\"kitty +kitten ssh\"\n</code></pre> <p>And then copy the <code>~/.terminfo</code> into <code>/root</code></p> <pre><code>sudo copy -r ~/.terminfo /root\n</code></pre>"}, {"location": "kitty/#reasons-to-migrate-from-urxvt-to-kitty", "title": "Reasons to migrate from urxvt to kitty", "text": "<ul> <li>It doesn't fuck up your terminal colors.</li> <li>You can use peek to record your screen.</li> <li>Easier to extend.</li> </ul>"}, {"location": "kitty/#references", "title": "References", "text": "<ul> <li>Homepage</li> </ul>"}, {"location": "kodi/", "title": "Kodi", "text": "<p>Kodi is a entertainment center software. It basically converts your device into a smart tv</p>"}, {"location": "kodi/#installation", "title": "Installation", "text": "<p>If you're trying to install it on Debian based distros (not ubuntu) check the official docs</p> <pre><code>sudo apt install software-properties-common\nsudo add-apt-repository -y ppa:team-xbmc/ppa\nsudo apt install kodi\n</code></pre>"}, {"location": "kodi/#troubleshooting", "title": "Troubleshooting", "text": ""}, {"location": "kodi/#movie-not-recognized-by-kodi", "title": "Movie not recognized by kodi", "text": "<p>Add your own .nfo file with the metadata</p>"}, {"location": "kodi/#import-data-from-nfo-files", "title": "Import data from nfo files", "text": "<p>If the nfo is separated on each movie, you have to remove it from the library and import it again, as the scanning doesn't import the data from the nfos.</p>"}, {"location": "kodi/#tv-show-file-naming", "title": "TV show file naming", "text": "<p>The correct TV show file naming</p>"}, {"location": "kodi/#references", "title": "References", "text": "<ul> <li>Home</li> </ul>"}, {"location": "koel/", "title": "Koel", "text": "<p>koel is a personal music streaming server.</p> <p>Note: Use <code>mopidy</code> instead</p>"}, {"location": "koel/#installation", "title": "Installation", "text": "<p>There are docker-compose files to host the service. Although they behave a little bit weird</p> <p>For example, you need to specify the DB_PORT. It has had several PR to fix it but weren't merged 1, 2.</p>"}, {"location": "koel/#api", "title": "API", "text": "<p>The API is not very well documented:</p> <ul> <li>Here you can see how to authenticate</li> <li>Here are the api docs</li> </ul>"}, {"location": "koel/#references", "title": "References", "text": "<ul> <li>Home</li> <li>Docs</li> <li>Source</li> </ul>"}, {"location": "krew/", "title": "Krew", "text": "<p>Krew is a tool that makes it easy to use kubectl plugins. Krew helps you discover plugins, install and manage them on your machine. It is similar to tools like apt, dnf or brew.</p>"}, {"location": "krew/#installation", "title": "Installation", "text": "<ol> <li> <p>Run this command to download and install krew:</p> <pre><code>(\nset -x; cd \"$(mktemp -d)\" &amp;&amp;\nOS=\"$(uname | tr '[:upper:]' '[:lower:]')\" &amp;&amp;\nARCH=\"$(uname -m | sed -e 's/x86_64/amd64/' -e 's/\\(arm\\)\\(64\\)\\?.*/\\1\\2/' -e 's/aarch64$/arm64/')\" &amp;&amp;\nKREW=\"krew-${OS}_${ARCH}\" &amp;&amp;\ncurl -fsSLO \"https://github.com/kubernetes-sigs/krew/releases/latest/download/${KREW}.tar.gz\" &amp;&amp;\ntar zxvf \"${KREW}.tar.gz\" &amp;&amp;\n./\"${KREW}\" install krew\n)\n</code></pre> </li> <li> <p>Add the <code>$HOME/.krew/bin</code> directory to your PATH environment variable. To do    this, update your <code>.bashrc</code> or <code>.zshrc</code> file and append the following line:</p> </li> </ol> <pre><code>export PATH=\"${KREW_ROOT:-$HOME/.krew}/bin:$PATH\"\n</code></pre> <ol> <li> <p>Restart your shell.</p> </li> <li> <p>Run <code>kubectl krew</code> to check the installation.</p> </li> </ol>"}, {"location": "krew/#references", "title": "References", "text": "<ul> <li>Git</li> <li>Docs</li> </ul>"}, {"location": "ksniff/", "title": "Ksniff", "text": "<p>Ksniff is a Kubectl plugin to ease sniffing on kubernetes pods using tcpdump and wireshark.</p>"}, {"location": "ksniff/#installation", "title": "Installation", "text": "<p>Recommended installation is done via krew</p> <pre><code>kubectl krew install sniff\n</code></pre> <p>For manual installation, download the latest release package, unzip it and use the attached makefile:</p> <pre><code>unzip ksniff.zip\nmake install\n</code></pre> <p>(I tried doing it manually and it failed for me).</p>"}, {"location": "ksniff/#usage", "title": "Usage", "text": "<pre><code>kubectl sniff &lt;POD_NAME&gt; [-n &lt;NAMESPACE_NAME&gt;] [-c &lt;CONTAINER_NAME&gt;] [-i &lt;INTERFACE_NAME&gt;] [-f &lt;CAPTURE_FILTER&gt;] [-o OUTPUT_FILE] [-l LOCAL_TCPDUMP_FILE] [-r REMOTE_TCPDUMP_FILE]\n\nPOD_NAME: Required. the name of the kubernetes pod to start capture it's traffic.\nNAMESPACE_NAME: Optional. Namespace name. used to specify the target namespace to operate on.\nCONTAINER_NAME: Optional. If omitted, the first container in the pod will be chosen.\nINTERFACE_NAME: Optional. Pod Interface to capture from. If omitted, all Pod interfaces will be captured.\nCAPTURE_FILTER: Optional. specify a specific tcpdump capture filter. If omitted no filter will be used.\nOUTPUT_FILE: Optional. if specified, ksniff will redirect tcpdump output to local file instead of wireshark. Use '-' for stdout.\nLOCAL_TCPDUMP_FILE: Optional. if specified, ksniff will use this path as the local path of the static tcpdump binary.\nREMOTE_TCPDUMP_FILE: Optional. if specified, ksniff will use the specified path as the remote path to upload static tcpdump to.\n</code></pre> <p>You'll need to remove the pods manually once you've finished analyzing the traffic.</p>"}, {"location": "ksniff/#issues", "title": "Issues", "text": ""}, {"location": "ksniff/#wtap_encap-0", "title": "<code>WTAP_ENCAP = 0</code>", "text": "<p>Upgrade your wireshark to a version greater or equal to <code>3.3.0</code>.</p>"}, {"location": "ksniff/#references", "title": "References", "text": "<ul> <li>Git</li> </ul>"}, {"location": "kubernetes_debugging/", "title": "Kubernetes Debugging", "text": ""}, {"location": "kubernetes_debugging/#network-debugging", "title": "Network debugging", "text": "<p>Sometimes you need to monitor the network traffic that goes between pods to solve an issue. There are different ways to see it:</p> <ul> <li>Using Mizu</li> <li>Running tcpdump against a running container</li> <li>Using ksniff</li> <li>Using ephemeral debug containers</li> </ul> <p>Of all the solutions, the cleaner and easier is to use Mizu.</p>"}, {"location": "kubernetes_debugging/#running-tcpdump-against-a-running-container", "title": "Running tcpdump against a running container", "text": "<p>If the pod you want to analyze has root permissions (bad idea) you'll be able to install <code>tcpdump</code> (<code>apt-get install tcpdump</code>) and pipe it into <code>wireshark</code> on your local machine.</p> <pre><code>kubectl exec my-app-pod -- tcpdump -i eth0 -w - | wireshark -k -i -\n</code></pre> <p>There's some issues with this, though:</p> <ul> <li>You have to <code>kubectl exec</code> and install arbitrary software from the internet on     a running Pod. This is fine for internet-connected dev environments, but     probably not something you'd want to do (or be able to do) in production.</li> <li>If this app had been using a minimal <code>distroless</code> base image or was built with     a <code>buildpack</code> you won't be able to install <code>tcpdump</code>.</li> </ul>"}, {"location": "kubernetes_debugging/#using-ephemeral-debug-containers", "title": "Using ephemeral debug containers", "text": "<p>Kubernetes 1.16 has a new Ephemeral Containers feature that is perfect for our use case. With Ephemeral Containers, we can ask for a new temporary container with the image of our choosing to run inside an existing Pod. This means we can keep the main images for our applications lightweight and then bolt on a heavy image with all of our favorite debug tools when necessary.</p> <pre><code>kubectl debug -it pod-to-debug-id --image=nicolaka/netshoot --target=pod-to-debug -- tcpdump -i eth0 -w - | wireshark -k -i\n</code></pre> <p>Where <code>nicolaka/netshoot</code> is an optimized network troubleshooting docker.</p> <p>There's some issues with this too, for example, as of Kubernetes 1.21 Ephemeral containers are not enabled by default, so chances are you won't have access to them yet in your environment.</p>"}, {"location": "laboral/", "title": "Laboral", "text": ""}, {"location": "laboral/#references", "title": "References", "text": "<ul> <li>Unai video on laboral struggles in the IT world</li> </ul>"}, {"location": "lazy_loading/", "title": "Lazy evaluation", "text": "<p>Lazy loading is an programming implementation paradigm which delays the evaluation of an expression until its value is needed and which also avoids repeated evaluations.</p> <p>Lazy evaluation is the preferred implementation when the operation is expensive, requiring either extensive processing time or memory. For example, in Python, one of the best-known techniques involving lazy evaluation is generators. Instead of creating whole sequences for the iteration, which can consume lots of memory, generators lazily evaluate the current need and yield one element at a time when requested.</p> <p>Other example are attributes that take long to compute:</p> <pre><code>class Person:\n    def __init__(self, name, occupation):\n        self.name = name\n        self.occupation = occupation\n        self.relatives = self._get_all_relatives()\n\n    def _get_all_relatives():\n        ...\n        # This is an expensive operation\n</code></pre> <p>This approach may cause initialization to take unnecessarily long, especially when you don't always need to access <code>Person.relatives</code>.</p> <p>A better strategy would be to get relatives when it's needed.</p> <pre><code>class Person:\n    def __init__(self, name, occupation):\n        self.name = name\n        self.occupation = occupation\n        self._relatives = None\n\n    @property\n    def relatives(self):\n        if self._relatives is None:\n            self._relatives = ... # Get all relatives\n        return self._relatives\n</code></pre> <p>In this case, the list of relatives is computed the first time <code>Person.relatives</code> is accessed. After that, it's stored in <code>Person._relatives</code> to prevent repeated evaluations.</p> <p>A perhaps more Pythonic approach would be to use a decorator that makes a property lazy-evaluated.</p> <pre><code>def lazy_property(fn):\n'''Decorator that makes a property lazy-evaluated.\n    '''\n    attr_name = '_lazy_' + fn.__name__\n\n    @property\n    def _lazy_property(self):\n        if not hasattr(self, attr_name):\n            setattr(self, attr_name, fn(self))\n        return getattr(self, attr_name)\n    return _lazy_property\n\nclass Person:\n    def __init__(self, name, occupation):\n        self.name = name\n        self.occupation = occupation\n\n    @lazy_property\n    def relatives(self):\n        # Get all relatives\n        relatives = ...\n        return relatives\n</code></pre> <p>This removes a lot of boilerplate, especially when an object has many lazily-evaluated properties.</p> <p>Another approach is to use the getattr special method.</p>"}, {"location": "lazy_loading/#references", "title": "References", "text": "<ul> <li>Steven Loria article on Lazy Properties</li> <li>Yong Cui article on Lazy attributes</li> </ul>"}, {"location": "letsencrypt/", "title": "letsencrypt", "text": "<p>Letsencrypt is a free, automated, and open certificate authority brought to you by the nonprofit Internet Security Research Group (ISRG). Basically it gives away SSL certificates, which are required to configure webservers to use HTTPS instead of HTTP for example.</p>"}, {"location": "letsencrypt/#configure-a-wildcard-dns-when-the-provider-is-not-supported", "title": "Configure a wildcard dns when the provider is not supported", "text": "<p>If you\u2019d like to obtain a wildcard certificate from Let\u2019s Encrypt or run certbot on a machine other than your target webserver, you can use one of Certbot\u2019s DNS plugins.</p> <p>They support some DNS providers and a generic protocol if your DNS provider is not in the first list and it doesn't either support RFC 2136 you need to set the manual renewal of certificates. Keep in mind though that Let's Encrypt doesn't support HTTP validation for wildcard domains.</p> <p>To do so you first need to install certbot. Of the different ways to do it, the cleanest for this case is to use docker (given that you're already using it and don't mind shutting down your web application service so that let's encrypt docker can bind to ports 80 or 443). I'd prefer not to bring down the service for this purpose. Even if it is just once each 2 months, because I feel that the automation of this process will be more difficult in the end. The option we have left is to install it with <code>pip</code> but as we want a clean environment, it's better to use <code>pipx</code>.</p> <pre><code>pipx install certbot\n</code></pre>"}, {"location": "libreelec/", "title": "LibreElec", "text": "<p>LibreElec is the lightweight distribution to run Kodi</p> <p>The root filesystem is mounted as readonly.</p>"}, {"location": "libreelec/#mount-directories-with-sshfs", "title": "Mount directories with sshfs", "text": "<ul> <li>Install the network-tool LibreElec addon.</li> <li>Configure the ssh credentials</li> <li>Add the following service file:     <code>/storage/.config/system.d/storage-media.mount</code></li> </ul> <pre><code>[Unit]\nDescription=remote external drive share\nRequires=multi-user.target network-online.service\nAfter=multi-user.target network-online.service\nBefore=kodi.service\n\n[Mount]\nWhat=/storage/.kodi/addons/virtual.network-tools/bin/sshfs#{{ user }}@{{ host }}:{{ source_path }}\nWhere=/storage/media\n\n[Install]\nWantedBy=multi-user.target\n</code></pre>"}, {"location": "libretube/", "title": "LibreTube", "text": "<p>Libretube is an alternative frontend for YouTube, for Android. </p> <p>YouTube has an extremely invasive privacy policy which relies on using user data in unethical ways. They store a lot of your personal data - ranging from ideas, music taste, content, political opinions, and much more than you think.</p> <p>This project is aimed at improving the users' privacy by being independent from Google and bypassing their data collection.</p> <p>Therefore, the app is using the Piped API, which uses proxies to circumvent Google's data collection and includes some other additional features.</p>"}, {"location": "libretube/#differences-to-newpipe", "title": "Differences to NewPipe", "text": "<p>With NewPipe, the extraction is done locally on your phone, and all the requests sent towards YouTube/Google are done directly from the network you're connected to, which doesn't use a middleman server in between. Therefore, Google can still access information such as the user's IP address. Aside from that, subscriptions can only be stored locally.</p> <p>LibreTube takes this one step further and proxies all requests via Piped (which uses the NewPipeExtractor). This prevents Google servers from accessing your IP address or any other personal data. Apart from that, Piped allows syncing your subscriptions between LibreTube and Piped, which can be used on desktop too.</p> <p>If the NewPipeExtractor breaks, it only requires an update of Piped and not LibreTube itself. Therefore, fixes usually arrive faster than in NewPipe.</p> <p>While LibreTube only supports YouTube, NewPipe also allows the use of other platforms like SoundCloud, PeerTube, Bandcamp and media.ccc.de. Both are great clients for watching YouTube videos. It depends on the individual's use case which one fits their needs better.</p>"}, {"location": "libretube/#other-software-that-uses-piped", "title": "Other software that uses Piped", "text": "<ul> <li>Yattee - an alternative frontend for YouTube, for IOS.</li> <li>Hyperpipe - an alternative privacy respecting frontend for YouTube Music.</li> <li>Musicale - an alternative to YouTube Music, with style.</li> <li>ytify - a complementary minimal audio streaming frontend for YouTube.</li> <li>PsTube - Watch and download videos without ads on Android, Linux, Windows, iOS, and Mac OSX.</li> <li>Piped-Material - A fork of Piped, focusing on better performance and a more usable design.</li> <li>ReacTube - Privacy friendly &amp; distraction free Youtube front-end using Piped API.</li> </ul>"}, {"location": "libretube/#references", "title": "References", "text": "<ul> <li>Source</li> </ul>"}, {"location": "life_logging/", "title": "Life logging", "text": ""}, {"location": "life_logging/#mobile-call-log-and-sms-log", "title": "Mobile call log and sms log", "text": "<p>Use Slight backup app.</p>"}, {"location": "life_management/", "title": "Life Management", "text": "<p>I understand life management as the act of analyzing yourself and your interactions with the world to define processes and automations that shape the way to efficiently achieve your goals.</p>"}, {"location": "life_review/", "title": "Life review", "text": "<p>Sometimes is good to stop, get into your cave and do an introspection on how is your life going.</p> <p>I like to do this exercise the last week of the year. Although I'd like to do it at least twice a year.</p> <p>This article is the checklist I follow to do my life review, it may seem a lot to you or maybe very simple. You can take it as a base or maybe to get some ideas and then create your own that fits your needs.</p> <p>The process then has many phases:</p> <ul> <li>Housekeeping</li> <li>Analysis</li> <li>Planning</li> </ul>"}, {"location": "life_review/#housekeeping", "title": "Housekeeping", "text": "<p>As they are time expensive, probably lot of time may have passed since your last life review, it's a good time to do some housekeeping tasks to have a tidy environment (and data!) before you start analyzing everything.</p> <ul> <li>Extract all your media (photos, videos, text) from all your devices (mobiles,   laptops, servers) to your central archive. For the portable devices I use   <code>syncthing</code> to sync all the important data to the   NAS, although it's usually untidy.</li> <li>Once it's extracted tidy them all. For example you could group the pictures   and videos in a tree of directories (<code>trips</code>, <code>trips/2022-06-Glasgow</code>,   <code>trips/2022-06-Glasgow/2022-06-10-hiking-trail-lakes</code>, ...). As this is an   unpleasant task I've created <code>claspy</code> a command line tool that helps you   categorize the files into their desired paths. Do the same for the documents,   music, binaries... everything! until you have an empty mobile and empty   <code>Downloads</code> directory.</li> <li>Update your ledger so that it reflects the reality.</li> <li>Update your task manager systems so that it reflects the   latest state.</li> <li>Update your digital garden so that you don't have any   uncommited changes.</li> </ul>"}, {"location": "life_review/#analysis", "title": "Analysis", "text": "<p>To do it I gather all the information from my life logging sources and start thinking of what do I want to change. It helps me to write a markdown document with the insights gathered in this process.</p>"}, {"location": "life_review/#what-you-have-learned", "title": "What you have learned", "text": "<p>It's always interesting to look back and see what you've learned throughout the year. I have these sources of data:</p> <ul> <li>Digital garden</li> <li>Anki</li> </ul>"}, {"location": "life_review/#digital-garden", "title": "Digital garden", "text": "<p>If you happen to have a digital garden you can look at your git history to know what has changed since the last year. That's cumbersome and ugly though, it's better to review your newsletters, although you may need to use something like <code>mkdocs-newsletter</code>.</p> <p>While you skim through the newsletters you can add to the analysis report the highlights of what you've learned.</p> <p>You can also check your repository insights.</p>"}, {"location": "life_review/#anki", "title": "Anki", "text": "<p>I use <code>anki</code> to record the knowledge that I need to have in my mind. The program has a \"Stats\" tab where you can see your insights of the last years to understand how are you learning. You can also go to the \"Browse\" tab to sort the cards by created and get an idea of which ones have been the most used decks.</p>"}, {"location": "life_review/#what-youve-read", "title": "What you've read", "text": "<p>Each time I finish a book I register it in a document with a rating and optionally a review. When doing the review I check which ones I read, which ones I liked more, what genres have been the most popular for me, which authors. With these data I create an analysis of what seems promising to read in the future.</p> <p>I also update the section of \"what you've learnt\" with the insights of these books.</p>"}, {"location": "life_review/#task-review", "title": "Task review", "text": "<p>Follow the review process of the task management article.</p>"}, {"location": "life_review/#planning", "title": "Planning", "text": ""}, {"location": "life_review/#what-to-read", "title": "What to read", "text": "<p>With the analysis of what I've read I research for new books and create an ordered list per genre.</p>"}, {"location": "lindy/", "title": "Lindy Hop", "text": ""}, {"location": "lindy/#awesome-dancers", "title": "Awesome dancers", "text": ""}, {"location": "lindy/#charleston", "title": "Charleston", "text": "<ul> <li>The DecaVita Sisters:<ul> <li>Freestyle Lindy Hop &amp; Charleston</li> <li>Moby \"Honey\"</li> </ul> </li> </ul>"}, {"location": "lindy/#solo-jazz", "title": "Solo Jazz", "text": "<ul> <li>Pedro Vieira at Little Big Swing Camp 2022</li> </ul>"}, {"location": "lindy/#lindy-hop", "title": "Lindy Hop", "text": "<ul> <li>The DecaVita Sisters:<ul> <li>Compromise - agreement in the moment</li> <li>Lindy hop improv</li> </ul> </li> </ul>"}, {"location": "linux/", "title": "Linux", "text": ""}, {"location": "linux/#references", "title": "References", "text": ""}, {"location": "linux/#learning", "title": "Learning", "text": "<ul> <li>https://explainshell.com/</li> <li>https://linuxcommandlibrary.com/</li> </ul>"}, {"location": "linux_snippets/", "title": "Linux snippets", "text": ""}, {"location": "linux_snippets/#limit-the-resources-a-docker-is-using", "title": "Limit the resources a docker is using", "text": "<p>You can either use limits in the <code>docker</code> service itself, see 1 and 2.</p> <p>Or/and you can limit it for each docker, see 1 and 2.</p>"}, {"location": "linux_snippets/#get-the-current-git-branch", "title": "Get the current git branch", "text": "<pre><code>git branch --show-current\n</code></pre>"}, {"location": "linux_snippets/#install-latest-version-of-package-from-backports", "title": "Install latest version of package from backports", "text": "<p>Add the backports repository:</p> <pre><code>vi /etc/apt/sources.list.d/bullseye-backports.list\n</code></pre> <pre><code>deb http://deb.debian.org/debian bullseye-backports main contrib\ndeb-src http://deb.debian.org/debian bullseye-backports main contrib\n</code></pre> <p>Configure the package to be pulled from backports</p> <pre><code>vi /etc/apt/preferences.d/90_zfs\n</code></pre> <pre><code>Package: src:zfs-linux\nPin: release n=bullseye-backports\nPin-Priority: 990\n</code></pre>"}, {"location": "linux_snippets/#rename-multiple-files-matching-a-pattern", "title": "Rename multiple files matching a pattern", "text": "<p>There is <code>rename</code> that looks nice, but you need to install it. Using only <code>find</code> you can do:</p> <pre><code>find . -name '*yml' -exec bash -c 'echo mv $0 ${0/yml/yaml}' {} \\; </code></pre> <p>If it shows what you expect, remove the <code>echo</code>.</p>"}, {"location": "linux_snippets/#force-ssh-to-use-password-authentication", "title": "Force ssh to use password authentication", "text": "<pre><code>ssh -o PreferredAuthentications=password -o PubkeyAuthentication=no exampleUser@example.com\n</code></pre>"}, {"location": "linux_snippets/#do-a-tail-f-with-grep", "title": "Do a tail -f with grep", "text": "<pre><code>tail -f file | grep --line-buffered my_pattern\n</code></pre>"}, {"location": "linux_snippets/#check-if-a-program-exists-in-the-users-path", "title": "Check if a program exists in the user's PATH", "text": "<pre><code>command -v &lt;the_command&gt;\n</code></pre> <p>Example use:</p> <pre><code>if ! command -v &lt;the_command&gt; &amp;&gt; /dev/null\nthen\necho \"&lt;the_command&gt; could not be found\"\nexit\nfi\n</code></pre>"}, {"location": "linux_snippets/#reset-failed-systemd-services", "title": "Reset failed systemd services", "text": "<p>Use systemctl to remove the failed status. To reset all units with failed status:</p> <pre><code>systemctl reset-failed\n</code></pre> <p>or just your specific unit:</p> <pre><code>systemctl reset-failed openvpn-server@intranert.service\n</code></pre>"}, {"location": "linux_snippets/#automatic-reboot-after-power-failure", "title": "Automatic reboot after power failure", "text": "<p>That's not something you can control in your operating system. That's what the BIOS is for. In most BIOS setups there'll be an option like After power loss with possible values like Power off and Reboot.</p> <p>You can also edit <code>/etc/default/grub</code> and add:</p> <pre><code>GRUB_RECORDFAIL_TIMEOUT=5\n</code></pre> <p>Then run:</p> <pre><code>sudo update-grub\n</code></pre> <p>This will make your machine display the boot options for 5 seconds before it boot the default option (instead of waiting forever for you to choose one).</p>"}, {"location": "linux_snippets/#ssh-tunnel", "title": "SSH tunnel", "text": "<pre><code>ssh -D 9090 -N -f user@host\n</code></pre> <p>If you need to forward an external port to a local one you can use</p> <pre><code>ssh -L LOCAL_PORT:DESTINATION:DESTINATION_PORT [USER@]SSH_SERVER\n</code></pre> <p>If you need a more powerful solution you can try sshuttle</p>"}, {"location": "linux_snippets/#fix-the-ssh-client-kex_exchange_identification-read-connection-reset-by-peer-error", "title": "Fix the SSH client kex_exchange_identification: read: Connection reset by peer error", "text": "<p>Restart the <code>ssh</code> service.</p>"}, {"location": "linux_snippets/#get-class-of-a-window", "title": "Get class of a window", "text": "<p>Use <code>xprop</code> and click the window.</p>"}, {"location": "linux_snippets/#change-the-brightness-of-the-screen", "title": "Change the brightness of the screen", "text": "<p>Get the current brightness level with <code>cat /sys/class/backlight/intel_backlight/brightness</code>. Imagine it's <code>1550</code>, then if you want to lower the brightness use:</p> <pre><code>sudo echo 500 &gt; /sys/class/backlight/intel_backlight/brightness\n</code></pre>"}, {"location": "linux_snippets/#force-umount-nfs-mounted-directory", "title": "Force umount nfs mounted directory", "text": "<pre><code>umount -l path/to/mounted/dir\n</code></pre>"}, {"location": "linux_snippets/#configure-fstab-to-mount-nfs", "title": "Configure fstab to mount nfs", "text": "<p>NFS stands for \u2018Network File System\u2019. This mechanism allows Unix machines to share files and directories over the network. Using this feature, a Linux machine can mount a remote directory (residing in a NFS server machine) just like a local directory and can access files from it.</p> <p>An NFS share can be mounted on a machine by adding a line to the <code>/etc/fstab</code> file.</p> <p>The default syntax for <code>fstab</code> entry of NFS mounts is as follows.</p> <pre><code>Server:/path/to/export /local_mountpoint nfs &lt;options&gt; 0 0\n</code></pre> <p>Where:</p> <ul> <li><code>Server</code>: The hostname or IP address of the NFS server where the exported directory resides.</li> <li><code>/path/to/export</code>: The shared directory (exported folder) path.</li> <li><code>/local_mountpoint</code>: Existing directory in the host where you want to mount the NFS share.</li> </ul> <p>You can specify a number of options that you want to set on the NFS mount:</p> <ul> <li><code>soft/hard</code>: When the mount option <code>hard</code> is set, if the NFS server crashes or becomes unresponsive, the NFS requests will be retried indefinitely. You can set the mount option <code>intr</code>, so that the process can be interrupted. When the NFS server comes back online, the process can be continued from where it was while the server became unresponsive.</li> </ul> <p>When the option <code>soft</code> is set, the process will be reported an error when the NFS server is unresponsive after waiting for a period of time (defined by the <code>timeo</code> option). In certain cases <code>soft</code> option can cause data corruption and loss of data. So, it is recommended to use <code>hard</code> and <code>intr</code> options.</p> <ul> <li><code>noexec</code>: Prevents execution of binaries on mounted file systems. This is useful if the system is mounting a non-Linux file system via NFS containing incompatible binaries.</li> <li><code>nosuid</code>: Disables set-user-identifier or set-group-identifier bits. This prevents remote users from gaining higher privileges by running a setuid program.</li> <li><code>tcp</code>: Specifies the NFS mount to use the TCP protocol.</li> <li><code>udp</code>: Specifies the NFS mount to use the UDP protocol.</li> <li><code>nofail</code>: Prevent issues when rebooting the host. The downside is that if you have services that depend on the volume to be mounted they won't behave as expected.</li> </ul>"}, {"location": "linux_snippets/#fix-limit-on-the-number-of-inotify-watches", "title": "Fix limit on the number of inotify watches", "text": "<p>Programs that sync files such as dropbox, git etc use inotify to notice changes to the file system. The limit can be see by -</p> <pre><code>cat /proc/sys/fs/inotify/max_user_watches\n</code></pre> <p>For me, it shows <code>65536</code>. When this limit is not enough to monitor all files inside a directory it throws this error.</p> <p>If you want to increase the amount of inotify watchers, run the following in a terminal:</p> <pre><code>echo fs.inotify.max_user_watches=100000 | sudo tee -a /etc/sysctl.conf &amp;&amp; sudo sysctl -p\n</code></pre> <p>Where <code>100000</code> is the desired number of inotify watches.</p>"}, {"location": "linux_snippets/#what-is-varlogtallylog", "title": "What is <code>/var/log/tallylog</code>", "text": "<p><code>/var/log/tallylog</code> is the file where the <code>PAM</code> linux module (used for authentication of the machine) keeps track of the failed ssh logins in order to temporarily block users.</p>"}, {"location": "linux_snippets/#manage-users", "title": "Manage users", "text": "<ul> <li>Change main group of user</li> </ul> <pre><code>usermod -g {{ group_name }} {{ user_name }}\n</code></pre> <ul> <li>Add user to group</li> </ul> <pre><code>usermod -a -G {{ group_name }} {{ user_name }}\n</code></pre> <ul> <li>Remove user from group.</li> </ul> <pre><code>usermod -G {{ remaining_group_names }} {{ user_name }}\n</code></pre> <p>You have to execute <code>groups {{ user }}</code> get the list and pass the remaining to the above command</p> <ul> <li>Change uid and gid of the user</li> </ul> <pre><code>usermod -u {{ newuid }} {{ login }}\ngroupmod -g {{ newgid }} {{ group }}\nfind / -user {{ olduid }} -exec chown -h {{ newuid }} {} \\;\nfind / -group {{ oldgid }} -exec chgrp -h {{ newgid }} {} \\;\nusermod -g {{ newgid }} {{ login }}\n</code></pre>"}, {"location": "linux_snippets/#manage-ssh-keys", "title": "Manage ssh keys", "text": "<ul> <li>Generate ed25519 key</li> </ul> <pre><code>ssh-keygen -t ed25519 -f {{ path_to_keyfile }}\n</code></pre> <ul> <li>Generate RSA key</li> </ul> <pre><code>ssh-keygen -t rsa -b 4096 -o -a 100 -f {{ path_to_keyfile }}\n</code></pre> <ul> <li>Generate different comment</li> </ul> <pre><code>ssh-keygen -t ed25519 -f {{ path_to_keyfile }} -C {{ email }}\n</code></pre> <ul> <li>Generate key headless, batch</li> </ul> <pre><code>ssh-keygen -t ed25519 -f {{ path_to_keyfile }} -q -N \"\"\n</code></pre> <ul> <li>Generate public key from private key</li> </ul> <pre><code>ssh-keygen -y -f {{ path_to_keyfile }} &gt; {{ path_to_public_key_file }}\n</code></pre> <ul> <li>Get fingerprint of key   <pre><code>ssh-keygen -lf {{ path_to_key }}\n</code></pre></li> </ul>"}, {"location": "linux_snippets/#measure-the-network-performance-between-two-machines", "title": "Measure the network performance between two machines", "text": "<p>Install <code>iperf3</code> with <code>apt-get install iperf3</code> on both server and client.</p> <p>On the server system run:</p> <pre><code>server#: iperf3 -i 10 -s\n</code></pre> <p>Where:</p> <ul> <li><code>-i</code>: the interval to provide periodic bandwidth updates</li> <li><code>-s</code>: listen as a server</li> </ul> <p>On the client system:</p> <pre><code>client#: iperf3 -i 10 -w 1M -t 60 -c [server hostname or ip address]\n</code></pre> <p>Where:</p> <ul> <li><code>-i</code>: the interval to provide periodic bandwidth updates</li> <li><code>-w</code>: the socket buffer size (which affects the TCP Window). The buffer size is also set on the server by this client command.</li> <li><code>-t</code>: the time to run the test in seconds</li> <li><code>-c</code>: connect to a listening server at\u2026</li> </ul> <p>Sometimes is interesting to test both ways as they may return different outcomes</p> <p>I've got the next results at home:</p> <ul> <li>From new NAS to laptop through wifi 67.5 MB/s</li> <li>From laptop to new NAS 59.25 MB/s</li> <li>From intel Nuc to new NAS 116.75 MB/s (934Mbit/s)</li> <li>From old NAS to new NAS 11 MB/s</li> </ul>"}, {"location": "linux_snippets/#measure-the-performance-iops-of-a-disk", "title": "Measure the performance, IOPS of a disk", "text": "<p>To measure disk IOPS performance in Linux, you can use the <code>fio</code> tool. Install it with</p> <pre><code>apt-get install fio\n</code></pre> <p>Then you need to go to the directory where your disk is mounted. The test is done by performing read/write operations in this directory.</p> <p>To do a random read/write operation test an 8 GB file will be created. Then <code>fio</code> will read/write a 4KB block (a standard block size) with the 75/25% by the number of reads and writes operations and measure the performance. </p> <pre><code>fio --randrepeat=1 --ioengine=libaio --direct=1 --gtod_reduce=1 --name=fiotest --filename=testfio --bs=4k --iodepth=64 --size=8G --readwrite=randrw --rwmixread=75\n</code></pre> <p>I've run this test in different environments with awesome results:</p> <ul> <li> <p>New NAS server NVME:    <pre><code>read: IOPS=297k, BW=1159MiB/s (1215MB/s)(3070MiB/2649msec)\n bw (  MiB/s): min= 1096, max= 1197, per=99.80%, avg=1156.61, stdev=45.31, samples=5\n iops        : min=280708, max=306542, avg=296092.00, stdev=11598.11, samples=5\nwrite: IOPS=99.2k, BW=387MiB/s (406MB/s)(1026MiB/2649msec); 0 zone resets\n bw (  KiB/s): min=373600, max=408136, per=99.91%, avg=396248.00, stdev=15836.85, samples=5\n iops        : min=93400, max=102034, avg=99062.00, stdev=3959.21, samples=5\ncpu          : usr=15.67%, sys=67.18%, ctx=233314, majf=0, minf=8\n</code></pre></p> </li> <li> <p>New NAS server ZFS pool with RAIDZ:</p> </li> </ul> <pre><code>read: IOPS=271k, BW=1059MiB/s (1111MB/s)(3070MiB/2898msec)\n bw (  MiB/s): min=  490, max= 1205, per=98.05%, avg=1038.65, stdev=306.74, samples=5\n iops        : min=125672, max=308484, avg=265893.20, stdev=78526.52, samples=5\nwrite: IOPS=90.6k, BW=354MiB/s (371MB/s)(1026MiB/2898msec); 0 zone resets\n bw (  KiB/s): min=167168, max=411776, per=98.26%, avg=356236.80, stdev=105826.16, samples=5\n iops        : min=41792, max=102944, avg=89059.20, stdev=26456.54, samples=5\ncpu          : usr=12.84%, sys=63.20%, ctx=234345, majf=0, minf=6\n</code></pre> <ul> <li>Laptop NVME:</li> </ul> <pre><code>read: IOPS=36.8k, BW=144MiB/s (151MB/s)(3070MiB/21357msec)\n bw (  KiB/s): min=129368, max=160304, per=100.00%, avg=147315.43, stdev=6640.40, samples=42\n iops        : min=32342, max=40076, avg=36828.86, stdev=1660.10, samples=42\nwrite: IOPS=12.3k, BW=48.0MiB/s (50.4MB/s)(1026MiB/21357msec); 0 zone resets\n bw (  KiB/s): min=42952, max=53376, per=100.00%, avg=49241.33, stdev=2151.40, samples=42\n iops        : min=10738, max=13344, avg=12310.33, stdev=537.85, samples=42\ncpu          : usr=14.32%, sys=32.17%, ctx=356674, majf=0, minf=7\n</code></pre> <ul> <li>Laptop ZFS pool through NFS (running in parallel with other network processes):</li> </ul> <pre><code>read: IOPS=4917, BW=19.2MiB/s (20.1MB/s)(3070MiB/159812msec)\n bw (  KiB/s): min=16304, max=22368, per=100.00%, avg=19681.46, stdev=951.52, samples=319\n iops        : min= 4076, max= 5592, avg=4920.34, stdev=237.87, samples=319\nwrite: IOPS=1643, BW=6574KiB/s (6732kB/s)(1026MiB/159812msec); 0 zone resets\n bw (  KiB/s): min= 5288, max= 7560, per=100.00%, avg=6577.35, stdev=363.32, samples=319\n iops        : min= 1322, max= 1890, avg=1644.32, stdev=90.82, samples=319\ncpu          : usr=5.21%, sys=10.59%, ctx=175825, majf=0, minf=8\n</code></pre> <ul> <li> <p>Intel Nuc server disk SSD:   <pre><code>  read: IOPS=11.0k, BW=46.9MiB/s (49.1MB/s)(3070MiB/65525msec)\n bw (  KiB/s): min=  280, max=73504, per=100.00%, avg=48332.30, stdev=25165.49, samples=130\n iops        : min=   70, max=18376, avg=12083.04, stdev=6291.41, samples=130\nwrite: IOPS=4008, BW=15.7MiB/s (16.4MB/s)(1026MiB/65525msec); 0 zone resets\n bw (  KiB/s): min=   55, max=24176, per=100.00%, avg=16153.84, stdev=8405.53, samples=130\n iops        : min=   13, max= 6044, avg=4038.40, stdev=2101.42, samples=130\ncpu          : usr=8.04%, sys=25.87%, ctx=268055, majf=0, minf=8\n</code></pre></p> </li> <li> <p>Intel Nuc server external HD usb disk :   <pre><code>\n</code></pre></p> </li> <li> <p>Intel Nuc ZFS pool through NFS (running in parallel with other network processes):</p> </li> </ul> <pre><code>  read: IOPS=18.7k, BW=73.2MiB/s (76.8MB/s)(3070MiB/41929msec)\n bw (  KiB/s): min=43008, max=103504, per=99.80%, avg=74822.75, stdev=16708.40, samples=83\n iops        : min=10752, max=25876, avg=18705.65, stdev=4177.11, samples=83\nwrite: IOPS=6264, BW=24.5MiB/s (25.7MB/s)(1026MiB/41929msec); 0 zone resets\n bw (  KiB/s): min=14312, max=35216, per=99.79%, avg=25003.55, stdev=5585.54, samples=83\n iops        : min= 3578, max= 8804, avg=6250.88, stdev=1396.40, samples=83\ncpu          : usr=6.29%, sys=13.21%, ctx=575927, majf=0, minf=10\n</code></pre> <ul> <li>Old NAS with RAID5:   <pre><code>read : io=785812KB, bw=405434B/s, iops=98, runt=1984714msec\nwrite: io=262764KB, bw=135571B/s, iops=33, runt=1984714msec\ncpu          : usr=0.16%, sys=0.59%, ctx=212447, majf=0, minf=8\n</code></pre></li> </ul> <p>Conclusions:</p> <ul> <li>New NVME are super fast (1215MB/s read, 406MB/s write)</li> <li>ZFS rocks, with a RAIDZ1, L2ARC and ZLOG it returned almost the same performance as the NVME ( 1111MB/s read, 371MB/s write)</li> <li>Old NAS with RAID is super slow (0.4KB/s read, 0.1KB/s write!)</li> <li>I should replace the laptop's NVME, the NAS one has 10x performace both on read and write.</li> </ul> <p>There is a huge difference between ZFS in local and through NFS. In local you get (1111MB/s read and 371MB/s write) while through NFS I got (20.1MB/s read and 6.7MB/s write). I've measured the network performance between both machines with <code>iperf3</code> and got:</p> <ul> <li>From NAS to laptop 67.5 MB/s</li> <li>From laptop to NAS 59.25 MB/s</li> </ul> <p>It was because I was running it over wifi.</p> <p>From the Intel nuc to the new server I get 76MB/s read and 25.7MB/s write. Still a huge difference though against the local transfer. The network speed measured with <code>iperf3</code> are 116 MB/s.</p>"}, {"location": "linux_snippets/#use-a-pass-password-in-a-makefile", "title": "Use a <code>pass</code> password in a Makefile", "text": "<pre><code>TOKEN ?= $(shell bash -c '/usr/bin/pass show path/to/token')\n\ndiff:\n@AUTHENTIK_TOKEN=$(TOKEN) terraform plan\n</code></pre>"}, {"location": "linux_snippets/#install-a-new-font", "title": "Install a new font", "text": "<p>Install a font manually by downloading the appropriate <code>.ttf</code> or <code>otf</code> files and placing them into <code>/usr/local/share/fonts</code> (system-wide), <code>~/.local/share/fonts</code> (user-specific) or <code>~/.fonts</code> (user-specific). These files should have the permission 644 (<code>-rw-r--r--</code>), otherwise they may not be usable.</p>"}, {"location": "linux_snippets/#get-vpn-password-from-pass", "title": "Get VPN password from <code>pass</code>", "text": "<p>To be able to retrieve the user and password from pass you need to run the openvpn command with the next flags:</p> <pre><code>sudo bash -c \"openvpn --config config.ovpn  --auth-user-pass &lt;(echo -e 'user_name\\n$(pass show vpn)')\"\n</code></pre> <p>Assuming that <code>vpn</code> is an entry of your <code>pass</code> password store.</p>"}, {"location": "linux_snippets/#download-ts-streams", "title": "Download TS streams", "text": "<p>Some sites give stream content with small <code>.ts</code> files that you can't download directly. Instead open the developer tools, reload the page and search for a request with extension <code>.m3u8</code>, that gives you the playlist of all the chunks of <code>.ts</code> files. Once you have that url you can use <code>yt-dlp</code> to download it.</p>"}, {"location": "linux_snippets/#df-and-du-showing-different-results", "title": "df and du showing different results", "text": "<p>Sometimes on a linux machine you will notice that both <code>df</code> command (display free disk space) and <code>du</code> command (display disk usage statistics) report different output. Usually, <code>df</code> will output a bigger disk usage than <code>du</code>.</p> <p>The <code>du</code> command estimates file space usage, and the <code>df</code> command shows file system disk space usage.</p> <p>There are many reasons why this could be happening:</p>"}, {"location": "linux_snippets/#disk-mounted-over-data", "title": "Disk mounted over data", "text": "<p>If you mount a disk on a directory that already holds data, then when you run <code>du</code> that data won't show, but <code>df</code> knows it's there.</p> <p>To troubleshoot this, umount one by one of your disks, and do an <code>ls</code> to see if there's any remaining data in the mount point.</p>"}, {"location": "linux_snippets/#used-deleted-files", "title": "Used deleted files", "text": "<p>When a file is deleted under Unix/Linux, the disk space occupied by the file will not be released immediately in some cases. The result of the command <code>du</code> doesn\u2019t include the size of the deleting file. But the impact of the command <code>df</code> for the deleting file\u2019s size due to its disk space is not released immediately. Hence, after deleting the file, the results of <code>df</code> and <code>du</code> are different until the disk space is freed.</p> <p>Open file descriptor is main causes of such wrong information. For example, if a file called <code>/tmp/application.log</code> is open by a third-party application OR by a user and the same file is deleted, both <code>df</code> and <code>du</code> report different outputs. You can use the <code>lsof</code> command to verify this:</p> <pre><code>lsof | grep tmp\n</code></pre> <p>To fix it:</p> <ul> <li>Use the <code>lsof</code> command as discussed above to find a deleted file opened by   other users and apps. See how to list all users in the system for more info.</li> <li>Then, close those apps and log out of those Linux and Unix users.</li> <li>As a sysadmin you restart any process or <code>kill</code> the process under Linux and   Unix that did not release the deleted file.</li> <li>Flush the filesystem using the <code>sync</code> command that synchronizes cached writes   to persistent disk storage.</li> <li>If everything else fails, try restarting the system using the <code>reboot</code> command   or <code>shutdown</code> command.</li> </ul>"}, {"location": "linux_snippets/#scan-a-physical-page-in-linux", "title": "Scan a physical page in Linux", "text": "<p>Install <code>xsane</code> and run it.</p>"}, {"location": "linux_snippets/#git-checkout-to-main-with-master-as-a-fallback", "title": "Git checkout to main with master as a fallback", "text": "<p>I usually use the alias <code>gcm</code> to change to the main branch of the repository, given the change from main to master now I have some repos that use one or the other, but I still want <code>gcm</code> to go to the correct one. The solution is to use:</p> <pre><code>alias gcm='git checkout \"$(git symbolic-ref refs/remotes/origin/HEAD | cut -d'/' -f4)\"'\n</code></pre>"}, {"location": "linux_snippets/#create-qr-code", "title": "Create QR code", "text": "<pre><code>qrencode -o qrcode.png 'Hello World!'\n</code></pre>"}, {"location": "linux_snippets/#trim-silences-of-sound-files", "title": "Trim silences of sound files", "text": "<p>To trim all silence longer than 2 seconds down to only 2 seconds long.</p> <pre><code>sox in.wav out6.wav silence -l 1 0.1 1% -1 2.0 1%\n</code></pre> <p>Note that SoX does nothing to bits of silence shorter than 2 seconds.</p> <p>If you encounter the <code>sox FAIL formats: no handler for file extension 'mp3'</code> error you'll need to install the <code>libsox-fmt-all</code> package.</p>"}, {"location": "linux_snippets/#adjust-the-replay-gain-of-many-sound-files", "title": "Adjust the replay gain of many sound files", "text": "<pre><code>sudo apt-get install python-rgain\nreplaygain -f *.mp3\n</code></pre>"}, {"location": "linux_snippets/#check-vulnerabilities-in-nodejs-applications", "title": "Check vulnerabilities in Node.js applications", "text": "<p>With <code>yarn audit</code> you'll see the vulnerabilities, with <code>yarn outdated</code> you can see the packages that you need to update.</p>"}, {"location": "linux_snippets/#check-vulnerabilities-in-rails-dependencies", "title": "Check vulnerabilities in rails dependencies", "text": "<pre><code>gem install bundler-audit\ncd project_with_gem_lock\nbundler-audit\n</code></pre>"}, {"location": "linux_snippets/#create-basic-auth-header", "title": "Create Basic Auth header", "text": "<pre><code>$ echo -n user:password | base64\ndXNlcjpwYXNzd29yZA==\n</code></pre> <p>Without the <code>-n</code> it won't work well.</p>"}, {"location": "linux_snippets/#install-one-package-from-debian-unstable", "title": "Install one package from Debian unstable", "text": "<ul> <li>Add the <code>unstable</code> repository to your <code>/etc/apt/sources.list</code></li> </ul> <pre><code># Unstable\ndeb http://deb.debian.org/debian/ unstable main contrib non-free\ndeb-src http://deb.debian.org/debian/ unstable main contrib non-free\n</code></pre> <ul> <li> <p>Configure <code>apt</code> to only use <code>unstable</code> when specified</p> <p>File: <code>/etc/apt/preferences</code> <pre><code>Package: * \nPin: release a=stable\nPin-Priority: 700\n\nPackage: *\nPin: release  a=testing\nPin-Priority: 600\n\nPackage: *\nPin: release a=unstable\nPin-Priority: 100\n</code></pre></p> </li> <li> <p>Update the package data with <code>apt-get update</code>.</p> </li> <li>See that the new versions are available with   <code>apt-cache policy   &lt;package_name&gt;</code></li> <li>To install a package from unstable you can run   <code>apt-get install -t unstable   &lt;package_name&gt;</code>.</li> </ul>"}, {"location": "linux_snippets/#fix-the-following-packages-have-been-kept-back", "title": "Fix the following packages have been kept back", "text": "<pre><code>sudo apt-get --with-new-pkgs upgrade\n</code></pre>"}, {"location": "linux_snippets/#monitor-outgoing-traffic", "title": "Monitor outgoing traffic", "text": ""}, {"location": "linux_snippets/#easy-and-quick-way-watch-lsof", "title": "Easy and quick way watch &amp; lsof", "text": "<p>You can simply use a combination of <code>watch</code> &amp; <code>lsof</code> command in Linux to get an idea of outgoing traffic on specific ports. Here is an example of outgoing traffic on ports <code>80</code> and <code>443</code>.</p> <pre><code>$ watch -n1 lsof -i TCP:80,443\n</code></pre> <p>Here is a sample output.</p> <pre><code>dropbox    2280 saml   23u  IPv4 56015285      0t0  TCP www.example.local:56003-&gt;snt-re3-6c.sjc.dropbox.com:http (ESTABLISHED)\nthunderbi  2306 saml   60u  IPv4 56093767      0t0  TCP www.example.local:34788-&gt;ord08s09-in-f20.1e100.net:https (ESTABLISHED)\nmono       2322 saml   15u  IPv4 56012349      0t0  TCP www.example.local:54018-&gt;204-62-14-135.static.6sync.net:https (ESTABLISHED)\nchrome    4068 saml  175u  IPv4 56021419      0t0  TCP www.example.local:42182-&gt;stackoverflow.com:http (ESTABLISHED)\n</code></pre> <p>You'll miss the short lived connections though.</p>"}, {"location": "linux_snippets/#fine-grained-with-tcpdump", "title": "Fine grained with tcpdump", "text": "<p>You can also use <code>tcpdump</code> command to capture all raw packets, on all interfaces, on all ports, and write them to file.</p> <pre><code>sudo tcpdump -tttt -i any -w /tmp/http.log\n</code></pre> <p>Or you can limit it to a specific port adding the arguments <code>port 443 or 80</code>. The <code>-tttt</code> flag is used to capture the packets with a human readable timestamp.</p> <p>To read the recorded information, run the <code>tcpdump</code> command with <code>-A</code> option. It will print ASCII text in recorded packets, that you can browse using page up/down keys.</p> <pre><code>tcpdump -A -r /tmp/http.log | less\n</code></pre> <p>However, <code>tcpdump</code> cannot decrypt information, so you cannot view information about HTTPS requests in it.</p>"}, {"location": "linux_snippets/#clean-up-system-space", "title": "Clean up system space", "text": ""}, {"location": "linux_snippets/#clean-package-data", "title": "Clean package data", "text": "<p>There is a couple of things to do when we want to free space in a no-brainer way. First, we want to remove those deb packages that get cached every time we do <code>apt-get install</code>.</p> <pre><code>apt-get clean\n</code></pre> <p>Also, the system might keep packages that were downloaded as dependencies but are not needed anymore. We can get rid of them with</p> <pre><code>apt-get autoremove\n</code></pre> <p>Remove data of unpurged packages.</p> <pre><code>sudo apt-get purge $(dpkg -l | grep '^rc' | awk '{print $2}')\n</code></pre> <p>If we want things tidy, we must know that whenever we <code>apt-get remove</code> a package, the configuration will be kept in case we want to install it again. In most cases we want to use <code>apt-get purge</code>. To clean those configurations from removed packages, we can use</p> <pre><code>dpkg --list | grep \"^rc\" | cut -d \" \" -f 3 | xargs --no-run-if-empty sudo dpkg --purge\n</code></pre> <p>So far we have not uninstalled anything. If now we want to inspect what packages are consuming the most space, we can type</p> <pre><code>dpkg-query -Wf '${Installed-Size}\\t${Package}\\n' | sort -n\n</code></pre>"}, {"location": "linux_snippets/#clean-snap-data", "title": "Clean snap data", "text": "<p>If you're using <code>snap</code> you can clean space by:</p> <ul> <li> <p>Reduce the number of versions kept of a package with   <code>snap set system refresh.retain=2</code></p> </li> <li> <p>Remove the old versions with <code>clean_snap.sh</code></p> </li> </ul> <pre><code>#!/bin/bash\n#Removes old revisions of snaps\n#CLOSE ALL SNAPS BEFORE RUNNING THIS\nset -eu\nLANG=en_US.UTF-8 snap list --all | awk '/disabled/{print $1, $3}' |\nwhile read snapname revision; do\nsnap remove \"$snapname\" --revision=\"$revision\"\ndone\n</code></pre>"}, {"location": "linux_snippets/#clean-journalctl-data", "title": "Clean journalctl data", "text": "<ul> <li>Check how much space it's using: <code>journalctl --disk-usage</code></li> <li>Rotate the logs: <code>journalctl --rotate</code></li> </ul> <p>Then you have three ways to reduce the data:</p> <ol> <li>Clear journal log older than X days: <code>journalctl --vacuum-time=2d</code></li> <li>Restrict logs to a certain size: <code>journalctl --vacuum-size=100M</code></li> <li>Restrict number of log files: <code>journactl --vacuum-files=5</code>.</li> </ol> <p>The operations above will affect the logs you have right now, but it won't solve the problem in the future. To let <code>journalctl</code> know the space you want to use open the <code>/etc/systemd/journald.conf</code> file and set the <code>SystemMaxUse</code> to the amount you want (for example <code>1000M</code> for a gigabyte). Once edited restart the service with <code>sudo systemctl restart systemd-journald</code>.</p>"}, {"location": "linux_snippets/#clean-up-docker-data", "title": "Clean up docker data", "text": "<p>To remove unused <code>docker</code> data you can run <code>docker system prune -a</code>. This will remove:</p> <ul> <li>All stopped containers</li> <li>All networks not used by at least one container</li> <li>All images without at least one container associated to them</li> <li>All build cache</li> </ul> <p>Sometimes that's not enough, and your <code>/var/lib/docker</code> directory still weights more than it should. In those cases:</p> <ul> <li>Stop the <code>docker</code> service.</li> <li>Remove or move the data to another directory</li> <li>Start the <code>docker</code> service.</li> </ul> <p>In order not to loose your persisted data, you need to configure your dockers to mount the data from a directory that's not within <code>/var/lib/docker</code>.</p>"}, {"location": "linux_snippets/#set-up-docker-logs-rotation", "title": "Set up docker logs rotation", "text": "<p>By default, the stdout and stderr of the container are written in a JSON file located in <code>/var/lib/docker/containers/[container-id]/[container-id]-json.log</code>. If you leave it unattended, it can take up a large amount of disk space.</p> <p>If this JSON log file takes up a significant amount of the disk, we can purge it using the next command.</p> <pre><code>truncate -s 0 &lt;logfile&gt;\n</code></pre> <p>We could setup a cronjob to purge these JSON log files regularly. But for the long term, it would be better to setup log rotation. This can be done by adding the following values in <code>/etc/docker/daemon.json</code>.</p> <pre><code>{\n\"log-driver\": \"json-file\",\n\"log-opts\": {\n\"max-size\": \"10m\",\n\"max-file\": \"10\"\n}\n}\n</code></pre>"}, {"location": "linux_snippets/#clean-old-kernels", "title": "Clean old kernels", "text": "<p>!!! warning \"I don't recommend using this step, rely on <code>apt-get autoremove</code>, it' safer\"</p> <p>The full command is</p> <pre><code>dpkg -l linux-* | awk '/^ii/{ print $2}' | grep -v -e `uname -r | cut -f1,2 -d\"-\"` | grep -e [0-9] | grep -E \"(image|headers)\" | xargs sudo apt-get -y purge\n</code></pre> <p>To test what packages will it remove use:</p> <pre><code>dpkg -l linux-* | awk '/^ii/{ print $2}' | grep -v -e `uname -r | cut -f1,2 -d\"-\"` | grep -e [0-9] | grep -e \"(image|headers)\" | xargs sudo apt-get --dry-run remove\n</code></pre> <p>Remember that your running kernel can be obtained by <code>uname -r</code>.</p>"}, {"location": "linux_snippets/#replace-a-string-with-sed-recursively", "title": "Replace a string with sed recursively", "text": "<pre><code>find . -type f -exec sed -i 's/foo/bar/g' {} +\n</code></pre>"}, {"location": "linux_snippets/#bypass-client-ssl-certificate-with-cli-tool", "title": "Bypass client SSL certificate with cli tool", "text": "<p>Websites that require clients to authorize with an TLS certificate are difficult to interact with through command line tools that don't support this feature.</p> <p>To solve it, we can use a transparent proxy that does the exchange for us.</p> <ul> <li>Export your certificate: If you have a <code>p12</code> certificate, you first need to   extract the key, crt and the ca from the certificate into the <code>site.pem</code>.</li> </ul> <pre><code>openssl pkcs12 -in certificate.p12 -out site.key.pem -nocerts -nodes # It asks for the p12 password\nopenssl pkcs12 -in certificate.p12 -out site.crt.pem -clcerts -nokeys\nopenssl pkcs12 -cacerts -nokeys -in certificate.p12 -out site-ca-cert.ca\n\ncat site.key.pem site.crt.pem site-ca-cert.ca &gt; site.pem\n</code></pre> <ul> <li>Build the proxy ca: Then we merge the site and the client ca's into the   <code>site-ca-file.cert</code> file:</li> </ul> <pre><code>openssl s_client -connect www.site.org:443 2&gt;/dev/null  | openssl x509 -text &gt; site-ca-file.cert\ncat site-ca-cert.ca &gt;&gt; web-ca-file.cert\n</code></pre> <ul> <li>Change your hosts file to redirect all requests to the proxy.</li> </ul> <pre><code># vim /etc/hosts\n[...]\n0.0.0.0 www.site.org\n</code></pre> <ul> <li>Run the proxy</li> </ul> <pre><code>docker run --rm \\\n-v $(pwd):/certs/ \\\n-p 3001:3001 \\\n-it ghostunnel/ghostunnel \\\nclient \\\n--listen 0.0.0.0:3001 \\\n--target www.site.org:443 \\\n--keystore /certs/site.pem \\\n--cacert /certs/site-ca-file.cert \\\n--unsafe-listen\n</code></pre> <ul> <li>Run the command line tool using the http protocol on the port 3001:</li> </ul> <pre><code>wpscan  --url http://www.site.org:3001/ --disable-tls-checks\n</code></pre> <p>Remember to clean up your env afterwards.</p>"}, {"location": "linux_snippets/#allocate-space-for-a-virtual-filesystem", "title": "Allocate space for a virtual filesystem", "text": "<p>Also useful to simulate big files</p> <pre><code>fallocate -l 20G /path/to/file\n</code></pre>"}, {"location": "linux_snippets/#identify-what-a-string-or-file-contains", "title": "Identify what a string or file contains", "text": "<p>Identify anything. <code>pyWhat</code> easily lets you identify emails, IP addresses, and more. Feed it a .pcap file or some text and it'll tell you what it is.</p>"}, {"location": "linux_snippets/#split-a-file-into-many-with-equal-number-of-lines", "title": "Split a file into many with equal number of lines", "text": "<p>You could do something like this:</p> <pre><code>split -l 200000 filename\n</code></pre> <p>Which will create files each with 200000 lines named <code>xaa</code>, <code>xab</code>, <code>xac</code>, ...</p>"}, {"location": "linux_snippets/#check-if-an-rsync-command-has-gone-well", "title": "Check if an rsync command has gone well", "text": "<p>Sometimes after you do an <code>rsync</code> between two directories of different devices (an usb and your hard drive for example), the sizes of the directories don't match. I've seen a difference of a 30% less on the destination. <code>du</code>, <code>ncdu</code> and <code>and</code> have a long story of reporting wrong sizes with advanced filesystems (ZFS, VxFS or compressing filesystems), these do a lot of things to reduce the disk usage (deduplication, compression, extents, files with holes...) which may lead to the difference in space.</p> <p>To check if everything went alright run <code>diff -r --brief source/ dest/</code>, and check that there is no output.</p>"}, {"location": "linux_snippets/#list-all-process-swap-usage", "title": "List all process swap usage", "text": "<pre><code>for file in /proc/*/status ; do awk '/VmSwap|Name/{printf $2 \" \" $3}END{ print \"\"}' $file; done | sort -k 2 -n -r | less\n</code></pre>"}, {"location": "logql/", "title": "Logql", "text": "<p>LogQL is Grafana Loki\u2019s PromQL-inspired query language. Queries act as if they are a distributed <code>grep</code> to aggregate log sources. LogQL uses labels and operators for filtering.</p> <p>There are two types of LogQL queries:</p> <ul> <li>Log queries: Return the contents of log lines.</li> <li>Metric queries: Extend log queries to calculate values based on query results.</li> </ul>"}, {"location": "logql/#usage", "title": "Usage", "text": ""}, {"location": "logql/#apply-a-pattern-to-the-value-of-a-label", "title": "Apply a pattern to the value of a label", "text": "<p>Some logs are sent in json and then one of their fields can contain other structured data. You may want to use that structured data to further filter the logs.</p> <pre><code>{app=\"ingress-nginx\"} | json | line_format `{{.log}}` | pattern `&lt;_&gt; - - &lt;_&gt; \"&lt;method&gt; &lt;_&gt; &lt;_&gt;\" &lt;status&gt; &lt;_&gt; &lt;_&gt; \"&lt;_&gt;\" &lt;_&gt;` | method != `GET`\n</code></pre> <ul> <li><code>{app=\"ingress-nginx\"}</code>: Show only the logs of the <code>ingress-nginx</code>.</li> <li><code>| json</code>:  Interpret the line as a json.</li> <li><code>``| line_format</code>{{.log}}<code>| pattern</code>&lt;&gt; - - &lt;&gt; \" &lt;&gt; &lt;&gt;\"  &lt;&gt; &lt;&gt; \"&lt;&gt;\" &lt;&gt;<code>```: interpret the</code>log` json field of the trace with the selected pattern <li><code>``| method !=</code>GET````: Filter the line using a key extracted by the pattern.</li>"}, {"location": "logql/#count-the-unique-values-of-a-label", "title": "Count the unique values of a label", "text": "<p>Sometimes you want to alert on the values of a log. For example if you want to make sure that you're receiving the logs from more than 20 hosts (otherwise something is wrong). Assuming that your logs attach a <code>host</code> label you can run</p> <pre><code>sum(count by(host) (rate({host=~\".+\"} [24h])) &gt; bool 0)\n</code></pre> <p>This query will: - <code>{host=~\".+\"}</code>: Fetch all log lines that contain the label <code>host</code> - <code>count by(host) (rate({host=~\".+\"} [24h])</code>: Calculates the number of entries in the last 24h. - <code>count by(host) (rate({host=~\".+\"} [24h])) &gt; bool 0</code>: Converts to <code>1</code> all the vector elements that have more than 1 message. - <code>sum(count by(host) (rate({host=~\".+\"} [24h])) &gt; bool 0)</code>: Sums all the vector elements to get the number of hosts that have more than one message. </p> <p><code>journald</code> promtail parser is known to fail between upgrades, it's useful too to make an alert to make sure that all your hosts are sending the traces. You can do it with: <code>sum(count by(host) (rate({job=\"systemd-journal\"} [24h])) &gt; bool 0)</code></p>"}, {"location": "logql/#references", "title": "References", "text": "<ul> <li>Docs</li> </ul>"}, {"location": "maison/", "title": "Maison", "text": "<p>Maison is a Python library to read configuration settings from configuration files using <code>pydantic</code> behind the scenes.</p> <p>It's useful to parse TOML config files.</p> <p>Note: \"If you want to use YAML for your config files use <code>goodconf</code> instead.\"</p>"}, {"location": "maison/#installation", "title": "Installation", "text": "<pre><code>pip install maison\n</code></pre>"}, {"location": "maison/#usage", "title": "Usage", "text": "<pre><code>from maison import ProjectConfig\n\nconfig = ProjectConfig(project_name=\"acme\")\nfoo_option = config.get_option(\"foo\")\n\nprint(foo_option)\n</code></pre>"}, {"location": "maison/#read-from-file", "title": "Read from file", "text": "<p>By default, <code>maison</code> will look for a <code>pyproject.toml</code> file. If you prefer to look elsewhere, provide a <code>source_files</code> list to <code>ProjectConfig</code> and <code>maison</code> will select the first source file it finds from the list.</p> <pre><code>from maison import ProjectConfig\n\nconfig = ProjectConfig(project_name=\"acme\", source_files=[\"acme.ini\", \"pyproject.toml\"])\n</code></pre>"}, {"location": "maison/#references", "title": "References", "text": "<ul> <li>Git</li> <li>Docs</li> </ul>"}, {"location": "map_management/", "title": "Map management", "text": "<p>As a privacy minded human, I try to avoid using proprietary software and services as much as possible. Map management is not an exception. Google maps is monopolizing mapping and routing, but there are better alternatives out there.</p> <p>For navigating on the go, I strongly recommend OSMand+, for browsing maps in the browser, use OpenStreetMaps or CyclOSM if you want to move by bike.</p> <p>To plan routes, you can use brouter.de, it works perfectly for bikes. For hiking is awesome too, it shows you a lot of data needed to plan your tracks (check the settings on the right). If you want to invest a little more time, you can even set your personalize profiles, so that the routing algorithm prioritizes the routes to your desires. It's based on brouter and both can be self-hosted, although brouter does not yet use Docker.</p>"}, {"location": "matrix/", "title": "Matrix", "text": ""}, {"location": "matrix/#installation", "title": "Installation", "text": "<pre><code>sudo apt install -y wget apt-transport-https\nsudo wget -O /usr/share/keyrings/element-io-archive-keyring.gpg https://packages.element.io/debian/element-io-archive-keyring.gpg\necho \"deb [signed-by=/usr/share/keyrings/element-io-archive-keyring.gpg] https://packages.element.io/debian/ default main\" | sudo tee /etc/apt/sources.list.d/element-io.list\nsudo apt update\nsudo apt install element-desktop\n</code></pre>"}, {"location": "mbsync/", "title": "mbsync", "text": "<p>mbsync is a command line application which synchronizes mailboxes; currently Maildir and IMAP4 mailboxes are supported. New messages, message deletions and flag changes can be propagated both ways; the operation set can be selected in a fine-grained manner.</p>"}, {"location": "mbsync/#installation", "title": "Installation", "text": "<pre><code>apt-get install isync\n</code></pre>"}, {"location": "mbsync/#configuration", "title": "Configuration", "text": "<p>Assuming that you want to sync the mails of <code>example@examplehost.com</code> and that you have your password stored in <code>pass</code> under <code>mail/example</code>.</p> <p>File: ~/.mbsyncrc</p> <pre><code>IMAPAccount example\nHost examplehost.com\nUser \"example@examplehost.com\"\nPassCmd \"/usr/bin/pass mail/example\"\n\nIMAPStore example-remote\nAccount example\nUseNamespace no\n\nMaildirStore example-local\nPath ~/mail/example/\nInbox ~/mail/example/Inbox\n\nChannel example\nMaster :example-remote:\nSlave :example-local:\nCreate Both\nPatterns *\nSyncState *\nCopyArrivalDate yes\nSync Pull\n</code></pre> <p>You need to manually create the directories where you store the emails.</p> <pre><code>mkdir -p ~/mail/example\n</code></pre>"}, {"location": "mbsync/#references", "title": "References", "text": "<ul> <li>Homepage</li> </ul>"}, {"location": "mdformat/", "title": "MDFormat", "text": "<p>MDFormat is an opinionated Markdown formatter that can be used to enforce a consistent style in Markdown files. Mdformat is a Unix-style command-line tool as well as a Python library.</p> <p>The features/opinions of the formatter include:</p> <ul> <li>Consistent indentation and whitespace across the board</li> <li>Always use ATX style headings</li> <li>Move all link references to the bottom of the document (sorted by label)</li> <li>Reformat indented code blocks as fenced code blocks</li> <li>Use 1. as the ordered list marker if possible, also for noninitial list items.</li> </ul> <p>It's based on the <code>markdown-it-py</code> Markdown parser, which is a Python implementation of <code>markdown-it</code>.</p>"}, {"location": "mdformat/#installation", "title": "Installation", "text": "<p>By default it uses CommonMark support:</p> <pre><code>pip install mdformat\n</code></pre> <p>This won't support task lists, if you want them use the github flavoured parser instead:</p> <pre><code>pip install mdformat-gfm\n</code></pre> <p>You may want to also install some interesting plugins:</p> <ul> <li><code>mdformat-beautysh</code>: format   <code>bash</code> and <code>sh</code> code blocks.</li> <li><code>mdformat-black</code>: format <code>python</code>   code blocks.</li> <li><code>mdformat-config</code>: format <code>json</code>,   <code>toml</code> and <code>yaml</code> code blocks.</li> <li><code>mdformat-web</code>: format <code>javascript</code>,   <code>css</code>, <code>html</code> and <code>xml</code> code blocks.</li> <li><code>mdformat-tables</code>: Adds   support for Github Flavored Markdown style tables.</li> <li><code>mdformat-frontmatter</code>:   Adds support for the yaml header with metadata of the file.</li> </ul> <p>To install them with <code>pipx</code> you can run:</p> <pre><code>pipx install --include-deps mdformat-gfm\npipx inject mdformat-gfm mdformat-beautysh mdformat-black mdformat-config \\\nmdformat-web mdformat-tables mdformat-frontmatter\n</code></pre>"}, {"location": "mdformat/#desires", "title": "Desires", "text": "<p>These are the functionalities I miss when writing markdown that can be currently fixed with <code>mdformat</code>:</p> <ul> <li>Long lines are wrapped.</li> <li>Long lines in lists are wrapped and the indentation is respected.</li> <li>Add correct blank lines between sections.</li> </ul> <p>I haven't found yet a way to achieve:</p> <ul> <li>Links are sent to the bottom of the document.</li> <li>Do   typographic replacements</li> <li>End paragraphs with a dot.</li> </ul>"}, {"location": "mdformat/#developing-mdformat-plugins", "title": "Developing mdformat plugins", "text": "<p>There are two kinds of plugins:</p> <ul> <li>Formatters: They change the output of the text. For example   <code>mdformatormat-black</code>.</li> <li>Parsers: They are extensions to the base CommonMark parser.</li> </ul> <p>You can see some plugin examples here.</p>"}, {"location": "mdformat/#issues", "title": "Issues", "text": "<ul> <li>It doesn't yet   support admonitions</li> <li>You can't   ignore some files,   nor   some part of the file</li> </ul>"}, {"location": "mdformat/#references", "title": "References", "text": "<ul> <li>Docs</li> <li>Source</li> </ul>"}, {"location": "mediatracker/", "title": "Mediatracker", "text": "<p>MediaTracker is a self hosted media tracker for movies, tv shows, video games, books and audiobooks </p>"}, {"location": "mediatracker/#installation", "title": "Installation", "text": "<p>With docker compose:</p> <pre><code>version: \"3\"\nservices:\nmediatracker:\ncontainer_name: mediatracker\nports:\n- 7481:7481\nvolumes:\n- /home/YOUR_HOME_DIRECTORY/.config/mediatracker/data:/storage\n- assetsVolume:/assets\nenvironment:\nSERVER_LANG: en\nTMDB_LANG: en\nAUDIBLE_LANG: us\nTZ: Europe/London\nimage: bonukai/mediatracker:latest\n\nvolumes:\nassetsVolume: null\n</code></pre> <p>If you attach more than one docker network the container becomes unreachable :S.</p>"}, {"location": "mediatracker/#install-the-jellyfin-plugin", "title": "Install the jellyfin plugin", "text": "<p>They created a Jellyfin plugin so that all scrobs are sent automatically to the mediatracker</p> <ul> <li>Add new Repository in Jellyfin (Dashboard -&gt; Plugins -&gt; Repositories -&gt; +) from url <code>https://raw.githubusercontent.com/bonukai/jellyfin-plugin-mediatracker/main/manifest.json</code></li> <li>Install MediaTracker plugin from Catalogue (Dashboard -&gt; Plugins -&gt; Catalogue)</li> </ul>"}, {"location": "mediatracker/#usage", "title": "Usage", "text": "<p>Some tips:</p> <ul> <li>Add the shows you want to watch to the watchlist so that it's easier to find them</li> <li>When you're ending an episode, click on the episode number on the watchlist element and then rate the episode itself.</li> </ul>"}, {"location": "mediatracker/#lists", "title": "Lists", "text": "<p>You can create public lists to share with the rest of the users, the way to share it though is a bit archaic so far, it's only through the list link, in the interface they won't be able to see it.</p>"}, {"location": "mediatracker/#alternatives", "title": "Alternatives", "text": "<p>Ryot has a better web design, it also has a jellyfin scrobbler, although it's not yet stable. There are other UI tweaks that is preventing me from migrating to ryot such as the easier media rating and the percentage over five starts rating system.</p>"}, {"location": "mediatracker/#references", "title": "References", "text": "<ul> <li>Source</li> <li>Issues</li> </ul>"}, {"location": "meditation/", "title": "Meditation", "text": "<p>Meditation is a practice where an individual uses a technique,such as mindfulness, or focusing the mind on a particular object, thought, or activity, to train attention and awareness, and achieve a mentally clear and emotionally calm and stable state.</p> <p>Meditation may reduce stress, anxiety, depression, and pain, and enhance peace, perception, self-concept, and well-being.</p>"}, {"location": "meditation/#types-of-meditation", "title": "Types of meditation", "text": "<p>Although there isn't a right or wrong way to meditate, it\u2019s important to find a practice that meets your needs and complements your personality.</p> <p>There are nine popular types of meditation practice:</p> <ul> <li> <p>Mindfulness meditation: You pay attention to your thoughts as they pass     through your mind. You don't judge the thoughts or become involved with     them. You simply observe and take note of any patterns.</p> <p>This practice combines concentration with awareness. You may find it helpful to focus on an object or your breath while you observe any bodily sensations, thoughts, or feelings.</p> <p>This type of meditation is good for people who don\u2019t have a teacher to guide them, as it can be easily practiced alone.</p> </li> <li> <p>Focused meditation: Involves concentration using any of the five senses.</p> <p>For example, you can focus on something internal, like your breath, or you can bring in external influences to help focus your attention.</p> <p>Try counting mala beads, listening to a gong, or staring at a candle flame.</p> <p>This practice may be simple in theory, but it can be difficult for beginners to hold their focus for longer than a few minutes at first.</p> <p>If your mind does wander, it\u2019s important to come back to the practice and refocus.</p> <p>As the name suggests, this practice is ideal for anyone who requires additional focus in their life.</p> </li> <li> <p>Movement meditation: It\u2019s an active form of meditation where the movement     guides you. It can be achieved through yoga, martial arts or by walking     through the woods, gardening, qigong, and other gentle forms of motion.</p> <p>Movement meditation is good for people who find peace in action and prefer to let their minds wander.</p> </li> <li> <p>Mantra meditation: Uses a repetitive sound to clear the mind. It can be     a word, phrase, or sound, such as the popular \u201cOm.\u201d</p> <p>It doesn't matter if your mantra is spoken loudly or quietly. After chanting the mantra for some time, you\u2019ll be more alert and in tune with your environment. This allows you to experience deeper levels of awareness.</p> <p>Some people enjoy mantra meditation because they find it easier to focus on a word than on their breath. This is also a good practice for people who don't like silence and enjoy repetition.</p> </li> <li> <p>Transcendental Meditation: It is more customizable than mantra meditation,     using a mantra or series of words that are specific to each practitioner.</p> <p>This practice is for those who like structure and are serious about maintaining a meditation practice.</p> </li> <li> <p>Progressive relaxation: Also known as body scan meditation, it's a practice     aimed at reducing tension in the body and promoting relaxation.</p> <p>Oftentimes, this form of meditation involves slowly tightening and relaxing one muscle group at a time throughout the body.</p> <p>In some cases, it may also encourage you to imagine a gentle wave flowing through your body to help release any tension.</p> <p>This form of meditation is often used to relieve stress and unwind before bedtime.</p> </li> <li> <p>Loving-kindness meditation: is used to strengthen feelings of compassion,     kindness, and acceptance toward oneself and others.</p> <p>It typically involves opening the mind to receive love from others and then sending a series of well wishes to loved ones, friends, acquaintances, and all living beings.</p> <p>Because this type of meditation is intended to promote compassion and kindness, it may be ideal for those holding feelings of anger or resentment. * Visualization meditation: Is a technique focused on enhancing feelings of relaxation, peace, and calmness by visualizing positive scenes or images.</p> <p>With this practice, it\u2019s important to imagine the scene vividly and use all five senses to add as much detail as possible.</p> <p>Another form of visualization meditation involves imagining yourself succeeding at specific goals, which is intended to increase focus and motivation.</p> <p>Many people use visualization meditation to boost their mood, reduce stress levels, and promote inner peace.</p> </li> <li> <p>Spiritual meditation: Spiritual meditation is used in Eastern religions,     such as Hinduism and Daoism, and in Christian faith..</p> <p>It\u2019s similar to prayer in that you reflect on the silence around you and seek a deeper connection with your God or Universe.</p> </li> </ul>"}, {"location": "meditation/#how-to-get-started", "title": "How to get started", "text": "<p>The easiest way to begin is to sit quietly and focus on your breath for 20 minutes every day. If it's too much for you, start in small moments of time, even 5 or 10 minutes, and grow from there.</p>"}, {"location": "meditation/#references", "title": "References", "text": "<ul> <li>healthline article on types of meditation</li> <li>NonCompete video on meditation for anti-capitalists</li> </ul>"}, {"location": "meditation/#to-review", "title": "To review", "text": "<ul> <li>https://wiki.nikitavoloboev.xyz/mindfulness/meditation</li> <li>https://www.healthline.com/health/4-7-8-breathing#Other-techniques-to-help-you-sleep</li> <li>https://threader.app/thread/1261481222359801856</li> <li>https://quietkit.com/box-breathing/</li> <li>https://www.healthline.com/health/mental-health/best-mindfulness-blogs#8</li> <li>https://www.mindful.org/how-to-meditate/</li> </ul>"}, {"location": "meditation/#books", "title": "Books", "text": "<ul> <li>The Mind Illuminated: A Complete Meditation Guide Integrating Buddhist Wisdom and Brain Science by Culadasa (John Yates)</li> </ul>"}, {"location": "mentoring/", "title": "Mentoring", "text": "<p>Mentoring is a process for the informal transmission of knowledge, social capital, and the psychosocial support perceived by the recipient as relevant to work, career, or professional development; mentoring entails informal communication, usually face-to-face and during a sustained period of time, between a person who is perceived to have greater relevant knowledge, wisdom, or experience (the mentor) and a person who is perceived to have less (the apprentice).</p>"}, {"location": "mentoring/#obstacles", "title": "Obstacles", "text": ""}, {"location": "mentoring/#apprentice-obstacles", "title": "Apprentice obstacles", "text": "<p>The most common obstacles I've found apprentices have in their early steps of learning are:</p> <ul> <li>Not knowing where to start.</li> <li>Not having a clear roadmap.</li> <li>Having wrong expectations.</li> <li>Feeling overwhelmed by big tasks.</li> <li>Not knowing how to break a big task in small actionable steps.</li> <li>Given a requirement, design possible solutions and choose the best one.</li> <li>Feeling insecure about themselves.</li> <li>Suffering from the impostor syndrome</li> </ul> <p>A mentor can greatly help the apprentice overcome them.</p>"}, {"location": "mentoring/#mentor-obstacles", "title": "Mentor obstacles", "text": "<p>The most common obstacles I've found as a mentor are:</p> <ul> <li>Use concepts that the apprentice doesn't yet understand.</li> <li>Try to impose my way of doing things.</li> <li>Try to impose the best solution or practices even though they are out of reach     of the apprentice yet.</li> </ul>"}, {"location": "mentoring/#mentorship-principles", "title": "Mentorship principles", "text": "<p>People involved in a mentorship experience a strong personal relationship, in order to make it pleasant and healthy it must be based on the next principles:</p> <ul> <li>Care</li> <li>Equality</li> <li>Transparency</li> </ul>"}, {"location": "mentoring/#care", "title": "Care", "text": "<p>As in any relationship, care must be one of the main focuses of both parties, by care I mean:</p> <ul> <li>Actively read the other person mood and state and adjust your behaviours     accordingly.</li> <li>Ask for the other person's well being, keep track of the events of their     lives, and ask them how they went afterwards.</li> <li>Know your weak spots, have an improvement plan, and make them visible when they     arise.</li> <li>Actively search for ways to make their life easier and more pleasant.</li> <li>Respect the other person's time, don't be late.</li> </ul> <p>Men must put special interest in this point as we're usually not taught on caring for others.</p>"}, {"location": "mentoring/#equality", "title": "Equality", "text": "<p>There's a high risk of having unhealthy power dynamics where the mentor is taken as in a higher position than the apprentice because he has more knowledge in the specific field of study. The reality is that there is a lot more involved in the experience than the transmission of knowledge of the field from mentor to apprentice.</p> <p>Only if you see yourself as equals you can build the best experience.</p>"}, {"location": "mentoring/#transparency", "title": "Transparency", "text": ""}, {"location": "mentoring/#shared-roles", "title": "Shared roles", "text": "<ul> <li>Actively defend and follow the mentorship     principles.</li> <li>Review and improve the mentoring workflows.</li> </ul>"}, {"location": "mentoring/#mentor-roles", "title": "Mentor roles", "text": "<p>The mentor can help through the next ways:</p> <ul> <li>Roadmap definition and maintenance</li> <li>Task management</li> <li>Overcome the mentor's obstacles</li> </ul>"}, {"location": "mentoring/#roadmap-definition-and-maintenance", "title": "Roadmap definition and maintenance", "text": ""}, {"location": "mentoring/#get-to-know-each-other", "title": "Get to know each other", "text": "<p>First of all we need to know what are the underlying goals of the apprentice in order to sketch the best roadmap.</p>"}, {"location": "mentoring/#define-and-maintain-a-roadmap", "title": "Define and maintain a roadmap", "text": "<p>It's very important that the apprentice has a clear idea of In order to</p> <p>Beginner Junior</p>"}, {"location": "mentoring/#overcome-the-mentor-obstacles", "title": "Overcome the mentor obstacles", "text": "<ul> <li>Be attentive of the apprentice reactions</li> </ul>"}, {"location": "mentoring/#task-management", "title": "Task management", "text": ""}, {"location": "mentoring/#apprentice-roles", "title": "Apprentice roles", "text": "<ul> <li>Try it's best to follow the agreed roadmap and tasks.</li> <li>Analyze themselves with respect to the mentoring workflows.</li> <li>Overcome the apprentice's obstacles.     *</li> </ul>"}, {"location": "mermaidjs/", "title": "MermaidJS", "text": "<p>MermaidJS is a Javascript library that lets you create diagrams using text and code.</p> <p>It can render the next diagram types:</p> <ul> <li>Flowchart</li> <li>Sequence.</li> <li>Gantt</li> <li>Class</li> <li>Git graph</li> <li>Entity Relationship</li> <li>User journey</li> </ul>"}, {"location": "mermaidjs/#installation", "title": "Installation", "text": "<p>Installing it requires node, I've only used it in mkdocs, which is easier to install and use.</p>"}, {"location": "mermaidjs/#usage", "title": "Usage", "text": ""}, {"location": "mermaidjs/#flowchart", "title": "Flowchart", "text": "<p>It can have two orientations top to bottom (<code>TB</code>) or left to right (<code>LR</code>).</p> <pre><code>graph TD\n    Start --&gt; Stop\n</code></pre> <p>By default the text shown is the same as the id, if you need a big text it's recommended to use the <code>id1[This is the text in the box]</code> syntax so it's easy to reference the node in the relationships.</p> <p>To link nodes, use <code>--&gt;</code> or <code>---</code>. If you cant to add text to the link use <code>A-- text --&gt;B</code></p>"}, {"location": "mermaidjs/#adding-links", "title": "Adding links", "text": "<p>You can add <code>click</code> events to the diagrams:</p> <pre><code>graph LR;\n    A--&gt;B;\n    B--&gt;C;\n    C--&gt;D;\n    click A callback \"Tooltip for a callback\"\n    click B \"http://www.github.com\" \"This is a tooltip for a link\"\n    click A call callback() \"Tooltip for a callback\"\n    click B href \"http://www.github.com\" \"This is a tooltip for a link\"\n</code></pre> <p>By default the links are opened in the same browser tab/window. It is possible to change this by adding a link target to the click definition (<code>_self</code>, <code>_blank</code>, <code>_parent</code>, or <code>_top</code>).</p> <pre><code>graph LR;\n    A--&gt;B;\n    B--&gt;C;\n    C--&gt;D;\n    D--&gt;E;\n    click A \"http://www.github.com\" _blank\n</code></pre>"}, {"location": "mermaidjs/#node-styling", "title": "Node styling", "text": "<p>You can define the style for each node with:</p> <pre><code>graph LR\n    id1(Start)--&gt;id2(Stop)\n    style id1 fill:#f9f,stroke:#333,stroke-width:4px\n    style id2 fill:#bbf,stroke:#f66,stroke-width:2px,color:#fff,stroke-dasharray: 5 5\n</code></pre> <p>Or if you're going to use the same style for multiple nodes, you can define classes:</p> <pre><code>graph LR\n    A:::someclass --&gt; B\n    classDef someclass fill:#f96;\n</code></pre>"}, {"location": "mermaidjs/#references", "title": "References", "text": "<ul> <li>Docs</li> </ul>"}, {"location": "mizu/", "title": "Mizu", "text": "<p>Mizu is an API Traffic Viewer for Kubernetes, think <code>TCPDump</code> and Chrome Dev Tools combined.</p>"}, {"location": "mizu/#installation", "title": "Installation", "text": "<pre><code>curl -Lo mizu \\\nhttps://github.com/up9inc/mizu/releases/latest/download/mizu_linux_amd64 \\\n&amp;&amp; chmod 755 mizu\n</code></pre>"}, {"location": "mizu/#usage", "title": "Usage", "text": "<p>At the core of Mizu functionality is the pod tap</p> <pre><code>mizu tap &lt;podname&gt;\n</code></pre> <p>To view traffic of several pods, identified by a regular expression:</p> <pre><code>mizu tap \"(catalo*|front-end*)\"\n</code></pre> <p>After tapping your pods, Mizu will tell you that \"Web interface is now available at <code>https://localhost:8899/</code>. Visit the link from Mizu to view traffic in the Mizu UI.</p>"}, {"location": "mizu/#references", "title": "References", "text": "<ul> <li>Homepage</li> <li>Docs</li> </ul>"}, {"location": "molecule/", "title": "Molecule", "text": "<p>Molecule is a testing tool for ansible roles.</p>"}, {"location": "molecule/#installation", "title": "Installation", "text": "<pre><code>pip install molecule\n</code></pre>"}, {"location": "molecule/#ci-configuration", "title": "CI configuration", "text": "<p>Since gitea supports github actions you can use the <code>setup-molecule</code> and <code>setup-lint</code> actions. For example:</p> <pre><code>---\nname: Molecule\n\n\"on\":\npull_request:\n\nenv:\nPY_COLORS: \"1\"\nANSIBLE_FORCE_COLOR: \"1\"\njobs:\nlint:\nname: Lint\nruns-on: ubuntu-latest\nsteps:\n- name: Checkout the codebase\nuses: actions/checkout@v3\n\n- name: Setup Lint\nuses: bec-galaxy/setup-lint@{Version}\n\n- name: Run Lint tests\nrun: ansible-lint\n\nmolecule:\nname: Molecule\nruns-on: ubuntu-latest\nneeds: lint\nsteps:\n- name: Checkout the codebase\nuses: actions/checkout@v3\n\n- name: Setup Molecule\nuses: bec-galaxy/setup-molecule@{Version}\n\n- name: Run Molecule tests\nrun: molecule test\n</code></pre> <p>That action installs the latest version of the packages, if you need to check a specific version of the packages you may want to create your own step or your own action.</p>"}, {"location": "molecule/#upgrade", "title": "Upgrade", "text": ""}, {"location": "molecule/#to-v500", "title": "To v5.0.0", "text": "<p>They've removed the <code>lint</code> command, the reason behind is that there are two different testing methods which are expected to be run in very different ways. Linting should be run per entire repository. Molecule executions are per scenario and one project can have even &gt;100 scenarios. Running lint on each of them would not only slowdown but also increase the maintenance burden on linter configuration and the way is called.</p> <p>They recommend users to run <code>ansible-lint</code> using <code>pre-commit</code> with or without <code>tox</code>. That gives much better control over how/when it is updated.</p> <p>You can see an example on how to do this in the CI configuration section.</p>"}, {"location": "molecule/#to-v400", "title": "To v4.0.0", "text": "<p>This version is seen as a clean-up or refactoring release, not expected to require users to change their existing scenarios in order to make use of the new version.</p>"}, {"location": "molecule/#snippets", "title": "Snippets", "text": ""}, {"location": "molecule/#get-variables-from-the-environment", "title": "Get variables from the environment", "text": "<p>You can configure your <code>molecule.yaml</code> file to read variables from the environment with:</p> <pre><code>provisioner:\nname: ansible\ninventory:\ngroup_vars:\nall:\nmy_secret: ${MY_SECRET}\n</code></pre> <p>It's useful to have a task that checks if this secret exists:</p> <pre><code>- name: Verify that the secret is set\nfail: msg: 'Please export my_secret: export MY_SECRET=$(pass show my_secret)'\nrun_once: true\nwhen: my_secret == None\n</code></pre> <p>In the CI you can set it as a secret in the repository.</p>"}, {"location": "molecule/#troubleshooting", "title": "Troubleshooting", "text": ""}, {"location": "molecule/#molecule-doesnt-find-the-moleculeyaml-file", "title": "Molecule doesn't find the <code>molecule.yaml</code> file", "text": "<p>This is expected default behavior since Molecule searches for scenarios using the <code>molecule/*/molecule.yml</code> glob. But if you would like to change the suffix to yaml, you can do that if you set the <code>MOLECULE_GLOB</code> environment variable like this:</p> <pre><code>export MOLECULE_GLOB='molecule/*/molecule.yaml'\n</code></pre>"}, {"location": "molecule/#references", "title": "References", "text": "<ul> <li>Source</li> <li>Docs</li> </ul>"}, {"location": "money_management/", "title": "Money Management", "text": "<p>Money management is the act of analyzing where you spend your money on with the least amount of mental load.</p> <p>Some years ago I started using the double entry counting method with beancount.</p>"}, {"location": "money_management/#system-inputs", "title": "System inputs", "text": "<p>I have two types of financial transactions to track:</p> <ul> <li> <p>The credit/debit card movements: Easy to track as usually the banks support     exporting them as CSV, and beancount have specific bank     importers.</p> </li> <li> <p>The cash movements: Harder to track as you need to keep them manually. This     has been my biggest source of errors, once I understood how to correctly use     beancount.</p> <p>In the latest iteration, I'm using the cone Android app to keep track of these expenses.</p> </li> </ul>"}, {"location": "money_management/#workflow", "title": "Workflow", "text": ""}, {"location": "money_management/#beancount-ledger-organization", "title": "Beancount ledger organization", "text": "<p>My beancount project directory tree is:</p> <pre><code>.\n\u251c\u2500\u2500 .git\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 2011\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 09.book\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 10.book\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 11.book\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 12.book\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 year.book\n\u251c\u2500\u2500 ...\n\u251c\u2500\u2500 2020\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 01.book\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 02.book\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 ...\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 11.book\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 12.book\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 year.book\n\u251c\u2500\u2500 ledger.book\n\u251c\u2500\u2500 .closed.accounts.book\n\u251c\u2500\u2500 roadmap.md\n\u2514\u2500\u2500 to.process\n    \u251c\u2500\u2500 cone.book\n    \u251c\u2500\u2500 bank1.csv\n    \u2514\u2500\u2500 bank2.csv\n</code></pre> <p>Where:</p> <ul> <li><code>.git</code>: I keep everything in a git repository to have a version controlled ledger.</li> <li>Each year has it's own directory with:<ul> <li>A <code>book</code> file per month, check below it's contents.</li> <li> <p>A <code>year.book</code> file with just include statements:</p> <p><pre><code>include \"01.book\"\ninclude \"02.book\"\ninclude \"03.book\"\ninclude \"04.book\"\ninclude \"05.book\"\ninclude \"06.book\"\ninclude \"07.book\"\ninclude \"08.book\"\ninclude \"09.book\"\n# include \"10.book\"\n# include \"11.book\"\n# include \"12.book\"\n</code></pre> * <code>ledger.book</code>: The beancount entry point where the accounts are defined. * <code>.closed.accounts.book</code>: To store the account closing statements. * <code>roadmap.md</code>: To store the financial plan for the semester/year/life. * <code>to.process</code>: To store the raw data from external sources.</p> </li> </ul> </li> </ul>"}, {"location": "money_management/#the-main-ledger", "title": "The main ledger", "text": "<p>The <code>ledger.book</code> file contains the beancount configuration, with the opening of accounts and inclusion of the monthly books. I like to split it in sections.</p> TL;DR: The full <code>ledger.book</code> <pre><code># Options\noption \"title\" \"Lyz Lair Ledge\"\noption \"operating_currency\" \"EUR\"\n\n# Events\n2016-12-19 event \"employer\" \"XXX\"\n\n# Eternal accounts\n\n# Assets\n\n2010-05-17 open Assets:Cash EUR\n2021-01-25 open Assets:Cash:Coins EUR\n2021-01-25 open Assets:Cash:Paper EUR\n2018-09-10 open Assets:Cashbox EUR\n2019-01-11 open Assets:Cashbox:Coins EUR\n2019-01-11 open Assets:Cashbox:Paper EUR\n2018-04-01 open Assets:Savings EUR\n2019-08-01 open Assets:Savings:CashFlowRefiller EUR\n2019-08-01 open Assets:Savings:UnexpectedExpenses EUR\n2019-08-01 open Assets:Savings:Home EUR\n2018-04-01 open Assets:CashFlowCard EUR\n2018-04-01 open Assets:CashDeposit EUR\n\n# Debts\n\n2016-01-01 open Assets:Debt:Person1 EUR\n2016-01-01 open Assets:Debt:Person2 EUR\n\n# Income\n\n2016-12-01 open Income:Employer1 EUR\n2019-05-21 open Income:Employer2 EUR\n2010-05-17 open Income:State EUR\n2019-01-01 open Income:Gifts EUR\n\n# Equity\n\n2010-05-17 open Equity:Opening-Balances\n2010-05-17 open Equity:Errors\n2010-05-17 open Equity:Forgiven\n\n# Expenses\n\n2010-01-01 open Expenses:Bills EUR\n2013-01-01 open Expenses:Bills:Gas EUR\n2010-01-01 open Expenses:Bills:Phone EUR\n2019-01-01 open Expenses:Bills:Light EUR\n2010-01-01 open Expenses:Bills:Rent EUR\n2010-01-01 open Expenses:Bills:PublicTransport EUR\n2017-01-01 open Expenses:Bills:Subscriptions EUR\n2016-01-01 open Expenses:Bills:Union EUR\n2010-01-01 open Expenses:Books EUR\n2010-12-01 open Expenses:Car EUR\n2010-12-01 open Expenses:Car:Fuel EUR\n2010-12-01 open Expenses:Car:Insurance EUR\n2010-12-01 open Expenses:Car:Repair EUR\n2010-12-01 open Expenses:Car:Taxes EUR\n2010-12-01 open Expenses:Car:Tickets EUR\n2010-01-01 open Expenses:Clothes EUR\n2018-11-01 open Expenses:Donations EUR\n2010-05-17 open Expenses:Financial EUR\n2010-01-01 open Expenses:Games EUR\n2010-01-01 open Expenses:Games:Steam EUR\n2010-01-01 open Expenses:Games:HumbleBundle EUR\n2019-06-01 open Expenses:Games:GOG EUR\n2020-06-01 open Expenses:Games:Itchio EUR\n2010-01-01 open Expenses:Gifts EUR\n2010-01-01 open Expenses:Gifts:Person1 EUR\n2010-01-01 open Expenses:Gifts:Person2 EUR\n2010-01-01 open Expenses:Gifts:Mine EUR\n2010-01-01 open Expenses:Groceries EUR\n2018-11-01 open Expenses:Groceries:Extras EUR\n2020-01-01 open Expenses:Groceries:Supermarket EUR\n2020-01-01 open Expenses:Groceries:Prepared EUR\n2020-01-01 open Expenses:Groceries:GreenGrocery EUR\n2010-01-01 open Expenses:Hardware EUR\n2010-01-01 open Expenses:Home EUR\n2010-01-01 open Expenses:Home:WashingMachine EUR\n2010-01-01 open Expenses:Home:DishWasher EUR\n2010-01-01 open Expenses:Home:Fridge EUR\n2020-06-01 open Expenses:Legal EUR\n2010-01-01 open Expenses:Medicines EUR\n2010-01-01 open Expenses:Social EUR\n2010-01-01 open Expenses:Social:Eat EUR\n2010-01-01 open Expenses:Social:Drink EUR\n2019-06-01 open Expenses:Taxes:Tax1 EUR\n2016-01-01 open Expenses:Taxes:Tax2 EUR\n2010-05-17 open Expenses:Trips EUR\n2010-05-17 open Expenses:Trips:Accommodation EUR\n2010-05-17 open Expenses:Trips:Drink EUR\n2010-05-17 open Expenses:Trips:Food EUR\n2010-05-17 open Expenses:Trips:Tickets EUR\n2010-05-17 open Expenses:Trips:Transport EUR\n2019-05-20 open Expenses:Work EUR\n2019-05-20 open Expenses:Work:Phone EUR\n2019-05-20 open Expenses:Work:Hardware EUR\n2019-05-20 open Expenses:Work:Trips EUR\n2019-05-20 open Expenses:Work:Trips:Accommodation EUR\n2019-05-20 open Expenses:Work:Trips:Drink EUR\n2019-05-20 open Expenses:Work:Trips:Food EUR\n2019-05-20 open Expenses:Work:Trips:Tickets EUR\n2019-05-20 open Expenses:Work:Trips:Transport EUR\n\n## Initialization\n\n2010-05-17 pad Assets:Cash Equity:Opening-Balances\n2016-01-01 pad Assets:Debt:Person1 Equity:Opening-Balances\n\n# Transfers\n\ninclude \".closed.accounts.book\"\ninclude \"2011/year.book\"\ninclude \"2012/year.book\"\ninclude \"2013/year.book\"\ninclude \"2014/year.book\"\ninclude \"2015/year.book\"\ninclude \"2016/year.book\"\ninclude \"2017/year.book\"\ninclude \"2018/year.book\"\ninclude \"2019/year.book\"\ninclude \"2020/year.book\"\n</code></pre>"}, {"location": "money_management/#assets", "title": "Assets", "text": "<p>Asset accounts represent something you have.</p> <pre><code># Assets\n\n2010-05-17 open Assets:Cash EUR\n2021-01-25 open Assets:Cash:Coins EUR\n2021-01-25 open Assets:Cash:Paper EUR\n2018-09-10 open Assets:Cashbox EUR\n2019-01-11 open Assets:Cashbox:Coins EUR\n2019-01-11 open Assets:Cashbox:Paper EUR\n2018-04-01 open Assets:Savings EUR\n2019-08-01 open Assets:Savings:CashFlowRefiller EUR\n2019-08-01 open Assets:Savings:UnexpectedExpenses EUR\n2019-08-01 open Assets:Savings:Home EUR\n2018-04-01 open Assets:CashFlowCard EUR\n2018-04-01 open Assets:CashDeposit EUR\n</code></pre> <p>Being a privacy minded person, I try to pay everything by cash. To track it, I've created the following asset accounts:</p> <ul> <li><code>Assets:Cash:Paper</code>: Paper money in my wallet. I like to have a 5, 10 and 20     euro bills as it gives the best flexibility without carrying too much money.</li> <li><code>Assets:Cash:Coins</code>: Coins in my wallet.</li> <li><code>Assets:Cashbox:Paper</code>: Paper money stored at home. I fill it up monthly to     my average monthly expenses, so I reduce the trips to the ATM to the     minimum. Once this account is below 100 EUR, I add the mental task to refill     it.</li> <li><code>Assets:Cashbox:Coins</code>: Coins stored at home. I keep it at 10 EUR in     coins of 2 EUR, so it's quick to count at the same time as it's able to     cover most of the things you need to buy with coins.</li> <li><code>Assets:Cashbox:SmallCoins</code>: If my coins wallet is starting to get heavy,     I extract the coins smaller than 50 cents into a container with copper     coins.</li> <li><code>Assets:CashDeposit</code>: You never know when the bank system is going to fuck     you, so it's always good to have some cash under the mattress.</li> </ul> <p>Having this level of granularity and doing weekly balances of each of those accounts has helped me understand the flaws in my processes that lead to the cash accounting errors.</p> <p>As most humans living in the first world, I'm forced to have at least one bank account. For security reasons I have two:</p> <ul> <li><code>Assets:CashFlowCard</code>: The bank account with an associated debit card. Here is     from where I make my expenses, such as home rental, supplies payment, ATM     money withdrawal. As it is exposed to all the payment platforms, I assume     that it will come a time when a vulnerability is found in one of them, so     I keep the least amount of money I can. As with the <code>Cashbox</code> I monthly     refill it with the expected expenses amount plus a safety amount.</li> <li> <p><code>Assets:Savings</code>: The bank account where I store my savings. I have it     subdivided in three sections:</p> <ul> <li><code>Assets:Savings:CashFlowRefiller</code>: Here I store the average monthly     expenses for the following two months.</li> <li><code>Assets:Savings:UnexpectedExpenses</code>: Deposit for unexpected expenses such     as car or domestic appliances repairs.</li> <li><code>Assets:Savings:Home</code>: Deposit for the initial payment or a house.</li> </ul> </li> </ul>"}, {"location": "money_management/#debts", "title": "Debts", "text": "<p>Debts can be tracked either as an asset or as a liability. If you expect them to owe you more often (you lend a friend some money), model it as an asset, if you're going to owe them (you borrow from the bank to buy a house), model it as a liability.</p> <pre><code># Debts\n\n2016-01-01 open Assets:Debt:Person1 EUR\n2016-01-01 open Assets:Debt:Person2 EUR\n</code></pre>"}, {"location": "money_management/#income", "title": "Income", "text": "<p>Income accounts represent where you get the money from.</p> <pre><code># Income\n\n2016-12-01 open Income:Employer1 EUR\n2019-05-21 open Income:Employer2 EUR\n2010-05-17 open Income:State EUR\n2019-01-01 open Income:Gifts EUR\n</code></pre>"}, {"location": "money_management/#equity", "title": "Equity", "text": "<p>I use equity accounts to make adjustments.</p> <pre><code># Equity\n\n2010-05-17 open Equity:Opening-Balances\n2010-05-17 open Equity:Errors\n2010-05-17 open Equity:Forgiven\n</code></pre> <ul> <li><code>Equity:Opening-Balances</code>: Used to set the initial balance of an account.</li> <li><code>Equity:Errors</code>: Used with the <code>pad</code> statements to track the errors in the     accounting.</li> <li><code>Equity:Forgiven</code>: Used in the transactions to forgive someone's debts.</li> </ul>"}, {"location": "money_management/#expenses", "title": "Expenses", "text": "<p>Expense accounts model where you expend the money on.</p> <pre><code># Expenses\n\n2010-01-01 open Expenses:Bills EUR\n2013-01-01 open Expenses:Bills:Gas EUR\n2010-01-01 open Expenses:Bills:Phone EUR\n2019-01-01 open Expenses:Bills:Light EUR\n2010-01-01 open Expenses:Bills:Rent EUR\n2010-01-01 open Expenses:Bills:PublicTransport EUR\n2017-01-01 open Expenses:Bills:Subscriptions EUR\n2016-01-01 open Expenses:Bills:Union EUR\n2010-01-01 open Expenses:Books EUR\n2010-12-01 open Expenses:Car EUR\n2010-12-01 open Expenses:Car:Fuel EUR\n2010-12-01 open Expenses:Car:Insurance EUR\n2010-12-01 open Expenses:Car:Repair EUR\n2010-12-01 open Expenses:Car:Taxes EUR\n2010-12-01 open Expenses:Car:Tickets EUR\n2010-01-01 open Expenses:Clothes EUR\n2018-11-01 open Expenses:Donations EUR\n2010-05-17 open Expenses:Financial EUR\n2010-01-01 open Expenses:Games EUR\n2010-01-01 open Expenses:Games:Steam EUR\n2010-01-01 open Expenses:Games:HumbleBundle EUR\n2019-06-01 open Expenses:Games:GOG EUR\n2020-06-01 open Expenses:Games:Itchio EUR\n2010-01-01 open Expenses:Gifts EUR\n2010-01-01 open Expenses:Gifts:Person1 EUR\n2010-01-01 open Expenses:Gifts:Person2 EUR\n2010-01-01 open Expenses:Gifts:Mine EUR\n2010-01-01 open Expenses:Groceries EUR\n2018-11-01 open Expenses:Groceries:Extras EUR\n2020-01-01 open Expenses:Groceries:Supermarket EUR\n2020-01-01 open Expenses:Groceries:Prepared EUR\n2020-01-01 open Expenses:Groceries:GreenGrocery EUR\n2010-01-01 open Expenses:Hardware EUR\n2010-01-01 open Expenses:Home EUR\n2010-01-01 open Expenses:Home:WashingMachine EUR\n2010-01-01 open Expenses:Home:DishWasher EUR\n2010-01-01 open Expenses:Home:Fridge EUR\n2020-06-01 open Expenses:Legal EUR\n2010-01-01 open Expenses:Medicines EUR\n2010-01-01 open Expenses:Social EUR\n2010-01-01 open Expenses:Social:Eat EUR\n2010-01-01 open Expenses:Social:Drink EUR\n2019-06-01 open Expenses:Taxes:Tax1 EUR\n2016-01-01 open Expenses:Taxes:Tax2 EUR\n2010-05-17 open Expenses:Trips EUR\n2010-05-17 open Expenses:Trips:Accommodation EUR\n2010-05-17 open Expenses:Trips:Drink EUR\n2010-05-17 open Expenses:Trips:Food EUR\n2010-05-17 open Expenses:Trips:Tickets EUR\n2010-05-17 open Expenses:Trips:Transport EUR\n2019-05-20 open Expenses:Work EUR\n2019-05-20 open Expenses:Work:Phone EUR\n2019-05-20 open Expenses:Work:Hardware EUR\n2019-05-20 open Expenses:Work:Trips EUR\n2019-05-20 open Expenses:Work:Trips:Accommodation EUR\n2019-05-20 open Expenses:Work:Trips:Drink EUR\n2019-05-20 open Expenses:Work:Trips:Food EUR\n2019-05-20 open Expenses:Work:Trips:Tickets EUR\n2019-05-20 open Expenses:Work:Trips:Transport EUR\n</code></pre> <p>I decided to split my expenses in:</p> <ul> <li><code>Expenses:Bills</code>: All the periodic bills I pay<ul> <li><code>Expenses:Bills:Gas</code>:</li> <li><code>Expenses:Bills:Phone</code>:</li> <li><code>Expenses:Bills:Light</code>:</li> <li><code>Expenses:Bills:Rent</code>:</li> <li><code>Expenses:Bills:PublicTransport</code>:</li> <li><code>Expenses:Bills:Subscriptions</code>: Newspaper, magazine, web service     subscriptions.</li> <li><code>Expenses:Bills:Union</code>:</li> </ul> </li> <li><code>Expenses:Books</code>:</li> <li><code>Expenses:Car</code>:<ul> <li><code>Expenses:Car:Fuel</code>:</li> <li><code>Expenses:Car:Insurance</code>:</li> <li><code>Expenses:Car:Repair</code>:</li> <li><code>Expenses:Car:Taxes</code>:</li> <li><code>Expenses:Car:Tickets</code>:</li> </ul> </li> <li><code>Expenses:Clothes</code>:</li> <li><code>Expenses:Donations</code>:</li> <li><code>Expenses:Financial</code>: Expenses related to financial operations or account     maintenance.</li> <li><code>Expenses:Games</code>:<ul> <li><code>Expenses:Games:Steam</code>:</li> <li><code>Expenses:Games:HumbleBundle</code>:</li> <li><code>Expenses:Games:GOG</code>:</li> <li><code>Expenses:Games:Itchio</code>:</li> </ul> </li> <li><code>Expenses:Gifts</code>:<ul> <li><code>Expenses:Gifts:Person1</code>:</li> <li><code>Expenses:Gifts:Person2</code>:</li> <li><code>Expenses:Gifts:Mine</code>:</li> </ul> </li> <li><code>Expenses:Groceries</code>:<ul> <li><code>Expenses:Groceries:Extras</code>:</li> <li><code>Expenses:Groceries:Supermarket</code>:</li> <li><code>Expenses:Groceries:Prepared</code>:</li> <li><code>Expenses:Groceries:GreenGrocery</code>:</li> </ul> </li> <li><code>Expenses:Hardware</code>:</li> <li><code>Expenses:Home</code>:<ul> <li><code>Expenses:Home:WashingMachine</code>:</li> <li><code>Expenses:Home:DishWasher</code>:</li> <li><code>Expenses:Home:Fridge</code>:</li> </ul> </li> <li><code>Expenses:Legal</code>:</li> <li><code>Expenses:Medicines</code>:</li> <li><code>Expenses:Social</code>:<ul> <li><code>Expenses:Social:Eat</code>:</li> <li><code>Expenses:Social:Drink</code>:</li> </ul> </li> <li><code>Expenses:Taxes</code>:<ul> <li><code>Expenses:Taxes:Tax1</code>:</li> <li><code>Expenses:Taxes:Tax2</code>:</li> </ul> </li> <li><code>Expenses:Trips</code>:<ul> <li><code>Expenses:Trips:Accommodation</code>:</li> <li><code>Expenses:Trips:Drink</code>:</li> <li><code>Expenses:Trips:Food</code>:</li> <li><code>Expenses:Trips:Tickets</code>:</li> <li><code>Expenses:Trips:Transport</code>:</li> </ul> </li> <li><code>Expenses:Work</code>:<ul> <li><code>Expenses:Work:Phone</code>:</li> <li><code>Expenses:Work:Hardware</code>:</li> <li><code>Expenses:Work:Trips</code>:</li> <li><code>Expenses:Work:Trips:Accommodation</code>:</li> <li><code>Expenses:Work:Trips:Drink</code>:</li> <li><code>Expenses:Work:Trips:Food</code>:</li> <li><code>Expenses:Work:Trips:Tickets</code>:</li> <li><code>Expenses:Work:Trips:Transport</code>:</li> </ul> </li> </ul>"}, {"location": "money_management/#initialization-of-accounts", "title": "Initialization of accounts", "text": "<pre><code># Initialization\n\n2010-05-17 pad Assets:Cash Equity:Opening-Balances\n2016-01-01 pad Assets:Debt:Person1 Equity:Opening-Balances\n</code></pre>"}, {"location": "money_management/#transfer-includes", "title": "Transfer includes", "text": "<p>I reference each year's <code>year.book</code> and the <code>.closed.accounts.book</code>.</p> <pre><code># Transfers\n\ninclude \".closed.accounts.book\"\ninclude \"2011/year.book\"\n...\ninclude \"2020/year.book\"\n</code></pre>"}, {"location": "money_management/#the-monthly-book", "title": "The monthly book", "text": "<p>Each month has a file with this structure:</p> <pre><code># Cash transfers\n\n# CashFlowCard\n\n# Savings\n\n# Balances taken at 2020-12-06T19:32\n\n## Active accounts\n\n2020-12-06 balance Assets:Cashbox:Paper  EUR\n2020-12-06 balance Assets:Cashbox:Coins  EUR\n2020-12-06 balance Assets:Cash:Paper  EUR\n2020-12-06 balance Assets:Cash:Coins  EUR\n\n2020-12-06 balance Assets:CashFlowCard  EUR\n2020-12-06 balance Assets:Savings  EUR\n\n## Deposits\n\n2020-12-06 balance Assets:Savings:CashFlowRefiller XXX EUR\n2020-12-06 balance Assets:Savings:UnexpectedExpenses XXX EUR\n2020-12-06 balance Assets:CashDeposit XXX EUR\n\n## Debts\n\n2020-12-06 balance Assets:Debt:Person1 XXX EUR\n\n## Equity\n\n# 2020-12-05 pad Assets:Cash Equity:Errors\n# 2020-12-05 pad Assets:Cashbox Equity:Errors\n\n# Weekly balances\n\n## Measure done on 2020-09-04T17:10\n2020-09-04 balance Assets:Cash:Coins XXX EUR\n2020-09-04 balance Assets:Cash:Paper XXX EUR\n2020-09-04 balance Assets:Cashbox:Coins XXX EUR\n2020-09-04 balance Assets:Cashbox:Paper XXX EUR\n</code></pre> <p>Where each section stores:</p> <ul> <li> <p><code>Cash transfers</code>: The transactions done by cash, extracted from the     Android <code>cone</code> application.</p> </li> <li> <p><code>CashFlowCard</code>: Bank account extracts transformed from the csv to postings     with <code>bean-extract</code>.</p> </li> <li> <p><code>Savings</code>: Bank account extracts transformed from the csv to postings     with <code>bean-extract</code>.</p> </li> <li> <p><code>Monthly balances</code>: I try to review the accounts once each month. This section is subdivided in:</p> <ul> <li><code>Active accounts</code>: The accounts whose value changes monthly.</li> <li><code>Deposits</code>: The accounts that don't change much each month.</li> <li><code>Debts</code>: The balance of debt accounts.</li> <li><code>Equity</code>: The <code>pad</code> statements to track the errors in the monthly account.</li> </ul> </li> <li> <p><code>Weekly balances</code>: As doing the monthly review is long, but it doesn't give me     the enough information to not mess up the cash transactions, I do a weekly     balance of those accounts.</p> </li> </ul>"}, {"location": "monitoring_comparison/", "title": "Monitoring Comparison", "text": "<p>As with any technology, when you want to adopt it, you first need to analyze your options. In this article we're going to compare the two most popular solutions at the moment, Nagios and Prometheus. Zabbix is similar in architecture and features to Nagios, so for the first iteration we're going to skip it.</p> <p>TL;DR: Prometheus is better, but it needs more effort.</p> <p>Nagios is suitable for basic monitoring of small and/or static systems where blackbox probing is sufficient.</p> <p>If you want to do whitebox monitoring, or have a dynamic or cloud based environment, then Prometheus is a good choice.</p>"}, {"location": "monitoring_comparison/#nagios", "title": "Nagios", "text": "<p>Nagios is an industry leader in IT infrastructure monitoring. It has four different products to choose from:</p> <ul> <li> <p>Nagios XI: Is an enterprise-ready server and network monitoring system that     supplies data to track app or network infrastructure health, performance,     availability, of the components, protocols, and services. It has     a user-friendly interface that allows UI configuration, customized     visualizations, and alert preferences.</p> </li> <li> <p>Nagios Log Server: It's used for log management and analysis of user     scenarios. It has the ability to correlate logged events across different     services and servers in real time, which helps with the investigation of     incidents and the performance of root cause analysis.</p> <p>Because Nagios Log Server\u2019s design is specifically for network security and audits, it lets users generate alerts for suspicious operations and commands. Log Server retains historical data from all events, supplying organizations with everything they need to pass a security audit.</p> </li> <li> <p>Nagios Network Analyzer: It's a tool for collecting and displaying either     metrics or extra information about an application network. It identifies     which IPs are communicating with the application servers and what requests     they\u2019re sending. The Network Analyzer maintains a record of all server     traffic, including who connected a specific server, to a specific port and     the specific request.</p> <p>This helps plan out server and network capacity, plus understand various kinds of security breaches likes unauthorized access, data leaks, DDoS, and viruses or malwares on servers.</p> </li> <li> <p>Nagios Fusion: is a compilation of the three tools Nagios offers. It provides     a complete solution that assists businesses in satisfying any and all of     their monitoring requirements. Its design is for scalability and for     visibility of the application and all of its dependencies.</p> </li> </ul>"}, {"location": "monitoring_comparison/#prometheus", "title": "Prometheus", "text": "<p>Prometheus is a free software application used for event monitoring and alerting. It records real-time metrics in a time series database (allowing for high dimensionality) built using a HTTP pull model, with flexible queries and real-time alerting. The project is written in Go and licensed under the Apache 2 License, with source code available on GitHub, and is a graduated project of the Cloud Native Computing Foundation, along with Kubernetes and Envoy.</p> <p>At the core of the Prometheus monitoring system is the main server, which ingests samples from monitoring targets. A target is any application that exposes metrics according to the open specification understood by Prometheus. Since Prometheus pulls data, rather than expecting targets to actively push stats into the monitoring system, it supports a variety of service discovery integrations, like that with Kubernetes, to immediately adapt to changes in the set of targets.</p> <p>The second core component is the Alertmanager, implementing the idea of time series based alerting. It intelligently removes duplicate alerts sent by Prometheus servers, groups the alerts into informative notifications, and dispatches them to a variety of integrations, like those with PagerDuty and Slack. It also handles silencing of selected alerts and advanced routing configurations for notifications.</p> <p>There are several additional Prometheus components, such as client libraries for different programming languages, and a growing number of exporters. Exporters are small programs that provide Prometheus compatible metrics from systems that are not natively instrumented.</p>"}, {"location": "monitoring_comparison/#comparison", "title": "Comparison", "text": "<p>For each dimension we'll check how each solution meets the criteria. An aggregation of all the results can be found in the summary.</p>"}, {"location": "monitoring_comparison/#open-source", "title": "Open source", "text": "<p>Only the Nagios Core is open sourced, it provides basic monitoring but it's enhanced by community contributions. It's also the base of the rest solutions, which are proprietary.</p> <p>Prometheus is completely open source under the Apache 2.0 license.</p>"}, {"location": "monitoring_comparison/#community", "title": "Community", "text": "<p>In Nagios, only the Nagios Core is an open-source tool. The rest are proprietary, so there is no community behind them.</p> <p>Community contributions to Nagios are gathered in the Nagios Exchange, it's hard to get other activity statistics than the overall number of contributions, but there are more than 850 addons, 4.5k plugins and 300 documentation contributions.</p> <p>Overall metrics (2021-02-22):</p> Metric Nagios Core Prometheus Stars 932 35.4k Forks 341 5.7k Watch 121 1.2k Commits 3.4k 8.5k Open Issues 195 290 Closed Issues 455 3.5k Open PR 9 116 Closed PR 155 4.5k <p>Last month metrics (2021-02-22):</p> Metric Nagios Core Prometheus Active PR 1 80 Active Issues 3 64 Commits 0 74 Authors 0 35 <p>We can see that Prometheus in comparison with Nagios Core is:</p> <ul> <li>More popular in terms of community contributions.</li> <li>More maintained.</li> <li>Growing more.</li> <li>Development is more distributed.</li> <li>Manages the issues collaboratively.</li> </ul> <p>This comparison is biased though, because Nagios comes from a time where GitHub and Git (and Youtube!) did not exist, and the communities formed around different sites.</p> <p>Also, given that Nagios has almost 20 years of existence, and that it forked from a previous monitoring project (NetSaint), the low number contributions indicate a stable and mature product, whereas the high numbers for Prometheus are indicators of a young, still in development product.</p> <p>Keep in mind that this comparison only analyzes the core, it doesn't take into account the metrics of the community contributions, as it is not easy to aggregate their statistics.</p> <p>Which makes Prometheus one of the biggest open-source projects in existence. It actually has hundreds of contributors maintaining it. The tool continues to be up-to-date to contemporary and popular apps, extending its list of exporters and responding to requests.</p> <p>On 16 January 2014, Nagios Enterprises redirected the nagios-plugins.org domain to a web server controlled by Nagios Enterprises without explicitly notifying the Nagios Plugins community team the consequences of their actions. Nagios Enterprises replaced the nagios-plugins team with a group of new, different members. The community team members who were replaced continued their work under the name Monitoring Plugins along with a new website with the new domain of monitoring-plugins.org. Which is a nasty move against the community.</p>"}, {"location": "monitoring_comparison/#configuration-and-usage", "title": "Configuration and usage", "text": "<p>Neither solution is easy to configure, you need to invest time in them.</p> <p>Nagios is easier to use for non technical users though.</p>"}, {"location": "monitoring_comparison/#visualizations", "title": "Visualizations", "text": "<p>The graphs and dashboards Prometheus provides don't meet today's needs. As a result, users resort to other visualization tools to display metrics collected by Prometheus, often Grafana.</p> <p>Nagios comes with a set of dashboards that fit the requirements of monitoring networks and infrastructure components. Yet, it still lacks graphs for more applicative-related issues.</p> <p>Personally I find Grafana dashboards more beautiful and easier to change. It also has a massive community behind providing customizable dashboards for free.</p>"}, {"location": "monitoring_comparison/#installation", "title": "Installation", "text": "<p>Nagios comes as a downloadable bundle with dedicated packages for every product with Windows or Linux distributions. After downloading and installing the tool, a set of first-time configurations is required. Once you\u2019ve installed the Nagios agents, data should start streaming into Nagios and its generic dashboards.</p> <p>Prometheus deployment is done through Docker containers that can spin up on every machine type, or through pre-compiled or self-compiled binaries.</p> <p>There are community maintained ansible roles for both solutions, doing a quick search I've found a Prometheus one that it's more maintained.</p> <p>For Kubernetes installation, I've only found helm charts for Prometheus.</p>"}, {"location": "monitoring_comparison/#kubernetes-integration", "title": "Kubernetes integration", "text": "<p>Prometheus, as Kubernetes are leading projects of the Cloud Native Computing Foundation, which is a Linux Foundation project that was founded in 2015 to help advance container technology and align the tech industry around its evolution.</p> <p>Prometheus has native support to be run in and to monitor Kubernetes clusters. Although Nagios can monitor Kubernetes, it's not meant to be run inside it.</p>"}, {"location": "monitoring_comparison/#documentation", "title": "Documentation", "text": "<p>I haven't used much the Nagios documentation, but I can tell you that even though it's improving Prometheus' is not very complete, and you find yourself often looking at issues and stackoverflow.</p>"}, {"location": "monitoring_comparison/#integrations", "title": "Integrations", "text": "<p>Official Prometheus\u2019 integrations are practically boundless. The long list of existing exporters combined with the user\u2019s ability to write new exporters allows integration with any tool, and PromQL allows users to query Prometheus data from any visualization tool that supports it.</p> <p>Nagios has a very limited list of official integrations. Most of them are operating systems which use the agents to monitor other network components. Others include MongoDB, Oracle, Selenium, and VMware. Once again, the community comes to rescue us with their contributions, keep in mind that you'll need to dive into the exchange for special monitoring needs.</p>"}, {"location": "monitoring_comparison/#alerts", "title": "Alerts", "text": "<p>Prometheus offers Alertmanager, a simple service that allows users to set thresholds and push alerts when breaches occur.</p> <p>Nagios uses a variety of media channels for alerts, including email, SMS, and audio alerts. Because its integration with the operating system is swift, Nagios even knows to generate a WinPopup message with the alert details.</p> <p>On a side note, there is an alert Nagios plugin that alerts for Prometheus query results.</p> <p>As Nagios doesn't support labels for the metrics, so there is no grouping, routing or deduplication of alerts as Prometheus do. Also the silence of alerts is done individually on each alert, while in Prometheus it's done using labels, which is more powerful.</p>"}, {"location": "monitoring_comparison/#advanced-monitorization", "title": "Advanced monitorization", "text": "<p>Nagios alerting is based on the return codes of scripts, Prometheus on the other hand alerts based on metrics, this fact together with the easy and powerful query language PromQL allows the user to make much more rich alerts that better represent the state of the system to monitor.</p> <p>In Nagios there is no concept of making queries to the gathered data.</p>"}, {"location": "monitoring_comparison/#data-storage", "title": "Data storage", "text": "<p>Nagios has no storage per-se, beyond the current check state. There are plugins which can store data such as for visualisation.</p> <p>Prometheus has a defined amount of data that's available (for example 30 days), to be able to store more you need to use Thanos, the prometheus long term storage solution.</p>"}, {"location": "monitoring_comparison/#high-availability", "title": "High availability", "text": "<p>Nagios servers are standalone, they are not meant to collaborate with other instances, so to achieve high availability you need to do it the old way, with multiple independent instances with a loadbalancer upfront.</p> <p>Prometheus can have different servers running collaboratively, monitoring between themselves. So you get high availability for free without any special configuration.</p>"}, {"location": "monitoring_comparison/#dynamic-infrastructure", "title": "Dynamic infrastructure", "text": "<p>In the past, infrastructure had a low rate of change, it was strange that you needed to add something to the monitorization system. Nowadays, with cloud infrastructures and kubernetes, instances are spawned and killed continuously.</p> <p>In Nagios, you need to manually configure each new service following the push architecture. In prometheus, thanks to the pull architecture and service discovery, new services are added and dead one removed automatically.</p>"}, {"location": "monitoring_comparison/#custom-script-execution", "title": "Custom script execution", "text": "<p>Nagios alerting is based on the return codes of scripts, therefore it's straightforward to create an alert based on a custom script.</p> <p>If you need to monitor something in Prometheus, and nobody has done it before, the development costs of an ad-hoc solutions are incredibly high, compared to Nagios. You'd need either to:</p> <ul> <li> <p>Use the script_exporter with     your script.  I've seen their repo, and the last commit is from March, and     they don't have a helm chart to install     it. I've searched     other alternative exporters, but this one seems to be the best for this     approach.</p> <p>The advantages of this approach is that you don't need to create and maintain a new prometheus exporter.</p> <p>The disadvantages though are that you'd have to:</p> <ul> <li>Manually install the required exporter resources in the cluster until a helm chart     exists.</li> <li>Create the helm charts yourself if they don't develop it.</li> <li> <p>Integrate your tool inside the script_exporter docker through one of these     ways:</p> <ul> <li>Changing the exporter Docker image to add it. Which would mean a Docker image     to maintain.</li> <li>Mounting the binary through a volume inside kubernetes. Which would mean     defining a way on how to upload it and assume the high availability penalty     that a stateful kubernetes service entail with the cluster configuration right     now.<ul> <li>If it's not already in your stack, it would mean adding a new exporter to maintain and a new development team to depend on.</li> </ul> </li> </ul> </li> </ul> <p>Alternatively you can use the script exporter binary in a baremetal or virtualized server instead of using a docker, that way you wouldn't need to maintain the different dockers for the different solutions, but you'd need a \"dedicated\" server for this purpose.</p> </li> <li> <p>Create your own exporter. You'd need to create a docker that exposes the command line functionality through a <code>metrics</code> endpoint. You wouldn't depend on a third party development team and would be able to use your script. On the other side it has the following disadvantages:</p> <ul> <li>We would need to create and maintain a new prometheus exporter. That would mean     creating and maintaining the Docker with the command line tool and a simple http     server that exposes the <code>/metrics</code> endpoint, that will run the command whenever the     Prometheus server accesses this endpoint.</li> <li>We add a new exporter to maintain but we develop it ourselves, so we don't depend on     third party developers.</li> </ul> </li> <li> <p>Use other exporters to do the check. For example, if you can deduce the     critical API call that will decide if the script fails or succeeds, you     could use the blackbox exporter to monitor it instead. The advantages of     this solution are:</p> <ul> <li>We don't add new infrastructure to develop or maintain.</li> <li>We don't depend on third party development teams.</li> </ul> <p>And the disadvantage is that if the logic changes, we would need to update how we do the check.</p> </li> </ul>"}, {"location": "monitoring_comparison/#network-monitorization", "title": "Network monitorization", "text": "<p>Both can use the Simple Network Management Protocol (SNMP) to communicate with network switches or other components by using SNMP protocol to query their status.</p> <p>Not being an expert on the topic, knowing it's been one of the core focus of Nagios in the past years and as I've not been able to find good comparison between both, I'm going to suppose that even though both support network monitoring, Nagios does a better job.</p>"}, {"location": "monitoring_comparison/#summary", "title": "Summary", "text": "Metric Nagios Prometheus Open Source \u2713* \u2713\u2713 Community \u2713 \u2713\u2713 Configuration and usage \u2713 x Visualizations \u2713 \u2713\u2713 Ansible Role \u2713 \u2713\u2713 Helm chart x \u2713 Kubernetes x \u2713 Documentation \u2713 x Integrations \u2713 \u2713\u2713 Alerts \u2713 \u2713\u2713 Advanced monitoring x \u2713 Custom script execution \u2713\u2713 \u2713 Data storage x \u2713 Dynamic infrastructure x \u2713 High availability \u2713 \u2713\u2713 Network Monitoring \u2713 \u2713 <p>* Only Nagios Core and the community contributions are open sourced.</p> <p>Where each symbol means:</p> <ul> <li>x: Doesn't meet the criteria.</li> <li>\u2713: Meets the criteria.</li> <li>\u2713\u2713: Meets the criteria and it's better than the other solution.</li> <li>?: I'm not sure.</li> </ul> <p>Nagios is the reference of the old-school monitoring solutions, suitable for basic monitoring of small, static and/or old-school systems where blackbox probing is sufficient.</p> <p>Prometheus is the reference of the new-wave monitoring solutions, suitable for more advanced monitoring of dynamic, new-wave systems (web applications, cloud, containers or Kubernetes) where whitebox monitoring is desired.</p>"}, {"location": "monitoring_comparison/#references", "title": "References", "text": "<ul> <li>Logz io post on Prometheus vs Nagios</li> </ul>"}, {"location": "mopidy/", "title": "Mopidy", "text": "<p>Mopidy is an extensible music server written in Python.</p> <p>The key features are:</p> <ul> <li>Plays music from many sources: local disk, Spotify, SoundCloud, Google Play Music, and more.</li> <li>Can be used as a server: Out of the box, Mopidy is an HTTP server. If you     install the Mopidy-MPD     extension, it becomes an MPD server too. Given that MPD is a popular, old,     and robust solution, you can benefit of the many solutions that exist out     there for MPD.</li> <li>Edit the playlist from any phone, tablet, or computer using a variety of MPD     and web clients.</li> <li>It supports Beets as a library     source.</li> <li>Is hackable: The awesome documentation, being Python based, the extension     system, JSON-RPC, and JavaScript APIs make Mopidy a perfect base for your     projects.</li> </ul>"}, {"location": "mopidy/#references", "title": "References", "text": "<ul> <li>Docs</li> <li>Git</li> <li>Home</li> </ul>"}, {"location": "mopidy/#developer-info", "title": "Developer info", "text": "<ul> <li>API reference</li> <li>Write your own extension</li> </ul>"}, {"location": "music_management/", "title": "Music management", "text": "<p>Music management is the set of systems and processes to get and categorize songs so it's easy to browse and discover new content. It involves the next actions:</p> <ul> <li>Automatically index and download metadata of new songs.</li> <li>Notify the user when a new song is added.</li> <li>Monitor the songs of an artist, and get them once they are released.</li> <li>A nice interface to browse the existent library, with the possibility of     filtering by author, genre, years, tags or release types.</li> <li>An interface to listen to the music</li> <li>An interface to rate and review library items.</li> <li>An interface to discover new content based on the ratings and item metadata.</li> </ul>"}, {"location": "music_management/#components", "title": "Components", "text": "<p>I've got a music collection built from mediarss downloads, bought CDs rips and friend library sharing. It is more less organized in a directory tree by genre, but I lack any library management features. I have  a lot of duplicates, incoherent naming scheme, no way of filtering or intelligent playlist generation.</p> <p>playlist_generator helped me with the last point, based on the metadata gathered with mep, but it's still not enough.</p> <p>So I'm in my way of migrate all the library to beets, and then I'll deprecate mep in favor to a mpd client that allows me to keep on saving the same metadata.</p> <p>Once it's implemented, I'll migrate all the metadata to the new system.</p>"}, {"location": "music_management/#lidarr", "title": "Lidarr", "text": "<p>I'm also using Lidarr to manage what content is missing.</p> <p>Both Lidarr and <code>beets</code> get their from MusicBrainz. This means that sometimes some artist may lack a release, if they do, please contribute to MusicBrainz and add the information. Be patient, Lidarr may take some time to fetch the information, as it probably is not available straight away from the API.</p> <p>One awesome feature of Lidarr is that you can select the type of releases you want for each artist. They are defined in <code>Settings/Profiles/Metadata Profiles</code>. To be able to fine grain your settings, you first need to understand what do , Primary Types, Secondary Types and Release Status means.</p> <p>If you want to set the missing picture of an artist, you need to add it at fanart.tv. It's a process that needs the moderators approval, so don't expect it to be automatic. They are really kind when you don't do things right, but still, check their upload guidelines before you contribute.</p>"}, {"location": "musicbrainz/", "title": "MusicBrainz", "text": "<p>MusicBrainz is an open music encyclopedia that collects music metadata and makes it available to the public.</p> <p>MusicBrainz aims to be:</p> <ul> <li>The ultimate source of music information by allowing anyone to contribute and     releasing the data under open licenses.</li> <li>The universal lingua franca for music by providing a reliable and unambiguous     form of music identification, enabling both people and machines to have     meaningful conversations about music.</li> </ul> <p>Like Wikipedia, MusicBrainz is maintained by a global community of users and we want everyone \u2014 including you \u2014 to participate and contribute.</p>"}, {"location": "musicbrainz/#contributing", "title": "Contributing", "text": "<p>Creating an account is free and easy. To be able to add new releases easier, I've seen that there are some UserScript importers, they suggest to use the ViolentMonkey addon and install the desired plugins.</p> <p>With the Discogs one, if you don't see the <code>Import into MB</code> button is because you can't import a Master release, you have to click on a specific release. If that doesn't work for you, check these issues (1 and 2). It works without authentication.</p> <p>All the data is fetched for you except for the album cover, which you have to manually add.</p> <p>Make sure that you fill up the Release Status otherwise it won't show up in Lidarr.</p> <p>Then be patient, sometimes you need to wait hours before the changes are propagated to Lidarr.</p>"}, {"location": "musicbrainz/#filling-up-releases", "title": "Filling up releases", "text": "<p>Some notes on the release fields</p>"}, {"location": "musicbrainz/#primary-types", "title": "Primary types", "text": "<ul> <li> <p>Album: Perhaps better defined as a \"Long Play\" (LP) release, generally     consists of previously unreleased material (unless this type is combined     with secondary types which change that, such as \"Compilation\").</p> </li> <li> <p>Single: A single typically has one main song and possibly         a handful of additional tracks or remixes of the main track; the single         is usually named after its main song; the single is primarily released         to get radio play and to promote release sales.</p> </li> <li> <p>EP: An EP is a so-called \"Extended Play\" release and often contains the     letters EP in the title. Generally an EP will be shorter than a full length     release (an LP or \"Long Play\"), usually less than four tracks, and the     tracks are usually exclusive to the EP, in other words the tracks don't come     from a previously issued release.  EP is fairly difficult to define; usually     it should only be assumed that a release is an EP if the artist defines it     as such.</p> </li> <li> <p>Broadcast: An episodic release that was originally broadcast via radio,     television, or the Internet, including podcasts.</p> </li> <li> <p>Other: Any release that does not fit or can't decisively be placed in any of     the categories above.</p> </li> </ul>"}, {"location": "musicbrainz/#secondary-types", "title": "Secondary types", "text": "<ul> <li> <p>Compilation: A compilation, for the purposes of the MusicBrainz database,     covers the following types of releases:</p> <ul> <li>A collection of recordings from various old sources (not necessarily     released) combined together. For example a \"best of\", retrospective or     rarities type release.</li> <li>A various artists song collection, usually based on a general theme     (\"Songs for Lovers\"), a particular time period (\"Hits of 1998\"), or some     other kind of grouping (\"Songs From the Movies\", the \"Caf\u00e9 del Mar\"     series, etc).</li> </ul> <p>The MusicBrainz project does not generally consider the following to be compilations:</p> <ul> <li>A reissue of an album, even if it includes bonus tracks.</li> <li>A tribute release containing covers of music by another artist.</li> <li>A classical release containing new recordings of works by a classical artist.</li> <li>A split release containing new music by several artists</li> </ul> <p>Compilation should be used in addition to, not instead of, other types: for example, a various artists soundtrack using pre-released music should be marked as both a soundtrack and a compilation. As a general rule, always select every secondary type that applies.</p> </li> <li> <p>Soundtrack: A soundtrack is the musical score to a movie, TV series, stage     show, video game, or other medium. Video game CDs with audio tracks should     be classified as soundtracks because the musical properties of the CDs are     more interesting to MusicBrainz than their data properties.</p> </li> <li> <p>Spokenword: Non-music spoken word releases.</p> </li> <li> <p>Interview: An interview release contains an interview, generally with an     artist.</p> </li> <li> <p>Audiobook: An audiobook is a book read by a narrator without music.</p> </li> <li> <p>Audio drama: An audio drama is an audio-only performance of a play (often,     but not always, meant for radio). Unlike audiobooks, it usually has multiple     performers rather than a main narrator.</p> </li> <li> <p>Live: A release that was recorded live.</p> </li> <li> <p>Remix: A release that primarily contains remixed material.</p> </li> <li> <p>DJ-mix: A DJ-mix is a sequence of several recordings played one after the     other, each one modified so that they blend together into a continuous flow     of music. A DJ mix release requires that the recordings be modified in some     manner, and the DJ who does this modification is usually (although not     always) credited in a fairly prominent way.</p> </li> <li> <p>Mixtape/Street: Promotional in nature (but not necessarily free), mixtapes     and street albums are often released by artists to promote new artists, or     upcoming studio albums by prominent artists. They are also sometimes used to     keep fans' attention between studio releases and are most common in rap     &amp; hip hop genres. They are often not sanctioned by the artist's label, may     lack proper sample or song clearances and vary widely in production and     recording quality. While mixtapes are generally DJ-mixed, they are distinct     from commercial DJ mixes (which are usually deemed compilations) and are     defined by having a significant proportion of new material, including     original production or original vocals over top of other artists'     instrumentals. They are distinct from demos in that they are designed for     release directly to the public and fans; not to labels.</p> </li> </ul>"}, {"location": "musicbrainz/#release-status", "title": "Release status", "text": "<p>Status describes how \"official\" a release is. Possible values are:</p> <ul> <li>Official: Any release officially sanctioned by the artist and/or their     record company. Most releases will fit into this category.</li> <li>Promotional: A give-away release or a release intended to promote an     upcoming official release (e.g. pre-release versions, releases included     with a magazine, versions supplied to radio DJs for air-play).</li> <li>Bootleg: An unofficial/underground release that was not sanctioned by     the artist and/or the record company. This includes unofficial live     recordings and pirated releases.</li> <li>Pseudo-release: An alternate version of a release where the titles have     been changed. These don't correspond to any real release and should be     linked to the original release using the transl(iter)ation     relationship.</li> </ul>"}, {"location": "musicbrainz/#references", "title": "References", "text": "<ul> <li>Home</li> </ul>"}, {"location": "nas/", "title": "NAS", "text": "<p>Network-attached storage or NAS, is a computer data storage server connected to a computer network providing data access to many other devices. Basically a computer where you can attach many hard drives.</p> <p>A quick search revealed two kind of solutions:</p> <ul> <li>Plug and play.</li> <li>Do It Yourself.</li> </ul> <p>The first ones are servers that some companies like Synology or Qnap sell with their own software. They're meant for users that want to have something that works and does not give too much work at the expense of being restricted to what the provider gives you. The second ones are similar servers but you need to build it from scratch with existent tools like Linux or ZFS. I personally feel a bit constrained and vendor locked by the first. For example they are not user-repairable, so if a part breaks after warranty, you have to replace the whole server. Or you may use without knowing their proprietary storage format, that may mean that it's difficult to recover the data once you want to move away from their solution It makes sense then to trade time and dedication for freedom.</p>"}, {"location": "nas/#software", "title": "Software", "text": ""}, {"location": "nas/#truenas", "title": "TrueNAS", "text": "<p>TrueNAS (formerly known as FreeNAS) is one of the most popular solution operating systems for storage servers. It\u2019s open-source, and it\u2019s been around for almost 20 years, so it seemed like a reliable choice. As you can use it on any hardware, therefore removing the vendor locking, and it's open source. They have different solutions, being the Core the most basic. They also have Truenas Scale with which you could even build distributed systems. A trusted friend prevented me from going in this direction as he felt that GlusterFS over ZFS was not a good idea.</p> <p>TrueNAS core looked good but I still felt a bit constrained and out of control, so I discarded it.</p>"}, {"location": "nas/#unraid", "title": "Unraid", "text": "<p>Unraid is a proprietary Linux-based operating system designed to run on home media server setups that operates as a network-attached storage device, application server, and virtualization host. I've heard that it can do wonders like tweaking the speed of disks based on the need. It's not \"that expensive\" but still it's a proprietary so nope.</p>"}, {"location": "nas/#debian-with-zfs", "title": "Debian with ZFS", "text": "<p>This solution gives you the most freedom and if you're used to use Linux like me, is the one where you may feel most at home.</p> <p>The idea is to use a normal Debian and configure it to use ZFS to manage the storage.</p>"}, {"location": "nas/#hardware", "title": "Hardware", "text": "<p>Depending the amount of data you need to hold and how do you expect it to grow you need to find the solution that suits your needs. After looking to many I've decided to make my own from scratch.</p> <p>Warning: If you pursue the beautiful and hard path of building one yourself, don't just buy the components online, there are thousands of things that can go wrong that will make you loose money. Instead go to your local hardware store and try to build the server with them. Even if it's a little bit more expensive you'll save energy and peace of mind.</p>"}, {"location": "nas/#disks", "title": "Disks", "text": "<p>ZFS tutorials suggest buying everything all at once for the final stage of your server. Knowing the disadvantages it entails, I'll start with a solution for 16TB that supports to easily expand in the future. I'll start with 5 disks of 8TB, make sure you read the ZFS section on storage planning to understand why. The analysis of choosing the disks to hold data gave two IronWolf Pro and two Exos 7E8 for a total amount of 1062$. I'll add another IronWolf Pro as a cold spare.</p>"}, {"location": "nas/#ram", "title": "RAM", "text": "<p>Most ZFS resources suggest using ECC RAM. The provider gives me two options:</p> <ul> <li>Kingston Server Premier DDR4 3200MHz 16GB CL22</li> <li>Kingston Server Premier DDR4 2666MHz 16GB CL19</li> </ul> <p>I'll go with two modules of 3200MHz CL22 because it has a smaller RAM latency.</p> <p>Which was a bummer, as it turned out that my motherboard doesn't support Registered ECC ram, but only Unregistered ECC ram.</p>"}, {"location": "nas/#motherboard", "title": "Motherboard", "text": "<p>After reading these reviews(1, 2) I've come to the decision to purchase the ASRock X570M Pro4 because, It supports:</p> <ul> <li>8 x SATA3 disks</li> <li>2 x M.2 disks</li> <li>4 x DDR4 RAM slots with speeds up to 4200+ and ECC support</li> <li>1 x AMD AM4 Socket Ryzen\u2122 2000, 3000, 4000 G-Series, 5000 and 5000 G-Series   Desktop Processors</li> <li>Supports NVMe SSD as boot disks</li> <li>Micro ATX Form Factor.</li> </ul> <p>And it gives me room enough to grow:</p> <ul> <li>It supports PCI 4.0 for the M.2 which is said to be capable of perform twice   the speed compared to previous 3<sup>rd</sup> generation. the chosen M2 are of 3<sup>rd</sup>   generation, so if I need more speed I can change them.</li> <li>I'm only going to use 2 slots of RAM giving me 32GB, but I could grow 32 more   easily.</li> </ul>"}, {"location": "nas/#cpu", "title": "CPU", "text": "<p>After doing some basic research I've chosen the Ryzen 7 5700x.</p>"}, {"location": "nas/#cpu-cooler", "title": "CPU cooler", "text": "<p>After doing some basic research I've chosen the Dark Rock 4 but just because the Enermax ETS-T50 AXE Silent Edition doesn't fit my case :(.</p>"}, {"location": "nas/#graphic-card", "title": "Graphic card", "text": "<p>As it's going to be a server and I don't need it for transcoding or gaming, I'll start without a graphic card.</p>"}, {"location": "nas/#server-case", "title": "Server Case", "text": "<p>Computer cases are boxes that hold all your computer components together.</p> <p>I'm ruling out the next ones:</p> <ul> <li>Fractal Design R6:   More expensive than the Node 804 and it doesn't have hot swappable disks.</li> <li>Silverstone Technology SST-CS381: Even though it's gorgeous it's too   expensive.</li> <li>Silverstone DS380: It only supports Mini-ITX which I don't have.</li> </ul> <p>The remaining are:</p> Model Fractal Node 804 Silverstone CS380 Form factor Micro - ATX Mid tower Motherboard Micro ATX Micro ATX Drive bays 8 x 3.5\", 2 x 2.5\" 8 x 3.5\", 2 x 5.25\" Hot-swap No yes Expansion Slots 5 7 CPU cooler height 160mm 146 mm PSU compatibility ATX ATX Fans Front: 4, Top: 4, Rear 3 Side: 2, Rear: 1 Price 115 184 Size 34 x 31 x 39 cm 35 x 28 x 21 cm <p>I like the Fractal Node 804 better and it's cheaper.</p>"}, {"location": "nas/#power-supply-unit", "title": "Power supply unit", "text": "<p>Using PCPartPicker I've seen that with 4 disks it consumes approximately 264W, when I have the 8 disks, it will consume up to 344W, if I want to increase the ram then it will reach 373W. So in theory I can go with a 400W power supply unit.</p> <p>You need to make sure that it has enough wires to connect to all the disks. Although that usually is not a problem as there are adapters:</p> <ul> <li>Molex to sata</li> <li>Sata to sata</li> </ul> <p>After an analysis on the different power supply units, I've decided to go with Be Quiet! Straight Power 11 450W Gold</p>"}, {"location": "nas/#wires", "title": "Wires", "text": "<p>Usually disks come without sata wires, so you have to buy them</p>"}, {"location": "nas/#hardware-conclusion", "title": "Hardware conclusion", "text": "Piece Purpose Number Total price ($) WD Red Pro (8TB) Data disk 2 516 Seagate Exos 7E8 (8TB) Data disk 3 691 WD Red SN700 (1TB) M.2 disks 2 246 Kingston Server DDR4 (16GB) ECC RAM 4 336 AsRock X570M Pro4 Motherboard 1 225 Ryzen 7 5700x CPU 1 274 Fractal Node 804 Case 1 137 Dark Rock 4 CPU Cooler 1 75 Be Quiet! Straight Power 11 PSU 1 99 Sata wires Sata 3 6.5 No graphic card Graphic card 0 0 CPU thermal paste thermal paste 0 0 <p>Total of 2605 $</p>"}, {"location": "nas/#references", "title": "References", "text": "<ul> <li>mtlynch NAS building guide</li> <li>NAS master building guide</li> </ul>"}, {"location": "networkx/", "title": "NetworkX", "text": "<p>NetworkX is a Python package for the creation, manipulation, and study of the structure, dynamics, and functions of complex networks.</p>"}, {"location": "networkx/#references", "title": "References", "text": "<ul> <li>Docs</li> <li>Git</li> <li>Home</li> </ul>"}, {"location": "news_management/", "title": "News Management", "text": "<p>The information world of today is overwhelming. It can reach a point that you just want to disconnect so as to avoid the continuous bombardment, but that leads to loosing connection with what's happening in the world. Without knowing what's going on it's impossible to act to shape it better.</p> <p>The problem to solve is to:</p> <ul> <li>Keep updated on the important articles for you.</li> <li>Don't invest too much time on it.</li> <li>Don't loose time reading articles you're not interested on.</li> </ul>"}, {"location": "news_management/#workflow", "title": "Workflow", "text": "<p>I've found three information types to explore:</p> <ul> <li>Written content: articles, blogs, newspapers...</li> <li>Listened content: mainly podcasts.</li> <li>Viewed content: youtube, twich channels.</li> </ul> <p>Each has it's advantages and disadvantages. I like the written content as it lets me decide the pace of information ingestion, it's compatible with incremental reading, and it's the best medium to learn by making annotations and summaries, it requires your full attention though. Listened content is best to keep updated while you do brainless tasks such as cooking or cleaning, but it makes difficult to save references or ideas. Viewed content is as attention demanding as reading unless you don't care about the visual content and take it as a podcast.</p>"}, {"location": "news_management/#written-content", "title": "Written content", "text": "<p>To process the written content I use an RSS reader (Feeder) to gather all written content in one place. I skim through the elements without reading them, and I send the ones that catch my attention to wallabag for later reading. Then I go to wallabag and read the elements that feels more attractive at that moment.</p> <p>Before starting to read, I define the amount of time I want to spend on getting updated. Half of that time goes to skimming through, and the other to deep reading the selected content. You'll probably won't be able to process either the whole content on your RSS reader nor the selected content, that's why a recommender system would be awesome.</p> <p>Finding the reading devices is very important. I prefer to browse it on a tablet as it's much more pleasant than a mobile or a computer, an e-reader would be better, although wallabag is supported on some e-readers, I haven't tried it yet. I can't wait for the PineNote to be released.</p> <p>The moments I've found suitable for reading content are while eating breakfast or dinner when I'm alone.</p>"}, {"location": "news_management/#listened-content", "title": "Listened content", "text": "<p>I've selected a small number of podcasts that I listen with AntennaPod while cooking or cleaning, instead of listening directly from the mobile, I use a bluetooth loudspeaker that I carry everywhere I go (at home! use headphones when you are outside. people with loudspeakers on the public transport or streets are hateful), if there is a reference I want to save, I write it down in the mobile inbox and process it later with pynbox.</p>"}, {"location": "news_management/#the-perfect-software-solution", "title": "The perfect software solution", "text": "<p>My current workflow could be improved by software, currently the key features I'd want are:</p> <ul> <li>One place for all sources: It's useless to go to <code>n</code> different websites to see     if there is new information. RSS has been with us for too long to fall on     that.</li> <li>The user has control of it's data: The user should be able to decide which     information is private and which one is public. Only the people it trusts     will have access to it's private data.</li> <li>There must be a filter of the incoming elements: It doesn't matter how well     you choose your sources, there's always going to be content that is not     interesting for you. So there needs to be a powerful filtering system.</li> </ul>"}, {"location": "news_management/#content-filtering", "title": "Content filtering", "text": "<p>Filtering content is a subsection of the recommender systems, of all the basic models, the ones that apply are:</p> <ul> <li>Collaborative     filtering: Where the     data of many users is used to filter out the relevant items.</li> <li>Content based     filtering: Where     the data of the user on past items is used to filter new elements.</li> </ul>"}, {"location": "news_management/#collaborative-filtering", "title": "Collaborative filtering", "text": "<p>External users give information on how they see the items, and the algorithm can use that data to decide which ones are relevant for a desired user. It's how social networks operate, and if you use Mastodon, Reddit, HackerNews, Facebook, Twitter or similar, then you may not even be interested in this article at all.</p> <p>All those platforms have one or more of the next flaws for me:</p> <ul> <li>There is no one place that aggregates the information of all information     sources.</li> <li>You can't mark content as seen.</li> <li>Your data lives in the servers of other people.</li> <li>They are based on closed sourced software.</li> <li>There's a mix of information with conversation between users.</li> <li>You may not be interested in all the elements the source publishes.</li> </ul> <p>Some of them support the export to RSS and if they don't, you'll probably can find bridge platforms that do. That'll solve all the problems but the two last ones. I've used the RSS feed of mastodon users, and I finally removed them because there was a lot of content I didn't like. I've also used the reddit and hackernews rss, but again, most of the posts weren't interesting to me.</p> <p>A partial solution I've been using with my friends is to share relevant content through wallabag. It's a read-it-later application that creates RSS feeds for the liked elements. That way you can get filtered content from the people you know. The downsides are:</p> <ul> <li>You get one feed for all the content, you don't have the possibility to filter     out by categories or tags.</li> <li>It's not very collaborative. You need to ask one by one for their RSS.</li> </ul> <p>A prettier (but more difficult) solution would be to create communities that commit to share the articles interesting for a specific topic. Like HackerNews but at smaller level, where you know and trust the members of the community, and where people outside the community can not upload data, just read it.</p> <p>This could be done through one instance of <code>lemmy</code>, with a closed community, but that service is envisioned to be federated and to encourage the interaction of the users on the site.</p> <p>Another possibility would be to create a simple backend that mimics the api interface of wallabag, so that users could use the browser addon and all the interfaces existent, for example the Feeder integration with wallabag instances. You'll be able to create communities, and invite users to them, they will be able to link a \"wallabag\" tag with a specific community channel. That way, an RSS feed will be created per community with all the articles shared by their members. Optionally, users could decide to ignore the entries of another user.</p> <p>But for any of these solutions to work, you'll need to convince the people to tweak how they browse the internet in order to contribute to the system, which it's kind of difficult :(.</p>"}, {"location": "news_management/#content-based-filtering", "title": "Content based filtering", "text": "<p>If you don't manage to convince the people to use collaborative filtering, you're on your own. Your best bet then is to deduce which elements are interesting based on how you rated other elements.</p> <p>I haven't found any rss reader that does good filtering of the elements. I've used Newsblur in the past, you're able to assign scores for the element tags and user. But I didn't find it very useful. Also when it comes to self host it, it's a little bit of a nightmare.</p> <p>Intermediate solutions between the sources and the reader aren't a viable option either, as you need to interact with that middleware outside the RSS reader.</p>"}, {"location": "notmuch/", "title": "notmuch", "text": "<p>notmuch is a command-line based program for indexing, searching, reading, and tagging large collections of email messages.</p>"}, {"location": "notmuch/#installation", "title": "Installation", "text": "<p>In order to use Notmuch, you will need to have your email messages stored in your local filesystem, one message per file. You can use <code>mbsync</code> to do that.</p> <pre><code>sudo apt-get install notmuch\n</code></pre>"}, {"location": "notmuch/#configuration", "title": "Configuration", "text": "<p>To configure Notmuch, just run</p> <pre><code>notmuch\n</code></pre> <p>This will interactively guide you through the setup process, and save the configuration to <code>~/.notmuch-config</code>. If you'd like to change the configuration in the future, you can either edit that file directly, or run <code>notmuch setup</code>.</p> <p>If you plan to use <code>afew</code> set the tags to <code>new</code>.</p> <p>To test everything works as expected, and create a database that indexes all of your mail run:</p> <pre><code>notmuch new\n</code></pre>"}, {"location": "notmuch/#references", "title": "References", "text": "<ul> <li>Docs</li> </ul>"}, {"location": "ombi/", "title": "Ombi", "text": "<p>Ombi is a self-hosted web application that automatically gives your shared Jellyfin users the ability to request content by themselves! Ombi can be linked to multiple TV Show and Movie DVR tools to create a seamless end-to-end experience for your users.</p> <p>If Ombi is not for you, you may try Overseerr.</p>"}, {"location": "ombi/#references", "title": "References", "text": "<ul> <li>Homepage</li> <li>Docs</li> </ul>"}, {"location": "openproject/", "title": "OpenProject", "text": "<p>OpenProject is an Open source project management software.</p> <p>The benefits over other similar software are:</p> <ul> <li>It's popular: More than 6.2k stars on github, 1.7k forks.</li> <li>It's development is active: in the   last week they've merged 44 merged pull requests by 16 people.</li> <li>They use their own software to   track their bugs</li> <li>Easy to install</li> <li>Easy to use</li> <li>The community version is flexible enough to adapt to different workflows.</li> <li>Good installation and operation's documentation.</li> <li>Very good API documentation.</li> <li>Supports LDAP</li> </ul> <p>The things I don't like are:</p> <ul> <li>It's not keyboard driven, you use the mouse a lot.</li> <li>The task editor doesn't support markdown</li> <li>You can't sort the work package views</li> <li>You   can't fold the hierarchy trees   so it's difficult to manage the tasks once you have many. You can see   my struggles with this issue here.</li> <li> <p>You can't order the tasks inside the <code>Relations</code> tab of a task.</p> </li> <li> <p>You can't propagate easily the change of attributes to all it's children. For   example if you want to make a parent task and all it's children appear on a   report that is searching for an attribute. You need to go to a view where you   see all the tasks (an hierarchy view) select them all and do a bulk edit.</p> </li> <li> <p>Versions or sprints can't be used across projects even if they are subprojects   of a project.</p> </li> <li>The manual order of the tasks is not saved across views, so you need to have   independent views in order not to get confused on which is the prioritized   list.</li> <li>Data can be exported as XML or CSV but it doesn't export everything. You have   access to the database though, so if you'd like a better extraction of the   data you in theory can do a selective dump of whatever you need.</li> <li>It doesn't yet have   tag support.   You can meanwhile add the strings you would use as tags in the description,   and then filter by text in description.</li> <li>There is no demo instance where you can try it. It's easy though to launch a   Proof of Concept environment yourself if you already know   <code>docker-compose</code>.</li> <li>You can't hide an element from a report for a day. For example if there is a   blocked task that you can't work on for today, you can't hide it till   tomorrow.</li> <li>Even thought the   Community (free) version has many features   the next aren't:</li> <li>The status column is not showing the status color.</li> <li>Status boards:     you can't have Kanban boards that show the state of the issues as columns.     You can make it yourself through a Basic board and with the columns as the     name of the state. But when you transition an issue from state, you need to     move the issue and change the property yourself. I've thought of creating a     script that works with the API to do this automatically, maybe through the     webhooks of the openproject, but it would make more sense to spend time on     <code>pydo</code>.</li> <li>Version boards:     Useful to transition issues between sprints when you didn't finish them in     time. Probably this is easily solved through bulk editing the issues.</li> <li>Custom actions     looks super cool, but as this gives additional value compared with the     competitors, I understand it's a paid feature.</li> <li>Display relations in the work package list:     It would be useful to quickly see which tasks are blocked, by whom and why.     Nothing critical though.</li> <li>Multiselect custom fields:     You can only do single valued fields. Can't understand why this is a paid     feature.</li> <li>2FA authentication is only an Enterprise feature.</li> <li>OpenID and SAML     are an enterprise feature.</li> </ul>"}, {"location": "openproject/#installation", "title": "Installation", "text": ""}, {"location": "openproject/#proof-of-concept", "title": "Proof of Concept", "text": "<p>It can be installed both on kubernetes and through docker-compose). I'm going to follow the <code>docker-compose</code> instructions for a Proof of Concept:</p> <ul> <li>Clone this repository:</li> </ul> <pre><code>git clone https://github.com/opf/openproject-deploy --depth=1 --branch=stable/12 openproject\ncd openproject/compose\n</code></pre> <ul> <li>Make sure you are using the latest version of the Docker images:</li> </ul> <pre><code>docker-compose pull\n</code></pre> <ul> <li>Launch the containers:</li> </ul> <pre><code>OPENPROJECT_HTTPS=false PORT=127.0.0.1:8080 docker-compose up\n</code></pre> <p>Where:</p> <ul> <li><code>OPENPROJECT_HTTPS=false</code>: Is required if you want to try it locally and you     haven't yet configured the proxy to do the ssl termination.</li> <li><code>PORT=127.0.0.1:8080</code>: Is required so that you only expose the service to     your localhost.</li> </ul> <p>After a while, OpenProject should be up and running on http://localhost:8080.</p>"}, {"location": "openproject/#production", "title": "Production", "text": "<p>It can be installed both on kubernetes and through docker-compose). I'm going to follow the <code>docker-compose</code>:</p> <ul> <li>Clone this repository:</li> </ul> <pre><code>git clone https://github.com/opf/openproject-deploy --depth=1 --branch=stable/12 /data/config\ncd /data/config/compose\n</code></pre> <ul> <li>Make sure you are using the latest version of the Docker images:</li> </ul> <pre><code>docker-compose pull\n</code></pre> <ul> <li> <p>Tweak the <code>docker-compose.yaml</code> file through the <code>docker-compose.override.yml</code></p> </li> <li> <p>Add the required environmental variables through a <code>.env</code> file</p> </li> </ul> <pre><code>OPENPROJECT_HOST__NAME=openproject.example.com\nOPENPROJECT_SECRET_KEY_BASE=secret\nPGDATA=/path/to/postgres/data\nOPDATA=/path/to/openproject/data\n</code></pre> <p>Where <code>secret</code> is the value of   <code>head /dev/urandom | tr -dc A-Za-z0-9 | head   -c 32 ; echo ''</code></p> <ul> <li>Launch the containers:</li> </ul> <pre><code>docker-compose up\n</code></pre> <ul> <li> <p>Configure the ssl proxy.</p> </li> <li> <p>Connect with user <code>admin</code> and password <code>admin</code>.</p> </li> <li> <p>Create the systemd service in <code>/etc/systemd/system/openproject.service</code></p> </li> </ul> <pre><code>[Unit]\nDescription=openproject\nRequires=docker.service\nAfter=docker.service\n\n[Service]\nRestart=always\nUser=root\nGroup=docker\nWorkingDirectory=/data/config/compose\n# Shutdown container (if running) when unit is started\nTimeoutStartSec=100\nRestartSec=2s\n# Start container when unit is started\nExecStart=/usr/bin/docker-compose up\n# Stop container when unit is stopped\nExecStop=/usr/bin/docker-compose down\n\n[Install]\nWantedBy=multi-user.target\n</code></pre>"}, {"location": "openproject/#operations", "title": "Operations", "text": "<ul> <li>Doing backups</li> <li>Upgrading</li> <li>Restoring the service</li> </ul>"}, {"location": "openproject/#workflows", "title": "Workflows", "text": ""}, {"location": "openproject/#the-plans", "title": "The plans", "text": "<p>I usually do a day, week, month, trimestre and year plans. To model this in OpenProjects I've created a version with each of these values. To sort them as I want them to appear I had to append a number so it would be:</p> <ul> <li> <ol> <li>Day</li> </ol> </li> <li> <ol> <li>Week</li> </ol> </li> <li> <ol> <li>Month</li> </ol> </li> <li>...</li> </ul>"}, {"location": "openproject/#tips", "title": "Tips", "text": ""}, {"location": "openproject/#bulk-editing", "title": "Bulk editing", "text": "<p>Select the work packages to edit holding the <code>Ctrl</code> key and then right click over them and select <code>Bulk Edit</code>.</p>"}, {"location": "openproject/#form-editing", "title": "Form editing", "text": "<p>Even though it looks that you can't tweak the forms of the issues you can add the sections on the right grey column to the ones on the left blue. You can't however:</p> <ul> <li>Remove a section.</li> <li>Rename a section.</li> </ul>"}, {"location": "openproject/#tweaking-the-work-package-status", "title": "Tweaking the work package status", "text": "<p>Once you create a new status you need to tweak the workflows to be able to transition the different statuses.</p> <p>In the admin settings select \u201cWorkflow\u201d in the left menu, select the role and Type to which you want to assign the status. Then uncheck \u201cOnly display statuses that are used by this type\u201d and click \u201cEdit\u201d.</p> <p>In the table check the transitions you want to allow for the selected role and type and click \u201cSave\u201d.</p>"}, {"location": "openproject/#deal-with-big-number-of-tasks", "title": "Deal with big number of tasks", "text": "<p>As the number of tasks increase, the views of your work packages starts becoming more cluttered. As you can't fold the hierarchy trees it's difficult to efficiently manage your backlog.</p> <p>I've tried setting up a work package type that is only used for the subtasks so that they are filtered out of the view, but then you don't know if they are parent tasks unless you use the details window. It's inconvenient but having to collapse the tasks every time it's more cumbersome. You'll also need to reserve the selected subtask type (in my case <code>Task</code>) for the subtasks.</p>"}, {"location": "openproject/#sorting-work-package-views", "title": "Sorting work package views", "text": "<p>They are sorted alphabetically, so the only way to sort them is by prepending a number. You can do <code>0. Today</code> instead of <code>Today</code>. It's good to do big increments between numbers, so the next report could be <code>10. Backlog</code>. That way if you later realize you want another report between Today and Backlog, you can use <code>5. New Report</code> and not rename all the reports.</p>"}, {"location": "openproject/#pasting-text-into-the-descriptions", "title": "Pasting text into the descriptions", "text": "<p>When I paste the content of the clipboard in the description, all new lines are removed (<code>\\n</code>), the workaround is to paste it inside a <code>code snippet</code>.</p>"}, {"location": "openproject/#references", "title": "References", "text": "<ul> <li>Docs</li> <li>Bug tracker</li> <li>Git</li> <li>Homepage</li> <li>Upgrading notes</li> </ul>"}, {"location": "oracle_database/", "title": "Oracle Database", "text": "<p>Oracle Database is an awful proprietary database, run away from it!</p>"}, {"location": "oracle_database/#install", "title": "Install", "text": "<ul> <li>Download or clone the files of their docker     repository.</li> <li>Create an account in their page to be able to download the required binary     files.     Fake person generator might come     handy for this step.</li> <li> <p>Download the     files.</p> </li> <li> <p>After downloading the file we need to copy it to the folder referring to the oracle version in the cloned folder. In this case, 19.3.0:</p> <pre><code>mv ~/Download/LINUX.X64_193000_db_home.zip ./docker-images/OracleDatabase/SingleInstance/dockerfiles/19.3.0/\n</code></pre> </li> <li> <p>The next step is to build the image. You need at least 20G free in     <code>/var/lib/docker</code>.</p> <pre><code>./docker-images/OracleDatabase/SingleInstance/dockerfiles/19.3.0/buildDockerImage.sh -v 19.3.0 -e\n</code></pre> </li> <li> <p>Confirm that the image was created     <pre><code>docker images\n\nREPOSITORY                                TAG           IMAGE ID       CREATED          SIZE\noracle/database                           19.3.0-ee     d8be8934332d   53 minutes ago   6.54GB\n</code></pre></p> </li> <li> <p>Run the database docker.     <pre><code>docker run --name myOracle1930 \\\n-p 127.0.0.1:1521:1521 \\\n-p 127.0.0.1:5500:5500 \\\n-e ORACLE_SID=ORCLCDB \\\n-e ORACLE_PDB=ORCLPDB1 \\\n-e ORACLE_PWD=root \\\n-e INIT_SGA_SIZE=1024 \\\n-e INIT_PGA_SIZE=1024 \\\n-e ORACLE_CHARACTERSET=AL32UTF8 \\\noracle/database:19.3.0-ee\n</code></pre></p> </li> </ul>"}, {"location": "orgmode/", "title": "Org Mode", "text": "<p><code>nvim-orgmode</code> is a Orgmode clone written in Lua for Neovim. Org-mode is a flexible note-taking system that was originally created for Emacs. It has gained wide-spread acclaim and was eventually ported to Neovim. This page is heavily focused to the nvim plugin, but you can follow the concepts for emacs as well.</p> <p>If you use Android try orgzly.</p>"}, {"location": "orgmode/#installation", "title": "Installation", "text": "<p>Add to your plugin config:</p> <pre><code>use {'nvim-orgmode/orgmode', config = function()\n  require('orgmode').setup{}\nend\n}\n</code></pre> <p>Then install it with <code>:PackerInstall</code>.</p> <p>Tweak the configuration:</p> <pre><code>-- init.lua\n\n-- Load custom treesitter grammar for org filetype\nrequire('orgmode').setup_ts_grammar()\n\n-- Treesitter configuration\nrequire('nvim-treesitter.configs').setup {\n  -- If TS highlights are not enabled at all, or disabled via `disable` prop,\n  -- highlighting will fallback to default Vim syntax highlighting\n  highlight = {\n    enable = true,\n    -- Required for spellcheck, some LaTex highlights and\n    -- code block highlights that do not have ts grammar\n    additional_vim_regex_highlighting = {'org'},\n  },\n  ensure_installed = {'org'}, -- Or run :TSUpdate org\n}\n\nrequire('orgmode').setup({\n  org_agenda_files = {'~/Dropbox/org/*', '~/my-orgs/**/*'},\n  org_default_notes_file = '~/Dropbox/org/refile.org',\n})\n</code></pre> <p>You can check the default configuration file here.</p>"}, {"location": "orgmode/#key-bindings", "title": "Key bindings", "text": "<p>Mappings or Key bindings can be changed on the <code>mappings</code> attribute of the <code>setup</code>. The program has these kinds of mappings:</p> <ul> <li>Org</li> <li>Agenda</li> <li>Capture</li> <li>Global</li> </ul> <p>For example the <code>global</code> mappings live under <code>mappings.global</code> and can be overridden like this:</p> <pre><code>require('orgmode').setup({\n  mappings = {\n    global = {\n      org_agenda = 'gA',\n      org_capture = 'gC'\n    }\n  }\n})\n</code></pre>"}, {"location": "orgmode/#be-ready-when-breaking-changes-come", "title": "Be ready when breaking changes come", "text": "<p>The developers have created an issue to track breaking changes, subscribe to it so you're notified in advance.</p>"}, {"location": "orgmode/#usage", "title": "Usage", "text": "<p>If you are new to Orgmode, check the vim Dotoo video, it's another plugin but the developers say it's the same. If you, like me, prefer written tutorials check the hands-on tutorial.</p> <p>If you get lost in any view you can use <code>g?</code> to get the mappings of the view.</p>"}, {"location": "orgmode/#org-file", "title": "Org File", "text": ""}, {"location": "orgmode/#headings", "title": "Headings", "text": "<p>Any line starting with one or more asterisks <code>*</code> but without any preceding whitespace is a heading (also called headline).</p> <pre><code>* Org Bullets\n* Vim table-mode\n</code></pre> <p>Once you are over a header, you can create a new header at the same level below the current subtree <code>&lt;leader&gt;&lt;enter&gt;</code>, if you want to add a heading after the current item use <code>&lt;control&gt;&lt;enter&gt;</code> if you use these key bindings:</p> <pre><code>require('orgmode').setup({\n  mappings = {\n    org = {\n      org_meta_return = '&lt;c-cr&gt;',\n      org_insert_heading_respect_content = '&lt;leader&gt;&lt;cr&gt;',\n    }\n  }\n})\n</code></pre> <p>To be able to use it in insert mode too add below the org-mode configuration:</p> <pre><code>vim.cmd[[\n  imap &lt;c-cr&gt; &lt;c-o&gt;&lt;c-cr&gt;\n  imap &lt;leader&gt;&lt;cr&gt; &lt;c-o&gt;&lt;leader&gt;&lt;cr&gt;\n]]\n</code></pre> <p>The number of asterisks denotes the level of the heading: the more asterisks, the deeper the level. That is how we achieve nested structures in our org files.</p> <pre><code>* Org Bullets\n** Synopsis\n* Vim table-mode\n</code></pre> <p>The content within a heading can be free form text, include links, be a list, or any combination thereof. For example:</p> <pre><code>* Org Bullets\n** Synopsis\n   This plugin is a clone of org-bullets. It replaces the asterisks in org\n   syntax with unicode characters.\n* Vim table-mode\n</code></pre> <p>The full syntax for a headline is</p> <pre><code>STARS KEYWORD PRIORITY TITLE TAGS\n*     TODO    [#A]     foo   :bar:baz:\n</code></pre> <p>Where:</p> <ul> <li><code>KEYWORD</code>: if present, turns the heading into a <code>TODO</code> item. </li> <li><code>PRIORITY</code> sets a priority level to be used in the Agenda.</li> <li><code>TITLE</code> is the main body of the heading.</li> <li><code>TAGS</code> is a colon surrounded and delimited list of tags used in searching in the Agenda.</li> </ul>"}, {"location": "orgmode/#toogle-line-to-headline", "title": "Toogle line to headline", "text": "<p>You can change a text line into a headline with <code>&lt;leader&gt;h</code> (Default: <code>&lt;leader&gt;o*</code>) with the next configuration:</p> <pre><code>org = {\n  org_toggle_heading = '&lt;leader&gt;h',\n}\n</code></pre> <p>If you have a checkbox inside a TODO item, it will transform it to a children TODO item.</p>"}, {"location": "orgmode/#change-heading-level", "title": "Change heading level", "text": "<p>To change the heading level use <code>&lt;&lt;</code> or <code>&gt;&gt;</code>. It doesn't work in visual mode though, if you want to change the level of the whole subtree you can use <code>&lt;S</code> and <code>&gt;S</code>. </p> <pre><code>org = {\n  org_do_promote = '&lt;&lt;',\n  org_do_demote = '&gt;&gt;',\n  org_promote_subtree = '&lt;S',\n  org_demote_subtree = '&gt;S',\n}\n</code></pre> <p>To be able to use it in insert mode too add below the org-mode configuration:</p> <pre><code>vim.cmd[[\n  imap &gt;&gt; &lt;esc&gt;&gt;&gt;\n  imap &lt;&lt; &lt;esc&gt;&lt;&lt;\n  imap &gt;S &lt;esc&gt;&gt;s\n  imap &lt;S &lt;esc&gt;&lt;s\n\n]]\n</code></pre> <p>If you don't like seeing so many stars, you can enable the <code>org_hide_leading_stars = true</code> option. To me it looks much cleaner.</p>"}, {"location": "orgmode/#moving-headings", "title": "Moving headings", "text": "<p>To move the subtrees up and down you can use <code>J</code> (Default <code>&lt;leader&gt;oJ</code>) and <code>K</code> (Default <code>&lt;leader&gt;oK</code>) with the next conf:</p> <pre><code>    org = {\n      org_move_subtree_up = \"K\",\n      org_move_subtree_down = \"J\",\n    }\n</code></pre>"}, {"location": "orgmode/#folding-headings", "title": "Folding headings", "text": "<p>To fold the headings you can use either the normal vim bindings <code>zc</code>, <code>zo</code>, <code>zM</code>, ... or <code>&lt;tab&gt;</code> to toogle the fold of an element or <code>&lt;shift&gt;&lt;tab&gt;</code> to toogle the whole file.</p>"}, {"location": "orgmode/#navigate-through-headings", "title": "Navigate through headings", "text": "<p>It's easy to navigate through your heading tree with:</p> <ul> <li>Next/previous heading of any level with <code>&lt;control&gt;j</code>/<code>&lt;control&gt;k</code> (Default <code>}</code>/<code>{</code>)</li> <li>Next/previous heading of the same level with <code>&lt;control&gt;n</code>/<code>&lt;control&gt;p</code> (Default <code>]]</code>/<code>[[</code>)</li> <li>Go to the parent heading with <code>gp</code> (Default <code>g{</code>)</li> </ul> <pre><code>org = {\n  org_next_visible_heading = '&lt;c-j&gt;',\n  org_previous_visible_heading = '&lt;c-k&gt;',\n  org_forward_heading_same_level = '&lt;c-n&gt;',\n  org_backward_heading_same_level = '&lt;c-p&gt;',\n  outline_up_heading = 'gp',\n}\n</code></pre> <p>To be able to use it in insert mode too add below the org-mode configuration:</p> <pre><code>vim.cmd[[\n  imap &lt;c-j&gt; &lt;esc&gt;&lt;c-j&gt;\n  imap &lt;c-k&gt; &lt;esc&gt;&lt;c-k&gt;\n  imap &lt;c-n&gt; &lt;esc&gt;&lt;c-n&gt;\n  imap &lt;c-p&gt; &lt;esc&gt;&lt;c-p&gt;\n]]\n</code></pre>"}, {"location": "orgmode/#todo-items", "title": "TODO items", "text": "<p><code>TODO</code> items are meant to model tasks that evolve between states. </p> <p>As creating <code>TODO</code> items is quite common you can:</p> <ul> <li>Create an item with the same level as the item above in the current position with <code>;t</code> (by default is <code>&lt;leader&gt;oit</code>).</li> <li>Create an item with the same level as the item above after all the children of the item above with <code>;T</code> (by default is <code>&lt;leader&gt;oit</code>).</li> </ul> <pre><code>org = {\n  org_insert_todo_heading = \";t\",\n  org_insert_todo_heading_respect_content = \";T\",\n}\n</code></pre> <p>To be able to use it in insert mode too add below the org-mode configuration:</p> <pre><code>vim.cmd[[\n  imap ;t &lt;c-o&gt;;t\n  imap ;T &lt;c-o&gt;;T\n]]\n</code></pre> <p>You can transition the state forward and backwards by using <code>t</code>/<code>T</code> (Default: <code>cit</code>/<code>ciT</code>) if you use:</p> <pre><code>org = {\n  org_todo = 't',\n  org_todo_prev = 'T',\n}\n</code></pre>"}, {"location": "orgmode/#todo-state-customization", "title": "TODO state customization", "text": "<p>By default they are <code>TODO</code> or <code>DONE</code> but you can define your own using the <code>org_todo_keywords</code> configuration. It accepts a list of unfinished states and finished states separated by a <code>'|'</code>. For example:</p> <pre><code>org_todo_keywords = { 'TODO', 'NEXT', '|', 'DONE' }\n</code></pre> <p>You can also use fast access states:</p> <pre><code>org_todo_keywords = { 'TODO(t)', 'NEXT(n)', '|', 'DONE(d)' }\n</code></pre> <p>Sadly you can't yet use different todo sequences.</p>"}, {"location": "orgmode/#priority", "title": "Priority", "text": "<p>TODO items can also have a priority, by default you have 3 levels <code>A</code>, <code>B</code> and <code>C</code>. If you don't set a priority it's set to <code>B</code>.</p> <p>You can increase/decrease the priority with <code>=</code>/<code>-</code> (Default: <code>ciR</code>/<code>cir</code>):</p> <pre><code>org = {\n  org_priority_up = '-',\n  org_priority_down = '=',\n}\n</code></pre> <p>I feel more comfortable with these priorities:</p> <ul> <li><code>A</code>: Critical</li> <li><code>B</code>: High</li> <li><code>C</code>: Normal</li> <li><code>D</code>: Low</li> </ul> <p>This gives you room to usually work on priorities <code>B-D</code> and if something shows up that is really really important, you can use <code>A</code>. You can set this setting with the next snippet:</p> <pre><code>require('orgmode').setup({\n  org_priority_highest = 'A',\n  org_priority_default = 'C',\n  org_priority_lowest = 'D',\n})\n</code></pre>"}, {"location": "orgmode/#dates", "title": "Dates", "text": "<p>TODO items can also have timestamps which are specifications of a date (possibly with a time or a range of times) in a special format, either <code>&lt;2003-09-16 Tue&gt;</code> or <code>&lt;2003-09-16 Tue 09:39&gt;</code> or <code>&lt;2003-09-16 Tue 12:00-12:30&gt;</code>. A timestamp can appear anywhere in the headline or body of an Org tree entry. Its presence causes entries to be shown on specific dates in the agenda.</p>"}, {"location": "orgmode/#date-types", "title": "Date types", "text": ""}, {"location": "orgmode/#appointments", "title": "Appointments", "text": "<p>Meant to be used for elements of the org file that have a defined date to occur, think of a calendar appointment. In the agenda display, the headline of an entry associated with a plain timestamp is shown exactly on that date. </p> <pre><code>* TODO Meet with Marie\n&lt;2023-02-24 Fri&gt;\n</code></pre> <p>When you insert the timestamps with the date popup picker with <code>;d</code> (Default: <code>&lt;leader&gt;oi.</code>) you can only select the day and not the time, but you can add it manually. </p> <p>You can also define a timestamp range that spans through many days <code>&lt;2023-02-24 Fri&gt;--&lt;2023-02-26 Sun&gt;</code>. The headline then is shown on the first and last day of the range, and on any dates that are displayed and fall in the range.  </p>"}, {"location": "orgmode/#recurring-tasks", "title": "Recurring tasks", "text": "<p>A timestamp may contain a repeater interval, indicating that it applies not only on the given date, but again and again after a certain interval of N hours (h), days (d), weeks (w), months (m), or years (y). The following shows up in the agenda every Wednesday:</p> <pre><code>* TODO Go to pilates\n  &lt;2007-05-16 Wed 12:30 +1w&gt;\n</code></pre> <p>When you mark a recurring task with the TODO keyword \u2018DONE\u2019, it no longer produces entries in the agenda. The problem with this is, however, is that then also the next instance of the repeated entry will not be active. Org mode deals with this in the following way: when you try to mark such an entry as done, it shifts the base date of the repeating timestamp by the repeater interval, and immediately sets the entry state back to TODO.</p> <p>As a consequence of shifting the base date, this entry is no longer visible in the agenda when checking past dates, but all future instances will be visible. </p> <p>With the <code>+1m</code> cookie, the date shift is always exactly one month. So if you have not paid the rent for three months, marking this entry DONE still keeps it as an overdue deadline. Depending on the task, this may not be the best way to handle it. For example, if you forgot to call your father for 3 weeks, it does not make sense to call him 3 times in a single day to make up for it. For these tasks you can use the <code>++</code> operator, for example <code>++1m</code>. Finally, there are tasks, like changing batteries, which should always repeat a certain time after the last time you did it you can use the <code>.+</code> operator. For example:</p> <pre><code>** TODO Call Father\n   DEADLINE: &lt;2008-02-10 Sun ++1w&gt;\n   Marking this DONE shifts the date by at least one week, but also\n   by as many weeks as it takes to get this date into the future.\n   However, it stays on a Sunday, even if you called and marked it\n   done on Saturday.\n\n** TODO Empty kitchen trash\n   DEADLINE: &lt;2008-02-08 Fri 20:00 ++1d&gt;\n   Marking this DONE shifts the date by at least one day, and also\n   by as many days as it takes to get the timestamp into the future.\n   Since there is a time in the timestamp, the next deadline in the\n   future will be on today's date if you complete the task before\n   20:00.\n\n** TODO Check the batteries in the smoke detectors\n   DEADLINE: &lt;2005-11-01 Tue .+1m&gt;\n   Marking this DONE shifts the date to one month after today.\n\n** TODO Wash my hands\n   DEADLINE: &lt;2019-04-05 08:00 Fri .+1h&gt;\n   Marking this DONE shifts the date to exactly one hour from now.\n</code></pre>"}, {"location": "orgmode/#scheduled", "title": "Scheduled", "text": "<p><code>SCHEDULED</code> defines when you are plan to start working on that task.</p> <p>The headline is listed under the given date. In addition, a reminder that the scheduled date has passed is present in the compilation for today, until the entry is marked as done.</p> <pre><code>*** TODO Call Trillian for a date on New Years Eve.\n    SCHEDULED: &lt;2004-12-25 Sat&gt;\n</code></pre> <p>If you want to delay the display of this task in the agenda, use <code>SCHEDULED: &lt;2004-12-25 Sat -2d&gt;</code> the task is still scheduled on the 25<sup>th</sup> but will appear two days later. In case the task contains a repeater, the delay is considered to affect all occurrences; if you want the delay to only affect the first scheduled occurrence of the task, use <code>--2d</code> instead. </p> <p>Scheduling an item in Org mode should not be understood in the same way that we understand scheduling a meeting. Setting a date for a meeting is just a simple appointment, you should mark this entry with a simple plain timestamp, to get this item shown on the date where it applies. This is a frequent misunderstanding by Org users. In Org mode, scheduling means setting a date when you want to start working on an action item. </p> <p>You can set it with <code>&lt;leader&gt;s</code> (Default: <code>&lt;leader&gt;ois</code>)</p>"}, {"location": "orgmode/#deadline", "title": "Deadline", "text": "<p><code>DEADLINE</code> defines when the task is supposed to be finished on. On the deadline date, the task is listed in the agenda. In addition, the agenda for today carries a warning about the approaching or missed deadline, starting <code>org_deadline_warning_days</code> before the due date (14 by default), and continuing until the entry is marked as done. An example:</p> <pre><code>* TODO Do this \nDEADLINE: &lt;2023-02-24 Fri&gt;\n</code></pre> <p>You can set it with <code>&lt;leader&gt;d</code> (Default: <code>&lt;leader&gt;oid</code>).</p> <p>Using too many tasks with a <code>DEADLINE</code> will clutter your agenda. Use it only for the actions that you need to have a reminder, instead try to using appointment dates instead. The problem of using appointments is that once the date is over you don't get a reminder in the agenda that it's overdue, if you need this, use <code>DEADLINE</code> instead.</p> <p>If you need a different warning period for a special task, you can specify it. For example setting a warning period of 5 days <code>DEADLINE: &lt;2004-02-29 Sun -5d&gt;</code>. If you do use <code>DEADLINES</code> for many small tasks you may want to configure the default number of days to <code>0</code>. Most of times you are able to finish the task in the day, for those that you can't specify a different warning period in the task.</p> <pre><code>require('orgmode').setup({\n  org_deadline_warning_days = 0,\n})\n</code></pre>"}, {"location": "orgmode/#date-management", "title": "Date management", "text": "<pre><code>  org = {\n    org_deadline = '&lt;leader&gt;d',\n    org_schedule = '&lt;leader&gt;s',\n    org_time_stamp = ';d',\n  }\n</code></pre> <p>To edit existing dates you can:</p> <ul> <li>Increase/decrease the date under the cursor by 1 day with <code>&lt;shift&gt;&lt;up&gt;</code>/` <li>Increase/decrease the part of the date under the cursor with <code>&lt;control&gt;a</code>/<code>&lt;control&gt;x</code></li> <li>Bring the date pop up with <code>&lt;control&gt;e</code> (Default <code>cid</code>)</li> <pre><code>  org = {\n    org_change_date = '&lt;c-e&gt;',\n  }\n</code></pre> <p>To be able to use the bindings in insert mode too add below the org-mode configuration:</p> <pre><code>vim.cmd[[\n  imap ;d &lt;c-o&gt;;d\n  imap &lt;c-e&gt; &lt;c-o&gt;&lt;c-e&gt;\n]]\n</code></pre> <p>You can also use the next abbreviations:</p> <ul> <li><code>:today:</code>: expands to today's date (example: &lt;2021-06-29 Tue&gt;)</li> <li><code>:itoday:</code>: expands to an invactive version of today's date (example: [2021-06-29 Tue])</li> <li><code>:now:</code>: expands to today's date and current time (example: &lt;2021-06-29 Tue 15:32&gt;)</li> <li><code>:inow:</code>: expands to invactive version of today's date and current time (example: [2021-06-29 Tue 15:32]</li> </ul>"}, {"location": "orgmode/#tags", "title": "Tags", "text": "<p>You can also use tags to organize your items. To edit them use <code>&lt;leader&gt;g</code> (Default <code>&lt;leader&gt;ot</code>).</p> <pre><code>  org = {\n    org_set_tags_command = '&lt;leader&gt;g',\n  },\n</code></pre> <p>When you press that key you can type:</p> <ul> <li><code>tag1</code>: It will add <code>:tag1:</code>.</li> <li><code>tag1:tag2</code>: It will add <code>:tag1:tag2:</code>.</li> <li>Press <code>ESC</code>: It will remove all tags from the item.</li> </ul> <p>Tags are seen as <code>:tag1:tag2:</code> on the right of the TODO item description.</p> <p>Note: tags can't have spaces so use <code>long_break</code> instead of <code>long break</code>.</p> <p>Tags make use of the hierarchical structure of outline trees. If a heading has a certain tag, all subheadings inherit the tag as well. For example, in the list</p> <pre><code>* Meeting with the French group      :work:\n** Summary by Frank                  :boss:notes:\n*** TODO Prepare slides for him      :action:\n</code></pre> <p>The final heading has the tags <code>work</code>, <code>boss</code>, <code>notes</code>, and <code>action</code> even though the final heading is not explicitly marked with those tags. You can also set tags that all entries in a file should inherit just as if these tags were defined in a hypothetical level zero that surrounds the entire file. Using a line like the next one:</p> <pre><code>#+FILETAGS: :Peter:Boss:Secret:\n</code></pre> <p>If you plan to use the Capture function on the file, add the <code>FILETAGS</code> like at the top of the file, otherwise it will end up in the middle of it as you capture new elements.</p>"}, {"location": "orgmode/#lists", "title": "`Lists", "text": "<p>Lists start with a dash:</p> <pre><code>- Org bullets\n</code></pre> <p>To create new list item press <code>&lt;control&gt;&lt;enter&gt;</code>.</p>"}, {"location": "orgmode/#checkboxes", "title": "Checkboxes", "text": "<p>Checkboxes or checklists are a special type of list:</p> <pre><code>- [ ] Item 1\n  - [ ] Subitem 1\n  - [ ] Subitem 2\n- [ ] Item 2\n</code></pre> <p>If you're over an item you can create new ones with <code>&lt;control&gt;&lt;enter&gt;</code> (if you have the <code>org_meta_return = '&lt;c-cr&gt;'</code> binding set). </p> <p>You can change the checkbox state with <code>&lt;control&gt;&lt;space&gt;</code>, if you check a subitem the parent item will be marked as started <code>&lt;3</code> automatically:</p> <pre><code>- [-] Item 1\n  - [X] Subitem 1\n  - [ ] Subitem 2\n- [ ] Item 2\n</code></pre> <p>You can't yet manage the checkboxes as you do the headings by promoting, demoting and moving them around.</p> <p>Follow this issue if you want to see the progress of it's children at the parent checkbox.</p>"}, {"location": "orgmode/#links", "title": "Links", "text": "<p>One final aspect of the org file syntax are links. Links are of the form <code>[[link][description]]</code>, where link can be an:</p> <ul> <li>Internal reference</li> <li>External reference</li> </ul> <p>A link that does not look like a URL refers to the current document. You can follow it with <code>gx</code> when point is on the link (Default <code>&lt;leader&gt;oo</code>) if you use the next configuration.</p> <pre><code>org = {\n  org_open_at_point = 'gx',\n}\n</code></pre>"}, {"location": "orgmode/#internal-document-links", "title": "Internal document links", "text": "<p>Org provides several refinements to internal navigation within a document. Most notably:</p> <ul> <li><code>[[*Some section]]</code>: points to a headline with the name <code>Some section</code>.</li> <li><code>[[#my-custom-id]]</code>: targets the entry with the <code>CUSTOM_ID</code> property set to <code>my-custom-id</code>. </li> </ul> <p>When the link does not belong to any of the cases above, Org looks for a dedicated target: the same string in double angular brackets, like <code>&lt;&lt;My Target&gt;&gt;</code>.</p> <p>If no dedicated target exists, the link tries to match the exact name of an element within the buffer. Naming is done, unsurprisingly, with the <code>NAME</code> keyword, which has to be put in the line before the element it refers to, as in the following example</p> <pre><code>#+NAME: My Target\n| a  | table      |\n|----+------------|\n| of | four cells |\n</code></pre> <p>Ultimately, if none of the above succeeds, Org searches for a headline that is exactly the link text but may also include a <code>TODO</code> keyword and tags, or initiates a plain text search.</p> <p>Note that you must make sure custom IDs, dedicated targets, and names are unique throughout the document. Org provides a linter to assist you in the process, if needed, but I have not searched yet one for nvim.</p>"}, {"location": "orgmode/#external-links", "title": "External links", "text": "<ul> <li>URL (<code>http://</code>, <code>https://</code>)</li> <li>Path to a file (<code>file:/path/to/org/file</code>). File links can contain additional information to jump to a particular location in the file when following a link. This can be:</li> <li><code>file:~/code/main.c::255</code>: A line number </li> <li><code>file:~/xx.org::My Target</code>: A search for <code>&lt;&lt;My Target&gt;&gt;</code></li> <li><code>file:~/xx.org::#my-custom-id</code>: A   search for-  a custom ID</li> </ul>"}, {"location": "orgmode/#properties", "title": "Properties", "text": "<p>Properties are key-value pairs associated with an entry. They live in a special drawer with the name <code>PROPERTIES</code>. Each property is specified on a single line, with the key (surrounded by colons) first, and the value after it:</p> <pre><code>* CD collection\n** Classic\n*** Goldberg Variations\n    :PROPERTIES:\n    :Title:     Goldberg Variations\n    :Composer:  J.S. Bach\n    :Publisher: Deutsche Grammophon\n    :NDisks:    1\n    :END:\n</code></pre> <p>You may define the allowed values for a particular property <code>Xyz</code> by setting a property <code>Xyz_ALL</code>. This special property is inherited, so if you set it in a level 1 entry, it applies to the entire tree. When allowed values are defined, setting the corresponding property becomes easier and is less prone to typing errors. For the example with the CD collection, we can pre-define publishers and the number of disks in a box like this:</p> <pre><code>* CD collection\n  :PROPERTIES:\n  :NDisks_ALL:  1 2 3 4\n  :Publisher_ALL: \"Deutsche Grammophon\" Philips EMI\n  :END:\n</code></pre> <p>If you want to set properties that can be inherited by any entry in a file, use a line like:</p> <pre><code>#+PROPERTY: NDisks_ALL 1 2 3 4\n</code></pre> <p>This can be interesting for example if you want to track when was a header created:</p> <pre><code>*** Title of header\n   :PROPERTIES:\n   :CREATED: &lt;2023-03-03 Fri 12:11&gt; \n   :END:\n</code></pre>"}, {"location": "orgmode/#archiving", "title": "Archiving", "text": "<p>When we no longer need certain parts of our org files, they can be archived. You can archive items by pressing <code>;A</code> (Default <code>&lt;Leader&gt;o$</code>) while on the heading. This will also archive any child headings. The default location for archived headings is <code>&lt;name-of-current-org-file&gt;.org_archive</code>, which can be changed with the <code>org_archive_location</code> option.</p> <p>The problem is that when you archive an element you loose the context of the item unless it's a first level item. </p> <p>Another way to archive is by adding the <code>:ARCHIVE:</code> tag with <code>;a</code> and once all elements are archived move it to the archive.</p> <pre><code>org = {\n  org_toggle_archive_tag = ';a',\n  org_archive_subtree = ';A',\n}\n\nThere are some work in progress to improve archiving in the next issues [1](https://github.com/nvim-orgmode/orgmode/issues/413), [2](https://github.com/nvim-orgmode/orgmode/issues/369) and [3](https://github.com/joaomsa/telescope-orgmode.nvim/issues/2). \n\n## Refiling\n\nRefiling lets you easily move around elements of your org file, such as headings or TODOs. You can refile with `&lt;leader&gt;r` with the next snippet:\n\n```lua\norg = {\n  org_refile = '&lt;leader&gt;r',\n}\n</code></pre> <p>When you press the refile key binding you are supposed to press <code>&lt;tab&gt;</code> to see the available options, once you select the correct file, if you will be shown a autocomplete with the possible items to refile it to. Luckily there is a Telescope plugin.</p> <p>Install it by adding to your plugin config:</p> <pre><code>use 'joaomsa/telescope-orgmode.nvim'\n</code></pre> <p>Then install it with <code>:PackerInstall</code>.</p> <p>You can setup the extension by doing:</p> <pre><code>require('telescope').load_extension('orgmode')\n</code></pre> <p>To replace the default refile prompt:</p> <pre><code>vim.api.nvim_create_autocmd('FileType', {\n  pattern = 'org',\n  group = vim.api.nvim_create_augroup('orgmode_telescope_nvim', { clear = true })\n  callback = function()\n    vim.keymap.set('n', '&lt;leader&gt;r', require('telescope').extensions.orgmode.refile_heading)\n    vim.keymap.set('n', '&lt;leader&gt;g', require('telescope').extensions.orgmode.search_headings)\n  end,\n})\n</code></pre> <p>If the auto command doesn't override the default <code>orgmode</code> one, bind it to another keys and never use it.</p> <p>If you refile from the capture window, until this issue is solved, your task will be refiled but the capture window won't be closed.</p> <p>Be careful that it only refiles the first task there is, so you need to close the capture before refiling the next</p> <p>The plugin also allows you to use <code>telescope</code> to search through the headings of the different files with <code>search_headings</code>, with the configuration above you'd use <code>&lt;leader&gt;g</code>.</p>"}, {"location": "orgmode/#agenda", "title": "Agenda", "text": "<p>The org agenda is used to get an overview of all your different org files. Pressing <code>ga</code> (Default: <code>&lt;leader&gt;oa</code>) gives you an overview of the various specialized views into the agenda that are available. Remember that you can press <code>g?</code> to see all the available key mappings for each view.</p> <pre><code>  global = {\n    org_agenda = 'ga',\n  },\n</code></pre> <p>You'll be presented with the next views:</p> <ul> <li><code>a</code>: Agenda for current week or day</li> <li><code>t</code>: List of all TODO entries</li> <li><code>m</code>: Match a TAGS/PROP/TODO query</li> <li><code>M</code>: Like <code>m</code>, but only TODO entries</li> <li><code>s</code>: Search for keywords</li> <li><code>q</code>: Quit</li> </ul> <p>So far the <code>nvim-orgmode</code> agenda view lacks the next features:</p> <ul> <li>Custom agenda commands</li> <li>These interactions with the items:</li> <li>Remove it</li> <li>Promote/demote it</li> <li>Order it up and down</li> </ul>"}, {"location": "orgmode/#move-around-the-agenda-view", "title": "Move around the agenda view", "text": "<ul> <li><code>.</code>: Go to Today</li> <li><code>J</code>: Opens a popup that allows you to select the date to jump to.</li> <li><code>&lt;c-n&gt;</code>: Next agenda span (Default <code>f</code>). For example if you are in the week view it will go to the next week.</li> <li><code>&lt;c-p&gt;</code>: Previous agenda span (Default <code>b</code>).</li> <li><code>/</code>: Opens a prompt that allows filtering current agenda view by category, tags and title. </li> </ul> <p>For example, having a <code>todos.org</code> file with headlines that have tags <code>mytag</code> or <code>myothertag</code>, and some of them have check in content, searching by <code>todos+mytag/check/</code> returns all headlines that are in <code>todos.org</code> file, that have <code>mytag</code> tag, and have <code>check</code> in headline title. </p> <p>Note that <code>regex</code> is case sensitive by default. Use the vim regex flag <code>\\c</code> to make it case insensitive. For more information see <code>:help vim.regex()</code> and <code>:help /magic</code>.</p> <p>Pressing <code>&lt;TAB&gt;</code> in filter prompt autocompletes categories and tags.</p> <ul> <li><code>q</code>: Quit                                                                                                </li> </ul> <pre><code>  agenda = {\n    org_agenda_later = '&lt;c-n&gt;',\n    org_agenda_earlier = '&lt;c-p&gt;',\n  },\n</code></pre>"}, {"location": "orgmode/#act-on-the-agenda-elements", "title": "Act on the agenda elements", "text": "<ul> <li><code>&lt;enter&gt;</code>: Open the file containing the element on your cursor position. By default it opens it in the same buffer as the agenda view, which is a bit uncomfortable for me, I prefer the behaviour of <code>&lt;tab&gt;</code> so I'm using that instead.</li> <li><code>t</code>: Change <code>TODO</code> state of an item both in the agenda and the original Org file</li> <li><code>=</code>/<code>-</code>: Change the priority of the element</li> <li><code>r</code>: Reload all org files and refresh the current agenda view.</li> </ul> <pre><code>  agenda = {\n    org_agenda_switch_to = '&lt;tab&gt;',\n    org_agenda_goto = '&lt;cr&gt;',\n    org_agenda_priority_up = '=',\n    org_agenda_set_tags = '&lt;leader&gt;g',\n    org_agenda_deadline = '&lt;leader&gt;d',\n    org_agenda_schedule = '&lt;leader&gt;s',\n  },\n</code></pre>"}, {"location": "orgmode/#agenda-views", "title": "Agenda views:", "text": "<ul> <li><code>vd</code>: Show the agenda of the day</li> <li><code>vw</code>: Show the agenda of the week</li> <li><code>vm</code>: Show the agenda of the month</li> <li><code>vy</code>: Show the agenda of the year</li> </ul> <p>Once you open one of the views you can do most of the same stuff that you on othe org mode file:</p> <p>There is still no easy way to define your custom agenda views, but it looks possible 1 and 2.</p>"}, {"location": "orgmode/#agenda-searches", "title": "Agenda searches", "text": "<p>When using the search agenda view you can:</p> <ul> <li>Search by TODO states with <code>/WAITING</code></li> <li> <p>Search by tags <code>+home</code>. The syntax for such searches follows a simple boolean logic:</p> </li> <li> <p><code>|</code>: or</p> </li> <li><code>&amp;</code>: and</li> <li><code>+</code>: include matches</li> <li><code>-</code>: exclude matches </li> </ul> <p>Here are a few examples:</p> <ul> <li><code>+computer&amp;+urgent</code>: Returns all items tagged both <code>computer</code> and <code>urgent</code>.</li> <li><code>+computer|+urgent</code>: Returns all items tagged either <code>computer</code> or <code>urgent</code>.</li> <li><code>+computer&amp;-urgent</code>: Returns all items tagged <code>computer</code> and not <code>urgent</code>.</li> </ul> <p>As you may have noticed, the syntax above can be a little verbose, so org-mode offers convenient ways of shortening it. First, <code>-</code> and <code>+</code> imply <code>and</code> if no boolean operator is stated, so example three above could be rewritten simply as:</p> <pre><code>+computer-urgent\n</code></pre> <p>Second, inclusion of matches is implied if no <code>+</code> or <code>-</code> is present, so example three could be further shortened to:</p> <pre><code>computer-urgent\n</code></pre> <p>Example number two, meanwhile, could be shortened to:</p> <pre><code>computer|urgent\n</code></pre> <p>There is no way (as yet) to express search grouping with parentheses. The <code>and</code> operators (<code>&amp;</code>, <code>+</code>, and <code>-</code>) always bind terms together more strongly than <code>or</code> (<code>|</code>). For instance, the following search</p> <pre><code>computer|work+email\n</code></pre> <p>Results in all headlines tagged either with <code>computer</code> or both <code>work</code> and <code>email</code>. An expression such as <code>(computer|work)&amp;email</code> is not supported at the moment. You can construct a regular expression though:</p> <pre><code>+{computer\\|work}+email\n</code></pre> <ul> <li>Search by properties: You can search by properties with the <code>PROPERTY=\"value\"</code> syntax. Properties with numeric values can be queried with inequalities <code>PAGES&gt;100</code>. To search by partial searches use a regular expression, for example if the entry had <code>:BIB_TITLE: Mysteries of the Amazon</code> you could use <code>BIB_TITLE={Amazon}</code></li> </ul>"}, {"location": "orgmode/#capture", "title": "Capture", "text": "<p>Capture lets you quickly store notes with little interruption of your work flow. It works the next way:</p> <ul> <li>Open the interface with <code>;c</code> (Default <code>&lt;leader&gt;oc</code>) that asks you what kind of element you want to capture. </li> <li>Select the template you want to use. By default you only have the <code>Task</code> template, that introduces a task into the same file where you're at, select it by pressing <code>t</code>.</li> <li>Fill up the template.</li> <li>Choose what to do with the captured content:</li> <li>Save it to the configured file by pressing <code>;w</code> (Default <code>&lt;control&gt;c</code>)</li> <li>Refile it to a file by pressing <code>;r</code> (Default <code>&lt;leader&gt;or</code>).</li> <li>Abort the capture <code>;q</code> (Default <code>&lt;leader&gt;ok</code>).</li> </ul> <pre><code>mappings = {\n  global = {\n    org_capture = ';c',\n    },\n  capture = {\n    org_capture_finalize = ';w',\n    org_capture_refile = ';r',\n    org_capture_kill = ';q',\n  },\n}\n</code></pre>"}, {"location": "orgmode/#configure-the-capture-templates", "title": "Configure the capture templates", "text": "<p>Capture lets you define different templates for the different inputs. Each template has the next elements:</p> <ul> <li>Keybinding: Keys to press to activate the template</li> <li>Description: What to show in the capture menu to describe the template</li> <li>Template: The actual template of the capture, look below to see how to create them.</li> <li>Target: The place where the captured element will be inserted to. For example <code>~/org/todo.org</code>. If you don't define it it will go to the file configured in <code>org_default_notes_file</code>.</li> <li>Headline: An optional headline of the Target file to insert the element. </li> </ul> <p>For example:</p> <pre><code>org_capture_templates = {\n  t = { description = 'Task', template = '* TODO %?\\n  %u' }\n}\n</code></pre> <p>For the template you can use the next variables:</p> <ul> <li><code>%?:</code>Default cursor position when template is opened</li> <li><code>%t</code>: Prints current date (Example: <code>&lt;2021-06-10 Thu&gt;</code>)</li> <li><code>%T</code>: Prints current date and time (Example: <code>&lt;2021-06-10 Thu 12:30&gt;</code>)</li> <li><code>%u</code>: Prints current date in inactive format (Example: <code>[2021-06-10 Thu]</code>)</li> <li><code>%U</code>: Prints current date and time in inactive format (Example: <code>[2021-06-10 Thu 12:30]</code>)</li> <li><code>%&lt;FORMAT&gt;</code>: Insert current date/time formatted according to lua date format (Example: <code>%&lt;%Y-%m-%d %A&gt;</code> produces <code>2021-07-02 Friday</code>)</li> <li><code>%x</code>: Insert content of the clipboard via the \"+\" register (see <code>:help clipboard</code>)</li> <li><code>%^{PROMPT|DEFAULT|COMPLETION...}</code>: Prompt for input, if completion is provided an <code>:h inputlist</code> will be used</li> <li><code>%(EXP)</code>: Runs the given lua code and inserts the result. NOTE: this will internally pass the content to the lua <code>load()</code> function. So the body inside <code>%()</code> should be the body of a function that returns a string.</li> <li><code>%f</code>: Prints the file of the buffer capture was called from.</li> <li><code>%F</code>: Like <code>%f</code> but inserts the full path.</li> <li><code>%n</code>: Inserts the current <code>$USER</code></li> <li><code>%a</code>: File and line number from where capture was initiated (Example: <code>[[file:/home/user/projects/myfile.txt +2]]</code>)</li> </ul> <p>For example:</p> <pre><code>{ \n  T = {\n    description = 'Todo',\n    template = '* TODO %?\\n %u',\n    target = '~/org/todo.org'\n  },\n  j = {\n    description = 'Journal',\n    template = '\\n*** %&lt;%Y-%m-%d %A&gt;\\n**** %U\\n\\n%?',\n    target = '~/sync/org/journal.org'\n  },\n  -- Nested key example:\n  e =  'Event',\n  er = {\n    description = 'recurring',\n    template = '** %?\\n %T',\n    target = '~/org/calendar.org',\n    headline = 'recurring'\n  },\n  eo = {\n    description = 'one-time',\n    template = '** %?\\n %T',\n    target = '~/org/calendar.org',\n    headline = 'one-time'\n  },\n  -- Example using a lua function\n  r = {\n    description = \"Repo URL\",\n    template = \"* [[%x][%(return string.match('%x', '([^/]+)$'))]]%?\",\n    target = \"~/org/repos.org\",\n  }\n}\n</code></pre>"}, {"location": "orgmode/#use-capture", "title": "Use capture", "text": ""}, {"location": "orgmode/#synchronize-with-external-calendars", "title": "Synchronize with external calendars", "text": "<p>You may want to synchronize your calendar entries with external ones shared with other people, such as nextcloud calendar or google.</p> <p>The orgmode docs have a tutorial to sync with google and suggests some orgmode packages that do that, sadly it won't work with <code>nvim-orgmode</code>. We'll need to go the \"ugly way\" by:</p> <ul> <li>Downloading external calendar events to ics with <code>vdirsyncer</code>.</li> <li>Importing the ics to orgmode</li> <li>Editing the events in orgmode</li> <li>Exporting from orgmode to ics</li> <li>Uploading then changes to the external calendar events with <code>vdirsyncer</code>.</li> </ul>"}, {"location": "orgmode/#importing-the-ics-to-orgmode", "title": "Importing the ics to orgmode", "text": "<p>There are many tools that do this:</p> <ul> <li><code>ical2orgpy</code> </li> <li><code>ical2org</code> in go</li> </ul> <p>They import an <code>ics</code> file</p>"}, {"location": "orgmode/#exporting-from-orgmode-to-ics", "title": "Exporting from orgmode to ics", "text": ""}, {"location": "orgmode/#other-interesting-features", "title": "Other interesting features", "text": "<p>Some interesting features for the future are:</p> <ul> <li>Effort estimates</li> <li>Clocking</li> </ul>"}, {"location": "orgmode/#troubleshooting", "title": "Troubleshooting", "text": ""}, {"location": "orgmode/#create-an-issue-in-the-orgmode-repository", "title": "Create an issue in the orgmode repository", "text": "<ul> <li>Create a new issue</li> <li>Create the <code>minimal_init.lua</code> file from this file <pre><code>vim.cmd([[set runtimepath=$VIMRUNTIME]])\nvim.cmd([[set packpath=/tmp/nvim/site]])\n\nlocal package_root = '/tmp/nvim/site/pack'\nlocal install_path = package_root .. '/packer/start/packer.nvim'\n\nlocal function load_plugins()\n  require('packer').startup({\n    {\n      'wbthomason/packer.nvim',\n      { 'nvim-treesitter/nvim-treesitter' },\n      { 'kristijanhusak/orgmode.nvim', branch = 'master' },\n    },\n    config = {\n      package_root = package_root,\n      compile_path = install_path .. '/plugin/packer_compiled.lua',\n    },\n  })\nend\n\n_G.load_config = function()\n  require('orgmode').setup_ts_grammar()\n  require('nvim-treesitter.configs').setup({\n    highlight = {\n      enable = true,\n      additional_vim_regex_highlighting = { 'org' },\n    },\n  })\n\n  vim.cmd([[packadd nvim-treesitter]])\n  vim.cmd([[runtime plugin/nvim-treesitter.lua]])\n  vim.cmd([[TSUpdateSync org]])\n\n  -- Close packer after install\n  if vim.bo.filetype == 'packer' then\n    vim.api.nvim_win_close(0, true)\n  end\n\n  require('orgmode').setup({\n    org_agenda_files = {\n      './*'\n    }\n  }\n  )\n\n  -- Reload current file if it's org file to reload tree-sitter\n  if vim.bo.filetype == 'org' then\n    vim.cmd([[edit!]])\n  end\nend\n\nif vim.fn.isdirectory(install_path) == 0 then\n  vim.fn.system({ 'git', 'clone', 'https://github.com/wbthomason/packer.nvim', install_path })\n  load_plugins()\n  require('packer').sync()\n  vim.cmd([[autocmd User PackerCompileDone ++once lua load_config()]])\nelse\n  load_plugins()\n  load_config()\nend\n</code></pre></li> <li>Add the leader configuration at the top of the file <code>vim.g.mapleader = ' '</code></li> <li>Open it with <code>nvim -u minimal_init.lua</code></li> </ul>"}, {"location": "orgmode/#sometimes-doesnt-work", "title": "Sometimes  doesn't work <p>Close the terminal and open a new one (pooooltergeist!).</p>", "text": ""}, {"location": "orgmode/#comparison-with-markdown", "title": "Comparison with Markdown", "text": "<p>What I like of Org mode over Markdown:</p> <ul> <li>The whole interface to interact with the elements of the document through key bindings:</li> <li>Move elements around.</li> <li>Create elements</li> <li>The TODO system is awesome</li> <li>The Agenda system</li> <li>How it handles checkboxes &lt;3</li> <li>Easy navigation between references in the document</li> <li>Archiving feature</li> <li>Refiling feature</li> <li><code>#</code> is used for comments.</li> <li>Create internal document links is easier, you can just copy and paste the heading similar to <code>[[*This is the heading]]</code> on markdown you need to edit it to <code>[](#this-is-the-heading)</code>.</li> </ul> <p>What I like of markdown over Org mode:</p> <ul> <li>The syntax of the headings <code>## Title</code> better than <code>** Title</code>. Although it makes sense to have <code>#</code> for comments.</li> <li>The syntax of the links: <code>[reference](link)</code> is prettier to read and write than <code>[[link][reference]]</code>, although this can be improved if only the reference is shown by your editor (nvim-orgmode doesn't do his yet)</li> </ul>"}, {"location": "orgmode/#references", "title": "References", "text": "<ul> <li>Source</li> <li>Docs</li> <li>Developer docs</li> <li>List of supported commands</li> </ul>"}, {"location": "orgzly/", "title": "Orgzly", "text": "<p>Orgzly is an android application to interact with orgmode files.</p>"}, {"location": "orgzly/#references", "title": "References", "text": "<ul> <li>Docs</li> <li>F-droid page</li> <li>Source</li> <li>Home</li> <li>Alternative docs</li> </ul>"}, {"location": "origami/", "title": "Origami", "text": "<p>Origami, is the art of paper folding, it comes from ori meaning \"folding\", and kami meaning \"paper\" (kami changes to gami due to rendaku)). In modern usage, the word \"origami\" is used as an inclusive term for all folding practices, regardless of their culture of origin. The goal is to transform a flat square sheet of paper into a finished sculpture through folding and sculpting techniques. Modern origami practitioners generally discourage the use of cuts, glue, or markings on the paper.</p>"}, {"location": "origami/#references", "title": "References", "text": "<ul> <li>Mark1626 Digital garden origami     section, for example the     Clover and Hydrangea     Tesslation.</li> </ul>"}, {"location": "osmand/", "title": "OsmAnd", "text": "<p>OsmAnd is a mobile application for global map viewing and navigating based on OpenStreetMaps. Perfect if you're looking for a privacy focused, community maintained open source alternative to google maps.</p>"}, {"location": "osmand/#issues", "title": "Issues", "text": "<ul> <li>Live map update not     working: test that it     works.</li> <li>Add amenity to favourites:     Once it's done, remove the \"Restaurant\", or any other amenity words from the     name of the favourite.</li> <li>Search within favourites     description: Nothing to     do.</li> <li>Favourites inherit the POI     data: Nothing to do.</li> </ul>"}, {"location": "osmand/#references", "title": "References", "text": "<ul> <li>Home</li> <li>Git</li> <li>Reddit</li> </ul>"}, {"location": "outrun/", "title": "Outrun", "text": "<p>Outrun lets you execute a local command using the processing power of another Linux machine.</p>"}, {"location": "outrun/#installation", "title": "Installation", "text": "<pre><code>pip install outrun\n</code></pre>"}, {"location": "outrun/#references", "title": "References", "text": "<ul> <li>Git</li> </ul>"}, {"location": "park_programming/", "title": "Park programming", "text": "<p>Park programming is as you may guess, programming in parks, something I'm trying these days, let's see how it goes.</p>"}, {"location": "park_programming/#how-to-park-program", "title": "How to park program", "text": "<p>To get the best experience out of it I've seen that before you get out it's good to:</p> <ul> <li>Store music in a directory so you don't have to stream it online.</li> <li>Find the best spots.</li> <li>Find the best times.</li> <li>Get out of your dark themed comfort zone and configure a light theme for the     windows you're going to use.</li> <li>Put on sun screen!</li> <li>Bring enough water.</li> <li>Bring a small cushion for the ass, and maybe another for the back.</li> <li>Pee before going out.</li> <li>Go with someone. I haven't yet tried how comfortable is park pair programming     though.</li> </ul> <p>While you're at it:</p> <ul> <li>Teether the internet from the mobile.</li> <li>Use headphones, be respectful <code>(\u0482\u2323\u0300_\u2323\u0301)</code>.</li> <li>Sit well, with your back straight.</li> </ul>"}, {"location": "park_programming/#find-the-best-spots", "title": "Find the best spots", "text": "<p>The best spots are the ones that meet the next criteria:</p> <ul> <li>Quiet places with few human transit.</li> <li>Green surrounding environment.</li> <li>Pleasant heat, sunlight and humidity.</li> <li>Correct seat height.</li> <li>Has laptop support.</li> <li>Seat with back support.</li> <li>Far from bugs.</li> </ul> <p>So far the best spots I've found have been:</p> <ul> <li>Benches in parks.</li> <li>Chess desks.</li> </ul> <p>Look out for different spots to fulfill the different scenarios in terms of amount of heat and sunlight.</p>"}, {"location": "park_programming/#find-the-best-times", "title": "Find the best times", "text": "<p>The best times are the ones that maximize the best spots criteria so:</p> <ul> <li>Weekdays over weekend</li> <li>Early mornings and late afternoons on summer, middle day on spring and autumn.</li> </ul>"}, {"location": "park_programming/#benefits", "title": "Benefits", "text": "<p>By doing park programming you:</p> <ul> <li>Move your ass out of your lair.</li> <li>Get a sun bath</li> <li>Breath fresh open air.</li> <li>Get to smile at passing by dogs or enjoy seeing them play.</li> <li>See trees and nature.</li> <li>Get higher chances to interact with other humans.</li> </ul>"}, {"location": "park_programming/#inconveniences", "title": "Inconveniences", "text": "<ul> <li>You'll probably don't be as comfortable as in your common development place.</li> </ul>"}, {"location": "pdm/", "title": "PDM", "text": "<p>PDM is a modern Python package manager with PEP 582 support. It installs and manages packages in a similar way to npm that doesn't need to create a virtualenv at all!</p>"}, {"location": "pdm/#features", "title": "Features", "text": "<ul> <li>PEP 582 local package installer and runner, no virtualenv involved at all.</li> <li>Simple and relatively fast dependency resolver, mainly for large binary   distributions.</li> <li>A PEP 517 build backend.</li> <li>PEP 621 project metadata.</li> </ul>"}, {"location": "pdm/#installation", "title": "Installation", "text": ""}, {"location": "pdm/#recommended-installation-method", "title": "Recommended installation method", "text": "<pre><code>curl -sSL https://raw.githubusercontent.com/pdm-project/pdm/main/install-pdm.py | python3 -\n</code></pre> <p>For security reasons, you should verify the checksum. The sha256 checksum is: <code>70ac95c53830ff41d700051c9caebd83b2b85b5d6066e8f853006f9f07293ff0</code>, if it doesn't match check if there is a newer version.</p>"}, {"location": "pdm/#other-methods", "title": "Other methods", "text": "<pre><code>pip install --user pdm\n</code></pre>"}, {"location": "pdm/#enable-pep-582-globally", "title": "Enable PEP 582 globally", "text": "<p>To make the Python interpreters aware of PEP 582 packages, one need to add the <code>pdm/pep582/sitecustomize.py</code> to the Python library search path.</p> <pre><code>pdm --pep582 zsh &gt;&gt; ~/.zshrc\n</code></pre>"}, {"location": "pdm/#use-it-with-the-ide", "title": "Use it with the IDE", "text": "<p>Now there are not built-in support or plugins for PEP 582 in most IDEs, you have to configure your tools manually. They say how to configure Pycharm and VSCode, but there's still no instructions for vim.</p> <p>PDM will write and store project-wide configurations in <code>.pdm.toml</code> and you are recommended to add following lines in the <code>.gitignore</code>:</p> <pre><code>.pdm.toml\n__pypackages__/\n</code></pre>"}, {"location": "pdm/#usage", "title": "Usage", "text": "<p>PDM provides a bunch of handful commands to help manage your project and dependencies.</p>"}, {"location": "pdm/#initialize-a-project", "title": "Initialize a project", "text": "<pre><code>pdm init\n</code></pre> <p>Answer several questions asked by PDM and a <code>pyproject.toml</code> will be created for you in the project root:</p> <pre><code>[project]\nname = \"pdm-test\"\nversion = \"0.0.0\"\ndescription = \"\"\nrequires-python = \"&gt;=3.7\"\ndependencies = []\n[[project.authors]]\nname = \"Frost Ming\"\nemail = \"mianghong@gmail.com\"\n\n[project.license]\ntext = \"MIT\"\n</code></pre> <p>If <code>pyproject.toml</code> is already present, it will be updated with the metadata following the PEP 621 specification.</p>"}, {"location": "pdm/#import-project-metadata-from-existing-project-files", "title": "Import project metadata from existing project files", "text": "<p>If you are already other package manager tools like <code>Pipenv</code> or <code>Poetry</code>, it is easy to migrate to PDM. PDM provides <code>import</code> command so that you don't have to initialize the project manually, it now supports:</p> <ol> <li>Pipenv's <code>Pipfile</code></li> <li>Poetry's section in <code>pyproject.toml</code></li> <li>Flit's section in <code>pyproject.toml</code></li> <li><code>requirements.txt</code> format used by Pip</li> </ol> <p>Also, when you are executing <code>pdm init</code> or <code>pdm install</code>, PDM can auto-detect possible files to import if your PDM project has not been initialized yet.</p>"}, {"location": "pdm/#adding-dependencies", "title": "Adding dependencies", "text": "<p><code>pdm add</code> can be followed by one or several dependencies, and the dependency specification is described in PEP 508, you have a summary of the possibilities here.</p> <pre><code>pdm add requests\n</code></pre> <p>PDM also allows extra dependency groups by providing <code>-G/--group &lt;name&gt;</code> option, and those dependencies will go to <code>[project.optional-dependencies.&lt;name&gt;]</code> table in the project file, respectively.</p> <p>After that, dependencies and sub-dependencies will be resolved properly and installed for you, you can view <code>pdm.lock</code> to see the resolved result of all dependencies.</p>"}, {"location": "pdm/#add-local-dependencies", "title": "Add local dependencies", "text": "<p>Local packages can be added with their paths:</p> <pre><code>pdm add ./sub-package\n</code></pre> <p>Local packages can be installed in editable mode (just like <code>pip install -e &lt;local project path&gt;</code> would) using <code>pdm add -e/--editable &lt;local project path&gt;</code>.</p>"}, {"location": "pdm/#add-development-only-dependencies", "title": "Add development only dependencies", "text": "<p>PDM also supports defining groups of dependencies that are useful for development, e.g. some for testing and others for linting. We usually don't want these dependencies appear in the distribution's metadata so using optional-dependencies is probably not a good idea. We can define them as development dependencies:</p> <pre><code>pdm add -d pytest\n</code></pre> <p>This will result in a <code>pyproject.toml</code> as following:</p> <pre><code>[tool.pdm.dev-dependencies]\ntest = [ \"pytest\",]\n</code></pre>"}, {"location": "pdm/#save-version-specifiers", "title": "Save version specifiers", "text": "<p>If the package is given without a version specifier like <code>pdm add requests</code>. PDM provides three different behaviors of what version specifier is saved for the dependency, which is given by <code>--save-&lt;strategy&gt;</code>(Assume <code>2.21.0</code> is the latest version that can be found for the dependency):</p> <ul> <li><code>minimum</code>: Save the minimum version specifier: <code>&gt;=2.21.0</code> (default).</li> <li><code>compatible</code>: Save the compatible version specifier:   <code>&gt;=2.21.0,&lt;3.0.0</code>(default).</li> <li><code>exact</code>: Save the exact version specifier: <code>==2.21.0</code>.</li> <li><code>wildcard</code>: Don't constrain version and leave the specifier to be wildcard:   <code>*</code>.</li> </ul>"}, {"location": "pdm/#supporting-pre-releases", "title": "Supporting pre-releases", "text": "<p>To help package maintainers, you can allow pre-releases to be validate candidates, that way you'll get the issues sooner. It will mean more time to maintain the broken CIs if you update your packages daily (as you should!), but it's the least you can do to help your downstream library maintainers</p> <p>By default, <code>pdm</code>'s dependency resolver will ignore prereleases unless there are no stable versions for the given version range of a dependency. This behavior can be changed by setting allow_prereleases to true in <code>[tool.pdm]</code> table:</p> <pre><code>[tool.pdm]\nallow_prereleases = true\n</code></pre>"}, {"location": "pdm/#update-existing-dependencies", "title": "Update existing dependencies", "text": "<p>To update all dependencies in the lock file use:</p> <pre><code>pdm update\n</code></pre> <p>To update the specified package(s):</p> <pre><code>pdm update requests\n</code></pre> <p>To update multiple groups of dependencies:</p> <pre><code>pdm update -G security -G http\n</code></pre> <p>To update a given package in the specified group:</p> <pre><code>pdm update -G security cryptography\n</code></pre> <p>If the group is not given, PDM will search for the requirement in the default dependencies set and raises an error if none is found.</p> <p>To update packages in development dependencies:</p> <pre><code># Update all default + dev-dependencies\npdm update -d\n# Update a package in the specified group of dev-dependencies\npdm update -dG test pytest\n</code></pre> <p>Keep in mind that <code>pdm update</code> doesn't touch the constrains in <code>pyproject.toml</code>, if you want to update them you'd need to use the <code>--unconstrained</code> flag which will ignore all the constrains of downstream packages and update them to the latest version setting the pin accordingly to your update strategy.</p> <p>Updating the <code>pyproject.toml</code> constrains to match the <code>pdm.lock</code> as close as possible makes sense to avoid unexpected errors when users use other version of the libraries, as the tests are run only against the versions specified in <code>pdm.lock</code>.</p>"}, {"location": "pdm/#about-update-strategy", "title": "About update strategy", "text": "<p>Similarly, PDM also provides 2 different behaviors of updating dependencies and sub-dependencies\uff0c which is given by <code>--update-&lt;strategy&gt;</code> option:</p> <ul> <li><code>reuse</code>: Keep all locked dependencies except for those given in the command   line (default).</li> <li><code>eager</code>: Try to lock a newer version of the packages in command line and their   recursive sub-dependencies and keep other dependencies as they are.</li> </ul>"}, {"location": "pdm/#update-packages-to-the-versions-that-break-the-version-specifiers", "title": "Update packages to the versions that break the version specifiers", "text": "<p>One can give <code>-u/--unconstrained</code> to tell PDM to ignore the version specifiers in the <code>pyproject.toml</code>. This works similarly to the <code>yarn upgrade -L/--latest</code> command. Besides, <code>pdm update</code> also supports the <code>--pre/--prerelease</code> option.</p>"}, {"location": "pdm/#remove-existing-dependencies", "title": "Remove existing dependencies", "text": "<p>To remove existing dependencies from project file and the library directory:</p> <pre><code># Remove requests from the default dependencies\npdm remove requests\n# Remove h11 from the 'web' group of optional-dependencies\npdm remove -G web h11\n# Remove pytest-cov from the `test` group of dev-dependencies\npdm remove -d pytest-cov\n</code></pre>"}, {"location": "pdm/#install-the-packages-pinned-in-lock-file", "title": "Install the packages pinned in lock file", "text": "<p>There are two similar commands to do this job with a slight difference:</p> <ul> <li><code>pdm install</code> will check the lock file and relock if it mismatches with   project file, then install.</li> <li><code>pdm sync</code> installs dependencies in the lock file and will error out if it   doesn't exist. Besides, <code>pdm sync</code> can also remove unneeded packages if   <code>--clean</code> option is given.</li> </ul> <p>All development dependencies are included as long as <code>--prod</code> is not passed and <code>-G</code> doesn't specify any dev groups.</p> <p>Besides, if you don't want the root project to be installed, add <code>--no-self</code> option, and <code>--no-editable</code> can be used when you want all packages to be installed in non-editable versions. With <code>--no-editable</code> turn on, you can safely archive the whole <code>__pypackages__</code> and copy it to the target environment for deployment.</p>"}, {"location": "pdm/#show-what-packages-are-installed", "title": "Show what packages are installed", "text": "<p>Similar to <code>pip list</code>, you can list all packages installed in the packages directory:</p> <pre><code>pdm list\n</code></pre> <p>Or show a dependency graph by:</p> <pre><code>$ pdm list --graph\ntempenv 0.0.0\n\u2514\u2500\u2500 click 7.0 [ required: &lt;7.0.0,&gt;=6.7 ]\nblack 19.10b0\n\u251c\u2500\u2500 appdirs 1.4.3 [ required: Any ]\n\u251c\u2500\u2500 attrs 19.3.0 [ required: &gt;=18.1.0 ]\n\u251c\u2500\u2500 click 7.0 [ required: &gt;=6.5 ]\n\u251c\u2500\u2500 pathspec 0.7.0 [ required: &lt;1,&gt;=0.6 ]\n\u251c\u2500\u2500 regex 2020.2.20 [ required: Any ]\n\u251c\u2500\u2500 toml 0.10.0 [ required: &gt;=0.9.4 ]\n\u2514\u2500\u2500 typed-ast 1.4.1 [ required: &gt;=1.4.0 ]\nbump2version 1.0.0\n</code></pre>"}, {"location": "pdm/#solve-the-locking-failure", "title": "Solve the locking failure", "text": "<p>If PDM is not able to find a resolution to satisfy the requirements, it will raise an error. For example,</p> <pre><code>pdm django==3.1.4 \"asgiref&lt;3\"\n...\n\ud83d\udd12 Lock failed\nUnable to find a resolution for asgiref because of the following conflicts:\nasgiref&lt;3 (from project)\nasgiref&lt;4,&gt;=3.2.10 (from &lt;Candidate django 3.1.4 from https://pypi.org/simple/django/&gt;)\nTo fix this, you could loosen the dependency version constraints in pyproject.toml. If that is not possible, you could also override the resolved version in [tool.pdm.overrides] table.\n</code></pre> <p>You can either change to a lower version of <code>django</code> or remove the upper bound of <code>asgiref</code>. But if it is not eligible for your project, you can tell PDM to forcely resolve <code>asgiref</code> to a specific version by adding the following lines to <code>pyproject.toml</code>:</p> <pre><code>[tool.pdm.overrides]\nasgiref = \"&gt;=3.2.10\"\n</code></pre> <p>Each entry of that table is a package name with the wanted version. The value can also be a URL to a file or a VCS repository like <code>git+https://...</code>. On reading this, PDM will pin <code>asgiref@3.2.10</code> or the greater version in the lock file no matter whether there is any other resolution available.</p> <p>Note: By using <code>[tool.pdm.overrides]</code> setting, you are at your own risk of any incompatibilities from that resolution. It can only be used if there is no valid resolution for your requirements and you know the specific version works. Most of the time, you can just add any transient constraints to the <code>dependencies</code> array.</p>"}, {"location": "pdm/#solve-circular-dependencies", "title": "Solve circular dependencies", "text": "<p>Sometimes <code>pdm</code> is not able to locate the best package combination, or it does too many loops, so to help it you can update your version constrains so that it has the minimum number of candidates.</p> <p>To solve circular dependencies we first need to locate what are the conflicting packages, <code>pdm</code> doesn't make it easy to detect them. To do that first try to update each of your groups independently with <code>pdm update -G group_name</code>. If that doesn't work remove from your <code>pyproject.toml</code> groups of dependencies until the command works and add back one group by group until you detect the ones that fail.</p> <p>Also it's useful to reduce the number of possibilities of versions of each dependency to make things easier to <code>pdm</code>. Locate all the outdated packages by doing <code>pdm show</code> on each package until this issue is solved and run <code>pdm update {package} --unconstrained</code> for each of them. If you're already on the latest version, update your <code>pyproject.toml</code> to match the latest state.</p> <p>Once you have everything to the latest compatible version, you can try to upgrade the rest of the packages one by one to the latest with <code>--unconstrained</code>.</p> <p>In the process of doing these steps you'll see some conflicts in the dependencies that can be manually solved by preventing those versions to be installed or maybe changing the <code>python-requires</code>, although this should be done as the last resource.</p> <p>It also helps to run <code>pdm update</code> with the <code>-v</code> flag, that way you see which are the candidates that are rejected, and you can put the constrain you want. For example, I was seeing the next traceback:</p> <pre><code>pdm.termui: Conflicts detected:\n  pyflakes&gt;=3.0.0 (from &lt;Candidate autoflake 2.0.0 from https://pypi.org/simple/autoflake/&gt;)\n  pyflakes&lt;2.5.0,&gt;=2.4.0 (from &lt;Candidate flake8 4.0.1 from unknown&gt;)\n</code></pre> <p>So I added a new dependency to pin it:</p> <pre><code>[tool.pdm.dev-dependencies]\n# The next ones are required to manually solve the dependencies issues\ndependencies = [\n    # Until flakeheaven supports flake8 5.x\n    # https://github.com/flakeheaven/flakeheaven/issues/132\n    \"flake8&gt;=4.0.1,&lt;5.0.0\",\n    \"pyflakes&lt;2.5.0\",\n]\n</code></pre> <p>If none of the above works, you can override them:</p> <pre><code>[tool.pdm.overrides]\n# To be removed once https://github.com/flakeheaven/flakeheaven/issues/132 is solved\n\"importlib-metadata\" = \"&gt;=3.10\"\n</code></pre> <p>If you get lost in understanding your dependencies, you can try using <code>pydeps</code> to get your head around it.</p>"}, {"location": "pdm/#building-packages", "title": "Building packages", "text": "<p>PDM can act as a PEP 517 build backend, to enable that, write the following lines in your <code>pyproject.toml</code>.</p> <pre><code>[build-system]\nrequires = [ \"pdm-pep517\",]\nbuild-backend = \"pdm.pep517.api\"\n</code></pre> <p><code>pip</code> will read the backend settings to install or build a package.</p>"}, {"location": "pdm/#choose-a-python-interpreter", "title": "Choose a Python interpreter", "text": "<p>If you have used <code>pdm init</code>, you must have already seen how PDM detects and selects the Python interpreter. After initialized, you can also change the settings by <code>pdm use &lt;python_version_or_path&gt;</code>. The argument can be either a version specifier of any length, or a relative or absolute path to the python interpreter, but remember the Python interpreter must conform with the <code>python_requires</code> constraint in the project file.</p>"}, {"location": "pdm/#how-requires-python-controls-the-project", "title": "How <code>requires-python</code> controls the project", "text": "<p>PDM respects the value of <code>requires-python</code> in the way that it tries to pick package candidates that can work on all python versions that <code>requires-python</code> contains. For example, if <code>requires-python</code> is <code>&gt;=2.7</code>, PDM will try to find the latest version of <code>foo</code>, whose <code>requires-python</code> version range is a superset of <code>&gt;=2.7</code>.</p> <p>So, make sure you write <code>requires-python</code> properly if you don't want any outdated packages to be locked.</p>"}, {"location": "pdm/#custom-file-generation", "title": "Custom file generation", "text": "<p>Warning: this method only works if you install the package with <code>pdm</code> if you use <code>pip</code> or any other package manager the <code>build.py</code> script won't be called. Thus a more generic approach is to run the initialization steps in a <code>your_command init</code> step or run the checks on each command.</p> <p>During the build, you may want to generate other files or download resources from the internet. You can achieve this by the setup-script build configuration:</p> <p><code>``toml</code> [tool.pdm.build] setup-script = \"build.py\" <pre><code>In the `build.py` script, pdm-pep517 looks for a build function and calls it with two arguments:\n\n* `src`: the path to the source directory\n* `dst`: the path to the distribution directory\n\nExample:\n\n```python\n# build.py\ndef build(src, dst):\n    target_file = os.path.join(dst, \"mypackage/myfile.txt\")\n    os.makedirs(os.path.dirname(target_file), exist_ok=True)\n    download_file_to(dst)\n</code></pre></p> <p>The generated file will be copied to the resulted wheel with the same hierarchy, you need to create the parent directories if necessary.</p>"}, {"location": "pdm/#build-distribution-artifacts", "title": "Build distribution artifacts", "text": "<pre><code>$ pdm build\n- Building sdist...\n- Built pdm-test-0.0.0.tar.gz\n- Building wheel...\n- Built pdm_test-0.0.0-py3-none-any.whl\n</code></pre>"}, {"location": "pdm/#publishing-artifacts", "title": "Publishing artifacts", "text": "<p>The artifacts can then be uploaded to PyPI by twine or through the <code>pdm-publish</code> plugin. The main developer didn't thought it was worth it, so branchvincent made the plugin (I love this possibility).</p> <p>Install it with <code>pdm plugin add pdm-publish</code>.</p> <p>Then you can upload them with;</p> <pre><code># Using token auth\npdm publish --password token\n# To test PyPI using basic auth\npdm publish -r testpypi -u username -P password\n# To custom index\npdm publish -r https://custom.index.com/\n</code></pre> <p>If you don't want to use your credentials in plaintext on the command, you can use the environmental variables <code>PDM_PUBLISH_PASSWORD</code> and <code>PDM_PUBLISH_USER</code>.</p>"}, {"location": "pdm/#show-the-current-python-environment", "title": "Show the current Python environment", "text": "<pre><code>$ pdm info\nPDM version:        1.11.3\nPython Interpreter: /usr/local/bin/python3.9 (3.9)\nProject Root:       /tmp/tmp.dBlK2rAn2x\nProject Packages:   /tmp/tmp.dBlK2rAn2x/__pypackages__/3.9\n</code></pre> <pre><code>$ pdm info --env\n{\n  \"implementation_name\": \"cpython\",\n  \"implementation_version\": \"3.9.8\",\n  \"os_name\": \"posix\",\n  \"platform_machine\": \"x86_64\",\n  \"platform_release\": \"4.19.0-5-amd64\",\n  \"platform_system\": \"Linux\",\n  \"platform_version\": \"#1 SMP Debian 4.19.37-5+deb10u1 (2019-07-19)\",\n  \"python_full_version\": \"3.9.8\",\n  \"platform_python_implementation\": \"CPython\",\n  \"python_version\": \"3.9\",\n  \"sys_platform\": \"linux\"\n}\n</code></pre>"}, {"location": "pdm/#manage-project-configuration", "title": "Manage project configuration", "text": "<p>Show the current configurations:</p> <pre><code>pdm config\n</code></pre> <p>Get one single configuration:</p> <pre><code>pdm config pypi.url\n</code></pre> <p>Change a configuration value and store in home configuration:</p> <pre><code>pdm config pypi.url \"https://test.pypi.org/simple\"\n</code></pre> <p>By default, the configuration are changed globally, if you want to make the config seen by this project only, add a <code>--local</code> flag:</p> <pre><code>pdm config --local pypi.url \"https://test.pypi.org/simple\"\n</code></pre> <p>Any local configurations will be stored in <code>.pdm.toml</code> under the project root directory.</p> <p>The configuration files are searched in the following order:</p> <ol> <li><code>&lt;PROJECT_ROOT&gt;/.pdm.toml</code> - The project configuration.</li> <li><code>~/.pdm/config.toml</code> - The home configuration.</li> </ol> <p>If <code>-g/--global</code> option is used, the first item will be replaced by <code>~/.pdm/global-project/.pdm.toml</code>.</p> <p>You can find all available configuration items in Configuration Page.</p>"}, {"location": "pdm/#run-scripts-in-isolated-environment", "title": "Run Scripts in Isolated Environment", "text": "<p>With PDM, you can run arbitrary scripts or commands with local packages loaded:</p> <pre><code>pdm run flask run -p 54321\n</code></pre> <p>PDM also supports custom script shortcuts in the optional <code>[tool.pdm.scripts]</code> section of <code>pyproject.toml</code>.</p> <p>You can then run <code>pdm run &lt;shortcut_name&gt;</code> to invoke the script in the context of your PDM project. For example:</p> <pre><code>[tool.pdm.scripts]\nstart_server = \"flask run -p 54321\"\n</code></pre> <p>And then in your terminal:</p> <pre><code>$ pdm run start_server\nFlask server started at http://127.0.0.1:54321\n</code></pre> <p>Any extra arguments will be appended to the command:</p> <pre><code>$ pdm run start_server -h 0.0.0.0\nFlask server started at http://0.0.0.0:54321\n</code></pre> <p>PDM supports 3 types of scripts:</p>"}, {"location": "pdm/#normal-command", "title": "Normal command", "text": "<p>Plain text scripts are regarded as normal command, or you can explicitly specify it:</p> <pre><code>[tool.pdm.scripts.start_server]\ncmd = \"flask run -p 54321\"\n</code></pre> <p>In some cases, such as when wanting to add comments between parameters, it might be more convenient to specify the command as an array instead of a string:</p> <pre><code>[tool.pdm.scripts.start_server]\ncmd = [ \"flask\", \"run\", \"-p\", \"54321\",]\n</code></pre>"}, {"location": "pdm/#shell-script", "title": "Shell script", "text": "<p>Shell scripts can be used to run more shell-specific tasks, such as pipeline and output redirecting. This is basically run via <code>subprocess.Popen()</code> with <code>shell=True</code>:</p> <pre><code>[tool.pdm.scripts.filter_error]\nshell = \"cat error.log|grep CRITICAL &gt; critical.log\"\n</code></pre>"}, {"location": "pdm/#call-a-python-function", "title": "Call a Python function", "text": "<p>The script can be also defined as calling a python function in the form <code>&lt;module_name&gt;:&lt;func_name&gt;</code>:</p> <pre><code>[tool.pdm.scripts.foobar]\ncall = \"foo_package.bar_module:main\"\n</code></pre> <p>The function can be supplied with literal arguments:</p> <pre><code>[tool.pdm.scripts.foobar]\ncall = \"foo_package.bar_module:main('dev')\"\n</code></pre>"}, {"location": "pdm/#environment-variables-support", "title": "Environment variables support", "text": "<p>All environment variables set in the current shell can be seen by <code>pdm run</code> and will be expanded when executed. Besides, you can also define some fixed environment variables in your <code>pyproject.toml</code>:</p> <pre><code>[tool.pdm.scripts.start_server]\ncmd = \"flask run -p 54321\"\n\n[tool.pdm.scripts.start_server.env]\nFOO = \"bar\"\nFLASK_ENV = \"development\"\n</code></pre> <p>Note how we use TOML's syntax to define a compound dictionary.</p> <p>A dotenv file is also supported via <code>env_file = \"&lt;file_path&gt;\"</code> setting.</p> <p>For environment variables and/or dotenv file shared by all scripts, you can define <code>env</code> and <code>env_file</code> settings under a special key named <code>_</code> of <code>tool.pdm.scripts</code> table:</p> <pre><code>[tool.pdm.scripts]\nstart_server = \"flask run -p 54321\"\nmigrate_db = \"flask db upgrade\"\n\n[tool.pdm.scripts._]\nenv_file = \".env\"\n</code></pre> <p>Besides, PDM also injects the root path of the project via <code>PDM_PROJECT_ROOT</code> environment variable.</p>"}, {"location": "pdm/#load-site-packages-in-the-running-environment", "title": "Load site-packages in the running environment", "text": "<p>To make sure the running environment is properly isolated from the outer Python interpreter, site-packages from the selected interpreter WON'T be loaded into <code>sys.path</code>, unless any of the following conditions holds:</p> <ol> <li>The executable is from <code>PATH</code> but not inside the <code>__pypackages__</code> folder.</li> <li><code>-s/--site-packages</code> flag is following <code>pdm run</code>.</li> <li><code>site_packages = true</code> is in either the script table or the global setting    key <code>_</code>.</li> </ol> <p>Note that site-packages will always be loaded if running with PEP 582 enabled(without the <code>pdm run</code> prefix).</p>"}, {"location": "pdm/#show-the-list-of-scripts-shortcuts", "title": "Show the list of scripts shortcuts", "text": "<p>Use <code>pdm run --list/-l</code> to show the list of available script shortcuts:</p> <pre><code>$ pdm run --list\nName        Type  Script           Description\n----------- ----- ---------------- ----------------------\ntest_cmd    cmd   flask db upgrade\ntest_script call  test_script:main call a python function\ntest_shell  shell echo $FOO        shell command\n</code></pre> <p>You can add an <code>help</code> option with the description of the script, and it will be displayed in the <code>Description</code> column in the above output.</p>"}, {"location": "pdm/#manage-caches", "title": "Manage caches", "text": "<p>PDM provides a convenient command group to manage the cache, there are four kinds of caches:</p> <ul> <li><code>wheels/</code> stores the built results of non-wheel distributions and files.</li> <li><code>http/</code> stores the HTTP response content.</li> <li><code>metadata/</code> stores package metadata retrieved by the resolver.</li> <li><code>hashes/</code> stores the file hashes fetched from the package index or calculated   locally.</li> <li><code>packages/</code> The centrialized repository for installed wheels.</li> </ul> <p>See the current cache usage by typing <code>pdm cache info</code>. Besides, you can use <code>add</code>, <code>remove</code> and <code>list</code> subcommands to manage the cache content.</p>"}, {"location": "pdm/#manage-global-dependencies", "title": "Manage global dependencies", "text": "<p>Sometimes users may want to keep track of the dependencies of global Python interpreter as well. It is easy to do so with PDM, via <code>-g/--global</code> option which is supported by most subcommands.</p> <p>If the option is passed, <code>~/.pdm/global-project</code> will be used as the project directory, which is almost the same as normal project except that <code>pyproject.toml</code> will be created automatically for you and it doesn't support build features. The idea is taken from Haskell's stack.</p> <p>However, unlike <code>stack</code>, by default, PDM won't use global project automatically if a local project is not found. Users should pass <code>-g/--global</code> explicitly to activate it, since it is not very pleasing if packages go to a wrong place. But PDM also leave the decision to users, just set the config <code>auto_global</code> to <code>true</code>.</p> <p>If you want global project to track another project file other than <code>~/.pdm/global-project</code>, you can provide the project path via <code>-p/--project &lt;path&gt;</code> option.</p> <p>Warning: Be careful with <code>remove</code> and <code>sync --clean</code> commands when global project is used, because it may remove packages installed in your system Python.</p>"}, {"location": "pdm/#configuration", "title": "Configuration", "text": "<p>All available configurations can be seen here.</p>"}, {"location": "pdm/#dependency-specification", "title": "Dependency specification", "text": "<p>The <code>project.dependencies</code> is an array of dependency specification strings following the PEP 440 and PEP 508.</p> <p>Examples:</p> <pre><code>dependencies = [ \"requests\", \"flask &gt;= 1.1.0\", \"pywin32; sys_platform == 'win32'\", \"pip @ https://github.com/pypa/pip.git@20.3.1\",]\n</code></pre>"}, {"location": "pdm/#editable-requirement", "title": "Editable requirement", "text": "<p>Beside of the normal dependency specifications, one can also have some packages installed in editable mode. The editable specification string format is the same as Pip's editable install mode.</p> <p>Examples:</p> <pre><code>dependencies = [\n    ...,\n    # Local dependency\n    \"-e path/to/SomeProject\",\n    # Dependency cloned\n    \"-e git+http://repo/my_project.git#egg=SomeProject\"\n]\n</code></pre> <p>Note: About editable installation. One can have editable installation and normal installation for the same package. The one that comes at last wins. However, editable dependencies WON'T be included in the metadata of the built artifacts since they are not valid PEP 508 strings. They only exist for development purpose.</p>"}, {"location": "pdm/#optional-dependencies", "title": "Optional dependencies", "text": "<p>You can have some requirements optional, which is similar to <code>setuptools</code>' <code>extras_require</code> parameter.</p> <pre><code>[project.optional-dependencies]\nsocks = [ \"PySocks &gt;= 1.5.6, != 1.5.7, &lt; 2\",]\ntests = [ \"ddt &gt;= 1.2.2, &lt; 2\", \"pytest &lt; 6\", \"mock &gt;= 1.0.1, &lt; 4; python_version &lt; \\\"3.4\\\"\",]\n</code></pre> <p>To install a group of optional dependencies:</p> <pre><code>pdm install -G socks\n</code></pre> <p><code>-G</code> option can be given multiple times to include more than one group.</p>"}, {"location": "pdm/#development-dependencies-groups", "title": "Development dependencies groups", "text": "<p>You can have several groups of development only dependencies. Unlike <code>optional-dependencies</code>, they won't appear in the package distribution metadata such as <code>PKG-INFO</code> or <code>METADATA</code>. And the package index won't be aware of these dependencies. The schema is similar to that of <code>optional-dependencies</code>, except that it is in <code>tool.pdm</code> table.</p> <pre><code>[tool.pdm.dev-dependencies]\nlint = [ \"flake8\", \"black\",]\ntest = [ \"pytest\", \"pytest-cov\",]\ndoc = [ \"mkdocs\",]\n</code></pre> <p>To install all of them:</p> <pre><code>pdm install\n</code></pre> <p>For more CLI usage, please refer to Manage Dependencies</p>"}, {"location": "pdm/#show-outdated-packages", "title": "Show outdated packages", "text": "<pre><code>pdm update --dry-run --unconstrained\n</code></pre>"}, {"location": "pdm/#console-scripts", "title": "Console scripts", "text": "<p>The following content:</p> <pre><code>[project.scripts]\nmycli = \"mycli.__main__:main\"\n</code></pre> <p>will be translated to <code>setuptools</code> style:</p> <pre><code>entry_points = {\"console_scripts\": [\"mycli=mycli.__main__:main\"]}\n</code></pre> <p>Also, <code>[project.gui-scripts]</code> will be translated to <code>gui_scripts</code> entry points group in <code>setuptools</code> style.</p>"}, {"location": "pdm/#entry-points", "title": "Entry points", "text": "<p>Other types of entry points are given by <code>[project.entry-points.&lt;type&gt;]</code> section, with the same format of <code>[project.scripts]</code>:</p> <pre><code>[project.entry-points.pytest11]\nmyplugin = \"mypackage.plugin:pytest_plugin\"\n</code></pre>"}, {"location": "pdm/#include-and-exclude-package-files", "title": "Include and exclude package files", "text": "<p>The way of specifying include and exclude files are simple, they are given as a list of glob patterns:</p> <pre><code>includes = [ \"**/*.json\", \"mypackage/\",]\nexcludes = [ \"mypackage/_temp/*\",]\n</code></pre> <p>In case you want some files to be included in sdist only, you use the <code>source-includes</code> field:</p> <pre><code>includes = [...]\nexcludes = [...]\nsource-includes = [\"tests/\"]\n</code></pre> <p>Note that the files defined in <code>source-includes</code> will be excluded automatically from non-sdist builds.</p>"}, {"location": "pdm/#default-values-for-includes-and-excludes", "title": "Default values for includes and excludes", "text": "<p>If you don't specify any of these fields, PDM also provides smart default values to fit the most common workflows.</p> <ul> <li>Top-level packages will be included.</li> <li><code>tests</code> package will be excluded from non-sdist builds.</li> <li><code>src</code> directory will be detected as the <code>package-dir</code> if it exists.</li> </ul> <p>If your project follows the above conventions you don't need to config any of these fields and it just works. Be aware PDM won't add PEP 420 implicit namespace packages automatically and they should always be specified in <code>includes</code> explicitly.</p>"}, {"location": "pdm/#determine-the-package-version-dynamically", "title": "Determine the package version dynamically", "text": "<p>The package version can be retrieved from the <code>__version__</code> variable of a given file. To do this, put the following under the <code>[tool.pdm]</code> table:</p> <pre><code>[tool.pdm.version]\nfrom = \"mypackage/__init__.py\"\n</code></pre> <p>Remember set <code>dynamic = [\"version\"]</code> in <code>[project]</code> metadata.</p> <p>PDM can also read version from SCM tags. If you are using <code>git</code> or <code>hg</code> as the version control system, define the <code>version</code> as follows:</p> <pre><code>[tool.pdm.version]\nuse_scm = true\n</code></pre> <p>In either case, you MUST delete the <code>version</code> field from the <code>[project]</code> table, and include <code>version</code> in the <code>dynamic</code> field, or the backend will raise an error:</p> <pre><code>dynamic = [ \"version\",]\n</code></pre>"}, {"location": "pdm/#cache-the-installation-of-wheels", "title": "Cache the installation of wheels", "text": "<p>If a package is required by many projects on the system, each project has to keep its own copy. This may become a waste of disk space especially for data science and machine learning libraries.</p> <p>PDM supports caching the installations of the same wheel by installing it into a centralized package repository and linking to that installation in different projects. To enabled it, run:</p> <pre><code>pdm config feature.install_cache on\n</code></pre> <p>It can be enabled on a project basis, by adding <code>--local</code> option to the command.</p> <p>The caches are located under <code>$(pdm config cache_dir)/packages</code>. One can view the cache usage by <code>pdm cache info</code>. But be noted the cached installations are managed automatically. They get deleted when not linked from any projects. Manually deleting the caches from the disk may break some projects on the system.</p> <p>Note: Only the installation of named requirements resolved from PyPI can be cached.</p>"}, {"location": "pdm/#working-with-a-virtualenv", "title": "Working with a virtualenv", "text": "<p>Although PDM enforces PEP 582 by default, it also allows users to install packages into the virtualenv. It is controlled by the configuration item <code>use_venv</code>. When it is set to <code>True</code> (default), PDM will use the virtualenv if:</p> <ul> <li>A virtualenv is already activated.</li> <li>Any of <code>venv</code>, <code>.venv</code>, <code>env</code> is a valid virtualenv folder.</li> </ul> <p>Besides, when <code>use-venv</code> is on and the interpreter path given is a venv-like path, PDM will reuse that venv directory as well.</p> <p>For enhanced virtualenv support such as virtualenv management and auto-creation, please go for pdm-venv, which can be installed as a plugin.</p>"}, {"location": "pdm/#use-pdm-in-continuous-integration", "title": "Use PDM in Continuous Integration", "text": "<p>Fortunately, if you are using GitHub Action, there is pdm-project/setup-pdm to make this process easier. Here is an example workflow of GitHub Actions, while you can adapt it for other CI platforms.</p> <pre><code>Testing:\nruns-on: ${{ matrix.os }}\nstrategy:\nmatrix:\npython-version: [3.7, 3.8, 3.9, 3.10]\nos: [ubuntu-latest, macOS-latest, windows-latest]\n\nsteps:\n- uses: actions/checkout@v1\n- name: Set up PDM\nuses: pdm-project/setup-pdm@main\nwith:\npython-version: ${{ matrix.python-version }}\n\n- name: Install dependencies\nrun: |\npdm sync -d -G testing\n- name: Run Tests\nrun: |\npdm run -v pytest tests\n</code></pre> <p>Note: Tips for GitHub Action users, there is a known compatibility issue on Ubuntu virtual environment. If PDM parallel install is failed on that machine you should either set <code>parallel_install</code> to <code>false</code> or set env <code>LD_PRELOAD=/lib/x86_64-linux-gnu/libgcc_s.so.1</code>. It is already handled by the <code>pdm-project/setup-pdm</code> action.</p> <p>Note: If your CI scripts run without a proper user set, you might get permission errors when PDM tries to create its cache directory. To work around this, you can set the HOME environment variable yourself, to a writable directory, for example:</p> <pre><code>```bash\nexport HOME=/tmp/home\n```\n</code></pre>"}, {"location": "pdm/#how-does-it-work", "title": "How does it work", "text": ""}, {"location": "pdm/#why-you-dont-need-to-use-virtualenvs", "title": "Why you don't need to use virtualenvs", "text": "<p>When you develop a Python project, you need to install the project's dependencies. For a long time, tutorials and articles have told you to use a virtual environment to isolate the project's dependencies. This way you don't contaminate the working set of other projects, or the global interpreter, to avoid possible version conflicts.</p>"}, {"location": "pdm/#problems-of-the-virtualenvs", "title": "Problems of the virtualenvs", "text": "<p>Virtualenvs are confusing for people that are starting with python. They also use a lot of space, as many virtualenvs have their own copy of the same libraries. They help us isolate project dependencies though, but things get tricky when it comes to nested venvs. One installs the virtualenv manager(like Pipenv or Poetry) using a venv encapsulated Python, and creates more venvs using the tool which is based on an encapsulated Python. One day a minor release of Python is out and one has to check all those venvs and upgrade them if required before they can safely delete the out-dated Python version.</p> <p>Another scenario is global tools. There are many tools that are not tied to any specific virtualenv and are supposed to work with each of them. Examples are profiling tools and third-party REPLs. We also wish them to be installed in their own isolated environments. It's impossible to make them work with virtualenv, even if you have activated the virtualenv of the target project you want to work on because the tool is lying in its own virtualenv and it can only see the libraries installed in it. So we have to install the tool for each project.</p> <p>The solution has been existing for a long time. PEP 582 was originated in 2018 and is still a draft proposal till the time I copied this article.</p> <p>Say you have a project with the following structure:</p> <pre><code>.\n\u251c\u2500\u2500 __pypackages__\n\u2502   \u2514\u2500\u2500 3.8\n\u2502       \u2514\u2500\u2500 lib\n\u2514\u2500\u2500 my_script.py\n</code></pre> <p>As specified in the PEP 582, if you run <code>python3.8 /path/to/my_script.py</code>, <code>__pypackages__/3.8/lib</code> will be added to <code>sys.path</code>, and the libraries inside will become import-able in <code>my_script.py</code>.</p> <p>Now let's review the two problems mentioned above under PEP 582. For the first problem, the main cause is that the virtual environment is bound to a cloned Python interpreter on which the subsequent library searching based. It takes advantage of Python's existing mechanisms without any other complex changes but makes the entire virtual environment to become unavailable when the Python interpreter is stale. With the local packages directory, you don't have a Python interpreter any more, the library path is directly appended to <code>sys.path</code>, so you can freely move and copy it.</p> <p>For the second, once again, you just call the tool against the project you want to analyze, and the <code>__pypackages__</code> sitting inside the project will be loaded automatically. This way you only need to keep one copy of the global tool and make it work with multiple projects.</p> <p><code>pdm</code> installs dependencies into the local package directory <code>__package__</code> and makes Python interpreters aware of it with a very simple setup.</p>"}, {"location": "pdm/#how-we-make-pep-582-packages-available-to-the-python-interpreter", "title": "How we make PEP 582 packages available to the Python interpreter", "text": "<p>Thanks to the site packages loading on Python startup. It is possible to patch the <code>sys.path</code> by executing the <code>sitecustomize.py</code> shipped with PDM. The interpreter can search the directories for the nearest <code>__pypackage__</code> folder and append it to the <code>sys.path</code> variable.</p>"}, {"location": "pdm/#plugins", "title": "Plugins", "text": "<p>PDM is aiming at being a community driven package manager. It is shipped with a full-featured plug-in system, with which you can:</p> <ul> <li>Develop a new command for PDM.</li> <li>Add additional options to existing PDM commands.</li> <li>Change PDM's behavior by reading dditional config items.</li> <li>Control the process of dependency resolution or installation.</li> </ul> <p>If you want to write a plugin, start here.</p>"}, {"location": "pdm/#issues", "title": "Issues", "text": "<ul> <li>You can't still   run <code>mypy</code> with <code>pdm</code> without   virtualenvs. pawamoy created a   patch   that is supposed to work, but I'd rather use virtualenvs until it's supported.   Once it's supported check the   vim-test issue to see how   to integrate it.</li> <li>It's not yet supported by   dependabot. Once   supported add it back to the cookiecutter template and spread it.</li> </ul>"}, {"location": "pdm/#references", "title": "References", "text": "<ul> <li>Git</li> <li>Docs</li> </ul>"}, {"location": "pedal_pc/", "title": "Pedal PC", "text": "<p>The Pedal PC idea gathers crazy projects that try to use the energy of your pedaling while you are working on your PC. The most interesting is PedalPC, but still crazy.</p> <p>Pedal-Power is another similar project, although it looks unmaintained.</p>"}, {"location": "pedal_pc/#references", "title": "References", "text": "<ul> <li>PedalPC Blog</li> </ul>"}, {"location": "peek/", "title": "Peek", "text": "<p>Peek is a simple animated GIF screen recorder with an easy to use interface.</p>"}, {"location": "peek/#installation", "title": "Installation", "text": "<pre><code>sudo apt-get install peek\n</code></pre> <p>If you try to use it with i3, you're going to have a bad time, you'd need to install Compton, and then the elements may not even be clickable.</p> <p>With kitty it works though :)</p>"}, {"location": "peek/#references", "title": "References", "text": "<ul> <li>Git</li> </ul>"}, {"location": "personal_interruption_analysis/", "title": "Personal Interruption Analysis", "text": "<p>This is the interruption analysis report applied to my personal life.</p> <p>I've identified the next interruption sources:</p> <ul> <li>Physical interruptions.</li> <li>Emails.</li> <li>Calls.</li> <li>Instant message applications.</li> <li>Calendar events.</li> <li>Other desktop notifications.</li> </ul>"}, {"location": "personal_interruption_analysis/#physical-interruptions", "title": "Physical interruptions", "text": "<p>The analysis is similar to the work physical interruptions.</p>"}, {"location": "personal_interruption_analysis/#emails", "title": "Emails", "text": "<p>Email can be used as one of the main aggregators of interruptions as it's supported by almost everything. I use it as the notification of things that don't need to be acted upon immediately or when more powerful mechanisms are not available. In my case emails can be categorized as:</p> <ul> <li>Bill or bank receipt emails: I receive at least one per provider per month,     the associated action is to download the attached pdf and remove the email.     I've got it automated using a Python program that I need to manually run. In     the future I expect it to be done automatically without my     interaction. There is no urgency to     act on them.</li> <li>General information: In the past I was subscribed to newsletters, now I prefer     to use RSS. They don't usually require any direct action, so they can     wait more than two days.</li> <li>Videogame deals: I was subscribed to Humblebundle, GOG and Steam notifications     to be notified on the deals, but then I migrated to     IsThereAnyDeal because it only sends the     notifications of the deals that match a defined criteria (reducing the     amount of emails), and monitors all sites in one place. I can act on them     with one or two days of delay.</li> <li>Source code manager notifications: The web where I host my source code sends     me emails when there are new pull requests or when there are comments on     existent ones. I try to act on them daily.</li> <li>The CI sends notifications when some job fails. Unless it's a new     pipeline or I'm actively working on it, a failed work job can wait many days     broken before I need to interact with ithe.</li> <li>Infrastructure notifications: For example LetsEncrypt renewals or cloud provider     notification or support cases. The related actions can wait a day or more.</li> <li>Monitorization notifications: I've configured Prometheus's     alertmanager to send the notifications to the email as     a fallback channel, but it's to be checked only if the main channel is down.</li> <li>Stranger emails: People whom I don't know that contacts me asking questions.     These can be dealt with daily.</li> </ul> <p>In conclusion, I can check the personal emails twice a day, one after breakfast and another in the middle of the afternoon. So its safe to disable the notifications.</p> <p>I'm eager to start the email automation project so I can spend even less time and willpower managing the email.</p>"}, {"location": "personal_interruption_analysis/#calls", "title": "Calls", "text": "<p>People are not used to call anymore, most of them prefer to chat. Even though it is much less efficient. I prefer to have less frequent calls where you have full focused interaction rather than many chat sessions.</p> <p>I categorize the calls in two groups:</p> <ul> <li>Social interactions: managed similar as the physical     social interactions,     with the people that I speak regularly, we arrange meetings that suit us     both, the others I tell which are good time spans to call me. If the     conversation allows it, I try to use headphones and simultaneously do     mindless tasks such as folding the clothes or cleaning the kitchen. To     prioritize and adjust the time between calls for each people I use     relationship management processes.</li> <li>Spam callers: Hateful events where you can't dump all the frustration that     they produce on the person that calls you as it's not their fault and they     surely are not enjoying either the situation. They have the lowest priority     and can be safely ignored and blocked. You can manually do it in the phone,     although it's not very effective as they change numbers. A better approach     is to add your number to do not call registries which legally allow you to     scare them off.</li> </ul> <p>As calls are very rare and of high priority, I have my phone configured to ring on incoming calls.</p>"}, {"location": "personal_interruption_analysis/#instant-messages", "title": "Instant messages", "text": "<p>It's the main communication channel for most people, so it has a great volume of events but most have low priority. They can be categorized as:</p> <ul> <li>Asking for help through direct messages: We don't have many as we've agreed to     use groups as much as     possible.     So they have high priority and I have the notifications enabled.</li> <li>Social interaction through direct messages: I don't have many as I try to     arrange one on one calls     instead,     so they have a low priority.</li> <li>Team group or support rooms: We've defined the interruption role so I check them     whenever an chosen interruption event comes.</li> <li>Information rooms: They have no priority and can be checked daily.</li> </ul> <p>In conclusion, I can check the personal chat applications three times per day, for example, after each meal. As I usually need them when I'm off the computer, I only have them configured at my mobile phone, with no sound notifications. That way I only check them when I want to.</p>"}, {"location": "personal_interruption_analysis/#desktop-notifications", "title": "Desktop notifications", "text": "<p>I have none but I've seen people have a notification each time the music player changes of song. It makes no sense at all.</p>"}, {"location": "pexpect/", "title": "pexpect", "text": "<p>pexpect is a pure Python module for spawning child applications; controlling them; and responding to expected patterns in their output. Pexpect works like Don Libes\u2019 Expect. Pexpect allows your script to spawn a child application and control it as if a human were typing commands.</p>"}, {"location": "pexpect/#installation", "title": "Installation", "text": "<pre><code>pip install pexpect\n</code></pre>"}, {"location": "pexpect/#usage", "title": "Usage", "text": "<pre><code>import pexpect\n\nchild = pexpect.spawn('ftp ftp.openbsd.org')\nchild.expect('Name .*: ')\nchild.sendline('anonymous')\n</code></pre> <p>If you're using it to spawn a program that asks something and then ends, you can catch the end with <code>.expect_exact(pexpect.EOF)</code>.</p> <pre><code>tui = pexpect.spawn(\"python source.py\", timeout=5)\ntui.expect(\"Give me .*\")\ntui.sendline(\"HI\")\ntui.expect_exact(pexpect.EOF)\n</code></pre> <p>The <code>timeout=5</code> is useful if the <code>pexpect</code> interaction is not well defined, so that the script is not hung forever.</p>"}, {"location": "pexpect/#send-key-presses", "title": "Send key presses", "text": "<p>To simulate key presses, you can use prompt_toolkit keys with REVERSE_ANSI_SEQUENCES.</p> <pre><code>from prompt_toolkit.input.ansi_escape_sequences import REVERSE_ANSI_SEQUENCES\nfrom prompt_toolkit.keys import Keys\n\ntui = pexpect.spawn(\"python source.py\", timeout=5)\ntui.send(REVERSE_ANSI_SEQUENCES[Keys.ControlC])\n</code></pre> <p>To make your code cleaner you can use a helper class:</p> <pre><code>from prompt_toolkit.input.ansi_escape_sequences import REVERSE_ANSI_SEQUENCES\nfrom prompt_toolkit.keys import Keys\n\nclass Keyboard(str, Enum):\n    ControlH = REVERSE_ANSI_SEQUENCES[Keys.ControlH]\n    Enter = \"\\r\"\n    Esc = REVERSE_ANSI_SEQUENCES[Keys.Escape]\n\n    # Equivalent keystrokes in terminals; see python-prompt-toolkit for\n    # further explanations\n    Alt = Esc\n    Backspace = ControlH\n</code></pre>"}, {"location": "pexpect/#read-output-of-command", "title": "Read output of command", "text": "<pre><code>import sys\nimport pexpect\nchild = pexpect.spawn('ls', encoding='utf-8')\nchild.logfile = sys.stdout\nchild.expect(pexpect.EOF)\n</code></pre> <p>For the tests, you can use the capsys fixture to do assertions on the content:</p> <pre><code>out, err = capsys.readouterr()\nassert \"WARNING! you took 1 seconds to process the last element\" in out\n</code></pre>"}, {"location": "pexpect/#references", "title": "References", "text": "<ul> <li>Docs</li> </ul>"}, {"location": "pilates/", "title": "Pilates", "text": "<p>Pilates is a physical fitness system based on controlled movements putting emphasis on alignment, breathing, developing a strong core, and improving coordination and balance. The core (or powerhouse), consisting of the muscles of the abdomen, low back, and hips, is thought to be the key to a person's stability.</p> <p>Pilates' system allows for different exercises to be modified in range of difficulty from beginner to advanced or to any other level, and also in terms of the instructor and practitioner's specific goals and/or limitations. Intensity can be increased over time as the body adapts itself to the exercises.</p> <p>You can think of yoga, but without the spiritual aspects.</p>"}, {"location": "pilates/#principles", "title": "Principles", "text": ""}, {"location": "pilates/#breathing", "title": "Breathing", "text": "<p>The breathing in Pilates is meant the to be deeper, with full inhalations and complete exhalations. In order to keep the lower abdominals close to the spine, the breathing needs to be directed to the back and sides of the lower rib cage. When exhaling, you need to squeeze out the lungs as they would wring a wet towel dry. To do that you need to contract the deep abdominal and pelvic floor muscles, feeling your bellybutton going to your back and a little bit up. When inhaling you need to maintain this engagement to keep the core in control.</p> <p>The difficult part comes when you try to properly coordinate this breathing practice with the exercise movement, breathes out with the effort and in on the return.</p> <p>This technique is important as it increases the intake of oxygen and the circulation of this oxygenated blood to every part of the body, cleaning and invigorating it.</p>"}, {"location": "pilates/#concentration", "title": "Concentration", "text": "<p>It demands intense focus, as you need to be aware of the position of each part of your body, and how they move to precisely do the exercise.</p>"}, {"location": "pilates/#control", "title": "Control", "text": "<p>You don't see many quick movements, most of the exercises are anaerobic. The difficult relies on controlling your muscles to do what you want them to while they fight against gravity, springs and other torture tools.</p>"}, {"location": "pilates/#flow", "title": "Flow", "text": "<p>Pilates aims for elegant economy of movement, creating flow through the use of appropriate transitions. Once precision has been achieved, the exercises are intended to flow within and into each other in order to build strength and stamina.</p> <p>A smoothly doing a roll down (from seated position with your legs straight, slowly lay down, vertebrae by vertebrae) is a difficult challenge, as you need every muscle to coordinate to share the load of the weight. The muscles that we use more are stronger, and some of them are barely used, Pilates positions and slow transitions force you to use those weak, forgotten muscles, and when the load is transferred from the strong to the weak, your body starts shaking or breaks the movement rate thus breaking the flow.</p> <p>Even though it looks silly, it's tough.</p>"}, {"location": "pilates/#postural-alignment", "title": "Postural alignment", "text": "<p>Being more aware of your body by bringing to it's limits with each exercise, seeing where it fails, strengthening the weak muscles and practicing flow and control results in a better postural alignment.</p>"}, {"location": "pilates/#precision", "title": "Precision", "text": "<p>The focus is on doing one precise and perfect movement, rather than many halfhearted ones. The goal is for this precision to eventually become second nature and carry over into everyday life as grace and economy of movement.</p>"}, {"location": "pilates/#relaxation", "title": "Relaxation", "text": "<p>Correct muscle firing patterns and improved mental concentration are enhanced with relaxation.</p>"}, {"location": "pilates/#stamina", "title": "Stamina", "text": "<p>Stamina is increased through the gradual strengthening of your body, and with the increasing precision of the motion, making them more efficient so there is less stress to perform the exercises.</p>"}, {"location": "pilates/#positions", "title": "Positions", "text": "<ul> <li>Feet in flex: your toes go away from your shins, so your foot follows your     shin line.</li> </ul>"}, {"location": "pilates/#exercises", "title": "Exercises", "text": "<p>I'm going to annotate the exercises I like most, probably the name is incorrect and the explanation not perfect. If you do them, please be careful, and in case of doubt ask a Pilates teacher.</p>"}, {"location": "pilates/#swing-from-table", "title": "Swing from table", "text": "<p>Lvl 0:</p> <ul> <li>Starting position (Inhale): Start at step 1 of the table.</li> <li>Step 1 (Exhale): Instead of going to the mat, when exhaling move your ass     between your arms without touching the mat until it's behind them. Round     your spine in the process. You'll feel a nice spine movement similar to the     cat - cow movement.</li> <li>Return (Inhale): Slowly go back to starting position.</li> </ul> <p>Lvl 1:</p> <ul> <li>Starting position (Inhale): Start at step 1 of the  table with     one leg straight in the air, in the same line as your shoulders, hips and     knee.</li> <li>Step 1 (Exhale): Similar to Lvl 0 but make sure that the heel of the foot     doesn't touch the mat, feet in flex.</li> <li>Return (Inhale): Slowly go back to starting position, do X repetitions and     then switch to the other foot.</li> </ul> <p>Lvl 2:</p> <ul> <li>Starting position (Inhale): Start at step 1 of the inverted plank.</li> <li>Step 1 (Exhale): Similar to Lvl 0.</li> <li>Return (Inhale): Slowly go back to starting position.</li> </ul> <p>Lvl 3:</p> <p>Similar to Lvl 2 with the leg up like Lvl 1.</p> <p>I've found that Lvl 2 and Lvl 3 give a less pleasant spine rub.</p>"}, {"location": "pilates/#table", "title": "Table", "text": "<ul> <li> <p>Starting position (Exhale): Sit in your mat with your legs parallel, knees bent and     your feet at two or three fists from your ass, hands on the mat behind you,     fingers pointing to your ass.</p> <p>If you have shoulder aches, you can point the fingers at 45 degrees or away from your ass. * Step 1 (Inhale): Slowly move your ass up until your shoulders, knees and hips are on the same line. To avoid neck pain, keep your chin down so you're looking at your knees. Your knees should be over your ankles and your arms should be extended. * Return (Exhale): Slowly come back to the starting position.</p> </li> </ul>"}, {"location": "pilates/#references", "title": "References", "text": ""}, {"location": "pilates/#books", "title": "Books", "text": "<ul> <li>Pilates anatomy by Rael Isacowitz and Karen     Clippinger:     With gorgeous illustrations.</li> </ul>"}, {"location": "pipenv/", "title": "Pipenv", "text": "<p>Pipenv is a tool that aims to bring the best of all packaging worlds (bundler, composer, npm, cargo, yarn, etc.) to the Python world.</p> <p>It automatically creates and manages a virtualenv for your projects, as well as adds/removes packages from your Pipfile as you install/uninstall packages. It also generates the ever-important Pipfile.lock, which is used to produce deterministic builds.</p>"}, {"location": "pipenv/#features", "title": "Features", "text": "<ul> <li>Enables truly deterministic builds, while easily specifying only     what you want.</li> <li>Generates and checks file hashes for locked dependencies.</li> <li>Automatically install required Pythons, if <code>pyenv</code> is available.</li> <li>Automatically finds your project home, recursively, by looking for a     <code>Pipfile</code>.</li> <li>Automatically generates a <code>Pipfile</code>, if one doesn't exist.</li> <li>Automatically creates a virtualenv in a standard location.</li> <li>Automatically adds/removes packages to a <code>Pipfile</code> when they are     un/installed.</li> <li>Automatically loads <code>.env</code> files, if they exist.</li> </ul> <p>The main commands are <code>install</code>, <code>uninstall</code>, and <code>lock</code>, which generates a <code>Pipfile.lock</code>. These are intended to replace <code>$ pip install</code> usage, as well as manual virtualenv management (to activate a virtualenv, run <code>$ pipenv shell</code>).</p>"}, {"location": "pipenv/#basic-concepts", "title": "Basic Concepts", "text": "<ul> <li>A virtualenv will automatically be created, when one doesn't exist.</li> <li>When no parameters are passed to <code>install</code>, all packages     <code>[packages]</code> specified will be installed.</li> <li>Otherwise, whatever virtualenv defaults to will be the default.</li> </ul>"}, {"location": "pipenv/#other-commands", "title": "Other Commands", "text": "<ul> <li><code>shell</code> will spawn a shell with the virtualenv activated.</li> <li><code>run</code> will run a given command from the virtualenv, with any     arguments forwarded (e.g. <code>$ pipenv run python</code>).</li> <li><code>check</code> asserts that PEP 508 requirements are being met by the     current environment.</li> <li><code>graph</code> will print a pretty graph of all your installed     dependencies.</li> </ul>"}, {"location": "pipenv/#installation", "title": "Installation", "text": "<p>In Debian: <code>apt-get install pipenv</code></p> <p>Or <code>pip install pipenv</code>.</p>"}, {"location": "pipenv/#references", "title": "References", "text": "<ul> <li>Git</li> </ul>"}, {"location": "pipx/", "title": "Pipx", "text": "<p>Pipx is a command line tool to install and run Python applications in isolated environments.</p> <p>Very useful not to pollute your user or device python environments.</p>"}, {"location": "pipx/#installation", "title": "Installation", "text": "<pre><code>pip install pipx\n</code></pre>"}, {"location": "pipx/#usage", "title": "Usage", "text": "<p>Now that you have pipx installed, you can install a program:</p> <pre><code>pipx install PACKAGE\n</code></pre> <p>for example</p> <pre><code>pipx install pycowsay\n</code></pre> <p>You can list programs installed:</p> <pre><code>pipx list\n</code></pre> <p>Or you can run a program without installing it:</p> <pre><code>pipx run pycowsay moooo!\n</code></pre> <p>You can view documentation for all commands by running pipx --help.</p>"}, {"location": "pipx/#upgrade", "title": "Upgrade", "text": "<p>You can use <code>pipx upgrade-all</code> to upgrade all your installed packages. If you want to just upgrade one, use <code>pipx upgrade PACKAGE</code>.</p> <p>If the package doesn't change the requirements of their dependencies so that the installed don't meet them, they won't be upgraded unless you use the <code>--pip-args '--upgrade-strategy eager'</code> flag.</p> <p>It uses the pip flag <code>upgrade-strategy</code> which can be one of:</p> <ul> <li><code>eager</code>: dependencies are upgraded regardless of whether the currently   installed version satisfies the requirements of the upgraded package(s).</li> <li><code>only-if-needed</code>: dependencies are upgraded only when they do not satisfy the   requirements of the upgraded package(s). This is the default value.</li> </ul>"}, {"location": "pipx/#references", "title": "References", "text": "<ul> <li>Docs</li> <li>Git</li> </ul>"}, {"location": "process_automation/", "title": "Process Automation", "text": "<p>I understand process automation as the act of analyzing yourself and your interactions with the world to find the way to reduce the time or willpower spent on your life processes.</p> <p>Once you've covered some minimum life requirements (health, money or happiness), time is your most valued asset. It's sad to waste it doing stuff that we need but don't increase our happiness.</p> <p>I've also faced the problem of having so much stuff in my mind. Having background processes increase your brain load and are a constant sink of willpower. As a result, when you really need that CPU time, your brain is tired and doesn't work to it's full performance. Automating processes, as well as life logging and task management, allows you to delegate those worries.</p> <p>Process automation can lead to habit building, which reduces even more the willpower consumption of processes, at the same time it reduces the error rate.</p> <p>The marvelous xkcd comic has gathered the essence and pitfalls of process automation many times:</p> <p></p> <p></p> <p></p>"}, {"location": "process_automation/#automating-home-chores", "title": "Automating home chores", "text": "<ul> <li>Using Grocy to maintain the house stock, shopping lists and meal   plans.</li> </ul>"}, {"location": "profanity/", "title": "profanity", "text": "<p>profanity is a console based XMPP client written in C using ncurses and libstrophe, inspired by Irssi.</p>"}, {"location": "profanity/#installation", "title": "Installation", "text": "<pre><code>sudo apt-get install profanity\n</code></pre>"}, {"location": "profanity/#usage", "title": "Usage", "text": ""}, {"location": "profanity/#connect", "title": "Connect", "text": "<p>To connect to an XMPP chat service:</p> <pre><code>/connect user@server.com\n</code></pre> <p>You will be prompted by the status bar to enter your password.</p>"}, {"location": "profanity/#send-one-to-one-message", "title": "Send one to one message", "text": "<p>To open a new window and send a message use the <code>/msg</code> command:</p> <pre><code>/msg mycontact@server.com Hello there!\n</code></pre> <p>Profanity uses the contact's nickname by default, if one exists. For example:</p> <pre><code>/msg Bob Are you there bob?\n</code></pre>"}, {"location": "profanity/#window-navigation", "title": "Window navigation", "text": "<p>To make a window visible in the main window area, use any of the following:</p> <ul> <li><code>Alt-1</code> to <code>Alt-0</code></li> <li><code>F1</code> to <code>F10</code></li> <li><code>Alt-left</code>, <code>Alt-right</code></li> </ul> <p>The <code>/win</code> command may also be used. Either the window number may be passed, or the window title:</p> <pre><code>/win 4\n/win someroom@chatserver.org\n/win MyBuddy\n</code></pre> <p>To close the current window:</p> <pre><code>/close\n</code></pre>"}, {"location": "profanity/#adding-contacts", "title": "Adding contacts", "text": "<p>To add someone to your roster:</p> <pre><code>/roster add newfriend@server.chat.com\n</code></pre> <p>To subscribe to a contacts presence (to be notified when they are online/offline etc):</p> <pre><code>/sub request newfriend@server.chat.com\n</code></pre> <p>To approve a contact's request to subscribe to your presence:</p> <pre><code>/sub allow newfriend@server.chat.com\n</code></pre>"}, {"location": "profanity/#giving-contacts-a-nickname", "title": "Giving contacts a nickname", "text": "<pre><code>/roster nick bob@company.org Bobster\n</code></pre>"}, {"location": "profanity/#logging-out", "title": "Logging out", "text": "<p>To quit profanity:</p> <pre><code>/quit\n</code></pre>"}, {"location": "profanity/#configure-omemo", "title": "Configure OMEMO", "text": "<pre><code>/omemo gen\n/carbons on\n</code></pre>"}, {"location": "profanity/#references", "title": "References", "text": "<ul> <li>Home</li> <li>Quickstart</li> </ul>"}, {"location": "projects/", "title": "Projects", "text": "<p>There is an ever growing pool of ideas where I want to invest my time. Sadly time is a finite currency, and even though I am lucky enough to be able to put my focus on maximizing it, it's never enough.</p> <p>I understand projects as a mental tool that groups ideas, processes and tools to achieve a specific goal. Following the digital garden metaphor, projects are plants in different phases of development where I've spent a different amount of effort.</p> <p>The development phases are:</p> <ul> <li> <p>Seeds: Are raw, basic ideas of projects that may once be.</p> </li> <li> <p>Seedlings: Are projects that don't yet have their first stable version, but     the drafts of the ADR and some code is already written.</p> </li> <li> <p>Growing: Projects that have a stable release and are     under active development.</p> </li> <li> <p>Dormant: Projects whose growth has temporally stopped.     I still believe they are useful and even though I don't want to work on them     at the moment, I see myself doing it in the future.</p> <p>I still maintain them by answering to issues, reviewing pull requests, keeping the continuous integration pipelines alive and developing fixes to important issues.</p> </li> <li> <p>Dying: Projects that I know are going to be deprecated     soon, and I'm looking for alternatives.</p> </li> <li> <p>Dead: Project no longer used.</p> </li> </ul>"}, {"location": "projects/#growing-plants", "title": "Growing plants", "text": ""}, {"location": "projects/#blue-book", "title": "Blue book", "text": "<p>What you're reading right now. I'm storing most of the new knowledge I learn every day. At the same time I'm migrating the notes of the previous version of this digital garden which consists on 7422 articles, almost 50 million lines.</p>"}, {"location": "projects/#repository-orm", "title": "Repository ORM", "text": "<p>I'm creating a Python library to make it easier to use the repository pattern in new projects.</p> <p>I monthly spin up new ideas for programs, and managing the storage of the information is cumbersome and repeating. My idea is to refactor that common codebase into a generic library that anyone can use.</p>"}, {"location": "projects/#pynbox", "title": "Pynbox", "text": "<p>I wanted a system to improve the management of ideas, tasks, references, suggestions when I'm not in front of the computer. Right now I've got Markor for Android to register these quicknotes, but the reality is that I don't act upon them, so it's just a log of tasks that never get done, and ideas, references and suggestions that aren't registered in my knowledge or media management systems.</p> <p>On the computer there are also cases of tasks that are not worth registering in the task management system, or ideas that I get at a moment but don't have time to process at the moment.</p> <p>The idea then is to automatically sync the Android quicknote with syncthing, and have a special format for the file that allows <code>pynbox</code> to extract the elements from that file to the \"inbox system\". For example:</p> <pre><code>t. buy groceries\ntv. IT crowd\ni. Improve the inbox management\n\nI want a system to improve ...\n</code></pre> <p>Gets introduced in the \"inbox system\" as a task, a TV suggestion and an idea.</p>"}, {"location": "projects/#dormant-plants", "title": "Dormant Plants", "text": ""}, {"location": "projects/#mkdocs-newsletter", "title": "mkdocs-newsletter", "text": "<p>MkDocs plugin to show the changes of documentation repositories in a user friendly format, at the same time that it's easy for the authors to maintain.</p> <p>It creates daily, weekly, monthly and yearly newsletter articles with the changes of each period. Those pages, stored under the <code>Newsletters</code> section, are filled with the changes extracted from the commit messages of the git history. The changes are grouped by categories, subcategories and then by file using the order of the site's navigation structure. RSS feeds are also created for each newsletter type, so it's easy for people to keep updated with the evolution of the site.</p> <p>I use it for this site newsletters.</p>"}, {"location": "projects/#autoimport", "title": "Autoimport", "text": "<p>Throughout the development of a python program you continuously need to manage the python import statements either because you need one new object or because you no longer need it. This means that you need to stop writing whatever you were writing, go to the top of the file, create or remove the import statement and then resume coding.</p> <p>This workflow break is annoying and almost always unnecessary. autoimport solves this problem if you execute it whenever you have an import error, for example by configuring your editor to run it when saving the file.</p> <p>The reasons why it is dormant are:</p> <ul> <li>The current features cover most of needs. Even though I'd like to be able to     import broken package     objects, and that it is     intelligent enough to use relative     imports.</li> <li>My hype is elsewhere.</li> </ul>"}, {"location": "projects/#clinv", "title": "Clinv", "text": "<p>As part of my DevSecOps work, I need to have an updated inventory of cloud assets organized under a risk management framework.</p> <p>As you can see in how do you document your infrastructure?, there is still a void on how to maintain an inventory of dynamic resources with a DevSecOps point of view.</p> <ul> <li>Manage a dynamic inventory of risk management resources (Projects, Services,   Information, People) and infrastructure resources (EC2, RDS, S3, Route53, IAM   users, IAM groups\u2026).</li> <li>Add risk management metadata to your AWS resources.</li> <li>Monitor if there are resources that are not inside your inventory.</li> <li>Perform regular expression searches on all your resources.</li> <li>Get all your resources information.</li> <li>Works from the command line.</li> </ul> <p>So I started building clinv.</p>"}, {"location": "projects/#yamlfix", "title": "yamlfix", "text": "<p>A simple opinionated yaml formatter that keeps your comments.</p> <p>The reasons why it is dormant are:</p> <ul> <li>The current features cover most of needs.</li> <li>My hype is elsewhere.</li> </ul>"}, {"location": "projects/#bruty", "title": "bruty", "text": "<p>Python program to bruteforce dynamic web applications with Selenium.</p>"}, {"location": "projects/#cookiecutter-python-template", "title": "Cookiecutter Python template", "text": "<p>Following the same reasoning as the previous section, I've spent a lot of time investigating quality measures for python projects, such as project structure, ci testing, ci building, dependency management, beautiful docs or pre-commits. With the cookiecutter template, it is easy to create a new project with the best quality measures with zero effort. Furthermore, with cruft I can keep all the projects generated with the template updated with the best practices.</p>"}, {"location": "projects/#mediarss", "title": "Mediarss", "text": "<p>I've always wanted to own the music I listen, because I don't want to give my data to the companies host the streaming services, nor I trust that they'll keep on giving the service.</p> <p>So I started building some small bash scrappers (I wasn't yet introduced to Python) to get the media. That's when I learned to hate the web developers for their constant changes and to love the API.</p> <p>Then I discovered youtube-dl, a Python command-line program to download video or music from streaming sites. But I still laked the ability to stay updated with the artist channels.</p> <p>So mediarss was born. A youtube-dl wrapper to periodically download new content.</p> <p>This way, instead of using Youtube, Soundcloud or Bandcamp subscriptions, I've got a YAML with all the links that I want to monitor.</p>"}, {"location": "projects/#playlist_generator", "title": "Playlist_generator", "text": "<p>When my music library started growing due to mediarss, I wanted to generate playlists filtering my content by:</p> <ul> <li>Rating score fetched with mep.</li> <li>First time/last listened.</li> <li>Never listened songs.</li> </ul> <p>The playlists I usually generate with these filters are:</p> <ul> <li>Random unheard songs.</li> <li>Songs discovered last month/year with a rating score greater than X.</li> <li>Songs that I haven't heard since 20XX  with a rating score greater than   X (this one gave me pleasant surprises ^^).</li> </ul>"}, {"location": "projects/#media-indexation", "title": "Media indexation", "text": "<p>I've got a music collection of more than 136362 songs, belonging to mediarss downloads, bought CDs rips and friend library sharing. It is more less organized in a directory tree by genre, but I lack any library management features. I've got a lot of duplicates, incoherent naming scheme, no way of filtering or intelligent playlist generation.</p> <p>playlist_generator helped me with the last point, based on the metadata gathered with mep, but it's still not enough.</p> <p>So I'm in my way of migrate all the library to beets, and then I'll deprecate mep in favor to a mpd client that allows me to keep on saving the same metadata.</p> <p>Once it's implemented, I'll migrate all the metadata to the new system.</p>"}, {"location": "projects/#home-stock-inventory", "title": "Home Stock inventory", "text": "<p>I try to follow the idea of emptying my mind as much as possible, so I'm able to spend my CPU time wisely.</p> <p>Keeping track of what do you have at home or what needs to be bought is an effort that should be avoided.</p> <p>So I've integrated Grocy in my life.</p>"}, {"location": "projects/#drode", "title": "Drode", "text": "<p>drode is a wrapper over the Drone and AWS APIs to make deployments more user friendly.</p> <p>It assumes that the projects are configured to continuous deliver all master commits to staging. Then those commits can be promoted to production or to staging for upgrades and rollbacks.</p> <p>It has the following features:</p> <ul> <li>Prevent failed jobs to be promoted to production.</li> <li>Promote jobs with less arguments than the drone command line.</li> <li>Wait for a drone build to end, then raise the terminal bell.</li> </ul>"}, {"location": "projects/#create-an-ordered-list-of-digital-gardens", "title": "Create an ordered list of digital gardens", "text": "<p>A best-of-lists compilation of awesome list of digital gardens.</p>"}, {"location": "projects/#seedlings", "title": "Seedlings", "text": ""}, {"location": "projects/#learn-about-antifascism", "title": "Learn about antifascism", "text": "<p>I'm summing up the key insights of Mark's and Pol's awesome books in this article</p>"}, {"location": "projects/#automate-email-management", "title": "Automate email management", "text": "<p>Most of the emails I receive require repetitive actions that can be automated, in the Email management article I'm gathering the steps to setup such an infrastructure.</p>"}, {"location": "projects/#learn-about-sleep", "title": "Learn about sleep", "text": "<p>I'm reading Matthew Walker's awesome book Why we sleep and summing up the key insights in this article</p>"}, {"location": "projects/#life", "title": "Life", "text": "<p>Life is a real time sandbox role game where you play as yourself surviving in today's world.</p>"}, {"location": "projects/#self-hosted-map", "title": "Self hosted map", "text": "<p>I love maps, as well as traveling and hiking. This project aims to create a web interface that let's me interact with the data gathered throughout my life. I'd like to:</p> <ul> <li>Browse the waypoints and routes that I've done.</li> <li>Create routes and export the gpx.</li> <li>Be able to search through the data</li> <li>Plan trips</li> </ul> <p>All the data must live in my servers.</p> <p>I first started with umap but it stopped being responsive when you add many points, and it's not easy to self-host. Then I went with folium, but it lacked the interactively I wanted, so I ended up using dash leaflet.</p> <p>The first phase of the project is to be able to import and browse the existing data. A second phase will be to add the routing functionality, maybe with Leaflet Routing Machine, which will probably need a self-hosted OSRM server. Meanwhile I'm using the brouter-web that uses brouter behind the scenes.</p>"}, {"location": "projects/#seeds", "title": "Seeds", "text": "<p>This is a gathering of tools, ideas or services that I'd like to enjoy.</p> <p>If you have any lead, as smallest as it may be on how to fulfill them, please contact me.</p>"}, {"location": "projects/#beancount-forecast", "title": "Beancount forecast", "text": "<p>I'd like to see a forecast of the evolution of my accounts given an amount of time. Maybe by doing seasonality analysis and forecast in time series as stated here and here.</p> <p>It will also be interesting to see for a given account the evolution of the subaccounts.</p>"}, {"location": "projects/#life-warnings", "title": "Life warnings", "text": "<p>I've always tackled the pursuit of the peace of mind by improving in task management, for example trying to predict when I have to do something in order to avoid a nuisance. Maybe it's more interesting to monitor and visibilice the warnings that are affecting you.</p> <p>Once a warning is active you need to keep track of its evolve throughout time maybe with a simple questionary. This track could be both quantitative (bother level from 1 to 10), and qualitative (a description of the state and evolution).</p> <p>The user will have a view (probably command line interface) of the active warnings, their priority, bother level, bother level change, last update, number of days it's been active, predicted end. Another interesting view can be the evolution of solved warnings and new warnings.</p> <p>Warnings are not completely solved, as they can happen again. That way we can see how often do they reactivate, and maybe we can get an estimate of the next occurrence. It can also be interesting to see how long did the last warnings last and the effectiveness of the different actions taken to solve it.</p> <p>We'll need an easy way to filter and merge warnings so as not to repeat similar ones. I think it's better to prioritize tags over categories to avoid hierarchy problems. For example, we can track a headache with the tags <code>health</code> and <code>head</code>.</p> <p>The activation/deactivation of warnings will change with the warning type:</p> <ul> <li>Manual: User triggers the state change through the program's command line</li> <li> <p>Script: A python or bash script that automatically gathers the data and     decides if the warning is still active or inactive.</p> <p>This method can give a huge boost in the motivation of self logging through quantified self.</p> </li> </ul>"}, {"location": "projects/#pomodoro-command-line", "title": "Pomodoro command line", "text": "<p>Command line to help with the pomodoro workflow, besides the basic stuff it will interact with the task manager, activitywatch and the notifications system so that:</p> <ul> <li>If you are in focus mode, the notifications will be deactivated, once the     pomodoro cycle ends, the notifications will show up.</li> <li>If you are in focus mode, and you check the notification applications,     a notification warning will be shown.</li> <li>As you'll check the notification systems between pomodoro cycles, unless you     start the pomodoro cycle in focus mode, it's assumed that you may need to     interact with them, but if X amount of minutes has passed since the start of     the cycle and you haven't seen them, then it's assumed that you are in focus     mode, and therefore the notifications will be deactivated.</li> <li>When you start a pomodoro cycle it will let you activate one of the task     manager tasks, so it will track the time spent in that task. If you change     the window manager focus to a window that is not related to the task at hand     it will stop recording and show you a warning.</li> </ul>"}, {"location": "projects/#version-update-manager", "title": "Version Update Manager", "text": "<p>Keeping software updated is not easy because:</p> <ul> <li>There are many technologies involved: package managers (apt, yum, pip, yarn,     npm, ...), programming languages (python, java, ruby, ...), operative     systems (Debian, Ubuntu, ...), deployment technologies (OS install, Docker,     Kubernetes, Ansible, Helm), template software (cruft).</li> <li>Each software maintainers use a different version system.</li> <li>Even a small increase in a version may break everything.</li> <li>Sometimes only the latest version is the supported version.</li> <li>It's not easy to check if the update went well.</li> <li>You not only need the desired package to be updated, but also it's     dependencies.</li> </ul> <p>I'd like to find a solution that:</p> <ul> <li>Gives an overall insight of the update status of a system.</li> <li>Automates the update process.</li> <li>Support both single system installation or aggregator of multiple systems.</li> </ul>"}, {"location": "projects/#shared-accounting", "title": "Shared accounting", "text": "<p>I use beancount for my personal accounting, I'd like to have a system that integrates more less easily with beancount and let's do a shared accounting with other people, for example in trips. I've used settle up in the past but it requires access to their servers, and an account linked to google, facebook or one you register in their servers.</p> <p>I've looked at facto but it uses a logic that doesn't apply to my case, it does a heavy use on a common account, instead of minimizing the transactions between the people. I also tried tabby, even though they still don't support Docker, but it doesn't suit my case either :(.</p> <p>Until a new solution shows up, I'll go with Tricky Tripper available in F-Droid, and manage the expenses myself and periodically send the html reports to the rest of the group.</p>"}, {"location": "projects/#improve-the-reliability-of-the-open-science-collections", "title": "Improve the reliability of the Open Science collections", "text": "<p>The current free knowledge efforts are based on the health of a collection of torrents.</p> <p>Something that is needed is a command line tool that reads the list of ill torrents, and downloads the ones that have a low number of seeders and DHT peers. The number of torrents to download could be limited by the amount the user wants to share. A second version could have an interaction with the torrent client so that when a torrent is no longer ill, it's automatically replaced with one that is.</p> <p>Once the tool is built:</p> <ul> <li>Update the Free knowledge post.</li> <li>Promote the tool in the relevant reddit posts     1     and     2,     try to add it to the freeread.org wiki     (source), and to the     awesome-libgen list.</li> </ul>"}, {"location": "projects/#monitor-and-notify-on-disk-prices", "title": "Monitor and notify on disk prices", "text": "<p>Diskprices.com sorts the prices of the disks on the different amazon sites based on many filters. It will be interesting to have a service that monitors the data on this site and alerts the user once there is a deal that matches its criteria.</p> <p>Once it's done, promote it in the DataHoarder reddit.</p>"}, {"location": "projects/#switch-to-a-better-browser", "title": "Switch to a better browser", "text": "<p>Firefox + Trydactil has it's limits, and Firefox has been following an ill path for a while, qutebrowser looks like the perfect replacement.</p> <p>I've just stumbled upon nyxt (code), and it looks superb.</p>"}, {"location": "projects/#automating-computer-file-management", "title": "Automating computer file management", "text": "<p>Today I've stumbled upon organize looks good for automating processes on files. Maybe it's interesting to run it with inotifywait instead of with a cron job.</p>"}, {"location": "projects/#self-hosted-search-engine", "title": "Self hosted search engine", "text": "<p>It would be awesome to be able to self host a personal search engine that performs prioritized queries in the data sources that I choose.</p> <p>This idea comes from me getting tired of:</p> <ul> <li>Forgetting to search in my gathered knowledge before going to the internet.</li> <li>Not being able to prioritize known trusted sources.</li> </ul> <p>Some sources I'd like to query:</p> <ul> <li>Markdown brains, like my blue and red books.</li> <li>Awesome lists.</li> <li>My browsing history.</li> <li>Blogs.</li> <li>learn-anything.</li> <li>Musicbrainz.</li> <li>themoviedb.</li> <li>Wikipedia</li> <li>Reddit.</li> <li>Stackoverflow.</li> <li>Startpage.</li> </ul> <p>Each source should be added as a plugin to let people develop their own.</p> <p>I'd also like to be able to rate in my browsing the web pages so they get more relevance in future searches. That rating can also influence the order of the different sources.</p> <p>It will archive the rated websites to avoid link rot maybe with ipwb.</p> <p>If we use a knowledge graph, we could federate to ask other nodes and help discover or prioritize content.</p> <p>The browsing could be related with knowledge graph tags.</p> <p>We can also have integration with Anki after a search is done.</p> <p>A possible architecture could be:</p> <ul> <li>A flask + Reactjs frontend.</li> <li>An elasticsearch instance for persistence.</li> <li>A Neo4j or knowledge graph to get relations.</li> </ul> <p>It must be open sourced and Linux compatible. And it would be awesome if I didn't have to learn how to use another editor.</p> <p>Maybe meilisearch (follow their blog) or searx could be a solution. Following another approach, archivy looks good too.</p> <p>Or I could use Jina is a search library linked to pydantic, or maybe quickwit if they're more stable and mature than right now.</p> <p>If I've already started the quantified self project, maybe adri's memex  is a good solution.</p>"}, {"location": "projects/#quantified-self", "title": "Quantified self", "text": "<p>I've been gathering data about myself for a long time, but I don't have a centralized solution that lets me extract information.</p> <p>There are already good solutions to start with, being the best HPI:</p> <ul> <li> <p>bionic or their explanations on how to export data can be useful too.</p> </li> <li> <p>rsarai hq</p> </li> </ul> <p>For the interface adri's memex looks awesome! It's inspired in the Andrew Louis talk Building a Memex whose blog posts seems to be a gold mine.</p> <p>Also look at hpi's compilation and the awesome-quantified-self resources.</p>"}, {"location": "projects/#improve-the-way-of-launching-applications-with-i3wm", "title": "Improve the way of launching applications with i3wm", "text": "<p>In the past I tried installing rofi without success, I should try again. If the default features are not enough, check adi1090x's custom resources.</p>"}, {"location": "projects/#improve-the-notification-management-in-linux", "title": "Improve the notification management in Linux", "text": "<p>I want to be able to group and silence the notifications under a custom logic. For example:</p> <ul> <li>If I want to focus on a task, only show the most important ones.</li> <li>Only show alerts once every X minutes. Or define that I want to receive them     the first 10 minutes of every hour.</li> <li>If I'm not working, silence all work alerts.</li> </ul> <p>From what I see dunst notification manager supports rules and filters, if it's not powerful enough, I may use it with a custom script that uses apprise.</p> <p>Check Archlinux dunst wiki page and the source code too.</p>"}, {"location": "projects/#improve-the-hard-drive-monitor-system", "title": "Improve the hard drive monitor system", "text": "<p>Use something like scrutiny (there's a linuxserver image) to collect and display the information. For alerts, use one of their supported providers.</p>"}, {"location": "projects/#improve-the-periodic-tasks-and-application-metrics-monitoring", "title": "Improve the periodic tasks and application metrics monitoring", "text": "<p>Setup an healthchecks instance with the linuxserver image to monitor cronjobs.</p> <p>For the notifications either use the prometheus metrics or an apprise compatible system.</p> <p>See the source code here.</p>"}, {"location": "projects/#aggregate-all-notifications", "title": "Aggregate all notifications", "text": "<p>Instead of reading the email, github, gitlab, discourse, reddit notifications, aggregate all in one place and show them to the user in a nice command line interface.</p> <p>For the aggregator server, my first choice would be gotify.</p>"}, {"location": "projects/#decentralized-encrypted-end-to-end-voip-and-video-software", "title": "Decentralized encrypted end to end VOIP and video software", "text": "<p>I'd like to be able to make phone and video calls keeping in mind that:</p> <ul> <li>Every connection must be encrypted end to end.</li> <li>I trust the security of a linux server more than a user device. This rules out   distributed solutions such as tox that exposes the client   IP in a DHT table.</li> <li>The server solution should be self hosted.</li> <li>It must use tested cryptography, which again rolls out tox.</li> </ul> <p>These are the candidates I've found:</p> <ul> <li>Riot. You'll need to host your own Synapse   server.</li> <li>Jami. I think it can be configured as decentralized if you   host your own DHTproxy, bootstrap and nameserver, but I need to delve further   into how it makes   a call.   I'm not sure, but you'll probably need to use push   notifications   so as not to expose a service from the user device.</li> <li>Linphone. If we host our   Flexisip server, although it asks   for a lot of permissions.</li> </ul> <p>Jitsi Meet it's not an option as it's not end to end encrypted. But if you want to use it, please use Disroot service or host your own.</p>"}, {"location": "projects/#self-hosted-voice-assistant", "title": "Self hosted voice assistant", "text": "<p>Host a virtual assistant in my servers to help me automate repetitive stuff.</p> <p>For offline voice recognition, vosk-api can be used. Or voiceliner once it supports offline voice recognition.</p>"}, {"location": "projects/#others", "title": "Others", "text": "<ul> <li>A tool or service to follow the updates of software, right now I subscribe to     the releases of the github repositories, but I'd like it to be provider     agnostic, and cleaner than the emails github sends.</li> <li>Movie/serie/music rating self hosted solution that based on your ratings   discovers new content.</li> <li>Hiking route classifier and rating self hosted web application.</li> <li>A command line friendly personal CRM like     Monica that is able to     register the time length and rating of     interactions to do data     analysis on my relations.</li> <li>Digital e-ink note taking system that is affordable, self hosted and performs   character recognition.</li> <li>A git issue tracker that keeps the context of why I subscribed to them. Until     I find one, I'll use the issues document.</li> <li>An easy way of creating markdown links to other file's sections. Similar to     mkdx     functionality. I tried using it but it didn't work for me, and I don't know     if it works for other files.</li> <li>A markdown formatter that fixes the indentation of lists.</li> <li>An e-reader support that could be fixed to the wall.</li> </ul>"}, {"location": "projects/#dying-plants", "title": "Dying plants", "text": ""}, {"location": "projects/#mep", "title": "mep", "text": "<p>I started  life logging with <code>mep</code>. One of the first programs I wrote when learning Bash.</p> <p>It is a mplayer wrapper that allows me to control it with i3 key bindings and store metadata of the music I listen.</p> <p>I don't even publish it because it's a horrible program that would make your eyes bleed. 600 lines of code, only 3 functions, 6 levels of nested ifs, no tests at all, but hey, the functions have docstrings! <code>(\uff4f\u30fb_\u30fb)\u30ce\u201d(\u1d17_ \u1d17\u3002)</code></p> <p>The thing is that it works, so I everyday close my eyes and open my ears, waiting for a solution that gives me the same features with mpd.</p>"}, {"location": "projects/#faker-optional", "title": "faker-optional", "text": "<p>Wrapper over other Faker providers to return their value or <code>None</code>. Useful to create data of type <code>Optional[Any]</code>.</p> <p>Not needed anymore as I use pydantic factories now.</p>"}, {"location": "prompt_toolkit_fullscreen_applications/", "title": "Prompt toolkit full screen applications", "text": "<p>Prompt toolkit can be used to build full screen interfaces. This section focuses in how to do it. If you want to build REPL applications instead go to this other article.</p> <p>Typically, an application consists of a layout (to describe the graphical part) and a set of key bindings.</p> <p>Every prompt_toolkit application is an instance of an <code>Application</code> object.</p> <pre><code>from prompt_toolkit import Application\n\napp = Application(full_screen=True)\napp.run()\n</code></pre> <p>When <code>run()</code> is called, the event loop will run until the application is done. An application will quit when exit() is called. The event loop is basically a while-true loop that waits for user input, and when it receives something (like a key press), it will send that to the appropriate handler, like for instance, a key binding.</p> <p>An application consists of several components. The most important are:</p> <ul> <li>I/O objects: the input and output device.</li> <li>The layout: this defines the graphical structure of the application. For     instance, a text box on the left side, and a button on the right side. You     can also think of the layout as a collection of \u2018widgets\u2019.</li> <li>A style: this defines what colors and underline/bold/italic styles are used     everywhere.</li> <li>A set of key bindings.</li> </ul>"}, {"location": "prompt_toolkit_fullscreen_applications/#the-layout", "title": "The layout", "text": "<p>With the <code>Layout</code> object you define the graphical structure of the application, it accepts as argument a nested structure of <code>Container</code> objects, these arrange the layout by splitting the screen in many regions, while controls (children of <code>UIControl</code>, such as <code>BufferControl</code> or <code>FormattedTextControl</code>) are responsible for generating the actual content.</p> <p>Some of the <code>Container</code>s you can use are: <code>HSplit</code>, <code>Vsplit</code>, <code>FloatContainer</code>, <code>Window</code> or <code>ScrollablePane</code>. The <code>Window</code> class itself is particular: it is a <code>Container</code> that can contain a <code>UIControl</code>. Thus, it\u2019s the adapter between the two. The <code>Window</code> class also takes care of scrolling the content and wrapping the lines if needed.</p>"}, {"location": "prompt_toolkit_fullscreen_applications/#conditional-containers", "title": "Conditional Containers", "text": "<p>Sometimes you only want to show containers when a condition is met, <code>ConditionalContainers</code> are meant to fulfill this case. They accept other containers, and a <code>filter</code> condition.</p> <p>You can read more about filters here, the simplest use case is if you have a boolean variable and you use the <code>to_filter</code> function.</p> <pre><code>from prompt_toolkit.layout import ConditionalContainer\nfrom prompt_toolkit.filters.utils import to_filter\n\nshow_header = True\nConditionalContainer(\n    Label('This is an optional text'), filter=to_filter(show_header)\n)\n</code></pre>"}, {"location": "prompt_toolkit_fullscreen_applications/#tables", "title": "Tables", "text": "<p>Currently they are not supported :(, although there is an old PR.</p>"}, {"location": "prompt_toolkit_fullscreen_applications/#controls", "title": "Controls", "text": ""}, {"location": "prompt_toolkit_fullscreen_applications/#focusing-windows", "title": "Focusing windows", "text": "<p>Focusing something can be done by calling the <code>focus()</code> method. This method is very flexible and accepts a <code>Window</code>, a <code>Buffer</code>, a <code>UIControl</code> and more.</p> <p>In the following example, we use <code>get_app()</code> for getting the active application.</p> <pre><code>from prompt_toolkit.application import get_app\n\n# This window was created earlier.\nw = Window()\n\n# ...\n\n# Now focus it.\nget_app().layout.focus(w)\n</code></pre> <p>To focus the next window in the layout you can use <code>app.layout.focus_next()</code>.</p>"}, {"location": "prompt_toolkit_fullscreen_applications/#key", "title": "[Key", "text": "<p>bindings](https://python-prompt-toolkit.readthedocs.io/en/master/pages/full_screen_apps.html#key-bindings)</p> <p>In order to react to user actions, we need to create a <code>KeyBindings</code> object and pass that to our <code>Application</code>.</p> <p>There are two kinds of key bindings:</p> <ul> <li>Global key bindings, which are always active.</li> <li>Key bindings that belong to a certain <code>UIControl</code> and are only active when     this control is focused. Both <code>BufferControl</code> and <code>FormattedTextControl</code>     take a <code>key_bindings</code> argument.</li> </ul> <p>For complex keys you can always look at the <code>Keys</code> class.</p>"}, {"location": "prompt_toolkit_fullscreen_applications/#global-key", "title": "[Global key", "text": "<p>bindings](https://python-prompt-toolkit.readthedocs.io/en/master/pages/full_screen_apps.html#global-key-bindings)</p> <p>Key bindings can be passed to the application as follows:</p> <pre><code>from prompt_toolkit import Application\nfrom prompt_toolkit.key_binding.key_processor import KeyPressEvent\n\nkb = KeyBindings()\napp = Application(key_bindings=kb)\napp.run()\n</code></pre> <p>To register a new keyboard shortcut, we can use the <code>add()</code> method as a decorator of the key handler:</p> <pre><code>from prompt_toolkit import Application\nfrom prompt_toolkit.key_binding import KeyBindings\nfrom prompt_toolkit.key_binding.key_processor import KeyPressEvent\n\nkb = KeyBindings()\n\n@kb.add('c-q')\ndef exit_(event: KeyPressEvent) -&gt; None:\n\"\"\"\n    Pressing Ctrl-Q will exit the user interface.\n\n    Setting a return value means: quit the event loop that drives the user\n    interface and return this value from the `Application.run()` call.\n    \"\"\"\n    event.app.exit()\n\napp = Application(key_bindings=kb, full_screen=True)\napp.run()\n</code></pre> <p>Here you can read for more complex patterns with key bindings.</p> <p>A more programmatically way to add bindings is:</p> <pre><code>from prompt_toolkit.key_binding import KeyBindings\nfrom prompt_toolkit.key_binding.bindings.focus import focus_next\n\nkb = KeyBindings()\nkb.add(\"tab\")(focus_next)\n</code></pre>"}, {"location": "prompt_toolkit_fullscreen_applications/#pass-more-than-one-key", "title": "Pass more than one key", "text": "<p>To map an action to two key presses use <code>kb.add('g', 'g')</code>.</p>"}, {"location": "prompt_toolkit_fullscreen_applications/#styles", "title": "Styles", "text": "<p>Many user interface controls, like <code>Window</code> accept a style argument which can be used to pass the formatting as a string. For instance, we can select a foreground color:</p> <ul> <li><code>fg:ansired</code>: ANSI color palette</li> <li><code>fg:ansiblue</code>: ANSI color palette</li> <li><code>fg:#ffaa33</code>: hexadecimal notation</li> <li><code>fg:darkred</code>: named color</li> </ul> <p>Or a background color:</p> <ul> <li><code>bg:ansired</code>: ANSI color palette</li> <li><code>bg:#ffaa33</code>: hexadecimal notation</li> </ul> <p>Like we do for web design, it is not a good habit to specify all styling inline. Instead, we can attach class names to UI controls and have a style sheet that refers to these class names. The Style can be passed as an argument to the <code>Application</code>.</p> <pre><code>from prompt_toolkit.layout import VSplit, Window\nfrom prompt_toolkit.styles import Style\n\nlayout = VSplit([\n    Window(BufferControl(...), style='class:left'),\n    HSplit([\n        Window(BufferControl(...), style='class:top'),\n        Window(BufferControl(...), style='class:bottom'),\n    ], style='class:right')\n])\n\nstyle = Style([\n     ('left', 'bg:ansired'),\n     ('top', 'fg:#00aaaa'),\n     ('bottom', 'underline bold'),\n ])\n</code></pre> <p>You may need to define the 24bit color depths to see the colors you expect:</p> <pre><code>from prompt_toolkit.output.color_depth import ColorDepth\n\napp = Application(\n    color_depth=ColorDepth.DEPTH_24_BIT,\n    # ...\n)\n</code></pre> <p>If you want to see if a style is being applied in a component, set the style to <code>bg:#dc322f</code> and it will be highlighted in red.</p>"}, {"location": "prompt_toolkit_fullscreen_applications/#dynamically-changing-the-style", "title": "Dynamically changing the style", "text": "<p>You'll need to create a widget, you can take as inspiration the package widgets.</p> <p>To create a row that changes color when it's focused use:</p> <pre><code>from prompt_toolkit.layout.controls import FormattedTextControl\nfrom prompt_toolkit.layout.containers import Window\nfrom prompt_toolkit.application import get_app\n\nclass Row:\n\"\"\"Define row.\n\n    Args:\n        text: text to print\n    \"\"\"\n\n    def __init__(\n        self,\n        text: str,\n    ) -&gt; None:\n\"\"\"Initialize the widget.\"\"\"\n        self.text = text\n        self.control = FormattedTextControl(\n            self.text,\n            focusable=True,\n        )\n\n        def get_style() -&gt; str:\n            if get_app().layout.has_focus(self):\n                return \"class:row.focused\"\n            else:\n                return \"class:row\"\n\n        self.window = Window(\n            self.control, height=1, style=get_style, always_hide_cursor=True\n        )\n\n    def __pt_container__(self) -&gt; Window:\n\"\"\"Return the window object.\n\n        Mandatory to be considered a widget.\n        \"\"\"\n        return self.window\n</code></pre> <p>An example of use would be:</p> <pre><code>layout = HSplit(\n    [\n        Row(\"Test1\"),\n        Row(\"Test2\"),\n        Row(\"Test3\"),\n    ]\n)\n\n# Key bindings\n\nkb = KeyBindings()\n\nkb.add(\"j\")(focus_next)\nkb.add(\"k\")(focus_previous)\n\n\n@kb.add(\"c-c\", eager=True)\n@kb.add(\"q\", eager=True)\ndef exit_(event: KeyPressEvent) -&gt; None:\n\"\"\"Exit the user interface.\"\"\"\n    event.app.exit()\n\n\n# Styles\n\nstyle = Style(\n    [\n        (\"row\", \"bg:#073642 #657b83\"),\n        (\"row.focused\", \"bg:#002b36 #657b83\"),\n    ]\n)\n\n# Application\n\napp = Application(\n    layout=Layout(layout),\n    full_screen=True,\n    key_bindings=kb,\n    style=style,\n    color_depth=ColorDepth.DEPTH_24_BIT,\n)\n\napp.run()\n</code></pre>"}, {"location": "prompt_toolkit_fullscreen_applications/#examples", "title": "Examples", "text": "<p>The best way to understand how it works is by running the examples in the repository, some interesting ones in increasing order of difficult are:</p> <ul> <li>Managing     autocompletion</li> <li>Managing     focus</li> <li>Managing     floats,     and floats with     transparency</li> <li>Add     margins,     such as line number or a scroll bar.</li> <li>Managing     styles.</li> <li>Wrapping lines in     buffercontrol, and line prefixes</li> <li>A working REPL     example.     Interesting to see the <code>SearchToolbar</code> in use (press <code>Ctrl+r</code>), and how to     interact with other windows with handlers.</li> <li>A working     editor:     Complex application that shows how to build a working menu.</li> <li>pyvim: A rewrite of vim in python,     it shows how to test, handle commands and a lot more, very interesting.</li> <li>pymux: Another full screen     application example.</li> </ul>"}, {"location": "prompt_toolkit_fullscreen_applications/#testing", "title": "Testing", "text": "<p>Prompt toolkit application testing can be done at different levels:</p> <ul> <li>Component level: Useful to test how a component manages it's data by itself.</li> <li>Application level: Useful to test how a user interacts with the component.</li> </ul> <p>If you don't know how to test something, I suggest you check how prompt toolkit tests itself. You can also check how do third party packages do their tests too, such as <code>prompt-toolkit-table</code> or <code>pyvim</code>.</p> <p>Keep in mind that you don't usually want to check the result of the <code>stdout</code> or <code>stderr</code> directly, but the state of your component or the application itself.</p>"}, {"location": "prompt_toolkit_fullscreen_applications/#component-level", "title": "Component level", "text": "<p>If your component accepts some input and does some magic on that input without the need to load the application, import the object directly and run tests changing the input directly and asserting the results of the output.</p>"}, {"location": "prompt_toolkit_fullscreen_applications/#application-level", "title": "Application level", "text": "<p>If you want to test the interaction with your component at application level, for example what happens when a user presses a key, you need to instantiate a dummy application and play with it.</p> <p>Imagine we have a <code>TableControl</code> component we want to test that accepts some input in the form of <code>data</code>. We'll use the <code>set_dummy_app</code> function to configure an application that outputs to <code>DummyOutput</code>, and a helper function <code>get_app_and_processor</code> to return the active app and a <code>processor</code> to send key presses.</p> <pre><code>def set_dummy_app(data: Any) -&gt; Any:\n\"\"\"Return a context manager that starts the dummy application.\n\n    This is important, because we need an `Application` with `is_done=False`\n    flag, otherwise no keys will be processed.\n    \"\"\"\n    app: Application[Any] = Application(\n        layout=Layout(Window(TableControl(data))),\n        output=DummyOutput(),\n        input=create_pipe_input(),\n    )\n\n    return set_app(app)\n\n\ndef get_app_and_processor() -&gt; Tuple[Application[Any], KeyProcessor]:\n\"\"\"Return the active application and it's key processor.\"\"\"\n    app = get_app()\n    key_bindings = app.layout.container.get_key_bindings()\n\n    if key_bindings is None:\n        key_bindings = KeyBindings()\n    processor = KeyProcessor(key_bindings)\n    return app, processor\n</code></pre> <p>We've loaded the <code>processor</code> with the key bindings defined in the container. If you want other bindings change them there. For example prompt-toolkit uses a fixture to set them. Remember that you have the <code>merge_key_bindings</code> to join two key binding objects with:</p> <pre><code>key_bindings = merge_key_bindings([key_bindings, control_bindings])\n</code></pre> <p>Once the functions are set, you can make your test. Imagine that we want to check that if the user presses <code>j</code>, the variable <code>_focused_row</code> is incremented by 1. This variable will be used by the component internally to change the style of the rows so that the next element is highlighted.</p> <pre><code>def test_j_moves_to_the_next_row(self, pydantic_data: PydanticData) -&gt; None:\n\"\"\"\n    Given: A well configured table\n    When: j is press\n    Then: the focus is moved to the next line\n    \"\"\"\n    with set_dummy_app(pydantic_data):\n        app, processor = get_app_and_processor()\n\n        processor.feed(KeyPress(\"j\", \"j\"))  # act\n\n        processor.process_keys()\n        assert app.layout.container.content._focused_row == 1\n</code></pre>"}, {"location": "prompt_toolkit_fullscreen_applications/#references", "title": "References", "text": "<ul> <li>Docs</li> <li>Git</li> <li>Projects using prompt_toolkit</li> </ul>"}, {"location": "prompt_toolkit_repl/", "title": "Prompt toolkit REPL", "text": "<p>Prompt toolkit can be used to build REPL interfaces. This section focuses in how to do it. If you want to build full screen applications instead go to this other article.</p>"}, {"location": "prompt_toolkit_repl/#testing", "title": "Testing", "text": "<p>Testing prompt_toolkit or any text-based user interface (TUI) with python is not well documented. Some of the main developers suggest mocking it while others use pexpect.</p> <p>With the first approach you can test python functions and methods internally but it can lead you to the over mocking problem. The second will limit you to test functionality exposed through your program's command line, as it will spawn a process and interact it externally.</p> <p>Given that the TUIs are entrypoints to your program, it makes sense to test them in end-to-end tests, so I'm going to follow the second option. On the other hand, you may want to test a small part of your TUI in a unit test, if you want to follow this other path, I'd start with monkeypatch, although I didn't have good results with it.</p> <pre><code>def test_method(monkeypatch):\n    monkeypatch.setattr('sys.stdin', io.StringIO('my input'))\n</code></pre>"}, {"location": "prompt_toolkit_repl/#using-pexpect", "title": "Using pexpect", "text": "<p>This method is useful to test end to end functions as you need to all the program as a command line. You can't use it to tests python functions internally.</p> <p>File: source.py</p> <pre><code>from prompt_toolkit import prompt\n\ntext = prompt(\"Give me some input: \")\nwith open(\"/tmp/tui.txt\", \"w\") as f:\n    f.write(text)\n</code></pre> <p>File: test_source.py</p> <p>```python import pexpect</p> <p>def test_tui() -&gt; None:     tui = pexpect.spawn(\"python source.py\", timeout=5)     tui.expect(\"Give me .*\")     tui.sendline(\"HI\")     tui.expect_exact(pexpect.EOF)</p> <pre><code>with open(\"/tmp/tui.txt\", \"r\") as f:\n    assert f.read() == \"HI\"\n</code></pre> <p>```</p> <p>Where:</p> <ul> <li>The <code>tui.expect_exact(pexpect.EOF)</code> line is required so that the tests aren't     run before the process has ended, otherwise the file might not exist yet.</li> <li>The <code>timeout=5</code> is required in case that the <code>pexpect</code> interaction is not well     defined, so that the test is not hung forever.</li> </ul> <p>Thank you Jairo Llopis for this solution.</p> <p>I've deduced the solution from his #260 PR into copier, and the comments of #1243</p>"}, {"location": "psu/", "title": "Power supply unit", "text": "<p>Power supply unit is the component of the computer that sources power from the primary source (the power coming from your wall outlet) and delivers it to its motherboard and all its components. Contrary to the common understanding, the PSU does not supply power to the computer; it instead converts the AC (Alternating Current) power from the source to the DC (Direct Current) power that the computer needs.</p> <p>There are two types of PSU: Linear and Switch-mode. Linear power supplies have a built-in transformer that steps down the voltage from the main to a usable one for the individual parts of the computer. The transformer makes the Linear PSU bulky, heavy, and expensive. Modern computers have switched to the switch-mode power supply, using switches instead of a transformer for voltage regulation. They\u2019re also more practical and economical to use because they\u2019re smaller, lighter, and cheaper than linear power supplies.</p> <p>PSU need to deliver at least the amount of power that each component requires, if it needs to deliver more, it simply won't work.</p> <p>Another puzzling question for most consumers is, \u201cDoes a PSU supply constant wattage to the computer?\u201d The answer is a flat No. The wattage you see on the PSUs casing or labels only indicates the maximum power it can supply to the system, theoretically. For example, by theory, a 500W PSU can supply a maximum of 500W to the computer. In reality, the PSU will draw a small portion of the power for itself and distributes power to each of the PC components according to its need. The amount of power the components need varies from 3.3V to 12V. If the total power of the components needs to add up to 250W, it would only use 250W of the 500W, giving you an overhead for additional components or future upgrades.</p> <p>Additionally, the amount of power the PSU supplies varies during peak periods and idle times. When the components are pushed to their limits, say when a video editor maximizes the GPU for graphics-intensive tasks, it would require more power than when the computer is used for simple tasks like web-browsing. The amount of power drawn from the PSU would depend on two things; the amount of power each component requires and the tasks that each component performs.</p>"}, {"location": "psu/#power-supply-efficiency", "title": "Power supply efficiency", "text": "<p>When PSU converts the AC power to DC, some of the power is wasted and is converted to heat. The more heat a PSU generates, the less efficient it is. Inefficient PSUs will likely damage the computer\u2019s components or shorten their lifespans in the long run. They also draw more power from the primary source resulting in higher electricity bills for consumers.</p> <p>You might\u2019ve seen 80 PLUS stickers on PSUs or its other variants like 80 PLUS Bronze, Silver, Gold, Platinum, and Titanium. 80 PLUS is the power supply\u2019s efficiency rating; the power supply must reach 80% efficiency to be certified. It\u2019s a voluntary standard, which means companies don\u2019t need to abide by the standard, but 80 PLUS certifications have become popular because a more efficient power supply can lessen the consumers\u2019 carbon footprint and help them save some bucks on their electric bills. Below is the efficiency rating that a PSU needs to achieve to get the desired rating.</p> Certification Levels Efficiency at 10% Load Efficiency at 20% Load Efficiency at 50% Load Efficiency at 100% Load 80 PLUS (White) \u2014 80% 80% 80% 80 PLUS Bronze \u2014 82% 85% 82% 80 PLUS Silver \u2014 85% 88% 85% 80 PLUS Gold \u2014 87% 90% 87% 80 PLUS Platinum \u2014 90% 92% 89% 80 PLUS Titanium 90% 92% 94% 90% <p>It\u2019s important to note that the 80% efficiency does not mean that the PSU will only supply 80% of its capacity to the computer. It means it will draw additional power from the primary source to only 20% of power is lost or generated as heat during the conversion. A 500W PSU will therefore draw 625W of power from the main to make it 80% efficient.</p> <p>Higher efficiency also means the internal components are subjected to less heat and are likely to have a longer lifespan. They may cost a bit more, but higher certified power supplies tend to be more reliable than others. Luckily, most manufacturers offer warranties.</p>"}, {"location": "psu/#power-supply-shopping-tips", "title": "Power supply shopping tips", "text": "<ul> <li> <p>Determine wattage requirements: You don't need to purchase much more potential     power capacity (wattage) than you\u2019ll ever use. You can calculate roughly how     much power your new or upgraded system will draw from the wall and look for     a capacity point that satisfies your demands. Several power supply sellers have     calculators that will give you a rough estimate of your system's power needs.     You can find a few below:</p> <ul> <li>PCPartPicker</li> <li>OuterVision PSU calculator</li> <li>be quiet! PSU     Calculator</li> <li>Cooler Master Power Calculator</li> <li>Seasonic Wattage Calculator</li> <li>MSI PSU Calculator</li> <li>Newegg PSU Calculator</li> </ul> </li> <li> <p>Consider upcoming GPU power requirements: Although the best graphics cards     are usually more power-efficient than previous generations, their power     consumption increases overall. This is why the latest 12+4 pin connector     that the upcoming generation graphics cards will use will provide up to 600     W of power. Currently, a pair of PCIe 6+2 pin connectors on dedicated cables     are officially rated for up to 300W, and three of these connectors can     deliver up to 450W safely. You should also add the up to 75W that the PCIe     slot can provide in these numbers.</p> <p>What troubles today's power supplies is not the maximum sustained power consumption of a GPU but its power spikes, and this is why various manufacturers suggest strong PSUs for high-end graphics cards. If the PSU's over current and over power protection features are conservatively set, the PSU can shut down once the graphics card asks for increased power, even for very short periods ( nanoseconds range). This is why EVGA offers two different OPP features in its G6 and P6 units, called firmware and hardware OPP. The first triggers at lower loads, in the millisecond range, while the latter triggers at higher loads that last for some nanoseconds. This way, short power spikes from the graphics card are addressed without shutting down the system.</p> <p>If you add the increased power demands of modern high-end CPUs, you can quickly figure out why strong PSUs are necessary again. Please look at our GPU Benchmarks and CPU Benchmarks hierarchies to see how each of these chips perform relative to each other.</p> </li> <li> <p>Check the physical dimensions of your case before buying: If you have     a standard ATX case, whether or not it is one of the best PC cases, an ATX     power supply will fit. But many higher-wattage PSUs are longer than the     typical 5.5 inches. So you'll want to be sure of your cases' PSU clearance.     If you have an exceptionally small or slim PC case, it may require a less     typical (and more compact) SFX power supply.</p> </li> <li> <p>Consider a modular power supply: If your case has lots of room behind the     motherboard, or your chassis doesn't have a window or glass side, you can     cable-wrap the wires you don't' need and stash them inside your rig. But if     the system you're' building doesn't' have space for this, or there is no     easy place to hide your cable mess, it's' worth paying extra for a modular     power supply. Modular PSUs let you only plug in the power cables you need     and leave the rest in the box.</p> </li> </ul>"}, {"location": "psu/#market-analysis", "title": "Market analysis", "text": "<p>I'm searching for a power supply unit that can deliver 264W and can grow up to 373W. This means that the load is:</p> Type 400W 450W 500W Min load 66% 59% 52% Max load 93% 83% 74% <p>Given that PSU look to be more efficient when they have a load of 50%, the 450W or 500W would be better. Although if the efficiency goes over 80 PLUS Gold, the difference is almost negligible. I'd prioritize first the efficiency. But any PSU from 400W to 500W will work for me.</p> <p>Toms Hardware, PCGamer, IGN recommends:</p> <ul> <li>Corsair CX450: It has 80 PLUS Bronze, so I'll discard it</li> <li>XPG Pylon 450: It has 80 PLUS Bronze too...</li> </ul> <p>None of the suggested PSU are suited for my case. I'm going to select then which is the brands that are more suggested:</p> Brand Tom's recommendations PCGamer recommendations IGN Corsair 6 2 3 Be quiet 1 1 1 Silverstone 1 1 1 Cooler Master 1 0 1 XPG 1 1 0 EVGA 1 0 0 Seasonic 0 1 0 <p>It looks that the most popular are Corsair, Be quiet and Silverstone.</p> <ul> <li>Corsair doesn't have anyone with certification above Bronce.</li> <li> <p>Silverstone     Under or equal to 500 it has one gold, at 520 it has a platinum and on the     550W it has has 4 gold and another platinum.</p> <ul> <li>ET500-MG</li> <li>NJ520</li> </ul> </li> <li> <p>Be quiet has both     gold and platinum on the range of 550 and above. Gold and bronze on the 500-400W     range.</p> <ul> <li>Pure Power 11 CM 500W</li> <li>Pure Power 11 500W</li> <li>SFX L Power 500W</li> <li>Straigt Power 11 450W</li> </ul> </li> </ul> <p>It looks like I have to forget of efficiency above Gold unless I want to go with 520W. After a quick search on the provider I see that the most interesting in price are:</p> <ul> <li>Be Quiet! Straight Power 11 450W</li> <li>Be Quiet! Pure Power 11 CM 500W</li> <li>Be Quiet! Pure Power 11 500W</li> </ul> <p>I'm also going to trust Be Quiet on the CPU cooler so I'm happy with staying on the same brand for all fans.</p> Type 400W 450W 500W Min load 66% 59% 52% Max load 93% 83% 74% Model Pure Power 11 CM 500W Pure Power 11 500W Straight Power 11 450W Continuous Power 500 500 450 Peak Power 550 550 500 Min load (my case) 52% 52% 59% Max load (my case) 74% 74% 83% Topology Active Clamp / SR / DC/DC Active Clamp / SR / DC/DC LLC / SR / DC/DC Fan dB(A) at 20% 9 9.3 9.4 Fan dB(A) at 50% 9.3 9.6 9.8 Fan dB(A) at 100% 18.8 21.6 12.3 dB(A) Min load 9.68 - 10.25 dB(A) Max load 13.86 - 11.45 SIP Protection Yes No Yes Efficiency Cert Gold Gold Gold Efficiency 20% 90.6 88.2 91 Efficiency 50% 92.1 91.3 93.1 Efficiency 100% 90.1 89.9 91.7 Cable management Semi-modular Fixed Modular Cable cm to mb 55 55 60 Max cable length 95 95 115 No. of cables 7 7 8 ATX-MB (20+4-pin) 1 1 1 P4+4 (CPU) 1 1 1 PCI-e 6+2 (GPU) 2 2 2 SATA 6 6 8 Dimensions 160 x 150 x 86 150 x 150 x 86 160 x 150 x 86 Warranty (Years) 5 5 5 Price (EUR) 85.93 79.02 99.60 <p>I'd discard the Pure Power 11 500W because it:</p> <ul> <li>Has significantly worse efficiency</li> <li>Doesn't have SIP protection</li> <li>The fan is the loudest</li> </ul> <p>Between the other two Straight Power 11 has the advantages:</p> <ul> <li>1% more efficiency.</li> <li>Will make 0.5dB more noise at min load but 2.41dB less at max load. So I expect it to be more silent</li> <li>Cables look better</li> <li>Has more cable length</li> <li>Has more SATA cables (equal to my number of drives)</li> </ul> <p>And the disadvantages:</p> <ul> <li>Is 13.66 EUR more expensive</li> <li>Has 50W less of power</li> </ul> <p>It doesn't look like I'm going to need the extra power, and if I need it (if I add a graphic card) then the 500W wouldn't work either. And the difference in money is not that big. Therefore I'll go with the Straight Power 11 450W</p>"}, {"location": "psu/#references", "title": "References", "text": "<ul> <li>Linuxhint article on PSU</li> </ul>"}, {"location": "pydantic_factories/", "title": "Pydantic factories", "text": "<p>Pydantic factories is a library offers powerful mock data generation capabilities for pydantic based models and dataclasses. It automatically creates FactoryBoy factories from a pydantic model.</p>"}, {"location": "pydantic_factories/#example", "title": "Example", "text": "<pre><code>from datetime import date, datetime\nfrom typing import List, Union\n\nfrom pydantic import BaseModel, UUID4\n\nfrom pydantic_factories import ModelFactory\n\n\nclass Person(BaseModel):\n    id: UUID4\n    name: str\n    hobbies: List[str]\n    age: Union[float, int]\n    birthday: Union[datetime, date]\n\n\nclass PersonFactory(ModelFactory[Any]):\n    __model__ = Person\n\n\nresult = PersonFactory.build()\n</code></pre> <p>This is possible because of the typing information available on the pydantic model and model-fields, which are used as a source of truth for data generation.</p> <p>The factory parses the information stored in the pydantic model and generates a dictionary of kwargs that are passed to the <code>Person</code> class' init method.</p>"}, {"location": "pydantic_factories/#installation", "title": "Installation", "text": "<pre><code>pip install pydantic-factories\n</code></pre>"}, {"location": "pydantic_factories/#basic-usage", "title": "Basic Usage", "text": ""}, {"location": "pydantic_factories/#build-methods", "title": "Build Methods", "text": "<p>The <code>ModelFactory</code> class exposes two build methods:</p> <ul> <li><code>.build(**kwargs)</code>: builds a single instance of the factory's model.</li> <li><code>.batch(size: int, **kwargs)</code>: build a list of size <code>n</code> instances.</li> </ul> <pre><code>result = PersonFactory.build()  # a single Person instance\n\nresult = PersonFactory.batch(size=5)  # list[Person, Person, Person, Person, Person]\n</code></pre> <p>Any kwargs you pass to <code>.build</code>, <code>.batch</code> or any of the persistence methods, will take precedence over whatever defaults are defined on the factory class itself.</p>"}, {"location": "pydantic_factories/#nested-models-and-complex-types", "title": "Nested Models and Complex types", "text": "<p>The automatic generation of mock data works for all types supported by pydantic, as well as nested classes that derive from <code>BaseModel</code> (including for 3<sup>rd</sup> party libraries) and complex types.</p>"}, {"location": "pydantic_factories/#defining-factory-attributes", "title": "Defining Factory Attributes", "text": "<p>The factory api is designed to be as semantic and simple as possible, lets look at several examples that assume we have the following models:</p> <pre><code>from datetime import date, datetime\nfrom enum import Enum\nfrom pydantic import BaseModel, UUID4\nfrom typing import Any, Dict, List, Union\n\n\nclass Species(str, Enum):\n    CAT = \"Cat\"\n    DOG = \"Dog\"\n\n\nclass Pet(BaseModel):\n    name: str\n    species: Species\n\n\nclass Person(BaseModel):\n    id: UUID4\n    name: str\n    hobbies: List[str]\n    age: Union[float, int]\n    birthday: Union[datetime, date]\n    pets: List[Pet]\n    assets: List[Dict[str, Dict[str, Any]]]\n</code></pre> <p>One way of defining defaults is to use hardcoded values:</p> <pre><code>pet = Pet(name=\"Roxy\", sound=\"woof woof\", species=Species.DOG)\n\n\nclass PersonFactory(ModelFactory):\n    __model__ = Person\n\n    pets = [pet]\n</code></pre> <p>In this case when we call <code>PersonFactory.build()</code> the result will be randomly generated, except the pets list, which will be the hardcoded default we defined.</p>"}, {"location": "pydantic_factories/#use-field", "title": "Use field", "text": "<p>This though is often not desirable. We could instead, define a factory for <code>Pet</code> where we restrict the choices to a range we like. For example:</p> <pre><code>from enum import Enum\nfrom pydantic_factories import ModelFactory, Use\nfrom random import choice\n\nfrom .models import Pet, Person\n\n\nclass Species(str, Enum):\n    CAT = \"Cat\"\n    DOG = \"Dog\"\n\n\nclass PetFactory(ModelFactory):\n    __model__ = Pet\n\n    name = Use(choice, [\"Ralph\", \"Roxy\"])\n    species = Use(choice, list(Species))\n\n\nclass PersonFactory(ModelFactory):\n    __model__ = Person\n\n    pets = Use(PetFactory.batch, size=2)\n</code></pre> <p>The signature for use is: <code>cb: Callable, *args, **defaults</code>, it can receive any sync callable. In the above example, we used the choice function from the standard library's random package, and the batch method of <code>PetFactory</code>.</p> <p>You do not need to use the <code>Use</code> field, you can place callables (including classes) as values for a factory's attribute directly, and these will be invoked at build-time. Thus, you could for example re-write the above <code>PetFactory</code> like so:</p> <pre><code>class PetFactory(ModelFactory):\n    __model__ = Pet\n\n    name = lambda: choice([\"Ralph\", \"Roxy\"])\n    species = lambda: choice(list(Species))\n</code></pre> <p><code>Use</code> is merely a semantic abstraction that makes the factory cleaner and simpler to understand.</p>"}, {"location": "pydantic_factories/#ignore-field", "title": "Ignore (field)", "text": "<p><code>Ignore</code> is another field exported by this library, and its used - as its name implies - to designate a given attribute as ignored:</p> <pre><code>from typing import TypeVar\n\nfrom odmantic import EmbeddedModel, Model\nfrom pydantic_factories import ModelFactory, Ignore\n\nT = TypeVar(\"T\", Model, EmbeddedModel)\n\n\nclass OdmanticModelFactory(ModelFactory[T]):\n    id = Ignore()\n</code></pre> <p>The above example is basically the extension included in pydantic-factories for the library ODMantic, which is a pydantic based mongo ODM.</p> <p>For ODMantic models, the id attribute should not be set by the factory, but rather handled by the odmantic logic itself. Thus, the id field is marked as ignored.</p> <p>When you ignore an attribute using <code>Ignore</code>, it will be completely ignored by the factory - that is, it will not be set as a kwarg passed to pydantic at all.</p>"}, {"location": "pydantic_factories/#require-field", "title": "Require (field)", "text": "<p>The <code>Require</code> field in turn specifies that a particular attribute is a required kwarg. That is, if a kwarg with a value for this particular attribute is not passed when calling <code>factory.build()</code>, a <code>MissingBuildKwargError</code> will be raised.</p> <p>What is the use case for this? For example, lets say we have a document called <code>Article</code> which we store in some DB and is represented using a non-pydantic model. We then need to store in our pydantic object a reference to an id for this article. This value should not be some mock value, but must rather be an actual id passed to the factory. Thus, we can define this attribute as required:</p> <pre><code>from pydantic import BaseModel\nfrom pydantic_factories import ModelFactory, Require\nfrom uuid import UUID\n\n\nclass ArticleProxy(BaseModel):\n    article_id: UUID\n    ...\n\n\nclass ArticleProxyFactory(ModelFactory):\n    __model__ = ArticleProxy\n\n    article_id = Require()\n</code></pre> <p>If we call <code>factory.build()</code> without passing a value for <code>article_id</code>, an error will be raised.</p>"}, {"location": "pydantic_factories/#creating-your-custom", "title": "[Creating your custom", "text": "<p>factories](https://starlite-api.github.io/pydantic-factories/usage/7-handling-custom-types/?h=custom)</p> <p>If your model has an attribute that is not supported by <code>pydantic-factories</code> and it depends on third party libraries, you can create your custom extension subclassing the <code>ModelFactory</code>, and overriding the <code>get_mock_value</code> method to add your logic.</p> <pre><code>from pydantic_factories import ModelFactory\n\nclass CustomFactory(ModelFactory[Any]):\n    \"\"\"Tweak the ModelFactory to add our custom mocks.\"\"\"\n\n    @classmethod\n    def get_mock_value(cls, field_type: Any) -&gt; Any:\n        \"\"\"Add our custom mock value.\"\"\"\n        if str(field_type) == \"my_super_rare_datetime_field\":\n            return cls._get_faker().date_time_between()\n\n        return super().get_mock_value(field_type)\n</code></pre> <p>Where <code>cls._get_faker()</code> is a <code>faker</code> instance that you can use to build your returned value.</p>"}, {"location": "pydantic_factories/#troubleshooting", "title": "Troubleshooting", "text": ""}, {"location": "pydantic_factories/#use-pydantic-factories-with-pytest-freezegun", "title": "Use pydantic-factories with pytest-freezegun", "text": "<p>As <code>pytest-freezegun</code> overrides the <code>datetime.datetime</code> class, <code>pydantic-factories</code> is not able to find the correct generator for the datetime fields. It's solved by creating a custom factory</p> <pre><code>from typing import Any\nfrom pydantic_factories import ModelFactory\n\n\nclass CustomFactory(ModelFactory[Any]):\n\"\"\"Tweak the ModelFactory to add our custom mocks.\"\"\"\n\n    @classmethod\n    def get_mock_value(cls, field_type: Any) -&gt; Any:\n\"\"\"Add our custom mock value.\"\"\"\n        if str(field_type) == \"&lt;class 'datetime.datetime'&gt;\":\n            return cls._get_faker().date_time_between()\n\n        return super().get_mock_value(field_type)\n</code></pre>"}, {"location": "pydantic_factories/#references", "title": "References", "text": "<ul> <li>Git</li> <li>Docs</li> </ul>"}, {"location": "pyment/", "title": "Pyment", "text": "<p>Pyment is a python3 program to automatically create, update or convert docstrings in existing Python files, managing several styles.</p>"}, {"location": "pyment/#installation", "title": "Installation", "text": "<pre><code>pip install pyment\n</code></pre>"}, {"location": "pyment/#usage", "title": "Usage", "text": "<pre><code>$: pyment  myfile.py    # will generate a patch\n$: pyment -w myfile.py  # will overwrite the file\n</code></pre> <p>As of 2021-11-17, the program is not production ready yet for me, I've tested it in one of my projects and found some bugs that needed to be fixed before it's usable. Despite the number of stars, it looks like the development pace has dropped dramatically, so it needs our help to get better :).</p>"}, {"location": "pyment/#references", "title": "References", "text": "<ul> <li>Git</li> </ul>"}, {"location": "pytest_httpserver/", "title": "Pytest httpserver", "text": "<p>pytest-httpserver is a python package which allows you to start a real HTTP server for your tests. The server can be configured programmatically to how to respond to requests.</p>"}, {"location": "pytest_httpserver/#installation", "title": "Installation", "text": "<pre><code>pip install pytest-httpserver\n</code></pre>"}, {"location": "pytest_httpserver/#usage", "title": "Usage", "text": "<pre><code>import requests\n\ndef test_json_client(httpserver: HTTPServer):\n    httpserver.expect_request(\"/foobar\").respond_with_json({\"foo\": \"bar\"})\n    assert requests.get(httpserver.url_for(\"/foobar\")).json() == {'foo': 'bar'}\n</code></pre>"}, {"location": "pytest_httpserver/#specifying-responses", "title": "Specifying responses", "text": "<p>Once you have set up the expected request, it is required to set up the response which will be returned to the client.</p> <p>In the example we used <code>respond_with_json()</code> but it is also possible to respond with an arbitrary content.</p> <pre><code>respond_with_data(\"Hello world!\", content_type=\"text/plain\")\n</code></pre> <p>In the example above, we are responding a <code>text/plain</code> content. You can specify the status also:</p> <pre><code>respond_with_data(\"Not found\", status=404, content_type=\"text/plain\")\n</code></pre>"}, {"location": "pytest_httpserver/#give-a-dynamic-response", "title": "Give a dynamic response", "text": "<p>If you need to produce dynamic content, use the <code>respond_with_handler</code> method, which accepts a callable (eg. a python function):</p> <pre><code>from werkzeug.wrappers import Response\nfrom werkzeug.wrappers.request import Request\n\ndef my_handler(request: Request) -&gt; Response:\n    # here, examine the request object\n    return Response(\"Hello world!\")\n\nrespond_with_handler(my_handler)\n</code></pre>"}, {"location": "pytest_httpserver/#references", "title": "References", "text": "<ul> <li>Docs</li> </ul>"}, {"location": "python/", "title": "Python", "text": "<p>Python is an interpreted, high-level and general-purpose programming language. Python's design philosophy emphasizes code readability with its notable use of significant indentation. Its language constructs and object-oriented approach aim to help programmers write clear, logical code for small and large-scale projects.</p>"}, {"location": "python/#install", "title": "Install", "text": "<pre><code>apt-get install python\n</code></pre>"}, {"location": "python/#install-a-specific-version", "title": "Install a specific version", "text": "<ul> <li> <p>Install dependencies     <pre><code>sudo apt install wget software-properties-common build-essential libnss3-dev zlib1g-dev libgdbm-dev libncurses5-dev libssl-dev libffi-dev libreadline-dev libsqlite3-dev libbz2-dev\n</code></pre></p> </li> <li> <p>Select the version in https://www.python.org/ftp/python/ and download it     <pre><code>wget https://www.python.org/ftp/python/3.9.2/Python-3.9.2.tgz\ncd Python-3.9.2/\n./configure --enable-optimizations\nsudo make altinstall\n</code></pre></p> </li> </ul>"}, {"location": "python/#generators", "title": "Generators", "text": "<p>Generator functions are a special kind of function that return a lazy iterator. These are objects that you can loop over like a list. However, unlike lists, lazy iterators do not store their contents in memory.</p> <p>An example would be an infinite sequence generator</p> <pre><code>def infinite_sequence():\n    num = 0\n    while True:\n        yield num\n        num += 1\n</code></pre> <p>You can use it as a list:</p> <pre><code>for i in infinite_sequence():\n...     print(i, end=\" \")\n...\n0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29\n30 31 32 33 34 35 36 37 38 39 40 41 42\n[...]\n</code></pre> <p>Instead of using a <code>for</code> loop, you can also call <code>next()</code> on the generator object directly. This is especially useful for testing a generator in the console:.</p> <pre><code>&gt;&gt;&gt; gen = infinite_sequence()\n&gt;&gt;&gt; next(gen)\n0\n&gt;&gt;&gt; next(gen)\n1\n&gt;&gt;&gt; next(gen)\n2\n&gt;&gt;&gt; next(gen)\n3\n</code></pre>"}, {"location": "python/#understanding-generators", "title": "Understanding Generators", "text": "<p>Generator functions look and act just like regular functions, but with one defining characteristic. Generator functions use the Python <code>yield</code> keyword instead of <code>return</code>.</p> <p><code>yield</code> indicates where a value is sent back to the caller, but unlike <code>return</code>, you don\u2019t exit the function afterward.Instead, the state of the function is remembered. That way, when <code>next()</code> is called on a generator object (either explicitly or implicitly within a for loop), the previously yielded variable <code>num</code> is incremented, and then yielded again.</p>"}, {"location": "python/#interesting-libraries-to-explore", "title": "Interesting libraries to explore", "text": "<ul> <li>di: a modern dependency injection     system, modeled around the simplicity of FastAPI's dependency injection.</li> <li>humanize: This modest package     contains various common humanization utilities, like turning a number into     a fuzzy human-readable duration (\"3 minutes ago\") or into a human-readable     size or throughput.</li> <li>tryceratops: A linter of     exceptions.</li> <li>schedule: Python job scheduling for     humans. Run Python functions (or any other callable) periodically using     a friendly syntax.</li> <li>huey: a little task queue for python.</li> <li>textual: Textual is a TUI (Text User     Interface) framework for Python using Rich as a renderer.</li> <li>parso: Parses Python code.</li> <li> <p>kivi: Create android/Linux/iOS/Windows applications with     python. Use it with kivimd to make it beautiful,     check the examples and the     docs.</p> <p>For beginner tutorials check the real python's and towards data science (and part 2). * apprise: Allows you to send a notification to almost all of the most popular notification services available to us today such as: Linux, Telegram, Discord, Slack, Amazon SNS, Gotify, etc. Look at all the supported notifications <code>(\u00ac\u00ba-\u00b0)\u00ac</code>. * aiomultiprocess: Presents a simple interface, while running a full AsyncIO event loop on each child process, enabling levels of concurrency never before seen in a Python application. Each child process can execute multiple coroutines at once, limited only by the workload and number of cores available. * twint: An advanced Twitter scraping &amp; OSINT tool written in Python that doesn't use Twitter's API, allowing you to scrape a user's followers, following, Tweets and more while evading most API limitations. Maybe use <code>snscrape</code> (is below) if <code>twint</code> doesn't work. * snscrape: A social networking service scraper in Python. * tweepy: Twitter for Python.</p> </li> </ul>"}, {"location": "python/#interesting-sources", "title": "Interesting sources", "text": "<ul> <li>Musa 550 looks like a nice way to     learn how to process geolocation data.</li> </ul>"}, {"location": "python_elasticsearch/", "title": "Python elasticsearch", "text": "<p>Python elasticsearch is the Official low-level client for Elasticsearch. Its goal is to provide common ground for all Elasticsearch-related code in Python; because of this it tries to be opinion-free and very extendable.</p>"}, {"location": "python_elasticsearch/#installation", "title": "Installation", "text": "<pre><code>pip install elasticsearch\n</code></pre>"}, {"location": "python_elasticsearch/#usage", "title": "Usage", "text": ""}, {"location": "python_elasticsearch/#connect-to-the-database", "title": "Connect to the database", "text": "<pre><code>from elasticsearch import Elasticsearch\n\nclient = Elasticsearch(\"http://localhost:9200\")\n</code></pre>"}, {"location": "python_elasticsearch/#get-all-indices", "title": "Get all indices", "text": "<pre><code>client.indices.get(index=\"*\")\n</code></pre>"}, {"location": "python_elasticsearch/#get-all-documents", "title": "Get all documents", "text": "<pre><code>resp = client.search(index=\"test-index\", query={\"match_all\": {}})\ndocuments = resp.body[\"hits\"][\"hits\"]\n</code></pre> <p>Use <code>pprint</code> to analyze the content of each <code>document</code>.</p>"}, {"location": "python_elasticsearch/#update-a-document", "title": "Update a document", "text": "<pre><code>doc = {\"partial_document\": \"value\"}\nresp = client.update(index=INDEX, id=id_, doc=doc)\n</code></pre>"}, {"location": "python_elasticsearch/#references", "title": "References", "text": "<ul> <li>Docs</li> <li>Source</li> <li>TowardsDataScience article</li> </ul>"}, {"location": "python_gnupg/", "title": "python-gnupg", "text": "<p>python-gnupg is a Python library to interact with <code>gpg</code> taking care of the internal details and allows its users to generate and manage keys, encrypt and decrypt data, and sign and verify messages.</p>"}, {"location": "python_gnupg/#installation", "title": "Installation", "text": "<pre><code>pip install python-gnupg\n</code></pre>"}, {"location": "python_gnupg/#usage", "title": "Usage", "text": "<p>You interface to the GnuPG functionality through an instance of the GPG class:</p> <pre><code>gpg = gnupg.GPG(gnupghome=\"/path/to/home/directory\")\n</code></pre> <p>Note: I've already created an adapter for gpg called <code>KeyStore</code> available in <code>pass-collaborate</code></p> <ul> <li>Decrypt a file:</li> </ul> <pre><code>gpg.decrypt_file(\"path/to/file\")\n</code></pre> <p>Note: You can't pass <code>Path</code> arguments to <code>decrypt_file</code>.</p> <ul> <li>Encrypt a file:</li> </ul> <pre><code>gpg.encrypt_file('path/to/file', recipients)\n</code></pre> <p>Where <code>recipients</code> is a <code>List[str]</code> of gpg Key IDs.</p> <ul> <li>List private keys:</li> </ul> <pre><code>&gt;&gt;&gt; public_keys = gpg.list_keys()\n&gt;&gt;&gt; private_keys = gpg.list_keys(True)\n</code></pre> <ul> <li>List the recipients that can decrypt a file</li> </ul> <pre><code>def list_recipients(self, path: Path) -&gt; List['GPGKey']:\n\"\"\"List the keys that can decrypt a file.\n\n    Args:\n        path: Path to the file to check.\n    \"\"\"\n    keys = []\n    for short_key in self.gpg.get_recipients_file(str(path)):\n        keys.append(self.gpg.list_keys(keys=[short_key])[0]['fingerprint'])\n\n    return keys\n</code></pre> <ul> <li>Receive keys from a keyserver</li> </ul> <pre><code>import_result = gpg.recv_keys('server-name', 'keyid1', 'keyid2', ...)\n</code></pre>"}, {"location": "python_gnupg/#references", "title": "References", "text": "<ul> <li>Docs</li> <li>Source</li> <li>Issues</li> </ul>"}, {"location": "python_internationalization/", "title": "Python Internationalization", "text": "<p>To make your code accessible to more people, you may want to support more than one language. It's not as easy as it looks as it's not enough to translate it but also it must look and feel local. The answer is internationalization.</p> <p>Internationalization (numeronymed as i18n) can be defined as the design process that ensures a program can be adapted to various languages and regions without requiring engineering changes to the source code.</p> <p>Common internationalization tasks include:</p> <ul> <li>Facilitating compliance with Unicode.</li> <li>Minimizing the use of concatenated strings.</li> <li>Accommodating support for double-byte languages (e.g. Japanese) and     right-to-left languages (for example, Hebrew).</li> <li>Avoiding hard-coded text.</li> <li>Designing for independence from cultural conventions (e. g., date and time     displays), limiting language, and character sets.</li> </ul> <p>Localization (l10n) refers to the adaptation of your program, once internationalized, to the local language and cultural habits. In theory it looks simple to implement. In practice though, it takes time and effort to provide the best Internationalization and Localization experience for your global audience.</p> <p>In Python, there is a specific bundled module for that and it\u2019s called gettext, which consists of a public API and a set of tools that help extract and generate message catalogs from the source code.</p>"}, {"location": "python_internationalization/#references", "title": "References", "text": "<ul> <li>Phrase blog on Localizing with GNU gettext</li> <li>Phrase blog on internationalization</li> </ul>"}, {"location": "python_jinja2/", "title": "Jinja2", "text": "<p>Jinja2 is a modern and designer-friendly templating language for Python, modelled after Django\u2019s templates. It is fast, widely used and secure with the optional sandboxed template execution environment:</p> <pre><code>&lt;title&gt;{% block title %}{% endblock %}&lt;/title&gt;\n&lt;ul&gt;\n{% for user in users %}\n  &lt;li&gt;&lt;a href=\"{{ user.url }}\"&gt;{{ user.username }}&lt;/a&gt;&lt;/li&gt;\n{% endfor %}\n&lt;/ul&gt;\n</code></pre> <p>Features:</p> <ul> <li>Sandboxed execution.</li> <li>Powerful automatic HTML escaping system for XSS prevention.</li> <li>Template inheritance.</li> <li>Compiles down to the optimal python code just in time.</li> <li>Optional ahead-of-time template compilation.</li> <li>Easy to debug. Line numbers of exceptions directly point to the correct line     in the template.</li> <li>Configurable syntax.</li> </ul>"}, {"location": "python_jinja2/#installation", "title": "Installation", "text": "<pre><code>pip install Jinja2\n</code></pre>"}, {"location": "python_jinja2/#usage", "title": "Usage", "text": "<p>The most basic way to create a template and render it is through <code>Template</code>. This however is not the recommended way to work with it if your templates are not loaded from strings but the file system or another data source:</p> <p><pre><code>&gt;&gt;&gt; from jinja2 import Template\n&gt;&gt;&gt; template = Template('Hello {{ name }}!')\n&gt;&gt;&gt; template.render(name='John Doe')\nu'Hello John Doe!'\n</code></pre> Jinja uses a central object called the template <code>Environment</code>. Instances of this class are used to store the configuration and global objects, and are used to load templates from the file system or other locations.</p> <p>The simplest way to configure Jinja to load templates for your application looks roughly like this:</p> <pre><code>from jinja2 import Environment, PackageLoader, select_autoescape\nenv = Environment(\n    loader=PackageLoader('yourapplication', 'templates'),\n    autoescape=select_autoescape(['html', 'xml'])\n)\n</code></pre> <p>This will create a template environment with the default settings and a loader that looks up the templates in the templates folder inside the <code>yourapplication</code> python package. Different loaders are available and you can also write your own if you want to load templates from a database or other resources. This also enables autoescaping for HTML and XML files.</p> <p>To load a template from this environment you just have to call the get_template() method which then returns the loaded Template:</p> <pre><code>template = env.get_template('mytemplate.html')\n</code></pre> <p>To render it with some variables, just call the <code>render()</code> method:</p> <pre><code>print(template.render(the='variables', go='here'))\n</code></pre>"}, {"location": "python_jinja2/#template-guidelines", "title": "Template guidelines", "text": ""}, {"location": "python_jinja2/#variables", "title": "Variables", "text": "<p>Reference variables using <code>{{ braces }}</code> notation.</p>"}, {"location": "python_jinja2/#iterationloops", "title": "Iteration/Loops", "text": "<p>One in each line:</p> <pre><code>{% for host in groups['tag_Function_logdb'] %}\nelasticsearch_discovery_zen_ping_unicast_hosts = {{ host }}:9300\n{% endfor %}\n</code></pre> <p>Inline:</p> <pre><code>ALLOWED_HOSTS = [{% for domain in domains %}\" {{ domain }}\",{% endfor %}]\n</code></pre>"}, {"location": "python_jinja2/#get-the-counter-of-the-iteration", "title": "Get the counter of the iteration", "text": "<pre><code>&gt;&gt;&gt; from jinja2 import Template\n\n&gt;&gt;&gt; s = \"{% for element in elements %}{{loop.index}} {% endfor %}\"\n&gt;&gt;&gt; Template(s).render(elements=[\"a\", \"b\", \"c\", \"d\"])\n1 2 3 4\n</code></pre>"}, {"location": "python_jinja2/#lookup", "title": "Lookup", "text": ""}, {"location": "python_jinja2/#get-environmental-variable", "title": "Get environmental variable", "text": "<pre><code>lookup('env','HOME')\n</code></pre>"}, {"location": "python_jinja2/#join", "title": "Join", "text": "<pre><code>lineinfile: dest=/etc/hosts line=\"{{ item.ip }} {{ item.aliases|join(' ') }}\"\n</code></pre>"}, {"location": "python_jinja2/#map", "title": "Map", "text": "<p>Get elements of a dictionary</p> <pre><code>  set_fact: asg_instances=\"{{ instances.results | map(attribute='instances') | map('first') | map(attribute='public_ip_address') | list}}\"\n</code></pre>"}, {"location": "python_jinja2/#default", "title": "Default", "text": "<p>Set default value of variable</p> <pre><code>name: 'lol'\nnew_name: \"{{ name | default('trol') }}\"\n</code></pre>"}, {"location": "python_jinja2/#rejectattr", "title": "Rejectattr", "text": "<p>Exclude elements from a list.</p> <p>Filters a sequence of objects by applying a test to the specified attribute of each object, and rejecting the objects with the test succeeding.</p> <p>If no test is specified, the attribute's value will be evaluated as a boolean.</p> <pre><code>{{ users|rejectattr(\"is_active\") }}\n{{ users|rejectattr(\"email\", \"none\") }}\n</code></pre>"}, {"location": "python_jinja2/#regex", "title": "Regex", "text": "<pre><code>{{ 'ansible' | regex_replace('^a.*i(.*)$', 'a\\\\1') }}\n{{ 'foobar' | regex_replace('^f.*o(.*)$', '\\\\1') }}\n{{ 'localhost:80' | regex_replace('^(?P&lt;host&gt;.+):(?P&lt;port&gt;\\\\d+)$', '\\\\g&lt;host&gt;, \\\\g&lt;port&gt;') }}\n</code></pre>"}, {"location": "python_jinja2/#slice-string", "title": "Slice string", "text": "<pre><code>{{ variable_name[:-8] }}\n</code></pre>"}, {"location": "python_jinja2/#conditional", "title": "Conditional", "text": ""}, {"location": "python_jinja2/#conditional-variable-definition", "title": "Conditional variable definition", "text": "<pre><code>{{ 'Update' if files else 'Continue' }}\n</code></pre>"}, {"location": "python_jinja2/#check-if-variable-is-defined", "title": "Check if variable is defined", "text": "<pre><code>{% if variable is defined %}\nVariable: {{ variable }} defined\n{% endif %}\n</code></pre>"}, {"location": "python_jinja2/#with-two-statements", "title": "With two statements:", "text": "<pre><code>{% if (backend_environment == 'backend' and environment == 'Dev'): %}\n\n{% elif ... %}\n{% else %}\n{% endif %}\n</code></pre>"}, {"location": "python_jinja2/#extract-extension-from-file", "title": "Extract extension from file", "text": "<pre><code>s3_object        : code/frontal/vodafone-v8.18.81.zip\nremote_clone_dir : \"{{deploy_dir}}/{{ s3_object | basename | splitext | first}}\"\n</code></pre>"}, {"location": "python_jinja2/#comments", "title": "Comments", "text": "<p><code>{# comment here #}</code></p>"}, {"location": "python_jinja2/#inheritance", "title": "Inheritance", "text": "<p>For simple inclusions use <code>include</code> for more complex <code>extend</code>.</p>"}, {"location": "python_jinja2/#include", "title": "Include", "text": "<p>To include a snippet from another file you can use <pre><code>{% include '_post.html' %}\n</code></pre></p>"}, {"location": "python_jinja2/#extend", "title": "Extend", "text": "<p>To inherit from another document you can use the <code>block</code> control statement. Blocks are given a unique name, which derived templates can reference when they provide their content</p> <p>.base.html <pre><code>&lt;html&gt;\n    &lt;head&gt;\n      {% if title %}\n      &lt;title&gt;{{ title }} - Microblog &lt;/title&gt;\n      {% else %}\n      &lt;title&gt;Welcome to Microblog&lt;/title&gt;\n      {% endif %}\n    &lt;/head&gt;\n    &lt;body&gt;\n        &lt;div&gt;\n          Microblog: &lt;a href=\"/index\"&gt;Home&lt;/a&gt;\n        &lt;/div&gt;\n        &lt;hr&gt;\n        {% block content %}{% endblock %}\n    &lt;/body&gt;\n&lt;/html&gt;\n</code></pre> .index.html <pre><code>{% extends \"base.html\" %}\n\n{% block content %}\n  &lt;h1&gt;Hi&lt;/h1&gt;\n{% endblock %}\n</code></pre></p>"}, {"location": "python_jinja2/#execute-a-function-and-return-the-value-to-a-variable", "title": "Execute a function and return the value to a variable", "text": "<pre><code>{% with messages = get_flashed_messages() %}\n{% if messages %}\n&lt;ul&gt;\n  {% for message in messages %}\n  &lt;li&gt;{{ message }}&lt;/li&gt;\n  {% endfor %}\n&lt;/ul&gt;\n{% endif %}\n{% endwith %}\n</code></pre>"}, {"location": "python_jinja2/#macros", "title": "Macros", "text": "<p>Macros are comparable with functions in regular programming languages. They are useful to put often used idioms into reusable functions to not repeat yourself (\u201cDRY\u201d).</p> <pre><code>{% macro input(name, value='', type='text', size=20) -%}\n    &lt;input type=\"{{ type }}\" name=\"{{ name }}\" value=\"{{\n        value|e }}\" size=\"{{ size }}\"&gt;\n{%- endmacro %}\n</code></pre> <p>The macro can then be called like a function in the namespace:</p> <pre><code>&lt;p&gt;{{ input('username') }}&lt;/p&gt;\n&lt;p&gt;{{ input('password', type='password') }}&lt;/p&gt;\n</code></pre>"}, {"location": "python_jinja2/#wrap-long-lines-and-indent", "title": "Wrap long lines and indent", "text": "<p>You can prepend the given string with a newline character, then use the <code>wordwrap</code> filter to wrap the text into multiple lines first, and use the replace filter to replace newline characters with newline plus '    ':</p> <pre><code>{{ ('\\n' ~ item.comment) | wordwrap(76) | replace('\\n', '\\n    ') }}\n</code></pre> <p>The above assumes you want each line to be no more than 80 characters. Change 76 to your desired line width minus 4 to leave room for the indentation.</p>"}, {"location": "python_jinja2/#test-if-variable-is-none", "title": "Test if variable is None", "text": "<p>Use the <code>none</code> test (not to be confused with Python's <code>None</code> object!):</p> <pre><code>{% if p is not none %}\n    {{ p.User['first_name'] }}\n{% else %}\n    NONE\n{% endif %}\n</code></pre>"}, {"location": "python_jinja2/#snippets", "title": "Snippets", "text": ""}, {"location": "python_jinja2/#escape-jinja-expansion-on-a-jinja-template", "title": "Escape jinja expansion on a jinja template", "text": "<pre><code>{% raw %}\n\nAnything in this block is treated as raw text,\nincluding {{ curly braces }} and\n{% other block-like syntax %}\n\n{% endraw %}\n</code></pre>"}, {"location": "python_jinja2/#references", "title": "References", "text": "<ul> <li>Docs</li> </ul>"}, {"location": "python_logging/", "title": "Python Logging", "text": "<p>Logging information in your Python programs makes it possible to debug problems when running.</p> <p>For command line application that the user is going to run directly, the <code>logging</code> module might be enough. For command line tools or APIs that are going to be run by a server, it might fall short. <code>logging</code> will write exceptions and breadcrumbs to a file, and unless you look at it directly most errors will pass unnoticed.</p> <p>To actively monitor and react to code exceptions use an application monitoring platform like sentry. They gather de data on your application and aggregate the errors in a user friendly way, such as:</p> <ul> <li>Showing the context of the error in a web interface.</li> <li>Gather both front and backend issues in one place.</li> <li>Show the trail of events that led to the exceptions with breadcrumbs.</li> <li>Show the probable commit that introduced the bug.</li> <li>Link problems with issue tracker issues.</li> <li>See the impact of each bug with the number of occurrences and users that are     experiencing it.</li> <li>Visualize all the data in dashboards.</li> <li>Get notifications on the issues raised.</li> </ul> <p>Check the demo to see its features.</p> <p>You can self-host sentry, but it uses a docker-compose that depends on 12 services, including postgres, redis and kafka with a minimum requirements of 4 cores and 8 GB of RAM.</p> <p>So I've looked for a simple solution, and arrived to GlitchTip, a similar solution that even uses the sentry SDK, but has a smaller system footprint, and it's open sourced, while sentry is not anymore. Check it's documentation and source code.</p>"}, {"location": "python_mysql/", "title": "MySQL Python", "text": ""}, {"location": "python_mysql/#installation", "title": "Installation", "text": "<pre><code>pip install mysql-connector-python\n</code></pre>"}, {"location": "python_mysql/#usage", "title": "Usage", "text": "<pre><code>import mysql.connector\n\n# Connect to server\ncnx = mysql.connector.connect(\n    host=\"127.0.0.1\",\n    port=3306,\n    user=\"mike\",\n    password=\"s3cre3t!\")\n\n# Get a cursor\ncur = cnx.cursor()\n\n# Execute a query\ncur.execute(\"SELECT CURDATE()\")\n\n# Fetch one result\nrow = cur.fetchone()\nprint(\"Current date is: {0}\".format(row[0]))\n\n# Close connection\ncnx.close()\n</code></pre>"}, {"location": "python_mysql/#iterate-over-the-results-of-the-cursor-execution", "title": "Iterate over the results of the cursor execution", "text": "<pre><code>cursor.execute(show_db_query)\nfor db in cursor:\n    print(db)\n</code></pre>"}, {"location": "python_mysql/#references", "title": "References", "text": "<ul> <li>Git</li> <li>Docs</li> <li>RealPython tutorial</li> </ul>"}, {"location": "python_optimization/", "title": "Python Optimization", "text": "<p>Optimization can be done through different metrics, such as, CPU performance (execution time) or memory footprint.</p> <p>Optimizing your code makes sense when you are sure that the business logic in the code is correct and not going to change soon.</p> <p>\"First make it work. Then make it right. Then make it fast.\" ~ Kent Beck</p> <p>Unless you're developing a performance-intensive product or a code dependency that is going to be used by other projects which might be performance-intensive, optimizing every aspect of the code can be overkill. For most of the scenarios, the 80-20 principle (80 percent of performance benefits may come from optimizing 20 percent of your code) will be more appropriate.</p> <p>Most of the time we make intuitive guesses on what the bottlenecks are, but more often than not, our guesses are either wrong or just approximately correct. So, it's always advisable to use profiling tools to identify how often a resource is used and who is using the resource. For instance, a profiler designed for profiling execution time will measure how often and for how various long parts of the code are executed. Using a profiling mechanism becomes a necessity when the codebase grows large, and you still want to maintain efficiency.</p>"}, {"location": "python_optimization/#making-python-command-line-fast", "title": "Making Python command line fast", "text": "<p>People like using software that feels fast, and Python programs tend to be slow to start running. What qualifies as fast is subjective, and varies by the type of tool and by the user's expectations.</p> <p>Roughly speaking, for a command line program, people expect results almost instantaneously. For a tool that appears to be doing a simple task a sub-second result is enough, but under 200ms is even better.</p> <p>Obviously to achieve this, your program actually has to be fast at doing its work. But what if you've written your code in Python, and it can take 800ms just to import your code, let alone start running it.</p>"}, {"location": "python_optimization/#how-fast-can-a-python-program-be", "title": "How fast can a Python program be?", "text": "<p>TBC with the next sources</p> <ul> <li>https://files.bemusement.org/talks/OSDC2008-FastPython/</li> <li>https://files.bemusement.org/talks/OSDC2008-FastPython/</li> <li>https://stackoverflow.com/questions/4177735/best-practice-for-lazy-loading-python-modules</li> <li>https://snarky.ca/lazy-importing-in-python-3-7/</li> <li>https://levelup.gitconnected.com/python-trick-lazy-module-loading-df9b9dc111af</li> </ul>"}, {"location": "python_optimization/#minimize-the-relative-import-statements-on-command-line-tools", "title": "Minimize the relative import statements on command line tools", "text": "<p>When developing a library, it's common to expose the main objects into the package <code>__init__.py</code> under the variable <code>__all__</code>. The problem with command line programs is that each time you run the command it will load those objects, which can mean an increase of 0.5s or even a second for each command, which is unacceptable.</p> <p>Following this string, if you manage to minimize the relative imports, you'll make your code faster.</p> <p>Python's wiki discusses different places to locate your import statements. If you put them on the top, the imports that you don't need for that command in particular will worsen your load time, if you add them inside the functions, if you run the function more than once, the performance drops too, and it's a common etiquete to have all your imports on the top.</p> <p>One step that you can do is to mark the imports required for type checking under a conditional:</p> <pre><code>from typing import TYPE_CHECKING\n\nif TYPE_CHECKING:\n   from model import Object\n</code></pre> <p>This change can be negligible, and it will force you to use <code>'Object'</code>, instead of <code>Object</code> in the typing information, which is not nice, so it may not be worth it.</p> <p>If you are still unable to make the loading time drop below an acceptable time, you can migrate to a server-client architecture, where all the logic is loaded by the backend (once as it's always running), and have a \"silly\" client that only does requests to the backend. Beware though, as you will add the network latency.</p>"}, {"location": "python_optimization/#dont-dynamically-install-the-package", "title": "Don't dynamically install the package", "text": "<p>If you install the package with <code>pip install -e .</code> you will see an increase on the load time of ~0.2s. It is useful to develop the package, but when you use it, do so from a virtualenv that installs it directly without the <code>-e</code> flag.</p>"}, {"location": "python_optimization/#references", "title": "References", "text": "<ul> <li>Satwik Kansal article on Scout APM</li> </ul>"}, {"location": "python_package_management/", "title": "Python Package Management", "text": "<p>Managing Python libraries is a nightmare for most developers, it has driven me crazy trying to keep all the requirements of the projects I maintain updated.</p> <p>I tried with pip-tools, but I was probably using it wrong. As package management has evolved a lot in the latest years, I'm going to compare Poetry, pipenv, <code>pdm</code> with my current workflow.</p> Tool Stars Forks Latest commit Commits Issues Open/New/Closed PR Open/New/Merged Poetry 17.3k 1.4k 11h 1992 1.1k/58/80 149/13/77 Pipenv 22.5k 1.7k 5d 7226 555/12/54 32/0/22 pdm 1.3k 54 11h 1539 12/3/43 3/2/11 <p>The <code>New</code> and <code>Closed</code> are taken from the Pulse insights of the last month. This data was taken on the 2021-11-30 so it will probably be outdated.</p> <p>Both Poetry and Pipenv are very popular, it looks that <code>Poetry</code> is more alive this last month, but they are both actively developed. <code>pdm</code> is actively developed but at other level.</p> <p>Pipenv has broad support. It is an official project of the Python Packaging Authority, alongside pip. It's also supported by the Heroku Python buildpack, which is useful for anyone with Heroku or Dokku-based deployment strategies.</p> <p>Poetry is a one-stop shop for dependency management and package management. It simplifies creating a package, managing its dependencies, and publishing it. Compared to Pipenv, Poetry's separate add and install commands are more explicit, and it's faster for everything except for a full dependency install.</p>"}, {"location": "python_package_management/#solver", "title": "Solver", "text": "<p>A Solver tries to find a working set of dependencies that all agree with each other. By looking back in time, it\u2019s happy to solve very old versions of packages if newer ones are supposed to be incompatible. This can be helpful, but is slow, and also means you can easily get a very ancient set of packages when you thought you were getting the latest versions.</p> <p>Pip\u2019s solver changed in version 20.3 to become significantly smarter. The old solver would ignore incompatible transitive requirements much more often than the new solver does. This means that an upper cap in a library might have been ignored before, but is much more likely to break things or change the solve now.</p> <p>Poetry has a unique and very strict (and slower) solver that goes even farther hunting for solutions. It forces you to cap Python if a dependency does. One key difference is that Poetry has the original environment specification to work with every time, while pip does not know what the original environment constraints were. This enables Poetry to roll back a dependency on a subsequent solve, while pip does not know what the original requirements were and so does not know if an older package is valid when it encounters a new cap.</p>"}, {"location": "python_package_management/#poetry", "title": "Poetry", "text": "<p>Features I like:</p> <ul> <li>Stores program and development requirements in the <code>pyproject.toml</code>     file.</li> <li>Don't need to manually edit requirements files to add new packages to the     program or dev requirements, simply use <code>poetry add</code>.</li> <li>Easy initialization of the development environment with <code>poetry install</code>.</li> <li> <p>Powerful dependency specification</p> <ul> <li>Installable packages with git dependencies???</li> <li>Easy to specify local directory dependencies, even in editable mode.</li> <li>Specify different dependencies for different python versions</li> </ul> </li> <li> <p>It manage the building of your package, you don't need to manually configure     <code>sdist</code> and <code>wheel</code>.</p> </li> <li>Nice dependency view with <code>poetry show</code>.</li> <li>Nice dependency search interface with <code>poetry search</code>.</li> <li>Sync your environment packages with the lock file.</li> </ul> <p>Things I don't like that much:</p> <ul> <li> <p>It does upper version capping by default, it even ignores your pins and adds     the <code>^&lt;new_version</code> pin if you run <code>poetry add     &lt;package&gt;@latest</code>https://github.com/python-poetry/poetry/issues/3503.     Given that upper version capping is becoming a big problem in     the Python environment I'd stay away from <code>poetry</code>.</p> <p>This is specially useless when you add dependencies that follow CalVer. <code>poetry add</code> packaging will still do <code>^21</code> for the version it adds. You shouldn\u2019t be capping versions, but you really shouldn\u2019t be capping CalVer.</p> <p>It's equally troublesome that it upper pins the python version.</p> </li> <li> <p>Have their own dependency specification format similar to <code>npm</code> and     incompatible with Python's     PEP508.</p> </li> <li> <p>No automatic process to update the dependencies constrains to match the latest     version available.  So if you have constrained a package to be <code>&lt;2.0.0</code> and     <code>3.0.0</code> is out there, you will have to manually edit the <code>pyproject.toml</code> so     that it accepts that new version. At least you can use <code>poetry show     --outdated</code> and it will tell you which is the new version, and if the output     is zero, you're sure you're on the last versions.</p> </li> </ul>"}, {"location": "python_package_management/#pdm", "title": "PDM", "text": "<p>Features I like:</p> <ul> <li>The pin strategy defaults to only add lower     pins helping preventing the upper     capping problem.</li> <li>It can't achieve dependency isolation without virtualenvs.</li> <li>Follows the Python's dependency specification format     PEP508.</li> <li>Supports different strategies to add and update dependencies.</li> <li>Command to update your requirements constrains when updating your packages.</li> <li>Sync your environment packages with the lock file.</li> <li>Easy to install package in editable mode.</li> <li>Easy to install local dependencies.</li> <li>You can force the installation of a package at your own risk even if it breaks     the version constrains. (Useful if you're blocked by a third party upper     bound)</li> <li>Changing the python version is as simple as running <code>python use     &lt;python_version&gt;</code>.</li> <li>Plugin system where adding functionality is feasible (like the <code>publish</code>     subcommand).</li> <li>Both global and local configuration.</li> <li>Nice interface to change the configuration.</li> <li>Automatic management of dependencies cache, where you only have one instance     of each package version, and if no project needs it, it will be removed.</li> <li>Has a nice interface to see the cache usage</li> <li>Has the possibility of managing the global packages too.</li> <li>Allows the definition of scripts possibly removing the need of a makefile</li> <li>It's able to read the version of the program from a file, avoiding the     duplication of the information.</li> <li>You can group your development dependencies in groups.</li> <li>Easy to define extra dependencies for your program.</li> <li>It has sensible defaults for <code>includes</code> and <code>excludes</code> when packaging.</li> <li>It's the fastest      and most     correct     one.</li> </ul> <p>Downsides:</p> <ul> <li>They don't say how to configure your environment to work with     vim.</li> </ul>"}, {"location": "python_package_management/#summary", "title": "Summary", "text": "<p>PDM offers the same features as Poetry with the additions of the possibility of selecting your version capping strategy, and doesn\u2019t cap as badly, and follows more PEP standards.</p>"}, {"location": "python_package_management/#references", "title": "References", "text": "<ul> <li>PDM developer comparison</li> <li>John Franey comparison</li> <li>Frost Ming comparison (developer of PDM)</li> <li>Henry Schreiner analysis on Poetry</li> </ul>"}, {"location": "python_plugin_system/", "title": "Python plugin system", "text": "<p>When building Python applications, it's good to develop the core of your program, and allow extension via plugins.</p> <p>I still don't know how to do it, but I'm going to gather interesting references until I tackle it.</p> <ul> <li>Beets plugin system     looks awesome.</li> </ul>"}, {"location": "python_poetry/", "title": "Poetry", "text": "<p>Poetry is a command line program that helps you declare, manage and install dependencies of Python projects, ensuring you have the right stack everywhere.</p> <p><code>poetry</code> saves all the information in the <code>pyproject.toml</code> file, including the project development and program dependencies, for example:</p> <pre><code>[tool.poetry]\nname = \"poetry-demo\"\nversion = \"0.1.0\"\ndescription = \"\"\nauthors = [\"S\u00e9bastien Eustace &lt;sebastien@eustace.io&gt;\"]\n\n[tool.poetry.dependencies]\npython = \"*\"\n\n[tool.poetry.dev-dependencies]\npytest = \"^3.4\"\n</code></pre>"}, {"location": "python_poetry/#installation", "title": "Installation", "text": "<p>Although the official docs tell you to run:</p> <pre><code>curl -sSL https://install.python-poetry.org | python3 -\n</code></pre> <p><code>pip install poetry</code> works too, which looks safer than executing arbitrary code from an url.</p> <p>To enable shell completion for <code>zsh</code> run:</p> <pre><code># Zsh\npoetry completions zsh &gt; ~/.zfunc/_poetry\n\n# Oh-My-Zsh\nmkdir $ZSH_CUSTOM/plugins/poetry\npoetry completions zsh &gt; $ZSH_CUSTOM/plugins/poetry/_poetry\n</code></pre> <p>For <code>zsh</code>, you must then add the following line in your <code>~/.zshrc</code> before <code>compinit</code>:</p> <pre><code>fpath+=~/.zfunc\n</code></pre> <p>For <code>oh-my-zsh</code>, you must then enable poetry in your <code>~/.zshrc</code> plugins:</p> <pre><code>plugins(\npoetry\n    ...\n    )\n</code></pre>"}, {"location": "python_poetry/#basic-usage", "title": "Basic Usage", "text": ""}, {"location": "python_poetry/#initializing-a-pre-existing-project", "title": "Initializing a pre-existing project", "text": "<p>Instead of creating a new project, Poetry can be used to \u2018initialise\u2019 a pre-populated directory with <code>poetry init</code>. You can use the next options</p> <ul> <li><code>--name</code>: Name of the package.</li> <li><code>--description</code>: Description of the package.</li> <li><code>--author</code>: Author of the package.</li> <li><code>--python</code>: Compatible Python versions.</li> <li><code>--dependency</code>: Package to require with a version constraint. Should be in     format <code>foo:1.0.0</code>.</li> <li><code>--dev-dependency</code>: Development requirements, see <code>--require</code>.</li> </ul>"}, {"location": "python_poetry/#installing-dependencies", "title": "Installing dependencies", "text": "<p>To install the defined dependencies for your project, just run the install command.</p> <pre><code>poetry install\n</code></pre> <p>When you run this command, one of two things may happen:</p> <ul> <li> <p>Installing without poetry.lock: If you have never run the command before and     there is also no <code>poetry.lock</code> file present, Poetry simply resolves all     dependencies listed in your <code>pyproject.toml</code> file and downloads the latest     version of their files.</p> <p>When Poetry has finished installing, it writes all of the packages and the exact versions of them that it downloaded to the <code>poetry.lock</code> file, locking the project to those specific versions. You should commit the <code>poetry.lock</code> file to your project repo so that all people working on the project are locked to the same versions of dependencies.</p> </li> <li> <p>Installing with poetry.lock: If there is already a <code>poetry.lock</code> file as     well as a <code>pyproject.toml</code>, <code>poetry</code> resolves and installs all dependencies     that you listed in <code>pyproject.toml</code>, but Poetry uses the exact versions listed     in <code>poetry.lock</code> to ensure that the package versions are consistent for     everyone working on your project. As a result you will have all dependencies     requested by your <code>pyproject.toml</code> file, but they may not all be at the very     latest available versions (some of the dependencies listed in the     <code>poetry.lock</code> file may have released newer versions since the file was     created). This is by design, it ensures that your project does not break     because of unexpected changes in dependencies.</p> </li> </ul> <p>The current project is installed in editable mode by default.</p> <p>If you don't want the development requirements use the <code>--no-dev</code> flag.</p> <p>To remove the untracked dependencies that are no longer in the lock file, use <code>--remove-untracked</code>.</p>"}, {"location": "python_poetry/#updating-dependencies-to-their-latest-versions", "title": "Updating dependencies to their latest versions", "text": "<p>The <code>poetry.lock</code> file prevents you from automatically getting the latest versions of your dependencies. To update to the latest versions, use the <code>update</code> command. This will fetch the latest matching versions (according to your <code>pyproject.toml</code> file) and update the lock file with the new versions. (This is equivalent to deleting the <code>poetry.lock</code> file and running <code>install</code> again.)</p> <p>The main problem is that <code>poetry add</code> does upper pinning of dependencies by default, which is a really bad idea. And they don't plan to change.</p> <p>There is currently no way of updating your <code>pyproject.toml</code> dependency definitions so they match the latest version beyond your constrains. So if you have constrained a package to be <code>&lt;2.0.0</code> and <code>3.0.0</code> is out there, you will have to manually edit the <code>pyproject.toml</code> so that it accepts that new version.  There is no automatic process that does this. At least you can use <code>poetry show --outdated</code> and it will tell you which is the new version, and if the output is zero, you're sure you're on the last versions.</p> <p>Some workarounds exists though, if you run <code>poetry add dependency@latest</code> it will update the lock to the latest. MousaZeidBaker made poetryup, a tool that is able to update the requirements to the latest version with <code>poetryup --latest</code> (although it still has some bugs). Given that it uses <code>poetry add &lt;package&gt;@latest</code> behind the scenes, it will change your version pin to  <code>^&lt;new_version&gt;</code>, which  as we've seen it's awful.</p> <p>Again, you should not be trying to do this, it's better to improve how you manage your dependencies.</p>"}, {"location": "python_poetry/#debugging-why-a-package-is-not-updated-to-the-latest-version", "title": "Debugging why a package is not updated to the latest version", "text": "<p>Sometimes packages are not updated with <code>poetry update</code> or <code>poetryup</code>, to debug why, you need to understand if some package is setting a constrain that prevents the upgrade. To do that, first check the outdated packages with <code>poetry show -o</code> and for each of them:</p> <ul> <li>Check what packages are using the     dependency.</li> <li>Search if there is an issue asking the maintainers to update their     dependencies, if it doesn't exist, create it.</li> </ul>"}, {"location": "python_poetry/#removing-a-dependency", "title": "Removing a dependency", "text": "<pre><code>poetry remove pendulum\n</code></pre> <p>With the <code>-D</code> or <code>--dev</code> flag, it removes the dependency from the development ones.</p>"}, {"location": "python_poetry/#building-the-package", "title": "Building the package", "text": "<p>Before you can actually publish your library, you will need to package it.</p> <pre><code>poetry build\n</code></pre> <p>This command will package your library in two different formats: <code>sdist</code> which is the source format, and <code>wheel</code> which is a compiled package.</p> <p>Once that\u2019s done you are ready to publish your library.</p>"}, {"location": "python_poetry/#publishing-to-pypi", "title": "Publishing to PyPI", "text": "<p>Poetry will publish to PyPI by default. Anything that is published to PyPI is available automatically through Poetry.</p> <pre><code>poetry publish\n</code></pre> <p>This will package and publish the library to PyPI, at the condition that you are a registered user and you have configured your credentials properly.</p> <p>If you pass the <code>--build</code> flag, it will also build the package.</p>"}, {"location": "python_poetry/#publishing-to-a-private-repository", "title": "Publishing to a private repository", "text": "<p>Sometimes, you may want to keep your library private but also being accessible to your team. In this case, you will need to use a private repository.</p> <p>You will need to add it to your global list of repositories.</p> <p>Once this is done, you can actually publish to it like so:</p> <pre><code>poetry publish -r my-repository\n</code></pre>"}, {"location": "python_poetry/#specifying-dependencies", "title": "Specifying dependencies", "text": "<p>If you want to add dependencies to your project, you can specify them in the <code>tool.poetry.dependencies</code> section.</p> <pre><code>[tool.poetry.dependencies]\npendulum = \"^1.4\"\n</code></pre> <p>As you can see, it takes a mapping of package names and version constraints.</p> <p>Poetry uses this information to search for the right set of files in package \u201crepositories\u201d that you register in the <code>tool.poetry.repositories</code> section, or on PyPI by default.</p> <p>Also, instead of modifying the <code>pyproject.toml</code> file by hand, you can use the add command.</p> <pre><code>poetry add pendulum\n</code></pre> <p>It will automatically find a suitable version constraint and install the package and subdependencies.</p> <p>If you want to add the dependency to the development ones, use the <code>-D</code> or <code>--dev</code> flag.</p>"}, {"location": "python_poetry/#using-your-virtual-environment", "title": "Using your virtual environment", "text": "<p>By default, <code>poetry</code> creates a virtual environment in <code>{cache-dir}/virtualenvs</code>. You can change the <code>cache-dir</code> value by editing the <code>poetry</code> config. Additionally, you can use the <code>virtualenvs.in-project</code> configuration variable to create virtual environment within your project directory.</p> <p>There are several ways to run commands within this virtual environment.</p> <p>To run your script simply use <code>poetry run python your_script.py</code>. Likewise if you have command line tools such as <code>pytest</code> or <code>black</code> you can run them using <code>poetry run pytest</code>.</p> <p>The easiest way to activate the virtual environment is to create a new shell with <code>poetry shell</code>.</p>"}, {"location": "python_poetry/#version-management", "title": "Version Management", "text": "<p><code>poetry version</code> shows the current version of the project. If you pass an argument, it will bump the version of the package, for example <code>poetry version minor</code>. But it doesn't read your commits to decide what kind of bump you apply, so I'd keep on using <code>pip-compile</code>.</p>"}, {"location": "python_poetry/#dependency-specification", "title": "Dependency Specification", "text": "<p>Dependencies for a project can be specified in various forms, which depend on the type of the dependency and on the optional constraints that might be needed for it to be installed.</p> <p>They don't follow Python's specification PEP508</p>"}, {"location": "python_poetry/#caret-requirements", "title": "Caret Requirements", "text": "<p>Caret requirements allow SemVer compatible updates to a specified version. An update is allowed if the new version number does not modify the left-most non-zero digit in the major, minor, patch grouping. In this case, if we ran <code>poetry update requests</code>, <code>poetry</code> would update us to the next versions:</p> Requirement Versions allowed <code>^1.2.3</code> <code>&gt;=1.2.3 &lt;2.0.0</code> <code>^1.2</code> <code>&gt;=1.2.0 &lt;2.0.0</code> <code>^1</code> <code>&gt;=1.0.0 &lt;2.0.0</code> <code>^0.2.3</code> <code>&gt;=0.2.3 &lt;0.3.0</code> <code>^0.0.3</code> <code>&gt;=0.0.3 &lt;0.0.4</code> <code>^0.0</code> <code>&gt;=0.0.0 &lt;0.1.0</code> <code>^0</code> <code>&gt;=0.0.0 &lt;1.0.0</code>"}, {"location": "python_poetry/#tilde-requirements", "title": "Tilde requirements", "text": "<p>Tilde requirements specify a minimal version with some ability to update. If you specify a major, minor, and patch version or only a major and minor version, only patch-level changes are allowed. If you only specify a major version, then minor- and patch-level changes are allowed.</p> Requirement Versions allowed <code>~1.2.3</code> <code>&gt;=1.2.3 &lt;1.3.0</code> <code>~1.2</code> <code>&gt;=1.2.0 &lt;1.3.0</code> <code>~1</code> <code>&gt;=1.0.0 &lt;2.0.0</code>"}, {"location": "python_poetry/#wildcard-requirements", "title": "Wildcard requirements", "text": "<p>Wildcard requirements allow for the latest (dependency dependent) version where the wildcard is positioned.</p> Requirement Versions allowed <code>*</code> <code>&gt;=0.0.0</code> <code>1.*</code> <code>&gt;=1.0.0 &lt;2.0.0</code> <code>1.2.*</code> <code>&gt;=1.2.0 &lt;1.3.0</code>"}, {"location": "python_poetry/#inequality-requirements", "title": "Inequality requirements", "text": "<p>Inequality requirements allow manually specifying a version range or an exact version to depend on.</p> <p>Here are some examples of inequality requirements:</p> <pre><code>&gt;= 1.2.0\n&gt; 1\n&lt; 2\n!= 1.2.3\n</code></pre>"}, {"location": "python_poetry/#exact-requirements", "title": "Exact requirements", "text": "<p>You can specify the exact version of a package. This will tell Poetry to install this version and this version only. If other dependencies require a different version, the solver will ultimately fail and abort any install or update procedures.</p> <p>Multiple version requirements can also be separated with a comma, e.g. <code>&gt;= 1.2, &lt; 1.5</code>.</p>"}, {"location": "python_poetry/#git-dependencies", "title": "git dependencies", "text": "<p>To depend on a library located in a git repository, the minimum information you need to specify is the location of the repository with the git key:</p> <pre><code>[tool.poetry.dependencies]\nrequests = { git = \"https://github.com/requests/requests.git\" }\n</code></pre> <p>Since we haven\u2019t specified any other information, Poetry assumes that we intend to use the latest commit on the <code>master</code> branch to build our project.</p> <p>You can combine the git key with the branch key to use another branch. Alternatively, use <code>rev</code> or <code>tag</code> to pin a dependency to a specific commit hash or tagged ref, respectively. For example:</p> <pre><code>[tool.poetry.dependencies]\n# Get the latest revision on the branch named \"next\"\nrequests = { git = \"https://github.com/kennethreitz/requests.git\", branch = \"next\" }\n# Get a revision by its commit hash\nflask = { git = \"https://github.com/pallets/flask.git\", rev = \"38eb5d3b\" }\n# Get a revision by its tag\nnumpy = { git = \"https://github.com/numpy/numpy.git\", tag = \"v0.13.2\" }\n</code></pre> <p>When using <code>poetry add</code> you can add:</p> <ul> <li>A https cloned repo: <code>poetry add     git+https://github.com/sdispater/pendulum.git</code></li> <li>A ssh cloned repo: <code>poetry add     git+ssh://git@github.com/sdispater/pendulum.git</code></li> </ul> <p>If you need to checkout a specific branch, tag or revision, you can specify it when using add:</p> <pre><code>poetry add git+https://github.com/sdispater/pendulum.git#develop\npoetry add git+https://github.com/sdispater/pendulum.git#2.0.5\n</code></pre>"}, {"location": "python_poetry/#path-dependencies", "title": "path dependencies", "text": "<p>To depend on a library located in a local directory or file, you can use the path property:</p> <pre><code>[tool.poetry.dependencies]\n# directory\nmy-package = { path = \"../my-package/\", develop = false }\n\n# file\nmy-package = { path = \"../my-package/dist/my-package-0.1.0.tar.gz\" }\n</code></pre> <p>When using <code>poetry add</code>, you can point them directly to the package or the file:</p> <pre><code>poetry add ./my-package/\npoetry add ../my-package/dist/my-package-0.1.0.tar.gz\npoetry add ../my-package/dist/my_package-0.1.0.whl\n</code></pre> <p>If you want the dependency to be installed in editable mode you can specify it in the <code>pyproject.toml</code> file. It means that changes in the local directory will be reflected directly in environment.</p> <pre><code>[tool.poetry.dependencies]\nmy-package = {path = \"../my/path\", develop = true}\n</code></pre>"}, {"location": "python_poetry/#url-dependencies", "title": "url dependencies", "text": "<p>To depend on a library located on a remote archive, you can use the url property:</p> <pre><code>[tool.poetry.dependencies]\n# directory\nmy-package = { url = \"https://example.com/my-package-0.1.0.tar.gz\" }\n</code></pre> <p>With the corresponding add call:</p> <pre><code>poetry add https://example.com/my-package-0.1.0.tar.gz\n</code></pre>"}, {"location": "python_poetry/#python-restricted-dependencies", "title": "Python restricted dependencies", "text": "<p>You can also specify that a dependency should be installed only for specific Python versions:</p> <pre><code>[tool.poetry.dependencies]\npathlib2 = { version = \"^2.2\", python = \"~2.7\" }\n\n[tool.poetry.dependencies]\npathlib2 = { version = \"^2.2\", python = \"~2.7 || ^3.2\" }\n</code></pre>"}, {"location": "python_poetry/#multiple-constraints-dependencies", "title": "Multiple constraints dependencies", "text": "<p>Sometimes, one of your dependency may have different version ranges depending on the target Python versions.</p> <p>Let\u2019s say you have a dependency on the package <code>foo</code> which is only compatible with Python <code>&lt;3.0</code> up to version <code>1.9</code> and compatible with Python <code>3.4+</code> from version <code>2.0</code>. You would declare it like so:</p> <pre><code>[tool.poetry.dependencies]\nfoo = [\n    {version = \"&lt;=1.9\", python = \"^2.7\"},\n    {version = \"^2.0\", python = \"^3.4\"}\n]\n</code></pre>"}, {"location": "python_poetry/#show-the-available-packages", "title": "Show the available packages", "text": "<p>To list all of the available packages, you can use the show command.</p> <pre><code>poetry show\n</code></pre> <p>If you want to see the details of a certain package, you can pass the package name.</p> <pre><code>poetry show pendulum\n\nname        : pendulum\nversion     : 1.4.2\ndescription : Python datetimes made easy\n\ndependencies:\n - python-dateutil &gt;=2.6.1\n - tzlocal &gt;=1.4\n - pytzdata &gt;=2017.2.2\n</code></pre> <p>By default it will print all the dependencies, if you pass <code>--no-dev</code> it will only show your package's ones.</p> <p>With the <code>-l</code> or <code>--latest</code> it will show the latest version of the packages, and with <code>-o</code> or <code>--outdated</code> it will show the latest version but only for packages that are outdated.</p>"}, {"location": "python_poetry/#search-for-dependencies", "title": "Search for dependencies", "text": "<p>This command searches for packages on a remote index.</p> <pre><code>poetry search requests pendulum\n</code></pre>"}, {"location": "python_poetry/#export-requirements-to", "title": "[Export requirements to", "text": "<p>requirements.txt](https://python-poetry.org/docs/cli/#export)</p> <pre><code>poetry export -f requirements.txt --output requirements.txt\n</code></pre>"}, {"location": "python_poetry/#project-setup", "title": "Project setup", "text": "<p>If you don't already have a cookiecutter for your python projects, you can use <code>poetry new poetry-demo</code>, and it will create the <code>poetry-demo</code> directory with the following content:</p> <pre><code>poetry-demo\n\u251c\u2500\u2500 pyproject.toml\n\u251c\u2500\u2500 README.rst\n\u251c\u2500\u2500 poetry_demo\n\u2502   \u2514\u2500\u2500 __init__.py\n\u2514\u2500\u2500 tests\n    \u251c\u2500\u2500 __init__.py\n    \u2514\u2500\u2500 test_poetry_demo.py\n</code></pre> <p>If you want to use the <code>src</code> project structure, pass the <code>--src</code> flag.</p>"}, {"location": "python_poetry/#checking-what-package-is-using-a-dependency", "title": "Checking what package is using a dependency", "text": "<p>Even though <code>poetry</code> is supposed to show the information of which packages depend on a specific package with <code>poetry show package</code>, I don't see it.</p> <p>Luckily snejus made a small script that shows the information. Save it somewhere in your <code>PATH</code>.</p> <pre><code>_RED='\\\\\\\\e[1;31m&amp;\\\\\\\\e[0m'\n_GREEN='\\\\\\\\e[1;32m&amp;\\\\\\\\e[0m'\n_YELLOW='\\\\\\\\e[1;33m&amp;\\\\\\\\e[0m'\n_format () {\ntr -d '\"' |\nsed \"s/ \\+&gt;[^ ]* \\+&lt;.*/$_YELLOW/\" | # ~ / ^ / &lt; &gt;= ~ a window\nsed \"s/ \\+&gt;[^ ]* *$/$_GREEN/\" |     # &gt;= no upper limit\nsed \"/&gt;/ !s/&lt;.*$/$_RED/\" |          # &lt; ~ upper limit\nsed \"/&gt;\\|&lt;/ !s/ .*/  $_RED/\"        # == ~ locked version\n}\n\n_requires () {\nsed -n \"/^name = \\\"$1\\\"/I,/\\[\\[package\\]\\]/{\n                /\\[package.dep/,/^$/{\n                    /^[^[]/ {\n                        s/= {version = \\(\\\"[^\\\"]*\\\"\\).*/, \\1/p;\n                        s/ =/,/gp\n             }}}\" poetry.lock |\nsed \"/,.*,/!s/&lt;/,&lt;/; s/^[^&lt;]\\+$/&amp;,/\" |\ncolumn -t -s , | _format\n}\n\n_required_by () {\nsed -n \"/\\[metadata\\]/,//d;\n            /\\[package\\]\\|\\[package\\.depen/,/^$/H;\n            /^name\\|^$1 = /Ip\" poetry.lock |\nsed -n \"/^$1/I{x;G;p};h\" |\nsed 's/.*\"\\(.*\\)\".*/\\1/' |\nsed '$!N;s/\\n/ /' |\ncolumn -t | _format\n}\n\ndeps() {\necho\necho -e \"\\e[1mREQUIRES\\e[0m\"\n_requires \"$1\" | xargs -i echo -e \"\\t{}\"\necho\necho -e \"\\e[1mREQUIRED BY\\e[0m\"\n_required_by \"$1\" | xargs -i echo -e \"\\t{}\"\necho\n}\n\ndeps $1\n</code></pre>"}, {"location": "python_poetry/#configuration", "title": "Configuration", "text": "<p>Poetry can be configured via the <code>config</code> command (see more about its usage here) or directly in the <code>config.toml</code> file that will be automatically be created when you first run that command. This file can typically be found in <code>~/.config/pypoetry</code>.</p> <p>Poetry also provides the ability to have settings that are specific to a project by passing the <code>--local</code> option to the config command.</p> <pre><code>poetry config virtualenvs.create false --local\n</code></pre>"}, {"location": "python_poetry/#list-the-current-configuration", "title": "List the current configuration", "text": "<p>To list the current configuration you can use the <code>--list</code> option of the <code>config</code> command:</p> <pre><code>poetry config --list\n</code></pre> <p>Which will give you something similar to this:</p> <pre><code>cache-dir = \"/path/to/cache/directory\"\nvirtualenvs.create = true\nvirtualenvs.in-project = null\nvirtualenvs.path = \"{cache-dir}/virtualenvs\"  # /path/to/cache/directory/virtualenvs\n</code></pre>"}, {"location": "python_poetry/#adding-or-updating-a-configuration-setting", "title": "Adding or updating a configuration setting", "text": "<p>To change or otherwise add a new configuration setting, you can pass a value after the setting\u2019s name:</p> <pre><code>poetry config virtualenvs.path /path/to/cache/directory/virtualenvs\n</code></pre> <p>For a full list of the supported settings see Available settings.</p>"}, {"location": "python_poetry/#removing-a-specific-setting", "title": "Removing a specific setting", "text": "<p>If you want to remove a previously set setting, you can use the <code>--unset</code> option:</p> <pre><code>poetry config virtualenvs.path --unset\n</code></pre>"}, {"location": "python_poetry/#adding-a-repository", "title": "Adding a repository", "text": "<p>Adding a new repository is easy with the <code>config</code> command.</p> <pre><code>poetry config repositories.foo https://foo.bar/simple/\n</code></pre> <p>This will set the url for repository <code>foo</code> to <code>https://foo.bar/simple/</code>.</p>"}, {"location": "python_poetry/#configuring-credentials", "title": "Configuring credentials", "text": "<p>If you want to store your credentials for a specific repository, you can do so easily:</p> <pre><code>poetry config http-basic.foo username password\n</code></pre> <p>If you do not specify the password you will be prompted to write it.</p> <p>To publish to PyPI, you can set your credentials for the repository named <code>pypi</code>.</p> <p>Note that it is recommended to use API tokens when uploading packages to PyPI. Once you have created a new token, you can tell Poetry to use it:</p> <pre><code>poetry config pypi-token.pypi my-token\n</code></pre> <p>If a system keyring is available and supported, the password is stored to and retrieved from the keyring. In the above example, the credential will be stored using the name <code>poetry-repository-pypi</code>. If access to keyring fails or is unsupported, this will fall back to writing the password to the <code>auth.toml</code> file along with the username.</p> <p>Keyring support is enabled using the keyring library. For more information on supported backends refer to the library documentation. It doesn't support pass by default, but Steffen Vogel created a specific keyring backend. Alternatively, you can use environment variables to provide the credentials:</p> <pre><code>export POETRY_PYPI_TOKEN_PYPI=my-token\nexport POETRY_HTTP_BASIC_PYPI_USERNAME=username\nexport POETRY_HTTP_BASIC_PYPI_PASSWORD=password\n</code></pre> <p>I've tried setting up the keyring but I get the next error:</p> <pre><code>  UploadError\n\n  HTTP Error 403: Invalid or non-existent authentication information. See https://pypi.org/help/#invalid-auth for more information.\n\n  at ~/.venvs/autodev/lib/python3.9/site-packages/poetry/publishing/uploader.py:216 in _upload\n      212\u2502                     self._register(session, url)\n      213\u2502                 except HTTPError as e:\n      214\u2502                     raise UploadError(e)\n      215\u2502\n    \u2192 216\u2502             raise UploadError(e)\n      217\u2502\n      218\u2502     def _do_upload(\n      219\u2502         self, session, url, dry_run=False\n      220\u2502     ):  # type: (requests.Session, str, Optional[bool]) -&gt; None\n</code></pre> <p>The keyring was configured with:</p> <pre><code>poetry config pypi-token.pypi internet/pypi.token\n</code></pre> <p>And I'm sure that the keyring works because <code>python -m keyring get internet pypi.token</code> works.</p> <p>I've also tried with the environmental variable <code>POETRY_PYPI_TOKEN_PYPI</code> but it didn't work either. And setting the configuration as <code>poetry config http-basic.pypi __token__ internet/pypi.token</code>.</p> <p>Finally I had to hardcode the token with <code>poetry config pypi-token.pypi \"$(pass show internet/pypi.token)</code>. Although I can't find where it's storing the value :S.</p>"}, {"location": "python_poetry/#references", "title": "References", "text": "<ul> <li>Git</li> <li>Docs</li> </ul>"}, {"location": "python_profiling/", "title": "Python Profiling", "text": "<p>Profiling is to find out where your code spends its time. Profilers can collect several types of information: timing, function calls, interruptions or cache faults. It can be useful to identify bottlenecks, which should be the first step when trying to optimize some code, or to study the evolution of the performance of your code.</p>"}, {"location": "python_profiling/#profiling-types", "title": "Profiling types", "text": "<p>There are two types of profiling:</p> Deterministic Profiling All events are monitored. It provides accurate information but has a big impact on performance (overhead). It means the code runs slower under profiling. Its use in production systems is often impractical. This type of profiling is suitable for small functions. Statistical profiling Sampling the execution state at regular intervals to compute indicators. This method is less accurate, but it also reduces the overhead."}, {"location": "python_profiling/#profiling-tools", "title": "Profiling tools", "text": "<p>The profiling tools you should use vary with the code you are working on. If you are writing a single algorithm or a small program, you should use a simple profiler like cProfile or even a fine-grained tool like <code>line_profiler</code>. In contrast, when you are optimizing a whole program, you may want to use a statistical profiler to avoid overhead, such as pyinstrument, or if you're debugging a running process, using py-spy.</p>"}, {"location": "python_profiling/#deterministic-profiling", "title": "Deterministic Profiling", "text": ""}, {"location": "python_profiling/#cprofile", "title": "cProfile", "text": "<p>Python comes with two built-in modules for deterministic profiling: cProfile and profile.  Both are different implementations of the same interface. The former is a C extension with relatively small overhead, and the latter is a pure Python module. As the official documentation says, the module profile would be suitable when we want to extend the profiler in some way. Otherwise, cProfile is preferred for long-running programs. Unfortunately, there is no built-in module for statistical profiling, but we will see some external packages for it.</p> <pre><code>$: python3 -m cProfile script.py\n\n58 function calls in 9.419 seconds\n\nOrdered by: standard namen\n\ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n1    0.000    0.000    9.419    9.419 part1.py:1(&lt;module&gt;)\n51   9.419    0.185    9.419    0.185 part1.py:1(computation)\n1    0.000    0.000    9.419    9.419 part1.py:10(function1)\n1    0.000    0.000    9.243    9.243 part1.py:15(function2)\n1    0.000    0.000    0.176    0.176 part1.py:20(function3)\n1    0.000    0.000    9.419    9.419 part1.py:24(main)\n</code></pre> <p>Where:</p> ncalls Is the number of calls. We should try to optimize functions that have a lot of calls or consume too much time per call. tottime The total time spent in the function itself, excluding sub calls. This is where we should look closely at. We can see that the function computation is called 51 times, and each time consumes 0.185s. cumtime Cumulative time. It includes sub calls. percall We have two \u201cper call\u201d metrics. The first one: total time per call, and the second one: cumulative time per call. Again, we should focus on the total time metric. <p>We can also sort the functions by some criteria, for example <code>python3 -m cProfile -s tottime script.py</code>.</p>"}, {"location": "python_profiling/#statistical-profiling", "title": "Statistical profiling", "text": ""}, {"location": "python_profiling/#py-spy", "title": "Py-spy", "text": "<p>Py-Spy is a statistical (sampling) profiler that lets you visualize the time each function consumes during the execution. An important feature is that you can attach the profiler without restarting the program or modifying the code, and has a low overhead. This makes the tool highly suitable for production code.</p> <p>To install it, just type:</p> <pre><code>pip install py-spy\n</code></pre> <p>To test the performance of a file use:</p> <pre><code>py-spy top python3 script.py\n</code></pre> <p>To assess the performance of a runnin process, specify it's PID:</p> <pre><code>py-spy top --pid $PID\n</code></pre> <p>They will show a <code>top</code> like interface showing the following data:</p> <pre><code>GIL: 100.00%, Active: 100.00%, Threads: 1\n\n  %Own   %Total  OwnTime  TotalTime  Function (filename:line)\n 61.00%  61.00%   10.50s    10.50s   computation (script.py:7)\n 39.00%  39.00%    7.50s     7.50s   computation (script.py:6)\n  0.00% 100.00%   0.000s    18.00s   &lt;module&gt; (script.py:30)\n  0.00% 100.00%   0.000s    18.00s   function2 (script.py:18)\n  0.00% 100.00%   0.000s    18.00s   main (script.py:26)\n  0.00% 100.00%   0.000s    18.00s   function1 (script.py:12)\n</code></pre>"}, {"location": "python_profiling/#pyinstrument", "title": "pyinstrument", "text": "<p>It is similar to cProfile in the sense that we can\u2019t attach the profiler to a running program, but that is where the similarities end, as pyinstrument doesn't track every function call that your program makes. Instead, it's recording the call stack every 1ms.</p> <p>Install it with:</p> <pre><code>pip install pyinstrument\n</code></pre> <p>Use:</p> <p>The advantages are that:</p> <ul> <li>The output is far more attractive.</li> <li>It has less overhead, so it distorts less the results.</li> <li>Doesn't show the internal calls that make cProfiling result reading difficult.</li> <li>It uses wall-clock time instead of CPU time. So it takes into account the IO     time.</li> </ul> <pre><code>$: pyinstrument script.py\n\n  _     ._   __/__   _ _  _  _ _/_   Recorded: 15:45:20  Samples:  51\n/_//_/// /_\\ / //_// / //_'/ //     Duration: 4.517     CPU time: 4.516\n/   _/                      v3.3.0\n\nProgram: script.py\n\n4.516 &lt;module&gt;  script.py:2\n\u2514\u2500 4.516 main  script.py:25\n   \u2514\u2500 4.516 function1  script.py:11\n      \u251c\u2500 4.425 function2  script.py:16\n      \u2502  \u2514\u2500 4.425 computation  script.py:2\n      \u2514\u2500 0.092 function3  script.py:21\n         \u2514\u2500 0.092 computation  script.py:2\n</code></pre> <p>With the possibility to generate an HTML report.</p> <p></p> <p>The disadvantages are that it's only easy to profile python script files, not full packages.</p> <p>You can also profile a chunk of code, which can be useful when developing or for writing performance tests.</p> <pre><code>from pyinstrument import Profiler\n\nprofiler = Profiler()\nprofiler.start()\n\n# code you want to profile\n\nprofiler.stop()\n\nprint(profiler.output_text(unicode=True, color=True))\n</code></pre> <p>To explore the profile in a web browser, use <code>profiler.open_in_browser()</code>. To save this HTML for later, use <code>profiler.output_html()</code>.</p>"}, {"location": "python_profiling/#introduce-profiling-in-your-test-workflow", "title": "Introduce profiling in your test workflow", "text": "<p>I run out of time, so here are the starting points:</p> <ul> <li>Niklas Meinzer     post</li> <li>Pypi page of pytest-benchmark,     Docs,     Git</li> <li>Docs of pytest-profiling</li> <li>uwpce guide on using     pstats</li> </ul> <p>The idea is to develop the following ideas:</p> <ul> <li>How to integrate profiling with pytest.</li> <li>How to compare benchmark results between CI runs.</li> <li>Some guidelines on writing performance tests</li> </ul> <p>And memray looks very promising.</p>"}, {"location": "python_profiling/#references", "title": "References", "text": "<ul> <li>Antonio Molner article on Python Profiling</li> </ul>"}, {"location": "python_properties/", "title": "Python Properties", "text": "<p>The <code>@property</code> is the pythonic way to use getters and setters in object-oriented programming. It can be used to make methods look like attributes.</p> <p>The <code>property</code> decorator returns an object that proxies any request to set or access the attribute value through the methods we have specified.</p> <pre><code>class Foo:\n    @property\n    def foo(self):\n        return 'bar'\n</code></pre> <p>We can specify a setter function for the new property</p> <pre><code>class Foo:\n    @property\n    def foo(self):\n        return self._foo\n\n    @foo.setter\n    def foo(self, value):\n        self._foo = value\n</code></pre> <p>We first decorate the <code>foo</code> method a as getter. Then we decorate a second method with exactly the same name by applying the <code>setter</code> attribute of the originally decorated <code>foo</code> method. The <code>property</code> function returns an object; this object always comes with its own <code>setter</code> attribute, which can then be applied as a decorator to other functions. Using the same name for the get and set methods is not required, but it does help group the multiple methods that access one property together.</p> <p>We can also specify a deletion function with <code>@foo.deleter</code>. We cannot specify a docstring using <code>property</code> decorators, so we need  to rely on the property copying the docstring from the initial getter method</p> <pre><code>class Silly:\n    @property\n    def silly(self):\n        \"This is a silly property\"\n        print(\"You are getting silly\")\n        return self._silly\n\n    @silly.setter\n    def silly(self, value):\n        print(\"You are making silly {}\".format(value))\n        self._silly = value\n\n    @silly.deleter\n    def silly(self):\n        print(\"Whoah, you kicked silly!\")\n        del self.silly\n</code></pre> <pre><code>&gt;&gt;&gt; s = Silly()\n&gt;&gt;&gt; s.silly = \"funny\"\nYou are making silly funny\n&gt;&gt;&gt; s.silly\nYou are getting silly\n'funny'\n&gt;&gt;&gt; del s.silly\nWhoah, you kicked silly!\n</code></pre>"}, {"location": "python_properties/#when-to-use-properties", "title": "When to use properties", "text": "<p>The most common use of a property is when we have some data on a class that we later want to add behavior to.</p> <p>The fact that methods are just callable attributes, and properties are just customizable attributes can help us make the decision. Methods should typically represent actions; things that can be done to, or performed by, the object. When you call a method, even with only one argument, it should do something. Method names a generally verbs.</p> <p>Once confirming that an attribute is not an action, we need to decide between standard data attributes and properties. In general, always use a standard attribute until you need to control access to that property in some way. In either case, your attribute is usually a noun . The only difference between an attribute and a property is that we can invoke custom actions automatically when a property is retrieved, set, or deleted</p>"}, {"location": "python_properties/#cache-expensive-data", "title": "Cache expensive data", "text": "<p>A common need for custom behavior is caching a value that is difficult to calculate or expensive to look up.</p> <p>We can do this with a custom getter on the property. The first time the value is retrieved, we perform the lookup or calculation. Then we could locally cache the value as a private attribute on our object, and the next time the value is requested, we return the stored data.</p> <pre><code>from urlib.request import urlopen\n\nclass Webpage:\n    def __init__(self, url):\n        self.url = url\n        self._content = None\n\n    @property\n    def content(self):\n        if not self._content:\n            print(\"Retrieving New Page..\")\n            self._content = urlopen(self.url).read()\n        return self._content\n</code></pre> <pre><code>&gt;&gt;&gt; import time\n&gt;&gt;&gt; webpage = Webpage(\"http://ccphillips.net/\")\n&gt;&gt;&gt; now = time.time()\n&gt;&gt;&gt; content1 = webpage.content\nRetrieving new Page...\n&gt;&gt;&gt; time.time() - now\n22.43316\n&gt;&gt;&gt; now = time.time()\n&gt;&gt;&gt; content2 = webpage.content\n&gt;&gt;&gt; time.time() -now\n1.926645\n&gt;&gt;&gt; content1 == content2\nTrue\n</code></pre>"}, {"location": "python_properties/#attributes-calculated-on-the-fly", "title": "Attributes calculated on the fly", "text": "<p>Custom getters are also useful for attributes that need to be calculated on the fly, based on other object attributes.</p> <p><pre><code>clsas AverageList(list):\n    @property\n    def average(self):\n        return sum(self) / len(self)\n</code></pre> <pre><code>&gt;&gt;&gt; a = AverageList([1,2,3,4])\n&gt;&gt;&gt; a.average\n2.5\n</code></pre></p> <p>Of course we could have made this a method instead, but then we should call it <code>calculate_average()</code>, since methods represent actions. But a property called <code>average</code> is more suitable, both easier to type, and easier to read.</p>"}, {"location": "python_properties/#abstract-properties", "title": "Abstract properties", "text": "<p>Sometimes you want to define properties in your abstract classes, to do that, use:</p> <pre><code>from abc import ABC, abstractmethod\n\nclass C(ABC):\n    @property\n    @abstractmethod\n    def my_abstract_property(self):\n        ...\n</code></pre> <p>If you want to use an abstract setter, you'll encounter the mypy <code>Decorated property not supported</code> error, you'll need to add a <code># type: ignore</code> until this issue is solved.</p>"}, {"location": "python_sh/", "title": "SH", "text": "<p>sh is a full-fledged subprocess replacement so beautiful that makes you want to cry. It allows you to call any program as if it were a function:</p> <pre><code>from sh import ifconfig\nprint(ifconfig(\"wlan0\"))\n</code></pre> <p>Output:</p> <pre><code>wlan0   Link encap:Ethernet  HWaddr 00:00:00:00:00:00\n        inet addr:192.168.1.100  Bcast:192.168.1.255  Mask:255.255.255.0\n        inet6 addr: ffff::ffff:ffff:ffff:fff/64 Scope:Link\n        UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1\n        RX packets:0 errors:0 dropped:0 overruns:0 frame:0\n        TX packets:0 errors:0 dropped:0 overruns:0 carrier:0\n        collisions:0 txqueuelen:1000\n        RX bytes:0 (0 GB)  TX bytes:0 (0 GB)\n</code></pre> <p>Note that these aren't Python functions, these are running the binary commands on your system by dynamically resolving your $PATH, much like Bash does, and then wrapping the binary in a function. In this way, all the programs on your system are available to you from within Python.</p>"}, {"location": "python_sh/#installation", "title": "Installation", "text": "<pre><code>pip install sh\n</code></pre>"}, {"location": "python_sh/#usage", "title": "Usage", "text": ""}, {"location": "python_sh/#passing-arguments", "title": "Passing arguments", "text": "<pre><code>sh.ls(\"-l\", \"/tmp\", color=\"never\")\n</code></pre> <p>If the command gives you a syntax error (like <code>pass</code>), you can use bash.</p> <pre><code>sh.bash(\"-c\", \"pass\")\n</code></pre>"}, {"location": "python_sh/#handling-exceptions", "title": "Handling exceptions", "text": "<p>Normal processes exit with exit code 0. You can access the program return code with <code>RunningCommand.exit_code</code>:</p> <pre><code>output = ls(\"/\")\nprint(output.exit_code) # should be 0\n</code></pre> <p>If a process terminates, and the exit code is not 0, sh generates an exception dynamically. This lets you catch a specific return code, or catch all error return codes through the base class <code>ErrorReturnCode</code>:</p> <pre><code>try:\n    print(ls(\"/some/non-existant/folder\"))\nexcept sh.ErrorReturnCode_2:\n    print(\"folder doesn't exist!\")\n    create_the_folder()\nexcept sh.ErrorReturnCode:\n    print(\"unknown error\")\n</code></pre> <p>The exception object is an sh command object, which has, between other , the <code>stderr</code> and <code>stdout</code> bytes attributes with the errors. To show them use:</p> <pre><code>except sh.ErrorReturnCode as error:\n    print(str(error.stderr, 'utf8'))\n</code></pre>"}, {"location": "python_sh/#redirecting-output", "title": "Redirecting output", "text": "<pre><code>sh.ifconfig(_out=\"/tmp/interfaces\")\n</code></pre>"}, {"location": "python_sh/#running-in-background", "title": "Running in background", "text": "<p>By default, each running command blocks until completion. If you have a long-running command, you can put it in the background with the <code>_bg=True</code> special kwarg:</p> <pre><code># blocks\nsleep(3)\nprint(\"...3 seconds later\")\n\n# doesn't block\np = sleep(3, _bg=True)\nprint(\"prints immediately!\")\np.wait()\nprint(\"...and 3 seconds later\")\n</code></pre> <p>You\u2019ll notice that you need to call <code>RunningCommand.wait()</code> in order to exit after your command exits.</p> <p>Commands launched in the background ignore <code>SIGHUP</code>, meaning that when their controlling process (the session leader, if there is a controlling terminal) exits, they will not be signalled by the kernel. But because <code>sh</code> commands launch their processes in their own sessions by default, meaning they are their own session leaders, ignoring <code>SIGHUP</code> will normally have no impact. So the only time ignoring <code>SIGHUP</code> will do anything is if you use <code>_new_session=False</code>, in which case the controlling process will probably be the shell from which you launched python, and exiting that shell would normally send a <code>SIGHUP</code> to all child processes.</p> <p>If you want to terminate the process use <code>p.kill()</code>.</p>"}, {"location": "python_sh/#output-callbacks", "title": "Output callbacks", "text": "<p>In combination with <code>_bg=True</code>, <code>sh</code> can use callbacks to process output incrementally by passing a callable function to <code>_out</code> and/or <code>_err</code>. This callable will be called for each line (or chunk) of data that your command outputs:</p> <pre><code>from sh import tail\n\ndef process_output(line):\n    print(line)\n\np = tail(\"-f\", \"/var/log/some_log_file.log\", _out=process_output, _bg=True)\np.wait()\n</code></pre> <p>To \u201cquit\u201d your callback, simply <code>return True</code>. This tells the command not to call your callback anymore. This does not kill the process though see Interactive callbacks for how to kill a process from a callback.</p> <p>The line or chunk received by the callback can either be of type str or bytes. If the output could be decoded using the provided encoding, a str will be passed to the callback, otherwise it would be raw bytes.</p>"}, {"location": "python_sh/#interactive-callbacks", "title": "Interactive callbacks", "text": "<p>Commands may communicate with the underlying process interactively through a specific callback signature. Each command launched through <code>sh</code> has an internal STDIN <code>queue.Queue</code> that can be used from callbacks:</p> <pre><code>    def interact(line, stdin):\n        if line == \"What... is the air-speed velocity of an unladen swallow?\":\n            stdin.put(\"What do you mean? An African or European swallow?\")\n\n        elif line == \"Huh? I... I don't know that....AAAAGHHHHHH\":\n            cross_bridge()\n            return True\n\n        else:\n            stdin.put(\"I don't know....AAGGHHHHH\")\n            return True\n\n    p = sh.bridgekeeper(_out=interact, _bg=True)\np.wait()\n</code></pre> <p>You can also kill or terminate your process (or send any signal, really) from your callback by adding a third argument to receive the process object:</p> <pre><code>def process_output(line, stdin, process):\n    print(line)\n    if \"ERROR\" in line:\n        process.kill()\n        return True\n\np = tail(\"-f\", \"/var/log/some_log_file.log\", _out=process_output, _bg=True)\n</code></pre> <p>The above code will run, printing lines from <code>some_log_file.log</code> until the word <code>ERROR</code> appears in a line, at which point the tail process will be killed and the script will end.</p>"}, {"location": "python_sh/#interacting-with-programs-that-ask-input-from-the-user", "title": "Interacting with programs that ask input from the user", "text": "<p>Note</p> <p>Check the interactive callbacks or this issue, as it looks like a cleaner solution.</p> <p><code>sh</code> allows you to interact with programs that asks for user input. The documentation is not clear on how to do it, but between the function callbacks documentation, and the example on how to enter an SSH password we can deduce how to do it.</p> <p>Imagine we've got a python script that asks the user to enter a username so it can save it in a file.</p> <p>File: /tmp/script.py</p> <pre><code>answer = input(\"Enter username: \")\n\nwith open(\"/tmp/user.txt\", \"w+\") as f:\n    f.write(answer)\n</code></pre> <p>When we run it in the terminal we get prompted and answer with <code>lyz</code>:</p> <pre><code>$: /tmp/script.py\nEnter username: lyz\n\n$: cat /tmp/user.txt\nlyz\n</code></pre> <p>To achieve the same goal automatically with <code>sh</code> we'll need to use the function callbacks. They are functions we pass to the sh command through the <code>_out</code> argument.</p> <pre><code>import sys\nimport re\n\naggregated = \"\"\n\ndef interact(char, stdin):\n    global aggregated\n    sys.stdout.write(char.encode())\n    sys.stdout.flush()\n    aggregated += char\n    if re.search(r\"Enter username: \", aggregated, re.MULTILINE):\n        stdin.put(\"lyz\\n\")\n\nsh.bash(\n    \"-c\",\n    \"/tmp/script.py\",\n    _out=interact,\n    _out_bufsize=0\n)\n</code></pre> <p>In the example above we've created an <code>interact</code> function that will get called on each character of the stdout of the command. It will be called on each character because we passed the argument <code>_out_bufsize=0</code>. Check the ssh password example to see why we need that.</p> <p>As it's run on each character, and we need to input the username once the program is expecting us to enter the input and not before, we need to keep track of all the printed characters through the global <code>aggregated</code> variable. Once the regular expression matches what we want, sh will inject the desired value.</p> <p>Remember to add the <code>\\n</code> at the end of the string you want to inject.</p> <p>If the output never matches the regular expression, you'll enter an endless loop, so you need to know before hand all the possible user input prompts.</p>"}, {"location": "python_sh/#testing", "title": "Testing", "text": "<p><code>sh</code> can be patched in your tests the typical way, with <code>unittest.mock.patch()</code>:</p> <pre><code>from unittest.mock import patch\nimport sh\n\ndef get_something():\n    return sh.pwd()\n\n@patch(\"sh.pwd\", create=True)\ndef test_something(pwd):\n    pwd.return_value = \"/\"\n    assert get_something() == \"/\"\n</code></pre> <p>The important thing to note here is that <code>create=True</code> is set. This is required because <code>sh</code> is a bit magical and patch will fail to find the <code>pwd</code> command as an attribute on the <code>sh</code> module.</p> <p>You may also patch the <code>Command</code> class:</p> <pre><code>from unittest.mock import patch\nimport sh\n\ndef get_something():\n    pwd = sh.Command(\"pwd\")\n    return pwd()\n\n@patch(\"sh.Command\")\ndef test_something(Command):\n    Command().return_value = \"/\"\n    assert get_something() == \"/\"\n</code></pre> <p>Notice here we do not need <code>create=True</code>, because <code>Command</code> is not an automatically generated object on the <code>sh</code> module (it actually exists).</p>"}, {"location": "python_sh/#tips", "title": "Tips", "text": ""}, {"location": "python_sh/#passing-environmental-variables-to-commands", "title": "Passing environmental variables to commands", "text": "<p>The <code>_env</code> special <code>kwarg</code> allows you to pass a dictionary of environment variables and their corresponding values:</p> <pre><code>import sh\nsh.google_chrome(_env={\"SOCKS_SERVER\": \"localhost:1234\"})\n</code></pre> <p><code>_env</code> replaces your process\u2019s environment completely. Only the key-value pairs in <code>_env</code> will be used for its environment. If you want to add new environment variables for a process in addition to your existing environment, try something like this:</p> <pre><code>import os\nimport sh\n\nnew_env = os.environ.copy()\nnew_env[\"SOCKS_SERVER\"] = \"localhost:1234\"\n\nsh.google_chrome(_env=new_env)\n</code></pre>"}, {"location": "python_sh/#avoid-exception-logging-when-killing-a-background-process", "title": "Avoid exception logging when killing a background process", "text": "<p>In order to catch this exception execute your process with <code>_bg_exec=False</code> and execute <code>p.wait()</code> if you want to handle the exception. Otherwise don't use the <code>p.wait()</code>.</p> <pre><code>p = sh.sleep(100, _bg=True, _bg_exc=False)\ntry:\n    p.kill()\n    p.wait()\nexcept sh.SignalException_SIGKILL as err:\n    print(\"foo\")\n\nfoo\n</code></pre>"}, {"location": "python_sh/#use-commands-that-return-a-syntaxerror", "title": "Use commands that return a SyntaxError", "text": "<p><code>pass</code> is a reserved python word so <code>sh</code> fails when calling the password store command <code>pass</code>.</p> <pre><code>pass_command = sh.Command('pass')\npass_command('show', 'new_file')\n</code></pre>"}, {"location": "python_sh/#references", "title": "References", "text": "<ul> <li>Docs</li> <li>Git</li> </ul>"}, {"location": "python_vlc/", "title": "Python VLC", "text": "<p>Python VLC is a library to control <code>vlc</code> from python.</p> <p>There is not usable online documentation, you'll have to go through the <code>help(&lt;component&gt;)</code> inside the python console.</p>"}, {"location": "python_vlc/#installation", "title": "Installation", "text": "<pre><code>pip install python-vlc\n</code></pre>"}, {"location": "python_vlc/#usage", "title": "Usage", "text": ""}, {"location": "python_vlc/#basic-usage", "title": "Basic usage", "text": "<p>You can create an instance of the <code>vlc</code> player with:</p> <pre><code>import vlc\n\nplayer = vlc.MediaPlayer('path/to/file.mp4')\n</code></pre> <p>The <code>player</code> has the next interesting methods:</p> <ul> <li><code>play()</code>: Opens the program and starts playing, if you've used <code>pause</code> it     resumes playing.</li> <li><code>pause()</code>: Pauses the video</li> <li><code>stop()</code>: Closes the player.</li> <li><code>set_fulscreen(1)</code>: Sets to fullscreen if you pass <code>0</code> as argument it returns     from fullscreen.</li> <li><code>set_media(vlc.Media('path/to/another/file.mp4'))</code>: Change the reproduced     file. It can even play pictures!</li> </ul> <p>If you want more control, it's better to use an <code>vlc.Instance</code> object to work with.</p>"}, {"location": "python_vlc/#configure-the-instance", "title": "Configure the instance", "text": "<pre><code>instance = Instance('--loop')\n</code></pre>"}, {"location": "python_vlc/#reproduce-many-files", "title": "Reproduce many files", "text": "<p>First you need to create a media list:</p> <pre><code>media_list = instance.media_list_new()\npath = \"/path/to/directory\"\nfiles = os.listdir(path)\nfor file_ in files:\n    media_list.add_media(instance.media_new(os.path.join(path,s)))\n</code></pre> <p>Then create the player:</p> <pre><code>media_player = instance.media_list_player_new()\nmedia_player.set_media_list(media_list)\n</code></pre> <p>Now you can use <code>player.next()</code> and <code>player.previous()</code>.</p>"}, {"location": "python_vlc/#set-playback-mode", "title": "Set playback mode", "text": "<pre><code>media_player.set_playback_mode(vlc.PlaybackMode.loop)\n</code></pre> <p>There are the next playback modes:</p> <ul> <li><code>default</code></li> <li><code>loop</code></li> <li><code>repeat</code></li> </ul>"}, {"location": "python_vlc/#references", "title": "References", "text": "<ul> <li>Home</li> <li>Source</li> </ul>"}, {"location": "pythonping/", "title": "pythonping", "text": "<p>pythonping is simple way to ping in Python. With it, you can send ICMP Probes to remote devices like you would do from the terminal.</p> <p>Warning: Since using <code>pythonping</code> requires root permissions or granting <code>cap_net_raw</code> capability to the python interpreter, try to measure the latency to a server by other means such as using <code>requests</code>.</p>"}, {"location": "pythonping/#installation", "title": "Installation", "text": "<pre><code>pip install pythonping\n</code></pre> <p>By default it requires root permissions to run because Operating systems are designed to require root for creating raw IP packets, and sniffing the traffic on the network card. These actions are required to do the ping.</p> <p>If you don't want to run your script with root, you can use the capabilities framework. You can give Python the same capabilities as <code>/bin/ping</code> by doing:</p> <pre><code>sudo setcap cap_net_raw+ep $(readlink -f $(which python))\n</code></pre> <p>This will allow Python to capture raw packets, without having to give it full root permission.</p> <p>If you want to remove the permissions you can do:</p> <pre><code>sudo setcap -r $(readlink -f $(which python))\n</code></pre> <p>You can check that you've removed it with:</p> <pre><code>sudo getcap $(readlink -f $(which python))\n</code></pre> <p>If it doesn't return any output is that it doesn't have any capabilities.</p>"}, {"location": "pythonping/#usage", "title": "Usage", "text": "<p>If you want to see the output immediately, emulating what happens on the terminal, use the verbose flag as below. Otherwise it won't show any information on the <code>stdout</code>.</p> <pre><code>from pythonping import ping\n\nping(\"127.0.0.1\", verbose=True)\n</code></pre> <p>This will yield the following result.</p> <pre><code>Reply from 127.0.0.1, 9 bytes in 0.17ms\nReply from 127.0.0.1, 9 bytes in 0.14ms\nReply from 127.0.0.1, 9 bytes in 0.12ms\nReply from 127.0.0.1, 9 bytes in 0.12ms\n</code></pre> <p>Regardless of the verbose mode, the ping function will always return a <code>ResponseList</code> object. This is a special iterable object, containing a list of <code>Response</code> items. In each <code>Response</code>, you can find the packet received and some meta information, like:</p> <ul> <li><code>error_message</code>: contains a string describing the error this response   represents. For example, an error could be \u201cNetwork Unreachable\u201d or   \u201cFragmentation Required\u201d. If you got a successful response, this property is   None..</li> <li><code>success</code>: is a bool indicating if the response is successful.</li> <li><code>time_elapsed</code>: and time_elapsed_ms indicate how long it took to receive this   response, respectively in seconds and milliseconds..</li> </ul> <p>On top of that, <code>ResponseList</code> adds some intelligence you can access from its own members. The fields are self-explanatory:</p> <ul> <li><code>rtt_min</code> and <code>rtt_min_ms</code>.</li> <li><code>rtt_max</code> and <code>rtt_max_ms</code>.</li> <li><code>rtt_avg</code> and <code>rtt_avg_ms</code>.</li> </ul> <p>You can also tune your ping by using some of its additional parameters:</p> <ul> <li><code>size</code>: is an integer that allows you to specify the size of the ICMP payload   you desire.</li> <li><code>timeout</code>: is the number of seconds you wish to wait for a response, before   assuming the target is unreachable.</li> <li><code>payload</code>: allows you to use a specific payload (bytes).</li> <li><code>count</code>: specify allows you to define how many ICMP packets to send.</li> <li><code>interval</code>: the time to wait between pings, in seconds.</li> <li><code>sweep_start</code> and <code>sweep_end</code>: allows you to perform a ping sweep, starting   from payload size defined in sweep_start and growing up to size defined in   sweep_end. Here, we repeat the payload you provided to match the desired size,   or we generate a random one if no payload was provided. Note that if you   defined size, these two fields will be ignored. df is a flag that, if set to   True, will enable the Don't Fragment flag in the IP header verbose enables the   verbose mode, printing output to a stream (see out) out is the target stream   of verbose mode. If you enable the verbose mode and do not provide out,   verbose output will be send to the sys.stdout stream. You may want to use a   file here.</li> <li><code>match</code>: is a flag that, if set to True, will enable payload matching between   a ping request and reply (default behaviour follows that of Windows which   counts a successful reply by a matched packet identifier only; Linux behaviour   counts a non equivalent payload with a matched packet identifier in reply as   fail, such as when pinging 8.8.8.8 with 1000 bytes and the reply is truncated   to only the first 74 of request payload with a matching packet identifier).</li> </ul>"}, {"location": "pythonping/#references", "title": "References", "text": "<ul> <li>Git</li> <li>ictshore article on pythonping</li> </ul>"}, {"location": "qbittorrent/", "title": "qBittorrent", "text": "<p>qBittorrent is my chosen client for Bittorrent.</p>"}, {"location": "qbittorrent/#installation", "title": "Installation", "text": "<p>Use binhex Docker</p> <ul> <li>Enable the announcement to all trackers</li> <li>Check the Advanced configurations</li> <li>I've tried the different Web UIs but none was of my licking.</li> <li>I've configured unpackerr to   unpack the compressed downloads.</li> </ul>"}, {"location": "qbittorrent/#migration-from-other-client", "title": "Migration from other client", "text": "<ul> <li>First install the service</li> <li>Make sure that the default download directory has all the downloaded data to   import.</li> <li>Then move all the <code>*.torrent</code> files from the old client to the torrent watch   directory.</li> </ul>"}, {"location": "qbittorrent/#python-interaction", "title": "Python interaction", "text": "<p>Use this library, you can see some examples here.</p>"}, {"location": "qbittorrent/#monitorization", "title": "Monitorization", "text": "<p>There's this nice prometheus exporter with it's graphana dashboard. With the information shown in the graphana dashboard it looks you can do alerts on whatever you want.</p> <p>When I have some time I'd like to monitor the next things:</p> <ul> <li>No <code>Forbidden client</code> string in the tracker messages. This happens when your   client is not whitelisted in one of the trackers.</li> <li>No <code>Unregistered torrent</code> string in the tracker messages. This happens when   the tracker has removed the torrent from their site, you can safely remove it   then.</li> <li>No torrent is in downloading state without receiving data for more than X   hours. This will mean that either the torrent is dead.</li> <li>If all downloading torrents are not receiving data for more than X hours could   mean that there is an error with your torrent client so that it can't   download.</li> <li>Configure the Hit and Run conditions per tracker to raise an alert if you   don't comply</li> <li>If you are not seeding during X hours it would mean that there is an error in   your application.</li> <li>Warn you when your buffer for a tracker is lower than X.</li> <li>Warn if a completed torrent is in a category for more than X hours, which will   mean that it wasn't automatically imported.</li> </ul>"}, {"location": "qbittorrent/#automatic-operation", "title": "Automatic operation", "text": "<p>I've found myself doing some micromanagement of the torrents that can probably be done by a program. For example:</p> <ul> <li>Remove the torrents of a category if their ratio is above X (not all   torrents).</li> <li>Remove torrents if your disk is getting full   in an intelligent way (torrents with most seeds first, allow or disallow the   removal of private tracker torrents, ...).</li> <li>For the trackers where you're building some ratio keep the interesting   torrents for a while until you build the desired buffer.</li> <li>Remove unregistered torrents</li> <li>Alert or remove the directories that are not being used by any active torrent.</li> </ul>"}, {"location": "qbittorrent/#client-recovery", "title": "Client recovery", "text": "<p>When your download client stops working and you can't recover it soon your heart gets a hiccup. You'll probably start browsing the private trackers webpages to see if you have a Hit and Run and if you can solve it before you get banned. If this happens while you're away from your infrastructure it can be even worse.</p> <p>Something you can do in these cases is to have another client configured so you can spawn it fast and import the torrents that are under the Hit and Run threat.</p>"}, {"location": "qbittorrent/#tools", "title": "Tools", "text": "<ul> <li>qbittools: a feature rich CLI for the management of torrents in qBittorrent. </li> <li>qbit_manage: tool will help manage tedious tasks in qBittorrent and automate them. </li> </ul>"}, {"location": "qbittorrent/#references", "title": "References", "text": "<ul> <li>Home</li> <li>Source</li> <li>FAQ</li> </ul>"}, {"location": "questionary/", "title": "questionary", "text": "<p>questionary is a Python library based on Prompt Toolkit to effortlessly building pretty command line interfaces. It makes it very easy to query your user for input.</p> <p></p>"}, {"location": "questionary/#installation", "title": "Installation", "text": "<pre><code>pip install questionary\n</code></pre>"}, {"location": "questionary/#usage", "title": "Usage", "text": ""}, {"location": "questionary/#asking-a-single-question", "title": "Asking a single question", "text": "<p>Questionary ships with a lot of different Question Types to provide the right prompt for the right question. All of them work in the same way though.</p> <pre><code>import questionary\n\nanswer = questionary.text(\"What's your first name\").ask()\n</code></pre> <p>Since our question is a text prompt, answer will contain the text the user typed after they submitted it.</p>"}, {"location": "questionary/#asking-multiple-questions", "title": "Asking Multiple Questions", "text": "<p>You can use the <code>form()</code> function to ask a collection of <code>Questions</code>. The questions will be asked in the order they are passed to <code>questionary.form</code>.</p> <pre><code>import questionary\n\nanswers = questionary.form(\n    first=questionary.confirm(\"Would you like the next question?\", default=True),\n    second=questionary.select(\"Select item\", choices=[\"item1\", \"item2\", \"item3\"]),\n).ask()\n</code></pre> <p>The output will have the following format:</p> <pre><code>{'first': True, 'second': 'item2'}\n</code></pre> <p>The <code>prompt()</code> function also allows you to ask a collection of questions, however instead of taking Question instances, it takes a dictionary:</p> <pre><code>import questionary\n\nquestions = [\n    {\n        \"type\": \"confirm\",\n        \"name\": \"first\",\n        \"message\": \"Would you like the next question?\",\n        \"default\": True,\n    },\n    {\n        \"type\": \"select\",\n        \"name\": \"second\",\n        \"message\": \"Select item\",\n        \"choices\": [\"item1\", \"item2\", \"item3\"],\n    },\n]\n\nquestionary.prompt(questions)\n</code></pre>"}, {"location": "questionary/#conditionally-skip-questions", "title": "Conditionally skip questions", "text": "<p>Sometimes it is helpful to be able to skip a question based on a condition. To avoid the need for an if around the question, you can pass the condition when you create the question:</p> <pre><code>import questionary\n\nDISABLED = True\nresponse = questionary.confirm(\"Are you amazed?\").skip_if(DISABLED, default=True).ask()\n</code></pre> <p>If the condition (in this case <code>DISABLED</code>) is <code>True</code>, the question will be skipped and the default value gets returned, otherwise the user will be prompted as usual and the default value will be ignored.</p>"}, {"location": "questionary/#exit-when-using-control-c", "title": "Exit when using control + c", "text": "<p>If you want the question to exit when it receives a <code>KeyboardInterrupt</code> event, use <code>unsafe_ask</code> instead of <code>ask</code>.</p>"}, {"location": "questionary/#question-types", "title": "Question types", "text": "<p>The different question types are meant to cover different use cases. The parameters and configuration options are explained in detail for each type. But before we get into to many details, here is a cheatsheet with the available question types:</p> <ul> <li> <p>Use   <code>Text</code>   to ask for free text input.</p> </li> <li> <p>Use   <code>Password</code>   to ask for free text where the text is hidden.</p> </li> <li> <p>Use   <code>File   Path</code>   to ask for a file or directory path with autocompletion.</p> </li> <li> <p>Use   <code>Confirmation</code>   to ask a yes or no question.</p> </li> </ul> <pre><code>&gt;&gt;&gt; questionary.confirm(\"Are you amazed?\").ask()\n? Are you amazed? Yes\nTrue\n</code></pre> <ul> <li> <p>Use   <code>Select</code>   to ask the user to select one item from a beautiful list.</p> </li> <li> <p>Use   <code>Raw   Select</code>   to ask the user to select one item from a list.</p> </li> <li> <p>Use   <code>Checkbox</code>   to ask the user to select any number of items from a list.</p> </li> <li> <p>Use   <code>Autocomplete</code>   to ask for free text with autocomplete help.</p> </li> </ul> <p>Check the examples to see them in action and how to use them.</p>"}, {"location": "questionary/#autocomplete-answers", "title": "Autocomplete answers", "text": "<p>If you want autocomplete with fuzzy finding use:</p> <pre><code>import questionary\nfrom prompt_toolkit.completion import FuzzyWordCompleter\n\nquestionary.autocomplete(\n    \"Save to (q to cancel): \",\n    choices=destination_directories,\n    completer=FuzzyWordCompleter(destination_directories),\n).ask()\n</code></pre>"}, {"location": "questionary/#styling", "title": "Styling", "text": ""}, {"location": "questionary/#dont-highlight-the-selected-option-by-default", "title": "Don't highlight the selected option by default", "text": "<p>If you don't want to highlight the default choice in the <code>select</code> question use the next style:</p> <pre><code>from questionary import Style\n\nchoice = select(\n    \"Question title: \",\n    choices=[\"a\", \"b\", \"c\"],\n    default=\"a\",\n    style=Style([(\"selected\", \"noreverse\")]),\n).ask()\n</code></pre>"}, {"location": "questionary/#testing", "title": "Testing", "text": "<p>To test questionary code, follow the guidelines of testing prompt_toolkit.</p>"}, {"location": "questionary/#references", "title": "References", "text": "<ul> <li>Docs</li> <li>Git</li> </ul>"}, {"location": "qwik/", "title": "Qwik", "text": "<p>Qwik is a new kind of web framework that can deliver instantly load web applications at any size or complexity. Your sites and apps can boot with about 1kb of JS (regardless of application complexity), and achieve consistent performance at scale.</p> <p>You can see a good overview in the Qwik presentation.</p>"}, {"location": "qwik/#references", "title": "References", "text": "<ul> <li>Home</li> <li>Git</li> </ul>"}, {"location": "ram/", "title": "RAM", "text": "<p>RAM is a form of computer memory that can be read and changed in any order, typically used to store working data and machine code.</p> <p>A random-access memory device allows data items to be read or written in almost the same amount of time irrespective of the physical location of data inside the memory, in contrast with other direct-access data storage media (such as hard disks), where the time required to read and write data items varies significantly depending on their physical locations on the recording medium, due to mechanical limitations such as media rotation speeds and arm movement.</p> <p>They are the faster device either to read or to write your data.</p>"}, {"location": "ram/#properties", "title": "Properties", "text": "<p>RAM sticks vary on:</p> <ul> <li> <p>Size: the amount of data that they can hold, measured in GB.</p> </li> <li> <p>Frequency: how often the RAM is accessed per second, measured in MHz.</p> </li> <li> <p>Clock latency (CL): number of cycles before the RAM responds.</p> </li> <li> <p>Type: as the technology evolves there are different types, such as DDR4.</p> </li> <li> <p>Form factor: There are different types of RAM in regards of the devices   they'll fit in:</p> </li> <li> <p>260-pin SO-DIMMs: laptop RAM, shorter, slower, more expensive, and won't fit     in a desktop system.</p> </li> <li> <p>288-pin DIMMs: desktop RAM - required for most desktop motherboards.</p> </li> <li> <p>ECC: Whether it has error correction code.</p> </li> </ul>"}, {"location": "ram/#speed", "title": "Speed", "text": "<p>RAM's speed is measured as a combination of frequency and clock latency. More cycles per second means the RAM is 'faster', but you also have to consider latency as well. If you compare MHz and CL, you can get an idea of actual speed. For example, 3600 MHz CL18 and 3200 MHz CL16 are the same speed on paper since the faster 3600 MHz takes more clocks to respond, but there are more clocks per second, so the response time is actually the same.</p> <p>!!! note In reality, faster RAM will be a little bit faster in modern architectures. Also, Ryzen specifically prefers 3600 MHz RAM because of how its FCLK works it likes whole-number multipliers, so if it can run at 1800 MHz (x2 = 3600 MHz with the RAM), then it will run 2-3% faster than equivalent 3200 MHz RAM.</p> <p>Summing up, the higher the speed, and the lower the CL, the better the overall performance.</p> <pre><code>RAM latency (lower the better) = (CAS Latency (CL) x 2000 ) / Frequency (MHz)\n</code></pre>"}, {"location": "ram/#ecc", "title": "ECC", "text": "<p>Error correction code memory (ECC memory) is a type of computer data storage that uses an error correction code to detect and correct n-bit data corruption which occurs in memory. ECC memory is used in most computers where data corruption cannot be tolerated, for example when using zfs.</p>"}, {"location": "ram/#how-to-choose-a-ram-stick", "title": "How to choose a RAM stick", "text": ""}, {"location": "ram/#cpu-brand", "title": "CPU brand", "text": "<p>Depending on your CPU brand you need to take into account the next advices:</p> <ul> <li>Intel: Intel CPUs aren\u2019t massively reliant on the performance of memory while   running, which might explain why RAM speed support has historically been   rather limited outside of Intel\u2019s enthusiast chipsets (Z-Series) and capped to   2666Mhz (at least until recently).</li> </ul> <p>If you\u2019re the owner of an Intel CPU we certainly suggest getting a good   quality RAM kit, but the speed of that RAM isn\u2019t as important. Save your money   for other components or a RAM capacity upgrade if required.</p> <ul> <li>AMD: In stark contrast to Intel, AMD\u2019s more recent \u2018Zen\u2019 line of CPUs has RAM   speed almost baked into the architecture of the CPU.</li> </ul> <p>AMD\u2019s infinity fabric technology uses the speed of the RAM to pass information   across sections of the CPU. This means that better memory will serve to boost   the CPU performance as well as helping in those intense applications we   mentioned earlier.</p>"}, {"location": "ram/#motherboard", "title": "Motherboard", "text": "<p>Many manufacturers list specific RAM kits as \u2018verified\u2019 with their products, meaning that the manufacturer has tested the motherboard model in question with a specific RAM kit and has confirmed full support for that kit, at its advertised speed and CAS latency.</p> <p>Try to purchase RAM listed on your motherboard\u2019s QVL where possible, for the best compatibility. However, this is almost always impractical given the availability of exact RAM kits at any given time.</p>"}, {"location": "ram/#achieving-stability", "title": "Achieving stability", "text": "<p>Speed, CAS latency, module size, and module quantity; in order to avoid running into problems you should balance these factors when considering your purchase.</p> <p>For example, 16GB of 3600MHz CL16 memory is much more likely to be stable than 32GB of the same modules, even if the settings in BIOS remain the same.</p> <p>Consider another example - you may want to run 64GB of RAM at 3600MHz, but to get it to run properly you need to lower the speed to 3000MHz.</p>"}, {"location": "ram/#conclusion", "title": "Conclusion", "text": "<p>In summary, a high-performance 3600MHz memory kit is ideal for AMD Ryzen CPUs. Decide the size, speed, if you need ECC and make sure which type of RAM does your CPU and motherboard combo support (ie, DDR3, DDR4, or DDR5), and that you're choosing the correct form factor. Then, buy a kit that is in line with your budget.</p> <p>You're probably looking for DDR4, probably <code>2x8 = 16 GB</code>. The sweet spot there will likely be 3600 MHz CL18 or 3200 MHz CL16 for $55 or so. Technically, you should check your motherboard's QVL (list of RAM that is guaranteed to work), but most big-name brands should work. There are other things to consider - like, does your cooler interfere with RAM? But, generally only top-tier coolers have RAM fitment issues.</p>"}, {"location": "ram/#references", "title": "References", "text": "<ul> <li>How to choose RAM: Speed vs Capacity</li> </ul>"}, {"location": "redox/", "title": "Redox", "text": "<p>Redox</p>"}, {"location": "redox/#installation", "title": "Installation", "text": "<p>First flash:</p> <p>Download the hex from the via website</p> <p>Try to flash it many times reseting the promicros.</p> <pre><code>sudo avrdude -b 57600 -p m32u4 -P /dev/ttyACM0 -c avr109 -D -U flash:w:redox_rev1_base_via.hex\n</code></pre> <p>Once the write has finished install via:</p> <p>https://github.com/the-via/releases/releases</p> <p>Reboot the computer</p> <p>launch it with <code>via-nativia</code>.</p>"}, {"location": "redox/#references", "title": "References", "text": "<ul> <li>Git</li> </ul>"}, {"location": "refinement_template/", "title": "Refinement Template", "text": ""}, {"location": "refinement_template/#refinement", "title": "Refinement", "text": ""}, {"location": "refinement_template/#doubts", "title": "Doubts", "text": ""}, {"location": "refinement_template/#expected-current-sprint-undone-tasks", "title": "Expected current sprint undone tasks", "text": ""}, {"location": "refinement_template/#review-the-proposed-kanban-board", "title": "Review the proposed Kanban board", "text": ""}, {"location": "refinement_template/#sprint-goals", "title": "Sprint Goals", "text": "<p>With this proposed plan we'll:</p> <p>*</p>"}, {"location": "regicide/", "title": "Regicide", "text": "<p>Regicide is a wonderful cooperative card game for 1 to 4 players. It's awesome how they've built such a rich game dynamic with a normal deck of cards. Even if you can play it with any deck, I suggest to buy the deck they sell because their cards are magnificent and they deserve the money for their impressive game. Another thing I love about them is that even if you can't or don't want to pay for the game, they give the rules for free.</p> <p>If you don't like reading the rules directly from their pdf (although it's quite short), they explain them in this video.</p>"}, {"location": "regicide/#variations", "title": "Variations", "text": "<p>Each game is different and challenging despite your experience, even so, I've thought that to make it even more varied and rich, the players can use one or many of the next expansions:</p> <ul> <li>Situation modifiers.</li> <li>Player modifiers.</li> </ul> <p>Each of the expansions make the game both more different and more complex, so it's not suggested to use them all at the same time, try one, and once the players are used to it, add another one.</p> <p>These expansions are yet untested ideas, so they might break the playability.</p> <p>If you have any suggestion please contact me or open a pull request.</p> <p>Throughout the next sections you'll encounter the <code>1 x level</code> notation to define the times an action will take place. It's assumed that the level of the enemies is:</p> Card Level J 1 Q 2 K 3"}, {"location": "regicide/#situation-modifiers", "title": "Situation modifiers", "text": "<p>You can spice up each game by making each round different by applying situation modifiers. Once a new enemy arrives the scene, roll up a dice to choose one of the next situations:</p> <ol> <li>Disloyal: The fighters you use in this round that match the enemy's suit will    be disloyal to you and will join the enemy's ranks in the next round. The    player will receive damage of both the enemy and their minions. Players will    need to kill their minions before they hit the royal enemy.</li> <li>Cornered: You're cornered and the enemy archers are firing you. At the start of     each player turn, it takes <code>2 x level</code> amount of damage.</li> <li>Exterminator: When it deals damage, <code>1 x level</code> of the player discarded    cards are taken out of the game.</li> <li>Enemy ambush: When the enemy enters the scene, it deals <code>2 x level</code> amount of     damage to the players.</li> <li>Enemy Spy: It has moles inside your ranks. When it comes to scene, you     drop <code>1 x level</code> amount of cards of that suit.</li> <li>Necromancer: When it hits the first player, the smallest discarded card    goes to the enemy ranks instead of the discarded pile. Players need to kill    this minion before hitting the enemy.</li> <li>Represor Enemy: It kills <code>1 x level</code> people from the tavern at the start of each     player's turn.</li> <li>Dexterous Enemy: It has a <code>20% x level</code> of chances to dodge the player's    attack.</li> <li>Quick Enemy: The enemy hits you in the first phase of your turn, instead of the last.</li> <li>Stronger Enemy: It deals <code>2 x level</code> more damage.</li> <li> <p>Tougher Enemy: It has <code>3 x level</code> more health.</p> </li> <li> <p>Blind Enemy: It attacks to any player that makes a noise in addition to the    player that is currently fighting it.</p> </li> <li>Random Enemy: There is no logic in it's actions, instead of attacking the    player who is playing, it attacks a random one.</li> <li> <p>Uneventful round: Nothing happens, play the round as the vanilla game.</p> </li> <li> <p>Softer Enemy: It has <code>3 x level</code> less health.</p> </li> <li>Weaker Enemy: It deals <code>2 x level</code> less damage.</li> <li>Clumsy Enemy: It has a <code>20% x level</code> of chances to fail when hitting the    players.</li> <li>Resistance Spies: You have moles inside the enemy ranks that removes their    suit invulnerability.</li> <li>Ambush: When the enemy enters the scene, you all deal <code>2 x level</code> amount of     damage to the enemy.</li> <li>Communist/Anarchist \"enemy\": It really is on your side to bring down the    monarchy, so you all get a <code>1 x level</code> to all the cards you play, and a <code>2    x level</code> to it's suit.</li> <li>Enemy Cornered: You cornered the enemy, and your archers are firing them. At the start of     each player turn, the enemy takes <code>2 x level</code> amount of damage.</li> <li>Loyal: The fighters you use in this round that match the enemy's suit will    be loyal to you and won't be moved to the discarded pile at the end of the    round. On the first round of each player, they'll use both the card in the    table and the one that they use.</li> </ol>"}, {"location": "regicide/#player-modifiers", "title": "Player modifiers", "text": "<p>At the start of the game players can decide their suit, they will get a bonus on the played cards of their suit, and a penalization on the opposite suit. The opposite suits are:</p> <ul> <li>\u2660 opposite of \u2665</li> <li>\u2663 opposite of \u2666</li> </ul> <p>The bonus depends on the level of the enemy being:</p> <ul> <li>J: +1 or -1</li> <li>Q: +2 or -2</li> <li>K: +3 or -3</li> </ul> <p>Imagine that I've chosen \u2666 as my suit, if I were to play:</p> <ul> <li>The 8\u2666 against a J\u2665, I'd draw <code>8+1</code> cards from the deck, and deal <code>8+1</code> damage</li> <li>The 7\u2663 against a Q\u2660, I'd deal 10 of damage <code>(7-2) * 2</code>.</li> <li>The 4\u26664\u2665 against a K\u2663, I'd heal and draw 11 cards <code>(4+4+3)</code>.</li> <li>The 4\u26664\u2663 against a K\u2660, I'd draw 8 cards <code>(4+4+3-3)</code> and deal 16 of damage.</li> </ul> <p>I haven't decide yet if the bonus should apply at the time of receiving damage, we played one without counting and it was playable, will test the other soon.</p>"}, {"location": "regicide/#player-classes", "title": "Player classes", "text": "<ul> <li> <p>Create player classes: Each player class has abilities that can use <code>X</code> amount     of times in the match:</p> <ul> <li>Archer:</li> <li>Healer:</li> <li>Paladin:</li> <li>Warrior:</li> <li>Wizard:</li> </ul> </li> </ul>"}, {"location": "regicide/#references", "title": "References", "text": "<ul> <li>Homepage</li> </ul>"}, {"location": "relationship_management/", "title": "Relationship Management", "text": "<p>I try to keep my mind as empty as possible of non relevant processes, that's why I use a task manager to handle my tasks and meetings. This system has a side effect, if there isn't something reminding you that you have to do something, you fail to do it. That principle applied to human relationships means that if you don't stumble that person in your daily life, it doesn't exist for you and you will probably not take enough care that the person deserves.</p> <p>To solve that problem I started creating periodic tasks to call these people or hang out. I've also used those tasks to keep a diary of the interactions.</p> <p>Recently I've found Monica a popular open source personal CRM that helps in the same direction.</p> <p>So I'm going to migrate all my information to the system and see how it goes.</p>"}, {"location": "remote_work/", "title": "Remote working", "text": "<p>Remote working is a work arrangement in which employees do not commute or travel (e.g. by bus, bicycle or car, etc.) to a central place of work, such as an office building, warehouse, or store.</p> <p>As a side effect, we're spending a lot of time in front of our computers, so we should be careful that our working environment helps us to stay healthy. For example we could:</p> <ul> <li> <p>Use an external monitor: Your laptop's screen is usually not big enough and     will force you to look down instead of look straight which can lead to neck     pain. Some prefer super big monitors (48 inches) while others feel that 24     inches is more than enough so you don't have to turn your head to reach each     side of the screen. For me the sweet spot is having two terminals with 100     characters of width one beside the other. If you use a tiling window manager     like i3wm, that should be enough.</p> <p>Some people valued that the screen was not fixed, so it could be tilted or it's height could be changed.</p> </li> <li> <p>Adjust the screen to your eye level: The center of the monitor should be at     eye level, if the monitor height adjustment is not enough, you can use some     old books or buy a screen support.</p> </li> <li> <p>Use an external keyboard: Sometimes the keys of the laptop keyboards have     a cheap feedback or a weird key disposition, which leads to finger and wrist     aches.  The use of an external keyboard (better if it's a mechanical one)     can help with this issue.</p> </li> <li> <p>The chair should support your back and don't be too hard to hurt your butt, nor too     soft.</p> </li> <li> <p>Your legs should not be hanging in the air, that will add unnecessary pressure on your     thighs which can lead to tingling. If you're in this situation, a leg     support comes handy.</p> <p>The legs shouldn't be crossed either in front or below you, they should be straight with a 90 degree angle between your thighs and your calves, with a waist level separation between the feet.</p> </li> <li> <p>The table height should be enough to have a 90 degree angle between your     forearms and your biceps , and your shoulders are in a relaxed stance. Small     people may need a table with no drawers between your elbows and your legs,     or you wont be able to fulfill the arm's requirement.</p> <p>The table height should be low enough to fulfill the leg's requirement above. Sometimes they are too high to be able to have a 90 degree angle between the thighs and calves even with feet support, in that case, change the desk or cut it's legs.</p> </li> <li> <p>Think about using a standing desk. Desk's with variable height are quite     expensive, but there is always the option to buy a laptop support that let's     you stand.</p> </li> <li> <p>Your hands should be at the same level as your forearms, you could use a wrist     support for that and also to soften the contact of your forearms with the     desk.</p> </li> <li> <p>If you're a heavy mouse user, think of using a vertical mouse instead of the     traditional to prevent the metacarpal syndrome. And try not to use it! learn     how to use a tiling window manager and Vim shortcuts for everything, such as     using tridactyl for Firefox.</p> </li> <li> <p>Keep your working place between 19 and 21 degrees centigrades, otherwise you     may unawarely contract your body.</p> </li> <li> <p>Use blue filter either in your glasses or in your screen.</p> </li> <li> <p>Have enough light so you don't need to strain your eyes. Having your monitor     screen as your only source of light is harmful.</p> </li> <li> <p>Try to promote initiatives that increase the social interaction between your     fellow workers.</p> </li> <li> <p>Stand up and walk around at least once each two hours. Meetings are a good     moment to do an outside walk.</p> </li> </ul> <p>Other tips non related with your work environment but with the consequences of your work experience can be:</p> <ul> <li> <p>Don't remain static, doing exercise daily is a must. As you don't need to go     out, it's quite easy to fall into the routine of waking up, sit in your     computer, eat and go back to sleep. Both anaerobic (pilates, yoga or     stretching) and aerobic (running, biking or dancing) give different     benefits.</p> </li> <li> <p>Drink enough water, around 8 to 10 glasses per day.</p> </li> <li> <p>Use the extra time that remote working gives you to strengthen your outside     work social relationships.</p> </li> <li> <p>Try not to be exposed to any screen light for an hour before you go to sleep.     If you use an e-book, don't rely on their builtin light, use an external     source instead.</p> </li> </ul> <p>If you have a remote work contract, make sure that your employer pays for any upgrades, it's their responsibility.</p>"}, {"location": "renovate/", "title": "Renovate", "text": "<p>Renovate is a program that does automated dependency updates. Multi-platform and multi-language.</p> <p>Why use Renovate?</p> <ul> <li>Get pull requests to update your dependencies and lock files.</li> <li>Reduce noise by scheduling when Renovate creates PRs.</li> <li>Renovate finds relevant package files automatically, including in monorepos.</li> <li>You can customize the bot's behavior with configuration files.</li> <li>Share your configuration with ESLint-like config presets.</li> <li>Get replacement PRs to migrate from a deprecated dependency to the community     suggested replacement (npm packages only).</li> <li>Open source.</li> <li>Popular (more than 9.7k stars and 1.3k forks)</li> <li>Beautifully integrate with main Git web applications (Gitea, Gitlab, Github).</li> <li>It supports most important languages: Python, Docker, Kubernetes, Terraform,     Ansible, Node, ...</li> </ul>"}, {"location": "renovate/#behind-the-scenes", "title": "Behind the scenes", "text": ""}, {"location": "renovate/#how-renovate-updates-a-package-file", "title": "How Renovate updates a package file", "text": "<p>Renovate:</p> <ul> <li>Scans your repositories to detect package files and their dependencies.</li> <li>Checks if any newer versions exist.</li> <li>Raises Pull Requests for available updates.</li> </ul> <p>The Pull Requests patch the package files directly, and include Release Notes for the newer versions (if they are available).</p> <p>By default:</p> <ul> <li>You'll get separate Pull Requests for each dependency.</li> <li>Major updates are kept separate from non-major updates.</li> </ul>"}, {"location": "renovate/#references", "title": "References", "text": "<ul> <li>Docs</li> </ul>"}, {"location": "requests/", "title": "Requests", "text": "<p>Requests is an elegant and simple HTTP library for Python, built for human beings.</p>"}, {"location": "requests/#installation", "title": "Installation", "text": "<pre><code>pip install requests\n</code></pre>"}, {"location": "requests/#usage", "title": "Usage", "text": ""}, {"location": "requests/#download-file", "title": "Download file", "text": "<pre><code>url = \"http://beispiel.dort/ichbineinbild.jpg\"\nfilename = url.split(\"/\")[-1]\nr = requests.get(url, timeout=0.5)\n\nif r.status_code == 200:\n    with open(filename, 'wb') as f:\n        f.write(r.content)\n</code></pre>"}, {"location": "requests/#encode-url", "title": "Encode url", "text": "<pre><code>requests.utils.quote('/test', safe='')\n</code></pre>"}, {"location": "requests/#get", "title": "Get", "text": "<pre><code>requests.get('{{ url }}')\n</code></pre>"}, {"location": "requests/#put-url", "title": "Put url", "text": "<pre><code>requests.put({{ url }})\n</code></pre>"}, {"location": "requests/#put-json-data-url", "title": "Put json data url", "text": "<pre><code>data = {\"key\": \"value\"}\nrequests.put({{ url }} json=data)\n</code></pre>"}, {"location": "requests/#use-cookies-between-requests", "title": "Use cookies between requests", "text": "<p>You can use Session objects to persists cookies or default data across all requests.</p> <pre><code>s = requests.Session()\n\ns.get('https://httpbin.org/cookies/set/sessioncookie/123456789')\nr = s.get('https://httpbin.org/cookies')\n\nprint(r.text)\n# '{\"cookies\": {\"sessioncookie\": \"123456789\"}}'\n\ns.auth = ('user', 'pass')\ns.headers.update({'x-test': 'true'})\n\n# both 'x-test' and 'x-test2' are sent\ns.get('https://httpbin.org/headers', headers={'x-test2': 'true'})\n</code></pre>"}, {"location": "requests/#use-a-proxy", "title": "Use a proxy", "text": "<pre><code>http_proxy  = \"http://10.10.1.10:3128\"\nhttps_proxy = \"https://10.10.1.11:1080\"\nftp_proxy   = \"ftp://10.10.1.10:3128\"\n\nproxies = { \n              \"http\"  : http_proxy, \n              \"https\" : https_proxy, \n              \"ftp\"   : ftp_proxy\n            }\n\nr = requests.get(url, headers=headers, proxies=proxies)\n</code></pre>"}, {"location": "requests/#references", "title": "References", "text": "<ul> <li>Docs</li> </ul>"}, {"location": "retroarch/", "title": "retroarch", "text": ""}, {"location": "retroarch/#installation", "title": "Installation", "text": "<p>To add the stable branch to your system type:</p> <pre><code>sudo add-apt-repository ppa:libretro/stable\nsudo apt-get update\nsudo apt-get install retroarch\n</code></pre> <p>Go to Main Menu/Online Updater and then update everything you can:</p> <ul> <li>Update Core Info Files</li> <li>Update Assets</li> <li>Update controller Profiles</li> <li>Update Databases</li> <li>Update Overlays</li> <li>Update GLSL Shaders</li> </ul>"}, {"location": "retroarch/#troubleshooting", "title": "Troubleshooting", "text": ""}, {"location": "retroarch/#icons-dont-show-up", "title": "Icons don't show up", "text": "<p>Dive through the menus until you find a way to update the assets</p>"}, {"location": "retroarch/#references", "title": "References", "text": "<ul> <li>Home</li> <li>Docs</li> </ul>"}, {"location": "rich/", "title": "Rich", "text": "<p>Rich is a Python library for rich text and beautiful formatting in the terminal.</p>"}, {"location": "rich/#installation", "title": "Installation", "text": "<pre><code>pip install rich\n</code></pre>"}, {"location": "rich/#usage", "title": "Usage", "text": ""}, {"location": "rich/#progress-display", "title": "Progress display", "text": "<p>Rich can display continuously updated information regarding the progress of long running tasks / file copies etc. The information displayed is configurable, the default will display a description of the \u2018task\u2019, a progress bar, percentage complete, and estimated time remaining.</p> <p>Rich progress display supports multiple tasks, each with a bar and progress information. You can use this to track concurrent tasks where the work is happening in threads or processes.</p> <p>It's beautiful, check it out with <code>python -m rich.progress</code>.</p>"}, {"location": "rich/#basic-usage", "title": "Basic Usage", "text": "<p>For basic usage call the track() function, which accepts a sequence (such as a list or range object) and an optional description of the job you are working on. The track method will yield values from the sequence and update the progress information on each iteration. Here\u2019s an example:</p> <pre><code>from rich.progress import track\n\nfor n in track(range(n), description=\"Processing...\"):\n    do_work(n)\n</code></pre>"}, {"location": "rich/#tables", "title": "Tables", "text": "<pre><code>from rich.console import Console\nfrom rich.table import Table\n\ntable = Table(title=\"Star Wars Movies\")\n\ntable.add_column(\"Released\", justify=\"right\", style=\"cyan\", no_wrap=True)\ntable.add_column(\"Title\", style=\"magenta\")\ntable.add_column(\"Box Office\", justify=\"right\", style=\"green\")\n\ntable.add_row(\"Dec 20, 2019\", \"Star Wars: The Rise of Skywalker\", \"$952,110,690\")\ntable.add_row(\"May 25, 2018\", \"Solo: A Star Wars Story\", \"$393,151,347\")\ntable.add_row(\"Dec 15, 2017\", \"Star Wars Ep. V111: The Last Jedi\", \"$1,332,539,889\")\ntable.add_row(\"Dec 16, 2016\", \"Rogue One: A Star Wars Story\", \"$1,332,439,889\")\n\nconsole = Console()\nconsole.print(table)\n</code></pre>"}, {"location": "rich/#rich-text", "title": "Rich text", "text": "<pre><code>from rich.console import Console\nfrom rich.text import Text\n\nconsole = Console()\ntext = Text.assemble((\"Hello\", \"bold magenta\"), \" World!\")\nconsole.print(text)\n</code></pre>"}, {"location": "rich/#live-display-text", "title": "Live display text", "text": "<pre><code>import time\n\nfrom rich.live import Live\n\nwith Live(\"Test\") as live:\n    for row in range(12):\n        live.update(f\"Test {row}\")\n        time.sleep(0.4)\n</code></pre> <p>If you don't want the text to have the default colors, you can embed it all in a <code>Text</code> object.</p>"}, {"location": "rich/#tree", "title": "Tree", "text": "<p>Rich has a <code>Tree</code> class which can generate a tree view in the terminal. A tree view is a great way of presenting the contents of a filesystem or any other hierarchical data. Each branch of the tree can have a label which may be text or any other Rich renderable.</p> <p>The following code creates and prints a tree with a simple text label:</p> <pre><code>from rich.tree import Tree\nfrom rich import print\n\ntree = Tree(\"Rich Tree\")\nprint(tree)\n</code></pre> <p>With only a single <code>Tree</code> instance this will output nothing more than the text \u201cRich Tree\u201d. Things get more interesting when we call <code>add()</code> to add more branches to the <code>Tree</code>. The following code adds two more branches:</p> <pre><code>tree.add(\"foo\")\ntree.add(\"bar\")\nprint(tree)\n</code></pre> <p>The <code>tree</code> will now have two branches connected to the original tree with guide lines.</p> <p>When you call <code>add()</code> a new <code>Tree</code> instance is returned. You can use this instance to add more branches to, and build up a more complex tree. Let\u2019s add a few more levels to the tree:</p> <pre><code>baz_tree = tree.add(\"baz\")\nbaz_tree.add(\"[red]Red\").add(\"[green]Green\").add(\"[blue]Blue\")\nprint(tree)\n</code></pre>"}, {"location": "rich/#references", "title": "References", "text": "<ul> <li>Git</li> <li>Docs</li> </ul>"}, {"location": "rtorrent/", "title": "Rtorrent", "text": ""}, {"location": "rtorrent/#debugging", "title": "Debugging", "text": "<ul> <li>Get into the docker with <code>docker exec -it docker_name bash</code></li> <li><code>cd /home/nobody</code></li> <li> <p>Open the <code>rtorrent.sh</code> file add <code>set -x</code> above the line you think is starting your <code>rtorrent</code> and <code>set +x</code> below to fetch the command that is launching your rtorrent instance, for example:</p> <pre><code>/usr/bin/tmux new-session -d -s rt -n rtorrent /usr/bin/rtorrent -b 12.5.232.12 -o ip=232.234.324.211\n</code></pre> </li> </ul> <p>If you manually run <code>/usr/bin/rtorrent -b 12.5.232.12 -o ip=232.234.324.211</code> you'll get more information on why <code>rtorrent</code> is not starting.</p>"}, {"location": "sanoid/", "title": "Sanoid", "text": "<p>Sanoid is the most popular tool right now, with it you can create, automatically thin, and monitor snapshots and pool health from a single eminently human-readable TOML config file at <code>/etc/sanoid/sanoid.conf</code>. Sanoid also requires a \"defaults\" file located at /etc/sanoid/sanoid.defaults.conf, which is not user-editable. A typical Sanoid system would have a single cron job:</p> <p><pre><code>* * * * * TZ=UTC /usr/local/bin/sanoid --cron\n</code></pre> Note: Using UTC as timezone is recommend to prevent problems with daylight saving times</p> <p>And its <code>/etc/sanoid/sanoid.conf</code> might look something like this:</p> <pre><code>[data/home]\nuse_template = production\n[data/images]\nuse_template = production\nrecursive = yes\nprocess_children_only = yes\n[data/images/win7]\nhourly = 4\n\n#############################\n# templates below this line #\n#############################\n\n[template_production]\nfrequently = 0\nhourly = 36\ndaily = 30\nmonthly = 3\nyearly = 0\nautosnap = yes\nautoprune = yes\n</code></pre> <p>Which would be enough to tell <code>sanoid</code> to take and keep 36 hourly snapshots, 30 dailies, 3 monthlies, and no yearlies for all datasets under <code>data/images</code> (but not <code>data/images</code> itself, since <code>process_children_only</code> is set). Except in the case of <code>data/images/win7</code>, which follows the same template (since it's a child of <code>data/images</code>) but only keeps 4 hourlies for whatever reason.</p> <p>For more full details on sanoid.conf settings see their wiki page</p> <p>The monitorization is designed to be done with Nagios, although there is some work in progress to add Prometheus metrics and there is an exporter</p> <p>What I like of <code>sanoid</code>:</p> <ul> <li>It's popular</li> <li>It has hooks to run your scripts at various stages in the lifecycle of a snapshot.</li> <li>It also handles the process of sending the backups to other locations with <code>syncoid</code></li> <li>It lets you search on all changes of a given file (or folder) over all available snapshots. This is useful in case you need to recover a file or folder but don't want to rollback an entire snapshot. with <code>findoid</code> (although when I used it it gave me an error :/)</li> <li>It's in the official repos</li> </ul> <p>What I don't like:</p> <ul> <li>Last release is almost 2 years ago.</li> <li>The last commit to <code>master</code> is done a year ago.</li> <li>It's made in Perl</li> </ul>"}, {"location": "sanoid/#installation", "title": "Installation", "text": "<p>The tool is in the official repositories so:</p> <pre><code>sudo apt-get install sanoid\n</code></pre> <p>You can find the example config file at <code>/usr/share/doc/sanoid/examples/sanoid.conf</code> and can copy it to <code>/etc/sanoid/sanoid.conf</code></p> <pre><code>mkdir /etc/sanoid/\ncp /usr/share/doc/sanoid/examples/sanoid.conf /etc/sanoid/sanoid.conf\ncp /usr/share/sanoid/sanoid.defaults.conf /etc/sanoid/sanoid.defaults.conf\n</code></pre> <p>Edit <code>/etc/sanoid/sanoid.conf</code> to suit your needs. The <code>/etc/sanoid/sanoid.defaults.conf</code> file contains the default values and should not be touched, use it only for reference.</p> <p>An example of a configuration can be:</p> <pre><code>######################\n# Filesystem Backups #\n######################\n\n[main/backup]\nuse_template = daily\nrecursive = yes\n[main/lyz]\nuse_template = frequent\n\n#############\n# Templates #\n#############\n\n[template_daily]\ndaily = 30\nmonthly = 6\n\n[template_frequent]\nfrequently = 4\nhourly = 25\ndaily = 30\nmonthly = 6\n</code></pre> <p>During installation from the Debian repositories, the <code>systemd</code> timer unit <code>sanoid.timer</code> is created which is set to run <code>sanoid</code> every 15 minutes. Therefore there is no need to create an entry in crontab. Having a crontab entry in addition to the <code>sanoid.timer</code> will result in errors similar to <code>cannot create snapshot '&lt;pool&gt;/&lt;dataset&gt;@&lt;snapshot&gt;': dataset already exists</code>.</p> <p>By default, the <code>sanoid.timer</code> timer unit runs the <code>sanoid-prune</code> service followed by the <code>sanoid</code> service. To edit any of the command-line options, you can edit these service files (<code>/lib/systemd/system/sanoid.timer</code>).</p> <p>Also <code>recursive</code> is not set by default, so the dataset's children won't be backed up unless you set this option.</p>"}, {"location": "sanoid/#usage", "title": "Usage", "text": "<p><code>sanoid</code> runs in the back with the <code>systemd</code> service, so there is nothing you need to do for it to run.</p> <p>To check the logs use <code>journalctl -eu sanoid</code>.</p> <p>To manage the snapshots look at the <code>zfs</code> article.</p>"}, {"location": "sanoid/#prune-snapshots", "title": "Prune snapshots", "text": "<p>If you want to manually prune the snapshots after you tweaked <code>sanoid.conf</code> you can run:</p> <pre><code>sanoid --prune-snapshots\n</code></pre>"}, {"location": "sanoid/#syncoid", "title": "Syncoid", "text": "<p><code>Sanoid</code> also includes a replication tool, <code>syncoid</code>, which facilitates the asynchronous incremental replication of ZFS filesystems. A typical <code>syncoid</code> command might look like this:</p> <pre><code>syncoid data/images/vm backup/images/vm\n</code></pre> <p>Which would replicate the specified ZFS filesystem (aka dataset) from the data pool to the backup pool on the local system, or</p> <pre><code>syncoid data/images/vm root@remotehost:backup/images/vm\n</code></pre> <p>Which would push-replicate the specified ZFS filesystem from the local host to remotehost over an SSH tunnel, or</p> <pre><code>syncoid root@remotehost:data/images/vm backup/images/vm\n</code></pre> <p>Which would pull-replicate the filesystem from the remote host to the local system over an SSH tunnel. In case of doubt using the pull strategy is always desired</p> <p><code>Syncoid</code> supports recursive replication (replication of a dataset and all its child datasets) and uses mbuffer buffering, lzop compression, and pv progress bars if the utilities are available on the systems used. If ZFS supports resumeable send/receive streams on both the source and target those will be enabled as default. It also automatically supports and enables resume of interrupted replication when both source and target support this feature.</p>"}, {"location": "sanoid/#configuration", "title": "Configuration", "text": ""}, {"location": "sanoid/#syncoid-configuration-caveats", "title": "Syncoid configuration caveats", "text": "<p>One key point is that pruning is not done by <code>syncoid</code> but only and always by <code>sanoid</code>. This means <code>sanoid</code> has to be run on the backup datasets as well, but without creating snapshots, only pruning (as set in the template).</p> <p>Also, the template is called <code>template_something</code> and only <code>something</code> must be use with <code>use_template</code>.</p> <p><pre><code>[SAN200/projects]\nuse_template = production\nrecursive = yes\nprocess_children_only = yes\n\n[BACKUP/SAN200/projects]\nuse_template = backup\nrecursive = yes\nprocess_children_only = yes\n</code></pre> Also note that <code>post_snapshot_script</code> cannot be used with <code>syncoid</code> especially with <code>recursive = yes</code>. This is because there cannot be two zfs send and receive at the same time on the same dataset.</p> <p><code>sanoid</code> does not wait for the script completion before continuing. This mean that should the <code>syncoid</code> process take a bit too much time, a new one will be spawned. And for reasons unknown to me yet, a new syncoid process will cancel the previous one (instead of just leaving). As some of the spawned <code>syncoid</code> will produce errors, the entire <code>sanoid</code> process will fail.</p> <p>So this approach does not work and has to be done independently, it seems. The good news is that the SystemD service of <code>Type= oneshot</code> can have several <code>Execstart=</code> lines.</p>"}, {"location": "sanoid/#send-encrypted-backups-to-a-encrypted-dataset", "title": "Send encrypted backups to a encrypted dataset", "text": "<p><code>syncoid</code>'s default behaviour is to create the destination dataset without encryption so the snapshots are transferred and can be read without encryption. You can check this with the <code>zfs get encryption,keylocation,keyformat</code> command both on source and destination.</p> <p>To prevent this from happening you have to [pass the <code>--sendoptions='w'](https://github.com/jimsalterjrs/sanoid/issues/548) to</code>syncoid<code>so that it tells zfs to send a raw stream. If you do so, you also need to [transfer the key file](https://github.com/jimsalterjrs/sanoid/issues/648) to the destination server so that it can do a</code>zfs loadkey` and then mount the dataset. For example:</p> <pre><code>server-host:$ sudo zfs list -t filesystem\nNAME                    USED  AVAIL     REFER  MOUNTPOINT\nserver_data             232M  38.1G      230M  /var/server_data\nserver_data/log         111K  38.1G      111K  /var/server_data/log\nserver_data/mail        111K  38.1G      111K  /var/server_data/mail\nserver_data/nextcloud   111K  38.1G      111K  /var/server_data/nextcloud\nserver_data/postgres    111K  38.1G      111K  /var/server_data/postgres\n\nserver-host:$ sudo zfs get keylocation server_data/nextcloud\nNAME                   PROPERTY     VALUE                                    SOURCE\nserver_data/nextcloud  keylocation  file:///root/zfs_dataset_nextcloud_pass  local\n\nserver-host:$ sudo syncoid --recursive --skip-parent --sendoptions=w server_data root@192.168.122.94:backup_pool\nINFO: Sending oldest full snapshot server_data/log@autosnap_2021-06-18_18:33:42_yearly (~ 49 KB) to new target filesystem:\n17.0KiB 0:00:00 [1.79MiB/s] [=================================================&gt;                                                                                                  ] 34%            \nINFO: Updating new target filesystem with incremental server_data/log@autosnap_2021-06-18_18:33:42_yearly ... syncoid_caedrium.com_2021-06-22:10:12:55 (~ 15 KB):\n41.2KiB 0:00:00 [78.4KiB/s] [===================================================================================================================================================] 270%            \nINFO: Sending oldest full snapshot server_data/mail@autosnap_2021-06-18_18:33:42_yearly (~ 49 KB) to new target filesystem:\n17.0KiB 0:00:00 [ 921KiB/s] [=================================================&gt;                                                                                                  ] 34%            \nINFO: Updating new target filesystem with incremental server_data/mail@autosnap_2021-06-18_18:33:42_yearly ... syncoid_caedrium.com_2021-06-22:10:13:14 (~ 15 KB):\n41.2KiB 0:00:00 [49.4KiB/s] [===================================================================================================================================================] 270%            \nINFO: Sending oldest full snapshot server_data/nextcloud@autosnap_2021-06-18_18:33:42_yearly (~ 49 KB) to new target filesystem:\n17.0KiB 0:00:00 [ 870KiB/s] [=================================================&gt;                                                                                                  ] 34%            \nINFO: Updating new target filesystem with incremental server_data/nextcloud@autosnap_2021-06-18_18:33:42_yearly ... syncoid_caedrium.com_2021-06-22:10:13:42 (~ 15 KB):\n41.2KiB 0:00:00 [50.4KiB/s] [===================================================================================================================================================] 270%            \nINFO: Sending oldest full snapshot server_data/postgres@autosnap_2021-06-18_18:33:42_yearly (~ 50 KB) to new target filesystem:\n17.0KiB 0:00:00 [1.36MiB/s] [===============================================&gt;                                                                                                    ] 33%            \nINFO: Updating new target filesystem with incremental server_data/postgres@autosnap_2021-06-18_18:33:42_yearly ... syncoid_caedrium.com_2021-06-22:10:14:11 (~ 15 KB):\n41.2KiB 0:00:00 [48.9KiB/s] [===================================================================================================================================================] 270%  \n\nserver-host:$ sudo scp /root/zfs_dataset_nextcloud_pass 192.168.122.94:\n</code></pre> <pre><code>backup-host:$ sudo zfs set keylocation=file:///root/zfs_dataset_nextcloud_pass  backup_pool/nextcloud\nbackup-host:$ sudo zfs load-key backup_pool/nextcloud\nbackup-host:$ sudo zfs mount backup_pool/nextcloud\n</code></pre> <p>If you also want to keep the <code>encryptionroot</code> you need to let zfs take care of the recursion instead of syncoid. In this case you can't use syncoid's stuff like <code>--exclude</code> from the manpage of zfs:</p> <pre><code>-R, --replicate\n   Generate a replication stream package, which will replicate the specified file system, and all descendent file systems, up to the named snapshot.  When received, all properties, snap\u2010\n   shots, descendent file systems, and clones are preserved.\n\n   If the -i or -I flags are used in conjunction with the -R flag, an incremental replication stream is generated.  The current values of properties, and current snapshot and file system\n   names are set when the stream is received.  If the -F flag is specified when this stream is received, snapshots and file systems that do not exist on the sending side are destroyed.\n   If the -R flag is used to send encrypted datasets, then -w must also be specified.\n</code></pre> <p>In this case this should work:</p> <pre><code>/sbin/syncoid --recursive --force-delete --sendoptions=\"Rw\" zpool/backups zfs-recv@10.29.3.27:zpool/backups\n</code></pre>"}, {"location": "sanoid/#troubleshooting", "title": "Troubleshooting", "text": ""}, {"location": "sanoid/#syncoid-no-tty-present-and-no-askpass-program-specified", "title": "Syncoid no tty present and no askpass program specified", "text": "<p>If you try to just sync a ZFS dataset between two machines, something like <code>syncoid pool/dataset user@remote:pool/dataset</code>, you\u2019ll eventually see <code>syncoid</code> throwing a sudo error: <code>sudo: no tty present and no askpass program specified</code>. That\u2019s because it\u2019s trying to run a sudo command on the remote, and sudo doesn\u2019t have a way to ask for a password with the way <code>syncoid</code>\u2019s running commands in the remote.</p> <p>Searching online, many people just saying to enable SSH as root, which might be fine on a local network, but not the best solution. Instead, you can enable passwordless <code>sudo</code> for <code>zfs</code> commands on a unprivileged user. Getting this done was very simple:</p> <pre><code>sudo visudo /etc/sudoers.d/zfs_receive_for_syncoid\n</code></pre> <p>And then fill it with the following:</p> <pre><code>&lt;your user&gt; ALL=NOPASSWD: /usr/sbin/zfs *\n</code></pre> <p>If you really want to put in the effort, you can even take a look at which <code>zfs</code> commands that <code>syncoid</code> is actually invoking, and then restrict passwordless sudo only for those commands. It\u2019s important that you do this for all commands that <code>syncoid</code> uses. Syncoid runs a few <code>zfs</code> commands with sudo to list snapshots and get some other information on the remote machine before doing the transfer.</p>"}, {"location": "sanoid/#references", "title": "References", "text": "<ul> <li>Source</li> <li>Docs</li> </ul>"}, {"location": "scrum/", "title": "Scrum", "text": "<p>Scrum is an agile framework for developing, delivering, and sustaining complex products, with an initial emphasis on software development, although it has been used in other fields such as personal task management.  It is designed for teams of ten or fewer members, who break their work into goals that can be completed within time-boxed iterations, called sprints, no longer than one month and most commonly two weeks. The Scrum Team track progress in 15-minute time-boxed daily meetings, called daily scrums. At the end of the sprint, the team holds sprint review, to demonstrate the work done, a sprint retrospective to improve continuously, and a sprint planning to prepare next sprint's tasks.</p> <p>For my personal scrum workflow and in the DevOps and DevSecOps teams I've found that Sprint goals are not operative, as multiple unrelated tasks need to be done, so it doesn't make sense to define just one goal.</p>"}, {"location": "scrum/#the-meetings", "title": "The meetings", "text": "<p>Scrum tries to minimize the time spent in meetings while keeping a clearly defined direction and a healthy environment between all the people involved in the project.</p> <p>To achieve that is uses four types of meetings:</p> <ul> <li>Daily.</li> <li>Refinement.</li> <li>Retros.</li> <li>Reviews.</li> <li>Plannings.</li> </ul>"}, {"location": "scrum/#daily-meetings", "title": "Daily meetings", "text": "<p>Dailies or weeklies are the meetings where the development team exposes at high level of detail the current work. Similar to the dailies in the scrum terms, in the meeting each development team member exposes:</p> <ul> <li>The advances in the assigned tasks, with special interest in the encountered     problems and deviations from the steps defined in the refinement.</li> <li>An estimation of the tasks that are going to be left unfinished by the end of     the sprint.</li> </ul> <p>The goals of the meeting are:</p> <ul> <li>Get a general knowledge of what everyone else is doing.</li> <li>Learn from the experience gained by the others while doing their tasks.</li> <li>Get a clear idea of where we stand in terms of completing the sprint tasks.</li> </ul> <p>As opposed to what it may seem, this meeting is not meant to keep track of the productivity of each of us, we work based on trust, and know that each of us is working our best.</p>"}, {"location": "scrum/#refinement-meetings", "title": "Refinement meetings", "text": "<p>Refinement are the meetings where the development team reviews the issues in the backlog and prepares the tasks that will probably be done in the following sprint.</p> <p>The goals of the meeting are:</p> <ul> <li>Next sprint tasks are ready to be worked upon in the next sprint. That means     each task:<ul> <li>Meets the Definition of Ready.</li> <li>All disambiguation in task description, validation criteria and steps     is solved.</li> </ul> </li> <li>Make the Planning meeting more dynamic.</li> </ul> <p>The meeting is composed of the following phases:</p> <ul> <li>Scrum master preparation.</li> <li>Development team refinement.</li> <li>Product owner refinement.</li> </ul>"}, {"location": "scrum/#refinement-preparation", "title": "Refinement preparation", "text": "<p>To prepare the refinement, the scrum master has to:</p> <ul> <li>Make a copy of the Refinement document template.</li> <li>Open the OKRs document if you have one and for category in OKR categories:</li> <li>Select the category label in the issue tracker and select the milestone of       the semester.</li> <li>Review which of those issues might enter the next sprint, and set the sprint       project on them.</li> <li> <p>Remove the milestone from the issue filter to see if there are interesting       issues without the milestone set.</p> </li> <li> <p>Go to the next sprint Kanban board:</p> </li> <li>Order the issues by priority.</li> <li>Make sure there are tasks with the <code>Good first issue</code> label.</li> <li> <p>Make sure that there are more tasks than we can probably do so we can remove       some instead of need to review the backlog and add more in the       refinement.</p> </li> <li> <p>Fill up the sprint goals section of the refinement document.</p> </li> <li>Create the Refinement developer team and product owner meeting calendar     events.</li> </ul>"}, {"location": "scrum/#development-team-refinement-meeting", "title": "Development team refinement meeting", "text": "<p>In this meeting the development team with the help of the scrum master, reviews the tasks to be added to the next sprint. The steps are defined in the refinement template.</p>"}, {"location": "scrum/#product-owner-refinement-meeting", "title": "Product owner refinement meeting", "text": "<p>In this meeting the product owner with the help of the scrum master reviews the tasks to be added to the next sprint. With the refinement document as reference:</p> <ul> <li>The expected current sprint undone tasks are reviewed.</li> <li>The sprint goals are discussed, modified and agreed. If there are many     changes, we might think of setting the goals together in next sprints.</li> <li>The scrum master does a quick description of each issue.</li> <li>Each task priority is discussed and updated.</li> </ul>"}, {"location": "scrum/#retro-meetings", "title": "Retro meetings", "text": "<p>Retrospectives or Retros are the meetings where the scrum team plan ways to increase the quality and effectiveness of the team.</p> <p>The scrum master conducts different dynamics to help the rest of the scrum team inspect how the last Sprint went with regards to individuals, interactions, processes, tools, and their Definition of Done and Ready. Assumptions that led them astray are identified and their origins explored. The most impactful improvements are addressed as soon as possible. They may even be added to the backlog for the next sprint.</p> <p>Although improvements may be implemented at any time, the sprint retrospective provides a formal opportunity to focus on inspection and adaptation.</p> <p>The sprint retrospective concludes the sprint.</p> <p>The meeting consists of five phases, all of them conducted by the scrum master:</p> <ul> <li>Set the stage: There is an opening dynamic to give people time to \u201carrive\u201d     and get into the right mood.</li> <li>Gather Data: Help everyone remember. Create a shared pool of information     (everybody sees the world differently). There is an initial dynamic to     measure the general feeling of the team and the issues to analyze further.</li> <li>Generate insights: Analyze why did things happen the way they did, identify     patterns and see the big picture.</li> <li>Decide what to do: Pick a few issues to work on and create concrete action     plans of how you\u2019ll address them. Adding the as issues in the scrum board.</li> <li>Close the retrospective: Clarify follow-ups, show appreciations, leave the     meeting with a general good feeling, and analyze how could the     retrospectives improve.</li> </ul> <p>If you have no idea how to conduct this meeting, you can take ideas from retromat.</p> <p>The goals of the meeting are:</p> <ul> <li>Analyze and draft a plan to iteratively improve the team's well-being, quality     and efficiency.</li> </ul>"}, {"location": "scrum/#review-meetings", "title": "Review meetings", "text": "<p>Reviews are the meetings where the product owner presents the sprint work to the rest of the team and the stakeholders. The idea of what is going to be done in the next sprint is also defined in this meeting.</p> <p>The meeting goes as follows:</p> <ul> <li>The product owner explains what items have been \u201cDone\u201d and what has not been     \u201cDone\u201d.</li> <li>The product owner discuss what went well during the sprint, what problems they     ran into, and how those problems were solved.</li> <li>The developers demonstrate the work that it has \u201cDone\u201d and answers questions.</li> <li>The product owner discusses the Product Backlog as it stands in terms of the     semester OKRs.</li> <li>The entire group collaborates on what to do next, so that the Sprint Review     provides valuable input to subsequent Sprint Planning.</li> </ul> <p>As the target audience are the stakeholders, the language must be changed accordingly, we should give overall ideas and not get caught in complicated high tech detailed explanations unless they ask them.</p> <p>The goals of the meeting are:</p> <ul> <li> <p>Increase the transparency on what the team has done in the sprint. By     explaining to the stake holders:</p> <ul> <li>What has been done.</li> <li>The reasons why we've implemented the specific outcomes for the tasks.</li> <li>The deviation from the expected plan of action.</li> <li>The status of the unfinished tasks with an explanation of why weren't they     closed.</li> <li>The meaning of the work done in terms of the semester OKRs.</li> </ul> </li> <li> <p>Increase the transparency on what the team plans to do for the following     sprint by explaining to the stakeholders:</p> <ul> <li>What do we plan to do in the next semester.</li> <li>How we plan to do it.</li> <li>The meaning of the plan in terms of the semester OKRs.</li> </ul> </li> <li> <p>Get the feedback from the stakeholders. We expect to gather and process their     feedback by processing their opinions both of the work done of the past     sprint and the work to be done in the next one. It will be gathered by the     scrum master and persisted in the board on the planning meetings.</p> </li> <li> <p>Incorporate the stakeholders in the decision making process of the team. By     inviting them to define with the rest of the scrum team the tasks for the     next sprint.</p> </li> </ul>"}, {"location": "scrum/#planning-meetings", "title": "Planning meetings", "text": "<p>Plannings are the meetings where the scrum team decides what it's going to do in the following sprint. The decision is made with the information gathered in the refinement, retro and review sessions.</p> <p>Conducted by the scrum master, usually only the members of the scrum team (developers, product owner and scrum master) are present, but stakeholders can also be invited.</p> <p>If the job has been done in the previous sessions, the backlog should be priorized and refined, so we should only add the newest issues gathered in the retro and review, refine them and decide what we want to do this sprint.</p> <p>The meeting goes as follows:</p> <ul> <li>We add the issues raised in the review to the backlog.</li> <li>We analyze the tasks on the top of the backlog, add them to the sprint     board without assigning it to any developer.</li> <li>Once all tasks are added, we the stats of past sprints to see if the scope is     realistic.</li> </ul> <p>The goals of the meeting are:</p> <ul> <li>Assert that the tasks added to the sprint follow the global path defined by     the semester OKRs.</li> <li>All team has a clear view of what needs to be done.</li> <li>The team makes a realistic work commitment.</li> </ul>"}, {"location": "scrum/#the-roles", "title": "The roles", "text": "<p>There are three roles required in the scrum team:</p> <ul> <li>Product owner.</li> <li>Scrum master.</li> <li>Developer.</li> </ul>"}, {"location": "scrum/#product-owner", "title": "Product owner", "text": "<p>Scrum product owner is accountable for maximizing the value of the product resulting from the work of the scrum team.</p> <p>It's roles are:</p> <ul> <li> <p>Assist the scrum master with:</p> <ul> <li>Priorization of the semester OKRs.</li> <li>Monitorization of the status of the semester OKRs on reviews and     plannings.</li> <li>Priorization of the sprint tasks.</li> </ul> </li> <li> <p>Conduct the daily meetings:</p> <ul> <li>Show the Kanban board in the meeting</li> <li>Remind the number of weeks left until the review meeting.</li> <li>Make sure that the team is aware of what tasks are going to be left undone     at the end of the sprint.</li> <li>Inform the affected stakeholders of the possible delay.</li> </ul> </li> <li> <p>Prepare and conduct the review meeting:</p> <ul> <li>With the help of the scrum master, prepare the reports:<ul> <li>Create the report of the sprint, including:<ul> <li>Make sure that the Definition of Done is     met for the closed tasks.</li> <li>Explanation of the done tasks.</li> <li>Status of uncompleted tasks, and reason why they weren't complete.</li> <li>The meaning of the work done in terms of the semester OKRs.</li> </ul> </li> <li>Create the report of the proposed next sprint's planning, with     arguments behind why we do each task.</li> </ul> </li> <li>Conduct the review meeting presenting the reports to the stakeholders.</li> </ul> </li> <li>Attend the daily, review, retro and planning meetings.</li> </ul>"}, {"location": "scrum/#scrum-master", "title": "Scrum master", "text": "<p>Scrum master is accountable for establishing Scrum as defined in this document.</p> <p>This position is going to be rotated between the members of the scrum team with a period of two sprints.</p> <p>It's roles are:</p> <ul> <li> <p>Monitoring the status of the semester OKRs on reviews and plannings.</p> <ul> <li>Create new tasks required to meet the objectives.</li> </ul> </li> <li> <p>Refining the backlog:</p> <ul> <li>Adjust priority.</li> <li>Refine the tasks that are going to enter next sprint.</li> <li>Organize the required meetings to refine the backlog with the team     members.</li> <li>Delete deprecated tasks.</li> </ul> </li> <li> <p>Assert that issues that are going to enter the new sprint meet the Definition     of Ready.</p> </li> <li> <p>Arrange, prepare the daily meetings:</p> <ul> <li>Update the calendar events according to the week needs.</li> </ul> </li> <li> <p>Arrange, prepare and conduct the review meeting:</p> <ul> <li>Create the calendar event inviting the scrum team and the stakeholders.</li> <li>With the help of the product owner, prepare the reports:<ul> <li>Create the report of the sprint, including:<ul> <li>Make sure that the Definition of Done is     met for the closed tasks.</li> <li>Explanation of the done tasks.</li> <li>Status of uncompleted tasks, and reason why they weren't complete.</li> <li>The meaning of the work done in terms of the semester OKRs.</li> </ul> </li> <li>Create the report of the proposed next sprint's planning, with     arguments behind why we do each task.</li> </ul> </li> <li>Update the planning with the requirements of the stakeholders.</li> <li>Upload the review reports to the documentation repository.</li> </ul> </li> <li> <p>Arrange, prepare and conduct the refinement meetings:</p> <ul> <li>Prepare the tasks that need to be refined:<ul> <li>Adjust the priority of the backlog tasks.</li> <li>Select the tasks that are most probably going to enter the next     sprint.</li> <li>Expand the description of those tasks so it's understandable by any     team member.</li> <li>If the task need some steps to be done before it can be worked upon,     do them or create a task to do them before the original task.</li> </ul> </li> <li>Create the required refinement calendar events inviting the members of the     scrum team.</li> <li>Conduct the refinement meeting.</li> <li>Update the tasks with the outcome of the meeting.</li> <li>Prepare the next sprint's Kanban board.</li> </ul> </li> <li> <p>Arrange, prepare and conduct the retro meeting:</p> <ul> <li>Prepare the dynamics of the meeting.</li> <li>Create the retro calendar event inviting the members of the scrum team.</li> <li>Conduct the retro meeting.</li> <li>Update the tasks with the outcome of the meeting.</li> <li>Upload the retro reports to the documentation repository.</li> </ul> </li> <li> <p>Arrange, prepare and conduct the planning meeting:</p> <ul> <li>Make sure that you've done the required refinement sessions to have the     tasks and Kanban board ready for the next sprint.</li> <li>Create the planning calendar event inviting the members of the scrum team.</li> <li>Conduct the planning meeting.</li> <li>Update the tasks with the outcome of the meeting and start the sprint.</li> </ul> </li> </ul>"}, {"location": "scrum/#developer", "title": "Developer", "text": "<p>Developers are the people in the scrum team that are committed to creating any aspect of a usable increment each sprint.</p> <p>It's roles are:</p> <ul> <li>Attend the daily, refinement, review, retro and planning meetings.</li> <li>Focus on completing the assigned sprint tasks.<ul> <li>Do the required work or be responsible to coordinate the work that others     do for the task to be complete.</li> <li>Make sure that the Definition of Done is met     before closing the task.</li> </ul> </li> </ul>"}, {"location": "scrum/#inter-team-workflow", "title": "Inter team workflow", "text": "<p>To improve the communication between the teams, you can:</p> <ul> <li>Present more clearly the team objectives and reasons behind our tasks, and     make the rest of the teams part of the decision making.</li> <li>Be aware of the other team's needs and tasks.</li> </ul> <p>To solve the first point, you can offer the rest of the teams different solutions depending the time they want to invest in staying informed:</p> <ul> <li>You can invite the other team members to the sprint reviews, where you show the     sprint's work and present what you plan to do in the next sprint. This could     be the best way to stay informed, as you'll try to sum up everything they     need to know in the shortest time.</li> <li>For those that want to be more involved with the decision making inside the     team, they could be invited to the planning sessions and even the     refinement ones where they are involved.</li> <li>For those that don't want to attend the review, they can either get a summary     from other members of their team that did attend, or they can read the     meeting notes that you publish after each one.</li> </ul> <p>The second point means that your team members become more involved in the other team's work. The different levels of involvement are linked to the amount of time invested and the quality of the interaction.</p> <p>The highest level of involvement would be that a member of your team is also part of the other team. This is easier for those teams that already use Scrum as their agile framework, that means:</p> <ul> <li>Attending the team's meetings (retro, review, planning and refinement).</li> <li>Inform the rest of your team of the outcomes of those meetings in the     daily meeting.</li> <li>Focus on doing that team's sprint tasks.</li> <li>Populate and refine the tasks related to your team in the other team issue     tracker.</li> </ul> <p>For those teams that are smaller or don't use Scrum as their agile framework, a your team members could accompany them by:</p> <ul> <li>Setting periodic meetings (weekly/biweekly/monthly) to discuss what are they     doing, what do they plan to do and how.</li> <li>Create the team related tasks in your backlog, coordinating with the scrum     master to refine and prioritize them.</li> </ul>"}, {"location": "scrum/#definitions", "title": "Definitions", "text": ""}, {"location": "scrum/#definition-of-ready", "title": "Definition of Ready", "text": "<p>The Definition of Ready (DoR) is a list of criteria which must be met before any task can be added to a sprint. It is agreed by the whole scrum team and reviewed in the planning sessions.</p>"}, {"location": "scrum/#expected-benefits", "title": "Expected Benefits", "text": "<ul> <li>Avoids beginning work on features that do not have clearly defined completion     criteria, which usually translates into costly back-and-forth discussion or     rework.</li> <li>Provides the team with an explicit agreement allowing it to \u201cpush back\u201d on     accepting ill-defined features to work on.</li> <li>The Definition of Ready provides a checklist which usefully guides     pre-implementation activities: discussion, estimation, design.</li> </ul>"}, {"location": "scrum/#example-of-a-definition-of-ready", "title": "Example of a Definition of Ready", "text": "<p>A task needs to meet the following criteria before being added to a sprint.</p> <ul> <li>Have a short title that summarizes the goal of the task.</li> <li>Have a description clear enough so any team member can understand why we     need to do the task</li> <li>Have a validation criteria for the task to be done</li> <li>Have a checklist of steps required to meet the validation criteria, clear     enough so that any team member can understand them.</li> <li>Have a scope that can be met in one sprint.</li> <li>Have the <code>Priority:</code> label set.</li> <li>If other teams are involved in the task, add the <code>Team:</code> labels.</li> <li>If it's associated to an OKR set the <code>OKR:</code> label.</li> </ul>"}, {"location": "scrum/#definition-of-done", "title": "Definition of Done", "text": "<p>The Definition of Done (DoD) is a list of criteria which must be met before any task can be closed. It is agreed by the whole scrum team and reviewed in the planning sessions.</p>"}, {"location": "scrum/#expected-benefits_1", "title": "Expected Benefits", "text": "<ul> <li>The Definition of Done limits the cost of rework once a feature has been     accepted as \u201cdone\u201d.</li> <li>Having an explicit contract limits the risk of misunderstanding and conflict     between the development team and the customer or product owner.</li> </ul>"}, {"location": "scrum/#common-pitfalls", "title": "Common Pitfalls", "text": "<ul> <li>Obsessing over the list of criteria can be counter-productive; the list needs     to define the minimum work generally required to get a product increment to     the \u201cdone\u201d state.</li> <li>Individual features or user stories may have specific \u201cdone\u201d criteria in     addition to the ones that apply to work in general.</li> <li>If the definition of done is merely a shared understanding, rather than     spelled out and displayed on a wall, it may lose much of its effectiveness;     a good part of its value lies in being an explicit contract known to all     members of the team.</li> </ul>"}, {"location": "scrum/#example-of-a-definition-of-done", "title": "Example of a Definition of Done", "text": "<p>A task needs to meet the following criteria before being closed.</p> <ul> <li> All changes must be documented.</li> <li> All related pull requests must be merged.</li> </ul>"}, {"location": "sed/", "title": "sed", "text": ""}, {"location": "sed/#snippets", "title": "Snippets", "text": ""}, {"location": "sed/#if-variable-has", "title": "If variable has /", "text": "<pre><code>sed \"s|$variable||g\"\n</code></pre>"}, {"location": "sed/#replace-in-files", "title": "Replace in files", "text": "<pre><code>sed -i {{ sed_replace_expression }} {{ files_to_replace }}\n</code></pre>"}, {"location": "sed/#replace-recursively-in-directories-and-subdirectories", "title": "Replace recursively in directories and subdirectories", "text": "<pre><code>find {{ directory }} -type f -exec sed -i 's/nano/vim/g' {} +\n</code></pre>"}, {"location": "sed/#non-greedy", "title": "Non greedy", "text": "<p>Sed doesn't support non greedy, use <code>.[^{{ character }}]*</code> instead</p>"}, {"location": "sed/#delete-match", "title": "Delete match", "text": "<pre><code>sed '/&lt;match&gt;/d' file\n</code></pre>"}, {"location": "sed/#delete-the-last-line-of-file", "title": "Delete the last line of file", "text": "<pre><code>sed '$d' -i file.txt\n</code></pre>"}, {"location": "sed/#delete-the-first-line-of-file", "title": "Delete the first line of file", "text": "<pre><code>sed '1d' -i file.txt\n</code></pre>"}, {"location": "seedvault/", "title": "Seedvault", "text": "<p>Seedvault is an open-source encrypted backup app for inclusion in Android-based operating systems.</p> <p>While every smartphone user wants to be prepared with comprehensive data backups in case their phone is lost or stolen, not every Android user wants to entrust their sensitive data to Google's cloud-based storage. By storing data outside Google's reach, and by using client-side encryption to protect all backed-up data, Seedvault offers users maximum data privacy with minimal hassle.</p> <p>Seedvault allows Android users to store their phone data without relying on Google's proprietary cloud storage. Users can decide where their phone's backup will be stored, with options ranging from a USB flash drive to a remote self-hosted cloud storage alternative such as NextCloud. Seedvault also offers an Auto-Restore feature: instead of permanently losing all data for an app when it is uninstalled, Seedvault's Auto-Restore will restore all backed-up data for the app upon reinstallation.</p> <p>Seedvault protects users' private data by encrypting it on the device with a key known only to the user. Each Seedvault account is protected by client-side encryption (AES/GCM/NoPadding). This encryption is unlockable only with a 12-word randomly-generated key.</p> <p>With Seedvault, backups run automatically in the background of the phone's operating system, ensuring that no data will be left behind if the device is lost or stolen. The Seedvault application requires no technical knowledge to operate, and does not require a rooted device.</p>"}, {"location": "seedvault/#installation", "title": "Installation", "text": "<p>After looking at their repo and docs I haven't found how to install it. Luckily GrapheneOS comes with it pre-installed. </p>"}, {"location": "seedvault/#usage", "title": "Usage", "text": "<p>To access the seedvault backup configuration (at least in GrapheneOS) go to <code>Settings/System/Backup</code>.</p>"}, {"location": "seedvault/#store-the-backup-remotely", "title": "Store the backup remotely", "text": "<p>You can use an USB drive or Nextcloud to store the backup outside your phone. The first is no good because you need to manually manage the backup extraction, and the second assumes you have a Nextcloud instance that you trust.</p> <p>If you don't like those options, you can also store the backup in the phone and use <code>syncthing</code> to sync it with your server. If you select local storage, the [backups are saved in the <code>.SeedVaultAndroidBackup</code> directory at the root directory of the internal storage (it may be hidden).</p>"}, {"location": "seedvault/#restore-the-backup", "title": "Restore the backup", "text": ""}, {"location": "seedvault/#references", "title": "References", "text": "<ul> <li>Home</li> <li>Source</li> </ul>"}, {"location": "selenium/", "title": "Selenium", "text": "<p>Selenium is a portable framework for testing web applications. It also provides a test domain-specific language (Selenese) to write tests in a number of popular programming languages.</p>"}, {"location": "selenium/#web-driver-backends", "title": "Web driver backends", "text": "<p>Selenium can be used with many browsers, such as Firefox, Chrome or PhantomJS. But first, install <code>selenium</code>:</p> <pre><code>pip install selenium\n</code></pre>"}, {"location": "selenium/#firefox", "title": "Firefox", "text": "<p>Assuming you've got firefox already installed, you need to download the geckodriver, unpack the tar and add the <code>geckodriver</code> binary somewhere in your <code>PATH</code>.</p> <pre><code>from selenium import webdriver\n\ndriver = webdriver.Firefox()\n\ndriver.get(\"https://duckduckgo.com/\")\n</code></pre> <p>If you need to get the status code of the requests use Chrome instead</p> <p>There is an issue with Firefox that doesn't support this feature.</p>"}, {"location": "selenium/#chrome", "title": "Chrome", "text": "<p>We're going to use Chromium instead of Chrome. Download the chromedriver of the same version as your Chromium, unpack the tar and add the <code>chromedriver</code> binary somewhere in your <code>PATH</code>.</p> <pre><code>from selenium import webdriver\nfrom selenium.webdriver.chrome.options import Options\n\nopts = Options()\nopts.binary_location = '/usr/bin/chromium'\ndriver = webdriver.Chrome(options=opts)\n\ndriver.get(\"https://duckduckgo.com/\")\n</code></pre> <p>If you don't want to see the browser, you can run it in headless mode adding the next line when defining the <code>options</code>:</p> <pre><code>opts.add_argument(\"--headless\")\n</code></pre>"}, {"location": "selenium/#phantomjs", "title": "PhantomJS", "text": "<p>PhantomJS is abandoned -&gt; Don't use it</p> <p>The development stopped in 2018</p> <p>PhantomJS is a headless Webkit, in conjunction with Selenium WebDriver, it can be used to run tests directly from the command line. Since PhantomJS eliminates the need for a graphical browser, tests run much faster.</p> <p>Don't install phantomjs from the official repos as it's not a working release -.-. <code>npm install -g phantomjs</code> didn't work either. I had to download the tar from the downloads page, which didn't work either. The project is abandoned, so don't use this.</p>"}, {"location": "selenium/#usage", "title": "Usage", "text": "<p>Assuming that you've got a configured <code>driver</code>, to get the url you're in after javascript has done it's magic use the <code>driver.current_url</code> method. To return the HTML of the page use <code>driver.page_source</code>.</p>"}, {"location": "selenium/#open-a-url", "title": "Open a URL", "text": "<pre><code>driver.get(\"https://duckduckgo.com/\")\n</code></pre>"}, {"location": "selenium/#get-page-source", "title": "Get page source", "text": "<pre><code>driver.page_source\n</code></pre>"}, {"location": "selenium/#get-current-url", "title": "Get current url", "text": "<pre><code>driver.current_url\n</code></pre>"}, {"location": "selenium/#click-on-element", "title": "Click on element", "text": "<p>Once you've opened the page you want to interact with <code>driver.get()</code>, you need to get the Xpath of the element to click on. You can do that by using your browser inspector, to select the element, and once on the code if you right click there is a \"Copy XPath\"</p> <p>Once that is done you should have something like this when you paste it down.</p> <pre><code>//*[@id=\u201dreact-root\u201d]/section/main/article/div[2]/div[2]/p/a\n</code></pre> <p>Similarly it is the same process for the input fields for username, password, and login button.</p> <p>We can go ahead and do that on the current page. We can store these xpaths as strings in our code to make it readable.</p> <p>We should have three xpaths from this page and one from the initial login.</p> <pre><code>first_login = '//*[@id=\u201dreact-root\u201d]/section/main/article/div[2]/div[2]/p/a'\nusername_input = '//*[@id=\"react-root\"]/section/main/div/article/div/div[1]/div/form/div[2]/div/label/input'\npassword_input = '//*[@id=\"react-root\"]/section/main/div/article/div/div[1]/div/form/div[3]/div/label/input'\nlogin_submit = '//*[@id=\"react-root\"]/section/main/div/article/div/div[1]/div/form/div[4]/button/div'\n</code></pre> <p>Now that we have the xpaths defined we can now tell Selenium webdriver to click and send some keys over for the input fields.</p> <pre><code>from selenium.webdriver.common.by import By\n\ndriver.find_element(By.XPATH, first_login).click()\ndriver.find_element(By.XPATH, username_input).send_keys(\"username\")\ndriver.find_element(By.XPATH, password_input).send_keys(\"password\")\ndriver.find_element(By.XPATH, login_submit).click()\n</code></pre> <p>Note</p> <p>Many pages suggest to use methods like <code>find_element_by_name</code>, <code>find_element_by_xpath</code> or <code>find_element_by_id</code>. These are deprecated now. You should use <code>find_element(By.</code> instead. So, instead of:</p> <pre><code>driver.find_element_by_xpath(\"your_xpath\")\n</code></pre> <p>It should be now:</p> <pre><code>driver.find_element(By.XPATH, \"your_xpath\")\n</code></pre> <p>Where <code>By</code> is imported with <code>from selenium.webdriver.common.by import By</code>.</p>"}, {"location": "selenium/#solve-element-isnt-clickable-in-headless-mode", "title": "Solve element isn't clickable in headless mode", "text": "<p>There are many things you can try to fix this issue. Being the first to configure the <code>driver</code> to use the full screen. Assuming you're using the undetectedchromedriver:</p> <pre><code>import undetected_chromedriver.v2 as uc\n\noptions = uc.ChromeOptions()\n\noptions.add_argument(\"--disable-dev-shm-usage\")\noptions.add_argument(\"--no-sandbox\")\noptions.add_argument(\"--headless\")\noptions.add_argument(\"--start-maximized\")\noptions.add_argument(\"--window-size=1920,1080\")\ndriver = uc.Chrome(options=options)\n</code></pre> <p>If that doesn't solve the issue use the next function:</p> <pre><code>def click(driver: uc.Chrome, xpath: str, mode: Optional[str] = None) -&gt; None:\n\"\"\"Click the element marked by the XPATH.\n\n    Args:\n        driver: Object to interact with selenium.\n        xpath: Identifier of the element to click.\n        mode: Type of click. It needs to be one of [None, position, wait]\n\n    The different ways to click are:\n\n    * None: The normal click of the driver.\n    * wait: Wait until the element is clickable and then click it.\n    * position: Deduce the position of the element and then click it with a javascript script.\n    \"\"\"\n    if mode is None:\n       driver.find_element(By.XPATH, xpath).click() \n    elif mode == 'wait':\n        # https://stackoverflow.com/questions/59808158/element-isnt-clickable-in-headless-mode\n        WebDriverWait(driver, 20).until(\n            EC.element_to_be_clickable((By.XPATH, xpath))\n        ).click()\n    elif mode == 'position':\n        # https://stackoverflow.com/questions/16807258/selenium-click-at-certain-position\n        element = driver.find_element(By.XPATH, xpath)\n        driver.execute_script(\"arguments[0].click();\", element)\n</code></pre>"}, {"location": "selenium/#close-the-browser", "title": "Close the browser", "text": "<pre><code>driver.close()\n</code></pre>"}, {"location": "selenium/#change-browser-configuration", "title": "Change browser configuration", "text": "<p>You can pass <code>options</code> to the initialization of the chromedriver to tweak how does the browser behave. To get a list of the actual <code>prefs</code> you can go to <code>chrome://prefs-internals</code>, there you can get the code you need to tweak.</p>"}, {"location": "selenium/#disable-loading-of-images", "title": "Disable loading of images", "text": "<pre><code>options = ChromeOptions()\noptions.add_experimental_option(\n    \"prefs\",\n    {\n        \"profile.default_content_setting_values.images\": 2,\n        \"profile.default_content_setting_values.cookies\": 2,\n    },\n)\n</code></pre>"}, {"location": "selenium/#disable-site-cookies", "title": "Disable site cookies", "text": "<pre><code>options = ChromeOptions()\noptions.add_experimental_option(\n    \"prefs\",\n    {\n        \"profile.default_content_setting_values.cookies\": 2,\n    },\n)\n</code></pre>"}, {"location": "selenium/#bypass-selenium-detectors", "title": "Bypass Selenium detectors", "text": "<p>Sometimes web servers react differently if they notice that you're using selenium. Browsers can be detected through different ways and some commonly used mechanisms are as follows:</p> <ul> <li>Implementing captcha / recaptcha to detect the automatic bots.</li> <li>Non-human behaviour (browsing too fast, not scrolling to the visible elements,     ...)</li> <li>Using an IP that's flagged as suspicious (VPN, VPS, Tor...)</li> <li>Detecting the term HeadlessChrome within headless Chrome UserAgent</li> <li>Using Bot Management service from Distil     Networks,     Akamai,     Datadome.</li> </ul> <p>They do it through different mechanisms:</p> <ul> <li>Use undetected-chromedriver</li> <li>Use Selenium stealth</li> <li>Rotate the user agent</li> <li>Changing browser properties</li> <li>Predefined Javascript variables</li> <li>Don't use selenium</li> </ul> <p>If you've already been detected, you might get blocked for a plethora of other reasons even after using these methods. So you may have to try accessing the site that was detecting you using a VPN, different user-agent, etc.</p>"}, {"location": "selenium/#use-undetected-chromedriver", "title": "Use undetected-chromedriver", "text": "<p><code>undetected-chromedriver</code> is a python library that uses an optimized Selenium Chromedriver patch which does not trigger anti-bot services like Distill Network / Imperva / DataDome / Botprotect.io Automatically downloads the driver binary and patches it.</p>"}, {"location": "selenium/#installation", "title": "Installation", "text": "<pre><code>pip install undetected-chromedriver\n</code></pre>"}, {"location": "selenium/#usage_1", "title": "Usage", "text": "<pre><code>import undetected_chromedriver.v2 as uc\ndriver = uc.Chrome()\ndriver.get('https://nowsecure.nl')  # my own test test site with max anti-bot protection\n</code></pre> <p>If you want to specify the path to the browser use <code>uc.Chrome(browser_executable_path=\"/path/to/your/file\")</code>.</p>"}, {"location": "selenium/#use-selenium-stealth", "title": "Use Selenium Stealth", "text": "<p><code>selenium-stealth</code> is a python package to prevent detection (by doing most of the steps of this guide) by making selenium more stealthy.</p> <p>Note</p> <p>It's less maintained than <code>undetected-chromedriver</code> so I'd use that other instead. I leave the section in case it's helpful if the other fails for you.</p>"}, {"location": "selenium/#installation_1", "title": "Installation", "text": "<pre><code>pip install selenium-stealth\n</code></pre>"}, {"location": "selenium/#usage_2", "title": "Usage", "text": "<pre><code>from selenium import webdriver\nfrom selenium_stealth import stealth\nimport time\n\noptions = webdriver.ChromeOptions()\noptions.add_argument(\"start-maximized\")\n\n# options.add_argument(\"--headless\")\n\noptions.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\noptions.add_experimental_option('useAutomationExtension', False)\ndriver = webdriver.Chrome(options=options, executable_path=r\"C:\\Users\\DIPRAJ\\Programming\\adclick_bot\\chromedriver.exe\")\n\nstealth(driver,\n        languages=[\"en-US\", \"en\"],\n        vendor=\"Google Inc.\",\n        platform=\"Win32\",\n        webgl_vendor=\"Intel Inc.\",\n        renderer=\"Intel Iris OpenGL Engine\",\n        fix_hairline=True,\n        )\n\nurl = \"https://bot.sannysoft.com/\"\ndriver.get(url)\ntime.sleep(5)\ndriver.quit()\n</code></pre> <p>You can test it with antibot.</p>"}, {"location": "selenium/#rotate-the-user-agent", "title": "Rotate the user agent", "text": "<p>Rotating the UserAgent in every execution of your Test Suite using <code>fake_useragent</code> module as follows:</p> <pre><code>from selenium import webdriver\nfrom selenium.webdriver.chrome.options import Options\nfrom fake_useragent import UserAgent\n\noptions = Options()\nua = UserAgent()\nuserAgent = ua.random\nprint(userAgent)\noptions.add_argument(f'user-agent={userAgent}')\ndriver = webdriver.Chrome(chrome_options=options)\ndriver.get(\"https://www.google.co.in\")\ndriver.quit()\n</code></pre> <p>You can also rotate it with <code>execute_cdp_cmd</code>:</p> <pre><code>from selenium import webdriver\n\ndriver = webdriver.Chrome(executable_path=r'C:\\WebDrivers\\chromedriver.exe')\nprint(driver.execute_script(\"return navigator.userAgent;\"))\n# Setting user agent as Chrome/83.0.4103.97\ndriver.execute_cdp_cmd('Network.setUserAgentOverride', {\"userAgent\": 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36'})\nprint(driver.execute_script(\"return navigator.userAgent;\"))\n</code></pre>"}, {"location": "selenium/#changing-browser-properties", "title": "Changing browser properties", "text": "<ul> <li> <p>Changing the property value of navigator for webdriver to undefined as follows:</p> <pre><code>driver.execute_cdp_cmd(\"Page.addScriptToEvaluateOnNewDocument\", {\n  \"source\": \"\"\"\n    Object.defineProperty(navigator, 'webdriver', {\n      get: () =&gt; undefined\n    })\n  \"\"\"\n})\n</code></pre> <p>You can find a relevant detailed discussion in Selenium webdriver: Modifying navigator.webdriver flag to prevent selenium detection</p> </li> <li> <p>Changing the values of navigator.plugins, navigator.languages, WebGL, hairline feature, missing image, etc.     You can find a relevant detailed discussion in Is there a version of     selenium webdriver that is not detectable?</p> </li> <li> <p>Changing the conventional Viewport</p> <p>You can find a relevant detailed discussion in How to bypass Google captcha with Selenium and python?</p> </li> </ul>"}, {"location": "selenium/#predefined-javascript-variables", "title": "Predefined Javascript variables", "text": "<p>One way of detecting Selenium is by checking for predefined JavaScript variables which appear when running with Selenium. The bot detection scripts usually look anything containing word <code>selenium</code>, <code>webdriver</code> in any of the variables (on window object), and also document variables called <code>$cdc_</code> and <code>$wdc_</code>. Of course, all of this depends on which browser you are on. All the different browsers expose different things.</p> <p>In Chrome, what people had to do was to ensure that <code>$cdc_</code> didn't exist as a document variable.</p> <p>You don't need to go compile the <code>chromedriver</code> yourself, if you open the file with <code>vim</code> and execute <code>:%s/cdc_/dog_/g</code> where <code>dog</code> can be any three characters that will work. With perl you can achieve the same result with:</p> <pre><code>perl -pi -e 's/cdc_/dog_/g' /path/to/chromedriver\n</code></pre>"}, {"location": "selenium/#dont-use-selenium", "title": "Don't use selenium", "text": "<p>Even with <code>undetected-chromedriver</code>, sometimes servers are able to detect that you're using selenium.</p> <p>A uglier but maybe efective way to go is not using selenium and do a combination of working directly with the chrome devtools protocol with <code>pycdp</code> (using this maintained fork) and doing the clicks with <code>pyautogui</code>. See an example on this answer.</p> <p>Keep in mind though that these tools don't look to be actively maintained, and that the approach is quite brittle to site changes. Is there really not other way to achieve what you want?</p>"}, {"location": "selenium/#set-timeout-of-a-response", "title": "Set timeout of a response", "text": "<p>For Firefox and Chromedriver:</p> <pre><code>driver.set_page_load_timeout(30)\n</code></pre> <p>The rest:</p> <pre><code>driver.implicitly_wait(30)\n</code></pre> <p>This will throw a <code>TimeoutException</code> whenever the page load takes more than 30 seconds.</p>"}, {"location": "selenium/#get-the-status-code-of-a-response", "title": "Get the status code of a response", "text": "<p>Surprisingly this is not as easy as with requests, there is no <code>status_code</code> method on the driver, you need to dive into the browser log to get it. Firefox has an open issue since 2016 that prevents you from getting this information. Use Chromium if you need this functionality.</p> <pre><code>from selenium.webdriver.common.desired_capabilities import DesiredCapabilities\n\ncapabilities = DesiredCapabilities.CHROME.copy()\ncapabilities['goog:loggingPrefs'] = {'performance': 'ALL'}\n\ndriver = webdriver.Chrome(desired_capabilities=capabilities)\n\ndriver.get(\"https://duckduckgo.com/\")\nlogs = driver.get_log(\"performance\")\nstatus_code = get_status(driver.current_url, logs)\n</code></pre> <p>Where <code>get_status</code> is:</p> <pre><code>def get_status(url: str, logs: List[Dict[str, Any]]) -&gt; int:\n\"\"\"Get the url response status code.\n\n    Args:\n        url: url to search\n        logs: Browser driver logs\n    Returns:\n        The status code.\n    \"\"\"\n    for log in logs:\n        if log[\"message\"]:\n            data = json.loads(log[\"message\"])\n            with suppress(KeyError):\n                if data[\"message\"][\"params\"][\"response\"][\"url\"] == url:\n                    return data[\"message\"][\"params\"][\"response\"][\"status\"]\n    raise ValueError(f\"Error retrieving the status code for url {url}\")\n</code></pre> <p>You have to use <code>driver.current_url</code> to handle well urls that redirect to other urls.</p> <p>If your url is not catched and you get a <code>ValueError</code>, use the next snippet inside the <code>with suppress(KeyError)</code> statement.</p> <p><pre><code>content_type = (\n    \"text/html\"\n    in data[\"message\"][\"params\"][\"response\"][\"headers\"][\"content-type\"]\n)\nresponse_received = (\n    data[\"message\"][\"method\"] == \"Network.responseReceived\"\n)\nif content_type and response_received:\n    __import__(\"pdb\").set_trace()  # XXX BREAKPOINT\n    pass\n</code></pre> And try to see why <code>url != data[\"message\"][\"params\"][\"response\"][\"url\"]</code>. Sometimes servers redirect the user to a url without the <code>www.</code>.</p>"}, {"location": "selenium/#troubleshooting", "title": "Troubleshooting", "text": ""}, {"location": "selenium/#chromedriver-hangs-up-unexpectedly", "title": "Chromedriver hangs up unexpectedly", "text": "<p>Some say that adding the <code>DBUS_SESSION_BUS_ADDRESS</code> environmental variable fixes it:</p> <pre><code>os.environ[\"DBUS_SESSION_BUS_ADDRESS\"] = \"/dev/null\"\n</code></pre> <p>But it still hangs for me. Right now the only solution I see is to assume it's going to hang and add functionality in your program to resume the work instead of starting from scratch. Ugly I know...</p>"}, {"location": "selenium/#issues", "title": "Issues", "text": "<ul> <li>Firefox driver doesn't have access to the     log: Update the section     above and start using Firefox instead of Chrome when you need to get the     status code of the responses.</li> </ul>"}, {"location": "semantic_versioning/", "title": "Semantic Versioning", "text": "<p>Semantic Versioning is a way to define your program's version based on the type of changes you've introduced. It's defined as a three-number string (separated with a period) in the format of <code>MAJOR.MINOR.PATCH</code>.</p> <p>Usually, it starts with 0.0.0. Then depending on the type of change you make to the library, you increment one of these and set subsequent numbers to zero:</p> <ul> <li><code>MAJOR</code> version if you make backward-incompatible changes.</li> <li><code>MINOR</code> version if you add a new feature.</li> <li><code>PATCH</code> version if you fix bugs.</li> </ul> <p>The version number in this context is used as a contract between the library developer and the systems pulling it in about how freely they can upgrade. For example, if you wrote your web server against <code>Django 3</code>, you should be good to go with all <code>Django 3</code> releases that are at least as new as your current one. This allows you to express your Django dependency in the format of <code>Django &gt;= 3.0.2, &lt;4</code>.</p> <p>In addition, we have to take into account the following considerations:</p> <ul> <li>A normal version number MUST take the form X.Y.Z where X, Y, and Z are   non-negative integers, and MUST NOT contain leading zeroes.</li> <li>Once a versioned package has been released, the contents of that version MUST   NOT be modified. Any modifications MUST be released as a new version.</li> <li>Major version zero (0.y.z) is for initial development. Anything may change at   any time. The public API should not be considered stable. But don't fall into   using ZeroVer instead.</li> <li>Releasing the version 1.0.0 is a declaration of intentions to your users that     the code is to be considered stable.</li> <li>Patch version Z (x.y.Z | x &gt; 0) MUST be incremented if only backwards   compatible bug fixes are introduced. A bug fix is defined as an internal   change that fixes incorrect behavior.</li> <li>Minor version Y (x.Y.z | x &gt; 0) MUST be incremented if new, backwards   compatible functionality is introduced to the public API. It MUST be   incremented if any public API functionality is marked as deprecated. It MAY be   incremented if substantial new functionality or improvements are introduced   within the private code. It MAY include patch level changes. Patch version   MUST be reset to 0 when minor version is incremented.</li> <li>Major version X (X.y.z | X &gt; 0) MUST be incremented if any backwards   incompatible changes are introduced to the public API. It MAY include minor   and patch level changes. Patch and minor version MUST be reset to 0 when major   version is incremented.</li> </ul> <p>!!! note \"Encoding this information in the version is just an extremely lossy, but very fast to parse and interpret, which may lead into issues</p> <p>By using this format whenever you rebuild your application, you\u2019ll automatically pull in any new feature/bugfix/security releases of Django, enabling you to use the latest and best version that still in theory guarantees to works with your project.</p> <p>This is great because:</p> <ul> <li>You enable automatic, compatible security fixes.</li> <li>It automatically pulls in bug fixes on the library side.</li> <li>Your application will keep building and working in the future as it did today     because the significant version pin protects you from pulling in versions     whose API would not match.</li> </ul>"}, {"location": "semantic_versioning/#commit-message-guidelines", "title": "Commit message guidelines", "text": "<p>If you like the idea behind Semantic Versioning, it makes sense to follow the Angular commit convention to automate the changelog maintenance and the program version bumping.</p> <p>Each commit message consists of a header, a body and a footer. The header has a defined format that includes a type, a scope and a subject:</p> <pre><code>&lt;type&gt;(&lt;scope&gt;): &lt;subject&gt;\n&lt;BLANK LINE&gt;\n&lt;body&gt;\n&lt;BLANK LINE&gt;\n&lt;footer&gt;\n</code></pre> <p>The header is mandatory and the scope of the header is optional.</p> <p>Any line of the commit message cannot be longer 100 characters.</p> <p>The footer could contain a closing reference to an issue.</p> <p>Samples:</p> <pre><code>docs(changelog): update changelog to beta.5\n\nfix(release): need to depend on latest rxjs and zone.js\n\nThe version in our package.json gets copied to the one we publish, and users need the latest of these.\n\ndocs(router): fix typo 'containa' to 'contains' (#36764)\n\nCloses #36763\n\nPR Close #36764\n</code></pre>"}, {"location": "semantic_versioning/#change-types", "title": "Change types", "text": "<p>Must be one of the following:</p> <ul> <li><code>feat</code>: A new feature.</li> <li><code>fix</code>: A bug fix.</li> <li><code>test</code>: Adding missing tests or correcting existing tests.</li> <li><code>docs</code>: Documentation changes.</li> <li><code>chore</code>: A package maintenance change such as updating the requirements.</li> <li><code>bump</code>: A commit to mark the increase of the version number.</li> <li><code>style</code>: Changes that do not affect the meaning of the code (white-space,     formatting, missing semi-colons, etc).</li> <li><code>ci</code>: Changes to our CI configuration files and scripts.</li> <li><code>perf</code>: A code change that improves performance.</li> <li><code>refactor</code>: A code change that neither fixes a bug nor adds a feature.</li> <li><code>build</code>: Changes that affect the build system or external dependencies.</li> </ul>"}, {"location": "semantic_versioning/#subject", "title": "Subject", "text": "<p>The subject contains a succinct description of the change:</p> <ul> <li>Use the imperative, present tense: \"change\" not \"changed\" nor \"changes\".</li> <li>Don't capitalize the first letter.</li> <li>No dot (.) at the end.</li> </ul>"}, {"location": "semantic_versioning/#body", "title": "Body", "text": "<p>Same as in the subject, use the imperative present tense. The body should include the motivation for the change and contrast this with previous behavior.</p>"}, {"location": "semantic_versioning/#footer", "title": "Footer", "text": "<p>The footer should contain any information about Breaking Changes and is also the place to reference issues that this commit Closes.</p> <p>Breaking Changes should start with the word <code>BREAKING CHANGE:</code> with a space or two newlines. The rest of the commit message is then used for this.</p>"}, {"location": "semantic_versioning/#revert", "title": "Revert", "text": "<p>If the commit reverts a previous commit, it should begin with <code>revert:</code> , followed by the header of the reverted commit. In the body it should say: <code>This reverts commit &lt;hash&gt;.</code>, where the hash is the SHA of the commit to revert.</p>"}, {"location": "semantic_versioning/#helpers", "title": "Helpers", "text": ""}, {"location": "semantic_versioning/#use-tool-to-bump-your-program-version", "title": "Use tool to bump your program version", "text": "<p>You can use the commitizen tool to:</p> <ul> <li>Automatically detect which type of change you're introducing and decide     which should be the next version number.</li> <li>Update the changelog</li> </ul> <p>By running <code>cz bump --changelog --no-verify</code>.</p> <p>The <code>--no-verify</code> part is required if you use pre-commit hooks.</p> <p>Whenever you want to release <code>1.0.0</code>, use <code>cz bump --changelog --no-verify --increment MAJOR</code>. If you are on a version <code>0.X.Y</code>, and you introduced a breaking change but don't want to upgrade to <code>1.0.0</code>, use the <code>--increment MINOR</code> flag.</p>"}, {"location": "semantic_versioning/#use-tool-to-create-the-commit-messages", "title": "Use tool to create the commit messages", "text": "<p>To get used to make correct commit messages, you can use the commitizen tool, that guides you through the steps of making a good commit message. Once you're used to the system though, it makes more sense to ditch the tool and write the messages yourself.</p> <p>In Vim, if you're using Vim fugitive you can change the configuration to:</p> <pre><code>nnoremap &lt;leader&gt;gc :terminal cz c&lt;CR&gt;\nnnoremap &lt;leader&gt;gr :terminal cz c --retry&lt;CR&gt;\n\n\" Open terminal mode in insert mode\nif has('nvim')\n    autocmd TermOpen term://* startinsert\nendif\nautocmd BufLeave term://* stopinsert\n</code></pre> <p>If some pre-commit hook fails, make the changes and then use <code>&lt;leader&gt;gr</code> to repeat the same commit message.</p>"}, {"location": "semantic_versioning/#pre-commit", "title": "Pre-commit", "text": "<p>To ensure that your project follows these guidelines, add the following to your pre-commit configuration:</p> <p>File: .pre-commit-config.yaml</p> <pre><code>- repo: https://github.com/commitizen-tools/commitizen\nrev: master\nhooks:\n- id: commitizen\nstages: [commit-msg]\n</code></pre>"}, {"location": "semantic_versioning/#when-to-do-a-major-release", "title": "When to do a major release", "text": "<p>Following the Semantic Versioning idea of a major update is problematic because:</p> <ul> <li>You can quickly get into the high version number problem.</li> <li>The fact that any change may break the users     code makes the definition of when a change should be major     blurry.</li> <li>Often the change that triggered the major change only affects a low percentage     of your users (usually those using that one feature you changed in an     incompatible fashion).</li> </ul> <p>Does dropping Python 2 require a major release? Many (most) packages did this, but the general answer is ironically no, it is not an addition or a breaking change, the version solver will ensure the correct version is used (unless the <code>Requires-Python</code> metadata slot is empty or not updated).</p> <p>If you mark a feature as deprecated (almost always in a minor release), you can remove that feature in a future minor release. You have to define in your library documentation what the deprecation period is. For example, NumPy and Python use three minor releases. Sometimes is useful to implement deprecations based on a period of time. SemVer purists argue that this makes minor releases into major releases, but as we've seen it\u2019s not that simple. The deprecation period ensures the \u201cnext\u201d version works, which is really useful, and usually gives you time to adjust before the removal happens. It\u2019s a great balance for projects that are well kept up using libraries that move forward at a reasonable pace. If you make sure you can see deprecations, you will almost always work with the next several versions.</p>"}, {"location": "semantic_versioning/#semantic-versioning-system-problems", "title": "Semantic versioning system problems", "text": "<p>On paper, semantic versioning seems to be addressing all we need to encode the evolution and state of our library. When implementation time comes some issues are raised though.</p> <p>!!! note \"The pitfalls mentioned below don't invalidate the Semantic Versioning system, you just need to be aware of them.\"</p>"}, {"location": "semantic_versioning/#maintaining-different-versions", "title": "Maintaining different versions", "text": "<p>Version numbers are just a mapping of a sequence of digits to our branching strategy in source control. For instance, if you are doing SemVer then your <code>X.Y.Z</code> version maps a branch to <code>X.Y</code> branch where you're doing your current feature work, an <code>X.Y.Z+1</code> branch for any bugfixes, and potentially an <code>X+1.0.0</code> branch where you doing some crazy new stuff. So you got your next branch, main branch, and bugfix branch. And all three of those branches are alive and receiving updates.</p> <p>For projects that have those 3 kinds of branches going, the concept of SemVer makes much more sense, but how many projects are doing that? You have to be a pretty substantial project typically to have the throughput to justify that much project overhead.</p> <p>There are a lot more projects that have a single <code>bugfix</code> branch and a <code>main</code> branch which has all feature work, whether it be massively backwards-incompatible or not. In that case why carry around two version numbers? This is how you end up with ZeroVer. If you're doing that why not just drop a digit and have your version be <code>X.Y</code>? PEP 440 supports it, and it would more truthfully represent your branching strategy appropriately in your version number. However, most library maintainers/developers out there don\u2019t have enough resources to maintain even two branches.</p> <p>Maintaining a library is very time-consuming, and most libraries have just a few active maintainers available that maintain other many libraries. To complicate matters even further, for most maintainers this is not a full-time job, but something on the side, part of their free time.</p> <p>Given the scarce human resources to maintain a library, in practice, there\u2019s a single supported version for any library at any given point in time: the latest one. Any version before that (be that major, minor, patch) is in essence abandoned:</p> <ul> <li>If you want security updates, you need to move to the latest version.</li> <li>If you want a bug to be fixed, you need to move to the newest version.</li> <li>If you want a new feature, it is only going to be available in the latest     version.</li> </ul> <p>If the only maintained version is the latest, you really just have an <code>X</code> version number that is monotonically increasing. Once again PEP 440 supports it, so why not! It still communicates your branch strategy of there being only a single branch at any one time. Now I know this is a bit too unconventional for some people, and you may get into the high version number problem, then maybe it makes sense to use calendar versioning to use the version number to indicate the release date to signify just how old of a version you\u2019re using, but if stuff is working does that really matter?</p>"}, {"location": "semantic_versioning/#high-version-numbers", "title": "High version numbers", "text": "<p>Another major argument is that people inherently judge a project based on what it\u2019s version number is. They\u2019ll implicitly assume that <code>foo 2.0</code> is better than <code>bar 1.0</code> (and <code>frob 3.0</code> is better still) because the version numbers are higher. However, there is a limit to this, if you go too high too quickly, people assume your project is unstable and shouldn\u2019t really be used, even if the reason that your project is so high is because you removed some tiny edge cases that nobody actually used and didn\u2019t actually impact many people, if any, at all.</p> <p>These are two different expressions of the same thing. The first is that people will look down on a project for not having a high enough version compared to its competitors. While it\u2019s true that some people will do this, it's not a significant reason to throw away the communication benefits of your version number. Ultimately, no matter what you do, people who judge a project as inferior because of something as shallow as \u201csmaller version number\u201d will find some other, equally shallow, reason to pick between projects.</p> <p>The other side of this is a bit different. When you have a large major version, like <code>42.0.0</code>, people assume that your library is not stable and that you regularly break compatibility and if you follow SemVer strictly, it does actually mean that you regularly break compatibility.</p> <p>There are two general cases:</p> <ul> <li>The true positives: where a project that does routinely break it\u2019s public API in meaningful ways.</li> <li>The false positives: Projects that strictly follow semantic versioning were     each change which is not backwards compatible requires bumping a major     version. This means that if you remove some function that nobody actually     uses you need to increase your major version. Do it again and you need to     increase your major version again. Do this enough times, for even very small     changes and you can quickly get into a large version number <code>6</code>. This case     is a false positive for the \u201cstability\u201d test, because the reality is that     your project is actually quite stable.</li> </ul>"}, {"location": "semantic_versioning/#difference-in-change-categorization", "title": "Difference in change categorization", "text": "<p>Here's a thought experiment: you need to add a new warning to your Python package that tries to follow SemVer. Would that single change cause you to increase the major, minor, or patch version number? You might think a patch number bump since it isn't a new feature or breaking anything. You might think it's a minor version bump because it isn't exactly a bugfix. And you might think it's a major version bump because if you ran your Python code with <code>-W</code> error you suddenly introduced a new exception which could break people's code. Brett Cannon did a poll, answered by 231 people with the results:</p> <ul> <li>Patch/Bugfix: 47.2%</li> <li>Minor/enhancement: 44.2%</li> <li>Major/breaking: 8.7%</li> </ul> <p>That speaks volumes to why SemVer does not inherently work: someone's bugfix may be someone else's breaking change. Because in Python we can't statically define what an API change is there will always be a disagreement between you and your dependencies as to what a \"feature\" or \"bugfix\" truly is.</p> <p>That builds one of the arguments for CalVer. Because SemVer is imperfect at describing if a particular change will break someone upgrading the software, that we should instead throw it out and replace it with something that doesn\u2019t purport to tell us that information.</p>"}, {"location": "semantic_versioning/#unintended-changes", "title": "Unintended changes", "text": "<p>A major version bump must happen not only when you rewrite an entire library with its complete API, but also when you\u2019re just renaming a single rarely used function (which some may erroneously view as a minor change). Or even worse, it\u2019s not always clear what\u2019s part of the public API and what\u2019s not.</p> <p>You have a library with some incidental, undocumented, and unspecified behavior that you consider to be obviously not part of the public interface. You change it to solve what seems like a bug to you, and make a patch release, only to find that you have angry hordes at the gate who, thanks to Hyrum\u2019s Law, depend on the old behavior.</p> <p>With a sufficient number of users of an API, it does not matter what you promise in the contract. All observable behaviors of your system will be depended on by somebody.</p> <p>Which has been represented perfectly by the people behind xkcd.</p> <p></p> <p>While every maintainer would like to believe they\u2019ve thought of every use case up-front and created the best API for everything. In practice it's impossible to think on every impact your changes will make.</p> <p>Even if you were very diligent/broad with your interpretation to avoid accidentally breaking people with a bugfix release, bugs can still happen in a bugfix release. It obviously isn't intentional, but it does happen which means SemVer can't protect you from having to test your code to see if a patch version is compatible with your code.</p> <p>This makes \u201ctrue\u201d SemVer pointless. Minor releases are impossible, and patch releases are nearly impossible. If you fix a bug, someone could be depending on the buggy behaviour.</p>"}, {"location": "semantic_versioning/#using-zerover", "title": "Using ZeroVer", "text": "<p>ZeroVer is a joke versioning system similar to Semantic Versioning with the sole difference that <code>MAJOR</code> is always <code>0</code>. From the specification, as long as you are in the <code>0.X.Y</code> versions, you can introduce incompatible changes at any point. It intended to make fun of people who use \u201csemantic versioning\u201d but never make a <code>1.0</code> release, thus defeating the purpose of semver.</p> <p>This one of the consequences of trying to strictly follow Semantic Versioning, because once you give the leap to <code>1.0</code> you need to increase the <code>major</code> on each change quickly leading to the problem of high version numbers. The best way to fight this behaviour is to remember the often overlooked SemVer 2.0 FAQ guideline:</p> <p>If your software is being used in production, it should probably already be 1.0.0. If you have a stable API on which users have come to depend, you should be 1.0.0. If you\u2019re worrying a lot about backwards compatibility, you should probably already be 1.0.0.</p>"}, {"location": "semantic_versioning/#when-to-use-it", "title": "When to use it", "text": "<p>Check the Deciding what version system to use for your programs article section.</p>"}, {"location": "semantic_versioning/#references", "title": "References", "text": "<ul> <li> <p>Home</p> </li> <li> <p>Bernat post on versioning</p> </li> <li>Why I don't like SemVer anymore by Snarky</li> <li>Versioning Software by donald stufft</li> </ul>"}, {"location": "semantic_versioning/#libraries", "title": "Libraries", "text": "<p>These libraries can be used to interact with a git history of commits that follow the semantic versioning commit guidelines.</p> <ul> <li>python-semantic-release</li> </ul>"}, {"location": "shag/", "title": "Shag", "text": "<p>Collection of shag videos:</p> <ul> <li> <p>Sandy Lewis and Mart\u00ed Gasol:</p> </li> <li> <p>Warsaw Collegiate Shag Festival 2017</p> </li> <li> <p>Warsaw Collegiate Shag Festival 2016</p> </li> <li> <p>Cherry Moreno and Filip Gorski:</p> </li> <li> <p>BCN Shag Festival 2018</p> </li> <li> <p>RTSF 2019</p> </li> <li> <p>Stephen and Chandrae:</p> </li> <li> <p>Minor Swing</p> </li> <li>[RTSF 2013](https://yewtu.be/watch?v=uUGsLoZYXb4</li> <li>RTSF 2016 with Arnas and Egle</li> <li> <p>MBSW 2013</p> </li> <li> <p>Warsaw Shag team</p> </li> <li>Dragon Swing 2016</li> <li>Warsaw Shag Festival 2018</li> <li>Warsaw Shag Festival 2017</li> </ul>"}, {"location": "signal/", "title": "Signal", "text": "<p>Signal is a cross-platform centralized encrypted messaging service developed by the Signal Technology Foundation and Signal Messenger LLC. It uses the Internet to send one-to-one and group messages, which can include files, voice notes, images and videos. It can also be used to make one-to-one and group voice and video calls.</p> <p>Signal uses standard cellular telephone numbers as identifiers and secures all communications to other Signal users with end-to-end encryption. The apps include mechanisms by which users can independently verify the identity of their contacts and the integrity of the data channel.</p> <p>Signal's software is free and open-source. Its clients are published under the GPLv3 license, while the server code is published under the AGPLv3 license. The official Android app generally uses the proprietary Google Play Services (installed on most Android devices), though it is designed to still work without them installed. Signal also has an official client app for iOS and desktop apps for Windows, MacOS and Linux.</p>"}, {"location": "signal/#pros-and-cons", "title": "Pros and cons", "text": "<p>Pros:</p> <ul> <li>Good security by default.</li> <li>Easy to use for non technical users.</li> <li>Good multi-device support.</li> </ul> <p>Cons:</p> <ul> <li>Uses phones to identify users.</li> <li>Centralized.</li> <li>Not available in     F-droid.</li> </ul>"}, {"location": "signal/#installation", "title": "Installation", "text": "<p>These instructions only work for 64 bit Debian-based Linux distributions such as Ubuntu, Mint etc.</p> <ul> <li>Install our official public software signing key</li> </ul> <pre><code>wget -O- https://updates.signal.org/desktop/apt/keys.asc | gpg --dearmor &gt; signal-desktop-keyring.gpg\ncat signal-desktop-keyring.gpg | sudo tee -a /usr/share/keyrings/signal-desktop-keyring.gpg &gt; /dev/null\n</code></pre> <ul> <li>Add our repository to your list of repositories</li> </ul> <pre><code>echo 'deb [arch=amd64 signed-by=/usr/share/keyrings/signal-desktop-keyring.gpg] https://updates.signal.org/desktop/apt xenial main' |\\\nsudo tee -a /etc/apt/sources.list.d/signal-xenial.list\n</code></pre> <ul> <li>Update your package database and install signal</li> </ul> <pre><code>sudo apt update &amp;&amp; sudo apt install signal-desktop\n</code></pre>"}, {"location": "signal/#backup-extraction", "title": "Backup extraction", "text": "<p>I'd first try to use signal-black.</p>"}, {"location": "signal/#references", "title": "References", "text": "<ul> <li>Home</li> </ul>"}, {"location": "sleep/", "title": "Sleep", "text": "<p>Sleep is a naturally recurring state of mind and body, characterized by altered consciousness, relatively inhibited sensory activity, reduced muscle activity and inhibition of nearly all voluntary muscles during rapid eye movement (REM) sleep,and reduced interactions with surroundings. Distinguished from wakefulness by a decreased ability to react to stimuli.</p> <p>Most of the content of this article is extracted from the Why we sleep book by Matthew Walker</p>"}, {"location": "sleep/#consequences-of-lack-of-sleep", "title": "Consequences of lack of sleep", "text": "<p>Sleeping less than six or seven hours a night can produce these consequences:</p> <ul> <li>Demolishing of the immune system.</li> <li>Doubling your risk of cancer.</li> <li>Is a key lifestyle factor determining and worsening the development of the Alzheimer's     disease.</li> <li>Disruption of blood sugar levels so profoundly that you would be classified as     pre-diabetic.</li> <li>Increase the likelihood of block and brittle of your coronary arteries.     Setting you on a path toward cardiovascular disease, stroke, and congestive     heart failure.</li> <li>Contributes to all major psychiatric conditions, including depression,     anxiety, and suicidality.</li> <li>Swelling concentrations of a hormone that makes you feel hungry while     suppressing a companion hormone that otherwise signals food satisfaction.</li> <li>Thwart the ability to learn and memorize.</li> </ul> <p>A balanced diet and exercise are of vital importance, but we now see sleep as the key factor in health. The physical and mental impairments caused by one night of bad sleep dwarf those caused by an equivalent absence of food or exercise.</p> <p>Therefore, the shorter you sleep, the shorter your life span.</p>"}, {"location": "sleep/#sleep-benefits", "title": "Sleep benefits", "text": "<p>We sleep for a lot of nighttime benefits that service both our brains and our body. There does not seem to be one major organ within the body, or process within the brain, that isn't optimally enhanced by sleep.</p> <p>Within the brain, sleep enriches our ability to learn, memorize and make logical decisions and choices. It recalibrates our emotional brain circuits, allowing us to navigate next day social and psychological challenges with cool-headed composture.</p> <p>Downstairs in the body, sleep:</p> <ul> <li>Restocks the armory of our immune system: helping fight malignancy, preventing     infection, and warding off sickness.</li> <li>Reforms the body's metabolic state by fine-tuning the balance of insulin and     circulating glucose.</li> <li>Regulates our appetite, helping control body weight through healthy food     selection rather than rash impulsivity.</li> <li>Maintains a flourishing microbiome within your gut essential to our     nutritional health being.</li> <li>Is tied to the fitness of our cardiovascular system, lowering blood pressure     while keeping our hearts in fine condition.</li> </ul> <p>Dreaming produces a neurochemical bath that mollifies painful memories and a virtual reality space in which the brain melds past and present knowledge, inspiring creativity.</p> <p>Therefore, Sleep is the single most effective thing we can do to reset our brain and body health each day.</p>"}, {"location": "sleep/#sleep-physiological-effects", "title": "Sleep physiological effects", "text": "<p>There are two main factors that determine when you want to sleep or stay awake:</p> <ul> <li>The signal sent by the suprachiasmatic nucleus following the circadian rhythm.</li> <li>Sleep pressure: The brain builds up a chemical substance that creates     the \"sleep pressure\". The longer you've been awake, the more that chemical     sleep pressure accumulates, and consequentially, the sleepier you feel.</li> </ul>"}, {"location": "sleep/#the-circadian-rhythm", "title": "The circadian rhythm", "text": "<p>We have an internal clock deep within the brain, called the suprachiasmatic nucleus, that creates a cycling, day-night rhythm, known as circadian rhythm, that makes you feel tired or alert at regular times of night and day, respectively. The circadian rhythm determines:</p> <ul> <li>When you want to be awake or asleep.</li> <li>Your timed preferences for eating and drinking.</li> <li>Your moods and emotions</li> <li>The amount of urine you produce.</li> <li>Your core body temperature.</li> <li>Your metabolic rate.</li> <li>The release of numerous hormones.</li> </ul> <p>Contrary to common belief, circadian rhythm is not defined by the daylight sun cycle. As Kleitman and Richardson demonstrated in 1938:</p> <ul> <li>When cut off from the daily cycle of light and dark, the body keeps on     maintaining the rhythm.</li> <li>The period of the circadian rhythm is different for each person, but has an     average of 24 hours and 15 minutes.</li> </ul> <p>Even if it's not defined by the sun light, it corrects those 15 minutes of delay to stay in sync with it. The suprachiasmatic nucleus can readjust by about one hour each day, that is why jet lag can be spawn through multiple days.</p> <p>That reset does not come free. Studies in airplane cabin crews who frequently fly on long haul routes and have little chance to recover have registered:</p> <ul> <li>The part of the brains related to learning and memory had physically shrunk,     suggesting the destruction of brain cells caused by the biological stress of     timezone travel.</li> <li>Their short term memory was significantly impaired.</li> <li>They had far higher rates of cancer and type 2 diabetes than the general     population.</li> </ul> <p>The peak and valley points of wakefulness or sleepiness vary too between people, it's known as their chronotype and it's strongly determined by genetics. The chronotype defines three types of people:</p> <ul> <li>Morning types: They have their peak of wakefulness early in the day and the     sleepiness early at night. They prefer to wake at or around dawn, and     function optimally at this time of day.</li> <li>Evening types: They prefer going to bed late and subsequently wake up late     the following morning, or even in the afternoon.</li> <li>In between: The remaining people fall somewhere in between, with a slight     leaning towards eveningness.</li> </ul>"}, {"location": "sleep/#melatonin", "title": "Melatonin", "text": "<p>The suprachiasmatic nucleus communicates its repeating signal of day and night to your brain and body by releasing melatonin into the bloodstream from the pineal gland. Soon after dusk, the suprachiasmatic nucleus starts increasing the levels of this hormone, telling the rest of the body that it's time to sleep. But melatonin has little influence on the generation of sleep itself.</p> <p>Once sleep is under way, melatonin decreases in concentration across the night and into the morning hours. With dawn, as sunlight enters the brain through the eyes (even through the closed lids), the pineal gland is instructed to stop releasing melatonin. The absence of circulating melatonin now informs the brain and body that it's time to return to a wakefulness active state for the rest of the day</p>"}, {"location": "sleep/#sleep-pressure", "title": "Sleep pressure", "text": "<p>While you are awake, the brain is releasing a chemical called adenosine. One consequence of the increasing accumulation of adenosine is the increase of the desire to sleep by turning down the \"volume\" of wake promoting regions in the brain and turning up the sleep inducing ones. Most people fall to the pressure after twelve to sixteen hours of being awake.</p>"}, {"location": "sleep/#caffeine", "title": "Caffeine", "text": "<p>You can artificially mute the sleep signal of adenosine by using a chemical that makes you feel more alert and awake, such as caffeine. Caffeine works by battling with adenosine for the privilege of latching on to adenosine receptors in the brain. Once caffeine occupies these receptors, it does not stimulate them like adenosine, making you sleepy. Rather, caffeine blocks and effectively inactivates the receptors acting as a masking agent.</p> <p>Levels of caffeine peak around thirty minutes after ingestion. What is problematic, though, is the persistence of caffeine in your system. It takes between five to seven hours to remove 50 percent of the caffeine concentration from your body.</p> <p>An enzyme within your liver removes caffeine from your system. Based in large part on genetics, some people have a more efficient version of the enzyme that degrades caffeine, allowing the liver to clear it from the bloodstream. Age is also a variable to take into account, the older we are the longer it takes our brain and body to remove it.</p> <p>When your liver evicts the caffeine from your system, you encounter the caffeine crash. Your energy levels plummet rapidly, finding difficult to function and concentrate, with a strong sense of sleepiness once again.</p> <p>For the entire time that caffeine is in your system, the adenosine keeps on building up. Your brain is not aware of this rising tide of sleep encouraging chemical because the wall of caffeine is holding it back from your perception. Once your liver dismantles the barricade, you feel a vicious backlash: you are hit with the sleepiness you had experienced two or three hours ago before you drank that cup of coffee plus all the extra adenosine that has accumulated in the hours in between.</p>"}, {"location": "sleep/#relationship-between-the-circadian-rhythm-and-the-sleep-pressure", "title": "Relationship between the circadian rhythm and the sleep pressure", "text": "<p>The two governing forces that regulate your sleep are ignorant of each other. Although they are not coupled, they are usually aligned.</p> <p></p> <p>Starting on the far left of the figure, the circadian rhythm begins to increase its activity a few hours before you wake up. It infuses the brain and body with an alerting energy signal. At first, the signal is faint, but gradually it builds with time. By early afternoon, the activating signal from the circadian rhythm peaks.</p> <p>Now let's look at the sleep pressure pattern. By mid to late morning, you have only been awake for a half of hours. As a result, adenosine concentrations have increased a little. Furthermore, the circadian rhythm is on its powerful upswing of alertness. This combination of strong activating output from the circadian rhythm together with low levels of adenosine result in a delightful sensation of being wide awake.</p> <p>The distance between the curved lines above will be a direct reflection of your desire to sleep.</p> <p>By eleven pm, you've been awake for fifteen hours, and your brain is drenched in high concentrations of adenosine. Additionally, the circadian rhythm line is descending, powering down your activity and alertness levels. This powerful combination triggers a strong desire for sleep.</p> <p>During sleep, a mass evacuation of adenosine gets under way as the brain has the chance to degrade and remove it. After eight hours of healthy sleep, the adenosine purge is complete. As this process is ending, the circadian activity rhythm has returned, and its energizing influence starts to approach, therefore naturally waking us up.</p>"}, {"location": "sleep/#all-nighters", "title": "All-nighters", "text": "<p>Scientists can demonstrate that the two forces determining when you want to be awake and sleep are independent and can be decoupled from their normal lockstep.</p> <p>When you skip one night's sleep and remain awake throughout the following day,</p> <p></p> <p>By remaining awake, and blocking access to the adenosine drain that sleep opens up, the brain is unable to rid itself of the chemical sleep pressure. The mounting adenosine levels continue to rise. This should mean that the longer you are awake, the sleepier you feel. But that's not true. Though you will feel increasingly sleepy throughout the nighttime phase, hitting a low point in your alertness around five or six in the morning, thereafter, you'll start to be more awake.  This effect is answered by the energy return of the circadian rhythm. Unlike sleep pressure, the circadian rhythm pays no attention to whether you are asleep or awake.</p>"}, {"location": "sleep/#am-i-getting-enough-sleep", "title": "Am I getting enough sleep?", "text": "<p>When you don't sleep enough, one consequence among many is that adenosine concentrations remain too high, so the next morning you continue to accumulate sleep debt</p> <p>If after waking up you could fall asleep at ten or eleven in the morning, it means that you're likely not getting enough sleep quantity or quality. The same can be said if you can't function optimally without caffeine before noon, you'll be most likely self-medicating your state of chronic sleep deprivation.</p> <p>Other sleep indicators can be if you would sleep more if you didn't set an alarm clock, or if you find yourself at your computer screen reading and then rereading the same sentence.</p> <p>Of course, even if you are giving yourself plenty of time to get a full night of shut-eye, next-day fatigue and sleepiness can still occur because you are suffering from an undiagnosed sleep disorder.</p>"}, {"location": "sleep/#the-sleep-cycle", "title": "The sleep cycle", "text": "<p>Humans cycle through two types of sleep in a regular pattern throughout the night with a period of 90 minutes. They were called non-rapid eye movement (NREM) and rapid eye movement (REM). Later, NREM sleep was further subdivided into four separate stages, named from 1 to 4 (all awful names <code>(\u0482\u2323\u0300_\u2323\u0301)</code>). Stages 3 and 4 are the deepest stages of NREM sleep, meaning that it's more difficult to wake you up in comparison with stages 1 and 2.</p> <p>In REM sleep, your eyes rapidly move from side to side underneath the lids. This movement are accompanied by active brainwaves, almost identical to those observed when you are awake. On the other hand, eyes remain still and the brainwaves also calm down in the NREM phases.</p> <p></p> <p>Even though we switch from sleep phases each 90 minutes, the ratio of NREM to REM sleep throughout the night changes across the night. In the first half of the night, the vast majority of time is spent in deep NREM and very little REM. But as we transition through the second half of the night, REM starts dominating.</p> <p>Although there is no scientific consensus, the need to remodel and update our neural circuits at night can explain this repeatable but asymmetric pattern. Throughout the day, the new memories are stored in the RAM of your brain, when you start to sleep, the brain needs to move the important ones to the hard drive, for long term retrieval. The brain needs to solve an optimization problem:</p> <ul> <li>The hard drive and the RAM have limited capacity.</li> <li>The RAM needs to be cleaned to be able to register the next day's memories.</li> <li>The brain needs RAM to do the analysis of which memories to keep and     which to remove.</li> </ul> <p>A key function of NREM sleep is to remove unnecessary neural connections, while REM sleep plays a role in strengthening those connections. The different roles and the capacity limits explains why the brain needs to switch between them. The asymmetry can be explained with the simile of creating a sculpture from a block of clay. At the beginning of the night, the long phases of NREM extensively removes unneeded material, with short REM phases to define the basic form. With each cycle, less material needs to be strongly removed and more enhancing of the details is required, thus the increase of REM sleep.</p> <p>A danger resides in this sleep profile. Since your brain desires most of its REM sleep in the last part of the night, if you wake up early, sleeping 6 hours instead of 8, you can be losing between 60 to 90% of all your REM sleep, even though you are losing 25% of your total sleep time. It works both ways, if you instead go to sleep two hours late, you'll loose a significant amount of deep NREM sleep. Preventing the brain to have the required REM or NREM daily rations results in many physical and mental issues.</p>"}, {"location": "sleep/#sleeping-time-and-sense-distortions", "title": "Sleeping time and sense distortions", "text": "<p>When you're asleep, you loose awareness of the outside world. Your ears are still hearing, your eyes, though closed, are still seeing, and the rest of the organs keep on working too.</p> <p>All these signals still flood into the center of your brain, but it's there, in the sensory convergence zone, where they end. The thalamus is the sensory gate to the brain that blocks them. If it lets them pass, they travel to the cortex at the top of your brain, where they are consciously perceived. By locking its gates shut when you're asleep, the thalamus imposes a sensory blackout in the brain. As a result, you are no longer consciously aware of the information transmitted from your sense organs.</p> <p>Another consequence of sleeping is a sense of time distortion experienced in two contradictory ways. While you loose your conscious mapping during sleep, at a non-conscious level, the brain keeps track of time with incredible precision. To distort it even more, you sense a time dilation in dreams. The signature patterns of brain-cell activity that occurs as you learn, gets recurrently repeated during sleep. That is, memories are being replayed at the level of brain-cell activity as you sleep. During REM sleep, the memories are replayed at half or quarter the speed in comparison of the activity when you're awake. This slow neural recounting may be the reason why we have that time dilation.</p>"}, {"location": "sleep/#how-your-brain-generates-sleep", "title": "How your brain generates sleep", "text": "<p>Brainwave activity of REM sleep looks similar to the one you have when you're awake. They cycle (going up and down) at a fast frequency of thirty or forty times per second in an unreliable pattern. This behaviour is explained by the fact that different parts of your waking brain are processing different pieces of information at different moments in time and in different ways.</p> <p></p>"}, {"location": "sleep/#references", "title": "References", "text": "<ul> <li>Why we sleep book by Matthew Walker</li> </ul>"}, {"location": "sponsor/", "title": "Sponsor", "text": "<p>It may arrive the moment in your life where someone wants to sponsor you. There are many sponsoring platforms you can use, each has their advantages and disadvantages.</p> <ul> <li>Liberapay.</li> <li>Ko-fi.</li> <li>Buy me a coffee.</li> <li>Github Sponsor.</li> </ul> Liberapay Ko-fi Buy Me a Coffee Github Sponsor Non-profit Yes No No No! (Microsoft!) Monthly fee No No No No Donation Commission 0% 0% 5% Not clear Paid plan No Yes No No Payment Processors Stripe, Paypal Stripe, Paypal Stripe, Standard Payout Stripe One time donations Possible but not user friendly Yes Yes Yes Membership Yes Yes Yes Yes Shop/Sales No Yes No No Based in France ? United States United States? Pay delay Instant Instant Instant Until 100$ User friendliness OK Good Good Good <p>Liberapay is the only non-profit recurrent donations platform. It's been the most recommended platform from the people I know from the open-source, activist environment.</p> <p>Ko-fi would be my next choice, as they don't do commissions on the donations and they support more features (that I don't need right now) than Liberapay.</p> <p>Each of these platforms use different payment processors such as:</p> <ul> <li>Stripe</li> <li>Paypal</li> </ul> <p>Usually Stripe takes less commissions than Paypal, also Paypal is known for closing user accounts and keeping their money.</p>"}, {"location": "sponsor/#conclusion", "title": "Conclusion", "text": "<p>If you just want something simple and don't mind the difficulties of doing one time donations of Liberapay, go with it, it's also the most ethical. If you want something more powerful, then Ko-fi is the best solution. You can even have both.</p> <p>Try to avoid Paypal and use Stripe for both platforms.</p>"}, {"location": "sponsor/#github-integration", "title": "Github integration", "text": ""}, {"location": "sponsor/#ko-fi-github-integration", "title": "Ko-fi Github integration", "text": "<ul> <li>Add <code>ko_fi: your_account_id</code> to the <code>.github/FUNDING.yml</code> file.</li> <li>Add a widget to your   <code>README.md</code>.</li> </ul>"}, {"location": "sponsor/#liberapay-github-integration", "title": "Liberapay Github integration", "text": "<ul> <li>Add <code>liberapay: your_user</code> to the <code>.github/FUNDING.yml</code> file.</li> <li>Add a widget to your <code>README.md</code>. You can get them on the Widgets section of   your settings.</li> </ul>"}, {"location": "sqlite/", "title": "SQLite", "text": "<p>SQLite is a relational database management system (RDBMS) contained in a C library. In contrast to many other database management systems, SQLite is not a client\u2013server database engine. Rather, it is embedded into the end program.</p> <p>SQLite is ACID-compliant and implements most of the SQL standard, generally following PostgreSQL syntax. However, SQLite uses a dynamically and weakly typed SQL syntax that does not guarantee the domain integrity.[7] This means that one can, for example, insert a string into a column defined as an integer. SQLite will attempt to convert data between formats where appropriate, the string \"123\" into an integer in this case, but does not guarantee such conversions and will store the data as-is if such a conversion is not possible.</p> <p>SQLite is a popular choice as embedded database software for local/client storage in application software such as web browsers. It is arguably the most widely deployed database engine, as it is used today by several widespread browsers, operating systems, and embedded systems (such as mobile phones), among others.</p>"}, {"location": "sqlite/#operators-and-statements", "title": "Operators and statements", "text": ""}, {"location": "sqlite/#upsert-statements", "title": "Upsert statements", "text": "<p>UPSERT is a special syntax addition to INSERT that causes the INSERT to behave as an UPDATE or a no-op if the INSERT would violate a uniqueness constraint. UPSERT is not standard SQL. UPSERT in SQLite follows the syntax established by PostgreSQL.</p> <p>The syntax that occurs in between the \"ON CONFLICT\" and \"DO\" keywords is called the \"conflict target\". The conflict target specifies a specific uniqueness constraint that will trigger the upsert. The conflict target is required for DO UPDATE upserts, but is optional for DO NOTHING. When the conflict target is omitted, the upsert behavior is triggered by a violation of any uniqueness constraint on the table of the INSERT.</p> <p>If the insert operation would cause the uniqueness constraint identified by the conflict-target clause to fail, then the insert is omitted and either the DO NOTHING or DO UPDATE operation is performed instead. In the case of a multi-row insert, this decision is made separately for each row of the insert.</p> <p>The special UPSERT processing happens only for uniqueness constraint on the table that is receiving the INSERT. A \"uniqueness constraint\" is an explicit UNIQUE or PRIMARY KEY constraint within the CREATE TABLE statement, or a unique index. UPSERT does not intervene for failed NOT NULL or foreign key constraints or for constraints that are implemented using triggers.</p> <p>Column names in the expressions of a DO UPDATE refer to the original unchanged value of the column, before the attempted INSERT. To use the value that would have been inserted had the constraint not failed, add the special \"excluded.\" table qualifier to the column name.</p> <pre><code>CREATE TABLE phonebook2(\nname TEXT PRIMARY KEY,\nphonenumber TEXT,\nvalidDate DATE\n);\nINSERT INTO phonebook2(name,phonenumber,validDate)\nVALUES('Alice','704-555-1212','2018-05-08')\nON CONFLICT(name) DO UPDATE SET\nphonenumber=excluded.phonenumber,\nvalidDate=excluded.validDate\n</code></pre>"}, {"location": "sqlite/#regexp", "title": "REGEXP", "text": "<p>The REGEXP operator is a special syntax for the <code>regexp()</code> user function. No <code>regexp()</code> user function is defined by default and so use of the REGEXP operator will normally result in an error message. If an application-defined SQL function named <code>regexp</code> is added at run-time, then the <code>X REGEXP Y</code> operator will be implemented as a call to <code>regexp(Y,X)</code>. If you're using sqlite3, you can check how to create the regexp function.</p>"}, {"location": "sqlite/#snippets", "title": "Snippets", "text": ""}, {"location": "sqlite/#import-a-table-from-another-database", "title": "Import a table from another database", "text": "<p>If you have an SQLite databases named <code>database1</code> with a table <code>t1</code> and <code>database2</code> with a table <code>t2</code> and want to import table <code>t2</code> from <code>database2</code> into <code>database1</code>. You need to open <code>database1</code> with <code>litecli</code>:</p> <pre><code>litecli database1\n</code></pre> <p>Attach the other database with the command:</p> <pre><code>ATTACH 'database2file' AS db2;\n</code></pre> <p>Then create the table <code>t2</code>, and copy the data over with:</p> <pre><code>INSERT INTO t2 SELECT * FROM db2.t2;\n</code></pre>"}, {"location": "sqlite/#get-the-columns-of-a-database", "title": "Get the columns of a database", "text": "<pre><code>PRAGMA table_info(table_name);\n</code></pre>"}, {"location": "sqlite/#troubleshooting", "title": "Troubleshooting", "text": ""}, {"location": "sqlite/#integer-autoincrement-not", "title": "[Integer autoincrement not", "text": "<p>working](https://stackoverflow.com/questions/16832401/sqlite-auto-increment-not-working)</p> <p>Rename the column type from <code>INT</code> to <code>INTEGER</code> and it starts working.</p> <p>From this:</p> <pre><code>CREATE TABLE IF NOT EXISTS foo (id INT PRIMARY KEY, bar INT)\n</code></pre> <p>to this:</p> <pre><code>CREATE TABLE IF NOT EXISTS foo (id INTEGER PRIMARY KEY, bar INT)\n</code></pre>"}, {"location": "sqlite/#references", "title": "References", "text": "<ul> <li>Home</li> <li>rqlite: is a lightweight, distributed     relational database, which uses SQLite as its storage engine. Forming     a cluster is very straightforward, it gracefully handles leader elections,     and tolerates failures of machines, including the leader.</li> </ul>"}, {"location": "sqlite3/", "title": "SQLite3", "text": "<p>SQLite3 is a python library that provides an SQL interface compliant with the DB-API 2.0 specification described by PEP 249.</p>"}, {"location": "sqlite3/#usage", "title": "Usage", "text": "<p>To use the module, you must first create a Connection object that represents the database, and a Cursor one to interact with it. Here the data will be stored in the <code>example.db</code> file:</p> <pre><code>import sqlite3\n\nconn = sqlite3.connect('example.db')\ncursor = conn.cursor()\n</code></pre> <p>Once we have a cursor we can <code>execute</code> the different SQL statements and the save them with the <code>commit</code> method of the Connection object. Finally we can close the connection with <code>close</code>.</p> <pre><code># Create table\ncursor.execute('''CREATE TABLE stocks\n             (date text, trans text, symbol text, qty real, price real)''')\n\n# Insert a row of data\ncursor.execute(\"INSERT INTO stocks VALUES ('2006-01-05','BUY','RHAT',100,35.14)\")\n\n# Save (commit) the changes\nconn.commit()\n\n# We can also close the connection if we are done with it.\n# Just be sure any changes have been committed or they will be lost.\nconn.close()\n</code></pre>"}, {"location": "sqlite3/#get-columns-of-a-query", "title": "Get columns of a query", "text": "<pre><code>cursor = connection.execute('select * from bar')\nnames = [description[0] for description in cursor.description]\n</code></pre>"}, {"location": "sqlite3/#get-a-list-of-the-tables", "title": "Get a list of the tables", "text": "<pre><code>sql_query = \"\"\"SELECT name FROM sqlite_master\n  WHERE type='table';\"\"\"\ncursor = sqliteConnection.cursor()\ncursor.execute(sql_query)\nprint(cursor.fetchall())\n</code></pre>"}, {"location": "sqlite3/#regexp", "title": "Regexp", "text": "<p>SQLite needs the user to define a regexp function to be able to use the filter.</p> <pre><code>import sqlite3\nimport re\n\ndef regexp(expr, item):\n    reg = re.compile(expr)\n    return reg.search(item) is not None\n\nconn = sqlite3.connect(':memory:')\nconn.create_function(\"REGEXP\", 2, regexp)\ncursor = conn.cursor()\n</code></pre>"}, {"location": "sqlite3/#references", "title": "References", "text": "<ul> <li>Docs</li> </ul>"}, {"location": "storage/", "title": "Storage", "text": "<p>I have a server at home to host some services for my closest ones. The server is an Intel NUC which is super awesome in terms of electric consumption, CPU and RAM versus cost. The downside is that it has no hard drive to store the services data. It does have some USB ports to connect external hard drives though. As the data kept growing I started buying bigger drives. While it was affordable I purchased two so as to have one to store the backup of the data. The problem came when it became unaffordable for me. Then I took the good idea to assume that I could only have one drive of 16TB with my data. Obviously the inevitable happened. The hard drive died and those 10TB of data that were not stored in any backup were lost.</p> <p>Luckily enough, it was not unique data like personal photos. The data could be regenerated by manual processes at the cost of precious time (I'm still suffering this <code>:(</code>). But every cloud has a silver lining, this failure gave me the energy and motivation to improve my home architecture. To prevent this from happening again, the solution needs to be:</p> <ul> <li>Robust: If disks die I will have time to replace them before data is lost.</li> <li>Flexible: It needs to expand as the data grows.</li> <li>Not very expensive.</li> <li>Easy to maintain.</li> </ul> <p>There are two types of solutions to store data:</p> <ul> <li>On one host: All disks are attached to a server and the storage capacity is     shared to other devices by the local network.</li> <li>Distributed: The disks are attached to many servers and they work together to     provide the storage through the local network.</li> </ul> <p>A NAS server represents the first solution, while systems like Ceph or GlusterFS over Odroid HC4 fall into the second.</p> <p>Both are robust and flexible but I'm more inclined towards building a NAS because it can hold the amount of data that I need, it's easier to maintain and the underlying technology has been more battle proven throughout the years.</p>"}, {"location": "strategy/", "title": "Strategy", "text": "<p>Strategy is a general plan to achieve one or more long-term or overall goals under conditions of uncertainty.</p> <p>Strategy is important because the resources available to achieve goals are usually limited. Strategy generally involves setting goals and priorities, determining actions to achieve the goals, and mobilizing resources to execute the actions. A strategy describes how the ends (goals) will be achieved by the means (resources). Strategy can be intended or can emerge as a pattern of activity as the person or organization adapts to its environment.</p> <p>It typically involves two major processes:</p> <ul> <li> <p>Formulation: Involves analyzing the environment or situation, making     a diagnosis, and developing guiding policies. It includes such activities as     strategic planning and strategic     thinking.</p> </li> <li> <p>Implementation: Refers to the action plans taken to achieve the goals     established by the guiding policy.</p> </li> </ul>"}, {"location": "strategy/#strategic-planning", "title": "Strategic planning", "text": "<p>Strategic planning is an organization's process of defining its strategy, or direction, and making decisions on allocating its resources to pursue this strategy. It helps coordinate the two processes required by the strategy, formulation and implementation. However, strategic planning is analytical in nature (i.e., it involves \"finding the dots\"); strategy formation itself involves synthesis (i.e., \"connecting the dots\") via strategic thinking. As such, strategic planning occurs around the strategy formation activity.</p>"}, {"location": "strategy/#strategic-thinking", "title": "Strategic thinking", "text": "<p>Strategic thinking is defined as a mental or thinking process applied by an individual in the context of achieving a goal or set of goals in a game or other endeavor. As a cognitive activity, it produces thought.</p> <p>Strategic thinking includes finding and developing a strategic foresight capacity for an organization or individual, by exploring all possible futures, and challenging conventional thinking to foster decision making today.</p> <p>The strategist must have a great capacity for both analysis and synthesis; analysis is necessary to assemble the data on which he makes his diagnosis, synthesis in order to produce from these data the diagnosis itself\u2014and the diagnosis in fact amounts to a choice between alternative courses of action.</p>"}, {"location": "sudokus/", "title": "Sudoku", "text": "<p>Sudoku is a logic-based, combinatorial number-placement puzzle. In classic Sudoku, the objective is to fill a 9 \u00d7 9 grid with digits so that each column, each row, and each of the nine 3 \u00d7 3 subgrids that compose the grid (also called \"boxes\", \"blocks\", or \"regions\") contain all of the digits from 1 to 9. The puzzle setter provides a partially completed grid, which for a well-posed puzzle has a single solution.</p>"}, {"location": "sudokus/#sudoku-strategies", "title": "Sudoku strategies", "text": "<ul> <li>Hidden pairs.</li> <li>Hidden triplets.</li> <li>Naked triplets.</li> </ul>"}, {"location": "tahoe/", "title": "Tahoe-LAFS", "text": "<p>Tahoe-LAFS is a free and open, secure, decentralized, fault-tolerant, distributed data store and distributed file system.</p> <p>Tahoe-LAFS is a system that helps you to store files. You run a client program on your computer, which talks to one or more storage servers on other computers. When you tell your client to store a file, it will encrypt that file, encode it into multiple pieces, then spread those pieces out among multiple servers. The pieces are all encrypted and protected against modifications. Later, when you ask your client to retrieve the file, it will find the necessary pieces, make sure they haven\u2019t been corrupted, reassemble them, and decrypt the result.</p> <p>The client creates more pieces (or \u201cshares\u201d) than it will eventually need, so even if some of the servers fail, you can still get your data back. Corrupt shares are detected and ignored, so the system can tolerate server-side hard-drive errors. All files are encrypted (with a unique key) before uploading, so even a malicious server operator cannot read your data. The only thing you ask of the servers is that they can (usually) provide the shares when you ask for them: you aren\u2019t relying upon them for confidentiality, integrity, or absolute availability.</p> <p>Tahoe does not provide locking of mutable files and directories. If there is more than one simultaneous attempt to change a mutable file or directory, then an <code>UncoordinatedWriteError</code> may result. This might, in rare cases, cause the file or directory contents to be accidentally deleted. The user is expected to ensure that there is at most one outstanding write or update request for a given file or directory at a time. One convenient way to accomplish this is to make a different file or directory for each person or process that wants to write.</p> <p>If mutable parts of a file store are accessed via sshfs, only a single sshfs mount should be used. There may be data loss if mutable files or directories are accessed via two sshfs mounts, or written both via sshfs and from other clients.</p>"}, {"location": "tahoe/#installation", "title": "Installation", "text": "<pre><code>apt-get install tahoe-lafs\n</code></pre> <p>Or if you want the latest version</p> <pre><code>pip install tahoe-lafs\n</code></pre> <p>If you plan to connect to servers protected through Tor, use <code>pip install tahoe-lafs[tor]</code> instead.</p>"}, {"location": "tahoe/#troubleshooting", "title": "Troubleshooting", "text": ""}, {"location": "tahoe/#pkg_resourcesdistributionnotfound-the-idna-distribution-was-not-found-and-is-required-by-twisted", "title": "pkg_resources.DistributionNotFound: The idna  distribution was not found and is required by Twisted", "text": "<pre><code>apt-get install python-idna\n</code></pre>"}, {"location": "tahoe/#references", "title": "References", "text": "<ul> <li>Git</li> <li>Docs</li> <li>Issues</li> </ul>"}, {"location": "talkey/", "title": "Talkey", "text": "<p>Talkey is a Simple Text-To-Speech (TTS) interface library with multi-language and multi-engine support.</p>"}, {"location": "talkey/#installation", "title": "Installation", "text": "<pre><code>pip install talkey\n</code></pre> <p>You need to install the TTS engines by yourself. Talkey supports:</p> <ul> <li>Flite</li> <li>SVOX Pico</li> <li>Festival</li> <li>eSpeak</li> <li>mbrola via eSpeak</li> </ul> <p>I've tried SVOX Pico, Festival and eSpeak. I've discarded Flite because it's not in the official repositories. Of those three the one that gives the most natural support is SVOX Pico. To install it execute:</p> <pre><code>sudo apt-get install libttspico-utils\n</code></pre> <p>It also supports the following networked TTS Engines:</p> <ul> <li>MaryTTS (needs hosting).</li> <li>Google TTS (cloud hosted)</li> </ul> <p>I obviously discard Google for privacy reasons, and MaryTTS too because it needs you to run a server, which is inconvenient for most users and pico gives us enough quality.</p>"}, {"location": "talkey/#usage", "title": "Usage", "text": "<p>At its simplest use case:</p> <pre><code>import talkey\ntts = talkey.Talkey()\ntts.say(\"I've been really busy being dead. You know, after you murdered me.\")\n</code></pre> <p>It automatically detects languages without any further configuration:</p> <pre><code>tts.say(\"La cabra siempre tira al monte\")\n</code></pre>"}, {"location": "talkey/#references", "title": "References", "text": "<ul> <li>Git</li> <li>Docs</li> </ul>"}, {"location": "task_management/", "title": "Task Management", "text": "<p>Task management is the process of managing a task through its life cycle. It involves planning, testing, tracking, and reporting. Task management can help either individual achieve goals, or groups of individuals collaborate and share knowledge for the accomplishment of collective goals.</p> <p>You can address task management at different levels. High level management ensures that you choose your tasks in order to accomplish a goal, low level management helps you get things done.</p> <p>When you do task management well, you benefit from:</p> <ul> <li>Reducing your mental load, so you can use those resources doing productive     work.</li> <li>Improving your efficiency.</li> <li>Making more realistic estimations, thus meeting the commited deadlines.</li> <li>Finishing what you start.</li> <li>Knowing you're working towards your ultimate goals</li> <li>Stop feeling lost or overburdened.</li> <li>Make context switches cheaper.</li> </ul> <p>On the other side, task management done wrong can consume your willpower in the exchange of lost time and a confused mind.</p> <p>The tricky reality is that the factors that decide if you do it right or wrong are different for every person, and even for a person it may change over the time or mood states. That's why I follow the thought that task management is a tool that is meant to help you. If it's not, you need to change your system until it does.</p> <p>A side effect is that you have to tailor your task management system yourself. It doesn't matter how good the systems you find in the internet are, until you start getting your hands dirty, you won't know if they works for you. So instead of trying to discover the perfect solution, start with one that introduces the least friction in your current workflow, and evolve from that point guided by the faults you find. Forget about instant solutions, this is a never ending marathon. Make sure that each step is small and easy, otherwise you will get tired too soon.</p> <p>I haven't written a guide yet on how to give your first steps, but you could start by following a simple workflow with simple tools.</p>"}, {"location": "task_tools/", "title": "Task Management tools", "text": "<p>I currently use two tools to manage my tasks: the inbox and the task manager.</p>"}, {"location": "task_tools/#inbox", "title": "Inbox", "text": "<p>The inbox does not refer only to your e-mail inbox. It is a broader concept that includes all the elements you have collected in different ways: tasks you have to do, ideas you have thought of, notes, bills, etc\u2026</p> <p>To achieve a stress-free productivity, emptying the inbox should be a daily activity. Note that this does not mean doing things, it just means identifying things and deciding what to do with them, when you get it done, your situation is as follows:</p> <ul> <li>You have eliminated every thing you do not need.</li> <li>You have completed small actions that require no more than two minutes.</li> <li>You have delegated some actions that you do not have to do.</li> <li>You have sorted in your task manager the actions you will do     when appropriate, because they require more than 2 minutes.</li> <li>You have sorted in your task manager or calendar the tasks     that have a due date.</li> <li>There have been only a few minutes, but you feel pretty good. Everything is     where it should be.</li> </ul> <p>I've developed <code>pynbox</code> to automate the management of the inbox. Help out if you like it!</p>"}, {"location": "task_tools/#task-manager", "title": "Task manager", "text": "<p>If you've never used a task manager, start with the simplest one and see what do you feel its lacking. Choose then a better task manager based on your needs.</p> <p>In the past I've used taskwarrior, but its limitations led me to start creating pydo although I didn't finish it. Then I moved on to the simplest task manager but it eventually fell short in my needs, so I started working with Openproject but working on a web interface is not for me, so now I'm migrating to orgmode.</p>"}, {"location": "task_tools/#the-simplest-task-manager", "title": "The simplest task manager", "text": "<p>The simplest task manager is a markdown file in your computer with a list of tasks to do. Annotate only the actionable tasks that you need to do today, otherwise it can quickly grow to be unmanageable.</p> <p>When you add a new item, choose it's location relative to the existent one based on its priority. Being the top tasks are the ones that need to be done first.</p> <pre><code>* Task with a high priority\n* Task with low priority\n</code></pre> <p>The advantages of using a plaintext file over a physical notebook is that you can use your editor skills to manage the elements more efficiently. For example by reordering them or changing the description.</p>"}, {"location": "task_tools/#add-task-state-sections", "title": "Add task state sections", "text": "<p>You'll soon encounter tasks that become blocked but need your monitoring. You can add a <code># Blocked</code> section and move those tasks under it. You can optionally add the reasons why it's blocked indented below the element.</p> <pre><code>* Unblocked task\n\n# Blocked\n\n* Blocked task\n  * Waiting for Y to happen\n</code></pre>"}, {"location": "task_tools/#divide-a-task-in-small-steps", "title": "Divide a task in small steps", "text": "<p>One of the main benefits of a task manager is that you free your mind of what you need to do next, so you can focus on the task at hand. When a task is big split it in smaller doable steps that drive to its completion. If the steps are also big split them further with more indentation levels.</p> <pre><code>* Complex task\n  * Do X\n  * Do Y\n    * Do Z\n    * Do W\n</code></pre>"}, {"location": "task_tools/#web-based-task-manager", "title": "Web based task manager", "text": "<p>Life happened and the development of pydo has fallen behind in my priority list. I've also reached a point where simplest one is no longer suitable for my workflow because:</p> <ul> <li>I loose a lot of time in the reviews.</li> <li>I loose a lot of time when doing the different plannings (year, trimester,     month, week, day).</li> <li>I find it hard to organize and refine the backlog.</li> </ul> <p>As <code>pydo</code> is not ready yet and I need a solution that works today better than the simplest task manager, I've done an analysis of the state of the art of self-hosted applications of all of them the two that were more promising were Taiga and OpenProject.</p>"}, {"location": "task_tools/#taiga", "title": "Taiga", "text": "<p>An Open source project with a lot of functionality. If you want to try it, you can create an account at Disroot (an awesome collective by the way). They have set up an instance where you can check if you like it.</p> <p>Some facts made me finally not choose it, for example:</p> <ul> <li>Subtasks can't have subtasks. Something I've found myself having quite often.     Specially if you refine your tasks in great detail.</li> <li>When browsing the backlog or the boards, you can't edit a task in that window,     you need to open it in another tab.</li> <li>I don't understand very well the different components, the difference between     tasks and issues for example.</li> </ul>"}, {"location": "task_tools/#openproject", "title": "OpenProject", "text": "<p>Check the OpenProject page to see the analysis of the tool.</p> <p>In the end I went with this option.</p>"}, {"location": "task_tools/#references", "title": "References", "text": "<ul> <li>GTD time management     framework.</li> </ul>"}, {"location": "task_workflows/", "title": "Task management workflows", "text": ""}, {"location": "task_workflows/#hype-flow-versus-a-defined-plan", "title": "Hype flow versus a defined plan", "text": "<p>I've found two ways to work on my tasks: following a plan and following the hype flow.</p> <p>The first one helps you finish what you started, and directs your efforts towards big goals. The side effect is that it achieves it by setting constrains on what to do, so you sometimes end up in the position of doing tasks that you don't want to at the moment, and suppressing yourself not to do the ones that you want.</p> <p>The second one takes advantage of letting you work on wherever you want at the moment, which boosts your creativity and productivity. This way imposes less constrains on you and is more pleasant because surfing the hype is awesome. The side effect is that if you have many interests, you can move forward very quickly on many directions leaving a lot of projects half done, instead of pushing in the direction of your big goals.</p> <p>The art here is to combine both at need, if you have a good plan, you may be able to start surfing the hype, and when the time constrains start to press you, switch to a stricter plan to be able to deliver value in time. This makes more sense in work environments, at personal level I usually just surf the hype unless I have a clear objective with a due date to reach.</p>"}, {"location": "task_workflows/#planning-workflows", "title": "Planning workflows", "text": "<p>Task management can be done at different levels. All of them help you in different ways to reduce the mental load, each also gives you extra benefits that can't be gained by the others. Going from lowest to highest abstraction level we have:</p> <ul> <li>Task plan.</li> <li>Pomodoro.</li> <li>Day plan.</li> <li>Week plan.</li> <li>Fortnight plan.</li> <li>Month plan.</li> <li>Trimester plan.</li> <li>Year plan.</li> </ul> <p>If you're starting your task management career, start with the first level. Once you're comfortable, move one step up until you reach the sweet spot between time invested in management and the profit it returns.</p> <p>Each of the plans defined below describe the most complete process, use them as a starting point to define the plan that works for you depending on your needs and how much time you want to invest at that particular moment. Even I don't follow them strictly. As they change over time, it's useful to find a way to be able to follow them without thinking too much on what are the specific steps, for example having a checklist or a script.</p>"}, {"location": "task_workflows/#task-plan", "title": "Task plan", "text": "<p>The task plan defines the steps required to finish a task. It's your most basic roadmap to address a task, and a good starting point if you feel overwhelmed when faced with an assignment.</p> <p>When done well, you'll better understand what you need to do, it will prevent you from wasting time at dead ends as you'll think before acting, and you'll develop the invaluable skill of breaking big problems into smaller ones.</p> <p>To define a task plan, follow the next steps:</p> <ul> <li>Decide what do you want to achieve when the task is finished.</li> <li>Analyze the possible ways to arrive to that goal. Try to assess different   solutions before choosing one.</li> <li>Once you have it, split it into steps small enough to be comfortable following   them without further analysis.</li> </ul> <p>Some people define the task plan whenever they add the task to their task manager. Others prefer to save some time each month to refine the plans of the tasks to be done the next one.</p> <p>The plan is an alive document that changes each Pomodoro cycle and that you'll need to check often. It has to be accessible and it should be easy for you to edit. If you don't know where to start, use the simplest task manager.</p> <p>Try not to overplan though, if at the middle of a task you realize that the rest of the steps don't make sense, all the time invested in their definition will be lost. That's why it's a good idea to have a great detail for the first steps and gradually move to rougher definitions on later ones.</p>"}, {"location": "task_workflows/#pomodoro", "title": "Pomodoro", "text": "<p>Pomodoro is a technique used to ensure that for short periods of time, you invest all your mental resources in doing the work needed to finish a task. It's your main unit of work and a good starting point if you have concentration issues.</p> <p>When done well, you'll start moving faster on your tasks, because uninterrupted work is the most efficient. You'll also begin to know if you're drifting from your day's plan, and will have space to adapt it or the task plan to time constrains or unexpected events.</p> <p>!!! note \"\" If you don't yet have a task plan or day plan, don't worry! Ignore the steps that involve them until you do.</p> <p>The next steps define a Pomodoro cycle:</p> <ul> <li>Select the cycle time span. Either 20 minutes or until the next interruption,   whichever is shortest.</li> <li>Decide what are you going to do.</li> <li>Analyze yourself to see if you're state of mind is ready to only do that for   the chosen time span. If it's not, maybe you need to take a \"Pomodoro break\",   take 20 minutes off doing something that replenish your willpower or the   personal attribute that is preventing you to be able to work.</li> <li>Start the timer.</li> <li>Work uninterruptedly on what you've decided until the timer goes off.</li> <li>Take 20s to look away from the screen (this is good for your ejes).</li> <li>Update your task and day plans:</li> <li>Tick off the done task steps.</li> <li>Refine the task steps that can be addressed in the next cycle.</li> <li>Check if you can still meet the day's plan.</li> <li>Check the   interruption channels that need to be checked each 20 minutes.</li> </ul> <p>At the fourth Pomodoro cycle, you'll have finished a Pomodoro iteration. At the end of the iteration:</p> <ul> <li>Check if you're going to meet the day plan, if you're not, change   change it or the task plan to make the time constrain.</li> <li>Get a small rest, you've earned it! Get off the chair, stretch or give a small   walk. What's important is that you take your mind off the task at hand and let   your body rest. Remember, this is a marathon, you need to take care of   yourself.</li> <li>Start a new Pomodoro iteration.</li> </ul> <p>If you're super focused at the end of a Pomodoro cycle, you can skip the task plan update until the end of the iteration.</p> <p>To make it easy to follow the pomodoro plan I use a script that:</p> <ul> <li>Uses timer to show the countdown.</li> <li>Uses safeeyes to track the eye   rests.</li> <li>Asks me to follow the list of steps I've previously defined.</li> </ul>"}, {"location": "task_workflows/#day-plan", "title": "Day plan", "text": "<p>This plan defines at day level which tasks are you going to work on and schedules when are you going to address them. It's the most basic roadmap to address a group of tasks. The goal is to survive the day. It's a good starting point if you forget to do tasks that need to be done in the day or if you miss appointments.</p> <p>It's also the next step of advance awareness, if you have a day plan, on each Pomodoro iteration you'll get the feeling whether you're going to finish what you proposed yourself.</p> <p>You can make your plan at the start of the day, start by getting an idea of:</p> <ul> <li>What do you need to do by checking:</li> <li>The last day's plan.</li> <li>Calendar events.</li> <li>The week's plan if you have it, or the prioritized list of     tasks to do.</li> <li>How much uninterrupted time you have between calendar events.</li> <li>Your state of mind.</li> </ul> <p>Then create the day schedule:</p> <ul> <li>Add the calendar events.</li> <li>Add the   interruption events.</li> <li>Setup an alert for the closest calendar event.</li> </ul> <p>And the day tasks plan:</p> <ul> <li>Decide the tasks to be worked on and think when you want to do them.</li> </ul> <p>To follow it throughout the day and when it's coming to an end:</p> <ul> <li>Update your week or/and task plans to meet the   time constrains.</li> <li>Optionally sketch the next day's plan.</li> </ul> <p>When doing the plan keep in mind to minimize the number of tasks and calendar events so as not to get overwhelmed, and not to schedule a new task before you finish what you've already started. It's better to eventually fall short on tasks, than never reaching your goal.</p> <p>To make it easy to follow I use a script that:</p> <ul> <li>Asks me to check the weather forecast.</li> <li>Uses timer to show the countdown.</li> <li>Uses safeeyes to track the eye   rests.</li> <li>Asks me to follow the list of steps I've previously defined.</li> </ul>"}, {"location": "task_workflows/#week-plan", "title": "Week plan", "text": "<p>The plan defines at week level which tasks are you going to work on and schedules when are you going to address them. It's the next roadmap level to address a group of tasks. The goal changes from surviving the day to start planning your life. It's a good starting point if you are comfortable working with the pomodoro, task and day plans, and want to start deciding where you're heading to.</p> <p>It's also the next step of advance awareness, if you have a week plan, each day you'll get the feeling whether you're going to finish what you proposed yourself.</p> <p>You can make your plan at the start of the week, similar to the day plan, start by getting an idea of:</p> <ul> <li>What do you need to do by:</li> <li>Closing the last week's plan.</li> <li>Checking upcoming calendar events.</li> <li>The month's plan if you have it, or the prioritized list of tasks to do,     identifying the task dependencies that may block the task development.</li> <li>How much uninterrupted time you have between calendar events.</li> <li>Your state of mind.</li> </ul> <p>To close the last week's plan:</p> <ul> <li> <p>Mark the plan items as done</p> </li> <li> <p>Get an idea of what percent of objectives you actually met. With the mindset   of seeing how much you can commit on the next one, not to think how bad you   performed, you did the best you could, and nothing else could be done.</p> </li> <li> <p>Clean the active tasks. Throughout the week, there will be tasks that you left   unfinished. For each of them:</p> </li> <li> <p>Decide if the task still makes sense and if it's actionable</p> </li> <li>Check if you can merge it with other tasks</li> <li>Check if it belongs to an active objective</li> <li>Remove the rest.</li> </ul> <p>Then create the week schedule:</p> <ul> <li>Arrange or move calendar events to maximize the uninterrupted periods, then   add them to the plan.</li> <li>Add the   interruption events.</li> <li>Decide the tasks to be worked on and roughly assign them to the week days.</li> </ul> <p>Follow it throughout the week, and when it's coming to an end:</p> <ul> <li>Update your month or/and task plans to meet the time constrains.</li> <li>Update the people that may depend on you of possible plan drifts.</li> </ul> <p>To make it easy to follow I use a script that:</p> <ul> <li>Asks me to follow the list of steps I've previously defined.</li> </ul>"}, {"location": "task_workflows/#fortnight-month-trimester-plan", "title": "Fortnight, month, trimester plan", "text": "<p>From the week plan you can increasingly think your roadmap, start with a fortnight plan, when you're comfortable go up to month and trimester plans. The idea is similar to the week plan but with less definition. You deal with bigger tasks that help shape your life in the long run.</p> <p>The process of planning and reviewing each of these should be as short as possible, otherwise you'll soon get tired of it. For example, for the month plan you can:</p> <ul> <li>Do the week plan review: Where you transfer the non planned tasks to the month   plan.</li> <li>Do the fortnight plan review</li> <li>Do the month plan review:</li> <li>Check the objectives you had and how many you meet, adding notes on your     progress.</li> <li>Analyze what to do with the new objectives, adding them to the trimester     plan</li> <li>Transfer the non planned elements to the semester plan.</li> <li>Do the month's planning:</li> <li>Review the semester plan if you have it.</li> <li>Decide which big tasks you want to work on</li> <li>Do the fortnight plan</li> <li>Do the week plan</li> </ul>"}, {"location": "task_workflows/#review-workflows", "title": "Review workflows", "text": "<p>Reviews are proceses to check what you've done in a period of time to learn from your mistakes. As plannings, reviews can be done at different levels of abstraction, each level gives you different benefits.</p>"}, {"location": "task_workflows/#year-review", "title": "Year review", "text": "<p>Year reviews are meant to give you an idea of:</p> <ul> <li>How much have you and your workflows evolved</li> <li>What roadmap decisions were right, which ones were wrong</li> <li>With the context you have now, you can think of how you could have avoided the   bad decisions.</li> </ul> <p>If you have the year's planning you can analyze it against your task management tools and life logs and create a review document analyzing all</p>"}, {"location": "task_workflows/#references", "title": "References", "text": "<ul> <li>Pomodoro article.</li> </ul>"}, {"location": "teeth/", "title": "Teeth", "text": "<p>Taking good care of your teeth can be easier if you remember that each visit to the dentist is both super expensive and painful. So those 10 minutes each day are really worth it.</p>"}, {"location": "teeth/#how-to-take-care-of-your-teeth", "title": "How to take care of your teeth", "text": "TL;DR: Daily actions to keep your teeth healty <ul> <li>Brush your teeth after every meal for at least two minutes.</li> <li>Floss each day before the last teeth brush.</li> <li>Use an electric toothbrush.</li> <li>Replace the brush each three months or at first     sign of wear and tear.</li> <li>Don't eat or drink anything but water after your nightly brush.</li> <li>Do not rinse after you brush your teeth.</li> <li>Use floss instead of a toothpick.</li> <li>Use mouthwash daily but not after brushing.</li> </ul> <p>Oral hygiene is the practice of keeping one's mouth clean and free of disease and other problems (e.g. bad breath) by regular brushing of the teeth (dental hygiene) and cleaning between the teeth. It is important that oral hygiene be carried out on a regular basis to enable prevention of dental disease and bad breath. The most common types of dental disease are tooth decay (cavities, dental caries) and gum diseases, including gingivitis, and periodontitis.</p> <p>General guidelines suggest brushing twice a day: after breakfast and before going to bed, but ideally the mouth would be cleaned after every meal. Cleaning between the teeth is called interdental cleaning and is as important as tooth brushing. This is because a toothbrush cannot reach between the teeth and therefore only removes about 50% of plaque from the surface of the teeth. There are many tools to clean between the teeth, including floss and interdental brushes; it is up to each individual to choose which tool they prefer to use.</p> <p>Over 80% of cavities occur inside fissures in teeth where brushing cannot reach food left trapped after eating and saliva and fluoride have no access to neutralize acid and remineralize demineralized teeth, unlike easy-to-clean parts of the tooth, where fewer cavities occur.</p>"}, {"location": "teeth/#teeth-brushing", "title": "Teeth brushing", "text": "<p>Routine tooth brushing is the principal method of preventing many oral diseases, and perhaps the most important activity an individual can practice to reduce dental plaque and tartar.</p> <p>The dental plaque contains a mixture of bacteria, their acids and sticky byproducts and food remnants. It forms naturally on teeth immediately after you've eaten but doesn't get nasty and start to cause damage to the teeth until it reaches a certain stage of maturity. The exact amount of time this takes isn't known but is at least more than 12 hours.</p> <p>Bacteria consume sugar and, as a byproduct, produce acids which dissolve mineral out of the teeth, leaving microscopic holes we can't see. If the process isn't stopped and they aren't repaired, these can become big, visible cavities.</p> <p>So controlling plaque reduces the risk of the individual suffering from plaque-associated diseases such as gingivitis, periodontitis, and caries.</p> <p>Many oral health care professionals agree that tooth brushing should be done for a minimum of two minutes, and be practiced at least twice a day, but ideally after each meal.</p> <p>Toothbrushing can only clean to a depth of about 1.5 mm inside the gingival pockets, but a sustained regime of plaque removal above the gum line can affect the ecology of the microbes below the gums and may reduce the number of pathogens in pockets up to 5 mm in depth.</p> <p>Toothpaste (dentifrice) with fluoride, or alternatives such as nano-hydroxyapatite, is an important tool to readily use when tooth brushing. The fluoride (or alternatives) in the dentifrice is an important protective factor against caries, and an important supplement needed to remineralize already affected enamel. However, in terms of preventing gum disease, the use of toothpaste does not increase the effectiveness of the activity with respect to the amount of plaque removed. People use toothpaste with nano-hydroxyapatite instead of fluoride as it performs the same function, and some people believe fluoride in toothpaste is a neurotoxin.</p> <p>For maximum benefit, use toothpaste with 1350-1500 ppmF (that's concentration of fluoride in parts per million) to prevent tooth decay.</p> <p>At night, you produce less saliva than during the day. Because of this, your teeth have less protection from saliva and are more vulnerable to acid attacks. That's why it's important to remove food from your teeth before bed so plaque bacteria can't feast overnight. Don't eat or drink anything except water after brushing at night. This also gives fluoride the longest opportunity to work.</p>"}, {"location": "teeth/#how-to-brush-your-teeth", "title": "How to brush your teeth", "text": "<p>The procedure I'm using right now is:</p> <ul> <li>Wet the brush but don't add any toothpaste.</li> <li>Slowly and systematically guide the bristle of the electric brush from tooth     to tooth, following the contour of the gums and their crowns, remembering to     massage the gums. For example, start with the upper left part on the     outside, reach the other side of your mouth, clean the bottom of your right     side teeth, then go back on the inside of each teeth until you arrive to     your left side. Try to avoid brushing with too much force as this can     damage the surface of your teeth.</li> <li>Rinse with water.</li> <li>Place a pea sized amount of toothpaste on the brush and repeat the cycle.</li> <li>Brush your tongue.</li> <li>Spit the extra toothpaste but don't rinse or drink anything in the next 30     minutes.</li> </ul> <p>The whole process should take at least two minutes.</p>"}, {"location": "teeth/#manual-versus-electric-tooth-brush", "title": "Manual versus electric tooth brush", "text": "<p>If you want to use a manual one, Oral health professionals recommend the use of a tooth brush with a small head and soft bristles as they are most effective in removing plaque without damaging the gums.</p> <p>The technique is crucial to the effectiveness of tooth brushing and disease prevention. Back and forth brushing is not effective in removing plaque at the gum line. Tooth brushing should employ a systematic approach, angle the bristles at a 45-degree angle towards the gums, and make small circular motions at that angle.</p> <p>When using an electric one, the bristle head should be guided from tooth to tooth slowly, following the contour of the gums and crowns of the tooth. The motion of the toothbrush head removes the need to manually oscillate the brush or make circles.</p> <p>Another study suggest that the effectiveness of electric toothbrushes at reducing plaque formation and gingivitis is superior to conventional manual toothbrushes.</p> <p>Regardless of the type, you are always best using a soft-bristled toothbrush with a small head and a flexible neck because this will most effectively remove plaque and debris from your teeth, without damaging your teeth and gums and drawing blood.</p>"}, {"location": "teeth/#toothbrush-replacement", "title": "Toothbrush replacement", "text": "<p>Remember to replace your brush at the first sign of wear-and-tear or every three months, whichever comes first. Frayed or broken bristles won't clean your mouth properly. Change your toothbrush once the bristles lose their flexibility. 1</p> <p>Also, after a couple of months of daily use, bacteria and food particles begin to accumulate on the toothbrush.</p>"}, {"location": "teeth/#to-rinse-or-not-to-rinse", "title": "To rinse or not to rinse", "text": "<p>There is a lot of controversy on the topic on whether you should rinse or not your mouth after brushing your teeth. (1, 2)</p> <p>People in favor of rinsing your mouth argue that:</p> <ul> <li>You\u2019ll get rid of the excess toothpaste along with any food or bacteria that     could have been stuck in your teeth or released by the brushing itself.</li> <li>You\u2019ll also be removing the fluoride from your mouth, which if swallowed,     might upset your stomach.</li> </ul> <p>People against rinsing your mouth argue that:</p> <ul> <li>When you rinse with water, you\u2019re potentially washing away any remnants of     toothpaste, including the fluoride that makes it work. That could mean that     even though you are brushing your teeth, it might not be as effective as it     should be.</li> </ul> <p>Whilst there have been studies on the effectiveness of rinsing, the results only indicate that there COULD be an advantage of one over the other. So it's up to you to evaluate the advantages and disadvantages of each method.</p> <p>Some people are prone to cavities, or might have poor dental health. If your teeth chip, crack or break easily, it\u2019s strongly recommended that you don't rinse after you brush.  Similarly, if you consume a lot of sugar, you should probably avoid rinsing. If you don't fit into these categories, then it\u2019s really based on your own preference.</p>"}, {"location": "teeth/#keep-your-brush-away-from-your-feces", "title": "Keep your brush away from your feces", "text": "<p>As the Mythbusters showed, Fecal coliform were found on toothbrushes stored at the bathroom. And even though none were of a level high enough to be dangerous, and experts confirm that such bacteria are impossible to completely avoid, you can reduce the risk by:</p> <ul> <li>Storing the brush in a cupboard or in other room.</li> <li>Putting a lid on your toothbrush</li> <li>Closing the lid on your toilet seat before flushing.</li> </ul>"}, {"location": "teeth/#how-to-floss", "title": "How to floss", "text": "<p>Tooth brushing alone will not remove plaque from all surfaces of the tooth as 40% of the surfaces are interdental. One technique that can be used to access these areas is dental floss. When the proper technique is used, flossing can remove plaque and food particles from between the teeth and below the gums. The American Dental Association (ADA) reports that up to 80% of plaque may be removed by this method. The ADA recommends cleaning between the teeth as part of one's daily oral hygiene regime, with a different piece of floss at each flossing session.</p> <p>The correct technique to ensure maximum plaque removal is as follows: (1, 2)</p> <ul> <li>Floss length: 15\u201325 cm wrapped around middle fingers.</li> <li>For upper teeth grasp the floss with thumb and index finger, for lower teeth     with both index fingers. Ensure that a length of roughly 2.5cm is left     between the fingers.</li> <li>Ease the floss gently between the teeth using a back and forth motion. Do not     snap the floss into the gums.</li> <li>When the floss reaches your gumline, curve it into a C-shape against a tooth     until you feel resistance.</li> <li>Hold the floss against the tooth. Gently scrape the side of the tooth, moving     the floss away from the gum. Repeat on the other side of the gap, along the     side of the next tooth.</li> <li>Do not forget the back of your last tooth.</li> <li>Ensure that the floss is taken below the gum margins using a back and forth up     and down motion.</li> </ul> <p>You should floss before brushing your teeth because any food, plaque, and bacteria released by flossing are removed by the afterwards brushing.</p> <p>Another tips regarding flossing are:</p> <ul> <li>Skip the toothpick: Use floss instead of a toothpick to remove food stuck in between your teeth. Using a toothpick can damage your gums and lead to an infection.</li> <li>Be gentle: Don't be too aggressive when flossing to avoid bleeding gums.</li> </ul> <p>When you first start flossing, your gums may be tender and bleed a little. Carry on flossing your teeth and the bleeding should stop as your gums become healthier.  If you're still getting regular bleeding after a few days, see your dental team. They can check if you're flossing correctly.</p>"}, {"location": "teeth/#mouth-washing", "title": "Mouth washing", "text": "<p>Using a mouthwash that contains fluoride can help prevent tooth decay, but don't use mouthwash (even a fluoride one) straight after brushing your teeth or it'll wash away the concentrated fluoride in the toothpaste left on your teeth. [1]</p> <p>Choose a different time to use mouthwash, such as after lunch. And remember not to eat or drink for 30 minutes after using a fluoride mouthwash.</p>"}, {"location": "teeth/#do-yearly-check-ups", "title": "Do yearly check ups", "text": "<p>First find a dentist that you trust, until you do, search for second and third options before diving into anything you may regret.</p> <p>Once you have it, yearly go to their dreaded places so they can:</p> <ul> <li>Check that everything is alright.</li> <li>Do a regular clean, but beware of unnecessary deep cleaning.</li> </ul>"}, {"location": "teeth/#references", "title": "References", "text": "<ul> <li>Wikipedia oral hygiene article</li> <li>CNN health article on oral     hygiene</li> </ul>"}, {"location": "teeth_deep_cleaning/", "title": "Teeth deep cleaning", "text": "<p>TL;DR: Ask the opinion of two or three independent dentists before doing a deep clean.</p> <p>Scaling and root planing, also known as conventional periodontal therapy, non-surgical periodontal therapy or deep cleaning, is a procedure involving removal of dental plaque and calculus (scaling or debridement) and then smoothing, or planing, of the (exposed) surfaces of the roots, removing cementum or dentine that is impregnated with calculus, toxins, or microorganisms, the etiologic agents that cause inflammation. It is a part of non-surgical periodontal therapy. This helps to establish a periodontium that is in remission of periodontal disease.</p> <p>As to the frequency of cleaning, research on this matter is inconclusive. That is, it has neither been shown that more frequent cleaning leads to better outcomes nor that it does not. Thus, any general recommendation for a frequency of routine cleaning (e.g. every six months, every year) has no empirical basis. (1)</p>"}, {"location": "teeth_deep_cleaning/#why-do-we-need-deep-cleaning", "title": "Why do we need deep cleaning", "text": "<p>We all have a plethora of bacteria in our mouths. Those bacteria mix with other substances to form sticky plaque on teeth, which is mostly banished by regular brushing and flossing.</p> <p></p> <p>Plaques that don't get brushed away can harden and form a substance known as tartar, which can only be removed with a dental cleaning. When tartar remains on the teeth, it can cause inflammation of the gums, a condition called gingivitis, characterized by red swollen gums that can bleed easily. A mild form of gum disease, gingivitis can usually be reversed through regular brushing and flossing along with cleanings by a dentist or hygienist.</p> <p></p> <p>If gingivitis isn't cured, it can advance to a more severe form of gum disease called periodontitis, in which the inflamed tissue begins to pull away from the teeth, forming spaces, or pockets. As the pockets become deeper, more of the tooth below the gum line is exposed to bacteria, which can damage the bone holding teeth in place.</p> <p>Eventually, if the pockets become deep enough, teeth can become loose and may even be lost. Dentists measure the depth of the pockets with a probe that has a tiny ruler on the end. Healthy gums have pockets that measure no more than 3 mm \u2014 or a little less than a tenth of an inch \u2014 deep. More than that and you\u2019re getting into trouble.</p> <p>One way to slow or halt the process is through deep cleaning, which removes the plaque below the gum line and smooths rough spots on the tooth root, making it harder for bacteria to accumulate there.</p>"}, {"location": "teeth_deep_cleaning/#signs-of-periodontitis", "title": "Signs of periodontitis", "text": "<ul> <li>Red or swollen gums</li> <li>Tender or bleeding gums</li> <li>Persistent bad breath</li> <li>Your teeth look like they\u2019ve been getting longer as gums recede.</li> <li>Teeth that are sensitive</li> <li>Loose teeth</li> <li>Pain when chewing</li> </ul>"}, {"location": "teeth_deep_cleaning/#evidence-based-dentistry", "title": "Evidence-based dentistry", "text": "<p>Several systematic reviews have been made of the effectiveness of scaling and root planing as evidence-based dentistry. A Cochrane review by Worthington et al. in 2013 considered only scaling and polishing of the teeth, but not root planing. After examining 88 papers they found only three studies that met all their requirements, remarking that \"the quality of the evidence was generally low.\"</p> <p>An extensive review that did involve root planing was published by the Canadian Agency for Drugs and Technologies in Health in 2016. It made a number of findings, including (1) In five randomized controlled trials, scaling and root planing \"was associated with a decrease in plaque from baseline at one month, three months, or six months;\" and (2) Four studies analyzed changes in the gingival index (GI) from the baseline and \"found a significant improvement from baseline in the scaling and root planing group at three months and six months.\" This study also discussed evidence-based guidelines for frequency of scaling with and without root planing for patients both with and without chronic periodontitis. The group that produced one of the main systematic reviews used in the 2016 Canadian review has published guidelines based on its findings. They recommend that scaling and root planing (SRP) should be considered as the initial treatment for patients with chronic periodontitis. They note that \"the strength of the recommendation is limited because SRP is considered the reference standard and thus used as an active control for periodontal trials and there are few studies in which investigators compare SRP with no treatment.\" They add however that \"root planing ... carries the risk of damaging the root surface and potentially causing tooth or root sensitivity. Generally expected post-SRP procedural adverse effects include discomfort.\"</p> <p>Enamel cracks, early caries and resin restorations can be damaged during scaling.</p>"}, {"location": "teeth_deep_cleaning/#effectiveness-of-the-procedure", "title": "Effectiveness of the procedure", "text": "<p>A scaling and root planing procedure is to be considered effective if the patient is subsequently able to maintain their periodontal health without further bone or attachment loss and if it prevents recurrent infection with periodontal pathogens.</p> <p>The long term effectiveness of scaling and root planing depends upon a number of factors. These factors include patient compliance, disease progress at the time of intervention, probing depth, and anatomical factors like grooves in the roots of teeth, concavities, and furcation involvement which may limit visibility of underlying deep calculus and debris.</p> <p>First and foremost, periodontal scaling and root planing is a procedure that must be done thoroughly and with attention to detail in order to ensure complete removal of all calculus and plaque from involved sites. If these causative agents are not removed, the disease will continue to progress and further damage will result. In cases of mild to moderate periodontitis, scaling and root planing can achieve excellent results if the procedure is thorough. As periodontitis increases in severity, a greater amount of supporting bone is destroyed by the infection. This is illustrated clinically by the deepening of the periodontal pockets targeted for cleaning and disinfection during the procedure. Once the periodontal pockets exceed 6 mm in depth, the effectiveness of deposit removal begins to decrease, and the likelihood of complete healing after one procedure begins to decline as well. The more severe the infection prior to intervention, the greater the effort required to arrest its progress and return the patient to health. Diseased pockets over 6 mm can be resolved through periodontal flap surgery.</p> <p>Although healing of the soft tissues will begin immediately following removal of the microbial biofilm and calculus that cause the disease, scaling and root planing is only the first step in arresting the disease process. Following initial cleaning and disinfection of all affected sites, it is necessary to prevent the infection from recurring. Therefore, patient compliance is, by far, the most important factor, having the greatest influence on the success or failure of periodontal intervention. Immediately following treatment, the patient will need to maintain excellent oral care at home. With proper homecare, which includes but is by no means limited to brushing twice daily for 2\u20133 minutes, flossing daily and use of mouthrinse, the potential for effective healing following scaling and root planing increases. Commitment to and diligence in the thorough completion of daily oral hygiene practices are essential to this success.</p> <p>The process which allows for the formation of deep periodontal pockets does not occur overnight. Therefore, it is unrealistic to expect the tissue to heal completely in a similarly short time period. Gains in gingival attachment may occur slowly over time, and ongoing periodontal maintenance visits are usually recommended (by some sources).</p>"}, {"location": "teeth_deep_cleaning/#side-effects", "title": "Side effects", "text": "<p>The process carries it's risks, such as:</p> <ul> <li>Pop out a filling</li> <li>Gums damage in an irreversible way.</li> <li>End up with an abscess if a tiny piece of tartar is knocked loose and becomes     trapped.</li> <li>Have more sensitivity after the procedure.</li> </ul>"}, {"location": "teeth_deep_cleaning/#conclusion", "title": "Conclusion", "text": "<ul> <li>Deep cleaning is an invasive procedure</li> <li>There is a lack of scientific studies supporting the frequency of it's     application, specially for people that doesn't suffer from periodontitis.</li> <li>It's an expensive procedure.</li> </ul> <p>So, ask the opinion of two or three independent dentists before doing a deep clean.</p>"}, {"location": "tenacity/", "title": "Tenacity", "text": "<p>Tenacity is an Apache 2.0 licensed general-purpose retrying library, written in Python, to simplify the task of adding retry behavior to just about anything.</p>"}, {"location": "tenacity/#installation", "title": "Installation", "text": "<pre><code>pip install tenacity\n</code></pre>"}, {"location": "tenacity/#usage", "title": "Usage", "text": "<p>Tenacity isn't api compatible with retrying but adds significant new functionality and fixes a number of longstanding bugs.</p> <p>The simplest use case is retrying a flaky function whenever an Exception occurs until a value is returned.</p> <pre><code>import random\nfrom tenacity import retry\n\n@retry\ndef do_something_unreliable():\n    if random.randint(0, 10) &gt; 1:\n        raise IOError(\"Broken sauce, everything is hosed!!!111one\")\n    else:\n        return \"Awesome sauce!\"\n\nprint(do_something_unreliable())\n</code></pre>"}, {"location": "tenacity/#basic-retry", "title": "Basic Retry", "text": "<p>As you saw above, the default behavior is to retry forever without waiting when an exception is raised.</p> <pre><code>@retry\ndef never_gonna_give_you_up():\n    print(\"Retry forever ignoring Exceptions, don't wait between retries\")\n    raise Exception\n</code></pre>"}, {"location": "tenacity/#stopping", "title": "Stopping", "text": "<p>Let\u2019s be a little less persistent and set some boundaries, such as the number of attempts before giving up.</p> <pre><code>@retry(stop=stop_after_attempt(7))\ndef stop_after_7_attempts():\n    print(\"Stopping after 7 attempts\")\n    raise Exception\n</code></pre> <p>We don\u2019t have all day, so let\u2019s set a boundary for how long we should be retrying stuff.</p> <pre><code>@retry(stop=stop_after_delay(10))\ndef stop_after_10_s():\n    print(\"Stopping after 10 seconds\")\n    raise Exception\n</code></pre> <p>You can combine several stop conditions by using the <code>|</code> operator:</p> <pre><code>@retry(stop=(stop_after_delay(10) | stop_after_attempt(5)))\ndef stop_after_10_s_or_5_retries():\n    print(\"Stopping after 10 seconds or 5 retries\")\n    raise Exception\n</code></pre>"}, {"location": "tenacity/#waiting-before-retrying", "title": "Waiting before retrying", "text": "<p>Most things don\u2019t like to be polled as fast as possible, so let\u2019s just wait 2 seconds between retries.</p> <pre><code>@retry(wait=wait_fixed(2))\ndef wait_2_s():\n    print(\"Wait 2 second between retries\")\n    raise Exception\n</code></pre> <p>Some things perform best with a bit of randomness injected.</p> <pre><code>@retry(wait=wait_random(min=1, max=2))\ndef wait_random_1_to_2_s():\n    print(\"Randomly wait 1 to 2 seconds between retries\")\n    raise Exception\n</code></pre> <p>Then again, it\u2019s hard to beat exponential backoff when retrying distributed services and other remote endpoints.</p> <pre><code>@retry(wait=wait_exponential(multiplier=1, min=4, max=10))\ndef wait_exponential_1():\n    print(\"Wait 2^x * 1 second between each retry starting with 4 seconds, then up to 10 seconds, then 10 seconds afterwards\")\n    raise Exception\n</code></pre> <p>Then again, it\u2019s also hard to beat combining fixed waits and jitter (to help avoid thundering herds) when retrying distributed services and other remote endpoints.</p> <pre><code>@retry(wait=wait_fixed(3) + wait_random(0, 2))\ndef wait_fixed_jitter():\n    print(\"Wait at least 3 seconds, and add up to 2 seconds of random delay\")\n    raise Exception\n</code></pre> <p>When multiple processes are in contention for a shared resource, exponentially increasing jitter helps minimise collisions.</p> <pre><code>@retry(wait=wait_random_exponential(multiplier=1, max=60))\ndef wait_exponential_jitter():\n    print(\"Randomly wait up to 2^x * 1 seconds between each retry until the range reaches 60 seconds, then randomly up to 60 seconds afterwards\")\n    raise Exception\n</code></pre>"}, {"location": "tenacity/#whether-to-retry", "title": "Whether to retry", "text": "<p>We have a few options for dealing with retries that raise specific or general exceptions, as in the cases here.</p> <pre><code>@retry(retry=retry_if_exception_type(IOError))\ndef might_io_error():\n    print(\"Retry forever with no wait if an IOError occurs, raise any other errors\")\n    raise Exception\n</code></pre> <p>We can also use the result of the function to alter the behavior of retrying.</p> <pre><code>def is_none_p(value):\n\"\"\"Return True if value is None\"\"\"\n    return value is None\n\n@retry(retry=retry_if_result(is_none_p))\ndef might_return_none():\n    print(\"Retry with no wait if return value is None\")\n</code></pre> <p>We can also combine several conditions:</p> <pre><code>def is_none_p(value):\n\"\"\"Return True if value is None\"\"\"\n    return value is None\n\n@retry(retry=(retry_if_result(is_none_p) | retry_if_exception_type()))\ndef might_return_none():\n    print(\"Retry forever ignoring Exceptions with no wait if return value is None\")\n</code></pre> <p>Any combination of stop, wait, etc. is also supported to give you the freedom to mix and match.</p> <p>It\u2019s also possible to retry explicitly at any time by raising the <code>TryAgain</code> exception:</p> <pre><code>@retry\ndef do_something():\n    result = something_else()\n    if result == 23:\n       raise TryAgain\n</code></pre>"}, {"location": "tenacity/#references", "title": "References", "text": "<ul> <li>Git</li> </ul>"}, {"location": "terraform/", "title": "Terraform", "text": "<p>Terraform is an open-source infrastructure as code software tool created by HashiCorp. It enables users to define and provision a datacenter infrastructure using an awful high-level configuration language known as Hashicorp Configuration Language (HCL), or optionally JSON. Terraform supports a number of cloud infrastructure providers such as Amazon Web Services, IBM Cloud , Google Cloud Platform, DigitalOcean, Linode, Microsoft Azure, Oracle Cloud Infrastructure, OVH, or VMware vSphere as well as OpenNebula and OpenStack.</p>"}, {"location": "terraform/#installation", "title": "Installation", "text": "<p>Go to the releases page, download the latest release, decompress it and add it to your <code>$PATH</code>.</p>"}, {"location": "terraform/#tools", "title": "Tools", "text": "<ul> <li>tfschema: A binary that allows you   to see the attributes of the resources of the different providers. There are   some times that there are complex attributes that aren't shown on the docs   with an example. Here you'll see them clearly.</li> </ul> <pre><code>tfschema resource list aws | grep aws_iam_user\n&gt; aws_iam_user\n&gt; aws_iam_user_group_membership\n&gt; aws_iam_user_login_profile\n&gt; aws_iam_user_policy\n&gt; aws_iam_user_policy_attachment\n&gt; aws_iam_user_ssh_key\n\ntfschema resource show aws_iam_user\n+----------------------+-------------+----------+----------+----------+-----------+\n| ATTRIBUTE            | TYPE        | REQUIRED | OPTIONAL | COMPUTED | SENSITIVE |\n+----------------------+-------------+----------+----------+----------+-----------+\n| arn                  | string      | false    | false    | true     | false     |\n| force_destroy        | bool        | false    | true     | false    | false     |\n| id                   | string      | false    | true     | true     | false     |\n| name                 | string      | true     | false    | false    | false     |\n| path                 | string      | false    | true     | false    | false     |\n| permissions_boundary | string      | false    | true     | false    | false     |\n| tags                 | map(string) | false    | true     | false    | false     |\n| unique_id            | string      | false    | false    | true     | false     |\n+----------------------+-------------+----------+----------+----------+-----------+\n\n# Open the documentation of the resource in the browser\n\ntfschema resource browse aws_iam_user\n</code></pre> <ul> <li> <p>terraforming: Tool to export existing   resources to terraform</p> </li> <li> <p>terraboard: Web dashboard to   visualize and query terraform tfstate, you can search, compare and see the   most active ones. There are deployments for k8s.</p> </li> </ul> <pre><code>export AWS_ACCESS_KEY_ID=XXXXXXXXXXXXXXXXXXXX\nexport AWS_SECRET_ACCESS_KEY=XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\nexport AWS_DEFAULT_REGION=eu-west-1\nexport AWS_BUCKET=terraform-tfstate-20180119\nexport TERRABOARD_LOG_LEVEL=debug\ndocker network create terranet\ndocker run -ti --rm --name db -e POSTGRES_USER=gorm -e POSTGRES_DB=gorm -e POSTGRES_PASSWORD=\"mypassword\" --net terranet postgres\ndocker run -ti --rm -p 8080:8080 -e AWS_REGION=\"$AWS_DEFAULT_REGION\" -e AWS_ACCESS_KEY_ID=\"${AWS_ACCESS_KEY_ID}\" -e AWS_SECRET_ACCESS_KEY=\"${AWS_SECRET_ACCESS_KEY}\" -e AWS_BUCKET=\"$AWS_BUCKET\" -e DB_PASSWORD=\"mypassword\" --net terranet camptocamp/terraboard:latest\n</code></pre> <ul> <li> <p>tfenv: Install different versions of terraform   <pre><code>git clone https://github.com/tfutils/tfenv.git ~/.tfenv\necho 'export PATH=\"$HOME/.tfenv/bin:$PATH\"' &gt;&gt; ~/.bashrc\necho 'export PATH=\"$HOME/.tfenv/bin:$PATH\"' &gt;&gt; ~/.zshrc\ntfenv list-remote\ntfenv install 0.12.8\nterraform version\ntfenv install 0.11.15\nterraform version\ntfenv use 0.12.8\nterraform version\n</code></pre></p> </li> <li> <p>https://github.com/eerkunt/terraform-compliance</p> </li> <li> <p>landscape: A program to   modify the <code>plan</code> and show a nicer version, really useful when it's shown as   json. Right now   it only works for terraform 11.</p> </li> </ul> <pre><code>terraform plan | landscape\n</code></pre> <ul> <li>k2tf: Program to convert k8s yaml   manifestos to HCL.</li> </ul>"}, {"location": "terraform/#editor-plugins", "title": "Editor Plugins", "text": "<p>For Vim:</p> <ul> <li>vim-terraform: Execute tf from     vim and autoformat when saving.</li> <li>vim-terraform-completion:     linter and autocomplete.</li> </ul>"}, {"location": "terraform/#good-practices-and-maintaining", "title": "Good practices and maintaining", "text": "<ul> <li>fmt: Formats the code   following hashicorp best practices.</li> </ul> <pre><code>terraform fmt\n</code></pre> <ul> <li> <p>Validate: Tests that   the syntax is correct.   <pre><code>terraform validate\n</code></pre></p> </li> <li> <p>Documentaci\u00f3n: Generates   a table in markdown with the inputs and outputs.</p> </li> </ul> <pre><code>terraform-docs markdown table *.tf &gt; README.md\n\n## Inputs\n\n| Name | Description | Type | Default | Required |\n|------|-------------|:----:|:-----:|:-----:|\n| broker_numbers | Number of brokers | number | `\"3\"` | no |\n| broker_size | AWS instance type for the brokers | string | `\"kafka.m5.large\"` | no |\n| ebs_size | Size of the brokers disks | string | `\"300\"` | no |\n| kafka_version | Kafka version | string | `\"2.1.0\"` | no |\n\n## Outputs\n\n| Name | Description |\n|------|-------------|\n| brokers_masked_endpoints | Zookeeper masked endpoints |\n| brokers_real_endpoints | Zookeeper real endpoints |\n| zookeeper_masked_endpoints | Zookeeper masked endpoints |\n| zookeeper_real_endpoints | Zookeeper real endpoints |\n</code></pre> <ul> <li>Terraform lint (tflint): Only works with   some AWS resources. It allows the validation against a third party API. For   example:   <pre><code>  resource \"aws_instance\" \"foo\" {\nami           = \"ami-0ff8a91507f77f867\"\ninstance_type = \"t1.2xlarge\" # invalid type!\n}\n</code></pre></li> </ul> <p>The code is valid, but in AWS there doesn't exist the type <code>t1.2xlarge</code>. This   test avoids this kind of issues.</p> <pre><code>wget https://github.com/wata727/tflint/releases/download/v0.11.1/tflint_darwin_amd64.zip\nunzip tflint_darwin_amd64.zip\nsudo install tflint /usr/local/bin/\ntflint -v\n</code></pre> <p>We can automate all the above to be executed before we do a commit using the pre-commit framework.</p> <pre><code>pip install pre-commit\ncd $proyectoConTerraform\necho \"\"\"repos:\n- repo: git://github.com/antonbabenko/pre-commit-terraform\n  rev: v1.19.0\n  hooks:\n    - id: terraform_fmt\n    - id: terraform_validate\n    - id: terraform_docs\n    - id: terraform_tflint\n\"\"\" &gt; .pre-commit-config.yaml\npre-commit install\npre-commit run terraform_fmt\npre-commit run terraform_validate --file dynamo.tf\npre-commit run -a\n</code></pre>"}, {"location": "terraform/#tests", "title": "Tests", "text": "<p>Motivation</p>"}, {"location": "terraform/#static-analysis", "title": "Static analysis", "text": ""}, {"location": "terraform/#linters", "title": "Linters", "text": "<ul> <li>conftest</li> <li>tflint</li> <li><code>terraform validate</code></li> </ul>"}, {"location": "terraform/#dry-run", "title": "Dry run", "text": "<ul> <li><code>terraform plan</code></li> <li>hashicorp sentinel</li> <li>terraform-compliance</li> </ul>"}, {"location": "terraform/#unit-tests", "title": "Unit tests", "text": "<p>There is no real unit testing in infrastructure code as you need to deploy it in a real environment</p> <ul> <li> <p><code>terratest</code> (works for k8s and terraform)</p> <p>Some sample code in:</p> <ul> <li>github.com/gruntwork-io/infrastructure-as-code-testing-talk</li> <li>gruntwork.io</li> </ul> </li> </ul>"}, {"location": "terraform/#e2e-test", "title": "E2E test", "text": "<ul> <li>Too slow and too brittle to be worth it</li> <li>Use incremental e2e testing</li> </ul>"}, {"location": "terraform/#variables", "title": "Variables", "text": "<p>It's a good practice to name the resource before the particularization of the resource, so you can search all the elements of that resource, for example, instead of <code>client_cidr</code> and <code>operations_cidr</code> use <code>cidr_operations</code> and <code>cidr_client</code></p> <pre><code>variable \"list_example\"{\ndescription = \"An example of a list\"\ntype = \"list\"\ndefault = [1, 2, 3]\n}\n\nvariable \"map_example\"{\ndescription = \"An example of a dictionary\"\ntype = \"map\"\ndefault = {\nkey1 = \"value1\"\nkey2 = \"value2\"\n}\n}\n</code></pre> <p>For the use of maps inside maps or lists investigate <code>zipmap</code></p> <p>To access you have to use <code>\"${var.list_example}\"</code></p> <p>For secret variables we use: <pre><code>variable \"db_password\" {\ndescription = \"The password for the database\"\n}\n</code></pre></p> <p>Which has no default value, we save that password in our keystore and pass it as environmental variable <pre><code>export TF_VAR_db_password=\"{{ your password }}\"\nterragrunt plan\n</code></pre></p> <p>As a reminder, Terraform stores all variables in its state file in plain text, including this database password, which is why your terragrunt config should always enable encryption for remote state storage in S3</p>"}, {"location": "terraform/#interpolation-of-variables", "title": "Interpolation of variables", "text": "<p>You can't interpolate in variables, so instead of <pre><code>variable \"sistemas_gpg\" {\ndescription = \"Sistemas public GPG key for Zena\"\ntype = \"string\"\ndefault = \"${file(\"sistemas_zena.pub\")}\"\n}\n</code></pre> You have to use locals</p> <pre><code>locals {\nsistemas_gpg = \"${file(\"sistemas_zena.pub\")}\"\n}\n\n\"${local.sistemas_gpg}\"\n</code></pre>"}, {"location": "terraform/#show-information-of-the-resources", "title": "Show information of the resources", "text": "<p>Get information of the infrastructure. Output variables show up in the console after you run <code>terraform apply</code>, you can also use <code>terraform output [{{ output_name }}]</code> to see the value of a specific output without applying any changes</p> <pre><code>output \"public_ip\" {\nvalue = \"${aws_instance.example.public_ip}\"\n}\n</code></pre> <pre><code>&gt; terraform apply\naws_security_group.instance: Refreshing state... (ID: sg-db91dba1)\naws_instance.example: Refreshing state... (ID: i-61744350)\nApply complete! Resources: 0 added, 0 changed, 0 destroyed.\nOutputs:\npublic_ip = 54.174.13.5\n</code></pre>"}, {"location": "terraform/#data-source", "title": "Data source", "text": "<p>A data source represents a piece of read-only information that is fetched from the provider every time you run Terraform. It does not create anything new</p> <pre><code>data \"aws_availability_zones\" \"all\" {}\n</code></pre> <p>And you reference it with <code>\"${data.aws_availability_zones.all.names}\"</code></p>"}, {"location": "terraform/#read-only-state-source", "title": "Read-only state source", "text": "<p>With <code>terraform_remote_state</code> you an fetch the Terraform state file stored by another set of templates in a completely read-only manner.</p> <p>From an app template we can read the info of the ddbb with <pre><code>data \"terraform_remote_state\" \"db\" {\nbackend = \"s3\"\nconfig {\nbucket = \"(YOUR_BUCKET_NAME)\"\nkey = \"stage/data-stores/mysql/terraform.tfstate\"\nregion = \"us-east-1\"\n}\n}\n</code></pre></p> <p>And you would access the variables inside the database terraform file with <code>data.terraform_remote_state.db.outputs.port</code></p> <p>To share variables from state, you need to to set them in the <code>outputs.tf</code> file.</p>"}, {"location": "terraform/#template_file-source", "title": "Template_file source", "text": "<p>It is used to load templates, it has two parameters, <code>template</code> which is a string and <code>vars</code> which is a map of variables. it has one output attribute called <code>rendered</code>, which is the result of rendering template. For example</p> <pre><code># File: user-data.sh\n#!/bin/bash\ncat &gt; index.html &lt;&lt;EOF\n&lt;h1&gt;Hello, World&lt;/h1&gt;\n&lt;p&gt;DB address: ${db_address}&lt;/p&gt;\n&lt;p&gt;DB port: ${db_port}&lt;/p&gt;\nEOF\nnohup busybox httpd -f -p \"${server_port}\" &amp;\n</code></pre> <pre><code>data \"template_file\" \"user_data\" {\ntemplate = \"${file(\"user-data.sh\")}\"\nvars {\nserver_port = \"${var.server_port}\"\ndb_address = \"${data.terraform_remote_state.db.address}\"\ndb_port = \"${data.terraform_remote_state.db.port}\"\n}\n}\n</code></pre>"}, {"location": "terraform/#resource-lifecycle", "title": "Resource lifecycle", "text": "<p>The <code>lifecycle</code> parameter is a meta-parameter, it exist on about every resource in Terraform. You can add a <code>lifecycle</code> block to any resource to configure how that resource should be created, updated or destroyed.</p> <p>The available options are: * <code>create_before_destroy</code>: Which if set to true will create a replacement   resource before destroying hte original resource * <code>prevent_destroy</code>: If set to true, any attempt to delete that resource   (<code>terraform destroy</code>), will fail, to delete it you have to first remove the   <code>prevent_destroy</code></p> <pre><code>resource \"aws_launch_configuration\" \"example\" {\nimage_id = \"ami-40d28157\"\ninstance_type = \"t2.micro\"\nsecurity_groups = [\"${aws_security_group.instance.id}\"]\nuser_data = &lt;&lt;-EOF\n              #!/bin/bash\n              echo \"Hello, World\" &gt; index.html\n              nohup busybox httpd -f -p \"${var.server_port}\" &amp;\n              EOF\nlifecycle {\ncreate_before_destroy = true\n}\n}\n</code></pre> <p>If you set the <code>create_before_destroy</code> on a resource, you also have to set it on every resource that X depends on (if you forget, you'll get errors about cyclical dependencies). In the case of the launch configuration, that means you need to set <code>create_before_destroy</code> to true on the security group:</p> <pre><code>resource \"aws_security_group\" \"instance\" {\nname = \"terraform-example-instance\"\ningress {\nfrom_port = \"${var.server_port}\"\nto_port = \"${var.server_port}\"\nprotocol = \"tcp\"\ncidr_blocks = [\"0.0.0.0/0\"]\n}\nlifecycle {\ncreate_before_destroy = true\n}\n}\n</code></pre>"}, {"location": "terraform/#use-collaboratively", "title": "Use collaboratively", "text": ""}, {"location": "terraform/#share-state", "title": "Share state", "text": "<p>The best option is to use S3 as bucket of the config.</p> <p>First create it <pre><code>resource \"aws_s3_bucket\" \"terraform_state\" {\nbucket = \"terraform-up-and-running-state\"\nversioning {\nenabled = true\n}\nlifecycle {\nprevent_destroy = true\n}\n}\n</code></pre></p> <p>And then configure terraform <pre><code>terraform remote config \\\n-backend=s3 \\\n-backend-config=\"bucket=(YOUR_BUCKET_NAME)\" \\\n-backend-config=\"key=global/s3/terraform.tfstate\" \\\n-backend-config=\"region=us-east-1\" \\\n-backend-config=\"encrypt=true\"\n</code></pre></p> <p>In this way terraform will automatically pull the latest state from this bucked and push the latest state after running a command</p>"}, {"location": "terraform/#lock-terraform", "title": "Lock terraform", "text": "<p>To avoid several people running terraform at the same time, we'd use <code>terragrunt</code> a wrapper for terraform that manages remote state for you automatically and provies locking by using DynamoDB (in the free tier)</p> <p>Inside the <code>terraform_config.tf</code> you create the dynamodb table and then configure your <code>s3</code> backend to use it</p> <pre><code># create a dynamodb table for locking the state file\nresource \"aws_dynamodb_table\" \"dynamodb-terraform-state-lock\" {\nname         = \"terraform-state-lock-dynamo\"\nhash_key     = \"LockID\"\nbilling_mode = \"PAY_PER_REQUEST\"\nattribute {\nname = \"LockID\"\ntype = \"S\"\n}\n}\n\nterraform {\nbackend \"s3\" {\nbucket = \"provider-tfstate\"\nkey    = \"global/s3/terraform.tfstate\"\nregion = \"eu-west-1\"\nencrypt = \"true\"\ndynamodb_table = \"global-s3\"\n}\n}\n</code></pre> <p>You'll probably need to execute an <code>terraform apply</code> with the <code>dynamodb_table</code> line commented</p> <p>If you want to unforce a lock, execute:</p> <pre><code>terraform force-unlock {{ unlock_id }}\n</code></pre> <p>You get the <code>unlock_id</code> from an error trying to execute any <code>terraform</code> command</p>"}, {"location": "terraform/#modules", "title": "Modules", "text": "<p>In terraform you can put code inside of a <code>module</code> and reuse in multiple places throughout your code.</p> <p>The provider resource should be specified by the user and not in the modules</p> <p>Whenever you add a module to your terraform template or modify its source parameter you need to run a get command before you run <code>plan</code> or <code>apply</code></p> <pre><code>terraform get\n</code></pre> <p>To extract output variables of a module to the parent tf file you should use</p> <p><code>${module.{{module.name}}.{{output_name}}}</code></p>"}, {"location": "terraform/#basics", "title": "Basics", "text": "<p>Any set of Terraform templates in a directory is a module.</p> <p>The good practice is to have a directory called <code>modules</code> in your parent project directory. There you git clone the desired modules. and for example inside <code>pro/services/bastion/main.tf</code> you'd call it with:</p> <pre><code>provider \"aws\" {\nregion = \"eu-west-1\"\n}\n\nmodule \"bastion\" {\nsource = \"../../../modules/services/bastion/\"\n}\n</code></pre>"}, {"location": "terraform/#outputs", "title": "Outputs", "text": "<p>Modules encapsulate their resources. A resource in one module cannot directly depend on resources or attributes in other modules, unless those are exported through outputs. These outputs can be referenced in other places in your configuration, for example:</p> <pre><code>resource \"aws_instance\" \"client\" {\nami               = \"ami-408c7f28\"\ninstance_type     = \"t1.micro\"\navailability_zone = \"${module.consul.server_availability_zone}\"\n}\n</code></pre>"}, {"location": "terraform/#import", "title": "Import", "text": "<p>You can import the different parts with <code>terraform import {{resource_type}}.{{resource_name}} {{ resource_id }}</code></p> <p>For examples see the documentation of the desired resource.</p>"}, {"location": "terraform/#bulk-import", "title": "Bulk import", "text": "<p>But if you want to bulk import sources, I suggest using <code>terraforming</code>.</p>"}, {"location": "terraform/#bad-points", "title": "Bad points", "text": "<ul> <li>Manually added resources wont be managed by terraform, therefore you can't use   it to enforce as shown in this   bug.</li> <li>If you modify the LC of an ASG, the instances don't get rolling updated, you   have to do it manually.</li> <li>They call the dictionaries <code>map</code>... (/\uff9f\u0414\uff9f)/</li> <li>The conditionals are really ugly. You need to use <code>count</code>.</li> <li>You can't split long strings xD</li> </ul>"}, {"location": "terraform/#best-practices", "title": "Best practices", "text": "<p>Name the resources with <code>_</code> instead of <code>-</code> so the editor's completion work :)</p>"}, {"location": "terraform/#vpc", "title": "VPC", "text": "<p>Don't use the default vpc</p>"}, {"location": "terraform/#security-groups", "title": "Security groups", "text": "<p>Instead of using <code>aws_security_group</code> to define the ingress and egress rules, use it only to create the empty security group and use <code>aws_security_group_rule</code> to add the rules, otherwise you'll get into a cycle loop</p> <p>The sintaxis of an egress security group must be <code>egress_from_{{source}}_to_destination</code>. The sintaxis of an ingress security group must be <code>ingress_to_{{destination}}_from_{{source}}</code></p> <p>Also set the order of the arguments, so they look like the name.</p> <p>For ingress rule:</p> <pre><code>security_group_id = ...\ncidr_blocks = ...\n</code></pre> <p>And in egress should look like:</p> <pre><code>security_group_id = ...\ncidr_blocks = ...\n</code></pre> <p>Imagine you want to filter the traffic from A -&gt; B, the egress rule from A to B should go besides the ingress rule from B to A.</p>"}, {"location": "terraform/#default-security-group", "title": "Default security group", "text": "<p>You can't manage the default security group of an vpc, therefore you have to adopt it and set it to no rules at all with <code>aws_default_security_group</code> resource</p>"}, {"location": "terraform/#iam", "title": "IAM", "text": "<p>You have to generate an gpg key and export it in base64</p> <pre><code>gpg --export {{ gpg_id }} | base64\n</code></pre> <p>To see the secrets you have to decrypt it <pre><code>terraform output secret | base64 --decode | gpg -d\n</code></pre></p>"}, {"location": "terraform/#sensitive-information", "title": "Sensitive information", "text": "<p>One of the most common questions we get about using Terraform to manage infrastructure as code is how to handle secrets such as passwords, API keys, and other sensitive data.</p> <p>Your secrets live in two places in a terraform environment:</p> <ul> <li>The Terraform state</li> <li>The Terraform source code.</li> </ul>"}, {"location": "terraform/#sensitive-information-in-the-terraform-state", "title": "Sensitive information in the Terraform State", "text": "<p>Every time you deploy infrastructure with Terraform, it stores lots of data about that infrastructure, including all the parameters you passed in, in a state file. By default, this is a terraform.tfstate file that is automatically generated in the folder where you ran terraform apply. </p> <p>That means that the secrets will end up in terraform.tfstate in plain text! This has been an open issue for more than 6 years now, with no clear plans for a first-class solution. There are some workarounds out there that can scrub secrets from your state files, but these are brittle and likely to break with each new Terraform release, so I don\u2019t recommend them.</p> <p>For the time being, you can:</p> <ul> <li>Store Terraform state in a backend that supports encryption: Instead of storing your state in a local <code>terraform.tfstate</code> file, Terraform natively supports a variety of backends, such as S3, GCS, and Azure Blob Storage. Many of these backends support encryption, so that instead of your state files being in plain text, they will always be encrypted, both in transit (e.g., via TLS) and on disk (e.g., via AES-256). Most backends also support collaboration features (e.g., automatically pushing and pulling state; locking), so using a backend is a must-have both from a security and teamwork perspective.</li> <li>Strictly control who can access your Terraform backend: Since Terraform state files may contain secrets, you\u2019ll want to carefully control who has access to the backend you\u2019re using to store your state files. For example, if you\u2019re using S3 as a backend, you\u2019ll want to configure an IAM policy that solely grants access to the S3 bucket for production to a small handful of trusted devs (or perhaps solely just the CI server you use to deploy to prod).</li> </ul> <p>There are several approaches here.</p> <p>First rely on the S3 encryption to protect the information in your state file.</p> <p>Second, use Vault provider to protect the state file.</p> <p>Third (but I won't use it) would be to use terrahelp</p>"}, {"location": "terraform/#sensitive-information-in-the-terraform-source-code", "title": "Sensitive information in the Terraform source code", "text": "<p>To store secrets in your source code you can:</p> <ul> <li>Use Secret Stores</li> <li>Use environment variables</li> <li>Use encrypted files</li> </ul> <p>Using Secret Stores is the best solution, but for that you'd need access and trust in a Secret Store provider which I don't have at the moment (if you want to follow this path check out Hashicorp Vault). Using environment variables is the worst solution because this technique helps you avoid storing secrets in plain text in your code, but it leaves the question of how to actually securely store and manage the secrets unanswered. So in a sense, this technique just kicks the can down the road, whereas the other techniques described later are more prescriptive. Although you could use a password manager such as <code>pass</code>. Using encrypted files is the solution that remains.</p> <p>If you don't want to install a secret store and are used to work with GPG, you can encrypt the secrets, store the cipher text in a file, and checking that file into the version control system. To encrypt some data, such as some secrets in a file, you need an encryption key. This key is itself a secret! This creates a bit of a conundrum: how do you securely store that key? You can\u2019t check the key into version control as plain text, as then there\u2019s no point of encrypting anything with it. You could encrypt the key with another key, but then you then have to figure out where to store that second key. So you\u2019re back to the \u201ckick the can down the road problem,\u201d as you still have to find a secure way to store your encryption key. Although you can use external solutions such as AWS KMS or GCP KMS we don't want to store that kind of information on big companies servers. A local and more beautiful way is to rely on PGP to do the encryption.</p> <p>We'll use then <code>sops</code> a Mozilla tool for managing secrets that can use PGP behind the scenes. <code>sops</code> can automatically decrypt a file when you open it in your text editor, so you can edit the file in plain text, and when you go to save those files, it automatically encrypts the contents again. </p> <p>Terraform does not yet have native support for decrypting files in the format used by <code>sops</code>. One solution is to install and use the custom provider for sops, <code>terraform-provider-sops</code>. Another option, is to use Terragrunt. To avoid installing more tools, it's better to use the terraform provider.</p> <p>First of all you may need to install <code>sops</code>, you can grab the latest release from their downloads page.</p> <p>Then in your terraform code you need to select the <code>sops</code> provider:</p> <pre><code>terraform {\nrequired_providers {\nsops = {\nsource = \"carlpett/sops\"\nversion = \"~&gt; 0.5\"\n}\n}\n}\n</code></pre> <p>Configure <code>sops</code> by defining the gpg keys in a <code>.sops.yaml</code> file at the top of your repository:</p> <pre><code>---\ncreation_rules:\n- pgp: &gt;-\n2829BASDFHWEGWG23WDSLKGL323534J35LKWERQS,\n2GEFDBW349YHEDOH2T0GE9RH0NEORIG342RFSLHH\n</code></pre> <p>Then create the secrets file with the command <code>sops secrets.enc.json</code> somewhere in your terraform repository. For example:</p> <pre><code>{\n\"password\": \"foo\",\n\"db\": {\"password\": \"bar\"}\n}\n</code></pre> <p>You'll be able to use these secrets in your terraform code. For example:</p> <pre><code>data \"sops_file\" \"secrets\" {\nsource_file = \"secrets.enc.json\"\n}\n\noutput \"root-value-password\" {\n  # Access the password variable from the map\nvalue = data.sops_file.secrets.data[\"password\"]\n}\n\noutput \"mapped-nested-value\" {\n  # Access the password variable that is under db via the terraform map of data\nvalue = data.sops_file.secrets.data[\"db.password\"]\n}\n\noutput \"nested-json-value\" {\n  # Access the password variable that is under db via the terraform object\nvalue = jsondecode(data.sops_file.secrets.raw).db.password\n}\n</code></pre> <p>Sops also supports encrypting the entire file when in other formats. Such files can also be used by specifying <code>input_type = \"raw\"</code>:</p> <pre><code>data \"sops_file\" \"some-file\" {\nsource_file = \"secret-data.txt\"\ninput_type = \"raw\"\n}\n\noutput \"do-something\" {\nvalue = data.sops_file.some-file.raw\n}\n</code></pre>"}, {"location": "terraform/#rds-credentials", "title": "RDS credentials", "text": "<p>The RDS credentials are saved in plaintext both in the definition and in the state file, see this bug for more information. The value of <code>password</code> is not compared against the value of the password in the cloud, so as long as the string in the code and the state file remains the same, it won't try to change it.</p> <p>As a workaround, you can create the RDS with a fake password <code>changeme</code>, and once the resource is created, run an <code>aws</code> command to change it. That way, the value in your code and the state is not the real one, but it won't try to change it.</p> <p>Inspired in this gist and the <code>local-exec</code> docs, you could do:</p> <pre><code>resource \"aws_db_instance\" \"main\" {\nusername = \"postgres\"\npassword = \"changeme\"\n...\n}\n\nresource \"null_resource\" \"master_password\" {\ntriggers {\ndb_host = aws_db_instance.main.address\n}\nprovisioner \"local-exec\" {\ncommand = \"pass generate rds_main_password; aws rds modify-db-instance --db-instance-identifier $INSTANCE --master-user-password $(pass show rds_main_password)\"\nenvironment = {\nINSTANCE = aws_db_instance.main.identifier\n}\n}\n}\n</code></pre> <p>Where the password is stored in your <code>pass</code> repository that can be shared with the team.</p> <p>If you're wondering why I added such a long line, well it's because of HCL! as you can't split long strings, marvelous isn't it? xD</p>"}, {"location": "terraform/#loops", "title": "Loops", "text": "<p>You can't use nested lists or dictionaries, see this 2015 bug</p>"}, {"location": "terraform/#loop-over-a-variable", "title": "Loop over a variable", "text": "<pre><code>variable \"vpn_egress_tcp_ports\" {\ndescription = \"VPN egress tcp ports \"\ntype = \"list\"\ndefault = [50, 51, 500, 4500]\n}\n\nresource \"aws_security_group_rule\" \"ingress_tcp_from_ops_to_vpn_instance\"{\ncount = \"${length(var.vpn_egress_tcp_ports)}\"\ntype = \"ingress\"\nfrom_port   = \"${element(var.vpn_egress_tcp_ports, count.index)}\"\nto_port     = \"${element(var.vpn_egress_tcp_ports, count.index)}\"\nprotocol    = \"tcp\"\ncidr_blocks = [ \"${var.cidr}\"]\nsecurity_group_id = \"${aws_security_group.pro_ins_vpn.id}\"\n}\n</code></pre>"}, {"location": "terraform/#refactoring", "title": "Refactoring", "text": "<p>Refactoring in terraform is ugly business</p>"}, {"location": "terraform/#refactoring-in-modules", "title": "Refactoring in modules", "text": "<p>If you try to refactor your terraform state into modules it will try to destroy and recreate all the elements of the module...</p>"}, {"location": "terraform/#refactoring-the-state-file", "title": "Refactoring the state file", "text": "<pre><code>terraform state mv -state-out=other.tfstate module.web module.web\n</code></pre>"}, {"location": "terraform/#google-cloud-integration", "title": "Google cloud integration", "text": "<p>You configure it in the terraform directory <pre><code>// Configure the Google Cloud provider\nprovider \"google\" {\ncredentials = \"${file(\"account.json\")}\"\nproject     = \"my-gce-project\"\nregion      = \"us-central1\"\n}\n</code></pre></p> <p>To download the json go to the Google Developers Console. Go to <code>Credentials</code> then <code>Create credentials</code> and finally <code>Service account key</code>.</p> <p>Select <code>Compute engine default service account</code> and select <code>JSON</code> as the key type.</p>"}, {"location": "terraform/#ignore-the-change-of-an-attribute", "title": "Ignore the change of an attribute", "text": "<p>Sometimes you don't care whether some attributes of a resource change, if that's the case use the <code>lifecycle</code> statement:</p> <pre><code>resource \"aws_instance\" \"example\" {\n  # ...\n\nlifecycle {\nignore_changes = [\n      # Ignore changes to tags, e.g. because a management agent\n      # updates these based on some ruleset managed elsewhere.\ntags,\n]\n}\n}\n</code></pre>"}, {"location": "terraform/#define-the-default-value-of-an-variable-that-contains-an-object-as-empty", "title": "Define the default value of an variable that contains an object as empty", "text": "<pre><code>variable \"database\" {\ntype = object({\nsize                 = number\ninstance_type        = string\nstorage_type         = string\nengine               = string\nengine_version       = string\nparameter_group_name = string\nmulti_az             = bool\n})\ndefault     = null\n</code></pre>"}, {"location": "terraform/#conditionals", "title": "Conditionals", "text": ""}, {"location": "terraform/#elif", "title": "Elif", "text": "<pre><code>locals {\ntest = \"${ condition ? value : (elif-condition ? elif-value : else-value)}\"\n}\n</code></pre>"}, {"location": "terraform/#do-a-conditional-if-a-variable-is-not-null", "title": "Do a conditional if a variable is not null", "text": "<pre><code>resource \"aws_db_instance\" \"instance\" {\ncount                = var.database == null ? 0 : 1\n...\n</code></pre>"}, {"location": "terraform/#debugging", "title": "Debugging", "text": "<p>You can set the <code>TF_LOG</code> environmental variable to one of the log levels <code>TRACE</code>, <code>DEBUG</code>, <code>INFO</code>, <code>WARN</code> or <code>ERROR</code> to change the verbosity of the logs.</p> <p>To remove the debug traces run <code>unset TF_LOG</code>.</p>"}, {"location": "terraform/#snippets", "title": "Snippets", "text": ""}, {"location": "terraform/#create-a-list-of-resources-based-on-a-list-of-strings", "title": "Create a list of resources based on a list of strings", "text": "<pre><code>variable \"subnet_ids\" {\ntype = list(string)\n}\n\nresource \"aws_instance\" \"server\" {\n  # Create one instance for each subnet\ncount = length(var.subnet_ids)\n\nami           = \"ami-a1b2c3d4\"\ninstance_type = \"t2.micro\"\nsubnet_id     = var.subnet_ids[count.index]\n\ntags = {\nName = \"Server ${count.index}\"\n}\n}\n</code></pre> <p>If you want to use this generated list on another resource extracting for example the id you can use</p> <pre><code>aws_instance.server.*.id\n</code></pre>"}, {"location": "terraform/#references", "title": "References", "text": "<ul> <li>Docs</li> <li>Modules registry</li> <li>terraform-aws-modules</li> <li>AWS providers</li> <li>AWS examples</li> <li>GCloud examples</li> <li>Good and bad sides of terraform</li> <li>Awesome Terraform</li> </ul>"}, {"location": "time_management/", "title": "Time management", "text": "<p>Time management is the process of planning and exercising conscious control of time spent on specific activities, especially to increase effectiveness, efficiency, and productivity. It involves a juggling act of various demands upon a person relating to work, social life, family, hobbies, personal interests, and commitments with the finiteness of time. Using time effectively gives the person \"choice\" on spending or managing activities at their own time and expediency.</p> <p>To be able to do time management, you first need to define how do you want to increase your effectiveness, efficiency, and productivity. For me, it means increasing the amount and quality of work per unit of time or effort. Understanding work as any task that gets me closer to a goal. It doesn't necessarily be related with professional work, it can be applied to a personal project, cleaning or hanging out with friends.</p> <p>The rest of the article describes the approaches I use to maximize this idea of efficiency. If you have a different understanding, goal or your brain works in a different way than mine, most of the guidelines may not apply to you, but they could spark some ideas that you can implement on your daily life.</p> <p>To increase the productivity we can:</p> <ul> <li>Reduce the time spent doing unproductive tasks.</li> <li>Improve the way you do the tasks.</li> <li>Improve how you manage your tools.</li> <li>Improve your state and environment to be more efficient.</li> </ul>"}, {"location": "time_management/#reduce-the-time-spent-doing-unproductive-tasks", "title": "Reduce the time spent doing unproductive tasks", "text": "<p>Sadly, the day has only 24 hours you can use. There's nothing to do about it, we can however reduce the amount of wasted time to make a better use of the time that we have.</p>"}, {"location": "time_management/#minimize-the-context-switches", "title": "Minimize the context switches", "text": "<p>Each time we switch from one task to another, the brain needs to load all the necessary information to be able to address the new task. Dumping the old task information and loading the new is both time consuming and exhausting, so do it consciously and sparingly.</p> <p>One way of improving this behaviour is by using the Pomodoro technique.</p>"}, {"location": "time_management/#interruption-management", "title": "Interruption management", "text": "<p>We've come to accept that we need to be available 24/7 and answer immediately, that makes us slaves of the interruptions, it drives our work and personal relations. I feel that out of respect of ourselves and the other's time, we need to change that perspective. Most of the times interruptions can wait 20 or 60 minutes, and many of them can be avoided with better task and time planning.</p> <p>Interruptions are one of the main productivity killers. Not only they unexpectedly break your workflow, they also add undesired mental load as you are waiting for them to happen, and need to check them often. As we've seen previously, to be productive you need to be able to work on a task for 20 minutes without checking the interruption channels.</p> <p>Fill up your own interruption analysis report and define your workflow to manage them.</p>"}, {"location": "time_management/#avoid-lost-time-doing-nothing", "title": "Avoid lost time doing nothing", "text": "<p>Sometimes I catch myself watching at the screen with zero mental activity and drooling. Other times I endlessly switch between browser tabs or the email client and the chat clients for no reason, it's just a reflex act. You probably have similar behaviours that lead you nowhere. Some should be an alert that you need a break (don't drool the keyboard please), but others are bad uncontrolled behaviours that could be identified and got rid of.</p>"}, {"location": "time_management/#fix-your-environment", "title": "Fix your environment", "text": "<p>When we loose time, we don't do it consciously, that's why it's difficult for us to stay alert and actively try to change those behaviours.  It's much easier to fix your environment so that the reasons that trigger the time loss don't happen at all.</p> <p>For example, if you keep on going back to the email client regularly even though you decided only to check it three times a day, instead of mentally punishing yourself when you check it, close the client or move it to another workspace so it's not where you're used to see it.</p>"}, {"location": "time_management/#dont-wait-switch-task", "title": "Don't wait, switch task", "text": "<p>Even though we want to minimize the context switches, staring at the screen for a long process to end makes no sense. If you do task management well, the context switch toll gets smaller enough that whenever you hit a block in the task you're working on, you can switch to another one. A block can be caused by a long running process or waiting for someone to do something.</p> <p>If you find concentrating difficult, don't do this, it's a hard skill to master.</p> <p>When a block comes, I first try to switch back to processes that I was already working on. Try to have as less processes as possible, less than three if possible. If there is only one active process, look at the task plan for the next step that could be done in parallel. As both processes work on the same task, they share most of the context, so the switch is cheap. If there is none, go to the day plan to start the first step of the next task in the plan.</p>"}, {"location": "time_management/#improve-the-way-you-do-the-tasks", "title": "Improve the way you do the tasks", "text": "<p>Improve how you manage your tasks to:</p> <ul> <li>Reduce your mental load, so you can use those resources doing productive     work.</li> <li>Improve your efficiency.</li> <li>Make more realistic estimations, thus meeting the commited deadlines.</li> <li>Finish what you start.</li> <li>Know you're working towards your ultimate goals</li> <li>Stop feeling lost or overburdened.</li> <li>Make context switches cheaper.</li> </ul>"}, {"location": "time_management/#improve-how-you-manage-your-tools", "title": "Improve how you manage your tools", "text": "<p>Most of the tasks or processes we do involve some kind of tool, the better you know how to use them, the better your efficiency will be. The more you use a tool, the more it's worth the investment of time to improve your usage of it.</p> <p>Whenever I use a tool, I try to think if I could configure it or use it in a way that will make it easier or quicker. Don't go crazy and try to change everything. Go step by step, and once you've internalized the improvement, implement the next.</p> <ul> <li>Email management.</li> <li>Instant messages management.</li> <li>Meetings.</li> </ul>"}, {"location": "time_management/#meetings", "title": "Meetings", "text": "<p>Calls, video calls, group calls or physical meetings are the best communication channel to transmit non trivial short messages. Even if they are the most efficient, they will break your working workflow, as you'll need to prepare yourself to know what to say and how, go to the meeting location, and then process all the information gathered. That's why if not used wisely, it can be a sink of productivity.</p> <p>Try to minimize and group the meetings, thus having less interruptions. Maximize the continuous uninterrupted time, so schedule them at the start or end of the morning or afternoon.</p> <p>Once you agreed to attend, make each of them count. Define an agenda and a time limit per section. That'll keep the conversation on track, and will give enough information to the attendees to decide if they need to be there. Likewise, whenever you're invited to a meeting, value if you need to go. If you don't, politely decline the offer. Sometimes assigning someone the role to conduct the meeting, or taking turns to talk can help.</p> <p>There are more informal meetings where you don't need all these constrains and formality. For example in a coffee break. You know that they are going to be unproductive but that's ok too. Master your tools and apply them where you think they are needed.</p>"}, {"location": "time_management/#improve-your-state", "title": "Improve your state", "text": "<p>To be able to work efficiently, manage your tasks and change your habits you need to have the appropriate state of mind. This last factor is often overlooked, but one of the most important.</p> <p>To be efficient you need to take care of yourself. Analyze how are you to detect what physical or mental attributes aren't at the optimum level and act accordingly by fixing them and adjusting your plans.</p> <p>This will be difficult to most of us, as we are disconnected from our bodies, and don't know how to study ourselves. If it's your case, you could start by meditating or to quantifying yourself.</p> <p>Some of the vectors you can work on to improve your state are:</p> <ul> <li>Sleep better.</li> <li>Work out.</li> <li>Hang out.</li> <li>Isolate your personal life from your work life.</li> <li>Procrastinate mindfully.</li> <li>Don't be a slave of the interruptions.</li> <li>Improve your working environment.</li> <li>Prevent illnesses through hygiene and exercise.</li> </ul>"}, {"location": "tool_management/", "title": "Tool management", "text": "<p>Most of the tasks or processes we do involve some kind of tool, the better you know how to use them, the better your efficiency will be. The more you use a tool, the more it's worth the investment of time to improve your usage of it.</p> <p>Whenever I use a tool, I try to think if I could configure it or use it in a way that will make it easier or quicker. Don't go crazy and try to change everything. Go step by step, and once you've internalized the improvement, implement the next.</p>"}, {"location": "torrents/", "title": "Torrents", "text": "<p>BitTorrent is a communication protocol for peer-to-peer file sharing (P2P), which enables users to distribute data and electronic files over the Internet in a decentralized manner.</p>"}, {"location": "torrents/#torrent-client-comparison", "title": "Torrent client comparison", "text": "<p>Each of us seeks something different for a torrent client, thus there is a wide set of software, you just need to find the one that's best for you. In my case I'm searching for a client that:</p> <ul> <li> <p>Scales well for many torrents</p> </li> <li> <p>Is robust</p> </li> <li> <p>Is maintained</p> </li> <li> <p>Is popular</p> </li> <li> <p>Is supported by the private trackers: Some torrent clients are banned by the   tracker because they don't report correctly to the tracker when   canceling/finishing a torrent session. If you use them then a few MB may not   be counted towards the stats near the end, and torrents may still be listed in   your profile for some time after you have closed the client. Each tracker has   their list of allowed clients. Make sure to check them.</p> </li> </ul> <p>Also, clients in alpha or beta versions should be avoided.</p> <ul> <li> <p>Can be easily monitored</p> </li> <li> <p>Has a Python library or an API to interact with</p> </li> <li> <p>Has clear and enough logs</p> </li> <li> <p>Has RSS support</p> </li> <li> <p>Has a pleasant UI</p> </li> <li> <p>Supports categories</p> </li> <li> <p>Can unpack content once it's downloaded</p> </li> <li> <p>No ads</p> </li> <li> <p>Easy to use behind a VPN with IP leakage protection.</p> </li> <li> <p>Easy to deploy</p> </li> </ul> <p>I don't need other features such as:</p> <ul> <li>Preview content</li> <li>Search in the torrent client</li> </ul>"}, {"location": "torrents/#rtorrent", "title": "Rtorrent", "text": "<p>RTorrent is entirely different from familiar open source torrents clients like uTorrent or Deluge. All the above-described torrent clients for Linux offer a graphical user interface, but rTorrent is a text-based app used in Terminal.</p> <p>RTorrent is written in C++ and demands an extremely low resource but provides a large scale of various features.</p> <p>You can use Rutorrent to have a graphical frontend.</p> <p>I've been using rutorrent for 3 years with binhex docker image but I've come through some issues. Let's see how it fulfills the needs:</p> <ul> <li>Scales well for many torrents: Nope, at least not for my case, once it reached   600 torrents it started behaving weird. Suddenly it stopped downloading and   uploading without apparent reason. The web interface started failing, the   docker was left unresponsive for big periods of time, with no trace in the   logs.</li> <li>Is robust: Nope, I had big issues trying to keep the image to the latest   version. The solution of the maintainer was to start from scratch and import   all the data.</li> <li>Is maintained: In theory it is but the latest release is of July 2019</li> <li>Is popular: It is, 3.7k in github and widely used in the torrenting community.</li> <li>Is supported by the private trackers: Yes</li> <li>Can be easily monitored: There are some prometheus exporters, but I haven't   tried them yet.</li> <li>Has a Python library or an API to interact with:   looks like there exists but the   latest commit is of 2014.</li> <li>Has clear and enough logs: Not at all, at least binhex image, the logs have   not helped me debug the problems. And the rtorrent logs are very difficult to   read.</li> <li>Has RSS support: Yes</li> <li>Has a pleasant UI: more less, there are more modern interfaces, but I ended up   using the stock.</li> <li>Supports categories: yes</li> <li>Can unpack content once it's downloaded: yes</li> <li>No ads: yes</li> <li>Easy to use behind a VPN with IP leakage protection: Yes thanks to   binhex docker</li> <li>Easy to deploy: Yes thanks to binhex docker.</li> </ul>"}, {"location": "torrents/#qbittorrent", "title": "Qbittorrent", "text": "<ul> <li>Scales well for many torrents:   looks like it does   but I'd have to try it myself.</li> <li>Is robust: It's what I've read, but we'd have to test it.</li> <li>Is maintained: Yes, last commit is of 5h ago, last release was last month,   nice insights with 15 PR   merged this week, 7 new, 9 closed issues and 21 new.</li> <li>Is popular: I think it's the most popular, 18.4k stars in github and   recommended everywhere.</li> <li>Is supported by the private trackers: Yes</li> <li>Can be easily monitored: There's   this nice prometheus exporter   with it's   graphana dashboard.   With the information shown in the graphana dashboard it looks you can do   alerts on whatever you want.</li> <li>Has a Python library or an API to interact with:   yup</li> <li>Has clear and enough logs: We'll have to use it to check this point.</li> <li>Has RSS support: Yup</li> <li>Has a pleasant UI: The stock one is a little bit outdated, but there are newer   Vue based interfaces.</li> <li>Supports categories: yup</li> <li>Can unpack content once it's downloaded:   yup</li> <li>No ads: yup</li> <li>Easy to use behind a VPN with IP leakage protection:   yup, another image done by   binhex, it doesn't have much support, but what can you do.</li> <li>Easy to deploy: yup</li> </ul> <p>Some nice features I like about qbittorrent:</p> <ul> <li>It disables DHT and PEX for private trackers:   You can enable all 3 of those options and your private torrents will stay   private. You can verify this by viewing the trackers tab on a private torrent   and the status for DHT/PEX/LSD will be \"Disabled\" and the message column will   say \"This torrent is private\".</li> <li>It works better with the *arr family than rutorrent.</li> </ul>"}, {"location": "torrents/#deluge", "title": "Deluge", "text": "<p>Deluge versions 2.x is not supported by some trackers</p>"}, {"location": "torrents/#transmission", "title": "Transmission", "text": "<p>If you search for something entirely free, open source, and comes with minimum configuration, then the Transmission torrent client is one of them. It supports cross-platform like Windows, Linux, Mac OS, and Unix-based systems.</p> <p>This powerful torrent client for Linux is incredibly lightweight and a system optimizer that doesn\u2019t take many resources from your system. It\u2019s neat, simple, and comes in plug-and-play mode. Transmission is perfect for users who want to download Torrents and nothing else.</p> <p>Not supported by private trackers</p>"}, {"location": "torrents/#frostwire", "title": "Frostwire", "text": "<p>Frostwire is out of the analysis as it has ads and it's not supported by private trackers.</p>"}, {"location": "torrents/#vuze", "title": "Vuze", "text": "<p>Not supported by some private trackers.</p>"}, {"location": "torrents/#aria2", "title": "Aria2", "text": "<p>Not supported by private trackers.</p>"}, {"location": "torrents/#tracker-comparison-conclusion", "title": "Tracker comparison conclusion", "text": "<p>If you need to use private trackers, you're pretty much tied to rtorrent or to qbittorrent. As I've used rutorrent for a while I'm going to try qbittorrent.</p>"}, {"location": "torrents/#private-tracker-torrent-client-configuration", "title": "Private tracker torrent client configuration", "text": "<p>Private trackers require you to configure your torrent client in a way so that you don't get kicked.</p> <ul> <li>Disable DHT: DHT can cause your stats to be recorded incorrectly and could   be seen as cheating.</li> <li>Disable PEX(peer exchange): This can also let outside people get access to   the tracker's torrents.</li> </ul>"}, {"location": "torrents/#references", "title": "References", "text": ""}, {"location": "tridactyl/", "title": "Tridactyl", "text": "<p>Tridactyl is a Vim-like interface for Firefox, inspired by Vimperator/Pentadactyl. </p>"}, {"location": "tridactyl/#usage", "title": "Usage", "text": ""}, {"location": "tridactyl/#select-text-from-the-page", "title": "Select text from the page", "text": "<p>You can either use <code>/</code> to search for the text and you'll be directly in <code>Visual mode</code> or you can use <code>v</code> to trigger the hints in the page, once you've selected your hint you can move around as you'd normally would in vim.</p>"}, {"location": "typer/", "title": "Typer", "text": "<p>Typer is a library for building CLI applications that users will love using and developers will love creating. Based on Python 3.6+ type hints.</p> <p>The key features are:</p> <ul> <li>Intuitive to write: Great editor support. Completion everywhere. Less time   debugging. Designed to be easy to use and learn. Less time reading docs.</li> <li>Easy to use: It's easy to use for the final users. Automatic help, and   automatic completion for all shells.</li> <li>Short: Minimize code duplication. Multiple features from each parameter   declaration. Fewer bugs.</li> <li>Start simple: The simplest example adds only 2 lines of code to your app: 1   import, 1 function call.</li> <li>Grow large: Grow in complexity as much as you want, create arbitrarily   complex trees of commands and groups of subcommands, with options and   arguments.</li> </ul>"}, {"location": "typer/#installation", "title": "Installation", "text": "<pre><code>pip install 'typer[all]'\n</code></pre>"}, {"location": "typer/#minimal-usage", "title": "Minimal usage", "text": "<pre><code>import typer\n\n\ndef main(name: str):\n    print(f\"Hello {name}\")\n\n\nif __name__ == \"__main__\":\n    typer.run(main)\n</code></pre>"}, {"location": "typer/#usage", "title": "Usage", "text": "<p>Create a <code>typer.Typer()</code> app, and create two subcommands with their parameters.</p> <pre><code>import typer\n\napp = typer.Typer()\n\n\n@app.command()\ndef hello(name: str):\n    print(f\"Hello {name}\")\n\n\n@app.command()\ndef goodbye(name: str, formal: bool = False):\n    if formal:\n        print(f\"Goodbye Ms. {name}. Have a good day.\")\n    else:\n        print(f\"Bye {name}!\")\n\n\nif __name__ == \"__main__\":\n    app()\n</code></pre>"}, {"location": "typer/#using-subcommands", "title": "Using subcommands", "text": "<p>In some cases, it's possible that your application code needs to live on a single file.</p> <pre><code>import typer\n\napp = typer.Typer()\nitems_app = typer.Typer()\napp.add_typer(items_app, name=\"items\")\nusers_app = typer.Typer()\napp.add_typer(users_app, name=\"users\")\n\n\n@items_app.command(\"create\")\ndef items_create(item: str):\n    print(f\"Creating item: {item}\")\n\n\n@items_app.command(\"delete\")\ndef items_delete(item: str):\n    print(f\"Deleting item: {item}\")\n\n\n@items_app.command(\"sell\")\ndef items_sell(item: str):\n    print(f\"Selling item: {item}\")\n\n\n@users_app.command(\"create\")\ndef users_create(user_name: str):\n    print(f\"Creating user: {user_name}\")\n\n\n@users_app.command(\"delete\")\ndef users_delete(user_name: str):\n    print(f\"Deleting user: {user_name}\")\n\n\nif __name__ == \"__main__\":\n    app()\n</code></pre> <p>Then you'll be able to call each subcommand with:</p> <pre><code>python main.py items create\n</code></pre> <p>For more complex code use nested subcommands</p>"}, {"location": "typer/#nested-subcommands", "title": "Nested Subcommands", "text": "<p>You can split the commands in different files for clarity once the code starts to grow:</p> <p>File: <code>reigns.py</code>:</p> <pre><code>import typer\n\napp = typer.Typer()\n\n\n@app.command()\ndef conquer(name: str):\n    print(f\"Conquering reign: {name}\")\n\n\n@app.command()\ndef destroy(name: str):\n    print(f\"Destroying reign: {name}\")\n\n\nif __name__ == \"__main__\":\n    app()\n</code></pre> <p>File: <code>towns.py</code>:</p> <pre><code>import typer\n\napp = typer.Typer()\n\n\n@app.command()\ndef found(name: str):\n    print(f\"Founding town: {name}\")\n\n\n@app.command()\ndef burn(name: str):\n    print(f\"Burning town: {name}\")\n\n\nif __name__ == \"__main__\":\n    app()\n</code></pre> <p>File: <code>lands.py</code>:</p> <pre><code>import typer\n\nimport reigns\nimport towns\n\napp = typer.Typer()\napp.add_typer(reigns.app, name=\"reigns\")\napp.add_typer(towns.app, name=\"towns\")\n\nif __name__ == \"__main__\":\n    app()\n</code></pre> <p>File: <code>users.py</code>:</p> <pre><code>import typer\n\napp = typer.Typer()\n\n\n@app.command()\ndef create(user_name: str):\n    print(f\"Creating user: {user_name}\")\n\n\n@app.command()\ndef delete(user_name: str):\n    print(f\"Deleting user: {user_name}\")\n\n\nif __name__ == \"__main__\":\n    app()\n</code></pre> <p>File: <code>items.py</code>:</p> <pre><code>import typer\n\napp = typer.Typer()\n\n\n@app.command()\ndef create(item: str):\n    print(f\"Creating item: {item}\")\n\n\n@app.command()\ndef delete(item: str):\n    print(f\"Deleting item: {item}\")\n\n\n@app.command()\ndef sell(item: str):\n    print(f\"Selling item: {item}\")\n\n\nif __name__ == \"__main__\":\n    app()\n</code></pre> <p>File: <code>main.py</code>:</p> <pre><code>import typer\n\nimport items\nimport lands\nimport users\n\napp = typer.Typer()\napp.add_typer(users.app, name=\"users\")\napp.add_typer(items.app, name=\"items\")\napp.add_typer(lands.app, name=\"lands\")\n\nif __name__ == \"__main__\":\n    app()\n</code></pre>"}, {"location": "typer/#using-the-context", "title": "Using the context", "text": "<p>When you create a Typer application it uses <code>Click</code> underneath. And every Click application has a special object called a \"Context\" that is normally hidden.</p> <p>But you can access the context by declaring a function parameter of type <code>typer.Context</code>.</p> <p>The context is also used to store objects that you may need for all the commands, for example a repository.</p> <p>Tiangolo (<code>typer</code>s main developer)suggests to use global variables or a function with <code>lru_cache</code>.</p>"}, {"location": "typer/#using-short-option-names", "title": "Using short option names", "text": "<pre><code>import typer\n\n\ndef main(user_name: str = typer.Option(..., \"--name\", \"-n\")):\n    print(f\"Hello {user_name}\")\n\n\nif __name__ == \"__main__\":\n    typer.run(main)\n</code></pre> <p>The <code>...</code> as the first argument is to make the option required</p>"}, {"location": "typer/#create-vvv", "title": "Create <code>-vvv</code>", "text": "<p>You can make a CLI option work as a counter with the <code>counter</code> parameter:</p> <pre><code>import typer\n\n\ndef main(verbose: int = typer.Option(0, \"--verbose\", \"-v\", count=True)):\n    print(f\"Verbose level is {verbose}\")\n\n\nif __name__ == \"__main__\":\n    typer.run(main)\n</code></pre>"}, {"location": "typer/#get-the-command-line-application-directory", "title": "Get the command line application directory", "text": "<p>You can get the application directory where you can, for example, save configuration files with <code>typer.get_app_dir()</code>:</p> <pre><code>from pathlib import Path\n\nimport typer\n\nAPP_NAME = \"my-super-cli-app\"\n\n\ndef main() -&gt; None:\n\"\"\"Define the main command line interface.\"\"\"\n    app_dir = typer.get_app_dir(APP_NAME)\n    config_path: Path = Path(app_dir) / \"config.json\"\n    if not config_path.is_file():\n        print(\"Config file doesn't exist yet\")\n\n\nif __name__ == \"__main__\":\n    typer.run(main)\n</code></pre> <p>It will give you a directory for storing configurations appropriate for your CLI program for the current user in each operating system.</p>"}, {"location": "typer/#exiting-with-an-error-code", "title": "Exiting with an error code", "text": "<p><code>typer.Exit()</code> takes an optional code parameter. By default, code is <code>0</code>, meaning there was no error.</p> <p>You can pass a code with a number other than <code>0</code> to tell the terminal that there was an error in the execution of the program:</p> <pre><code>import typer\n\n\ndef main(username: str):\n    if username == \"root\":\n        print(\"The root user is reserved\")\n        raise typer.Exit(code=1)\n    print(f\"New user created: {username}\")\n\n\nif __name__ == \"__main__\":\n    typer.run(main)\n</code></pre>"}, {"location": "typer/#create-a-version-command", "title": "Create a <code>--version</code> command", "text": "<p>You could use a callback to implement a <code>--version</code> CLI option.</p> <p>It would show the version of your CLI program and then it would terminate it. Even before any other CLI parameter is processed.</p> <pre><code>from typing import Optional\n\nimport typer\n\n__version__ = \"0.1.0\"\n\n\ndef version_callback(value: bool) -&gt; None:\n\"\"\"Print the version of the program.\"\"\"\n    if value:\n        print(f\"Awesome CLI Version: {__version__}\")\n        raise typer.Exit()\n\n\ndef main(\n    version: Optional[bool] = typer.Option(\n        None, \"--version\", callback=version_callback, is_eager=True\n    ),\n) -&gt; None:\n    ...\n\n\nif __name__ == \"__main__\":\n    typer.run(main)\n</code></pre>"}, {"location": "typer/#print-to-stderr", "title": "Print to stderr", "text": "<p>You can print to \"standard error\" with a Rich <code>Console(stderr=True)</code></p> <pre><code>from rich.console import Console\n\nerr_console = Console(stderr=True)\nerr_console.print(\"error message\")\n</code></pre>"}, {"location": "typer/#testing", "title": "Testing", "text": "<p>Testing is similar to <code>click</code> testing, but you import the <code>CliRunner</code> directly from <code>typer</code>:</p> <pre><code>from typer.testing import CliRunner\n</code></pre>"}, {"location": "typer/#references", "title": "References", "text": "<ul> <li>Docs</li> <li>Source</li> <li>Issues</li> </ul>"}, {"location": "use_warnings/", "title": "Using warnings to evolve your package", "text": "<p>Regardless of the versioning system you're using, once you reach your first stable version, the commitment to your end users must be that you give them time to adapt to the changes in your program. So whenever you want to introduce a breaking change release it under a new interface, and in parallel, start emitting <code>DeprecationWarning</code> or <code>UserWarning</code> messages whenever someone invokes the old one. Maintain this state for a defined period (for example six months), and communicate explicitly in the warning message the timeline for when users have to migrate.</p> <p>This gives everyone time to move to the new interface without breaking their system, and then the library may remove the change and get rid of the old design chains forever. As an added benefit, only people using the old interface will ever see the warning, as opposed to affecting everyone (as seen with the semantic versioning major version bump).</p> <p>If you're following semantic versioning you'd do this change in a <code>minor</code> release, and you'll finally remove the functionality in another <code>minor</code> release. As you've given your users enough time to adequate to the new version of the code, it's not understood as a breaking change.</p> <p>This allows too for your users to be less afraid and stop upper-pinning you in their dependencies.</p> <p>Another benefit of using warnings is that if you configure your test runner to capture the warnings (which you should!) you can use your test suite to see the real impact of the deprecation, you may even realize why was that feature there and that you can't deprecate it at all.</p>"}, {"location": "use_warnings/#using-warnings", "title": "Using warnings", "text": "<p>Even though there are many warnings, I usually use <code>UserWarning</code> or <code>DeprecationWarning</code>. The full list is:</p> Class Description Warning This is the base class of all warning category classes. UserWarning The default category for warn(). DeprecationWarning Warn other developers about deprecated features. FutureWarning Warn other end users of applications about deprecated features. SyntaxWarning Warn about dubious syntactic features. RuntimeWarning Warn about dubious runtime features. PendingDeprecationWarning Warn about features that will be deprecated in the future (ignored by default). ImportWarning Warn triggered during the process of importing a module (ignored by default). UnicodeWarning Warn related to Unicode. BytesWarning Warn related to bytes and bytearray. ResourceWarning Warn related to resource usage (ignored by default)."}, {"location": "use_warnings/#how-to-raise-a-warning", "title": "How to raise a warning", "text": "<p>Warning messages are typically issued in situations where it is useful to alert the user of some condition in a program, where that condition doesn\u2019t warrant raising an exception and terminating the program.</p> <pre><code>import warnings\n\ndef f():\n    warnings.warn('Message', DeprecationWarning)\n</code></pre>"}, {"location": "use_warnings/#suppressing-a-warning", "title": "Suppressing a warning", "text": "<p>To disable in the whole file, add to the top:</p> <pre><code>import warnings\nwarnings.filterwarnings(\"ignore\", message=\"divide by zero encountered in divide\")\n</code></pre> <p>If you want this to apply to only one section of code, then use the warnings context manager:</p> <pre><code>import warnings\nwith warnings.catch_warnings():\n    warnings.filterwarnings(\"ignore\", message=\"divide by zero encountered in divide\")\n    # .. your divide-by-zero code ..\n</code></pre> <p>And if you want to disable it for the whole code base configure pytest accordingly.</p>"}, {"location": "use_warnings/#how-to-evolve-your-code", "title": "How to evolve your code", "text": "<p>To ensure that the transition is smooth you need to tweak your code so that the user can switch a flag and make sure that their code keeps on working with the new changes. For example imagine that we have a class <code>MyClass</code> with a method <code>my_method</code>.</p> <pre><code>class MyClass:\n    def my_method(self, argument):\n        # my_method code goes here\n</code></pre> <p>You can add an argument <code>deprecate_my_method</code> that defaults to <code>False</code>, or you can take the chance to change the signature of the function, so that if the user is using the old argument, it uses the old behaviour and gets the warning, and if it's using the new argument, it uses the new. The advantage of changing the signature is that you don't need to do another deprecation for the temporal argument flag.</p> <p>!!! note \"Or you can use environmental variables\"</p> <pre><code>class MyClass:\n    def __init__(self, deprecate_my_method = False):\n        self.deprecate_my_method = deprecate_my_method\n\n    def my_method(self, argument):\n        if self.deprecate_my_method:\n            # my_method new functionality\n        else:\n            warnings.warn(\"Use my_new_method instead\", UserWarning)\n            # my_method old code goes here\n</code></pre> <p>That way when users get the new version of your code, if they are not using <code>my_method</code> they won't get the exception, and if they are, they can change how they initialize their classes with <code>MyClass(deprecate_my_method=True)</code>, run their tests tweaking their code to meet the new functionality and make sure that they are ready for the method to be deprecated. Once removed, another UserWarning will be raised to stop using <code>deprecate_my_method</code> as an argument to initialize the class as it is no longer needed.</p> <p>Until you remove the old code, you need to keep both functionalities and make sure all your test suite works with both cases. To do that, create the warning, run the tests and see what tests are raising the exception. For each of them you need to think if this test will make sense with the new code:</p> <ul> <li>If it doesn't, make sure that the warning is raised.</li> <li>If it is, make sure that the warning is raised and create     another test with the <code>deprecate_my_method</code> enabled.</li> </ul> <p>Once the deprecation date arrives you'll need to search for the date in your code to see where the warning is raised and used, remove the old functionality and update the tests. If you used a temporal argument to let the users try the new behaviour, issue the warning to deprecate it.</p>"}, {"location": "use_warnings/#use-environmental-variables", "title": "Use environmental variables", "text": "<p>A cleaner way to handle it is with environmental variables, that way you don't need to change the signature of the function twice. I've learned this from boto where they informed their users this way:</p> <ul> <li>If you wish to test the new feature we have created a new environment variable     <code>BOTO_DISABLE_COMMONNAME</code>. Setting this to <code>true</code> will suppress the warning and     use the new functionality.</li> <li>If you are concerned about this change causing disruptions, you can pin your     version of <code>botocore</code> to <code>&lt;1.28.0</code> until you are ready to migrate.</li> <li> <p>If you are only concerned about silencing the warning in your logs, use     <code>warnings.filterwarnings</code> when instantiating a new service client.</p> <pre><code>import warnings\nwarnings.filterwarnings('ignore', category=FutureWarning, module='botocore.client')\n</code></pre> </li> </ul>"}, {"location": "use_warnings/#testing-warnings", "title": "Testing warnings", "text": "<p>To test the function with pytest you can use <code>pytest.warns</code>:</p> <pre><code>import warnings\nimport pytest\n\n\ndef test_warning():\n    with pytest.warns(UserWarning, match='my warning'):\n        warnings.warn(\"my warning\", UserWarning)\n</code></pre> <p>For the <code>DeprecationWarnings</code> you can use <code>deprecated_call</code>:</p> <p>Or you can use <code>deprecated</code>:</p> <pre><code>def test_myfunction_deprecated():\n    with pytest.deprecated_call():\n        f()\n</code></pre> <pre><code>@deprecated(version='1.2.0', reason=\"You should use another function\")\ndef some_old_function(x, y):\n    return x + y\n</code></pre> <p>But it adds a dependency to your program, although they don't have any downstream dependencies.</p>"}, {"location": "use_warnings/#references", "title": "References", "text": "<ul> <li>Bernat post on versioning</li> </ul>"}, {"location": "vdirsyncer/", "title": "vdirsyncer", "text": "<p>vdirsyncer is a Python command-line tool for synchronizing calendars and addressbooks between a variety of servers and the local filesystem. The most popular usecase is to synchronize a server with a local folder and use a set of other programs such as <code>khal</code> to change the local events and contacts. Vdirsyncer can then synchronize those changes back to the server.</p> <p>However, <code>vdirsyncer</code> is not limited to synchronizing between clients and servers. It can also be used to synchronize calendars and/or addressbooks between two servers directly.</p> <p>It aims to be for calendars and contacts what OfflineIMAP is for emails.</p>"}, {"location": "vdirsyncer/#installation", "title": "Installation", "text": "<p>Although it's available in the major package managers, you can get a more bleeding edge version with <code>pip</code>.</p> <pre><code>pipx install vdirsyncer\npipx inject vdirsyncer aiohttp-oauthlib\n</code></pre> <p>If you don't have <code>pipx</code> you can use <code>pip</code>.</p> <p>You also need to install some dependencies for it to work:</p> <pre><code>sudo apt-get install libxml2 libxslt1.1 zlib1g\n</code></pre>"}, {"location": "vdirsyncer/#configuration", "title": "Configuration", "text": "<p>In this example we set up contacts synchronization, but calendar sync works almost the same. Just swap <code>type = \"carddav\"</code> for <code>type = \"caldav\"</code> and <code>fileext = \".vcf\"</code> for <code>fileext = \".ics\"</code>.</p> <p>By default, <code>vdirsyncer</code> looks for its configuration file in the following locations:</p> <ul> <li>The file pointed to by the <code>VDIRSYNCER_CONFIG</code> environment variable.</li> <li><code>~/.vdirsyncer/config</code>.</li> <li><code>$XDG_CONFIG_HOME/vdirsyncer/config</code>, which is normally     <code>~/.config/vdirsyncer/config</code>.</li> </ul> <p>You need to create the directory as it's not created by default and the base config file.</p> <p>The config file should start with a general section, where the only required parameter is status_path. The following is a minimal example:</p> <pre><code>[general]\nstatus_path = \"~/.vdirsyncer/status/\"\n</code></pre> <p>After the <code>general</code> section, an arbitrary amount of pair and storage sections might come.</p> <p>In vdirsyncer, synchronization is always done between two storages. Such storages are defined in storage sections, and which pairs of storages should actually be synchronized is defined in pair section. This format is copied from OfflineIMAP, where storages are called repositories and pairs are called accounts.</p>"}, {"location": "vdirsyncer/#syncing-a-calendar", "title": "Syncing a calendar", "text": "<p>To sync to a nextcloud calendar:</p> <pre><code>[pair my_calendars]\na = \"my_calendars_local\"\nb = \"my_calendars_remote\"\ncollections = [\"from a\", \"from b\"]\nmetadata = [\"color\"]\n\n[storage my_calendars_local]\ntype = \"filesystem\"\npath = \"~/.calendars/\"\nfileext = \".ics\"\n\n[storage my_calendars_remote]\ntype = \"caldav\"\n#Can be obtained from nextcloud\nurl = \"https://yournextcloud.example.lcl/remote.php/dav/calendars/USERNAME/personal/\"\nusername = \"&lt;USERNAME&gt;\"\n#Instead of inserting my plaintext password I fetch it using pass\npassword.fetch = [\"command\", \"pass\", \"nextcloud\"]\n#SSL certificate fingerprint\nverify_fingerprint = \"FINGERPRINT\"\n#Verify ssl certificate. Set to false if it is self signed and not installed on local machine\nverify = true\n</code></pre> <p>Read the SSl and certificate validation section to see how to create the <code>verify_fingerprint</code>.</p>"}, {"location": "vdirsyncer/#syncing-an-address-book", "title": "Syncing an address book", "text": "<p>The following example synchronizes ownCloud\u2019s addressbooks to <code>~/.contacts/</code>:</p> <pre><code>[pair my_contacts]\na = \"my_contacts_local\"\nb = \"my_contacts_remote\"\ncollections = [\"from a\", \"from b\"]\n\n[storage my_contacts_local]\ntype = \"filesystem\"\npath = \"~/.contacts/\"\nfileext = \".vcf\"\n\n[storage my_contacts_remote]\ntype = \"carddav\"\n\n# We can simplify this URL here as well. In theory it shouldn't matter.\nurl = \"https://owncloud.example.com/remote.php/carddav/\"\nusername = \"bob\"\npassword = \"asdf\"\n</code></pre> <p>Note<p>Configuration for other servers can be found at Servers.</p> </p> <p>After running <code>vdirsyncer discover</code> and <code>vdirsyncer sync</code>, <code>~/.contacts/</code> will contain subdirectories for each addressbook, which in turn will contain a bunch of <code>.vcf</code> files which all contain a contact in VCARD format each. You can modify their contents, add new ones and delete some, and your changes will be synchronized to the CalDAV server after you run <code>vdirsyncer sync</code> again.</p>"}, {"location": "vdirsyncer/#conflict-resolution", "title": "Conflict resolution", "text": "<p>If the same item is changed on both sides <code>vdirsyncer</code> can manage the conflict in three ways:</p> <ul> <li>Displaying an error message (the default).</li> <li>Choosing one alternative version over the other.</li> <li>Starts a command of your choice that is supposed to merge the two alternative     versions.</li> </ul> <p>Options 2 and 3 require adding a <code>conflict_resolution</code> parameter to the pair section. Option 2 requires giving either <code>a wins</code> or <code>b wins</code> as value to the parameter:</p> <pre><code>[pair my_contacts]\n...\nconflict_resolution = \"b wins\"\n</code></pre> <p>Earlier we wrote that <code>b = \"my_contacts_remote\"</code>, so when <code>vdirsyncer</code> encounters the situation where an item changed on both sides, it will simply overwrite the local item with the one from the server.</p> <p>Option 3 requires specifying as value of <code>conflict_resolution</code> an array starting with <code>command</code> and containing paths and arguments to a command. For example:</p> <pre><code>[pair my_contacts]\n...\nconflict_resolution = [\"command\", \"vimdiff\"]\n</code></pre> <p>In this example, <code>vimdiff &lt;a&gt; &lt;b&gt;</code> will be called with <code>&lt;a&gt;</code> and <code>&lt;b&gt;</code> being two temporary files containing the conflicting files. The files need to be exactly the same when the command returns. More arguments can be passed to the command by adding more elements to the array.</p>"}, {"location": "vdirsyncer/#ssl-and-certificate-validation", "title": "SSL and certificate validation", "text": "<p>To pin the certificate by fingerprint:</p> <pre><code>[storage foo]\ntype = \"caldav\"\n...\nverify_fingerprint = \"94:FD:7A:CB:50:75:A4:69:82:0A:F8:23:DF:07:FC:69:3E:CD:90:CA\"\n#verify = false  # Optional: Disable CA validation, useful for self-signed certs\n</code></pre> <p>SHA1-, SHA256- or MD5-Fingerprints can be used.</p> <p>You can use the following command for obtaining a SHA-1 fingerprint:</p> <pre><code>echo -n | openssl s_client -connect unterwaditzer.net:443 | openssl x509 -noout -fingerprint -sha256\n</code></pre> <p>Note that <code>verify_fingerprint</code> doesn't suffice for <code>vdirsyncer</code> to work with self-signed certificates (or certificates that are not in your trust store). You most likely need to set <code>verify = false</code> as well. This disables verification of the SSL certificate\u2019s expiration time and the existence of it in your trust store, all that\u2019s verified now is the fingerprint.</p> <p>However, please consider using Let\u2019s Encrypt such that you can forget about all of that. It is easier to deploy a free certificate from them than configuring all of your clients to accept the self-signed certificate.</p>"}, {"location": "vdirsyncer/#storing-passwords", "title": "Storing passwords", "text": "<p><code>vdirsyncer</code> can fetch passwords from several sources other than the config file.</p> <p>Say you have the following configuration:</p> <pre><code>[storage foo]\ntype = \"caldav\"\nurl = ...\nusername = \"foo\"\npassword = \"bar\"\n</code></pre> <p>But it bugs you that the password is stored in cleartext in the config file. You can do this:</p> <pre><code>[storage foo]\ntype = \"caldav\"\nurl = ...\nusername = \"foo\"\npassword.fetch = [\"command\", \"~/get-password.sh\", \"more\", \"args\"]\n</code></pre> <p>You can fetch the username as well:</p> <pre><code>[storage foo]\ntype = \"caldav\"\nurl = ...\nusername.fetch = [\"command\", \"~/get-username.sh\"]\npassword.fetch = [\"command\", \"~/get-password.sh\"]\n</code></pre> <p>Or really any kind of parameter in a storage section.</p> <p>With <code>pass</code> for example, you might find yourself writing something like this in your configuration file:</p> <pre><code>password.fetch = [\"command\", \"pass\", \"caldav\"]\n</code></pre>"}, {"location": "vdirsyncer/#google", "title": "Google", "text": "<p><code>vdirsyncer</code> supports synchronization with Google calendars with the restriction that VTODO files are rejected by the server.</p> <p>Synchronization with Google contacts is less reliable due to negligence of Google\u2019s CardDAV API. Google\u2019s CardDAV implementation is allegedly a disaster in terms of data safety. Always back up your data.</p> <p>At first run you will be asked to authorize application for Google account access.</p> <p>To use this storage type, you need to install some additional dependencies:</p> <pre><code>pip install vdirsyncer[google]\n</code></pre>"}, {"location": "vdirsyncer/#official-steps", "title": "Official steps", "text": "<p>As of 2022-10-13 these didn't work for me, see the next section</p> <p>Furthermore you need to register <code>vdirsyncer</code> as an application yourself to obtain <code>client_id</code> and <code>client_secret</code>, as it is against Google\u2019s Terms of Service to hardcode those into open source software:</p> <ul> <li>Go to the Google API Manager and     create a new project under any name.</li> <li>Within that project, enable the <code>CalDAV</code> and <code>CardDAV</code> APIs (not the Calendar     and Contacts APIs, those are different and won\u2019t work). There should be     a searchbox where you can just enter those terms.</li> <li>In the sidebar, select <code>Credentials</code> and create a new <code>OAuth Client ID</code>. The application type is <code>Other</code>.</li> <li>You\u2019ll be prompted to create a OAuth consent screen first. Fill out that form     however you like.</li> <li>Finally you should have a Client ID and a Client secret. Provide these in your     storage config.</li> </ul> <p>The <code>token_file</code> parameter should be a <code>filepath</code> where vdirsyncer can later store authentication-related data. You do not need to create the file itself or write anything to it.</p> <pre><code>[storage example_for_google_calendar]\ntype = \"google_calendar\"\ntoken_file = \"...\"\nclient_id = \"...\"\nclient_secret = \"...\"\n#start_date = null\n#end_date = null\n#item_types = []\n</code></pre>"}, {"location": "vdirsyncer/#use-nekr0z-patch-solution", "title": "Use Nekr0z patch solution", "text": "<p>look the previous section if you have doubts on any of the steps</p> <p>If the official steps failed for you, try these ones:</p> <ul> <li>Go to the Google API Manager and     create a new project under any name.</li> <li>Selected the vdirsyncer project</li> <li>Went to Credentials -&gt; Create Credentials -&gt; OAuth Client ID</li> <li>Select \"Web Application\"</li> <li>Under \"Authorised redirect URIs\" added <code>http://127.0.0.1:8088</code> pressed \"Create\".</li> <li>Edit your <code>vdirsyncer</code> <code>config</code> [storage google] section to have the new     client_id and client_secret ().</li> <li> <p>Find the location of the <code>vdirsyncer/storage/google.py</code> in your environment     (mine was in     <code>~/.local/pipx/venvs/vdirsyncer/lib/python3.10/site-packages/vdirsyncer/storage</code>) and changed line 65 from</p> <pre><code>redirect_uri=\"urn:ietf:wg:oauth:2.0:oob\",\n</code></pre> <p>to</p> <pre><code>redirect_uri=\"http://127.0.0.1:8088\",\n</code></pre> </li> <li> <p>Run <code>vdirsyncer discover my_calendar</code>.</p> </li> <li>Opened the link in my browser (on my desktop machine).</li> <li> <p>Proceeded with Google authentication until \"Firefox can not connect to 127.0.0.1:8088.\" was displayed.     from the browser's address bar that looked like:</p> <p>http://127.0.0.1:8088/?state=SOMETHING&amp;code=HERECOMESTHECODE&amp;scope=https://www.googleapis.com/auth/calendar * Copy the <code>HERECOMESTHECODE</code> part. * Paste the code into the session where <code>vdirsyncer</code> was running</p> </li> </ul> <p>If the <code>redirect_ui</code> line has changed, you need to find in the code where does it start the <code>wsgi</code> server and specify the same port as you have used in the google configuration, namely <code>8088</code>.</p>"}, {"location": "vdirsyncer/#see-differences-between-syncs", "title": "See differences between syncs", "text": "<p>If you create a git repository where you have your calendars you can do a <code>git diff</code> and see the files that have changed. If you do a commit after each sync you can have all the history.</p>"}, {"location": "vdirsyncer/#references", "title": "References", "text": "<ul> <li>Docs</li> <li>Git</li> </ul>"}, {"location": "velero/", "title": "Velero", "text": "<p>Velero is an open source tool to safely backup and restore, perform disaster recovery, and migrate Kubernetes cluster resources and persistent volumes.</p>"}, {"location": "velero/#installation", "title": "Installation", "text": ""}, {"location": "velero/#client-instalation", "title": "Client instalation", "text": "<p>You interact with <code>velero</code> through a  client command line.</p> <ul> <li>Download the latest release\u2019s   tarball for your client platform.</li> <li>Extract the tarball:</li> </ul> <p><pre><code>tar -xvf &lt;RELEASE-TARBALL-NAME&gt;.tar.gz\n</code></pre> * Move the extracted velero binary to somewhere in your <code>$PATH</code>.</p>"}, {"location": "velero/#server-configuration", "title": "Server configuration", "text": "<p>Instead of configuring the server through the <code>velero</code> command line it's better to use the vmware-tanzu/velero chart.</p> <p>To configure backup policy use the <code>extraObjects</code> chart field. For example to backup everything with the next policy do:</p> <ul> <li>Hourly backups for a day.</li> <li>Daily backups for a month.</li> <li>Montly backups for a year.</li> <li>Yearly backups forever.</li> </ul> <pre><code>extraObjects:\n- apiVersion: velero.io/v1\nkind: Schedule\nmetadata:\ncreationTimestamp: null\nname: backup-1h\nnamespace: velero\nspec:\nschedule: '@every 1h'\ntemplate:\ncsiSnapshotTimeout: 0s\nhooks: {}\nincludedNamespaces:\n- '*'\nmetadata: {}\nttl: 25h0m0s\nuseOwnerReferencesInBackup: false\nstatus: {}\n- apiVersion: velero.io/v1\nkind: Schedule\nmetadata:\ncreationTimestamp: null\nname: backup-1d\nnamespace: velero\nspec:\nschedule: '@every 24h'\ntemplate:\ncsiSnapshotTimeout: 0s\nhooks: {}\nincludedNamespaces:\n- '*'\nmetadata: {}\nttl: 730h0m0s\nuseOwnerReferencesInBackup: false\nstatus: {}\n- apiVersion: velero.io/v1\nkind: Schedule\nmetadata:\ncreationTimestamp: null\nname: backup-1m\nnamespace: velero\nspec:\nschedule: '@every 730h'\ntemplate:\ncsiSnapshotTimeout: 0s\nhooks: {}\nincludedNamespaces:\n- '*'\nmetadata: {}\nttl: 8760h0m0s\nuseOwnerReferencesInBackup: false\nstatus: {}\n- apiVersion: velero.io/v1\nkind: Schedule\nmetadata:\ncreationTimestamp: null\nname: backup-1y\nnamespace: velero\nspec:\nschedule: '@every 8760h'\ntemplate:\ncsiSnapshotTimeout: 0s\nhooks: {}\nincludedNamespaces:\n- '*'\nmetadata: {}\nttl: 0s\nuseOwnerReferencesInBackup: false\nstatus: {}\n</code></pre>"}, {"location": "velero/#monitorization", "title": "Monitorization", "text": "<p>Assuming you're using prometheus you can add the next prometheus rules in the chart:</p> <pre><code># -------------------------------------------------------------\n# --   Monitor the failures when doing backups or restores   --\n# -------------------------------------------------------------\n- alert: VeleroBackupPartialFailures\nannotations:\nmessage: Velero backup {{ $labels.schedule }} has {{ $value | humanizePercentage }} partialy failed backups.\nexpr: increase(velero_backup_partial_failure_total{schedule!=\"\"}[1h]) &gt; 0\nfor: 15m\nlabels:\nseverity: warning\n- alert: VeleroBackupFailures\nannotations:\nmessage: Velero backup {{ $labels.schedule }} has {{ $value | humanizePercentage }} failed backups.\nexpr: increase(velero_backup_failure_total{schedule!=\"\"}[1h]) &gt; 0\nfor: 15m\nlabels:\nseverity: warning\n- alert: VeleroBackupSnapshotFailures\nannotations:\nmessage: Velero backup {{ $labels.schedule }} has {{ $value | humanizePercentage }} failed snapshot backups.\nexpr: increase(velero_volume_snapshot_failure_total{schedule!=\"\"}[1h]) &gt; 0\nfor: 15m\nlabels:\nseverity: warning\n- alert: VeleroRestorePartialFailures\nannotations:\nmessage: Velero restore {{ $labels.schedule }} has {{ $value | humanizePercentage }} partialy failed restores.\nexpr: increase(velero_restore_partial_failure_total{schedule!=\"\"}[1h]) &gt; 0\nfor: 15m\nlabels:\nseverity: warning\n- alert: VeleroRestoreFailures\nannotations:\nmessage: Velero restore {{ $labels.schedule }} has {{ $value | humanizePercentage }} failed restores.\nexpr: increase(velero_restore_failure_total{schedule!=\"\"}[1h]) &gt; 0\nfor: 15m\nlabels:\nseverity: warning\n\n# ------------------------------------------------\n# --   Monitor the backup rate of each policy   --\n# ------------------------------------------------\n- alert: VeleroHourlyBackupFailure\nannotations:\nmessage: There are no new hourly velero backups in the last hour.\nexpr: time() - velero_backup_last_successful_timestamp{schedule=\"backup-1h\"} &gt; 3600 for: 10m\nlabels:\nseverity: warning\n- alert: VeleroDailyBackupFailure\nannotations:\nmessage: There are no new daily velero backups in the last day.\nexpr: time() - velero_backup_last_successful_timestamp{schedule=\"backup-1d\"} &gt; 3600 * 24 for: 15m\nlabels:\nseverity: warning\n- alert: VeleroMonthlyBackupFailure\nannotations:\nmessage: There are no new montly velero backups in the last month.\nexpr: time() - velero_backup_last_successful_timestamp{schedule=\"backup-1m\"} &gt; 3600 * 24 * 30\nfor: 15m\nlabels:\nseverity: warning\n- alert: VeleroYearlyBackupFailure\nannotations:\nmessage: There are no new yearly velero backups in the last year.\nexpr: time() - velero_backup_last_successful_timestamp{schedule=\"backup-1y\"} &gt; 3600 * 24 * 365\nfor: 15m\nlabels:\nseverity: warning\n\n# -------------------------------------\n# --   Monitor the backup cleaning   --\n# -------------------------------------\n- alert: VeleroBackupCleaningFailure\nannotations:\nmessage: There are more backups than the expected, deletion policy may not be working well.\n# Desired backups:\n# - 25 hourlys\n# - 31 dailys\n# - 12 monthlys\n# - 10 yearlys\nexpr: velero_backup_total &gt; (25 + 31 + 12 + 10)\nfor: 15m\nlabels:\nseverity: warning\n</code></pre>"}, {"location": "velero/#usage", "title": "Usage", "text": ""}, {"location": "velero/#restore-backups", "title": "Restore backups", "text": "<p>To get the available backups you can use:</p> <pre><code>velero get backups\n</code></pre> <p>Imagine we want to restore the backup <code>backup-1h-20230109125511</code>.</p> <p>We first need to update your backup storage location to read-only mode. This prevents backup objects from being created or deleted in the backup storage location during the restore process.</p> <pre><code>kubectl patch backupstoragelocation default \\\n--namespace velero \\\n--type merge \\\n--patch '{\"spec\":{\"accessMode\":\"ReadOnly\"}}'\n</code></pre> <p>Where <code>default</code> is the only backup storage location I have, check what's the name of your's by running <code>kubectl get backupstoragelocation -n velero</code>.</p> <p>You'd now run the <code>velero restore create</code> commands (we'll see more examples in next sections). For example:</p> <ul> <li>Create a restore with a default name <code>backup-1-&lt;timestamp&gt;</code> from backup <code>backup-1</code>.</li> </ul> <pre><code>velero restore create --from-backup backup-1\n</code></pre> <ul> <li>Create a restore with name <code>restore-1</code> from a backup called <code>backup-1</code> .</li> </ul> <pre><code>velero restore create restore-1 --from-backup backup-1\n</code></pre> <ul> <li>Create a restore from the latest successful backup triggered by schedule <code>schedule-1</code>.</li> </ul> <pre><code>velero restore create --from-schedule schedule-1\n</code></pre> <p>When you run the command they'll will show you how to monitor the evolution of the restore with commands similar to:</p> <pre><code>velero restore describe backup-1h-20230109125511-20230110145314\nvelero restore logs backup-1h-20230109125511-20230110145314\n</code></pre> <p>Once they're done, remember to reset the ReadWrite permissions on the backup location.</p> <pre><code>kubectl patch backupstoragelocation default \\\n--namespace velero \\\n--type merge \\\n--patch '{\"spec\":{\"accessMode\":\"ReadWrite\"}}'\n</code></pre>"}, {"location": "velero/#overwrite-existing-resources", "title": "Overwrite existing resources", "text": "<p>The only way I've found to do this is by removing the namespace and then restore it with <code>velero</code>. If you want to try to do it in a cleaner way keep on reading (although for me it didn't work!).</p> <p>By default, Velero is configured to be non-destructive during a restore. This means that it will never overwrite data that already exists in your cluster. When Velero attempts to create a resource during a restore, the resource being restored is compared to the existing resources on the target cluster by the Kubernetes API Server. If the resource already exists in the target cluster, Velero skips restoring the current resource and moves onto the next resource to restore, without making any changes to the target cluster.</p> <p>You can change this policy for a restore by using the <code>--existing-resource-policy</code> restore flag. The available options are <code>none</code> (default) and <code>update</code>. If you choose to update existing resources during a restore (<code>--existing-resource-policy=update</code>), Velero will attempt to update an existing resource to match the resource being restored:</p> <ul> <li> <p>If the existing resource in the target cluster is the same as the resource Velero is attempting to restore, Velero will add a <code>velero.io/backup-name</code> label with the backup name and a <code>velero.io/restore-name</code> label with the restore name to the existing resource. If patching the labels fails, Velero adds a restore error and continues restoring the next resource.</p> </li> <li> <p>If the existing resource in the target cluster is different from the backup, Velero will first try to patch the existing resource to match the backup resource. If the patch is successful, Velero will add a <code>velero.io/backup-name</code> label with the backup name and a <code>velero.io/restore-name</code> label with the restore name to the existing resource. If the patch fails, Velero adds a restore warning and tries to add the <code>velero.io/backup-name</code> and <code>velero.io/restore-name</code> labels on the resource. If the labels patch also fails, then Velero logs a restore error and continues restoring the next resource.</p> </li> </ul> <p>Even with these flags the restore of PersistentVolumeClaim doesn't work.</p>"}, {"location": "velero/#restore-only-a-namespace", "title": "Restore only a namespace", "text": "<pre><code>velero restore create --include-namespaces monitoring\n</code></pre>"}, {"location": "velero/#restore-only-a-subsection-of-the-backup", "title": "Restore only a subsection of the backup", "text": "<p>You can list the backed up resources with:</p> <pre><code>velero describe backups backup-1h-20230109125511\n</code></pre> <p>Then you can create a restore for only <code>persistentvolumeclaims</code> and <code>persistentvolumes</code> within a backup.</p> <pre><code>velero restore create --from-backup backup-2 --include-resources persistentvolumeclaims,persistentvolumes\n</code></pre>"}, {"location": "velero/#restore-from-a-snapshot-not-done-by-velero", "title": "Restore from a snapshot not done by velero", "text": "<p>If you want to use an EBS snapshot that is not managed by <code>velero</code> you need to:</p> <ul> <li>Locate a <code>velero</code> backup which has done a backup of the resources you want to restore. Imagine it's <code>backup-1</code>. </li> <li>Go to the <code>backup-1</code> directory in the S3 bucket where velero stores the data.</li> <li>Download and decompress the <code>backup-1-volumesnapshots.json.gz</code> file.</li> <li>Edit the result <code>json</code> and restore the <code>snap-.*</code> strings of the <code>pvc</code> that you want to restore for the ones that are not managed by velero.</li> <li>Compress the file and upload it to the S3 bucket.</li> <li>Restore the backup</li> </ul> <p>Keep in mind that if you are trying to restore a backup created by an EBS lifecycle hook you'll receive an error when restoring because these snapshots have a tag that starts with <code>aws:</code> which is reserved for AWS only. The solution is to copy the snapshot into a new one, assign a tag, for example <code>Name</code>, and use that snapshot instead. If you don't define any tag you'll get another error :/.</p>"}, {"location": "velero/#overview-of-velero", "title": "Overview of Velero", "text": "<p>Each Velero operation \u2013 on-demand backup, scheduled backup, restore \u2013 is a custom resource, defined with a Kubernetes Custom Resource Definition (CRD) and stored in etcd. Velero also includes controllers that process the custom resources to perform backups, restores, and all related operations.</p> <p>You can back up or restore all objects in your cluster, or you can filter objects by type, namespace, and/or label.</p>"}, {"location": "velero/#backups", "title": "Backups", "text": ""}, {"location": "velero/#on-demand-backups", "title": "On demand backups", "text": "<p>The backup operation:</p> <ul> <li>Uploads a tarball of copied Kubernetes objects into cloud object storage.</li> <li>Calls the cloud provider API to make disk snapshots of persistent volumes, if specified.</li> </ul> <p>You can optionally specify backup hooks to be executed during the backup. For example, you might need to tell a database to flush its in-memory buffers to disk before taking a snapshot. </p> <p>Note that cluster backups are not strictly atomic. If Kubernetes objects are being created or edited at the time of backup, they might not be included in the backup. The odds of capturing inconsistent information are low, but it is possible.</p>"}, {"location": "velero/#scheduled-backups", "title": "Scheduled backups", "text": "<p>The schedule operation allows you to back up your data at recurring intervals. You can create a scheduled backup at any time, and the first backup is then performed at the schedule\u2019s specified interval. These intervals are specified by a Cron expression.</p> <p>Velero saves backups created from a schedule with the name <code>&lt;SCHEDULE NAME&gt;-&lt;TIMESTAMP&gt;</code>, where <code>&lt;TIMESTAMP&gt;</code> is formatted as <code>YYYYMMDDhhmmss</code>.</p>"}, {"location": "velero/#backup-workflow", "title": "Backup workflow", "text": "<p>When you run <code>velero backup create test-backup</code>:</p> <ul> <li>The Velero client makes a call to the Kubernetes API server to create a <code>Backup</code> object.</li> <li>The <code>BackupController</code> notices the new <code>Backup</code> object and performs validation.</li> <li>The <code>BackupController</code> begins the backup process. It collects the data to back up by querying the API server for resources.</li> <li>The <code>BackupController</code> makes a call to the object storage service \u2013 for example, AWS S3 \u2013 to upload the backup file.</li> </ul>"}, {"location": "velero/#set-expiration-date-of-a-backup", "title": "Set expiration date of a backup", "text": "<p>When you create a backup, you can specify a TTL (time to live) by adding the flag <code>--ttl &lt;DURATION&gt;</code>. If Velero sees that an existing backup resource is expired, it removes:</p> <ul> <li>The backup resource</li> <li>The backup file from cloud object storage</li> <li>All <code>PersistentVolume</code> snapshots</li> <li>All associated Restores</li> </ul> <p>The TTL flag allows the user to specify the backup retention period with the value specified in hours, minutes and seconds in the form <code>--ttl 24h0m0s</code>. If not specified, a default TTL value of 30 days will be applied.</p> <p>If backup fails to delete, a label <code>velero.io/gc-failure=&lt;Reason&gt;</code> will be added to the backup custom resource.</p> <p>You can use this label to filter and select backups that failed to delete.</p>"}, {"location": "velero/#restores", "title": "Restores", "text": "<p>The restore operation allows you to restore all of the objects and persistent volumes from a previously created backup. You can also restore only a filtered subset of objects and persistent volumes. </p> <p>By default, backup storage locations are created in read-write mode. However, during a restore, you can configure a backup storage location to be in read-only mode, which disables backup creation and deletion for the storage location. This is useful to ensure that no backups are inadvertently created or deleted during a restore scenario.</p> <p>A restored object includes a label with key velero.io/restore-name and value . <p>You can optionally specify restore hooks to be executed during a restore or after resources are restored. For example, you might need to perform a custom database restore operation before the database application containers start.</p>"}, {"location": "velero/#restore-workflow", "title": "Restore workflow", "text": "<p>When you run <code>velero restore create</code>:</p> <ul> <li>The Velero client makes a call to the Kubernetes API server to create a <code>Restore</code> object.</li> <li>The <code>RestoreController</code> notices the new <code>Restore</code> object and performs validation.</li> <li>The <code>RestoreController</code> fetches the backup information from the object storage service. It then runs some preprocessing on the backed up resources to make sure the resources will work on the new cluster. For example, using the backed-up API versions to verify that the restore resource will work on the target cluster.</li> <li>The <code>RestoreController</code> starts the restore process, restoring each eligible resource one at a time.</li> </ul> <p>By default, Velero performs a non-destructive restore, meaning that it won\u2019t delete any data on the target cluster. If a resource in the backup already exists in the target cluster, Velero will skip that resource. You can configure Velero to use an update policy instead using the <code>--existing-resource-policy</code> restore flag. When this flag is set to update, Velero will attempt to update an existing resource in the target cluster to match the resource from the backup.</p>"}, {"location": "velero/#references", "title": "References", "text": "<ul> <li>Docs</li> <li>Source</li> <li>Home</li> </ul>"}, {"location": "versioning/", "title": "Program versioning", "text": "<p>The Don't Repeat Yourself principle encourages developers to abstract code into a separate components and reuse them rather than write it over and over again. If this happens across the system, the best practice is to put it inside a package that lives on its own (a library) and then pull it in from the applications when required.</p> <p>!!! note \"This article is heavily based on the posts in the references sections, the main credit goes to them, I've just refactored all together under my personal opinion.\"</p> <p>As most of us can\u2019t think of every feature that the library might offer or what bugs it might contain, these packages tend to evolve. Therefore, we need some mechanism to encode these evolutions of the library, so that downstream users can understand how big the change is. Most commonly, developers use three methods:</p> <ul> <li>A version number.</li> <li>A changelog.</li> <li>The git history.</li> </ul> <p>The version change is used as a concise way for the project to communicate this evolution, and it's what we're going to analyze in this article. However, encoding all the information of a change into a number switch has proven to be far from perfect.</p> <p>That's why keeping a good and detailed changelog makes a lot of sense, as it will better transmit that intent, and what will be the impact of upgrading. Once again, this falls into the same problem as before, while a change log is more descriptive, it still only tells you what changes (or breakages) a project intended to make, it doesn\u2019t go into any detail about unintended consequences of changes made. Ultimately, a change log\u2019s accuracy is no different than that of the version itself, it's just (hopefully!) more detailed.</p> <p>Fundamentally, any indicator of change that isn\u2019t a full diff is just a lossy encoding of that change. You can't expect though to read all the diffs of the libraries that you use, that's why version numbers and changelogs make a lot of sense. We just need to be aware of the limits of each system.</p> <p>That being said, you'll use version numbers in two ways:</p> <ul> <li> <p>As a producer of applications and libraries where you\u2019ll have to decide what     versioning system to use.</p> </li> <li> <p>As a consumer of dependencies, you\u2019ll have to express what versions of a given library your     application/library is compatible.</p> </li> </ul>"}, {"location": "versioning/#deciding-what-version-system-to-use-for-your-programs", "title": "Deciding what version system to use for your programs", "text": "<p>The two most popular versioning systems are:</p> <ul> <li> <p>Semantic Versioning: A way to define your program's     version based on the type of changes you've introduced.</p> </li> <li> <p>Calendar Versioning: A versioning convention based     on your project's release calendar, instead of arbitrary numbers.</p> </li> </ul> <p>Each has it's advantages and disadvantages. From a consumer perspective, I think that projects should generally default to SemVer-ish, following the spirit of the documentation rather than the letter of the specification because:</p> <ul> <li>Your version number becomes a means of communicating your changes intents to     your end users.</li> <li>If you use the semantic versioning commit message     guidelines, you are more     likely to have a useful git history and can automatically maintain the     project's changelog.</li> </ul> <p>There are however, corner cases where CalVer makes more sense:</p> <ul> <li> <p>You\u2019re tracking something that is already versioned using dates or for which     the version number can only really be described as a point in time release.     The <code>pytz</code> is a good example of both of these cases, the Olson TZ database is     versioned using a date based scheme and the information that it is providing     is best represented as a snapshot of the state of what timezones were like     at a particular point in time.</p> </li> <li> <p>Your project is going to be breaking compatibility in every release and you do     not want to make any promises of compatibility. You should still document     this fact in your README, but if there\u2019s no promise of compatibility between     releases, then there\u2019s no information to be communicated in the version     number.</p> </li> <li> <p>Your project is never going to intentionally breaking compatibility in     a release, and you strive to always maintain compatibility. Projects can     always just use the latest version of your software. Your changes will     only ever be additive, and if you need to change your API, you\u2019ll do     something like leave the old API intact, and add a new API with the new     semantics. An example of this case would be the Ubuntu versions.</p> </li> </ul>"}, {"location": "versioning/#how-to-evolve-your-code-version", "title": "How to evolve your code version", "text": "<p>Assuming you're using Semantic Versioning you can improve your code evolution by:</p> <ul> <li>Avoid becoming a ZeroVer package.</li> <li>Use Warnings to avoid major changes</li> </ul>"}, {"location": "versioning/#avoid-becoming-a-zerover-package", "title": "Avoid becoming a ZeroVer package", "text": "<p>Once your project reach it's first level of maturity you should release <code>1.0.0</code> to avoid falling into ZeroVer. For example you can use one of the next indicators:</p> <ul> <li>If you're frequently using it and haven't done any breaking change in 3 months.</li> <li>If 30 users are depending on it. For example counting the project stars.</li> </ul>"}, {"location": "versioning/#use-warnings-to-avoid-major-changes", "title": "Use Warnings to avoid major changes", "text": "<p>Semantic versioning uses the major version to defend against breaking changes, and at the same offers maintainers the freedom to evolve the library without breaking users. Nevertheless, this does not seem to work that well.</p> <p>So it's better to use Warnings to avoid major changes.</p>"}, {"location": "versioning/#communicate-with-your-users", "title": "Communicate with your users", "text": "<p>You should warn your users not to blindly trust that any version change is not going to break their code and that you assume that they are actively testing the package updates.</p>"}, {"location": "versioning/#keep-the-requires-python-metadata-updated", "title": "Keep the <code>Requires-Python</code> metadata updated", "text": "<p>It's important not to upper cap the Python version and to maintain the <code>Requires-Python</code> package metadata updated. Dependency solvers will use this information to fetch the correct versions of the packages for the users.</p>"}, {"location": "versioning/#deciding-how-to-manage-the-versions-of-your-dependencies", "title": "Deciding how to manage the versions of your dependencies", "text": "<p>As a consumer of other dependencies, you need to specify in your package what versions does your code support. The traditional way to do it is by pinning those versions in your package definition. For example in python it lives either in the <code>setup.py</code> or in the <code>pyproject.toml</code>.</p>"}, {"location": "versioning/#lower-version-pinning", "title": "Lower version pinning", "text": "<p>When you're developing a program that uses a dependency, you usually don't know if a previous version of that dependency is compatible with your code, so in theory it makes sense to specify that you don't support any version smaller than the actual with something like <code>&gt;=1.2</code>. If you follow this train of thought, each time you update your dependencies, you should update your lower pins, because you're only running your test suite on those versions. If the libraries didn't do upper version pinning, then there would be no problem as you wouldn't be risking to get into version conflicts.</p> <p>A more relaxed approach would be not to update the pins when you update, in that case, you should run your tests both against the oldest possible values and the newest to ensure that everything works as expected. This way you'll be more kind to your users as you'll reduce possible version conflicts, but it'll add work to the maintainers.</p> <p>The most relaxed approach would be not to use pins at all, it will suppress most of the version conflicts but you won't be sure that the dependencies that your users are using are compatible with your code.</p> <p>Think about how much work you want to invest in maintaining your package and how much stability you want to offer before you choose one or the other method. Once you've made your choice, it would be nice if you communicate it to your users through your documentation.</p>"}, {"location": "versioning/#upper-version-pinning", "title": "Upper version pinning", "text": "<p>Program maintainers often rely on upper version pinning to guarantee that their code is not going to be broken due to a dependency update.</p> <p>We\u2019ll cover the valid use cases for capping after this section. But, just to be clear, if you know you do not support a new release of a library, then absolutely, go ahead and cap it as soon as you know this to be true. If something does not work, you should cap (or maybe restrict a single version if the upstream library has a temporary bug rather than a design direction that\u2019s causing the failure). You should also do as much as you can to quickly remove the cap, as all the downsides of capping in the next sections still apply.</p> <p>The following will assume you are capping before knowing that something does not work, but just out of general principle, like Poetry recommends and defaults to with <code>poetry add</code>. In most cases, the answer will be don\u2019t. For simplicity, I will also assume you are being tempted to cap to major releases (<code>^1.0.0</code> in Poetry or <code>~=1.0</code> in all other tooling that follows Python standards via PEP 440) following the false security that only <code>major</code> changes can to break your code. If you cap to minor versions <code>~=1.0.0</code>, this is much worse, and the arguments below apply even more strongly.</p>"}, {"location": "versioning/#version-limits-break-code-too", "title": "Version limits break code too", "text": "<p>Following this path will effectively opt you out of bug fixes and security updates, as most of the projects only maintain the latest version of their program and what worse, you'll be preventing everyone using your library not to use the latest version of those libraries. All in exchange to defend yourself against a change that in practice will rarely impact you. Sure, you can move on to the next version of each of your pins each time they increase a major via something like <code>Click&gt;=8, &lt;9</code>. However, this involves manual intervention on their code, and you might not have the time to do this for every one of your projects.</p> <p>If we add the fact that not only major but any other version change may break your code due to unintended changes and the difference in the change categorization, then you can treat all changes equally, so it makes no sense on pinning the major version either.</p> <p>This is specially useless when you add dependencies that follow CalVer. <code>poetry add</code> packaging will still do <code>^21</code> for the version it adds. You shouldn\u2019t be capping versions, but you really shouldn\u2019t be capping CalVer.</p>"}, {"location": "versioning/#semver-never-promises-to-break-your-code", "title": "SemVer never promises to break your code", "text": "<p>A really easy but incorrect generalization of the SemVer rules is \u201ca major version will break my code\u201d. Even if the library follows true SemVer perfectly, a major version bump does not promise to break downstream code. It promises that some downstream code may break. If you use <code>pytest</code> to test your code, for example, the next major version will be very unlikely to break. If you write a <code>pytest</code> extension, however, then the chances of something breaking are much higher (but not 100%, maybe not even 50%). Quite ironically, the better a package follows SemVer, the smaller the change will trigger a major version, and therefore the less likely a major version will break a particular downstream code.</p> <p>As a general rule, if you have a reasonably stable dependency, and you only use the documented API, especially if your usage is pretty light/general, then a major update is extremely unlikely to break your code. It\u2019s quite rare for light usage of a library to break on a major update. It can happen, of course, but is unlikely. If you are using something very heavily, if you are working on a framework extension, or if you use internals that are not publicly documented, then your chances of breaking on a major release are much higher. Python has a culture of producing <code>FutureWarnings</code>, <code>DeprecationWarnings</code>, or <code>PendingDeprecationWarnings</code> (make sure they are on in your testing, and turn into errors), good libraries will use them.</p>"}, {"location": "versioning/#version-conflicts", "title": "Version conflicts", "text": "<p>And then there\u2019s another aspect version pinning will introduce: version conflicts.</p> <p>An application or library will have a set of libraries it depends on directly. These are libraries you\u2019re directly importing within the application/library you\u2019re maintaining, but then the libraries themselves may rely on other libraries. This is known as a transitive dependency. Very soon, you\u2019ll get to a point where two different components use the same library, and both of them might express version constraints on it.</p> <p>For example, consider the case of <code>tenacity</code>: a general-purpose retrying library. Imagine you were using this in your application, and being a religious follower of semantic versioning, you\u2019ve pinned it to the version that was out when you created the app in early 2018: <code>4.11</code>. The constraint would specify version <code>4.11</code> or later, but less than the next major version <code>5</code>.</p> <p>At the same time, you also connect to an HTTP service. This connection is handled by another library, and the maintainer of that decided to also use <code>tenacity</code> to offer automatic retry functionality. They pinned it similarly following the semantic versioning convention. Back in 2018, this caused no issues. But then August comes, and version <code>5.0</code> is released.</p> <p>The service and its library maintainers have a lot more time on their hands (perhaps because they are paid to do so), so they quickly move to version <code>5.0</code>. Or perhaps they want to use a feature from the new major version. Now they introduce the pin greater than five but less than six on tenacity. Their public interface does not change at all at this point, so they do not bump their major version. It\u2019s just a patch release.</p> <p>Python can only have one version of a library installed at a given time. At this point, there is a version conflict. You\u2019re requesting a version between four and five, while the service library is requesting a version between five and six. Both constraints cannot be satisfied.</p> <p>If you use a version of pip older than <code>20.2</code> (the release in which it added a dependency resolver) it will just install a version matching the first constraint it finds and ignore any subsequent constraints. Versions of pip after <code>20.2</code> would fail with an error indicating that the constraint cannot be satisfied.</p> <p>Either way, your application no longer works. The only way to make it work is to either pin the service library down to the last working patch number, or upgrade your version pinning of <code>tenacity</code>. This is generating extra work for you with minimal benefit. Often it might not be even possible to use two conflicting libraries until one of them relaxes their requirements. It also means you must support a wide version range; ironically. If you update to requiring `tenacity</p> <p>5<code>, your update can\u2019t be installed with another library still on</code>4.11<code>. So you have to support</code>tenacity&gt;=4.11,&lt;6` for a while until most libraries have similarly updated.</p> <p>And for those who might think this doesn\u2019t happen often, let me say that <code>tenacity</code> released another major version a year later in November 2019. Thus, the cycle starts all over again. In both cases, your code most likely did not need to change at all, as just a small part of their public API changed. In my experience, this happens a lot more often than when a major version bump breaks you. I've found myself investing most of my project maintenance time opening issues in third party dependencies to update their pins.</p>"}, {"location": "versioning/#it-doesnt-scale", "title": "It doesn\u2019t scale", "text": "<p>If you have a single library that doesn\u2019t play well, then you probably will get a working solve easily (this is one reason that this practice doesn\u2019t seem so bad at first). If more packages start following this tight capping, however, you end up with a situation where things simply cannot solve. A moderately sized application can have a hundred or more dependencies when expanded, so such issues in my experience start to appear every few months. You need only 5-6 of such cases for every 100 libraries for this issue to pop up every two months on your plate. And potentially for a multiple of your applications.</p> <p>The entire point of packaging is to allow you to get lots of packages that each do some job for you. We should be trying to make it easy to be able to add dependencies, not harder.</p> <p>The implication of this is you should be very careful when you see tight requirements in packages and you have any upper bound caps anywhere in the dependency chain. If something caps dependencies, there\u2019s a very good chance adding two such packages will break your solve, so you should pick just one, or just avoid them altogether, so you can add one in the future. This is a good rule, actually: Never add a library to your dependencies that has excessive upper bound capping. When I have failed to follow this rule for a larger package, I have usually come to regret it.</p> <p>If you are doing the capping and are providing a library, you now have a commitment to quickly release an update, ideally right before any capped dependency comes out with a new version. Though if you cap, how to you install development versions or even know when a major version is released? This makes it harder for downstream packages to update, because they have to wait for all the caps to be moved for all upstream.</p>"}, {"location": "versioning/#it-conflicts-with-tight-lower-bounds", "title": "It conflicts with tight lower bounds", "text": "<p>A tight lower bound is only bad if packages cap upper bounds. If you can avoid upper-cap packages, you can accept tight lower bound packages, which are much better; better features, better security, better compatibility with new hardware and OS\u2019s. A good packaging system should allow you to require modern packages; why develop for really old versions of things if the packaging system can upgrade them? But a upper bound cap breaks this. Hopefully anyone who is writing software and pushing versions will agree that tight lower limits are much better than tight upper limits, so if one has to go, it\u2019s the upper limits.</p> <p>It is also rather rare that packages solve for lower bounds in CI (I would love to see such a solver become an option, by the way!), so setting a tight lower bound is one way to avoid rare errors when old packages are cached that you don\u2019t actually support. CI almost never has a cache of old packages, but users do.</p>"}, {"location": "versioning/#capping-dependencies-hides-incompatibilities", "title": "Capping dependencies hides incompatibilities", "text": "<p>Another serious side effect of capping dependencies is that you are not notified properly of incoming incompatibilities, and you have to be extra proactive in monitoring your dependencies for updates. If you don\u2019t cap your dependencies, you are immediately notified when a dependency releases a new version, probably by your CI, the first time you build with that new version. If you are running your CI with the <code>--dev</code> flag on your <code>pip install</code> (uncommon, but probably a good idea), then you might even catch and fix the issue before a release is even made. If you don\u2019t do this, however, then you don\u2019t know about the incompatibility until (much) later.</p> <p>If you are not following all of your dependencies, you might not notice that you are out of date until it\u2019s both a serious problem for users and it\u2019s really hard for you to tell what change broke your usage because several versions have been released. While I\u2019m not a huge fan of Google\u2019s live-at-head philosophy (primarily because it has heavy requirements not applicable for most open-source projects), I appreciate and love catching a dependency incompatibility as soon as you possibly can; the smaller the change set, the easier it is to identify and fix the issue.</p>"}, {"location": "versioning/#capping-all-dependencies-hides-real-incompatibilities", "title": "Capping all dependencies hides real incompatibilities", "text": "<p>If you see <code>X&gt;=1.1</code>, that tells you that the package is using features from <code>1.1</code> and do not support <code>1.0</code>. If you see <code>X&lt;1.2</code>, this should tell you that there\u2019s a problem with <code>1.2</code> and the current software, specifically something they know the dependency will not fix/revert. Not that you just capped all your dependencies and have no idea if that will or won\u2019t work at all. A cap should be like a TODO; it\u2019s a known issue that needs to be worked on soon. As in yesterday.</p>"}, {"location": "versioning/#pinning-the-python-version-is-special", "title": "Pinning the Python version is special", "text": "<p>Another practice pushed by Poetry is adding an upper cap to the Python version. This is misusing a feature designed to help with dropping old Python versions to instead stop new Python versions from being used. \u201cScrolling back\u201d through older releases to find the newest version that does not restrict the version of Python being used is exactly the wrong behavior for an upper cap, and that is what the purpose of this field is. Current versions of pip do seem to fail when this is capped, rather than scrolling back to find an older uncapped version, but I haven\u2019t found many libraries that have \u201cadded\u201d this after releasing to be sure of that.</p> <p>To be clear, this is very different from a library: specifically, you can\u2019t downgrade your Python version if this is capped to something below your current version. You can only fail. So this does not \u201cfix\u201d something by getting an older, working version, it only causes hard failures if it works the way you might hope it does. This means instead of seeing the real failure and possibly helping to fix it, users just see a <code>Python doesn\u2019t match</code> error. And, most of the time, it\u2019s not even a real error; if you support Python <code>3.x</code> without warnings, you should support Python <code>3.x+1</code> (and <code>3.x+2</code>, too).</p> <p>Capping to <code>&lt;4</code> (something like <code>^3.6</code> in Poetry) is also directly in conflict with the Python developer\u2019s own statements; they promise the <code>3-&gt;4</code> transition will be more like the <code>1-&gt;2</code> transition than the <code>2-&gt;3</code> transition. When Python 4 does come out, it will be really hard to even run your CI on 4 until all your dependencies uncap. And you won\u2019t actually see the real failures, you\u2019ll just see incompatibility errors, so you won\u2019t even know what to report to those libraries. And this practice makes it hard to test development versions of Python.</p> <p>And, if you use Poetry, as soon as someone caps the Python version, every Poetry project that uses it must also cap, even if you believe it is a detestable practice and confusing to users. It is also wrong unless you fully pin the dependency that forced the cap. If the dependency drops it in a patch release or something else you support, you no longer would need the cap.</p>"}, {"location": "versioning/#applications-are-slightly-different", "title": "Applications are slightly different", "text": "<p>If you have a true application (that is, if you are not intending your package to be used as a library), upper version constraints are much less problematic, and some of the reasons above don't apply. This due to two reasons.</p> <p>First, if you are writing a library, your \u201cusers\u201d are specifying your package in their dependencies; if an update breaks them, they can always add the necessary exclusion or cap for you to help end users. It\u2019s a leaky abstraction, they shouldn\u2019t have to care about what your dependencies are, but when capping interferes with what they can use, that\u2019s also a leaky and unfixable abstraction. For an application, the \u201cusers\u201d are more likely to be installing your package directly, where the users are generally other developers adding to requirements for libraries.</p> <p>Second, for an app that is installed from PyPI, you are less likely to have to worry about what else is installed (the other issues are still true). Many (most?) users will not be using <code>pipx</code> or a fresh virtual environment each time, so in practice, you\u2019ll still run into problems with tight constraints, but there is a workaround (use <code>pipx</code>, for example). You still are still affected by most of the arguments above, though, so personally I\u2019d still not recommend adding untested caps.</p>"}, {"location": "versioning/#when-is-it-ok-to-set-an-upper-limit", "title": "When is it ok to set an upper limit?", "text": "<p>Valid reasons to add an upper limit are:</p> <ul> <li> <p>If a dependency is known to be broken, block out the broken version. Try very     hard to fix this problem quickly, then remove the block if it\u2019s fixable on     your end. If the fix happens upstream, excluding just the broken version is     fine (or they can \u201cyank\u201d the bad release to help everyone).</p> </li> <li> <p>If you know upstream is about to make a major change that is very likely to     break your usage, you can cap. But try to fix this as quickly as possible so     you can remove the cap by the time they release. Possibly add development     branch/release testing until this is resolved.</p> </li> <li> <p>If upstream asks users to cap, then I still don\u2019t like it, but it is okay if     you want to follow the upstream recommendation. You should ask yourself: do     you want to use a library that may intentionally break you and require     changes on your part without help via deprecation periods? A one-time major     rewrite might be an acceptable reason. Also, if you are upstream, it is very     un-Pythonic to break users without deprecation warnings first. Don\u2019t do it     if possible.</p> </li> <li> <p>If you are writing an extension for an ecosystem/framework (pytest extension,     Sphinx extension, Jupyter extension, etc), then capping on the major version     of that library is acceptable. Note this happens once - you have a single     library that can be capped. You must release as soon as you possibly can     after a new major release, and you should be closely following upstream</p> <ul> <li>probably using development releases for testing, etc. But doing this for one library is probably manageable.</li> </ul> </li> <li> <p>You are releasing two or more libraries in sync with each other. You control     the release cadence for both libraries. This is likely the \u201cbest\u201d reason to     cap. Some of the above issues don\u2019t apply in this case - since you control     the release cadence and can keep them in sync.</p> </li> <li> <p>You depend on private internal details of a library. You should also rethink     your choices - this can be broken in a minor or patch release, and often is.</p> </li> </ul> <p>If you cap in these situations, I wouldn\u2019t complain, but I wouldn\u2019t really recommend it either:</p> <ul> <li> <p>If you have a heavy dependency on a library, maybe cap. A really large API     surface is more likely to be hit by the possible breakage.</p> </li> <li> <p>If a library is very new, say on version 1 or a ZeroVer library, and has very     few users, maybe cap if it seems rather unstable. See if the library authors     recommend capping - they might plan to make a large change if it\u2019s early in     development. This is not blanket permission to cap ZeroVer libraries!</p> </li> <li> <p>If a library looks really unstable, such as having a history of making big     changes, then cap. Or use a different library. Even better, contact the     authors, and make sure that your usage is safe for the near future.</p> </li> </ul>"}, {"location": "versioning/#summary", "title": "Summary", "text": "<p>No more than 1-2 of your dependencies should fall into the categories of acceptable upper pinning. In every other case, do not cap your dependences, specially if you are writing a library! You could probably summarize it like this: if there\u2019s a high chance (say <code>75%+</code>) that a dependency will break for you when it updates, you can add a cap. But if there\u2019s no reason to believe it will break, do not add the cap; you will cause more severe (unfixable) pain than the breakage would.</p> <p>If you have an app instead of a library, you can be cautiously more relaxed, but not much. Apps do not have to live in shared environments, though they might.</p> <p>Notice many of the above instances are due to very close/special interaction with a small number of libraries (either a plugin for a framework, synchronized releases, or very heavy usage). Most libraries you use do not fall into this category. Remember, library authors don\u2019t want to break users who follow their public API and documentation. If they do, it\u2019s for a special and good reason (or it is a bad library to depend on). They will probably have a deprecation period, produce warnings, etc.</p> <p>If you do version cap anything, you are promising to closely follow that dependency, update the cap as soon as possible, follow beta or RC releases or the development branch, etc. When a new version of a library comes out, end users should be able to start trying it out. If they can\u2019t, your library\u2019s dependencies are a leaky abstraction (users shouldn\u2019t have to care about what dependencies libraries use).</p>"}, {"location": "versioning/#automatically-upgrade-and-test-your-dependencies", "title": "Automatically upgrade and test your dependencies", "text": "<p>Now that you have minimized the upper bound pins and defined the lower bound pins you need to ensure that your code works with the latest version of your dependencies.</p> <p>One way to do it is running a periodic cronjob (daily probably) that updates your requirements lock, optionally your lower bounds, and checks that the tests keep on passing.</p>"}, {"location": "versioning/#monitor-your-dependencies-evolution", "title": "Monitor your dependencies evolution", "text": "<p>You rely on your dependencies to fulfill critical parts of your package, therefore it makes sense to know how they are changing in order to:</p> <ul> <li>Change your package to use new features.</li> <li>Be aware of the new possibilities to solve future problems.</li> <li>Get an idea of the dependency stability and future.</li> </ul> <p>Depending on how much you rely on the dependency, different levels of monitorization can be used, ordered from least to most you could check:</p> <ul> <li> <p>Release messages: Some projects post them in their blogs, you can use     their RSS feed to keep updated. If the project uses Github to create the     release messages, you can get notifications on just those release messages.</p> <p>If the project uses Semantic Versioning, it can help you dismiss all changes that are <code>micro</code>, review without urgency the <code>minor</code> and prioritize the <code>major</code> ones. If all you're given is a CalVer style version then you're forced to dedicate the same time to each of the changes.</p> </li> <li> <p>Changelog: if you get a notification of a new release, head to the changelog     to get a better detail of what has changed.</p> </li> <li> <p>Pull requests: Depending on the project release workflow, it may     take some time from a change to be accepted until it's published under a new     release, if you monitor the pull requests, you get an early idea of what     will be included in the new version.</p> </li> <li> <p>Issues: Most of changes introduced in a project are created from the outcome     of a repository issue, where a user expresses their desire to introduce the     change. If you monitor them you'll get the idea of how the project will     evolve in the future.</p> </li> </ul>"}, {"location": "versioning/#summary_1", "title": "Summary", "text": "<p>Is semantic versioning irrevocably broken? Should it never be used? I don\u2019t think so. It still makes a lot of sense where there are ample resources to maintain multiple versions in parallel. A great example of this is Django. However, it feels less practical for projects that have just a few maintainers.</p> <p>In this case, it often leads to opting people out of bug fixes and security updates. It also encourages version conflicts in environments that can\u2019t have multiple versions of the same library, as is the case with Python. Furthermore, it makes it a lot harder for developers to learn from their mistakes and evolve the API to a better place. Rotten old design decisions will pull down the library for years to come.</p> <p>A better solution at hand can be using CalVer and a time-window based warning system to evolve the API and remove old interfaces. Does it solve all problems? Absolutely not.</p> <p>One thing it makes harder is library rewrites. For example, consider virtualenv's recent rewrite. Version 20 introduced a completely new API and changed some behaviours to new defaults. For such use cases in a CalVer world, you would likely need to release the rewritten project under a new name, such as virtualenv2. Then again, such complete rewrites are extremely rare (in the case of virtualenv, it involved twelve years passing).</p> <p>No version scheme will allow you to predict with any certainty how compatible your software will be with potential future versions of your dependencies. The only reasonable choices are for libraries to choose minimum versions/excluded versions only, never maximum versions. For applications, do the same thing, but also add in a lock file of known, good versions with exact pins (this is the fundamental difference between install_requires and requirements.txt).</p>"}, {"location": "versioning/#this-doesnt-necessarily-apply-to-other-ecosystems", "title": "This doesn't necessarily apply to other ecosystems", "text": "<p>All of  this advice coming from me does not necessarily apply to all other packaging ecosystems. Python's flat dependency management has its pros and cons, hence why some other ecosystems do things differently.</p>"}, {"location": "versioning/#references", "title": "References", "text": "<ul> <li>Bernat post on versioning</li> <li>Should You Use Upper Bound Version Constraints? by Henry Schreiner</li> <li>Why I don't like SemVer anymore by Snarky</li> <li>Versioning Software by donald stufft</li> </ul>"}, {"location": "vial/", "title": "Vial", "text": "<p>Vial is an open-source cross-platform (Windows, Linux and Mac) GUI and a QMK fork for configuring your keyboard in real time.</p>"}, {"location": "vial/#installation", "title": "Installation", "text": "<p>Even though you can use a web version you can install it locally through an AppImage</p> <ul> <li>Download the latest version</li> <li>Give it execution permissions</li> <li>Add the file somewhere in your <code>$PATH</code></li> </ul> <p>On linux you need to configure an <code>udev</code> rule.</p> <p>For a universal access rule for any device with Vial firmware, run this in your shell while logged in as your user (this will only work with sudo installed):</p> <pre><code>export USER_GID=`id -g`; sudo --preserve-env=USER_GID sh -c 'echo \"KERNEL==\\\"hidraw*\\\", SUBSYSTEM==\\\"hidraw\\\", ATTRS{serial}==\\\"*vial:f64c2b3c*\\\", MODE=\\\"0660\\\", GROUP=\\\"$USER_GID\\\", TAG+=\\\"uaccess\\\", TAG+=\\\"udev-acl\\\"\" &gt; /etc/udev/rules.d/99-vial.rules &amp;&amp; udevadm control --reload &amp;&amp; udevadm trigger'\n</code></pre> <p>This command will automatically create a <code>udev</code> rule and reload the <code>udev</code> system.</p>"}, {"location": "vim/", "title": "Vim", "text": "<p>Vim is a lightweight keyboard driven editor. It's the road to fly over the keyboard as it increases productivity and usability.</p> <p>If you doubt between learning emacs or vim, go with emacs with spacemacs</p> <p>I am a power vim user for more than 10 years, and seeing what my friends do with emacs, I suggest you to learn it while keeping the vim movement.</p> <p>Spacemacs is a preconfigured Emacs with those bindings and a lot of more stuff, but it's a good way to start.</p>"}, {"location": "vim/#vi-vs-vim-vs-neovim", "title": "Vi vs Vim vs Neovim", "text": "<p>TL;DR: Use Neovim</p> <p>Small comparison:</p> <ul> <li>Vi</li> <li>Follows the Single Unix Specification and POSIX.</li> <li>Original code written by Bill Joy in 1976.</li> <li>BSD license.</li> <li> <p>Doesn't even have a git repository <code>-.-</code>.</p> </li> <li> <p>Vim</p> </li> <li>Written by Bram Moolenaar in 1991.</li> <li>Vim is free and open source software, license is compatible with the GNU General Public License.</li> <li>C: 47.6 %, Vim Script: 44.8%, Roff 1.9%, Makefile 1.7%, C++ 1.2%</li> <li>Commits: 7120, Branch: 1, Releases: 5639, Contributor: 1</li> <li> <p>Lines: 1.295.837</p> </li> <li> <p>Neovim</p> </li> <li>Written by the community from 2014</li> <li>Published under the Apache 2.0 license</li> <li>Commits: 7994, Branch 1, Releases: 9, Contributors: 303</li> <li>Vim script: 46.9%, C:38.9%, Lua 11.3%, Python 0.9%, C++ 0.6%</li> <li>Lines: 937.508 (27.65% less code than vim)</li> <li>Refactor: Simplify maintenance and encourage contributions</li> <li>Easy update, just symlinks</li> <li>Ahead of vim, new features inserted in Vim 8.0 (async)</li> </ul> <p>Neovim is a refactor of Vim to make it viable for another 30 years of hacking.</p> <p>Neovim very intentionally builds on the long history of Vim community knowledge and user habits. That means \u201cswitching\u201d from Vim to Neovim is just an \u201cupgrade\u201d.</p> <p>From the start, one of Neovim\u2019s explicit goals has been simplify maintenance and encourage contributions.  By building a codebase and community that enables experimentation and low-cost trials of new features..</p> <p>And there\u2019s evidence of real progress towards that ambition. We\u2019ve successfully executed non-trivial \u201coff-the-roadmap\u201d patches: features which are important to their authors, but not the highest priority for the project.</p> <p>These patches were included because they:</p> <ul> <li>Fit into existing conventions/design.</li> <li>Included robust test coverage (enabled by an advanced test framework and CI).</li> <li>Received thoughtful review by other contributors.</li> </ul> <p>One downside though is that it's not able to work with \"big\" files for me 110kb file broke it. Although after some debugging it worked.</p>"}, {"location": "vim/#installation", "title": "Installation", "text": "<p>The version of <code>nvim</code> released by debian is too old, use the latest by downloading it directly from the releases page and unpacking it somewhere in your home and doing a link to the <code>bin/nvim</code> file somewhere in your <code>$PATH</code>.</p>"}, {"location": "vim/#configuration", "title": "Configuration", "text": "<p>Nvim moved away from vimscript and now needs to be configured in lua. You can access the config file in <code>~/.config/nvim/init.lua</code>. It's not created by default so you need to do it yourself.</p> <p>To access the editor's setting we need to use the global variable <code>vim</code>. Okay, more than a variable this thing is a module. It has an <code>opt</code> property to change the program options.  This is the syntax you should follow.</p> <pre><code>vim.opt.option_name = value\n</code></pre> <p>Where <code>option_name</code> can be anything in this list. And value must be whatever that option expects. You can also see the list with <code>:help option-list</code>.</p>"}, {"location": "vim/#key-bindings", "title": "Key bindings", "text": "<p>We need to learn about <code>vim.keymap.set</code>. Here is a basic usage example.</p> <pre><code>vim.keymap.set('n', '&lt;space&gt;w', '&lt;cmd&gt;write&lt;cr&gt;', {desc = 'Save'})\n</code></pre> <p>After executing this, the sequence <code>Space + w</code> will call the <code>write</code> command. Basically, we can save changes made to a file with <code>Space + w</code>.</p> <p>Let's dive into what does the  <code>vim.keymap.set</code> parameters mean.</p> <pre><code>vim.keymap.set({mode}, {lhs}, {rhs}, {opts})\n</code></pre> <ul> <li><code>{mode}</code>:  mode where the keybinding should execute. It can be a list of modes. We need to specify the mode's short name. Here are some of the most common.</li> <li><code>n</code>: Normal mode.</li> <li><code>i</code>: Insert mode.</li> <li><code>x</code>: Visual mode.</li> <li><code>s</code>: Selection mode.</li> <li><code>v</code>: Visual + Selection.</li> <li><code>t</code>: Terminal mode.</li> <li><code>o</code>: Operator-pending.</li> <li> <p><code>''</code>: Yes, an empty string. Is the equivalent of <code>n + v + o</code>.</p> </li> <li> <p><code>{lhs}</code>: is the key we want to bind.</p> </li> <li><code>{rhs}</code> is the action we want to execute. It can be a string with a command or an expression. You can also provide a lua function.</li> <li> <p><code>{opts}</code> this must be a lua table. If you don't know what is a \"lua table\" just think is a way of storing several values in one place. Anyway, it can have these properties.</p> </li> <li> <p><code>desc</code>: A string that describes what the keybinding does. You can write anything you want.</p> </li> <li><code>remap</code>: A boolean that determines if our keybinding can be recursive. The default value is <code>false</code>. Recursive keybindings can cause some conflicts if used incorrectly. Don't enable it unless you know what you're doing.</li> <li><code>buffer</code>: It can be a boolean or a number. If we assign the boolean <code>true</code> it means the keybinding will only be effective in the current file. If we assign a number, it needs to be the \"id\" of an open buffer.</li> <li><code>silent</code>: A boolean. Determines whether or not the keybindings can show a message. The default value is <code>false</code>.</li> <li><code>expr</code>: A boolean. If enabled it gives the chance to use vimscript or lua to calculate the value of <code>{rhs}</code>. The default value is <code>false</code>.</li> </ul>"}, {"location": "vim/#the-leader-key", "title": "The leader key", "text": "<p>When creating keybindings we can use the special sequence <code>&lt;leader&gt;</code> in the <code>{lhs}</code> parameter, it'll take the value of the global variable mapleader.</p> <p>So mapleader is a global variable in vimscript that can be string. For example.</p> <pre><code>vim.g.mapleader = ' '\n</code></pre> <p>After defining it we can use it as a prefix in our keybindings.</p> <pre><code>vim.keymap.set('n', '&lt;leader&gt;w', '&lt;cmd&gt;write&lt;cr&gt;')\n</code></pre> <p>This will make <code>&lt;space key&gt;</code> + <code>w</code> save the current file.</p> <p>There are different opinions on what key to use as the <code>&lt;leader&gt;</code> key. The <code>&lt;space&gt;</code> is the most comfortable as it's always close to your thumbs, and it works well with both hands. Nevertheless, you can only use it in normal mode, because in insert <code>&lt;space&gt;&lt;whatever&gt;</code> will be triggered as you write. An alternative is to use <code>;</code> which is also comfortable (if you use the english key distribution) and you can use it in insert mode. </p> <p>If you want to define more than one leader key you can either:</p> <ul> <li>Change the <code>mapleader</code> many times in your file: As the value of <code>mapleader</code> is used at the moment the mapping is defined, you can indeed change that while plugins are loading. For that, you have to explicitly <code>:runtime</code> the plugins in your <code>~/.vimrc</code> (and count on the canonical include guard to prevent redefinition later):</li> </ul> <p><pre><code>let mapleader = ','\nruntime! plugin/NERD_commenter.vim\nruntime! ...\nlet mapleader = '\\'\nrunime! plugin/mark.vim\n...\n</code></pre> * Use the keys directly instead of using <code>&lt;leader&gt;</code> </p> <pre><code>\" editing mappings\nnnoremap ,a &lt;something&gt;\nnnoremap ,k &lt;something else&gt;\nnnoremap ,d &lt;and something else&gt;\n\n\" window management mappings\nnnoremap gw &lt;something&gt;\nnnoremap gb &lt;something else&gt;\n</code></pre> <p>Defining <code>mapleader</code> and/or using <code>&lt;leader&gt;</code> may be useful if you change your mind often on what key to use a leader but it won't be of any use if your mappings are stable.</p>"}, {"location": "vim/#spelling", "title": "Spelling", "text": "<pre><code>set.spell = true\nset.spelllang = 'en_us'\nset.spellfile = '/home/your_user/.config/nvim/spell/my_dictionary.add'\n</code></pre>"}, {"location": "vim/#testing", "title": "Testing", "text": "<p>The <code>vim-test</code> alternatives for neovim are:</p> <ul> <li><code>neotest</code></li> <li><code>nvim-test</code></li> </ul> <p>The first one is the most popular so it's the first to try.</p>"}, {"location": "vim/#neotest", "title": "neotest", "text": ""}, {"location": "vim/#installation_1", "title": "Installation", "text": "<p>Add to your <code>packer</code> configuration:</p> <pre><code>use {\n  \"nvim-neotest/neotest\",\n  requires = {\n    \"nvim-lua/plenary.nvim\",\n    \"nvim-treesitter/nvim-treesitter\",\n    \"antoinemadec/FixCursorHold.nvim\"\n  }\n}\n</code></pre> <p>To get started you will also need to install an adapter for your test runner. For example for python add also:</p> <pre><code>use  \"nvim-neotest/neotest-python\"\n</code></pre> <p>Then configure the plugin with:</p> <pre><code>require(\"neotest\").setup({ -- https://github.com/nvim-neotest/neotest\n  adapters = {\n    require(\"neotest-python\")({ -- https://github.com/nvim-neotest/neotest-python\n      dap = { justMyCode = false },\n    }),\n  }\n})\n</code></pre> <p>It also needs a font that supports icons. If you don't see them install one of these.</p>"}, {"location": "vim/#plugin-managers", "title": "Plugin managers", "text": "<p>Neovim has builtin support for installing plugins. You can manually download the plugins in any directory shown in <code>:set packpath?</code>, for example <code>~/.local/share/nvim/site</code>. In one of those directories we have to create a directory called <code>pack</code> and inside <code>pack</code> we must create a \"package\". A package is a directory that contains several plugins. It must have this structure.</p> <pre><code>package-directory\n\u251c\u2500\u2500 opt\n\u2502   \u251c\u2500\u2500 [plugin 1]\n\u2502   \u2514\u2500\u2500 [plugin 2]\n\u2514\u2500\u2500 start\n    \u251c\u2500\u2500 [plugin 3]\n    \u2514\u2500\u2500 [plugin 4]\n</code></pre> <p>In this example we are creating a directory with two other directory inside: opt and start. Plugins in opt will only be loaded if we execute the command packadd. The plugins in start will be loaded automatically during the startup process.</p> <p>So to install a plugin like <code>lualine</code> and have it load automatically, we should place it for example here <code>~/.local/share/nvim/site/pack/github/start/lualine.nvim</code></p> <p>As I'm using <code>chezmoi</code> to handle the plugins of <code>zsh</code> and other stuff I tried to work with that. It was a little cumbersome to add the plugins but it did the job until I had to install <code>telescope</code> which needs to run a command after each install, and that was not easy with <code>chezmoi</code>. Then I analyzed the  most popular plugin managers in the Neovim ecosystem right now:</p> <ul> <li><code>packer</code></li> <li><code>paq</code></li> </ul> <p>If you prefer minimalism take a look at <code>paq</code>. If you want something full of features use <code>packer</code>. I went with <code>packer</code>.</p>"}, {"location": "vim/#packer", "title": "Packer", "text": ""}, {"location": "vim/#installation_2", "title": "Installation", "text": "<p>Create the <code>~/.config/nvim/lua/plugins.lua</code> file with the contents:</p> <pre><code>vim.cmd [[packadd packer.nvim]]\n\nreturn require('packer').startup(function(use)\n  -- Packer can manage itself\n  use 'wbthomason/packer.nvim'\n\n  -- Example of another plugin. Nice buffer closing \n  use 'moll/vim-bbye'\n\nend)\n</code></pre> <p>And load the file in your <code>~/.config/nvim/init.lua</code>:</p> <pre><code>-- -------------------\n-- --    Plugins    --\n-- -------------------\nrequire('plugins')\n</code></pre> <p>You can now run the <code>packer</code> commands.</p>"}, {"location": "vim/#usage", "title": "Usage", "text": "<p>Whenever you make changes to your plugin configuration you need to:</p> <ul> <li>Regenerate the compiled loader file:</li> </ul> <pre><code>:PackerCompile\n</code></pre> <ul> <li>Remove any disabled or unused plugins</li> </ul> <pre><code>:PackerClean\n</code></pre> <ul> <li>Clean, then install missing plugins</li> </ul> <pre><code>:PackerInstall\n</code></pre> <p>To update the packages to the latest version you can run:</p> <pre><code>:PackerUpdate\n</code></pre> <p>To show the list of installed plugins run:</p> <pre><code>:PackerStatus\n</code></pre>"}, {"location": "vim/#buffer-and-file-management", "title": "Buffer and file management", "text": "<p>In the past I used ctrlp as a remaining of the migration from vim to nvim. Today I've seen that there are <code>nvim</code> native plugins to do the same. I'm going to start with <code>Telescope</code>, a popular plugin (8.4k stars)</p>"}, {"location": "vim/#telescope", "title": "Telescope", "text": ""}, {"location": "vim/#install", "title": "Install", "text": "<p>It is suggested to either use the latest release tag or their release branch (which will get consistent updates) 0.1.x. If you're  using <code>packer</code> you can add this to your <code>plugins.lua</code>:</p> <pre><code>use {\n  'nvim-telescope/telescope.nvim', tag = '0.1.x',\n  requires = { {'nvim-lua/plenary.nvim'} }\n}\n</code></pre> <p>You may need to have installed <code>treesitter</code> look for those instructions to install it.</p> <p><code>telescope</code> uses <code>ripgrep</code> to do <code>live-grep</code>. I've tried using <code>ag</code> instead with this config, but it didn't work.</p> <pre><code>require('telescope').setup{\n  defaults = {\n     vimgrep_arguments = {\n        \"ag\",\n        \"--nocolor\",\n        \"--noheading\",\n        \"--numbers\",\n        \"--column\",\n        \"--smart-case\",\n        \"--silent\",\n        \"--vimgrep\",\n    }\n  }\n}\n</code></pre> <p>It's a good idea also to have <code>fzf</code> fuzzy finder, to do that we need to install the <code>telescope-fzf-native</code> plugin. To do that add to your <code>plugins.lua</code> config file:</p> <pre><code>  use {\n    'nvim-telescope/telescope-fzf-native.nvim', \n    run = 'make' \n  }\n</code></pre> <p>Run <code>:PackerInstall</code> and then configure it in your <code>init.lua</code>:</p> <pre><code>-- You dont need to set any of these options. These are the default ones. Only\n-- the loading is important\nrequire('telescope').setup {\n  extensions = {\n    fzf = {\n      fuzzy = true,                    -- false will only do exact matching\n      override_generic_sorter = true,  -- override the generic sorter\n      override_file_sorter = true,     -- override the file sorter\n      case_mode = \"smart_case\",        -- or \"ignore_case\" or \"respect_case\"\n                                       -- the default case_mode is \"smart_case\"\n    }\n  }\n}\n-- To get fzf loaded and working with telescope, you need to call\n-- load_extension, somewhere after setup function:\nrequire('telescope').load_extension('fzf')\n</code></pre> <p>It also needs <code>fd</code> for further features. You should be using it too for your terminal.</p> <p>To check that everything is fine run <code>:checkhealth telescope</code>.</p>"}, {"location": "vim/#usage_1", "title": "Usage", "text": "<p><code>telescope</code> has different ways to find files:</p> <ul> <li><code>find_files</code>: Uses <code>fd</code> to find a string in the file names.</li> <li><code>live_grep</code>: Uses <code>rg</code> to find a string in the file's content.</li> <li><code>buffers</code>: Searches strings in the buffer names.</li> </ul> <p>You can configure each of these commands with the next bindings:</p> <pre><code>local builtin = require('telescope.builtin')\nlocal key = vim.keymap\nkey.set('n', '&lt;leader&gt;f', builtin.find_files, {})\nkey.set('n', '&lt;leader&gt;a', builtin.live_grep, {})\nkey.set('n', '&lt;leader&gt;b', builtin.buffers, {})\n</code></pre> <p>By default it searches on all files. You can ignore some of them with:</p> <pre><code>require('telescope').setup{\n  defaults = {\n    -- Default configuration for telescope goes here:\n    -- config_key = value,\n    file_ignore_patterns = {\n      \"%.svg\",\n      \"%.bmp\",\n      \"%.jpg\",\n      \"%.jpeg\",\n      \"%.gif\",\n      \"%.png\",\n    },\n  }\n}\n</code></pre> <p>You can also replace some other default <code>vim</code> commands like history browsing, spell checker suggestions or searching in the current buffer with:</p> <pre><code>key.set('n', '&lt;C-r&gt;', builtin.command_history, {})\nkey.set('n', 'z=', builtin.spell_suggest, {})\nkey.set('n', '/', builtin.current_buffer_fuzzy_find, {})\n</code></pre>"}, {"location": "vim/#follow-symbolic-links", "title": "Follow symbolic links", "text": "<p>By default symbolic links are not followed either for files or directories, to enable it use</p> <pre><code>  require('telescope').setup {\n    pickers = {\n      find_files = {\n        follow = true\n      }\n    }\n  }\n</code></pre>"}, {"location": "vim/#heading-navigation", "title": "Heading navigation", "text": "<p>It's a <code>telescope</code> plugin to navigate through your markdown headers</p>"}, {"location": "vim/#installation_3", "title": "Installation", "text": "<p>Install with your favorite package manager:</p> <pre><code>use('nvim-telescope/telescope.nvim')\nuse('crispgm/telescope-heading.nvim')\n</code></pre> <p><code>telescope-heading</code> supports Tree-sitter for parsing documents and finding headings.</p> <pre><code>-- make sure you have already installed treesitter modules\nrequire('nvim-treesitter.configs').setup({\n    ensure_installed = {\n        -- ..\n        'markdown',\n        'rst',\n        -- ..\n    },\n})\n\n-- enable treesitter parsing\nlocal telescope = require('telescope')\ntelescope.setup({\n    -- ...\n    extensions = {\n        heading = {\n            treesitter = true,\n        },\n    },\n})\n\n-- `load_extension` must be after `telescope.setup`\ntelescope.load_extension('heading')\n\n-- Set the key binding\n\nlocal key = vim.keymap\nkey.set('n', '&lt;leader&gt;h', ':Telescope heading&lt;cr&gt;')\n</code></pre>"}, {"location": "vim/#treesitter", "title": "Treesitter", "text": "<p><code>treesitter</code> it's a neovim parser generator tool and an incremental parsing library. It can build a concrete syntax tree for a source file and efficiently update the syntax tree as the source file is edited. With it you can do nice things like:</p> <ul> <li>Highlight code</li> <li>Incremental selection of the code</li> <li>Indentation</li> <li>Folding</li> </ul>"}, {"location": "vim/#installation_4", "title": "Installation", "text": "<p>Add these lines to your <code>plugins.lua</code> file:</p> <pre><code>  use {\n    'nvim-treesitter/nvim-treesitter',\n    run = function()\n        local ts_update = require('nvim-treesitter.install').update({ with_sync = true })\n        ts_update()\n    end,\n  }\n</code></pre> <p>Install it with <code>:PackerInstall</code>.</p> <p>The base configuration is:</p> <pre><code>require('nvim-treesitter.configs').setup({\n  ensure_installed = {\n    'bash',\n    'beancount',\n    'dockerfile',\n    'make',\n    'terraform',\n    'toml',\n    'vue',\n    'lua',\n    'markdown',\n    'python',\n    'css',\n    'html',\n    'javascript',\n    'json',\n    'yaml',\n  },\n})\n</code></pre> <p>Select the languages you want to install from the available ones, close and reopen the vim window to install them.</p> <p>To do so you need to run:</p> <pre><code>:TSInstall &lt;language&gt;\n</code></pre> <p>To update the parsers run</p> <pre><code>:TSUpdate\n</code></pre>"}, {"location": "vim/#usage_2", "title": "Usage", "text": "<p>By default it doesn't enable any feature, you need to enable them yourself.</p>"}, {"location": "vim/#highlight-code", "title": "Highlight code", "text": "<p>Enable the feature with:</p> <pre><code>require('nvim-treesitter.configs').setup({\n  highlight = {\n    enable = true,\n  },\n})\n</code></pre> <p>Improves the default syntax for the supported languages.</p>"}, {"location": "vim/#incremental-selection", "title": "Incremental selection", "text": "<p>It lets you select pieces of your code by the function they serve. For example imagine that we have the next snippet:</p> <pre><code>def function():\n  if bool is True:\n    print('this is a Test')\n</code></pre> <p>And your cursor is in the <code>T</code> of the <code>print</code> statement. If you were to press the <code>Enter</code> key it will enter in visual mode selecting the <code>Test</code> word, if you were to press <code>Enter</code> key again it will increment the scope of the search, so it will select all the contents of the print statement <code>'this is a Test'</code>, if you press <code>Enter</code> again it will increase the scope. </p> <p>If you went too far, you can use the <code>Return</code> key to reduce the scope. For these keybindings to work you need to set:</p> <pre><code>require('nvim-treesitter.configs').setup({\n  incremental_selection = {\n    enable = true,\n    keymaps = {\n      init_selection = \"&lt;cr&gt;\", -- set to `false` to disable one of the mappings\n      node_incremental = \"&lt;cr&gt;\",\n      node_decremental = \"&lt;bs&gt;\",\n      -- scope_incremental = \"grc\",\n    },\n  },\n})\n</code></pre>"}, {"location": "vim/#indentation", "title": "Indentation", "text": "<pre><code>require'nvim-treesitter.configs'.setup {\n  indent = {\n    enable = true\n  }\n}\n</code></pre>"}, {"location": "vim/#folding", "title": "Folding", "text": "<p>Tree-sitter based folding</p> <pre><code>set.foldmethod = 'expr'\nset.foldexpr = 'nvim_treesitter#foldexpr()'\nset.foldenable = true                   \nset.foldminlines = 3\n</code></pre> <p>It won't fold code sections that have have less than 3 lines.</p> <p>If you add files through <code>telescope</code> you may see an <code>E490: No fold found</code> error when trying to access the folds, there's an open issue that tracks this, the workaround for me was to add this snippet in the <code>telescope</code> configuration::</p> <pre><code>require('telescope').setup {\n    defaults = {\n        mappings = {\n            i = {\n                [\"&lt;CR&gt;\"] = function()\n                    vim.cmd [[:stopinsert]]\n                    vim.cmd [[call feedkeys(\"\\&lt;CR&gt;\")]]\n                end\n            }\n        }\n    }\n}\n</code></pre> <p>To save the foldings when you save a file use the next snippet. Sorry but I don't know how to translate that into lua.</p> <pre><code>vim.cmd[[\n  augroup remember_folds\n    autocmd!\n    autocmd BufWinLeave * silent! mkview\n    autocmd BufWinEnter * silent! loadview\n  augroup END\n]]\n</code></pre>"}, {"location": "vim/#git", "title": "Git", "text": "<p>There are many plugins to work with git in neovim the most interesting ones are:</p> <ul> <li>vim-fugitive</li> <li>neogit</li> <li>lazygit</li> <li>vgit</li> </ul> <p>I've been using <code>vim-fugitive</code> for some years now and it works very well but is built for <code>vim</code>. Now that I'm refurbishing all the neovim configuration I want to try some neovim native plugins.</p> <p><code>neogit</code> looks interesting as it's a magit clone for <code>neovim</code>. <code>lazygit</code> is the most popular one as it's a command line tool not specific to <code>neovim</code>. As such you'd need to launch a terminal inside neovim or use a plugin like lazygit.nvim. I'm not able to understand how to use <code>vgit</code> by looking at their readme, there's not more documentation and there is no videos showing it's usage. It's also the least popular although it looks active.</p> <p>At a first look <code>lazygit</code> is too much and <code>neogit</code> a little more verbose than <code>vim-fugitive</code> but it looks closer to my current workflow. I'm going to try <code>neogit</code> then.</p>"}, {"location": "vim/#neogit", "title": "Neogit", "text": ""}, {"location": "vim/#installation_5", "title": "Installation", "text": "<pre><code>use { 'TimUntersberger/neogit', requires = 'nvim-lua/plenary.nvim' }\n</code></pre> <p>Now you have to add the following lines to your <code>init.lua</code></p> <pre><code>local neogit = require('neogit')\n\nneogit.setup()\n</code></pre> <p>That uses the default configuration, but there are many options that can be set. For example to disable the commit confirmation use:</p> <pre><code>neogit.setup({\n  disable_commit_confirmation = true\n})\n\n### Improve the commit message window\n\n[create custom keymaps with lua](https://blog.devgenius.io/create-custom-keymaps-in-neovim-with-lua-d1167de0f2c2)\n[create specific bindings for a file type](https://stackoverflow.com/questions/72984648/neovim-lua-how-to-use-different-mappings-depending-on-file-type)\nhttps://neovim.discourse.group/t/how-to-create-an-auto-command-for-a-specific-filetype-in-neovim-0-7/2404\n[create autocmd in neovim](https://alpha2phi.medium.com/neovim-for-beginners-lua-autocmd-and-keymap-functions-3bdfe0bebe42)\n[autocmd events](https://neovim.io/doc/user/autocmd.html#autocmd-events)\n\n\n# [Abbreviations](https://davidxmoody.com/2014/better-vim-abbreviations/)\n\nIn order to reduce the amount of typing and fix common typos, I use the Vim\nabbreviations support. Those are split into two files,\n`~/.vim/abbreviations.vim` for abbreviations that can be used in every type of\nformat and `~/.vim/markdown-abbreviations.vim` for the ones that can interfere\nwith programming typing.\n\nThose files are sourced in my `.vimrc`\n\n```vim\n\" Abbreviations\nsource ~/.vim/abbreviations.vim\nautocmd BufNewFile,BufReadPost *.md source ~/.vim/markdown-abbreviations.vim\n</code></pre> <p>To avoid getting worse in grammar, I don't add abbreviations for words that I doubt their spelling or that I usually mistake. Instead I use it for common typos such as <code>teh</code>.</p> <p>The process has it's inconveniences:</p> <ul> <li>You need different abbreviations for the capitalized versions, so you'd need     two abbreviations for <code>iab cant can't</code> and <code>iab Cant Can't</code></li> <li>It's not user friendly to add new words, as you need to open a file.</li> </ul> <p>The Vim Abolish plugin solves that. For example:</p> <pre><code>\" Typing the following:\nAbolish seperate separate\n\n\" Is equivalent to:\niabbrev seperate separate\niabbrev Seperate Separate\niabbrev SEPERATE SEPARATE\n</code></pre> <p>Or create more complex rules, were each <code>{}</code> gets captured and expanded with different caps</p> <pre><code>:Abolish {despa,sepe}rat{e,es,ed,ing,ely,ion,ions,or}  {despe,sepa}rat{}\n</code></pre> <p>With a bang (<code>:Abolish!</code>) the abbreviation is also appended to the file in <code>g:abolish_save_file</code>. By default <code>after/plugin/abolish.vim</code> which is loaded by default.</p> <p>Typing <code>:Abolish! im I'm</code> will append the following to the end of this file:</p> <pre><code>Abolish im I'm\n</code></pre> <p>To make it quicker I've added a mapping for <code>&lt;leader&gt;s</code>.</p> <pre><code>nnoremap &lt;leader&gt;s :Abolish!&lt;Space&gt;\n</code></pre> <p>Check the README for more details.</p>"}, {"location": "vim/#troubleshooting", "title": "Troubleshooting", "text": "<p>Abbreviations with dashes or if you only want the first letter in capital need to be specified with the first letter in capital letters as stated in this issue.</p> <pre><code>Abolish knobas knowledge-based\nAbolish w what\n</code></pre> <p>Will yield <code>KnowledgeBased</code> if invoked with <code>Knobas</code>, and <code>WHAT</code> if invoked with <code>W</code>. Therefore the following definitions are preferred:</p> <pre><code>Abolish Knobas Knowledge-based\nAbolish W What\n</code></pre>"}, {"location": "vim/#auto-complete-prose-text", "title": "Auto complete prose text", "text": "<p>Tools like YouCompleteMe allow you to auto complete variables and functions. If you want the same functionality for prose, you need to enable it for markdown and text, as it's disabled by default.</p> <pre><code>let g:ycm_filetype_blacklist = {\n      \\ 'tagbar' : 1,\n      \\ 'qf' : 1,\n      \\ 'notes' : 1,\n      \\ 'unite' : 1,\n      \\ 'vimwiki' : 1,\n      \\ 'pandoc' : 1,\n      \\ 'infolog' : 1\n  \\}\n</code></pre> <p>When writing prose you don't need all possible suggestions, as navigating the options is slower than keep on typing. So I'm limiting the results just to one, to avoid unnecessary distractions.</p> <pre><code>\" Limit the results for markdown files to 1\nau FileType markdown let g:ycm_max_num_candidates = 1\nau FileType markdown let g:ycm_max_num_identifier_candidates = 1\n</code></pre>"}, {"location": "vim/#find-synonyms", "title": "Find synonyms", "text": "<p>Sometimes the prose linters tell you that a word is wordy or too complex, or you may be repeating a word too much. The thesaurus query plugin allows you to search synonyms of the word under the cursor. Assuming you use Vundle, add the following lines to your config.</p> <p>File: ~/.vimrc</p> <pre><code>Plugin 'ron89/thesaurus_query.vim'\n\n\" Thesaurus\nlet g:tq_enabled_backends=[\"mthesaur_txt\"]\nlet g:tq_mthesaur_file=\"~/.vim/thesaurus\"\nnnoremap &lt;leader&gt;r :ThesaurusQueryReplaceCurrentWord&lt;CR&gt;\ninoremap &lt;leader&gt;r &lt;esc&gt;:ThesaurusQueryReplaceCurrentWord&lt;CR&gt;\n</code></pre> <p>Run <code>:PluginInstall</code> and download the thesaurus text from gutenberg.org</p> <p>Next time you find a word like <code>therefore</code> you can press <code>:ThesaurusQueryReplaceCurrentWord</code> and you'll get a window with the following:</p> <pre><code>In line: ... therefore ...\nCandidates for therefore, found by backend: mthesaur_txt\nSynonyms: (0)accordingly (1)according to circumstances (2)and so (3)appropriately (4)as a consequence\n          (5)as a result (6)as it is (7)as matters stand (8)at that rate (9)because of that (10)because of this\n          (11)compliantly (12)conformably (13)consequently (14)equally (15)ergo (16)finally (17)for that\n          (18)for that cause (19)for that reason (20)for this cause (21)for this reason (22)for which reason\n          (23)hence (24)hereat (25)in that case (26)in that event (27)inconsequence (28)inevitably\n          (29)it follows that (30)naturally (31)naturellement (32)necessarily (33)of course (34)of necessity\n          (35)on that account (36)on that ground (37)on this account (38)propter hoc (39)suitably\n          (40)that being so (41)then (42)thence (43)thereat (44)therefor (45)thus (46)thusly (47)thuswise\n          (48)under the circumstances (49)whence (50)wherefore (51)wherefrom\nType number and &lt;Enter&gt; (empty cancels; 'n': use next backend; 'p' use previous backend):\n</code></pre> <p>If for example you type <code>45</code> and hit enter, it will change it for <code>thus</code>.</p>"}, {"location": "vim/#keep-foldings", "title": "Keep foldings", "text": "<p>When running fixers usually the foldings go to hell. To keep the foldings add the following snippet to your vimrc file</p> <pre><code>augroup remember_folds\n  autocmd!\n  autocmd BufLeave * mkview\n  autocmd BufEnter * silent! loadview\naugroup END\n</code></pre>"}, {"location": "vim/#python-folding-done-right", "title": "Python folding done right", "text": "<p>Folding Python in Vim is not easy, the python-mode plugin doesn't do it for me by default and after fighting with it for 2 hours...</p> <p>SimpylFold does the trick just fine.</p>"}, {"location": "vim/#delete-a-file-inside-vim", "title": "Delete a file inside vim", "text": "<pre><code>:call delete(expand('%')) | bdelete!\n</code></pre> <p>You can make a function so it's easier to remember</p> <pre><code>function! Rm()\n  call delete(expand('%')) | bdelete!\nendfunction\n</code></pre> <p>Now you need to run <code>:call Rm()</code>.</p>"}, {"location": "vim/#task-management", "title": "Task management", "text": "<p>Check the <code>nvim-orgmode</code> file.</p>"}, {"location": "vim/#references", "title": "References", "text": "<ul> <li>Source</li> <li>Getting started guide</li> <li>Docs</li> </ul>"}, {"location": "vim/#troubleshooting_1", "title": "Troubleshooting", "text": "<p>When you run into problems run <code>:checkhealth</code> to see if it rings a bell</p>"}, {"location": "vim/#deal-with-big-files", "title": "Deal with big files", "text": "<p>Sometimes <code>neovim</code> freezes when opening big files, one way to deal with it is to disable some functionality when loading them</p> <pre><code>local aug = vim.api.nvim_create_augroup(\"buf_large\", { clear = true })\n\nvim.api.nvim_create_autocmd({ \"BufReadPre\" }, {\n  callback = function()\n    local ok, stats = pcall(vim.loop.fs_stat, vim.api.nvim_buf_get_name(vim.api.nvim_get_current_buf()))\n    if ok and stats and (stats.size &gt; 100000) then\n      vim.b.large_buf = true\n      -- vim.cmd(\"syntax off\") I don't yet need to turn the syntax off\n      vim.opt_local.foldmethod = \"manual\"\n      vim.opt_local.spell = false\n      set.foldexpr = 'nvim_treesitter#foldexpr()' -- Disable fold expression with treesitter, it freezes the loading of files\n    else\n      vim.b.large_buf = false\n    end\n  end,\n  group = aug,\n  pattern = \"*\",\n})\n</code></pre> <p>When it opens a file it will decide if it's a big file. If it is, it will unset the <code>foldexpr</code> which made it break for me.</p> <p>Telescope's preview also froze the terminal. To deal with it I had to disable treesitter for the preview</p> <pre><code>require('telescope').setup{\n  defaults = {\n    preview = {\n      enable = true,\n      treesitter = false,\n    },\n  ...\n</code></pre>"}, {"location": "vim/#tips", "title": "Tips", "text": ""}, {"location": "vim/#run-a-command-when-opening-vim", "title": "Run a command when opening vim", "text": "<pre><code>nvim -c ':DiffViewOpen'\n</code></pre>"}, {"location": "vim/#run-lua-snippets", "title": "Run lua snippets", "text": "<p>Run lua snippet within neovim with <code>:lua &lt;your snippet&gt;</code>. Useful to test the commands before binding it to keys.</p>"}, {"location": "vim/#bind-a-lua-function-to-a-key-binding", "title": "Bind a lua function to a key binding", "text": "<pre><code>key.set({'n'}, 't', \":lua require('neotest').run.run()&lt;cr&gt;\", {desc = 'Run the closest test'})\n</code></pre>"}, {"location": "vim/#use-relativenumber", "title": "Use relativenumber", "text": "<p>If you enable the <code>relativenumber</code> configuration you'll see how to move around with <code>10j</code> or <code>10k</code>.</p>"}, {"location": "vim/#troubleshooting_2", "title": "Troubleshooting", "text": ""}, {"location": "vim/#telescope-changes-working-directory-when-opening-a-file", "title": "Telescope changes working directory when opening a file", "text": "<p>In my case was due to a snippet I have to remember the folds:</p> <pre><code>vim.cmd[[\n  augroup remember_folds\n    autocmd!\n    autocmd BufWinLeave * silent! mkview\n    autocmd BufWinEnter * silent! loadview\n  augroup END\n]]\n</code></pre> <p>It looks that it had saved a view with the other working directory so when a file was loaded the <code>cwd</code> changed. To solve it I created a new <code>mkview</code> in the correct directory.</p>"}, {"location": "vim/#resources", "title": "Resources", "text": "<ul> <li>Nvim news</li> <li>spacevim</li> <li>awesome-neovim</li> <li>awesome-vim: a list of vim       resources maintained by the community</li> </ul>"}, {"location": "vim/#vimrc-tweaking", "title": "Vimrc tweaking", "text": "<ul> <li>List of nvim configs</li> <li>jessfraz vimrc</li> </ul>"}, {"location": "vim/#learning", "title": "Learning", "text": "<ul> <li>vim golf</li> <li>Vim game tutorial: very funny and challenging,       buuuuut at lvl 3 you have to pay :(.</li> <li>PacVim:       Pacman like vim game to learn.</li> <li>Vimgenius: Increase your speed and improve your       muscle memory with Vim Genius, a timed flashcard-style game designed to       make you faster in Vim. It\u2019s free and you don\u2019t need to sign up. What are       you waiting for?</li> <li>Openvim: Interactive tutorial for vim.</li> </ul>"}, {"location": "vim_tabs/", "title": "Vim tabs", "text": "<p>This article is almost a copy paste of joshldavis post</p> <p>First I have to admit, I was a heavy user of tabs in Vim.</p> <p>I was using tabs in Vim as you\u2019d use tabs in most other programs (Firefox, Terminal, Adium, etc.). I was used to the idea of a tab being the place where a document lives.</p> <p>When you want to edit a document, you open a new tab and edit away! That\u2019s how tabs work so that must be how they work in Vim right?</p> <p>Nope.</p>"}, {"location": "vim_tabs/#stop-the-tab-madness", "title": "Stop the Tab Madness", "text": "<p>If you are using tabs like this then you are really limiting yourself and using a feature of Vim that wasn't meant to work like this.</p> <p>Before I explain that, let\u2019s be sure we understand what a buffer is in Vim as well as a few other basic things.</p> <p>After that, I\u2019ll explain the correct way to use tabs within Vim.</p>"}, {"location": "vim_tabs/#buffers", "title": "Buffers", "text": "<p>A buffer is nothing more than text that you are editing. For example, when you open a file, the content of the file is loaded into a buffer. So when you issue this command:</p> <pre><code>vim .vimrc\n</code></pre> <p>You are actually launching Vim with a single buffer that is filled with the contents of the <code>.vimrc</code> file.</p> <p>Now let\u2019s look at what happens when you try to edit multiple files. Let\u2019s issue this command:</p> <pre><code>vim .vimrc .bashrc\n</code></pre> <p>In Vim run <code>:bnext</code></p> <p>Vim does what it did before, but instead of just 1 buffer, it opens another buffer that is filled with <code>.bashrc</code>. So now we have two buffers open.</p> <p>If you want to pause editing <code>.vimrc</code> and move to <code>.bashrc</code>, you could run this command in Vim <code>:bnext</code> which will show the <code>.bashrc</code> buffer. There are various other commands to manipulate buffers which you can see if you type <code>:h buffer-list</code>. Or you can use easymotion.</p>"}, {"location": "vim_tabs/#windows", "title": "Windows", "text": "<p>A window in Vim is just a way to view a buffer. Whenever you create a new vertical or horizontal split, that is a window. For example, if you were to type in <code>:help window</code>, it would launch a new window that shows the help documentation.</p> <p>The important thing to note is that a window can view any buffer it wishes; it isn't forced to look at the same buffer. When editing a file, if we type <code>:vsplit</code>, we will get a vertical split and in the other window, we will see the current buffer we are editing.</p> <p>That should no longer be confusing because a window lets us look at any buffer. It just so happens that when creating a new split: <code>:split</code> or <code>:vsplit</code>, the buffer that we view is just the current one.</p> <p>By running any of the buffer commands from <code>:h buffer-list</code>, we can modify which buffer a window is viewing.</p> <p>For an example of this, by running the following commands, we will start editing two files in Vim, open a new window by horizontally splitting, and then view the second buffer in the original window.</p> <pre><code>vim .vimrc .bashrc\n</code></pre> <p>In Vim run: <code>:split</code> and <code>:bnext</code></p>"}, {"location": "vim_tabs/#so-a-tab-is", "title": "So a Tab is\u2026?", "text": "<p>So now that we know what a buffer is and what a window is. Here is what Vim says in the Vim documentation regarding a buffer/window/tab:</p> <p>Summary:</p> <ul> <li>A buffer is the in-memory text of a file.</li> <li>A window is a viewport on a buffer.</li> <li>A tab page is a collection of windows.</li> </ul> <p>According to the documentation, a tab is just a collection of windows. This goes back to our earlier definition in that a tab is really just a layout.</p> <p>A tab is only designed to give you a different layout of windows.</p>"}, {"location": "vim_tabs/#the-tab-problem", "title": "The Tab Problem", "text": "<p>Tabs were only designed to let us have different layouts of windows. They aren't intended to be where an open file lives; an open file is instead loaded into a buffer.</p> <p>If you can view the same buffer across all tabs, how is this like a normal tab in most other editors?</p> <p>If you try to force a single tab to point to a single buffer, that is just futile. Vim just wasn't meant to work like this.</p>"}, {"location": "vim_tabs/#the-buffer-solution", "title": "The Buffer Solution", "text": "<p>To reconcile all of this and learn how to use Vim\u2019s buffers/windows effectively, it might be useful to stop using tabs altogether until you understand how to edit with just using buffers/windows.</p> <p>The first thing I did was install a plugin that allows me to visualize all the buffers open across the top. I use bufferline for this.</p> <p>Instead of replicating tabs across the top like we did in the previous solution, we are instead going to use the power of being able to open many buffers simultaneously without worrying about which ones are open.</p> <p>In my experience, CtrlP gives a powerful fuzzy finder to navigate through the buffers.</p> <p>Instead of worrying about closing buffers and managing your pseudo-tabs that was mentioned in the previous solution, you just open files that you want to edit using CtrlP and don't worry about closing buffers or how many you have opened.</p> <p>When you are done editing a file, you just save it and then open CtrlP and continue onto the next file.</p> <p>CtrlP offers a few different ways to fuzzy find. You can use the following fuzziness:</p> <ul> <li>Find in your current directory.</li> <li>Find within all your open buffers.</li> <li>Find within all your open buffers sorted by Most Recently Used (MRU).</li> <li>Find with a mix of all the above.</li> </ul>"}, {"location": "vim_tabs/#using-tabs-correctly", "title": "Using Tabs Correctly", "text": "<p>This doesn't mean you should stop using tabs altogether. You should just use them how Vim intended you to use them.</p> <p>Instead you should use them to change the layout among windows. Imagine you are working on a C project. It might be helpful to have one tab dedicated to normal editing, but another tab for using a vertical split for the file.h and file.c files to make editing between them easier.</p> <p>Tabs also work really well to divide up what you are working on. You could be working on one part of the project in one tab and another part of the project in another tab.</p> <p>Just remember though, if you are using a single tab for each file, that isn't how tabs in Vim were designed to be used.</p>"}, {"location": "vim_tabs/#default-option-when-switching", "title": "Default option when switching", "text": "<p>The default behavior when trying to switch the buffer is to not allow you to change buffer if it's not saved, but we can change it if we set one of the next options:</p> <ul> <li><code>set hidden</code>: allow to switch buffers even though it's changes aren't saved.</li> <li><code>set autowrite</code>: Auto save when switching buffers.</li> </ul>"}, {"location": "vim_tabs/#share-buffers-and-all-vim-information-between-vim-instances", "title": "Share buffers and all vim information between vim instances.", "text": "<p>This is not my ideal behavior, nvim should let the user use the window manager to manage the windows... duh, instead of vsplitting buffers or using tabs.</p> <p>But sadly as of Nvim 0.1.7 and Vim 8.0 it's not implemented. You have the <code>--server</code> option but it only sends files to the already opened vim instance.  you can't connect two vim instances to the same buffer pool.</p> <p>It's been discussed in neovim 1, 2.</p> <p>Currently gVim cannot have separate 'toplevel' windows for the same process/session. There is a TODO item to implement an inter-process communication system between multiple Vim instances to make it behave as though the separate processes are unified. (See :help todo and search for \"top-level\".)</p> <p>There is an interesting hax formalized in here which I will want to have time to test.</p> <p>Another solution would be to try to use neovim remote</p>"}, {"location": "virtual_assistant/", "title": "Virtual assistant", "text": "<p>Virtual assistant is a software agent that can perform tasks or services for an individual based on commands or questions.</p> <p>Of the open source solutions kalliope is the one I've liked most. I've also looked at mycroft but it seems less oriented to self hosted solutions, although it's possible. Mycroft has a bigger community behind though.</p> <p>To interact with it I may start with the android app, but then I'll probably install a Raspberry pi zero with Pirate Audio and an akaso external mic in the kitchen to speed up the grocy inventory management.</p>"}, {"location": "virtual_assistant/#stt", "title": "STT", "text": "<p>The only self hosted Speech-To-Text (STT) solution available now is CMUSphinx, which is based on pocketsphinx that has 2.8k stars but last update was on 28<sup>th</sup> of March of 2020.</p> <p>The CMUSphinx documentation suggest you to use Vosk based on vosk-api with 1.2k stars and last updated 2 days ago. There is an open issue to support it in kalliope, with already a french proposal.</p> <p>That led me to the issue to support DeepSpeech, Mozilla's STT solution, that has 16.5k stars and updated 3 days ago, so it would be the way to go in my opinion if the existent one fails. Right now there is no support, but this would be the place to start. For spanish, based on the mozilla discourse thread I arrived to DeepSpeech-Polyglot that has taken many datasets such as Common Voice one and generated the models.</p>"}, {"location": "vite/", "title": "Vite", "text": "<p>Vite is a build tool that aims to provide a faster and leaner development experience for modern web projects. It consists of two major parts:</p> <ul> <li> <p>A dev server that provides rich feature enhancements over native ES modules,     for example extremely fast Hot Module Replacement (HMR).</p> </li> <li> <p>A build command that bundles your code with Rollup, pre-configured to output     highly optimized static assets for production.</p> </li> </ul> <p>Vite is opinionated and comes with sensible defaults out of the box, but is also highly extensible via its Plugin API and JavaScript API with full typing support.</p>"}, {"location": "vite/#references", "title": "References", "text": "<ul> <li>Docs</li> </ul>"}, {"location": "vitest/", "title": "Vitest", "text": "<p>Vitest is a blazing fast unit-test framework powered by Vite.</p>"}, {"location": "vitest/#install", "title": "Install", "text": "<p>Add it to your project with:</p> <pre><code>npm install -D vitest\n</code></pre> <p>If you've used Vite, Vitest will read the configuration from the <code>vite.config.js</code> file so add the <code>test</code> property there.</p> <pre><code>/// &lt;reference types=\"vitest\" /&gt;\nimport { defineConfig } from 'vite'\n\nexport default defineConfig({\ntest: {\n// ...\n},\n})\n</code></pre> <p>To run the tests use <code>npx vitest</code>, to see the coverage use <code>npx vitest --coverage</code>.</p>"}, {"location": "vitest/#references", "title": "References", "text": "<ul> <li>Home</li> </ul>"}, {"location": "vscodium/", "title": "VSCodium", "text": "<p>VSCodium are binary releases of VS Code without MS branding/telemetry/licensing.</p>"}, {"location": "vscodium/#references", "title": "References", "text": "<ul> <li>Home</li> <li>Git</li> </ul>"}, {"location": "vue_snippets/", "title": "Vue snippets", "text": ""}, {"location": "vue_snippets/#apply-a-style-to-a-component-given-a-condition", "title": "Apply a style to a component given a condition", "text": "<p>if you use <code>:class</code> you can write javascript code in the value, for example:</p> <pre><code>&lt;b-form-radio\n  class=\"user-retrieve-language p-2\"\n  :class=\"{'font-weight-bold': selected === language.key}\"\n  v-for=\"language in languages\"\n  v-model=\"selected\"\n  :id=\"language.key\"\n  :checked=\"selected === language.key\"\n  :value=\"language.key\"\n&gt;\n</code></pre>"}, {"location": "vue_snippets/#get-assets-url", "title": "Get assets url", "text": "<p>If you're using Vite, you can save the assets such as images or audios in the <code>src/assets</code> directory, and you can get the url with:</p> <pre><code>getImage() {\nreturn new URL(`../assets/pictures/${this.active_id}.jpg`, import.meta.url).href\n},\n</code></pre> <p>This way it will give you the correct url whether you're in the development environment or in production.</p>"}, {"location": "vue_snippets/#play-audio-files", "title": "Play audio files", "text": "<p>You can get the file and save it into a <code>data</code> element with:</p> <pre><code>getAudio() {\nthis.audio = new Audio(new URL(`../assets/audio/${this.active_id}.mp3`, import.meta.url).href)\n},\n</code></pre> <p>You can start playing with <code>this.audio.play()</code>, and stop with <code>this.audio.pause()</code>.</p>"}, {"location": "vue_snippets/#run-function-in-background", "title": "Run function in background", "text": "<p>To achieve that you need to use the javascript method called <code>setInterval()</code>. It\u2019s a simple function that would repeat the same task over and over again. Here\u2019s an example:</p> <pre><code>function myFunction() {\nsetInterval(function(){ alert(\"Hello world\"); }, 3000);\n}\n</code></pre> <p>If you add a call to this method for any button and click on it, it will print Hello world every 3 seconds (3000 milliseconds) until you close the page.</p> <p>In Vue you could do something like:</p> <pre><code>&lt;script&gt;\nexport default {\n  data: () =&gt; ({\n    inbox_retry: undefined\n  }),\n  methods: {\n    retryGetInbox() {\n      this.inbox_retry = setInterval(() =&gt; {\n        if (this.showError) {\n          console.log('Retrying the fetch of the inbox')\n          // Add your code here.\n        } else {\n          clearInterval(this.inbox_retry)\n        }\n      }, 30000)\n    }\n  },\n</code></pre> <p>You can call <code>this.retryGetInbox()</code> whenever you want to start running the function periodically. Once <code>this.showError</code> is <code>false</code>, we stop running the function with <code>clearInterval(this.inbox_retry)</code>.</p>"}, {"location": "vue_snippets/#truncate-text-given-a-height", "title": "Truncate text given a height", "text": "<p>By default css is able to truncate text with the size of the screen but only on one line, if you want to fill up a portion of the screen (specified in number of lines or height css parameter) and then truncate all the text that overflows, you need to use vue-clamp.</p> <p>They have a nice demo in their page where you can see their features.</p>"}, {"location": "vue_snippets/#installation", "title": "Installation", "text": "<p>If you're lucky and this issue has been solved, you can simply:</p> <pre><code>npm i --save vue-clamp\n</code></pre> <p>Else you need to create the Vue component yourself</p> VueClamp.vue <pre><code>&lt;script&gt;\nimport { addListener, removeListener } from \"resize-detector\";\nimport { defineComponent } from \"vue\";\nimport { h } from \"vue\";\n\nexport default defineComponent({\n  name: \"vue-clamp\",\n  props: {\n    tag: {\n      type: String,\n      default: \"div\",\n    },\n    autoresize: {\n      type: Boolean,\n      default: false,\n    },\n    maxLines: Number,\n    maxHeight: [String, Number],\n    ellipsis: {\n      type: String,\n      default: \"\u2026\",\n    },\n    location: {\n      type: String,\n      default: \"end\",\n      validator(value) {\n        return [\"start\", \"middle\", \"end\"].indexOf(value) !== -1;\n      },\n    },\n    expanded: Boolean,\n  },\n  data() {\n    return {\n      offset: null,\n      text: this.getText(),\n      localExpanded: !!this.expanded,\n    };\n  },\n  computed: {\n    clampedText() {\n      if (this.location === \"start\") {\n        return this.ellipsis + (this.text.slice(0, this.offset) || \"\").trim();\n      } else if (this.location === \"middle\") {\n        const split = Math.floor(this.offset / 2);\n        return (\n          (this.text.slice(0, split) || \"\").trim() +\n          this.ellipsis +\n          (this.text.slice(-split) || \"\").trim()\n        );\n      }\n\n      return (this.text.slice(0, this.offset) || \"\").trim() + this.ellipsis;\n    },\n    isClamped() {\n      if (!this.text) {\n        return false;\n      }\n      return this.offset !== this.text.length;\n    },\n    realText() {\n      return this.isClamped ? this.clampedText : this.text;\n    },\n    realMaxHeight() {\n      if (this.localExpanded) {\n        return null;\n      }\n      const { maxHeight } = this;\n      if (!maxHeight) {\n        return null;\n      }\n      return typeof maxHeight === \"number\" ? `${maxHeight}px` : maxHeight;\n    },\n  },\n  watch: {\n    expanded(val) {\n      this.localExpanded = val;\n    },\n    localExpanded(val) {\n      if (val) {\n        this.clampAt(this.text.length);\n      } else {\n        this.update();\n      }\n      if (this.expanded !== val) {\n        this.$emit(\"update:expanded\", val);\n      }\n    },\n    isClamped: {\n      handler(val) {\n        this.$nextTick(() =&gt; this.$emit(\"clampchange\", val));\n      },\n      immediate: true,\n    },\n  },\n  mounted() {\n    this.init();\n\n    this.$watch(\n      (vm) =&gt; [vm.maxLines, vm.maxHeight, vm.ellipsis, vm.isClamped].join(),\n      this.update\n    );\n    this.$watch((vm) =&gt; [vm.tag, vm.text, vm.autoresize].join(), this.init);\n  },\n  updated() {\n    this.text = this.getText();\n    this.applyChange();\n  },\n  beforeUnmount() {\n    this.cleanUp();\n  },\n  methods: {\n    init() {\n      const contents = this.$slots.default();\n\n      if (!contents) {\n        return;\n      }\n\n      this.offset = this.text.length;\n\n      this.cleanUp();\n\n      if (this.autoresize) {\n        addListener(this.$el, this.update);\n        this.unregisterResizeCallback = () =&gt; {\n          removeListener(this.$el, this.update);\n        };\n      }\n      this.update();\n    },\n    update() {\n      if (this.localExpanded) {\n        return;\n      }\n      this.applyChange();\n      if (this.isOverflow() || this.isClamped) {\n        this.search();\n      }\n    },\n    expand() {\n      this.localExpanded = true;\n    },\n    collapse() {\n      this.localExpanded = false;\n    },\n    toggle() {\n      this.localExpanded = !this.localExpanded;\n    },\n    getLines() {\n      return Object.keys(\n        Array.prototype.slice\n          .call(this.$refs.content.getClientRects())\n          .reduce((prev, { top, bottom }) =&gt; {\n            const key = `${top}/${bottom}`;\n            if (!prev[key]) {\n              prev[key] = true;\n            }\n            return prev;\n          }, {})\n      ).length;\n    },\n    isOverflow() {\n      if (!this.maxLines &amp;&amp; !this.maxHeight) {\n        return false;\n      }\n\n      if (this.maxLines) {\n        if (this.getLines() &gt; this.maxLines) {\n          return true;\n        }\n      }\n\n      if (this.maxHeight) {\n        if (this.$el.scrollHeight &gt; this.$el.offsetHeight) {\n          return true;\n        }\n      }\n      return false;\n    },\n    getText() {\n      // Look for the first non-empty text node\n      const [content] = (this.$slots.default() || []).filter(\n        (node) =&gt; !node.tag &amp;&amp; !node.isComment\n      );\n      return content ? content.children : \"\";\n    },\n    moveEdge(steps) {\n      this.clampAt(this.offset + steps);\n    },\n    clampAt(offset) {\n      this.offset = offset;\n      this.applyChange();\n    },\n    applyChange() {\n      this.$refs.text.textContent = this.realText;\n    },\n    stepToFit() {\n      this.fill();\n      this.clamp();\n    },\n    fill() {\n      while (\n        (!this.isOverflow() || this.getLines() &lt; 2) &amp;&amp;\n        this.offset &lt; this.text.length\n      ) {\n        this.moveEdge(1);\n      }\n    },\n    clamp() {\n      while (this.isOverflow() &amp;&amp; this.getLines() &gt; 1 &amp;&amp; this.offset &gt; 0) {\n        this.moveEdge(-1);\n      }\n    },\n    search(...range) {\n      const [from = 0, to = this.offset] = range;\n      if (to - from &lt;= 3) {\n        this.stepToFit();\n        return;\n      }\n      const target = Math.floor((to + from) / 2);\n      this.clampAt(target);\n      if (this.isOverflow()) {\n        this.search(from, target);\n      } else {\n        this.search(target, to);\n      }\n    },\n    cleanUp() {\n      if (this.unregisterResizeCallback) {\n        this.unregisterResizeCallback();\n      }\n    },\n  },\n  render() {\n    const contents = [\n      h(\n        \"span\",\n        {\n          ref: \"text\",\n          attrs: {\n            \"aria-label\": this.text?.trim(),\n          },\n        },\n        this.realText\n      ),\n    ];\n\n    const { expand, collapse, toggle } = this;\n    const scope = {\n      expand,\n      collapse,\n      toggle,\n      clamped: this.isClamped,\n      expanded: this.localExpanded,\n    };\n    const before = this.$slots.before\n      ? this.$slots.before(scope)\n      : this.$slots.before;\n    if (before) {\n      contents.unshift(...(Array.isArray(before) ? before : [before]));\n    }\n    const after = this.$slots.after\n      ? this.$slots.after(scope)\n      : this.$slots.after;\n    if (after) {\n      contents.push(...(Array.isArray(after) ? after : [after]));\n    }\n    const lines = [\n      h(\n        \"span\",\n        {\n          style: {\n            boxShadow: \"transparent 0 0\",\n          },\n          ref: \"content\",\n        },\n        contents\n      ),\n    ];\n    return h(\n      this.tag,\n      {\n        style: {\n          maxHeight: this.realMaxHeight,\n          overflow: \"hidden\",\n        },\n      },\n      lines\n    );\n  },\n});\n&lt;/script&gt;\n</code></pre>"}, {"location": "vue_snippets/#usage", "title": "Usage", "text": "<p>If you were able to install it with <code>npm</code>, use:</p> <pre><code>&lt;template&gt;\n&lt;v-clamp autoresize :max-lines=\"3\"&gt;{{ text }}&lt;/v-clamp&gt;\n&lt;/template&gt;\n\n&lt;script&gt;\nimport VClamp from 'vue-clamp'\n\nexport default {\n  components: {\n    VClamp\n  },\n  data () {\n    return {\n      text: 'Some very very long text content.'\n    }\n  }\n}\n&lt;/script&gt;\n</code></pre> <p>Else use:</p> <pre><code>&lt;template&gt;\n  &lt;VueClamp maxHeight=\"30vh\"&gt;\n  {{ text }}\n  &lt;/VueClamp&gt;\n&lt;/template&gt;\n\n&lt;script&gt;\nimport VueClamp from './VueClamp.vue'\n\nexport default {\n  components: {\n    VueClamp\n  },\n  data () {\n    return {\n      text: 'Some very very long text content.'\n    }\n  }\n}\n&lt;/script&gt;\n</code></pre>"}, {"location": "vue_snippets/#display-time-ago-from-timestamp", "title": "Display time ago from timestamp", "text": "<p>Install with:</p> <pre><code>npm install vue2-timeago@next\n</code></pre>"}, {"location": "vuejs/", "title": "Vue.js", "text": "<p>Vue.js is a progressive JavaScript framework for building user interfaces. It builds on top of standard HTML, CSS and JavaScript, and provides a declarative and component-based programming model that helps you efficiently develop user interfaces, be it simple or complex.</p> <p>Here is a minimal example:</p> <pre><code>import { createApp } from 'vue'\n\ncreateApp({\ndata() {\nreturn {\ncount: 0\n}\n}\n}).mount('#app')\n</code></pre> <pre><code>&lt;div id=\"app\"&gt;\n  &lt;button @click=\"count++\"&gt;\n    Count is: {{ count }}\n  &lt;/button&gt;\n&lt;/div&gt;\n</code></pre> <p>The above example demonstrates the two core features of Vue:</p> <ul> <li> <p>Declarative Rendering: Vue extends standard HTML with a template syntax that     allows us to declaratively describe HTML output based on JavaScript state.</p> </li> <li> <p>Reactivity: Vue automatically tracks JavaScript state changes and     efficiently updates the DOM when changes happen.</p> </li> </ul>"}, {"location": "vuejs/#features", "title": "Features", "text": ""}, {"location": "vuejs/#single-file-components", "title": "Single file components", "text": "<p>Single-File Component (also known as <code>*.vue</code> files, abbreviated as SFC) encapsulates the component's logic (JavaScript), template (HTML), and styles (CSS) in a single file. Here's the previous example, written in SFC format:</p> <pre><code>&lt;script&gt;\nexport default {\n  data() {\n    return {\n      count: 0\n    }\n  }\n}\n&lt;/script&gt;\n\n&lt;template&gt;\n  &lt;button @click=\"count++\"&gt;Count is: {{ count }}&lt;/button&gt;\n&lt;/template&gt;\n\n&lt;style scoped&gt;\nbutton {\n  font-weight: bold;\n}\n&lt;/style&gt;\n</code></pre>"}, {"location": "vuejs/#api-styles", "title": "API Styles", "text": "<p>Vue components can be authored in two different API styles: Options API and Composition API.</p>"}, {"location": "vuejs/#options-api", "title": "Options API", "text": "<p>With Options API, we define a component's logic using an object of options such as <code>data</code>, <code>methods</code>, and <code>mounted</code>. Properties defined by options are exposed on this inside functions, which points to the component instance:</p> <pre><code>&lt;script&gt;\nexport default {\n  // Properties returned from data() becomes reactive state\n  // and will be exposed on `this`.\n  data() {\n    return {\n      count: 0\n    }\n  },\n\n  // Methods are functions that mutate state and trigger updates.\n  // They can be bound as event listeners in templates.\n  methods: {\n    increment() {\n      this.count++\n    }\n  },\n\n  // Lifecycle hooks are called at different stages\n  // of a component's lifecycle.\n  // This function will be called when the component is mounted.\n  mounted() {\n    console.log(`The initial count is ${this.count}.`)\n  }\n}\n&lt;/script&gt;\n\n&lt;template&gt;\n  &lt;button @click=\"increment\"&gt;Count is: {{ count }}&lt;/button&gt;\n&lt;/template&gt;\n</code></pre> <p>The Options API is centered around the concept of a \"component instance\" (<code>this</code> as seen in the example), which typically aligns better with a class-based mental model for users coming from OOP language backgrounds. It is also more beginner-friendly by abstracting away the reactivity details and enforcing code organisation via option groups.</p>"}, {"location": "vuejs/#composition-api", "title": "Composition API", "text": "<p>With Composition API, we define a component's logic using imported API functions. In SFCs, Composition API is typically used with <code>&lt;script setup&gt;</code>. The setup attribute is a hint that makes Vue perform compile-time transforms that allow us to use Composition API with less boilerplate. For example, imports and top-level variables / functions declared in <code>&lt;script setup&gt;</code> are directly usable in the template.</p> <p>Here is the same component, with the exact same template, but using Composition API and <code>&lt;script setup&gt;</code> instead:</p> <pre><code>&lt;script setup&gt;\nimport { ref, onMounted } from 'vue'\n\n// reactive state\nconst count = ref(0)\n\n// functions that mutate state and trigger updates\nfunction increment() {\n  count.value++\n}\n\n// lifecycle hooks\nonMounted(() =&gt; {\n  console.log(`The initial count is ${count.value}.`)\n})\n&lt;/script&gt;\n\n&lt;template&gt;\n  &lt;button @click=\"increment\"&gt;Count is: {{ count }}&lt;/button&gt;\n&lt;/template&gt;\n</code></pre> <p>The Composition API is centered around declaring reactive state variables directly in a function scope, and composing state from multiple functions together to handle complexity. It is more free-form, and requires understanding of how reactivity works in Vue to be used effectively. In return, its flexibility enables more powerful patterns for organizing and reusing logic.</p>"}, {"location": "vuejs/#initialize-a-project", "title": "Initialize a project", "text": "<p>To create a build-tool-enabled Vue project on your machine, run the following command in your command line. If you don't have <code>npm</code> follow these instructions.</p> <pre><code>npm init vue@latest\n</code></pre> <p>This command will install and execute create-vue, the official Vue project scaffolding tool. You will be presented with prompts for a number of optional features such as TypeScript and testing support. If you are unsure about an option, simply choose <code>No</code>. Follow their instructions.</p> <p>Once the project is created, follow the instructions to install dependencies and start the dev server:</p> <pre><code>cd &lt;your-project-name&gt;\nnpm install\nnpm run dev\n</code></pre> <p>When you are ready to ship your app to production, run the following:</p> <pre><code>npm run build\n</code></pre>"}, {"location": "vuejs/#the-basics", "title": "The basics", "text": ""}, {"location": "vuejs/#creating-a-vue-application", "title": "Creating a Vue Application", "text": "<p>Every Vue application starts by creating a new application instance with the <code>createApp</code> function:</p> <pre><code>import { createApp } from 'vue'\n\nconst app = createApp({\n  /* root component options */\n})\n</code></pre> <p>The object we are passing into <code>createApp</code> is in fact a component. Every app requires a \"root component\" that can contain other components as its children.</p> <p>If you are using Single-File Components, we typically import the root component from another file:</p> <pre><code>import { createApp } from 'vue'\n// import the root component App from a single-file component.\nimport App from './App.vue'\n\nconst app = createApp(App)\n</code></pre> <p>An application instance won't render anything until its <code>.mount()</code> method is called. It expects a \"container\" argument, which can either be an actual DOM element or a selector string:</p> <pre><code>&lt;div id=\"app\"&gt;&lt;/div&gt;\n</code></pre> <pre><code>app.mount('#app')\n</code></pre> <p>The content of the app's root component will be rendered inside the container element. The container element itself is not considered part of the app.</p> <p>The <code>.mount()</code> method should always be called after all app configurations and asset registrations are done. Also note that its return value, unlike the asset registration methods, is the root component instance instead of the application instance.</p> <p>You are not limited to a single application instance on the same page. The <code>createApp</code> API allows multiple Vue applications to co-exist on the same page, each with its own scope for configuration and global assets:</p> <pre><code>const app1 = createApp({\n/* ... */\n})\napp1.mount('#container-1')\n\nconst app2 = createApp({\n/* ... */\n})\napp2.mount('#container-2')\n</code></pre>"}, {"location": "vuejs/#app-configurations", "title": "App configurations", "text": "<p>The application instance exposes a <code>.config</code> object that allows us to configure a few app-level options, for example defining an app-level error handler that captures errors from all descendent components:</p> <pre><code>app.config.errorHandler = (err) =&gt; {\n/* handle error */\n}\n</code></pre> <p>The application instance also provides a few methods for registering app-scoped assets. For example, registering a component:</p> <pre><code>app.component('TodoDeleteButton', TodoDeleteButton)\n</code></pre> <p>This makes the <code>TodoDeleteButton</code> available for use anywhere in our app.</p> <p>You can use also environment variables</p>"}, {"location": "vuejs/#declarative-rendering", "title": "Declarative rendering", "text": "<p>The core feature of Vue is declarative rendering: using a template syntax that extends HTML, we can describe how the HTML should look like based on JavaScript state. When the state changes, the HTML updates automatically.</p> <p>State that can trigger updates when changed are considered reactive. In Vue, reactive state is held in components.</p> <p>We can declare reactive state using the data component option, which should be a function that returns an object:</p> <pre><code>export default {\ndata() {\nreturn {\nmessage: 'Hello World!'\n}\n}\n}\n</code></pre> <p>The message property will be made available in the template. This is how we can render dynamic text based on the value of message, using mustaches syntax:</p> <pre><code>&lt;h1&gt;{{ message }}&lt;/h1&gt;\n</code></pre> <p>The double mustaches interprets the data as plain text, not HTML. In order to output real HTML, you will need to use the <code>v-html</code> directive, although you should try to avoid it for security reasons.</p> <p>Directives are prefixed with <code>v-</code> to indicate that they are special attributes provided by Vue, they apply special reactive behavior to the rendered DOM.</p>"}, {"location": "vuejs/#attribute-bindings", "title": "Attribute bindings", "text": "<p>To bind an attribute to a dynamic value, we use the <code>v-bind</code> directive:</p> <pre><code>&lt;div v-bind:id=\"dynamicId\"&gt;&lt;/div&gt;\n</code></pre> <p>A directive is a special attribute that starts with the <code>v-</code> prefix. They are part of Vue's template syntax. Similar to text interpolations, directive values are JavaScript expressions that have access to the component's state.</p> <p>The part after the colon (<code>:id</code>) is the \"argument\" of the directive. Here, the element's <code>id</code> attribute will be synced with the <code>dynamicId</code> property from the component's state.</p> <p>Because <code>v-bind</code> is used so frequently, it has a dedicated shorthand syntax:</p> <pre><code>&lt;div :id=\"dynamicId\"&gt;&lt;/div&gt;\n</code></pre>"}, {"location": "vuejs/#class-binding", "title": "Class binding", "text": "<p>For example to turn the <code>h1</code> in red:</p> <pre><code>&lt;script&gt;\nexport default {\n  data() {\n    return {\n      titleClass: 'title'\n    }\n  }\n}\n&lt;/script&gt;\n\n&lt;template&gt;\n  &lt;h1 :class='titleClass'&gt;Make me red&lt;/h1&gt; &lt;!-- add dynamic class binding here --&gt;\n&lt;/template&gt;\n\n&lt;style&gt;\n.title {\n  color: red;\n}\n&lt;/style&gt;\n</code></pre> <p>You can have multiple classes toggled by having more fields in the object. In addition, the <code>:class</code> directive can also co-exist with the plain class attribute. So given the following state:</p> <pre><code>data() {\nreturn {\nisActive: true,\nhasError: false\n}\n}\n</code></pre> <p>And the following template:</p> <pre><code>&lt;div\n  class=\"static\"\n  :class=\"{ active: isActive, 'text-danger': hasError }\"\n&gt;&lt;/div&gt;\n</code></pre> <p>It will render:</p> <pre><code>&lt;div class=\"static active\"&gt;&lt;/div&gt;\n</code></pre> <p>When <code>isActive</code> or <code>hasError</code> changes, the class list will be updated accordingly. For example, if <code>hasError</code> becomes true, the class list will become <code>static active text-danger</code>.</p> <p>The bound object doesn't have to be inline:</p> <pre><code>data() {\nreturn {\nclassObject: {\nactive: true,\n'text-danger': false\n}\n}\n}\n</code></pre> <pre><code>&lt;div :class=\"classObject\"&gt;&lt;/div&gt;\n</code></pre> <p>This will render the same result. We can also bind to a computed property that returns an object. This is a common and powerful pattern:</p> <pre><code>data() {\nreturn {\nisActive: true,\nerror: null\n}\n},\ncomputed: {\nclassObject() {\nreturn {\nactive: this.isActive &amp;&amp; !this.error,\n'text-danger': this.error &amp;&amp; this.error.type === 'fatal'\n}\n}\n}\n</code></pre> <pre><code>&lt;div :class=\"classObject\"&gt;&lt;/div&gt;\n</code></pre>"}, {"location": "vuejs/#style-binding", "title": "Style binding", "text": "<p><code>:style</code> supports binding to JavaScript object values.</p> <pre><code>data() {\nreturn {\nactiveColor: 'red',\nfontSize: 30\n}\n}\n</code></pre> <pre><code>&lt;div :style=\"{ color: activeColor, fontSize: fontSize + 'px' }\"&gt;&lt;/div&gt;\n</code></pre> <p>It is often a good idea to bind to a style object directly so that the template is cleaner:</p> <pre><code>data() {\nreturn {\nstyleObject: {\ncolor: 'red',\nfontSize: '13px'\n}\n}\n}\n</code></pre> <pre><code>&lt;div :style=\"styleObject\"&gt;&lt;/div&gt;\n</code></pre> <p>Again, object style binding is often used in conjunction with computed properties that return objects.</p>"}, {"location": "vuejs/#event-listeners", "title": "Event listeners", "text": "<p>We can listen to DOM events using the <code>v-on</code> directive:</p> <pre><code>&lt;button v-on:click=\"increment\"&gt;{{ count }}&lt;/button&gt;\n</code></pre> <p>Due to its frequent use, <code>v-on</code> also has a shorthand syntax:</p> <pre><code>&lt;button @click=\"increment\"&gt;{{ count }}&lt;/button&gt;\n</code></pre> <p>Here, <code>increment</code> references a function declared using the methods option:</p> <pre><code>export default {\ndata() {\nreturn {\ncount: 0\n}\n},\nmethods: {\nincrement() {\n// update component state\nthis.count++\n}\n}\n}\n</code></pre> <p>Inside a method, we can access the component instance using <code>this</code>. The component instance exposes the data properties declared by data. We can update the component state by mutating these properties.</p> <p>You should avoid using arrow functions when defining methods, as that prevents Vue from binding the appropriate this value.</p>"}, {"location": "vuejs/#event-modifiers", "title": "Event modifiers", "text": "<p>It is a very common need to call <code>event.preventDefault()</code> or <code>event.stopPropagation()</code> inside event handlers. Although we can do this easily inside methods, it would be better if the methods can be purely about data logic rather than having to deal with DOM event details.</p> <p>To address this problem, Vue provides event modifiers for <code>v-on</code>. Recall that modifiers are directive postfixes denoted by a dot.</p> <ul> <li><code>.stop</code></li> <li><code>.prevent</code></li> <li><code>.self</code></li> <li><code>.capture</code></li> <li><code>.once</code></li> <li><code>.passive</code></li> </ul> <pre><code>&lt;!-- the click event's propagation will be stopped --&gt;\n&lt;a @click.stop=\"doThis\"&gt;&lt;/a&gt;\n\n&lt;!-- the submit event will no longer reload the page --&gt;\n&lt;form @submit.prevent=\"onSubmit\"&gt;&lt;/form&gt;\n\n&lt;!-- modifiers can be chained --&gt;\n&lt;a @click.stop.prevent=\"doThat\"&gt;&lt;/a&gt;\n\n&lt;!-- just the modifier --&gt;\n&lt;form @submit.prevent&gt;&lt;/form&gt;\n\n&lt;!-- only trigger handler if event.target is the element itself --&gt;\n&lt;!-- i.e. not from a child element --&gt;\n&lt;div @click.self=\"doThat\"&gt;...&lt;/div&gt;\n\n&lt;!-- use capture mode when adding the event listener --&gt;\n&lt;!-- i.e. an event targeting an inner element is handled here before being handled by that element --&gt;\n&lt;div @click.capture=\"doThis\"&gt;...&lt;/div&gt;\n\n&lt;!-- the click event will be triggered at most once --&gt;\n&lt;a @click.once=\"doThis\"&gt;&lt;/a&gt;\n\n&lt;!-- the scroll event's default behavior (scrolling) will happen --&gt;\n&lt;!-- immediately, instead of waiting for `onScroll` to complete  --&gt;\n&lt;!-- in case it contains `event.preventDefault()`                --&gt;\n&lt;div @scroll.passive=\"onScroll\"&gt;...&lt;/div&gt;\n</code></pre>"}, {"location": "vuejs/#key-modifiers", "title": "Key Modifiers", "text": "<p>When listening for keyboard events, we often need to check for specific keys. Vue allows adding key modifiers for <code>v-on</code> or <code>@</code> when listening for key events:</p> <pre><code>&lt;!-- only call `vm.submit()` when the `key` is `Enter` --&gt;\n&lt;input @keyup.enter=\"submit\" /&gt;\n</code></pre> <p>You can directly use any valid key names exposed via <code>KeyboardEvent.key</code> as modifiers by converting them to kebab-case.</p> <pre><code>&lt;input @keyup.page-down=\"onPageDown\" /&gt;\n</code></pre> <p>Vue provides aliases for the most commonly used keys:</p> <ul> <li><code>.enter</code></li> <li><code>.tab</code></li> <li><code>.delete</code> (captures both \"Delete\" and \"Backspace\" keys)</li> <li><code>.esc</code></li> <li><code>.space</code></li> <li><code>.up</code></li> <li><code>.down</code></li> <li><code>.left</code></li> <li><code>.right</code></li> </ul> <p>You can use the following modifiers to trigger mouse or keyboard event listeners only when the corresponding modifier key is pressed:</p> <ul> <li><code>.ctrl</code></li> <li><code>.alt</code></li> <li><code>.shift</code></li> <li><code>.meta</code></li> </ul> <p>For example:</p> <pre><code>&lt;!-- Alt + Enter --&gt;\n&lt;input @keyup.alt.enter=\"clear\" /&gt;\n\n&lt;!-- Ctrl + Click --&gt;\n&lt;div @click.ctrl=\"doSomething\"&gt;Do something&lt;/div&gt;\n</code></pre> <p>The <code>.exact</code> modifier allows control of the exact combination of system modifiers needed to trigger an event.</p> <pre><code>&lt;!-- this will fire even if Alt or Shift is also pressed --&gt;\n&lt;button @click.ctrl=\"onClick\"&gt;A&lt;/button&gt;\n\n&lt;!-- this will only fire when Ctrl and no other keys are pressed --&gt;\n&lt;button @click.ctrl.exact=\"onCtrlClick\"&gt;A&lt;/button&gt;\n\n&lt;!-- this will only fire when no system modifiers are pressed --&gt;\n&lt;button @click.exact=\"onClick\"&gt;A&lt;/button&gt;\n</code></pre>"}, {"location": "vuejs/#mouse-button-modifiers", "title": "Mouse Button Modifiers", "text": "<ul> <li><code>.left</code></li> <li><code>.right</code></li> <li><code>.middle</code></li> </ul> <p>These modifiers restrict the handler to events triggered by a specific mouse button.</p>"}, {"location": "vuejs/#form-bindings", "title": "Form bindings", "text": ""}, {"location": "vuejs/#basic-usage", "title": "Basic usage", "text": ""}, {"location": "vuejs/#text", "title": "Text", "text": "<p>Using <code>v-bind</code> and <code>v-on</code> together, we can create two-way bindings on form input elements:</p> <pre><code>&lt;input :value=\"text\" @input=\"onInput\"&gt;\n&lt;p&gt;{{ text }}&lt;/p&gt;\n</code></pre> <pre><code>methods: {\nonInput(e) {\n// a v-on handler receives the native DOM event\n// as the argument.\nthis.text = e.target.value\n}\n}\n</code></pre> <p>To simplify two-way bindings, Vue provides a directive, <code>v-model</code>, which is essentially a syntax sugar for the above:</p> <pre><code>&lt;input v-model=\"text\"&gt;\n</code></pre> <p><code>v-model</code> automatically syncs the <code>&lt;input&gt;</code>'s value with the bound state, so we no longer need to use a event handler for that.</p> <p><code>v-model</code> works not only on text inputs, but also other input types such as checkboxes, radio buttons, and select dropdowns.</p>"}, {"location": "vuejs/#multiline-text", "title": "Multiline text", "text": "<pre><code>&lt;span&gt;Multiline message is:&lt;/span&gt;\n&lt;p style=\"white-space: pre-line;\"&gt;{{ message }}&lt;/p&gt;\n&lt;textarea v-model=\"message\" placeholder=\"add multiple lines\"&gt;&lt;/textarea&gt;\n</code></pre>"}, {"location": "vuejs/#checkbox", "title": "Checkbox", "text": "<pre><code>&lt;input type=\"checkbox\" id=\"checkbox\" v-model=\"checked\" /&gt;\n&lt;label for=\"checkbox\"&gt;{{ checked }}&lt;/label&gt;\n</code></pre> <p>We can also bind multiple checkboxes to the same array or Set value:</p> <pre><code>export default {\ndata() {\nreturn {\ncheckedNames: []\n}\n}\n}\n</code></pre> <pre><code>&lt;div&gt;Checked names: {{ checkedNames }}&lt;/div&gt;\n\n&lt;input type=\"checkbox\" id=\"jack\" value=\"Jack\" v-model=\"checkedNames\"&gt;\n&lt;label for=\"jack\"&gt;Jack&lt;/label&gt;\n\n&lt;input type=\"checkbox\" id=\"john\" value=\"John\" v-model=\"checkedNames\"&gt;\n&lt;label for=\"john\"&gt;John&lt;/label&gt;\n\n&lt;input type=\"checkbox\" id=\"mike\" value=\"Mike\" v-model=\"checkedNames\"&gt;\n&lt;label for=\"mike\"&gt;Mike&lt;/label&gt;\n</code></pre>"}, {"location": "vuejs/#radio-checkboxes", "title": "Radio checkboxes", "text": "<pre><code>&lt;div&gt;Picked: {{ picked }}&lt;/div&gt;\n\n&lt;input type=\"radio\" id=\"one\" value=\"One\" v-model=\"picked\" /&gt;\n&lt;label for=\"one\"&gt;One&lt;/label&gt;\n\n&lt;input type=\"radio\" id=\"two\" value=\"Two\" v-model=\"picked\" /&gt;\n&lt;label for=\"two\"&gt;Two&lt;/label&gt;\n</code></pre>"}, {"location": "vuejs/#select", "title": "Select", "text": "<p>Single select:</p> <pre><code>&lt;div&gt;Selected: {{ selected }}&lt;/div&gt;\n\n&lt;select v-model=\"selected\"&gt;\n  &lt;option disabled value=\"\"&gt;Please select one&lt;/option&gt;\n  &lt;option&gt;A&lt;/option&gt;\n  &lt;option&gt;B&lt;/option&gt;\n  &lt;option&gt;C&lt;/option&gt;\n&lt;/select&gt;\n</code></pre> <p>Multiple select (bound to array):</p> <pre><code>&lt;div&gt;Selected: {{ selected }}&lt;/div&gt;\n\n&lt;select v-model=\"selected\" multiple&gt;\n  &lt;option&gt;A&lt;/option&gt;\n  &lt;option&gt;B&lt;/option&gt;\n  &lt;option&gt;C&lt;/option&gt;\n&lt;/select&gt;\n</code></pre> <p>Select options can be dynamically rendered with <code>v-for</code>:</p> <pre><code>export default {\ndata() {\nreturn {\nselected: 'A',\noptions: [\n{ text: 'One', value: 'A' },\n{ text: 'Two', value: 'B' },\n{ text: 'Three', value: 'C' }\n]\n}\n}\n}\n</code></pre> <pre><code>&lt;select v-model=\"selected\"&gt;\n  &lt;option v-for=\"option in options\" :value=\"option.value\"&gt;\n    {{ option.text }}\n  &lt;/option&gt;\n&lt;/select&gt;\n\n&lt;div&gt;Selected: {{ selected }}&lt;/div&gt;\n</code></pre>"}, {"location": "vuejs/#value-bindings", "title": "Value bindings", "text": "<p>For radio, checkbox and select options, the <code>v-model</code> binding values are usually static strings (or booleans for checkbox):.</p> <pre><code>&lt;!-- `picked` is a string \"a\" when checked --&gt;\n&lt;input type=\"radio\" v-model=\"picked\" value=\"a\" /&gt;\n\n&lt;!-- `toggle` is either true or false --&gt;\n&lt;input type=\"checkbox\" v-model=\"toggle\" /&gt;\n\n&lt;!-- `selected` is a string \"abc\" when the first option is selected --&gt;\n&lt;select v-model=\"selected\"&gt;\n  &lt;option value=\"abc\"&gt;ABC&lt;/option&gt;\n&lt;/select&gt;\n</code></pre> <p>But sometimes we may want to bind the value to a dynamic property on the current active instance. We can use <code>v-bind</code> to achieve that. In addition, using <code>v-bind</code> allows us to bind the input value to non-string values.</p>"}, {"location": "vuejs/#checkbox_1", "title": "Checkbox", "text": "<pre><code>&lt;input\n  type=\"checkbox\"\n  v-model=\"toggle\"\n  true-value=\"yes\"\n  false-value=\"no\" /&gt;\n</code></pre> <p><code>true-value</code> and <code>false-value</code> are Vue-specific attributes that only work with <code>v-model</code>. Here the toggle property's value will be set to 'yes' when the box is checked, and set to 'no' when unchecked. You can also bind them to dynamic values using <code>v-bind</code>:</p> <pre><code>&lt;input\n  type=\"checkbox\"\n  v-model=\"toggle\"\n  :true-value=\"dynamicTrueValue\"\n  :false-value=\"dynamicFalseValue\" /&gt;\n</code></pre>"}, {"location": "vuejs/#radio", "title": "Radio", "text": "<pre><code>&lt;input type=\"radio\" v-model=\"pick\" :value=\"first\" /&gt;\n&lt;input type=\"radio\" v-model=\"pick\" :value=\"second\" /&gt;\n</code></pre> <p><code>pick</code> will be set to the value of first when the first radio input is checked, and set to the value of second when the second one is checked.</p>"}, {"location": "vuejs/#select-options", "title": "Select Options", "text": "<pre><code>&lt;select v-model=\"selected\"&gt;\n  &lt;!-- inline object literal --&gt;\n  &lt;option :value=\"{ number: 123 }\"&gt;123&lt;/option&gt;\n&lt;/select&gt;\n</code></pre> <p><code>v-model</code> supports value bindings of non-string values as well! In the above example, when the option is selected, selected will be set to the object literal value of <code>{ number: 123 }</code>.</p>"}, {"location": "vuejs/#form-modifiers", "title": "Form modifiers", "text": ""}, {"location": "vuejs/#lazy", "title": "<code>.lazy</code>", "text": "<p>By default, <code>v-model</code> syncs the input with the data after each input event. You can add the lazy modifier to instead sync after change events:</p> <pre><code>&lt;!-- synced after \"change\" instead of \"input\" --&gt;\n&lt;input v-model.lazy=\"msg\" /&gt;\n</code></pre>"}, {"location": "vuejs/#number", "title": "<code>.number</code>", "text": "<p>If you want user input to be automatically typecast as a number, you can add the number modifier to your <code>v-model</code> managed inputs:</p> <pre><code>&lt;input v-model.number=\"age\" /&gt;\n</code></pre> <p>If the value cannot be parsed with <code>parseFloat()</code>, then the original value is used instead.</p> <p>The number modifier is applied automatically if the input has <code>type=\"number\"</code>.</p>"}, {"location": "vuejs/#trim", "title": "<code>.trim</code>", "text": "<p>If you want whitespace from user input to be trimmed automatically, you can add the trim modifier to your <code>v-model</code> managed inputs:</p> <pre><code>&lt;input v-model.trim=\"msg\" /&gt;\n</code></pre>"}, {"location": "vuejs/#conditional-rendering", "title": "Conditional rendering", "text": "<p>We can use the <code>v-if</code> directive to conditionally render an element:</p> <pre><code>&lt;h1 v-if=\"awesome\"&gt;Vue is awesome!&lt;/h1&gt;\n</code></pre> <p>This <code>&lt;h1&gt;</code> will be rendered only if the value of <code>awesome</code> is truthy. If awesome changes to a falsy value, it will be removed from the DOM.</p> <p>We can also use <code>v-else</code> and <code>v-else-if</code> to denote other branches of the condition:</p> <pre><code>&lt;h1 v-if=\"awesome\"&gt;Vue is awesome!&lt;/h1&gt;\n&lt;h1 v-else&gt;Oh no \ud83d\ude22&lt;/h1&gt;\n</code></pre> <p>Because <code>v-if</code> is a directive, it has to be attached to a single element. But what if we want to toggle more than one element? In this case we can use <code>v-if</code> on a <code>&lt;template&gt;</code> element, which serves as an invisible wrapper. The final rendered result will not include the <code>&lt;template&gt;</code> element.</p> <pre><code>&lt;template v-if=\"ok\"&gt;\n  &lt;h1&gt;Title&lt;/h1&gt;\n  &lt;p&gt;Paragraph 1&lt;/p&gt;\n  &lt;p&gt;Paragraph 2&lt;/p&gt;\n&lt;/template&gt;\n</code></pre> <p>Another option for conditionally displaying an element is the <code>v-show</code> directive. The usage is largely the same:</p> <pre><code>&lt;h1 v-show=\"ok\"&gt;Hello!&lt;/h1&gt;\n</code></pre> <p>The difference is that an element with <code>v-show</code> will always be rendered and remain in the DOM; <code>v-show</code> only toggles the display CSS property of the element.</p> <p><code>v-show</code> doesn't support the <code>&lt;template&gt;</code> element, nor does it work with <code>v-else</code>.</p> <p>Generally speaking, <code>v-if</code> has higher toggle costs while <code>v-show</code> has higher initial render costs. So prefer <code>v-show</code> if you need to toggle something very often, and prefer <code>v-if</code> if the condition is unlikely to change at runtime.</p>"}, {"location": "vuejs/#list-rendering", "title": "List rendering", "text": "<p>We can use the <code>v-for</code> directive to render a list of elements based on a source array:</p> <pre><code>&lt;ul&gt;\n  &lt;li v-for=\"todo in todos\" :key=\"todo.id\"&gt;\n    {{ todo.text }}\n  &lt;/li&gt;\n&lt;/ul&gt;\n</code></pre> <p>Here <code>todo</code> is a local variable representing the array element currently being iterated on. It's only accessible on or inside the <code>v-for</code> element.</p> <p>Notice how we are also giving each todo object a unique <code>id</code>, and binding it as the special key attribute for each <code>&lt;li&gt;</code>. The key allows Vue to accurately move each <code>&lt;li&gt;</code> to match the position of its corresponding object in the array.</p> <p>There are two ways to update the list:</p> <ul> <li> <p>Call mutating methods on the source array:</p> <pre><code>this.todos.push(newTodo)\n</code></pre> </li> <li> <p>Replace the array with a new one:</p> <pre><code>this.todos = this.todos.filter(/* ... */)\n</code></pre> </li> </ul> <p>Example:</p> <pre><code>&lt;script&gt;\n// give each todo a unique id\nlet id = 0\n\nexport default {\n  data() {\n    return {\n      newTodo: '',\n      todos: [\n        { id: id++, text: 'Learn HTML' },\n        { id: id++, text: 'Learn JavaScript' },\n        { id: id++, text: 'Learn Vue' }\n      ]\n    }\n  },\n  methods: {\n    addTodo() {\n      this.todos.push({ id: id++, text: this.newTodo})\n      this.newTodo = ''\n    },\n    removeTodo(todo) {\n      this.todos = this.todos.filter((element) =&gt; element.id != todo.id)\n    }\n  }\n}\n&lt;/script&gt;\n\n&lt;template&gt;\n  &lt;form @submit.prevent=\"addTodo\"&gt;\n    &lt;input v-model=\"newTodo\"&gt;\n    &lt;button&gt;Add Todo&lt;/button&gt;\n  &lt;/form&gt;\n  &lt;ul&gt;\n    &lt;li v-for=\"todo in todos\" :key=\"todo.id\"&gt;\n      {{ todo.text }}\n      &lt;button @click=\"removeTodo(todo)\"&gt;X&lt;/button&gt;\n    &lt;/li&gt;\n  &lt;/ul&gt;\n&lt;/template&gt;\n</code></pre> <p><code>v-for</code> also supports an optional second alias for the index of the current item:</p> <pre><code>data() {\nreturn {\nparentMessage: 'Parent',\nitems: [{ message: 'Foo' }, { message: 'Bar' }]\n}\n}\n</code></pre> <pre><code>&lt;li v-for=\"(item, index) in items\"&gt;\n  {{ parentMessage }} - {{ index }} - {{ item.message }}\n&lt;/li&gt;\n</code></pre> <p>Similar to template <code>v-if</code>, you can also use a <code>&lt;template&gt;</code> tag with <code>v-for</code> to render a block of multiple elements. For example:</p> <pre><code>&lt;ul&gt;\n  &lt;template v-for=\"item in items\"&gt;\n    &lt;li&gt;{{ item.msg }}&lt;/li&gt;\n    &lt;li class=\"divider\" role=\"presentation\"&gt;&lt;/li&gt;\n  &lt;/template&gt;\n&lt;/ul&gt;\n</code></pre> <p>It's not recommended to use <code>v-if</code> and <code>v-for</code> on the same element due to implicit precedence. Instead of:</p> <pre><code>&lt;li v-for=\"todo in todos\" v-if=\"!todo.isComplete\"&gt;\n  {{ todo.name }}\n&lt;/li&gt;\n</code></pre> <p>Use:</p> <pre><code>&lt;template v-for=\"todo in todos\"&gt;\n  &lt;li v-if=\"!todo.isComplete\"&gt;\n    {{ todo.name }}\n  &lt;/li&gt;\n&lt;/template&gt;\n</code></pre>"}, {"location": "vuejs/#v-for-with-an-object", "title": "<code>v-for</code> with an object", "text": "<p>You can also use <code>v-for</code> to iterate through the properties of an object.</p> <pre><code>data() {\nreturn {\nmyObject: {\ntitle: 'How to do lists in Vue',\nauthor: 'Jane Doe',\npublishedAt: '2016-04-10'\n}\n}\n}\n</code></pre> <pre><code>&lt;ul&gt;\n  &lt;li v-for=\"value in myObject\"&gt;\n    {{ value }}\n  &lt;/li&gt;\n&lt;/ul&gt;\n</code></pre> <p>You can also provide a second alias for the property's name:</p> <pre><code>&lt;li v-for=\"(value, key) in myObject\"&gt;\n  {{ key }}: {{ value }}\n&lt;/li&gt;\n</code></pre> <p>And another for the index:</p> <pre><code>&lt;li v-for=\"(value, key, index) in myObject\"&gt;\n  {{ index }}. {{ key }}: {{ value }}\n&lt;/li&gt;\n</code></pre>"}, {"location": "vuejs/#v-for-with-a-range", "title": "v-for with a Range", "text": "<p><code>v-for</code> can also take an integer. In this case it will repeat the template that many times, based on a range of <code>1...n</code>.</p> <pre><code>&lt;span v-for=\"n in 10\"&gt;{{ n }}&lt;/span&gt;\n</code></pre> <p>Note here <code>n</code> starts with an initial value of 1 instead of 0.</p>"}, {"location": "vuejs/#v-for-with-a-component", "title": "<code>v-for</code> with a Component", "text": "<p>You can directly use <code>v-for</code> on a component, like any normal element (don't forget to provide a key):</p> <pre><code>&lt;my-component v-for=\"item in items\" :key=\"item.id\"&gt;&lt;/my-component&gt;\n</code></pre> <p>However, this won't automatically pass any data to the component, because components have isolated scopes of their own. In order to pass the iterated data into the component, we should also use props:</p> <pre><code>&lt;my-component\n  v-for=\"(item, index) in items\"\n  :item=\"item\"\n  :index=\"index\"\n  :key=\"item.id\"\n&gt;&lt;/my-component&gt;\n</code></pre> <p>The reason for not automatically injecting item into the component is because that makes the component tightly coupled to how <code>v-for</code> works. Being explicit about where its data comes from makes the component reusable in other situations.</p>"}, {"location": "vuejs/#computed-property", "title": "Computed Property", "text": "<p>We can declare a property that is reactively computed from other properties using the <code>computed</code> option:</p> <pre><code>export default {\n// ...\ncomputed: {\nfilteredTodos() {\nif (this.hideCompleted) {\nreturn this.todos.filter((t) =&gt; t.done === false)\n} else {\nreturn this.todos\n}\n}\n}\n}\n}\n</code></pre> <p>A computed property tracks other reactive state used in its computation as dependencies. It caches the result and automatically updates it when its dependencies change. So it's better than defining the function as a <code>method</code></p>"}, {"location": "vuejs/#lifecycle-hooks", "title": "Lifecycle hooks", "text": "<p>Each Vue component instance goes through a series of initialization steps when it's created - for example, it needs to set up data observation, compile the template, mount the instance to the DOM, and update the DOM when data changes. Along the way, it also runs functions called lifecycle hooks, giving users the opportunity to add their own code at specific stages.</p> <p>For example, the <code>mounted</code> hook can be used to run code after the component has finished the initial rendering and created the DOM nodes:</p> <pre><code>export default {\nmounted() {\nconsole.log(`the component is now mounted.`)\n}\n}\n</code></pre> <p>There are also other hooks which will be called at different stages of the instance's lifecycle, with the most commonly used being <code>mounted</code>, <code>updated</code>, and <code>unmounted</code>.</p> <p>All lifecycle hooks are called with their <code>this</code> context pointing to the current active instance invoking it. Note this means you should avoid using arrow functions when declaring lifecycle hooks, as you won't be able to access the component instance via this if you do so.</p>"}, {"location": "vuejs/#template-refs", "title": "Template Refs", "text": "<p>While Vue's declarative rendering model abstracts away most of the direct DOM operations for you, there may still be cases where we need direct access to the underlying DOM elements. To achieve this, we can use the special <code>ref</code> attribute:</p> <pre><code>&lt;input ref=\"input\"&gt;\n</code></pre> <p><code>ref</code> allows us to obtain a direct reference to a specific DOM element or child component instance after it's mounted. This may be useful when you want to, for example, programmatically focus an input on component mount, or initialize a 3<sup>rd</sup> party library on an element.</p> <p>The resulting ref is exposed on <code>this.$refs</code>:</p> <pre><code>&lt;script&gt;\nexport default {\nmounted() {\nthis.$refs.input.focus()\n}\n}\n&lt;/script&gt;\n</code></pre> <pre><code>&lt;template&gt;\n  &lt;input ref=\"input\" /&gt;\n&lt;/template&gt;\n</code></pre> <p>Note that you can only access the <code>ref</code> after the component is mounted. If you try to access <code>$refs.input</code> in a template expression, it will be <code>null</code> on the first render. This is because the element doesn't exist until after the first render!</p>"}, {"location": "vuejs/#watchers", "title": "Watchers", "text": "<p>Computed properties allow us to declaratively compute derived values. However, there are cases where we need to perform \"side effects\" in reaction to state changes, for example, mutating the DOM, or changing another piece of state based on the result of an async operation.</p> <p>With Options API, we can use the <code>watch</code> option to trigger a function whenever a reactive property changes:</p> <pre><code>export default {\ndata() {\nreturn {\nquestion: '',\nanswer: 'Questions usually contain a question mark. ;-)'\n}\n},\nwatch: {\n// whenever question changes, this function will run\nquestion(newQuestion, oldQuestion) {\nif (newQuestion.indexOf('?') &gt; -1) {\nthis.getAnswer()\n}\n}\n},\nmethods: {\nasync getAnswer() {\nthis.answer = 'Thinking...'\ntry {\nconst res = await fetch('https://yesno.wtf/api')\nthis.answer = (await res.json()).answer\n} catch (error) {\nthis.answer = 'Error! Could not reach the API. ' + error\n}\n}\n}\n}\n</code></pre> <pre><code>&lt;p&gt;\n  Ask a yes/no question:\n  &lt;input v-model=\"question\" /&gt;\n&lt;/p&gt;\n&lt;p&gt;{{ answer }}&lt;/p&gt;\n</code></pre>"}, {"location": "vuejs/#deep-watchers", "title": "Deep watchers", "text": "<p><code>watch</code> is shallow by default: the callback will only trigger when the watched property has been assigned a new value - it won't trigger on nested property changes. If you want the callback to fire on all nested mutations, you need to use a deep watcher:</p> <pre><code>export default {\nwatch: {\nsomeObject: {\nhandler(newValue, oldValue) {\n// Note: `newValue` will be equal to `oldValue` here\n// on nested mutations as long as the object itself\n// hasn't been replaced.\n},\ndeep: true\n}\n}\n}\n</code></pre> <p>Note<p>\"Deep watch requires traversing all nested properties in the watched object, and can be expensive when used on large data structures. Use it only when necessary and beware of the performance implications.\"</p> </p>"}, {"location": "vuejs/#eager-watchers", "title": "Eager watchers", "text": "<p><code>watch</code> is lazy by default: the callback won't be called until the watched source has changed. But in some cases we may want the same callback logic to be run eagerly, for example, we may want to fetch some initial data, and then re-fetch the data whenever relevant state changes.</p> <p>We can force a watcher's callback to be executed immediately by declaring it using an object with a <code>handler</code> function and the <code>immediate: true</code> option:</p> <pre><code>export default {\n// ...\nwatch: {\nquestion: {\nhandler(newQuestion) {\n// this will be run immediately on component creation.\n},\n// force eager callback execution\nimmediate: true\n}\n}\n// ...\n}\n</code></pre>"}, {"location": "vuejs/#environment-variables", "title": "Environment variables", "text": "<p>If you're using Vue 3 and Vite you can use the environment variables by defining them in <code>.env</code> files.</p> <p>You can specify environment variables by placing the following files in your project root:</p> <ul> <li><code>.env</code>: Loaded in all cases.</li> <li><code>.env.local</code>: Loaded in all cases, ignored by git.</li> <li><code>.env.[mode]</code>: Only loaded in specified mode.</li> <li><code>.env.[mode].local</code>: Only loaded in specified mode, ignored by git.</li> </ul> <p>An env file simply contains <code>key=value</code> pairs of environment variables, by default only variables that start with <code>VITE_</code> will be exposed.:</p> <pre><code>DB_PASSWORD=foobar\nVITE_SOME_KEY=123\n</code></pre> <p>Only <code>VITE_SOME_KEY</code> will be exposed as <code>import.meta.env.VITE_SOME_KEY</code> to your client source code, but <code>DB_PASSWORD</code> will not. So for example in a component you can use:</p> <pre><code>export default {\n  props: {},\n  mounted() {\n    console.log(import.meta.env.VITE_SOME_KEY)\n  },\n  data: () =&gt; ({\n  }),\n}\n</code></pre>"}, {"location": "vuejs/#make-http-requests", "title": "Make HTTP requests", "text": "<p>There are many ways to do requests to external services:</p> <ul> <li>Fetch API</li> <li>Axios</li> </ul>"}, {"location": "vuejs/#fetch-api", "title": "Fetch API", "text": "<p>The Fetch API is a standard API for making HTTP requests on the browser.</p> <p>It a great alternative to the old <code>XMLHttpRequestconstructor</code> for making requests.</p> <p>It supports all kinds of requests, including GET, POST, PUT, PATCH, DELETE, and OPTIONS, which is what most people need.</p> <p>To make a request with the Fetch API, we don\u2019t have to do anything. All we have to do is to make the request directly with the <code>fetch</code> object. For instance, you can write:</p> <pre><code>&lt;template&gt;\n  &lt;div id=\"app\"&gt;\n    {{data}}\n  &lt;/div&gt;\n&lt;/template&gt;&lt;script&gt;export default {\nname: \"App\",\ndata() {\nreturn {\ndata: {}\n}\n},\nbeforeMount(){\nthis.getName();\n},\nmethods: {\nasync getName(){\nconst res = await fetch('https://api.agify.io/?name=michael');\nconst data = await res.json();\nthis.data = data;\n}\n}\n};\n&lt;/script&gt;\n</code></pre> <p>In the code above, we made a simple GET request from an API and then convert the data from JSON to a JavaScript object with the <code>json()</code> method.</p>"}, {"location": "vuejs/#adding-headers", "title": "Adding headers", "text": "<p>Like most HTTP clients, we can send request headers and bodies with the Fetch API.</p> <p>To send a request with HTTP headers, we can write:</p> <pre><code>&lt;template&gt;\n  &lt;div id=\"app\"&gt;\n    &lt;img :src=\"data.src.tiny\"&gt;\n  &lt;/div&gt;\n&lt;/template&gt;&lt;script&gt;\nexport default {\nname: \"App\",\ndata() {\nreturn {\ndata: {\nsrc: {}\n}\n};\n},\nbeforeMount() {\nthis.getPhoto();\n},\nmethods: {\nasync getPhoto() {\nconst headers = new Headers();\nheaders.append(\n\"Authorization\",\n\"api_key\"\n);\nconst request = new Request(\n\"https://api.pexels.com/v1/curated?per_page=11&amp;page=1\",\n{\nmethod: \"GET\",\nheaders,\nmode: \"cors\",\ncache: \"default\"\n}\n);      const res = await fetch(request);\nconst { photos } = await res.json();\nthis.data = photos[0];\n}\n}\n};\n&lt;/script&gt;\n</code></pre> <p>In the code above, we used the <code>Headers</code> constructor, which is used to add requests headers to Fetch API requests.</p> <p>The append method appends our 'Authorization' header to the request.</p> <p>We\u2019ve to set the mode to 'cors' for a cross-domain request and headers is set to the headers object returned by the <code>Headers</code> constructor.</p>"}, {"location": "vuejs/#adding-body-to-a-request", "title": "Adding body to a request", "text": "<p>To make a request body, we can write the following:</p> <pre><code>&lt;template&gt;\n  &lt;div id=\"app\"&gt;\n    &lt;form @submit.prevent=\"createPost\"&gt;\n      &lt;input placeholder=\"name\" v-model=\"post.name\"&gt;\n      &lt;input placeholder=\"title\" v-model=\"post.title\"&gt;\n      &lt;br&gt;\n      &lt;button type=\"submit\"&gt;Create&lt;/button&gt;\n    &lt;/form&gt;\n    {{data}}\n  &lt;/div&gt;\n&lt;/template&gt;&lt;script&gt;\nexport default {\nname: \"App\",\ndata() {\nreturn {\npost: {},\ndata: {}\n};\n},\nmethods: {\nasync createPost() {\nconst request = new Request(\n\"https://jsonplaceholder.typicode.com/posts\",\n{\nmethod: \"POST\",\nmode: \"cors\",\ncache: \"default\",\nbody: JSON.stringify(this.post)\n}\n);      const res = await fetch(request);\nconst data = await res.json();\nthis.data = data;\n}\n}\n};\n&lt;/script&gt;\n</code></pre> <p>In the code above, we made the request by stringifying the this.post object and then sending it with a POST request.</p>"}, {"location": "vuejs/#axios", "title": "Axios", "text": "<p>Axios is a popular HTTP client that works on both browser and Node.js apps.</p> <p>We can install it by running:</p> <pre><code>npm i axios\n</code></pre> <p>Then we can use it to make requests a simple GET request as follows:</p> <pre><code>&lt;template&gt;\n  &lt;div id=\"app\"&gt;{{data}}&lt;/div&gt;\n&lt;/template&gt;&lt;script&gt;\n\nimport axios from 'axios'\n\nexport default {\nname: \"App\",\ndata() {\nreturn {\ndata: {}\n};\n},\nbeforeMount(){\nthis.getName();\n},\nmethods: {\nasync getName(){\nconst { data } = await axios.get(\"https://api.agify.io/?name=michael\");\nthis.data = data;\n}\n}\n};\n&lt;/script&gt;\n</code></pre> <p>In the code above, we call the <code>axios.get</code> method with the URL to make the request.</p> <p>Then we assign the response data to an object.</p>"}, {"location": "vuejs/#adding-headers_1", "title": "Adding headers", "text": "<p>If we want to make a request with headers, we can write:</p> <pre><code>&lt;template&gt;\n  &lt;div id=\"app\"&gt;\n    &lt;img :src=\"data.src.tiny\"&gt;\n  &lt;/div&gt;\n&lt;/template&gt;&lt;script&gt;\n\nimport axios from 'axios'\n\nexport default {\nname: \"App\",\ndata() {\nreturn {\ndata: {}\n};\n},\nbeforeMount() {\nthis.getPhoto();\n},\nmethods: {\nasync getPhoto() {\nconst {\ndata: { photos }\n} = await axios({\nurl: \"https://api.pexels.com/v1/curated?per_page=11&amp;page=1\",\nheaders: {\nAuthorization: \"api_key\"\n}\n});\nthis.data = photos[0];\n}\n}\n};\n&lt;/script&gt;\n</code></pre> <p>In the code above, we made a GET request with our Pexels API key with the axios method, which can be used for making any kind of request.</p> <p>If no request verb is specified, then it\u2019ll be a GET request.</p> <p>As we can see, the code is a bit shorter since we don\u2019t have to create an object with the <code>Headers</code> constructor.</p> <p>If we want to set the same header in multiple requests, we can use a request interceptor to set the header or other config for all requests.</p> <p>For instance, we can rewrite the above example as follows:</p> <pre><code>// main.js:\n\nimport Vue from \"vue\";\nimport App from \"./App.vue\";\nimport axios from 'axios'\n\naxios.interceptors.request.use(\nconfig =&gt; {\nreturn {\n...config,\nheaders: {\nAuthorization: \"api_key\"\n}\n};\n},\nerror =&gt; Promise.reject(error)\n);\n\nVue.config.productionTip = false;\n\nnew Vue({\nrender: h =&gt; h(App)\n}).$mount(\"#app\");\n</code></pre> <pre><code>&lt;template&gt;\n  &lt;div id=\"app\"&gt;\n    &lt;img :src=\"data.src.tiny\"&gt;\n  &lt;/div&gt;\n&lt;/template&gt;&lt;script&gt;\n\nimport axios from 'axios'\n\nexport default {\nname: \"App\",\ndata() {\nreturn {\ndata: {}\n};\n},\nbeforeMount() {\nthis.getPhoto();\n},\nmethods: {\nasync getPhoto() {\nconst {\ndata: { photos }\n} = await axios({\nurl: \"https://api.pexels.com/v1/curated?per_page=11&amp;page=1\"\n});\nthis.data = photos[0];\n}\n}\n};\n&lt;/script&gt;\n\nWe moved the header to `main.js` inside the code for our interceptor.\n\nThe first argument that\u2019s passed into `axios.interceptors.request.use` is\na function for modifying the request config for all requests.\n\nAnd the 2nd argument is an error handler for handling error of all requests.\n\nLikewise, we can configure interceptors for responses as well.\n\n#### Adding body to a request\n\nTo make a POST request with a request body, we can use the `axios.post` method.\n\n```html\n&lt;template&gt;\n  &lt;div id=\"app\"&gt;\n    &lt;form @submit.prevent=\"createPost\"&gt;\n      &lt;input placeholder=\"name\" v-model=\"post.name\"&gt;\n      &lt;input placeholder=\"title\" v-model=\"post.title\"&gt;\n      &lt;br&gt;\n      &lt;button type=\"submit\"&gt;Create&lt;/button&gt;\n    &lt;/form&gt;\n    {{data}}\n  &lt;/div&gt;\n&lt;/template&gt;&lt;script&gt;\n\nimport axios from 'axios'\n\nexport default {\nname: \"App\",\ndata() {\nreturn {\npost: {},\ndata: {}\n};\n},\nmethods: {\nasync createPost() {\nconst { data } = await axios.post(\n\"https://jsonplaceholder.typicode.com/posts\",\nthis.post\n);\nthis.data = data;\n}\n}\n};\n&lt;/script&gt;\n</code></pre> <p>We make the POST request with the <code>axios.post</code> method with the request body in the second argument. Axios also sets the Content-Type header to application/json. This enables web frameworks to automatically parse the data.</p> <p>Then we get back the response data by getting the data property from the resulting response.</p>"}, {"location": "vuejs/#shorthand-methods-for-axios-http-requests", "title": "Shorthand methods for Axios HTTP requests", "text": "<p>Axios also provides a set of shorthand methods for performing different types of requests. The methods are as follows:</p> <ul> <li><code>axios.request(config)</code></li> <li><code>axios.get(url[, config])</code></li> <li><code>axios.delete(url[, config])</code></li> <li><code>axios.head(url[, config])</code></li> <li><code>axios.options(url[, config])</code></li> <li><code>axios.post(url[, data[, config]])</code></li> <li><code>axios.put(url[, data[, config]])</code></li> <li><code>axios.patch(url[, data[, config]])</code></li> </ul> <p>For instance, the following code shows how the previous example could be written using the <code>axios.post()</code> method:</p> <pre><code>axios.post('/login', {\nfirstName: 'Finn',\nlastName: 'Williams'\n})\n.then((response) =&gt; {\nconsole.log(response);\n}, (error) =&gt; {\nconsole.log(error);\n});\n</code></pre> <p>Once an HTTP POST request is made, Axios returns a promise that is either fulfilled or rejected, depending on the response from the backend service.</p> <p>To handle the result, you can use the <code>then()</code>. method. If the promise is fulfilled, the first argument of <code>then()</code> will be called; if the promise is rejected, the second argument will be called. According to the documentation, the fulfillment value is an object containing the following information:</p> <pre><code>{\n// `data` is the response that was provided by the server\ndata: {},\n\n// `status` is the HTTP status code from the server response\nstatus: 200,\n\n// `statusText` is the HTTP status message from the server response\nstatusText: 'OK',\n\n// `headers` the headers that the server responded with\n// All header names are lower cased\nheaders: {},\n\n// `config` is the config that was provided to `axios` for the request\nconfig: {},\n\n// `request` is the request that generated this response\n// It is the last ClientRequest instance in node.js (in redirects)\n// and an XMLHttpRequest instance the browser\nrequest: {}\n}\n</code></pre>"}, {"location": "vuejs/#using-interceptors", "title": "Using interceptors", "text": "<p>One of the key features of Axios is its ability to intercept HTTP requests. HTTP interceptors come in handy when you need to examine or change HTTP requests from your application to the server or vice versa (e.g., logging, authentication, or retrying a failed HTTP request).</p> <p>With interceptors, you won\u2019t have to write separate code for each HTTP request. HTTP interceptors are helpful when you want to set a global strategy for how you handle request and response.</p> <pre><code>axios.interceptors.request.use(config =&gt; {\n// log a message before any HTTP request is sent\nconsole.log('Request was sent');\n\nreturn config;\n});\n\n// sent a GET request\naxios.get('https://api.github.com/users/sideshowbarker')\n.then(response =&gt; {\nconsole.log(response.data);\n});\n</code></pre> <p>In this code, the <code>axios.interceptors.request.use()</code> method is used to define code to be run before an HTTP request is sent. Also, <code>axios.interceptors.response.use()</code> can be used to intercept the response from the server. Let\u2019s say there is a network error; using the response interceptors, you can retry that same request using interceptors.</p>"}, {"location": "vuejs/#handling-errors", "title": "Handling errors", "text": "<p>To catch errors when doing requests you could use:</p> <pre><code>try {\nlet res = await axios.get('/my-api-route');\n\n// Work with the response...\n} catch (error) {\nif (error.response) {\n// The client was given an error response (5xx, 4xx)\nconsole.log(err.response.data);\nconsole.log(err.response.status);\nconsole.log(err.response.headers);\n} else if (error.request) {\n// The client never received a response, and the request was never left\nconsole.log(err.request);\n} else {\n// Anything else\nconsole.log('Error', err.message);\n}\n}\n</code></pre> <p>The differences in the <code>error</code> object, indicate where the request encountered the issue.</p> <ul> <li> <p><code>error.response</code>: If your <code>error</code> object has a <code>response</code> property, it means     that your server returned a 4xx/5xx error. This will assist you choose what     sort of message to return to users.</p> </li> <li> <p><code>error.request</code>: This error is caused by a network error, a hanging backend     that does not respond instantly to each request, unauthorized or     cross-domain requests, and lastly if the backend API returns an error.</p> <p>This occurs when the browser was able to initiate a request but did not receive a valid answer for any reason.</p> </li> <li> <p>Other errors: It's possible that the <code>error</code> object does not have either     a <code>response</code> or <code>request</code> object attached to it. In this case it is implied that     there was an issue in setting up the request, which eventually triggered an     error.</p> <p>For example, this could be the case if you omit the URL parameter from the <code>.get()</code> call, and thus no request was ever made.</p> </li> </ul>"}, {"location": "vuejs/#sending-multiple-requests", "title": "Sending multiple requests", "text": "<p>One of Axios\u2019 more interesting features is its ability to make multiple requests in parallel by passing an array of arguments to the <code>axios.all()</code> method. This method returns a single promise object that resolves only when all arguments passed as an array have resolved.</p> <p>Here\u2019s a simple example of how to use <code>axios.all</code> to make simultaneous HTTP requests:</p> <pre><code>// execute simultaneous requests\naxios.all([\naxios.get('https://api.github.com/users/mapbox'),\naxios.get('https://api.github.com/users/phantomjs')\n])\n.then(responseArr =&gt; {\n//this will be executed only when all requests are complete\nconsole.log('Date created: ', responseArr[0].data.created_at);\nconsole.log('Date created: ', responseArr[1].data.created_at);\n});\n\n// logs:\n// =&gt; Date created:  2011-02-04T19:02:13Z\n// =&gt; Date created:  2017-04-03T17:25:46Z\n</code></pre> <p>This code makes two requests to the GitHub API and then logs the value of the <code>created_at</code> property of each response to the console. Keep in mind that if any of the arguments rejects then the promise will immediately reject with the reason of the first promise that rejects.</p> <p>For convenience, Axios also provides a method called <code>axios.spread()</code> to assign the properties of the response array to separate variables. Here\u2019s how you could use this method:</p> <pre><code>axios.all([\naxios.get('https://api.github.com/users/mapbox'),\naxios.get('https://api.github.com/users/phantomjs')\n])\n.then(axios.spread((user1, user2) =&gt; {\nconsole.log('Date created: ', user1.data.created_at);\nconsole.log('Date created: ', user2.data.created_at);\n}));\n\n// logs:\n// =&gt; Date created:  2011-02-04T19:02:13Z\n// =&gt; Date created:  2017-04-03T17:25:46Z\n</code></pre> <p>The output of this code is the same as the previous example. The only difference is that the <code>axios.spread()</code> method is used to unpack values from the response array.</p>"}, {"location": "vuejs/#veredict", "title": "Veredict", "text": "<p>If you\u2019re working on multiple requests, you\u2019ll find that Fetch requires you to write more code than Axios, even when taking into consideration the setup needed for it. Therefore, for simple requests, Fetch API and Axios are quite the same. However, for more complex requests, Axios is better as it allows you to configure multiple requests in one place.</p> <p>If you're making a simple request use the Fetch API, for the other cases use axios because:</p> <ul> <li>It allows you to configure multiple requests in one place</li> <li>Code is shorter.</li> <li>It allows you to place all the API calls under services so that these can be     reused across components wherever they are     needed.</li> <li>It's easy to set a timeout of the request.</li> <li>It supports HTTP interceptors by befault</li> <li>It does automatic JSON data transformation.</li> <li>It's supported by old browsers, although you can bypass the problem with fetch     too.</li> <li>It has a progress indicator for large files.</li> <li>Supports simultaneous requests by default.</li> </ul> <p>Axios provides an easy-to-use API in a compact package for most of your HTTP communication needs. However, if you prefer to stick with native APIs, nothing stops you from implementing Axios features.</p> <p>For more information read:</p> <ul> <li>How To Make API calls in Vue.JS Applications by Bhargav Bachina</li> <li>Axios vs. fetch(): Which is best for making HTTP requests? by Faraz     Kelhini</li> </ul>"}, {"location": "vuejs/#vue-router", "title": "Vue Router", "text": "<p>Creating a Single-page Application with Vue + Vue Router feels natural, all we need to do is map our components to the routes and let Vue Router know where to render them. Here's a basic example:</p> <pre><code>&lt;script src=\"https://unpkg.com/vue@3\"&gt;&lt;/script&gt;\n&lt;script src=\"https://unpkg.com/vue-router@4\"&gt;&lt;/script&gt;\n\n&lt;div id=\"app\"&gt;\n  &lt;h1&gt;Hello App!&lt;/h1&gt;\n  &lt;p&gt;\n    &lt;!-- use the router-link component for navigation. --&gt;\n    &lt;!-- specify the link by passing the `to` prop. --&gt;\n    &lt;!-- `&lt;router-link&gt;` will render an `&lt;a&gt;` tag with the correct `href` attribute --&gt;\n    &lt;router-link to=\"/\"&gt;Go to Home&lt;/router-link&gt;\n    &lt;router-link to=\"/about\"&gt;Go to About&lt;/router-link&gt;\n  &lt;/p&gt;\n  &lt;!-- route outlet --&gt;\n  &lt;!-- component matched by the route will render here --&gt;\n  &lt;router-view&gt;&lt;/router-view&gt;\n&lt;/div&gt;\n</code></pre> <p>Note how instead of using regular <code>a</code> tags, we use a custom component <code>router-link</code> to create links. This allows Vue Router to change the URL without reloading the page, handle URL generation as well as its encoding.</p> <p><code>router-view</code> will display the component that corresponds to the url. You can put it anywhere to adapt it to your layout.</p> <pre><code>// 1. Define route components.\n// These can be imported from other files\nconst Home = { template: '&lt;div&gt;Home&lt;/div&gt;' }\nconst About = { template: '&lt;div&gt;About&lt;/div&gt;' }\n\n// 2. Define some routes\n// Each route should map to a component.\n// We'll talk about nested routes later.\nconst routes = [\n{ path: '/', component: Home },\n{ path: '/about', component: About },\n]\n\n// 3. Create the router instance and pass the `routes` option\n// You can pass in additional options here, but let's\n// keep it simple for now.\nconst router = VueRouter.createRouter({\n// 4. Provide the history implementation to use. We are using the hash history for simplicity here.\nhistory: VueRouter.createWebHashHistory(),\nroutes, // short for `routes: routes`\n})\n\n// 5. Create and mount the root instance.\nconst app = Vue.createApp({})\n// Make sure to _use_ the router instance to make the\n// whole app router-aware.\napp.use(router)\n\napp.mount('#app')\n\n// Now the app has started!\n</code></pre> <p>By calling <code>app.use(router)</code>, we get access to it as <code>this.$router</code> as well as the current route as <code>this.$route</code> inside of any component:</p> <pre><code>// Home.vue\nexport default {\n  computed: {\n    username() {\n      // We will see what `params` is shortly\n      return this.$route.params.username\n    },\n  },\n  methods: {\n    goToDashboard() {\n      if (isAuthenticated) {\n        this.$router.push('/dashboard')\n      } else {\n        this.$router.push('/login')\n      }\n    },\n  },\n}\n</code></pre> <p>To access the router or the route inside the <code>setup</code> function, call the <code>useRouter</code> or <code>useRoute</code> functions.</p>"}, {"location": "vuejs/#dynamic-route-matching-with-params", "title": "Dynamic route matching with params", "text": "<p>Very often we will need to map routes with the given pattern to the same component. For example we may have a User component which should be rendered for all users but with different user IDs. In Vue Router we can use a dynamic segment in the path to achieve that, we call that a <code>param</code>:</p> <pre><code>const User = {\ntemplate: '&lt;div&gt;User&lt;/div&gt;',\n}\n\n// these are passed to `createRouter`\nconst routes = [\n// dynamic segments start with a colon\n{ path: '/users/:id', component: User },\n]\n</code></pre> <p>Now URLs like <code>/users/johnny</code> and <code>/users/jolyne</code> will both map to the same route.</p> <p>A <code>param</code> is denoted by a colon <code>:.</code> When a route is matched, the value of its params will be exposed as <code>this.$route.params</code> in every component. Therefore, we can render the current user ID by updating User's template to this:</p> <pre><code>const User = {\n  template: '&lt;div&gt;User {{ $route.params.id }}&lt;/div&gt;',\n}\n</code></pre> <p>You can have multiple <code>params</code> in the same route, and they will map to corresponding fields on <code>$route.params</code>. Examples:</p> pattern matched path $route.params /users/:username /users/eduardo { username: 'eduardo' } /users/:username/posts/:postId /users/eduardo/posts/123 { username: 'eduardo', postId: '123' } <p>In addition to <code>$route.params</code>, the <code>$route</code> object also exposes other useful information such as <code>$route.query</code> (if there is a query in the URL), <code>$route.hash</code>, etc.</p>"}, {"location": "vuejs/#reacting-to-params-changes", "title": "Reacting to params changes", "text": "<p>One thing to note when using routes with params is that when the user navigates from <code>/users/johnny</code> to <code>/users/jolyne</code>, the same component instance will be reused. Since both routes render the same component, this is more efficient than destroying the old instance and then creating a new one. However, this also means that the lifecycle hooks of the component will not be called.</p> <p>To react to <code>params</code> changes in the same component, you can simply <code>watch</code> anything on the <code>$route</code> object, in this scenario, the <code>$route.params</code>:</p> <pre><code>const User = {\ntemplate: '...',\ncreated() {\nthis.$watch(\n() =&gt; this.$route.params,\n(toParams, previousParams) =&gt; {\n// react to route changes...\n}\n)\n},\n}\n</code></pre> <p>Or, use the <code>beforeRouteUpdate</code> navigation guard, which also allows to cancel the navigation:</p> <pre><code>const User = {\ntemplate: '...',\nasync beforeRouteUpdate(to, from) {\n// react to route changes...\nthis.userData = await fetchUser(to.params.id)\n},\n}\n</code></pre>"}, {"location": "vuejs/#components", "title": "Components", "text": "<p>Components allow us to split the UI into independent and reusable pieces, and think about each piece in isolation. It's common for an app to be organized into a tree of nested components</p>"}, {"location": "vuejs/#defining-a-component", "title": "Defining a component", "text": "<p>When using a build step, we typically define each Vue component in a dedicated file using the <code>.vue</code> extension.</p> <pre><code>&lt;script&gt;\nexport default {\n  data() {\n    return {\n      count: 0\n    }\n  }\n}\n&lt;/script&gt;\n\n&lt;template&gt;\n  &lt;button @click=\"count++\"&gt;You clicked me {{ count }} times.&lt;/button&gt;\n&lt;/template&gt;\n</code></pre>"}, {"location": "vuejs/#using-a-component", "title": "Using a component", "text": "<p>To use a child component, we need to import it in the parent component. Assuming we placed our counter component inside a file called <code>ButtonCounter.vue</code>, the component will be exposed as the file's default export:</p> <pre><code>&lt;script&gt;\nimport ButtonCounter from './ButtonCounter.vue'\n\nexport default {\n  components: {\n    ButtonCounter\n  }\n}\n&lt;/script&gt;\n\n&lt;template&gt;\n  &lt;h1&gt;Here is a child component!&lt;/h1&gt;\n  &lt;ButtonCounter /&gt;\n&lt;/template&gt;\n</code></pre> <p>To expose the imported component to our template, we need to register it with the <code>components</code> option. The component will then be available as a tag using the key it is registered under.</p> <p>Components can be reused as many times as you want:</p> <pre><code>&lt;h1&gt;Here are many child components!&lt;/h1&gt;\n&lt;ButtonCounter /&gt;\n&lt;ButtonCounter /&gt;\n&lt;ButtonCounter /&gt;\n</code></pre> <p>When clicking on the buttons, each one maintains its own, separate count. That's because each time you use a component, a new instance of it is created.</p>"}, {"location": "vuejs/#passing-props", "title": "Passing props", "text": "<p>Props are custom attributes you can register on a component. Vue components require explicit <code>props</code> declaration so that Vue knows what external props passed to the component should be treated as fallthrough attributes.</p> <pre><code>&lt;!-- BlogPost.vue --&gt;\n&lt;script&gt;\nexport default {\n  props: ['title']\n}\n&lt;/script&gt;\n\n&lt;template&gt;\n  &lt;h4&gt;{{ title }}&lt;/h4&gt;\n&lt;/template&gt;\n</code></pre> <p>When a value is passed to a prop attribute, it becomes a property on that component instance. The value of that property is accessible within the template and on the component's <code>this</code> context, just like any other component property.</p> <p>A component can have as many props as you like and, by default, any value can be passed to any prop.</p> <p>Once a prop is registered, you can pass data to it as a custom attribute, like this:</p> <pre><code>&lt;BlogPost title=\"My journey with Vue\" /&gt;\n&lt;BlogPost title=\"Blogging with Vue\" /&gt;\n&lt;BlogPost title=\"Why Vue is so fun\" /&gt;\n</code></pre> <p>In a typical app, however, you'll likely have an array of posts in your parent component:</p> <pre><code>export default {\n// ...\ndata() {\nreturn {\nposts: [\n{ id: 1, title: 'My journey with Vue' },\n{ id: 2, title: 'Blogging with Vue' },\n{ id: 3, title: 'Why Vue is so fun' }\n]\n}\n}\n}\n</code></pre> <p>Then want to render a component for each one, using <code>v-for</code>:</p> <pre><code>&lt;BlogPost\n  v-for=\"post in posts\"\n  :key=\"post.id\"\n  :title=\"post.title\"\n /&gt;\n</code></pre> <p>We declare long prop names using camelCase because this avoids having to use quotes when using them as property keys.</p> <pre><code>export default {\nprops: {\ngreetingMessage: String\n}\n}\n</code></pre> <pre><code>&lt;span&gt;{{ greetingMessage }}&lt;/span&gt;\n</code></pre> <p>However, the convention is using kebab-case when passing props to a child component.</p> <pre><code>&lt;MyComponent greeting-message=\"hello\" /&gt;\n</code></pre>"}, {"location": "vuejs/#passing-different-value-types-on-props", "title": "Passing different value types on props", "text": "<ul> <li> <p>Numbers:</p> <pre><code>&lt;!-- Even though `42` is static, we need v-bind to tell Vue that --&gt;\n&lt;!-- this is a JavaScript expression rather than a string.       --&gt;\n&lt;BlogPost :likes=\"42\" /&gt;\n\n&lt;!-- Dynamically assign to the value of a variable. --&gt;\n&lt;BlogPost :likes=\"post.likes\" /&gt;\n</code></pre> </li> <li> <p>Boolean:     <pre><code>&lt;!-- Including the prop with no value will imply `true`. --&gt;\n&lt;BlogPost is-published /&gt;\n\n&lt;!-- Even though `false` is static, we need v-bind to tell Vue that --&gt;\n&lt;!-- this is a JavaScript expression rather than a string.          --&gt;\n&lt;BlogPost :is-published=\"false\" /&gt;\n\n&lt;!-- Dynamically assign to the value of a variable. --&gt;\n&lt;BlogPost :is-published=\"post.isPublished\" /&gt;\n</code></pre></p> </li> <li> <p>Array</p> <pre><code>&lt;!-- Even though the array is static, we need v-bind to tell Vue that --&gt;\n&lt;!-- this is a JavaScript expression rather than a string.            --&gt;\n&lt;BlogPost :comment-ids=\"[234, 266, 273]\" /&gt;\n\n&lt;!-- Dynamically assign to the value of a variable. --&gt;\n&lt;BlogPost :comment-ids=\"post.commentIds\" /&gt;\n</code></pre> </li> <li> <p>Object</p> <pre><code>&lt;!-- Even though the object is static, we need v-bind to tell Vue that --&gt;\n&lt;!-- this is a JavaScript expression rather than a string.             --&gt;\n&lt;BlogPost\n  :author=\"{\n    name: 'Veronica',\n    company: 'Veridian Dynamics'\n  }\"\n /&gt;\n\n&lt;!-- Dynamically assign to the value of a variable. --&gt;\n&lt;BlogPost :author=\"post.author\" /&gt;\n</code></pre> <p>If you want to pass all the properties of an object as props, you can use v-bind without an argument.</p> <pre><code>export default {\ndata() {\nreturn {\npost: {\nid: 1,\ntitle: 'My Journey with Vue'\n}\n}\n}\n}\n</code></pre> <p>The following template:</p> <pre><code>&lt;BlogPost v-bind=\"post\" /&gt;\n</code></pre> <p>Will be equivalent to:</p> <pre><code>&lt;BlogPost :id=\"post.id\" :title=\"post.title\" /&gt;\n</code></pre> </li> </ul> <p>All props form a one-way-down binding between the child property and the parent one: when the parent property updates, it will flow down to the child, but not the other way around.</p> <p>Every time the parent component is updated, all props in the child component will be refreshed with the latest value. This means you should not attempt to mutate a prop inside a child component.</p>"}, {"location": "vuejs/#one-way-data-flow-in-props", "title": "One-way data flow in props", "text": ""}, {"location": "vuejs/#prop-validation", "title": "Prop validation", "text": "<p>Components can specify requirements for their props, if a requirement is not met, Vue will warn you in the browser's JavaScript console.</p> <pre><code>export default {\nprops: {\n// Basic type check\n//  (`null` and `undefined` values will allow any type)\npropA: Number,\n// Multiple possible types\npropB: [String, Number],\n// Required string\npropC: {\ntype: String,\nrequired: true\n},\n// Number with a default value\npropD: {\ntype: Number,\ndefault: 100\n},\n// Object with a default value\npropE: {\ntype: Object,\n// Object or array defaults must be returned from\n// a factory function. The function receives the raw\n// props received by the component as the argument.\ndefault(rawProps) {\n// default function receives the raw props object as argument\nreturn { message: 'hello' }\n}\n},\n// Custom validator function\npropF: {\nvalidator(value) {\n// The value must match one of these strings\nreturn ['success', 'warning', 'danger'].includes(value)\n}\n},\n// Function with a default value\npropG: {\ntype: Function,\n// Unlike object or array default, this is not a factory function - this is a function to serve as a default value\ndefault() {\nreturn 'Default function'\n}\n}\n}\n}\n</code></pre> <p>Additional details:</p> <ul> <li>All props are optional by default, unless <code>required: true</code> is specified.</li> <li>An absent optional prop will have <code>undefined</code> value.</li> <li>If a <code>default</code> value is specified, it will be used if the resolved prop value     is <code>undefined</code>, this includes both when the prop is absent, or an explicit     <code>undefined</code> value is passed.</li> </ul>"}, {"location": "vuejs/#listening-to-events", "title": "Listening to Events", "text": "<p>As we develop our <code>&lt;BlogPost&gt;</code> component, some features may require communicating back up to the parent. For example, we may decide to include an accessibility feature to enlarge the text of blog posts, while leaving the rest of the page at its default size.</p> <p>In the parent, we can support this feature by adding a <code>postFontSize</code> data property:</p> <pre><code>data() {\nreturn {\nposts: [\n/* ... */\n],\npostFontSize: 1\n}\n}\n</code></pre> <p>Which can be used in the template to control the font size of all blog posts:</p> <pre><code>&lt;div :style=\"{ fontSize: postFontSize + 'em' }\"&gt;\n  &lt;BlogPost\n    v-for=\"post in posts\"\n    :key=\"post.id\"\n    :title=\"post.title\"\n   /&gt;\n&lt;/div&gt;\n</code></pre> <p>Now let's add a button to the <code>&lt;BlogPost&gt;</code> component's template:</p> <pre><code>&lt;!-- BlogPost.vue, omitting &lt;script&gt; --&gt;\n&lt;template&gt;\n  &lt;div class=\"blog-post\"&gt;\n    &lt;h4&gt;{{ title }}&lt;/h4&gt;\n    &lt;button&gt;Enlarge text&lt;/button&gt;\n  &lt;/div&gt;\n&lt;/template&gt;\n</code></pre> <p>The button currently doesn't do anything yet - we want clicking the button to communicate to the parent that it should enlarge the text of all posts. To solve this problem, component instances provide a custom events system. The parent can choose to listen to any event on the child component instance with <code>v-on</code> or <code>@,</code> just as we would with a native DOM event:</p> <pre><code>&lt;BlogPost\n  ...\n  @enlarge-text=\"postFontSize += 0.1\"\n /&gt;\n</code></pre> <p>Then the child component can emit an event on itself by calling the built-in <code>$emit</code> method, passing the name of the event:</p> <pre><code>&lt;!-- BlogPost.vue, omitting &lt;script&gt; --&gt;\n&lt;template&gt;\n  &lt;div class=\"blog-post\"&gt;\n    &lt;h4&gt;{{ title }}&lt;/h4&gt;\n    &lt;button @click=\"$emit('enlarge-text')\"&gt;Enlarge text&lt;/button&gt;\n  &lt;/div&gt;\n&lt;/template&gt;\n</code></pre> <p>The first argument to <code>this.$emit()</code> is the event name. Any additional arguments are passed on to the event listener.</p> <p>Thanks to the <code>@enlarge-text=\"postFontSize += 0.1\"</code> listener, the parent will receive the event and update the value of <code>postFontSize</code>.</p> <p>We can optionally declare emitted events using the emits option:</p> <pre><code>&lt;!-- BlogPost.vue --&gt;\n&lt;script&gt;\nexport default {\n  props: ['title'],\n  emits: ['enlarge-text']\n}\n&lt;/script&gt;\n</code></pre> <p>This documents all the events that a component emits and optionally validates them. It also allows Vue to avoid implicitly applying them as native listeners to the child component's root element.</p>"}, {"location": "vuejs/#event-arguments", "title": "Event arguments", "text": "<p>It's sometimes useful to emit a specific value with an event. For example, we may want the <code>&lt;BlogPost&gt;</code> component to be in charge of how much to enlarge the text by. In those cases, we can pass extra arguments to $emit to provide this value:</p> <pre><code>&lt;button @click=\"$emit('increaseBy', 1)\"&gt;\n  Increase by 1\n&lt;/button&gt;\n</code></pre> <p>Then, when we listen to the event in the parent, we can use an inline arrow function as the listener, which allows us to access the event argument:</p> <pre><code>&lt;MyButton @increase-by=\"(n) =&gt; count += n\" /&gt;\n</code></pre> <p>Or, if the event handler is a method:</p> <p></p> <p>Then the value will be passed as the first parameter of that method:</p> <pre><code>methods: {\nincreaseCount(n) {\nthis.count += n\n}\n}\n</code></pre>"}, {"location": "vuejs/#declaring-emitted-events", "title": "Declaring emitted events", "text": "<p>Emitted events can be explicitly declared on the component via the <code>emits</code> option.</p> <pre><code>export default {\nemits: ['inFocus', 'submit']\n}\n</code></pre> <p>The <code>emits</code> option also supports an object syntax, which allows us to perform runtime validation of the payload of the emitted events:</p> <pre><code>export default {\nemits: {\nsubmit(payload) {\n// return `true` or `false` to indicate\n// validation pass / fail\n}\n}\n}\n</code></pre> <p>Although optional, it is recommended to define all emitted events in order to better document how a component should work.</p>"}, {"location": "vuejs/#content-distribution-with-slots", "title": "Content distribution with Slots", "text": "<p>In addition to passing data via props, the parent component can also pass down template fragments to the child via slots:</p> <pre><code>&lt;ChildComp&gt;\n  This is some slot content!\n&lt;/ChildComp&gt;\n</code></pre> <p>In the child component, it can render the slot content from the parent using the <code>&lt;slot&gt;</code> element as outlet:</p> <pre><code>&lt;!-- in child template --&gt;\n&lt;slot/&gt;\n</code></pre> <p>Content inside the <code>&lt;slot&gt;</code> outlet will be treated as \"fallback\" content: it will be displayed if the parent did not pass down any slot content:</p> <pre><code>&lt;slot&gt;Fallback content&lt;/slot&gt;\n</code></pre> <p>Slot content is not just limited to text. It can be any valid template content. For example, we can pass in multiple elements, or even other components:</p> <pre><code>&lt;FancyButton&gt;\n  &lt;span style=\"color:red\"&gt;Click me!&lt;/span&gt;\n  &lt;AwesomeIcon name=\"plus\" /&gt;\n&lt;/FancyButton&gt;\n</code></pre> <p>Slot content has access to the data scope of the parent component, because it is defined in the parent. However, slot content does not have access to the child component's data. As a rule, remember that everything in the parent template is compiled in parent scope; everything in the child template is compiled in the child scope. You can however use child content using scoped slots.</p>"}, {"location": "vuejs/#named-slots", "title": "Named Slots", "text": "<p>There are times when it's useful to have multiple slot outlets in a single component.</p> <p>For these cases, the <code>&lt;slot&gt;</code> element has a special attribute, <code>name</code>, which can be used to assign a unique ID to different slots so you can determine where content should be rendered:</p> <pre><code>&lt;div class=\"container\"&gt;\n  &lt;header&gt;\n    &lt;slot name=\"header\"&gt;&lt;/slot&gt;\n  &lt;/header&gt;\n  &lt;main&gt;\n    &lt;slot&gt;&lt;/slot&gt;\n  &lt;/main&gt;\n  &lt;footer&gt;\n    &lt;slot name=\"footer\"&gt;&lt;/slot&gt;\n  &lt;/footer&gt;\n&lt;/div&gt;\n</code></pre> <p>To pass a named slot, we need to use a <code>&lt;template&gt;</code> element with the <code>v-slot</code> directive, and then pass the name of the slot as an argument to <code>v-slot</code>:</p> <p><pre><code>&lt;BaseLayout&gt;\n  &lt;template #header&gt;\n    &lt;h1&gt;Here might be a page title&lt;/h1&gt;\n  &lt;/template&gt;\n\n  &lt;template #default&gt;\n    &lt;p&gt;A paragraph for the main content.&lt;/p&gt;\n    &lt;p&gt;And another one.&lt;/p&gt;\n  &lt;/template&gt;\n\n  &lt;template #footer&gt;\n    &lt;p&gt;Here's some contact info&lt;/p&gt;\n  &lt;/template&gt;\n&lt;/BaseLayout&gt;\n</code></pre> Where <code>#</code> is the shorthand of <code>v-slot</code>.</p>"}, {"location": "vuejs/#dynamic-components", "title": "Dynamic components", "text": "<p>Sometimes, it's useful to dynamically switch between components, like in a tabbed interface, for example in this page.</p> <p>The above is made possible by Vue's <code>&lt;component&gt;</code> element with the special <code>is</code> attribute:</p> <pre><code>&lt;!-- Component changes when currentTab changes --&gt;\n&lt;component :is=\"currentTab\"&gt;&lt;/component&gt;\n</code></pre> <p>In the example above, the value passed to <code>:is</code> can contain either:</p> <ul> <li>The name string of a registered component, OR.</li> <li>The actual imported component object.</li> </ul> <p>You can also use the is attribute to create regular HTML elements.</p> <p>When switching between multiple components with <code>&lt;component :is=\"...\"&gt;</code>, a component will be unmounted when it is switched away from. We can force the inactive components to stay \"alive\" with the built-in <code>&lt;KeepAlive&gt;</code> component.</p>"}, {"location": "vuejs/#async-components", "title": "Async components", "text": "<p>In large applications, we may need to divide the app into smaller chunks and only load a component from the server when it's needed. To make that possible, Vue has a <code>defineAsyncComponent</code> function:</p> <pre><code>import { defineAsyncComponent } from 'vue'\n\nconst AsyncComp = defineAsyncComponent(() =&gt;\nimport('./components/MyComponent.vue')\n)\n</code></pre> <p>Asynchronous operations inevitably involve loading and error states, <code>defineAsyncComponent()</code> supports handling these states via advanced options:</p> <pre><code>const AsyncComp = defineAsyncComponent({\n// the loader function\nloader: () =&gt; import('./Foo.vue'),\n\n// A component to use while the async component is loading\nloadingComponent: LoadingComponent,\n// Delay before showing the loading component. Default: 200ms.\ndelay: 200,\n\n// A component to use if the load fails\nerrorComponent: ErrorComponent,\n// The error component will be displayed if a timeout is\n// provided and exceeded. Default: Infinity.\ntimeout: 3000\n})\n</code></pre>"}, {"location": "vuejs/#testing", "title": "Testing", "text": "<p>When designing your Vue application's testing strategy, you should leverage the following testing types:</p> <ul> <li>Unit: Checks that inputs to a given function, class, or composable are     producing the expected output or side effects.</li> <li>Component: Checks that your component mounts, renders, can be interacted     with, and behaves as expected. These tests import more code than unit tests,     are more complex, and require more time to execute.</li> <li>End-to-end: Checks features that span multiple pages and make real network     requests against your production-built Vue application. These tests often     involve standing up a database or other backend.</li> </ul>"}, {"location": "vuejs/#unit-testing", "title": "Unit testing", "text": "<p>Unit tests will catch issues with a function's business logic and logical correctness.</p> <p>Take for example this increment function:</p> <pre><code>// helpers.js\nexport function increment (current, max = 10) {\nif (current &lt; max) {\nreturn current + 1\n}\nreturn current\n}\n</code></pre> <p>Because it's very self-contained, it'll be easy to invoke the <code>increment</code> function and assert that it returns what it's supposed to, so we'll write a Unit Test.</p> <p>If any of these assertions fail, it's clear that the issue is contained within the <code>increment</code> function.</p> <pre><code>// helpers.spec.js\nimport { increment } from './helpers'\n\ndescribe('increment', () =&gt; {\ntest('increments the current number by 1', () =&gt; {\nexpect(increment(0, 10)).toBe(1)\n})\n\ntest('does not increment the current number over the max', () =&gt; {\nexpect(increment(10, 10)).toBe(10)\n})\n\ntest('has a default max of 10', () =&gt; {\nexpect(increment(10)).toBe(10)\n})\n})\n</code></pre> <p>Unit testing is typically applied to self-contained business logic, components, classes, modules, or functions that do not involve UI rendering, network requests, or other environmental concerns.</p> <p>These are typically plain JavaScript / TypeScript modules unrelated to Vue. In general, writing unit tests for business logic in Vue applications does not differ significantly from applications using other frameworks.</p> <p>There are two instances where you DO unit test Vue-specific features:</p> <ul> <li>Composables</li> <li>Components</li> </ul>"}, {"location": "vuejs/#component-testing", "title": "Component testing", "text": "<p>In Vue applications, components are the main building blocks of the UI. Components are therefore the natural unit of isolation when it comes to validating your application's behavior. From a granularity perspective, component testing sits somewhere above unit testing and can be considered a form of integration testing. Much of your Vue Application should be covered by a component test and we recommend that each Vue component has its own spec file.</p> <p>Component tests should catch issues relating to your component's props, events, slots that it provides, styles, classes, lifecycle hooks, and more.</p> <p>Component tests should not mock child components, but instead test the interactions between your component and its children by interacting with the components as a user would. For example, a component test should click on an element like a user would instead of programmatically interacting with the component.</p> <p>Component tests should focus on the component's public interfaces rather than internal implementation details. For most components, the public interface is limited to: events emitted, props, and slots. When testing, remember to test what a component does, not how it does it. For example:</p> <ul> <li>For Visual logic assert correct render output based on inputted props and     slots.</li> <li>For Behavioral logic: assert correct render updates or emitted events in     response to user input events.</li> </ul> <p>The recommendation is to use Vitest for components or composables that render headlessly, and Cypress Component Testing for components whose expected behavior depends on properly rendering styles or triggering native DOM event.</p> <p>The main differences between Vitest and browser-based runners are speed and execution context. In short, browser-based runners, like Cypress, can catch issues that node-based runners, like Vitest, cannot (e.g. style issues, real native DOM events, cookies, local storage, and network failures), but browser-based runners are orders of magnitude slower than Vitest because they do open a browser, compile your stylesheets, and more.</p> <p>Component testing often involves mounting the component being tested in isolation, triggering simulated user input events, and asserting on the rendered DOM output. There are dedicated utility libraries that make these tasks simpler.</p> <ul> <li> <p><code>@testing-library/vue</code> is a Vue testing library focused on testing components     without relying on implementation details. Built with accessibility in mind,     its approach also makes refactoring a breeze. Its guiding principle is that     the more tests resemble the way software is used, the more confidence they     can provide.</p> </li> <li> <p><code>@vue/test-utils</code> is the official low-level component testing library that was     written to provide users access to Vue specific APIs. It's also the     lower-level library <code>@testing-library/vue</code> is built on top of.</p> </li> </ul> <p>I recommend using cypress so that you can use the same language either you are doing E2E tests or unit tests.</p> <p>If you're using Vuetify don't try to do component testing, I've tried for days and was unable to make it work.</p>"}, {"location": "vuejs/#e2e-testing", "title": "E2E Testing", "text": "<p>While unit tests provide developers with some degree of confidence, unit and component tests are limited in their abilities to provide holistic coverage of an application when deployed to production. As a result, end-to-end (E2E) tests provide coverage on what is arguably the most important aspect of an application: what happens when users actually use your applications.</p> <p>End-to-end tests focus on multi-page application behavior that makes network requests against your production-built Vue application. They often involve standing up a database or other backend and may even be run against a live staging environment.</p> <p>End-to-end tests will often catch issues with your router, state management library, top-level components (e.g. an App or Layout), public assets, or any request handling. As stated above, they catch critical issues that may be impossible to catch with unit tests or component tests.</p> <p>End-to-end tests do not import any of your Vue application's code, but instead rely completely on testing your application by navigating through entire pages in a real browser.</p> <p>End-to-end tests validate many of the layers in your application. They can either target your locally built application, or even a live Staging environment. Testing against your Staging environment not only includes your frontend code and static server, but all associated backend services and infrastructure.</p>"}, {"location": "vuejs/#e2e-tests-decisions", "title": "E2E tests decisions", "text": "<p>When doing E2E tests keep in mind:</p> <ul> <li> <p>Cross-browser testing: One of the primary benefits that end-to-end (E2E)     testing is known for is its ability to test your application across multiple     browsers. While it may seem desirable to have 100% cross-browser coverage,     it is important to note that cross browser testing has diminishing returns     on a team's resources due the additional time and machine power required to     run them consistently. As a result, it is important to be mindful of this     trade-off when choosing the amount of cross-browser testing your application     needs.</p> </li> <li> <p>Faster feedback loops: One of the primary problems with end-to-end (E2E) tests     and development is that running the entire suite takes a long time.     Typically, this is only done in continuous integration and deployment     (CI/CD) pipelines. Modern E2E testing frameworks have helped to solve this     by adding features like parallelization, which allows for CI/CD pipelines to     often run magnitudes faster than before. In addition, when developing     locally, the ability to selectively run a single test for the page you are     working on while also providing hot reloading of tests can help to boost     a developer's workflow and productivity.</p> </li> <li> <p>Visibility in headless mode: When end-to-end (E2E) tests are run in continuous     integration / deployment pipelines, they are often run in headless browsers     (i.e., no visible browser is opened for the user to watch). A critical     feature of modern E2E testing frameworks is the ability to see snapshots     and/or videos of the application during testing, providing some insight into     why errors are happening. Historically, it was tedious to maintain these     integrations.</p> </li> </ul> <p>Vue developers suggestion is to use Cypress as it provides the most complete E2E solution with features like an informative graphical interface, excellent debuggability, built-in assertions and stubs, flake-resistance, parallelization, and snapshots. It also provides support for Component Testing. However, it only supports Chromium-based browsers and Firefox.</p>"}, {"location": "vuejs/#installation", "title": "Installation", "text": "<p>In a Vite-based Vue project, run:</p> <pre><code>npm install -D vitest happy-dom @testing-library/vue@next\n</code></pre> <p>Next, update the Vite configuration to add the test option block:</p> <pre><code>// vite.config.js\nimport { defineConfig } from 'vite'\n\nexport default defineConfig({\n// ...\ntest: {\n// enable jest-like global test APIs\nglobals: true,\n// simulate DOM with happy-dom\n// (requires installing happy-dom as a peer dependency)\nenvironment: 'happy-dom'\n}\n})\n</code></pre> <p>Then create a file ending in <code>*.test.js</code> in your project. You can place all test files in a test directory in project root, or in test directories next to your source files. Vitest will automatically search for them using the naming convention.</p> <pre><code>// MyComponent.test.js\nimport { render } from '@testing-library/vue'\nimport MyComponent from './MyComponent.vue'\n\ntest('it should work', () =&gt; {\nconst { getByText } = render(MyComponent, {\nprops: {\n/* ... */\n}\n})\n\n// assert output\ngetByText('...')\n})\n</code></pre> <p>Finally, update <code>package.json</code> to add the test script and run it:</p> <pre><code>{\n// ...\n\"scripts\": {\n\"test\": \"vitest\"\n}\n}\n</code></pre> <pre><code>npm test\n</code></pre>"}, {"location": "vuejs/#deploying", "title": "Deploying", "text": "<p>It is common these days to run front-end and back-end services inside Docker containers. The front-end service usually talks using a API with the back-end service.</p> <pre><code>FROM node as ui-builder\nRUN mkdir /usr/src/app\nWORKDIR /usr/src/app\nENV PATH /usr/src/app/node_modules/.bin:$PATH\nCOPY package.json /usr/src/app/package.json\nRUN npm install\nRUN npm install -g @vue/cli\nCOPY . /usr/src/app\nRUN npm run build\n\nFROM nginx\nCOPY  --from=ui-builder /usr/src/app/dist /usr/share/nginx/html\nEXPOSE 80\nCMD [\"nginx\", \"-g\", \"daemon off;\"]\n</code></pre> <p>The above makes use of the multi-stage build feature of Docker. The first half of the Dockerfile build the artifacts and second half use those artifacts and create a new image from them.</p> <p>To build the production image, run:</p> <pre><code>docker build -t myapp .\n</code></pre> <p>You can run the container by executing the following command:</p> <pre><code>docker run -it -p 80:80 --rm myapp-prod\n</code></pre> <p>The application will now be accessible at <code>http://localhost</code>.</p>"}, {"location": "vuejs/#configuration-through-environmental-variables", "title": "Configuration through environmental variables", "text": "<p>In production you want to be able to scale up or down the frontend and the backend independently, to be able to do that you usually have one or many docker for each role. Usually there is an SSL Proxy that acts as gate keeper and is the only component exposed to the public.</p> <p>If the user requests for <code>/api</code> it will forward the requests to the backend, if it asks for any other url it will forward it to the frontend.</p> <p>Note<p>\"You probably don't need to configure the backend api url as an environment variable see here why.\"</p> </p> <p>For the frontend, we need to configure the application. This is usually done through environmental variables, such as <code>EXTERNAL_BACKEND_URL</code>. The problem is that these environment variables are set at build time, and can't be changed at runtime by default, so you can't offer a generic fronted Docker and particularize for the different cases. I've literally cried for hours trying to find a solution for this until Jos\u00e9 Silva came to my rescue. The tweak is to use a docker entrypoint to inject the values we want. To do so you need to:</p> <ul> <li> <p>Edit the site main <code>index.html</code> (if you use Vite is in <code>/index.html</code> otherwise     it might be at <code>public/index.html</code> to add a placeholder that will be     replaced by the dynamic configurations.</p> <pre><code>&lt;!DOCTYPE html&gt;\n&lt;html lang=\"en\"&gt;\n&lt;head&gt;\n    &lt;script&gt;\n// CONFIGURATIONS_PLACEHOLDER\n&lt;/script&gt;\n    ...\n</code></pre> </li> <li> <p>Create an executable file named <code>entrypoint.sh</code> in the root of the project.</p> <pre><code>#!/bin/sh\n\nJSON_STRING='window.configs = { \\\n  \"VITE_APP_VARIABLE_1\":\"'\"${VITE_APP_VARIABLE_1}\"'\", \\\n  \"VITE_APP_VARIABLE_2\":\"'\"${VITE_APP_VARIABLE_2}\"'\" \\\n}'\n\nsed -i \"s@// CONFIGURATIONS_PLACEHOLDER@${JSON_STRING}@\" /usr/share/nginx/html/index.html\n\nexec \"$@\"\n</code></pre> <p>Its function is to replace the placeholder in the index.html by the configurations, injecting them in the browser window.</p> </li> <li> <p>Create a file named <code>src/utils/env.js</code> with the following utility function:</p> <pre><code>export default function getEnv(name) {\nreturn window?.configs?.[name] || process.env[name]\n}\n</code></pre> <p>Which allows us to easily get the value of the configuration. If it exists in <code>window.configs</code> (used in remote environments like staging or production) it will have priority over the <code>process.env</code> (used for development).</p> </li> <li> <p>Replace the content of the <code>App.vue</code> file with the following:</p> <pre><code>&lt;template&gt;\n  &lt;div id=\"app\"&gt;\n    &lt;img alt=\"Vue logo\" src=\"./assets/logo.png\"&gt;\n    &lt;div&gt;{{ variable1 }}&lt;/div&gt;\n    &lt;div&gt;{{ variable2 }}&lt;/div&gt;\n  &lt;/div&gt;\n&lt;/template&gt;\n\n&lt;script&gt;\nimport getEnv from '@/utils/env'export default {\nname: 'App',\ndata() {\nreturn {\nvariable1: getEnv('VITE_APP_VARIABLE_1'),\nvariable2: getEnv('VITE_APP_VARIABLE_2')\n}\n}\n}\n&lt;/script&gt;\n</code></pre> <p>At this point, if you create the <code>.env.local</code> file, in the root of the project, with the values for the printed variables:</p> <pre><code>VITE_APP_VARIABLE_1='I am the develoment variable 1'\nVITE_APP_VARIABLE_2='I am the develoment variable 2'\n</code></pre> <p>And run the development server <code>npm run dev</code> you should see those values printed in the application (http://localhost:8080).</p> </li> <li> <p>Update the <code>Dockerfile</code> to load the <code>entrypoint.sh</code>.</p> <pre><code>FROM node as ui-builder\nRUN mkdir /usr/src/app\nWORKDIR /usr/src/app\nENV PATH /usr/src/app/node_modules/.bin:$PATH\nCOPY package.json /usr/src/app/package.json\nRUN npm install\nRUN npm install -g @vue/cli\nCOPY . /usr/src/app\nARG VUE_APP_API_URL\nENV VUE_APP_API_URL $VUE_APP_API_URL\nRUN npm run build\n\nFROM nginx\nCOPY  --from=ui-builder /usr/src/app/dist /usr/share/nginx/html\nCOPY entrypoint.sh /usr/share/nginx/\nENTRYPOINT [\"/usr/share/nginx/entrypoint.sh\"]\nEXPOSE 80\nCMD [\"nginx\", \"-g\", \"daemon off;\"]\n</code></pre> </li> <li> <p>Build the docker</p> <pre><code>docker build -t my-app .\n</code></pre> </li> </ul> <p>Now if you have a <code>.env.production.local</code> file with the next contents:</p> <pre><code>VITE_APP_VARIABLE_1='I am the production variable 1'\nVITE_APP_VARIABLE_2='I am the production variable 2'\n</code></pre> <p>And run <code>docker run -it -p 80:80 --env-file=.env.production.local --rm my-app</code>, you'll see the values of the production variables. You can also pass the variables directly with <code>-e VITE_APP_VARIABLE_1=\"Overriden variable\"</code>.</p>"}, {"location": "vuejs/#deploy-static-site-on-github-pages", "title": "Deploy static site on github pages", "text": "<p>Sites in Github pages have the url structure of <code>https://github_user.github.io/repo_name/</code> we need to tell vite that the base url is <code>/repo_name/</code>, otherwise the application will try to load the assets in <code>https://github_user.github.io/assets/</code> instead of <code>https://github_user.github.io/rpeo_name/assets/</code>.</p> <p>To change it, add in the <code>vite.config.js</code> file:</p> <pre><code>export default defineConfig({\nbase: '/repo_name/'\n})\n</code></pre> <p>Now you need to configure the deployment workflow, to do so, create a new file: <code>.github/workflows/deploy.yml</code> and paste the following code:</p> <pre><code>---\nname: Deploy\n\non:\npush:\nbranches:\n- main\nworkflow_dispatch:\n\njobs:\nbuild:\nname: Build\nruns-on: ubuntu-latest\nsteps:\n- name: Checkout repo\nuses: actions/checkout@v2\n\n- name: Setup Node\nuses: actions/setup-node@v1\nwith:\nnode-version: 16\n\n- name: Install dependencies\nuses: bahmutov/npm-install@v1\n\n- name: Build project\nrun: npm run build\n\n- name: Upload production-ready build files\nuses: actions/upload-artifact@v2\nwith:\nname: production-files\npath: ./dist\n\ndeploy:\nname: Deploy\nneeds: build\nruns-on: ubuntu-latest\nif: github.ref == 'refs/heads/main'\nsteps:\n- name: Download artifact\nuses: actions/download-artifact@v2\nwith:\nname: production-files\npath: ./dist\n\n- name: Deploy to GitHub Pages\nuses: peaceiris/actions-gh-pages@v3\nwith:\ngithub_token: ${{ secrets.GITHUB_TOKEN }}\npublish_dir: ./dist\n</code></pre> <p>You'd probably need to change your repository settings under Actions/General and set the Workflow permissions to Read and write permissions.</p> <p>Once the workflow has been successful, in the repository settings under Pages you need to enable Github Pages to use the <code>gh-pages</code> branch as source.</p>"}, {"location": "vuejs/#tip-handling-vue-router-with-a-custom-404-page", "title": "Tip Handling Vue Router with a Custom 404 Page", "text": "<p>One thing to keep in mind when setting up the Github Pages site, is that working with Vue Router gets a little tricky.</p> <p>If you\u2019re using history mode in Vue router, you\u2019ll notice that if you try to go directly to a page other than <code>/</code> you\u2019ll get a 404 error. This is because Github Pages does not automatically redirect all requests to serve <code>index.html</code>.</p> <p>Luckily, there is an easy little workaround. All you have to do is duplicate your <code>index.html</code> file and name the copy <code>404.html</code>.</p> <p>What this does is make your 404 page serve the same content as your <code>index.html</code>, which means your Vue router will be able to display the right page.</p>"}, {"location": "vuejs/#testing_1", "title": "Testing", "text": ""}, {"location": "vuejs/#debug-jest-tests", "title": "Debug Jest tests", "text": "<p>If you're not developing in Visual code, running a debugger is not easy in the middle of the tests, so to debug one you can use <code>console.log()</code> statements and when you run them with <code>yarn test:unit</code> you'll see the traces.</p>"}, {"location": "vuejs/#troubleshooting", "title": "Troubleshooting", "text": ""}, {"location": "vuejs/#failed-to-resolve-component-x", "title": "Failed to resolve component: X", "text": "<p>If you've already imported the component with <code>import X from './X.vue</code> you may have forgotten to add the component to the <code>components</code> property of the module:</p> <pre><code>export default {\nname: 'Inbox',\ncomponents: {\nX\n}\n}\n</code></pre>"}, {"location": "vuejs/#references", "title": "References", "text": "<ul> <li>Docs</li> <li>Homepage</li> <li>Tutorial</li> <li> <p>Examples</p> </li> <li> <p>Awesome Vue     Components</p> </li> </ul>"}, {"location": "vuejs/#axios_1", "title": "Axios", "text": "<ul> <li>Docs</li> <li>Git</li> <li>Homepage</li> </ul>"}, {"location": "vuetify/", "title": "Vuetify", "text": "<p>Vuetify is a Vue UI Library with beautifully handcrafted Material Components.</p>"}, {"location": "vuetify/#install", "title": "Install", "text": "<p>First you need <code>vue-cli</code>, install it with:</p> <pre><code>sudo npm install -g @vue/cli\n</code></pre> <p>Then run:</p> <pre><code>vue add vuetify\n</code></pre> <p>If you're using Vite select <code>Vite Preview (Vuetify 3 + Vite)</code>.</p>"}, {"location": "vuetify/#usage", "title": "Usage", "text": ""}, {"location": "vuetify/#flex", "title": "Flex", "text": "<p>Control the layout of flex containers with alignment, justification and more with responsive flexbox utilities.</p> <p>Note<p>\"I suggest you use this page only as a reference, if it's the first time you see this content, it's better to see it at the source as you can see Flex in action at the same time you read, which makes it much more easy to understand.\"</p> </p> <p>Using <code>display</code> utilities you can turn any element into a flexbox container transforming direct children elements into flex items. Using additional flex property utilities, you can customize their interaction even further.</p> <p>You can also customize flex utilities to apply based upon various breakpoints.</p> <ul> <li><code>.d-flex</code></li> <li><code>.d-inline-flex</code></li> <li><code>.d-sm-flex</code></li> <li><code>.d-sm-inline-flex</code></li> <li><code>.d-md-flex</code></li> <li><code>.d-md-inline-flex</code></li> <li><code>.d-lg-flex</code></li> <li><code>.d-lg-inline-flex</code></li> <li><code>.d-xl-flex</code></li> <li><code>.d-xl-inline-flex</code></li> </ul> <p>You define the attributes inside the <code>class</code> of the Vuetify object. For example:</p> <pre><code>&lt;v-card class=\"d-flex flex-row mb-6\" /&gt;\n</code></pre>"}, {"location": "vuetify/#display-breakpoints", "title": "Display breakpoints", "text": "<p>With Vuetify you can control various aspects of your application based upon the window size.</p> Device Code Type Range Extra small xs Small to large phone <code>&lt; 600px</code> Small sm Small to medium tablet <code>600px &gt; &lt; 960px</code> Medium md Large tablet to laptop <code>960px &gt; &lt; 1264px*</code> Large lg Desktop <code>1264px &gt; &lt; 1904px*</code> Extra large xl 4k and ultra-wide <code>&gt; 1904px*</code> <p>The breakpoint service is a programmatic way of accessing viewport information within components. It exposes a number of properties on the <code>$vuetify</code> object that can be used to control aspects of your application based upon the viewport size. The <code>name</code> property correlates to the currently active breakpoint; e.g. xs, sm, md, lg, xl.</p> <p>In the following snippet, we use a switch statement and the current breakpoint name to modify the <code>height</code> property of the <code>v-card</code> component:</p> <pre><code>&lt;template&gt;\n  &lt;v-card :height=\"height\"&gt;\n    ...\n  &lt;/v-card&gt;\n&lt;/template&gt;\n\n&lt;script&gt;\nexport default {\ncomputed: {\nheight () {\nswitch (this.$vuetify.breakpoint.name) {\ncase 'xs': return 220\ncase 'sm': return 400\ncase 'md': return 500\ncase 'lg': return 600\ncase 'xl': return 800\n}\n},\n},\n}\n&lt;/script&gt;\n</code></pre> <p>The following is the public signature for the breakpoint service:</p> <pre><code>{\n// Breakpoints\nxs: boolean\nsm: boolean\nmd: boolean\nlg: boolean\nxl: boolean\n\n// Conditionals\nxsOnly: boolean\nsmOnly: boolean\nsmAndDown: boolean\nsmAndUp: boolean\nmdOnly: boolean\nmdAndDown: boolean\nmdAndUp: boolean\nlgOnly: boolean\nlgAndDown: boolean\nlgAndUp: boolean\nxlOnly: boolean\n\n// true if screen width &lt; mobileBreakpoint\nmobile: boolean\nmobileBreakpoint: number\n\n// Current breakpoint name (e.g. 'md')\nname: string\n\n// Dimensions\nheight: number\nwidth: number\n\n// Thresholds\n// Configurable through options\n{\nxs: number\nsm: number\nmd: number\nlg: number\n}\n\n// Scrollbar\nscrollBarWidth: number\n}\n</code></pre> <p>Access these properties within Vue files by referencing <code>$vuetify.breakpoint.&lt;property&gt;</code> For example to log the current viewport width to the console once the component fires the mounted lifecycle hook you can use:</p> <pre><code>&lt;!-- Vue Component --&gt;\n\n&lt;script&gt;\nexport default {\nmounted () {\nconsole.log(this.$vuetify.breakpoint.width)\n}\n}\n&lt;/script&gt;\n</code></pre>"}, {"location": "vuetify/#flex-direction", "title": "Flex direction", "text": "<p>By default, <code>d-flex</code> applies <code>flex-direction: row</code> and can generally be omitted.</p> <p>The <code>flex-column</code> and <code>flex-column-reverse</code> utility classes can be used to change the orientation of the flexbox container.</p> <p>There are also responsive variations for flex-direction.</p> <ul> <li><code>.flex-row</code></li> <li><code>.flex-row-reverse</code></li> <li><code>.flex-column</code></li> <li><code>.flex-column-reverse</code></li> <li><code>.flex-sm-row</code></li> <li><code>.flex-sm-row-reverse</code></li> <li><code>.flex-sm-column</code></li> <li><code>.flex-sm-column-reverse</code></li> <li><code>.flex-md-row</code></li> <li><code>.flex-md-row-reverse</code></li> <li><code>.flex-md-column</code></li> <li><code>.flex-md-column-reverse</code></li> <li><code>.flex-lg-row</code></li> <li><code>.flex-lg-row-reverse</code></li> <li><code>.flex-lg-column</code></li> <li><code>.flex-lg-column-reverse</code></li> <li><code>.flex-xl-row</code></li> <li><code>.flex-xl-row-reverse</code></li> <li><code>.flex-xl-column</code></li> <li><code>.flex-xl-column-reverse</code></li> </ul>"}, {"location": "vuetify/#flex-justify", "title": "Flex justify", "text": "<p>The <code>justify-content</code> flex setting can be changed using the flex justify classes. This by default will modify the flexbox items on the x-axis but is reversed when using <code>flex-direction: column</code>, modifying the y-axis. Choose from:</p> <ul> <li><code>start</code> (browser default): Everything together on the left.</li> <li><code>end</code>: Everything together on the right.</li> <li><code>center</code>: Everything together on the center.</li> <li><code>space-between</code>: First item on the top left, second on the center, third at     the end, with space between the items.</li> <li><code>space-around</code>: Like <code>space-between</code> but with space on the top left and right     too.</li> </ul> <p>For example:</p> <pre><code>&lt;v-card class=\"d-flex justify-center mb-6\" /&gt;\n</code></pre> <p>There are also responsive variations for <code>justify-content</code>.</p> <ul> <li><code>.justify-start</code></li> <li><code>.justify-end</code></li> <li><code>.justify-center</code></li> <li><code>.justify-space-between</code></li> <li><code>.justify-space-around</code></li> <li><code>.justify-sm-start</code></li> <li><code>.justify-sm-end</code></li> <li><code>.justify-sm-center</code></li> <li><code>.justify-sm-space-between</code></li> <li><code>.justify-sm-space-around</code></li> <li><code>.justify-md-start</code></li> <li><code>.justify-md-end</code></li> <li><code>.justify-md-center</code></li> <li><code>.justify-md-space-between</code></li> <li><code>.justify-md-space-around</code></li> <li><code>.justify-lg-start</code></li> <li><code>.justify-lg-end</code></li> <li><code>.justify-lg-center</code></li> <li><code>.justify-lg-space-between</code></li> <li><code>.justify-lg-space-around</code></li> <li><code>.justify-xl-start</code></li> <li><code>.justify-xl-end</code></li> <li><code>.justify-xl-center</code></li> <li><code>.justify-xl-space-between</code></li> <li><code>.justify-xl-space-around</code></li> </ul>"}, {"location": "vuetify/#flex-align", "title": "Flex align", "text": "<p>The <code>align-items</code> flex setting can be changed using the flex align classes. This by default will modify the flexbox items on the y-axis but is reversed when using <code>flex-direction: column</code>, modifying the x-axis. Choose from:</p> <ul> <li><code>start</code>: Everything together on the top.</li> <li><code>end</code>: Everything together on the bottom.</li> <li><code>center</code>: Everything together on the center.</li> <li><code>baseline</code>: (I don't understand this one).</li> <li><code>align-stretch</code>: Align content to the top but extend the container to the     bottom. For example:</li> </ul> <pre><code>&lt;v-card class=\"d-flex align-center mb-6\" /&gt;\n</code></pre> <p>There are also responsive variations for <code>align-items</code>.</p> <ul> <li><code>.align-start</code></li> <li><code>.align-end</code></li> <li><code>.align-center</code></li> <li><code>.align-baseline</code></li> <li><code>.align-stretch</code></li> <li><code>.align-sm-start</code></li> <li><code>.align-sm-end</code></li> <li><code>.align-sm-center</code></li> <li><code>.align-sm-baseline</code></li> <li><code>.align-sm-stretch</code></li> <li><code>.align-md-start</code></li> <li><code>.align-md-end</code></li> <li><code>.align-md-center</code></li> <li><code>.align-md-baseline</code></li> <li><code>.align-md-stretch</code></li> <li><code>.align-lg-start</code></li> <li><code>.align-lg-end</code></li> <li><code>.align-lg-center</code></li> <li><code>.align-lg-baseline</code></li> <li><code>.align-lg-stretch</code></li> <li><code>.align-xl-start</code></li> <li><code>.align-xl-end</code></li> <li><code>.align-xl-center</code></li> <li><code>.align-xl-baseline</code></li> <li><code>.align-xl-stretch</code></li> </ul> <p>The <code>align-self</code> attribute works like <code>align</code> but for a single element instead of all the children.</p>"}, {"location": "vuetify/#margins", "title": "Margins", "text": "<p>You can define the margins you want with:</p> <ul> <li><code>ma-2</code>: 2 points in all directions.</li> <li><code>mb-2</code>: 2 points of margin on bottom.</li> <li><code>mt-2</code>: 2 points of margin on top.</li> <li><code>mr-2</code>: 2 points of margin on right.</li> <li><code>ml-2</code>: 2 points of margin on left.</li> </ul> <p>If instead of a number you use <code>auto</code> it will fill it till the end of the container.</p> <p>To center things around, you can use <code>mx-auto</code> to center in the X axis and <code>my-auto</code> for the Y axis.</p> <p>If you are using a <code>flex-column</code> and you want to put an element to the bottom, you'll use <code>mt-auto</code> so that the space filled on top of the element is filled automatically.</p>"}, {"location": "vuetify/#flex-grow-and-shrink", "title": "Flex grow and shrink", "text": "<p>Vuetify has helper classes for applying grow and shrink manually. These can be applied by adding the helper class in the format <code>flex-{condition}-{value}</code>, where condition can be either <code>grow</code> or <code>shrink</code> and value can be either <code>0</code> or <code>1</code>. The condition <code>grow</code> will permit an element to grow to fill available space, whereas <code>shrink</code> will permit an element to shrink down to only the space needs for its contents. However, this will only happen if the element must shrink to fit their container such as a container resize or being effected by a <code>flex-grow-1</code>. The value <code>0</code> will prevent the condition from occurring whereas <code>1</code> will permit the condition. The following classes are available:</p> <ul> <li><code>flex-grow-0</code></li> <li><code>flex-grow-1</code></li> <li><code>flex-shrink-0</code></li> <li><code>flex-shrink-1</code></li> </ul> <p>For example:</p> <pre><code>&lt;template&gt;\n  &lt;v-container&gt;\n    &lt;v-row\n      no-gutters\n      style=\"flex-wrap: nowrap;\"\n    &gt;\n      &lt;v-col\n        cols=\"2\"\n        class=\"flex-grow-0 flex-shrink-0\"\n      &gt;\n        &lt;v-card&gt;\n          I'm 2 column wide\n        &lt;/v-card&gt;\n      &lt;/v-col&gt;\n      &lt;v-col\n        cols=\"1\"\n        style=\"min-width: 100px; max-width: 100%;\"\n        class=\"flex-grow-1 flex-shrink-0\"\n      &gt;\n        &lt;v-card&gt;\n          I'm 1 column wide and I grow to take all the space\n        &lt;/v-card&gt;\n      &lt;/v-col&gt;\n      &lt;v-col\n        cols=\"5\"\n        style=\"min-width: 100px;\"\n        class=\"flex-grow-0 flex-shrink-1\"\n      &gt;\n        &lt;v-card&gt;\n          I'm 5 column wide and I shrink if there's not enough space\n        &lt;/v-card&gt;\n      &lt;/v-col&gt;\n    &lt;/v-row&gt;\n  &lt;/v-container&gt;\n&lt;/template&gt;\n</code></pre>"}, {"location": "vuetify/#position-elements-with-flex", "title": "Position elements with Flex", "text": "<p>If the properties above don't give you the control you need you can use rows and columns directly. Vuetify comes with a 12 point grid system built using Flexbox. The grid is used to create specific layouts within an application\u2019s content.</p> <p>Using <code>v-row</code> (as a flex-container) and <code>v-col</code> (as a flex-item).</p> <pre><code> &lt;v-container&gt;\n   &lt;v-row&gt;\n    &lt;v-col&gt;\n      &lt;v-card\n        class=\"pa-2\"\n        outlined\n        tile\n      &gt;\n        One of three columns\n      &lt;/v-card&gt;\n    &lt;/v-col&gt;\n    &lt;v-col&gt;\n      &lt;v-card\n        class=\"pa-2\"\n        outlined\n        tile\n      &gt;\n        One of three columns\n      &lt;/v-card&gt;\n    &lt;/v-col&gt;\n    &lt;v-col&gt;\n      &lt;v-card\n        class=\"pa-2\"\n        outlined\n        tile\n      &gt;\n        One of three columns\n      &lt;/v-card&gt;\n    &lt;/v-col&gt;\n  &lt;/v-row&gt;\n&lt;/v-container&gt;\n</code></pre> <p><code>v-row</code> has the next properties:</p> <ul> <li><code>align</code>: set the vertical alignment of flex items (one of     <code>start</code>, <code>center</code> and <code>end</code>). It also has one property for each device size     (<code>align-md</code>, <code>align-xl</code>, ...). The <code>align-content</code> variation is also     available.</li> <li><code>justify</code>: set the horizontal alignment of the flex items (one of <code>start</code>,     <code>center</code>, <code>end</code>, <code>space-around</code>, <code>space-between</code>). It also has one property for each device size     (<code>justify-md</code>, <code>justify-xl</code>, ...).</li> <li><code>no-gutters</code>: Removes the spaces between items.</li> <li><code>dense</code>: Reduces the spaces between items.</li> </ul> <p><code>v-col</code> has the next properties:</p> <ul> <li> <p><code>cols</code>: Sets the default number of columns the component extends. Available     options are <code>1 -&gt; 12</code> and <code>auto</code>. you can use <code>lg</code>, <code>md</code>, ... to define the     number of columns for the other sizes.</p> </li> <li> <p><code>offset</code>: Sets the default offset for the column. You can also use <code>offset-lg</code>     and the other sizes.</p> </li> </ul>"}, {"location": "vuetify/#keep-the-structure-even-if-some-components-are-hidden", "title": "Keep the structure even if some components are hidden", "text": "<p>If you want the components to remain in their position even if the items around disappear, you need to use <code>&lt;v-row&gt;</code> and <code>&lt;v-col&gt;</code>. For example:</p> <pre><code>&lt;v-row align=end justify=center class=\"mt-auto\"&gt;\n  &lt;v-col align=center&gt;\n    &lt;v-btn\n      v-show=isNotFirstElement\n      ...\n    &gt;Button&lt;/v-btn&gt;\n  &lt;/v-col&gt;\n  &lt;v-col align=center&gt;\n    &lt;v-rating\n      v-show=\"isNotLastElement\"\n      ...\n    &gt;&lt;/v-rating&gt;\n  &lt;/v-col&gt;\n  &lt;v-col align=center&gt;\n    &lt;v-btn\n      v-show=\"isNotLastVisitedElement &amp;&amp; isNotLastElement\"\n      ...\n    &gt;Button&lt;/v-btn&gt;\n  &lt;/v-col&gt;\n&lt;/v-row&gt;\n</code></pre> <p>If instead you had use the next snippet, whenever one of the elements got hidden, the rest would move around to fill up the remaining space.</p> <pre><code>&lt;v-row align=end justify=center class=\"mt-auto\"&gt;\n  &lt;v-btn\n    v-show=isNotFirstElement\n    ...\n  &gt;Button&lt;/v-btn&gt;\n  &lt;v-rating\n    v-show=\"isNotLastElement\"\n    ...\n  &gt;&lt;/v-rating&gt;\n  &lt;v-btn\n    v-show=\"isNotLastVisitedElement &amp;&amp; isNotLastElement\"\n    ...\n  &gt;Button&lt;/v-btn&gt;\n&lt;/v-row&gt;\n</code></pre>"}, {"location": "vuetify/#themes", "title": "Themes", "text": "<p>Vuetify comes with two themes pre-installed, light and dark. To set the default theme of your application, use the <code>defaultTheme</code> option.</p> <p>File: <code>src/plugins/vuetify.js</code></p> <pre><code>import { createApp } from 'vue'\nimport { createVuetify } from 'vuetify'\n\nexport default createVuetify({\ntheme: {\ndefaultTheme: 'dark'\n}\n})\n</code></pre> <p>Adding new themes is as easy as defining a new property in the <code>theme.themes</code> object. A theme is a collection of colors and options that change the overall look and feel of your application. One of these options designates the theme as being either a light or dark variation. This makes it possible for Vuetify to implement Material Design concepts such as elevated surfaces having a lighter overlay color the higher up they are.</p> <p>File: <code>src/plugins/vuetify.js</code></p> <pre><code>import { createApp } from 'vue'\nimport { createVuetify, ThemeDefinition } from 'vuetify'\n\nexport default createVuetify({\ntheme: {\ndefaultTheme: 'myCustomLightTheme',\nthemes: {\nmyCustomLightTheme: {\ndark: false,\ncolors: {\nbackground: '#FFFFFF',\nsurface: '#FFFFFF',\nprimary: '#510560',\n'primary-darken-1': '#3700B3',\nsecondary: '#03DAC6',\n'secondary-darken-1': '#018786',\nerror: '#B00020',\ninfo: '#2196F3',\nsuccess: '#4CAF50',\nwarning: '#FB8C00',\n}\n}\n}\n}\n})\n</code></pre> <p>To dynamically change theme during runtime.</p> <pre><code>&lt;template&gt;\n&lt;v-app&gt;\n&lt;v-btn @click=\"toggleTheme\"&gt;toggle theme&lt;/v-btn&gt;\n...\n&lt;/v-app&gt;\n&lt;/template&gt;\n\n&lt;script&gt;\nimport { useTheme } from 'vuetify'\n\nexport default {\nsetup () {\nconst theme = useTheme()\n\nreturn {\ntheme,\ntoggleTheme: () =&gt; theme.global.name.value = theme.global.current.value.dark ? 'light' : 'dark'\n}\n}\n}\n&lt;/script&gt;\n</code></pre> <p>Most components support the <code>theme</code> prop. When used, a new context is created for that specific component and all of its children. In the following example, the <code>v-btn</code> uses the dark theme applied by its parent <code>v-card</code>.</p> <pre><code>&lt;template&gt;\n&lt;v-app&gt;\n&lt;v-card theme=\"dark\"&gt;\n&lt;!-- button uses dark theme --&gt;\n&lt;v-btn&gt;foo&lt;/v-btn&gt;\n&lt;/v-card&gt;\n&lt;/v-app&gt;\n&lt;/template&gt;\n</code></pre>"}, {"location": "vuetify/#elements", "title": "Elements", "text": ""}, {"location": "vuetify/#cards", "title": "Cards", "text": "<p>The <code>v-card</code> can be used to place any kind of text on your site, in this case use the <code>variant=text</code>.</p>"}, {"location": "vuetify/#buttons", "title": "Buttons", "text": "<p>The <code>sizes</code> can be: <code>x-small</code>, <code>small</code>, <code>default</code>, <code>large</code>, <code>x-large</code>.</p>"}, {"location": "vuetify/#illustrations", "title": "Illustrations", "text": "<p>You can get nice illustrations for your web on Drawkit, for example I like to use the Classic kit.</p>"}, {"location": "vuetify/#icons", "title": "Icons", "text": "<p>The <code>v-icon</code> component provides a large set of glyphs to provide context to various aspects of your application.</p> <pre><code>&lt;v-icon&gt;fas fa-user&lt;/v-icon&gt;\n</code></pre> <p>If you have the FontAwesome icons installed, browse them here</p>"}, {"location": "vuetify/#install-font-awesome-icons", "title": "Install font awesome icons", "text": "<pre><code>npm install @fortawesome/fontawesome-free -D\n</code></pre> <pre><code>// src/plugins/vuetify.js\nimport '@fortawesome/fontawesome-free/css/all.css' // Ensure your project is capable of handling css files\nimport { createVuetify } from 'vuetify'\nimport { aliases, fa } from 'vuetify/lib/iconsets/fa'\n\nexport default createVuetify({\nicons: {\ndefaultSet: 'fa',\naliases,\nsets: {\nfa,\n},\n},\n})\n</code></pre> <pre><code>&lt;template&gt;\n  &lt;v-icon icon=\"fas fa-home\" /&gt;\n&lt;/template&gt;\n</code></pre>"}, {"location": "vuetify/#fonts", "title": "Fonts", "text": "<p>By default it uses the webfontload plugin which slows down a lot the page load, instead you can install the fonts directly. For example for the Roboto font:</p> <ul> <li> <p>Install the font</p> <pre><code>npm install --save typeface-roboto\n</code></pre> </li> <li> <p>Uninstall the webfontload plugin</p> <pre><code>npm remove webfontloader\n</code></pre> </li> <li> <p>Remove the loading of the webfontload in <code>/main.js</code> the lines:</p> <p><pre><code>import { loadFonts } from './plugins/webfontloader'\n\nloadFonts()\n</code></pre> * Add the font in the <code>App.vue</code> file:</p> <pre><code>&lt;style lang=\"sass\"&gt;\n@import '../node_modules/typeface-roboto/index.css'\n&lt;/style&gt;\n</code></pre> </li> </ul>"}, {"location": "vuetify/#carousels", "title": "Carousels", "text": "<p>Vuetify has their own carousel component, here's it's API. In the Awesome Vue.js compilation there are other suggestions. As some users say, it looks like Vuetify's doesn't have the best responsive behaviour.</p> <p>The best looking alternatives I've seen are:</p> <ul> <li>vue-agile:     Demo.</li> <li>vue-picture-swipe</li> <li>vue-slick-carousel:     Demo. It     doesn't yet support Vue3</li> <li>swiper: Demo</li> <li>vue-splide: Demo</li> </ul>"}, {"location": "vuetify/#vuetify-component", "title": "Vuetify component", "text": "<p>I tried binding the model with <code>v-model</code> but when I click on the arrows, the image doesn't change and the binded property doesn't change. If I change the property with other component, the image does change</p>"}, {"location": "vuetify/#vue-agile", "title": "vue-agile", "text": "<p>If you encounter the <code>modules have no default</code> error, add this to your <code>vite.config.js</code>:</p> <pre><code>export default defineConfig({\n...\noptimizeDeps: { include: [ 'lodash.throttle', 'lodash.orderby' ] },\n...\n})\n</code></pre>"}, {"location": "vuetify/#small-vertical-carousel", "title": "Small vertical carousel", "text": "<p>If you want to do a vertical carousel for example the one shown in the video playlists, you can't yet use <code>v-slide-group</code>. vue-agile doesn't either yet have vertical option.</p>"}, {"location": "vuetify/#audio", "title": "Audio", "text": "<ul> <li>vuejs-sound-player</li> <li>vue-audio-visual: Demo</li> <li>vue3-audio-player:     Demo</li> <li>vuetify-audio:     Demo</li> </ul>"}, {"location": "vuetify/#testing", "title": "Testing", "text": "<p>I tried doing component tests with Jest, Vitest and Cypress and found no way of making component tests, they all fail one way or the other.</p> <p>E2E tests worked with Cypress however, that's going to be my way of action till this is solved.</p>"}, {"location": "vuetify/#references", "title": "References", "text": "<ul> <li>Docs</li> <li>Home</li> <li>Git</li> <li>Discord</li> </ul>"}, {"location": "wake_on_lan/", "title": "Wake on LAN", "text": "<p>Wake on LAN (WoL) is a feature to switch on a computer via the network.</p>"}, {"location": "wake_on_lan/#usage", "title": "Usage", "text": ""}, {"location": "wake_on_lan/#host-configuration", "title": "Host configuration", "text": "<p>On the host you want to activate the wake on lan execute:</p> <pre><code>$: ethtool *interface* | grep Wake-on\n\nSupports Wake-on: pumbag\nWake-on: d\n</code></pre> <p>The Wake-on values define what activity triggers wake up: d (disabled), p (PHY activity), u (unicast activity), m (multicast activity), b (broadcast activity), a (ARP activity), and g (magic packet activity). The value g is required for WoL to work, if not, the following command enables the WoL feature in the driver:</p> <pre><code>$: ethtool -s interface wol g\n</code></pre> <p>If it was not enabled check in the Arch wiki how to make the change persistent.</p> <p>To trigger WoL on a target machine, its MAC address must be known. To obtain it, execute the following command from the machine:</p> <pre><code>$: ip link\n\n1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default\n   link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n2: enp1s0: &lt;BROADCAST,MULTICAST,PROMISC,UP,LOWER_UP&gt; mtu 1500 qdisc fq_codel master br0 state UP group default qlen 1000\nlink/ether 48:05:ca:09:0e:6a brd ff:ff:ff:ff:ff:ff\n</code></pre> <p>Here the MAC address is <code>48:05:ca:09:0e:6a</code>.</p> <p>In its simplest form, Wake-on-LAN broadcasts the magic packet as an ethernet frame, containing the MAC address within the current network subnet, below the IP protocol layer. The knowledge of an IP address for the target computer is not necessary, as it operates on layer 2 (Data Link).</p> <p>If used to wake up a computer over the internet or in a different subnet, it typically relies on the router to relay the packet and broadcast it. In this scenario, the external IP address of the router must be known. Keep in mind that most routers by default will not relay subnet directed broadcasts as a safety precaution and need to be explicitly told to do so.</p>"}, {"location": "wake_on_lan/#client-trigger", "title": "Client trigger", "text": "<p>If you are connected directly to another computer through a network cable, or the traffic within a LAN is not firewalled, then using Wake-on-LAN should be straightforward since there is no need to worry about port redirects.</p> <p>If it's firewalled you need to configure the client firewall to allow the outgoing UDP traffic to the port 9.</p> <p>In the simplest case the default broadcast address 255.255.255.255 is used:</p> <pre><code>$ wakeonlan *target_MAC_address*\n</code></pre> <p>To broadcast the magic packet only to a specific subnet or host, use the <code>-i</code> switch:</p> <pre><code>$ wakeonlan -i *target_IP* *target_MAC_address*\n</code></pre>"}, {"location": "wake_on_lan/#references", "title": "References", "text": "<ul> <li>Arch wiki post</li> </ul>"}, {"location": "wallabag/", "title": "Wallabag", "text": "<p>Wallabag is a self-hosted read-it-later application: it saves a web page by keeping content only. Elements like navigation or ads are deleted.</p>"}, {"location": "wallabag/#installation", "title": "Installation", "text": "<p>They provide a working docker-compose</p> <pre><code>version: '3'\nservices:\nwallabag:\nimage: wallabag/wallabag\nenvironment:\n- MYSQL_ROOT_PASSWORD=wallaroot\n- SYMFONY__ENV__DATABASE_DRIVER=pdo_mysql\n- SYMFONY__ENV__DATABASE_HOST=db\n- SYMFONY__ENV__DATABASE_PORT=3306\n- SYMFONY__ENV__DATABASE_NAME=wallabag\n- SYMFONY__ENV__DATABASE_USER=wallabag\n- SYMFONY__ENV__DATABASE_PASSWORD=wallapass\n- SYMFONY__ENV__DATABASE_CHARSET=utf8mb4\n- SYMFONY__ENV__SECRET=supersecretenv\n- SYMFONY__ENV__MAILER_HOST=127.0.0.1\n- SYMFONY__ENV__MAILER_USER=~\n- SYMFONY__ENV__MAILER_PASSWORD=~\n- SYMFONY__ENV__FROM_EMAIL=wallabag@example.com\n- SYMFONY__ENV__DOMAIN_NAME=https://your-wallabag-url-instance.com\n- SYMFONY__ENV__SERVER_NAME=\"Your wallabag instance\"\nports:\n- \"80\"\nvolumes:\n- /opt/wallabag/images:/var/www/wallabag/web/assets/images\nhealthcheck:\ntest: [\"CMD\", \"wget\" ,\"--no-verbose\", \"--tries=1\", \"--spider\", \"http://localhost\"]\ninterval: 1m\ntimeout: 3s\ndepends_on:\n- db\n- redis\ndb:\nimage: mariadb\nenvironment:\n- MYSQL_ROOT_PASSWORD=wallaroot\nvolumes:\n- /opt/wallabag/data:/var/lib/mysql\nhealthcheck:\ntest: [\"CMD\", \"mysqladmin\" ,\"ping\", \"-h\", \"localhost\"]\ninterval: 20s\ntimeout: 3s\nredis:\nimage: redis:alpine\nhealthcheck:\ntest: [\"CMD\", \"redis-cli\", \"ping\"]\ninterval: 20s\ntimeout: 3s\n</code></pre> <p>If you don't want to enable the public registration use the <code>SYMFONY__ENV__FOSUSER_REGISTRATION=false</code> flag. The emailer configuration is only used when a user creates an account, so if you're only going to use it for yourself, it's safe to disable.</p> <p>Remember to change all passwords to a random value.</p> <p>If you create RSS feeds for a user, all articles are shared by default, if you only want to share the starred articles, add to your nginx config:</p> <pre><code>    location ~* /feed/.*/.*/(?!starred){\n        deny all;\n        return 404;\n    }\n</code></pre>"}, {"location": "wallabag/#references", "title": "References", "text": "<ul> <li>Docs</li> </ul>"}, {"location": "week_management/", "title": "Week Management", "text": "<p>I've been polishing a week reviewing and planning method that suits my needs. I usually follow it on Wednesdays, as I'm too busy on Mondays and Tuesdays and it gives enough time to plan the weekend.</p>"}, {"location": "week_management/#week-review", "title": "Week review", "text": "<p>Life logging is the only purpose of my weekly review.</p> <p>I've made <code>diw</code> a small python script that for each overdue task allows me to:</p> <ul> <li>Review: Opens vim to write a diary entry related with the task. The text is   saved as an annotation of the task and another form is filled to record whom   I've shared it with. The last information is used to help me take care of   people around me.</li> <li>Skip: Don't interact with this task.</li> <li>Done: Complete the task.</li> <li>Delete: Remove the task.</li> <li>Reschedule: Opens a form to specify the new due date.</li> </ul>"}, {"location": "week_management/#week-planning", "title": "Week planning", "text": "<p>The purpose of the planning is to make sure that I know what I need to do and arrange all tasks in a way that allows me not to explode.</p> <p>First I empty the INBOX file, refactoring all the information in the other knowledge sinks. It's the place to go to quickly gather information, such as movie/book/serie recommendations, human arrangements, miscellaneous thoughts or tasks. This file lives in my mobile. I edit it with Markor and transfer it to my computer with Share via HTTP.</p> <p>Taking different actions to each INBOX element type:</p> <ul> <li>Tasks or human arrangements: Do it if it can be completed in less than   3 minutes. Otherwise, create a taskwarrior task.</li> <li>Behavior: Add it to taskwarrior.</li> <li>Movie/Serie recommendation: Introduce it into my media monitorization system.</li> <li>Book recommendation: Introduce into my library management system.</li> <li>Miscellaneous thoughts: Refactor into the blue-book, project documentation or   Anki.</li> </ul> <p>Then I split my workspace in two terminals, in the first I run <code>task due.before:7d diary</code> where diary is a taskwarrior report that shows pending tasks that are not in the backlog sorted by due date. On the other I:</p> <ul> <li>Execute <code>gcal .</code> to show the calendar of the previous, current and next month.</li> <li>Check the weather for the whole week to decide which plans are suitable.</li> <li>Analyze the tasks that need to be done answering the following questions:</li> <li>Do I need to do this task this week? If not, reschedule or delete it.</li> <li>Does it need a due date? If not, remove the <code>due</code> attribute.     Having the minimum number of tasks with a fixed date reduces wasted     rescheduling time and allows better prioritizing.</li> <li>Can I do the task on the selected date? As most humans, I tend to     underestimate both the required time to complete a task and to switch     contexts. To avoid it, If the day is full it's better to reschedule.</li> <li>Check that every day has at least one task. Particularly tasks that will   help with life logging.</li> <li>If there aren't enough things to fill up all days, check the things that   I want to do list and try to do one.</li> </ul>"}, {"location": "wesnoth/", "title": "The Battle for Wesnoth", "text": "<p>The Battle for Wesnoth is an open source, turn-based strategy game with a high fantasy theme. It features both singleplayer and online/hotseat multiplayer combat.</p> <p>Explore the world of Wesnoth and take part in its different adventures! Embark on a desperate quest to reclaim your rightful throne\u2026 Flee the Lich Lords to a new home across the sea\u2026 Delve into the darkest depths of the earth to craft a jewel of fire itself\u2026 Defend your kingdom against the ravaging hordes of a foul necromancer\u2026 Or lead a straggly band of survivors across the blazing sands to confront an unseen evil.</p>"}, {"location": "wesnoth/#references", "title": "References", "text": "<ul> <li>Home</li> <li>Wiki</li> <li>Game manual</li> <li>Mainline campaigns</li> </ul>"}, {"location": "wesnoth_loyalist/", "title": "Loyalist", "text": "<p>The Loyalist are a faction of Humans who are loyal to the throne of Wesnoth.</p> <p>The race of men is an extremely diverse one. Although they originally came from the Old Continent, men have spread all over the world and split into many different cultures and races. Although they are not imbued with magic like other creatures, humans can learn to wield it and able to learn more types than most others. They have no extra special abilities or aptitudes except their versatility and drive. While often at odds with all races, they can occasionally form alliances with the less aggressive races such as elves and dwarves. The less scrupulous among them do not shrink back from hiring orcish mercenaries, either. They have no natural enemies, although the majority of men, like most people of all races, have an instinctive dislike of the undead. Men are shorter than the elves, but taller still than dwarves. Their skin color can vary, from almost white to dark brown. Humans are a versatile race who specialize in many different areas.</p> <p>Similarly, the Loyalist faction is a very versatile melee oriented faction with important ranged support from bowmen and mages.</p>"}, {"location": "wesnoth_loyalist/#how-to-play-loyalists", "title": "How to play loyalists", "text": "<p>Loyalists are considered to be the most versatile faction in the game. They have most available unit types to recruit (8), more than any other faction.</p> <p>Loyalists are mostly melee oriented faction, with only two ranged unit types, the mage and the bowman, but their ranged units play a significant role in the loyalists strategy and have to be used effectively. About the melee troops the loyalist player gets to choose among:</p> <ul> <li> <p>Heavy Infantrymen: Few strikes, high damage per strike, slow, higher     hit-points, good resistances, low defenses, deals impact damage which is good     against undead.</p> </li> <li> <p>Spearmen: Average strikes, average damage per strike, average movement,     medium hit-points, normal resistances, average defenses, deals pierce damage     which is good against drakes, has a weak ranged attack.</p> </li> <li> <p>Fencer: High number of strikes, low damage per strike, quick, low hit-points,     bad resistances, good defenses, deals blade damage which is good against     elusive foots or wose, deals less total damage than the other two, is     a skirmisher.</p> </li> </ul> <p>The loyalists unit price ranges from the cheap spearmen and mermen (14 gold) to the expensive horsemen (23 gold).</p> <p>Loyalist units generally have good hit-points and they deal good damage, even the cheap spearmen and bowmen. Even their scouts have high hit-points and better melee attack compared to most of the scouts of the other factions. But on the other hand, the loyalist scouts have lower defenses than most of the other scouts and they do not have a ranged attack.</p> <p>The loyalists have some units with good resistances, like Heavy Infantry or Cavalryman, which can be very good for defending if you notice that your opponent doesn't have the proper units to counter them. Other units, like the bowman, who is more vulnerable than the previous two, but is also good for defense because it has better defense, is cheaper and it deals solid damage back in both melee and range. Similar goes for the spearman.</p> <p>When attacking, Loyalists have units which can be pretty devastating, like the horseman for attacking enemy mages and other ranged units, mages against the entrenched melee units, fencers for killing enemy injured and valuable units trying to heal behind their lines. But the mentioned units are also very vulnerable and they can die very easily if not properly supported.</p> <p>The loyalists as a faction generally have average defenses (they do not get 70% defense anywhere, with the exception of the fencer, which is a special case) and they are not a terrain dependent faction (like the Elves or Knalgans are). Therefore, even if it good to put spearmen and bowmen on mountains, villages and castles where they get 60% defense, you should also take the faction you are facing into account when deciding where to place your units. For example, if you are fighting Knalgans, you should try not to move your units next to hills and mountains and/or take the hills and mountains with your units and leave the forest to the dwarves.</p> <p>All the loyalist units are lawful, which obviously leads to the conclusion that the loyalists should be more aggressive and attack at day (or even start at dawn, because your opponent in most of the cases will be reluctant to attack too aggressively and leave units on bad ground when the morning arrives) while at night the loyalists should at least play conservatively, or in some cases even run away. Of course, that is also dependent upon the faction of your opponent.</p> <p>The greatest weakness of the loyalists is their limited mobility. The loyalists' units have normal movement, some of their units like the horseman, cavalryman, merman fighter and the fencer are pretty quick when they are in their favorable environment, but all their units get slowed over rough terrain a lot compared to the other factions' units . Half of the loyalists units can not even move on/over mountains, and their scouts get significantly slowed in forest, for example. This lack of mobility will often give your opponent an opportunity to outmaneuver you in the game.To prevent that, sometimes it is good to attack hard on a certain point and defend only with minimal amount of units elsewhere, rather than uniformly spreading your units around the front.</p> <p>When you don't know which faction you are facing, it is good to get a mixture of units. On most of the default 1v1 maps, this initial recruitment list would be good : spearman, bowman, fencer, mage, merman fighter, cavalryman. As always, you should carefully consider about the exact placement of each individual unit from the initial recruits on the castle to come up with the most optimal combination that will allow you the quickest village grabbing in the first turns of the game.</p>"}, {"location": "wesnoth_loyalist/#loyalists-vs-undead", "title": "Loyalists vs. Undead", "text": "<p>When fighting undead, your recruiting pattern will depend on whether your opponent spams skeleton archers or dark adepts for ranged attacks. If you see lots of skeletons, you'll have to focus heavily on heavy infrantrymen and mages. If you see lots of dark adepts, you'll want some cavalry, horsemen and maybe some spearmen.</p> <ul> <li> <p>Bowman: (D-) Almost entirely useless. You might use them to poke at     walking corpses, ghouls, vampire bats and ghosts, but mages are     much better at all of these things. I would only buy a bowman to counter     a large zombie horde.</p> </li> <li> <p>Cavalryman: (B+) You'll want some cavalry for scouting and dealing     initial damage to dark adepts so that a horseman can finish them. They     are also decent at fighting Skeletons in the daytime, because cavalry are     blade-resistant. However, the cheap skeleton archers will really ruin     cavalry quickly at night, so if there are any skeleton archers on the other     side you won't be able to use cavalry to hold territory. If your opponent     over-recruits dark adepts, however, you can use cold-resistant cavalry to hold     territory against them. They are also decent for holding territory against     ghouls because they can run away and heal (unlike your heavy infantrymen).</p> </li> <li> <p>Fencer: (C-) Fencers are a bad recruit in this match up because they are     vulnerable to the blade and pierce damage of skeletons and cannot damage     them much in return. They also are incapable of holding territory against     dark adepts, who cut right through the high defense of fencers. However,     you may want to have a fencer or two around for trapping dark adepts or     getting in that last hit. With luck, they may also be able to frustrate     non-magical attacks from skeletons and the like.</p> </li> <li> <p>Heavy Infantryman: (B+) You need heavy infantry to hold territory     against skeletons and skeleton archers, and they will be your unit of     choice for dealing melee damage, especially to the cheap skeleton archers.     Any heavy infantrymen in the daytime can kill a full health skeleton     archer in two hits, while a strong heavy infantrymen in the daytime can     kill a full health skeleton in two hits, dealing 18 damage per strike (even     a mage in daytime cannot kill a skeleton in one round, unless supported by     a lieutenant). A fearless heavy infantryman may be dangerous even at     night. If you don't have enough heavy infantryman to go around, you can     get your initial hits in on a skeleton archer with them and finish him     with a mage. Just keep heavy infantrymen away from dark adepts, and     only let ghouls hit heavy infantrymen if they are on a village (or has     a white mage or other healer next to him), since they can't easily run     away to heal. Also beware walking corpses, which deal a surprising amount     of damage at all times of day since heavy infantrymen can't dodge and     impact damage goes around their resistances, and the ranged cold attack of     ghosts. The biggest problem with heavy infantrymen is they are slow,     which means they're hard to retreat at night and hard to advance in day.     Without shielding units they'll get trapped and killed, and if you have to     shield a unit maybe it should be a mage instead.</p> </li> <li> <p>Horseman: (B) - Because they deal pierce damage, horsemen may not be     very useful when faced with skeletons. However, if your opponent     over-recruits dark adepts, horsemen can be extremely useful, as dark     adepts deal no return damage to the normally risky charge attack.     horsemen can even be used to finish skeleton archers, their nemesis, in     the daytime. However, if your opponent recruits enough skeleton archers     you will have a hard time shielding your horsemen from their devastating     pierce attacks, and skeleton archers are dirt cheap. horsemen can also     one-shot bats and zombies, which can be useful if you need to clear out     a lot of level 0 units quickly. I would want to have at least one horseman     around to keep my opponent from getting too bold with dark adepts, if not     more. Your opponent will be forced to recruit dark adepts if you have     heavy infantrymen in the field.</p> </li> <li> <p>Mage: (A+) mages are an absolute necessity against Undead. If you do     not have mages it will be almost impossible for you to kill ghosts, but     with mages it's a piece of cake. mages are the best unit for killing     almost everything Undead can throw at you, and can even be used to finish     dark adepts in the daytime. Your main problem is that dark adepts are     cheaper and deal almost as much damage, so your opponent can spam dark     adepts while you cannot afford to spam mages. You will also have the     difficult task of shielding fragile, expensive mages against lots of cheap     Undead units. Your opponent will use skeletons and ghouls to attack your     mages when he can, but bats, zombies or just about any other unit will     do for killing your mages in a pinch. Shield your mages well, surround     them with damage soakers and if you can deliver them safely to their targets     you'll be able to clear out the Undead quickly.</p> </li> <li> <p>Merman Fighter: (C-) Mermen make a decent navy against Undead, since     bats and ghosts will have a hard time killing them with their high     defense. Even dark adepts will find Mermen annoying because of their 20%     cold resistance. However, Mermen will have a hard time hurting anything the     Undead have with their lame pierce weapon. Generally Mermen are only good     for holding water hexes and scouting, but don't underestimate how useful     that can be. Some well-placed Mermen on a water map can prevent bats from     sneaking behind your lines and capturing villages or killing injured units.     Even on mostly land maps, a Merman in a river can help hold a defensive     line, or a quick Merman can use a river to slip behind the enemy to trap     dark adepts or other units that are trying to escape at daytime.</p> </li> <li> <p>Spearman: (C-) Spearmen are mostly useful as cheap units for holding     villages and occupying space when faced with dark adepts or skeleton     archers. (You'll want to avoid letting dark adepts hit your heavy     infantrymen because of their vulnerability to cold.) However, you don't     really want spearmen to take hits from dark adepts, it would be better     to let the cold-resistant cavalry absorb the damage. The only units     spearmen are good for attacking are dark adepts and walking corpses.     spearmen are completely useless against skeletons unless you level one     into a swordsman, and even then they're pretty mediocre. However, if there     are lots of skeleton archers you won't be able to use much cavalry or     horsemen, so a spearman or two may be necessary as defenders and damage     soakers even if they are lousy at dealing damage to Undead.</p> </li> </ul>"}, {"location": "wesnoth_loyalist/#loyalists-vs-rebels", "title": "Loyalists vs. Rebels", "text": "<p>Rebels are a very versatile faction like the Loyalists. So you need to counter their versatility with yours. Their weakest trait is low hit-points. That is compensated somewhat by relatively higher defenses and mobility, so you'll need a combination of hard-hitters and speed. mages are also nice for their magical attacks. Anyways, the key for a Loyalist general is to effectively use his hard-hitting units and kill as many units as he can at day. And at night, to defend well as elves ignore the time of day.</p> <p>The smartest thing you can do upon discovering that your opponent is playing Rebels, in multiplayer, is to assume that your opponent has woses and recruit accordingly. woses are a necessary counter to Loyalists' daytime attacks and as soon as your opponent realizes that you are Loyalists he will almost certainly recruit woses. Remember that just because you don't see any woses doesn't mean there aren't a couple hiding in the forest! To fight woses you will need cavalry as meat shields, mages to deal damage, and maybe fencers to get the last hit in. Sadly, cavalry are very vulnerable to elvish archers, so you'll need to make sure that the archers die, which can be difficult if they are in forest. Get horsemen which can one-shot archers (despite their high dodge) and keep up with your cavalry. If one horseman misses, follow up with another one, it's a lucky archer that can dodge 4/4 one-shot attempts. heavy infantrymen are also great against everything but woses and mages, and will reduce the effectiveness of archer spam. spearmen are useless against woses, recruit them sparingly and keep them away from unscouted forest.</p> <p>An alternative way of playing this match up is to maximize the mobility offered by the Loyalists' fastest units, namely the two horse units. By using them to outrun the Rebel units you can achieve some devastating rushes. Some matches require use of both styles of playing. Again, their versatility must be countered with yours.</p> <ul> <li> <p>Bowman: (C-) Rebels in general are effective in both melee and ranged     attacks, so recruiting a ranged unit is less beneficial than most factions     because most of the Rebels' units can retaliate against you. They can be     useful for taking out an Scout or two, but otherwise, this is not really     a smart buy.</p> </li> <li> <p>Cavalryman: (B+) A good scout and an effective counter to woses. Watch     out for archers, though, they can really tear horses to shreds. And as     always, they are effective against ranged oriented units like the mage.</p> <p>(A) If you're going for some faster action, cavalrymen are vital in the attack. They do great damage during the day, and combined with their dangerous mobility, they can be a fearsome unit indeed. Just be careful of archers ZoCing you and tearing the unit to shreds.</p> </li> <li> <p>Fencer: (B+) The fencer shares the 70% defense in the forest like most     of the elves, but it has negative resistances. Theoretically, they should be     effective against woses, but more often than not woses will crush them     easily. In spite of this, Fencers still have skirmsher, which means that     they can sneak behind an injured unit and finish them off. Do not     over-recruit them, as enemy mages will tear though their high defense.</p> </li> <li> <p>Heavy Infantryman: (C-) Usefulness similar to an archer. It's too slow     to deal with most of the Rebel units, and it is owned by Mages, woses, and     even archers. Even though it has a high damage potential and good     resistances, because of its inherently low defense shamans can easily get     hits on it and turn it into a piece of metal for a turn.</p> </li> <li> <p>Horseman: (B) One horseman may be nice, but no more than that. Enemy     archers will tear them apart and shamans totally mess them up. They're     expensive too. Their greatest use is probably killing mages or an archer     that has gone slightly off-track.</p> <p>(A-) Again, when mobility is required, these units need to come and do serious damage. Since Rebels are mostly comprised of low-hp units, horsemen at day can usually rip them apart. Again be careful of archers, for this unit is worth 23 gold. Shield him properly if he is damaged.</p> </li> <li> <p>Mage: (A-) You'll need these guys to tear though those high elvish     60-70% evasion in the forest; but be careful, archers and shamans will     retaliate and some pretty nasty things may come from that. One purely     positive thing though, they just absolutely destroy woses. 13-3 at day.     Ouch. mages are expensive and fragile though, so keep them protected.</p> </li> <li> <p>Merman Fighter: (B+) If the map has a lot of water, maybe recruit a few     to prevent the Rebel's Mermen Hunters from taking over the waters. Otherwise     they don't really contribute much else.</p> </li> <li> <p>Spearman - A - A necessity. To defend at night, to kill pretty much     anything except for woses, and to be cheap and cost only 14 gold. These     guys pretty much tear though most of the Rebel units, if it were not only     for the high-defense archer and shaman. Get a bunch, and move them like     a wall against the enemy units.</p> </li> </ul> <p>At the start of the game, recruit 1-2 cavalrymen, depending on the map size, 1 mage, (1 merman fighter if you need some water coverage), maybe 1 fencer, and the rest spearman. Later on you maybe can recruit some horseman if you opponent recruits mass mages, or more cavalrymen and mages if he masses woses. Otherwise, spearmen and mages should help you get through most of the match.</p> <p>If you're going for speed, recruit 2-3 cavalrymen, depending on the map, 1 horseman, maybe 1 fencer, and the rest spearmen. Use your spearmen to fill the holes and consolidate the territory that the horses took over. Use the horses to outrun the archers and attack when they can.</p>"}, {"location": "wesnoth_loyalist/#loyalists-vs-northerners", "title": "Loyalists vs Northerners", "text": "<p>The main problem you will have when facing northerners are two things: poison and numbers. If northerners didn't have assassins, it wouldn't be much of a big deal; you could out manoeuvre them with your superior scouting units and skirmishers to grab their villages and then finish them off one by one with spearmen/bowmen. Thing is, assassins poison makes defending at night extremely difficult, and by poisoning your units, they often force you to retreat and heal, or die due to retaliation. The key to winning this match up is to use your superior versatility; the northerners greatest weakness is that ALL of their units are very average and don't often have many special abilities; you have masses. Use your first strike in spearmen to defend against grunts, trolls and wolfriders (even at night), use your magic to finish off wounded assassins, and you can charge any of their wounded units and kill them with one hit. Since all of your units are lawful and theirs are chaotic, you'll want to press your advantage at day and kill as much as possible (but don't attack assassins with bowmen; you don't want to be poisoned and be forced to retreat in your daytime assault). Another problem the northerners have is that all though their units are many in number, they do not have a very high damage per hex ratio. Use this to your advantage; travel in packs and use ZoC lines to prevent your units from being swarmed too easily, and keep your vulnerable units like mages behind damage soakers like spearmen or heavy infantry. If you can, try and make night time defenses near rivers; northerners will hurt themselves when they attack you, as they have low defense in water and are very annoyed by deep water, because it prevents them from easily swarming you.</p> <ul> <li> <p>Spearmen: (A-) Pretty much your best unit against northerners. Their     first strike ability is great in defense and their attack power to cost     ratio is quite high. They also have good health and can retaliate in ranged.     As in many matchups, the bulk of your army.</p> </li> <li> <p>Mage: (B+) The mage is fragile and expensive and has a weak melee     attack, which is the exact opposite of northerners. Not a great defender     unless your opponent goes mad with assassins. However, they have lots of     attack power at day and are very useful for nocking trolls of their     perches and finishing off those damn assassins. You'll want a few of     these, but make sure you keep them protected.</p> </li> <li> <p>Bowmen: (B) Good unit to have. They can attack grunts and trolls     (although you'd want mages to attack trolls) without retaliation, and     can defend against assassins in the night. They can also take out any     wolves that stray too far from the main orchish army. They're not as good as     mages, but they're cheaper and tougher.</p> </li> <li> <p>Cavalryman: (B) cavalryman are superior to their wolves and can hold     ground against grunts and trolls reasonably well. It's also a good idea     if your opponent likes to spam lots of assassins, to let them be the     target of their poison; cavalryman and can run away and heal. Your heavy     infantrymen? Not so much. Beware though, of the cheap orchish archer, as     cavalrymen are weak to pierce.</p> </li> <li> <p>Heavy Infantryman: (B-) Heavy infantryman are heavily resistant to     physical attacks like blade, pierce and to a lesser degree, impact. You'd     think this would make them great units against northerners, if the orchish     archer didn't have a ranged fire attack. heavy infantrymen are also much     too slow too effectively deal with poison, and that's a 19 gold unit that     can't fight. However, they are useful in defense if you opponent hasn't     spammed many archers, and they can even be useful in attack to crush up     injured grunts.</p> </li> <li> <p>Horseman: (B) The horseman is a little controversial in this matchup.     northerners are cheap and melee orientated, which is exactly what horseman     do badly against. They're also quite tough, which means one hit kills are     rare. However, they have one very important advantage over the northerners-     high damage per hex. A horseman is very useful for softening up units or     outright killing them (particularly when paired with a mage) which will be     important in breaking ZoC lines, which can be a real pain with all the units     northerners can field. Get one or two, but no more, else it quickly produces     diminishing returns.</p> </li> <li> <p>Fencer: (C+) The fencer is a melee based unit that is fragile against     blade attacks of grunts, which means they don't have an awful lot of     fighting effectiveness with them. On the other hand, the fencer's skirmisher     ability is really valuable with the many northener units, and can finish off     injured archers or go on sneaky village grabbing incursions. One or two     might be useful, but no more, this unit is not a serious fighter! Make sure     you keep them on 70% terrain if you can as well. Two hitter grunts can     have trouble taking them out.</p> </li> <li> <p>Merman Fighter: (C+) Water based melee unit that doesn't do well against     the orc nagas, recruit only if there's lots of water and keep them in     defendible terrain, else they'll quickly be killed.</p> </li> </ul> <p>As usual, recruit cavalrymen and mermen depending on map (3-4 cavalryman for larger maps, 2-3 for medium sized ones and probably no more than one or two for small maps). Recruit plenty of spearmen and attack at day.</p> <p>Speaking of that, make sure you travel in large groups and do not attack with small numbers even at day. Northerners will swarm you too easily. Also, make sure you don't overextend yourself at day, as northerners do a lot more damage at night. Pay careful attention to your mages, they will be the ones the northerners will be going after, so retreat them quickly. When defending, try to avoid attacking units that can retaliate, unless you have a reasonably high chance of killing them that turn. Avoid attacking trolls if you can't heavily damage them or outright kill them, they're regenaration will quickly mean they can heal from any scratches your bowmen might have dealt them.</p>"}, {"location": "wesnoth_northerners/", "title": "Northerners", "text": "<p>Northerners are a faction of Orcs and their allies who live in the north of the Great Continent, thus their name. Northerners consist of the warrior orcs race, the enslaved goblins, trolls who are tricked into combat by the orcs, and the serpentine naga. The Northerners play best by taking advantage of having many low-cost and high HP soldiers.</p>"}, {"location": "wesnoth_rebels/", "title": "Rebels", "text": "<p>Rebels are a faction of Elves and their various forest-dwelling allies. They get their human name, Rebels, from the time of Heir to the Throne, when they started the rebellion against the evil Queen Asheviere. Elves are a magical race that are masters of the bow and are capable of living many years longer than humans. In harmony with nature, the elves find allies with the human mages, certain merfolk, and tree creatures called woses. Rebels are best played taking advantage of their high forest defense, mastery of ranged attacks, and the elves' neutral alignment.</p>"}, {"location": "wireshark/", "title": "Wireshark", "text": "<p>Wireshark is the world\u2019s foremost and widely-used network protocol analyzer. It lets you see what\u2019s happening on your network at a microscopic level and is the de facto (and often de jure) standard across many commercial and non-profit enterprises, government agencies, and educational institutions.</p>"}, {"location": "wireshark/#installation", "title": "Installation", "text": "<pre><code>apt-get install wireshark\n</code></pre> <p>If the version delivered by your distribution is not high enough, use Jezz's Docker</p> <pre><code>docker run -d \\\n-v /etc/localtime:/etc/localtime:ro \\\n-v /tmp/.X11-unix:/tmp/.X11-unix \\\n-e DISPLAY=unix$DISPLAY \\\n-v /tmp/wireshark:/data \\\njess/wireshark\n</code></pre>"}, {"location": "wireshark/#usage", "title": "Usage", "text": ""}, {"location": "wireshark/#filter", "title": "Filter", "text": "<p>You can filter by traffic type with <code>tcp and tcp.port == 80</code>, <code>http or ftp</code> or <code>not ftp</code>.</p> <p>It's also possible to nest many operators with <code>(http or ftp) and ip.addr == 192.168.1.14</code></p> <p>The most common filters are:</p> Item Description ip.addr IP address (check both source and destination) tcp.port TCP Layer 4 port (check both source and destination) udp.port UDP Layer 4 port (check both source and destination) ip.src IP source address ip.dst IP destination address tcp.srcport TCP source port tcp.dstport TCP destination port udp.srcport UDP source port udp.dstport UDP destination port icmp.type ICMP numeric type ip.tos.precedence IP precedence eth.addr MAC address ip.ttl IP Time to Live (TTL)"}, {"location": "wireshark/#references", "title": "References", "text": "<ul> <li>Home</li> </ul>"}, {"location": "work_interruption_analysis/", "title": "Work Interruption Analysis", "text": "<p>This is the interruption analysis report applied to my everyday work.</p> <p>I've identified the next interruption sources:</p> <ul> <li>Physical interruptions.</li> <li>Emails.</li> <li>Calls.</li> <li>Instant message applications.</li> <li>Calendar events.</li> </ul>"}, {"location": "work_interruption_analysis/#physical-interruptions", "title": "Physical interruptions", "text": "<p>Physical interactions are when someone comes to your desk and expect you to attend them immediately. These interruptions can be categorized as:</p> <ul> <li>Asking for help.</li> <li>Social interactions.</li> </ul> <p>The obvious solution is to remote work as much as possible. It's less easy for people to interrupt through digital channels than physically.</p> <p>It goes the other way around too. Be respectful to your colleagues and try to use asynchronous communications as much as possible, so they can manage when they attend you.</p>"}, {"location": "work_interruption_analysis/#asking-for-help", "title": "Asking for help", "text": "<p>These interruptions are the most difficult to delay, as it's hard to tell a person to wait when it's already in front of you. If you don't take care of them you may end up in the situation where you can receive 5 o 6 interruptions per minute which can drive you crazy. By definition all these events require an immediate action. The priority and delay may depend on many factors, such as the person or moment.</p> <p>The first thing I'd do is make a mental prioritization of the people that interrupt you, to decide which ones do you accept and which ones you need to regulate. Once you have it, work on how to assertively tell them that they need to reduce their interruptions. You can agree with them a non interruption time where they can aggregate and prepare all the questions so you can work through them efficiently. Often they are able to answer most of them themselves. The length of the period needs to be picked wisely as you want to be interrupted the minimum number of times while you don't make them loose their time trying to solve something you could work out quickly.</p> <p>Other times it's easier to forward them to the team's interruption manager.</p>"}, {"location": "work_interruption_analysis/#social-interactions", "title": "Social interactions", "text": "<p>Depending how popular you are, you'll have more or less of these interactions. The way I've found to be able to be in control of them is by scheduling social events in my calendar and introducing them in my task management workflow. For example, we agree to go to have lunch all together at the same hour every day, or I arrange a coffee break with someone every Monday at a defined hour.</p>"}, {"location": "work_interruption_analysis/#emails", "title": "Emails", "text": "<p>Email can be used as one of the main aggregators of interruptions as it's supported by almost everything. I use it as the notification of things that don't need to be acted upon immediately or when more powerful mechanisms are not available. In my case emails can be categorized as:</p> <ul> <li>General information: They don't usually require any direct action, so they can     wait more than 24 hours.</li> <li>Support to internal agents: At work, we have decided that email is not to be     used as the internal main communication channel, so I don't receive many and     their priority is low.</li> <li>Support to external agents: I'm lucky to not have many of these and they have     less priority than internal people so they can wait 4 or more hours.</li> <li>Infrastructure notifications: For example LetsEncrypt renewals or cloud provider     notification or support cases. The related actions can wait 4 hours or more.</li> <li>Calendar events: Someone creates a new meeting, changes an existing one or     confirms/declines its assistance. We have defined a policy that we don't     create or change events with less than 24 hours notice, and in the special     cases that we need to, they will be addressed in the chat rooms. So these     mails can be read once per day.</li> <li>Monitorization notifications: We've configured Prometheus's     alertmanager to send the notifications to the email as     a fallback channel, but it's to be checked only if the main channel is down.</li> <li>Source code manager notifications: The web where we host our source code sends     us emails when there are new pull requests or when there are comments on     existent ones. I automatically mark them as read and move them to a mail     directory as I manage these interruptions with other workflow.</li> <li>The CI sends notifications when some job fails. Unless it's a new     pipeline or I'm actively working on it, a failed job can wait four     hours broken before I interact with it.</li> <li>The issue tracker notifications: It sends them on new or changed issues.     At work, I filter them out as I delegate it's management to the Scrum     Master.</li> </ul> <p>In conclusion, I can check the work email only when I start working, on the lunch break and when I'm about to leave. So its safe to disable the notifications.</p> <p>I'm eager to start the email automation project so I can spend even less time and willpower managing the email.</p>"}, {"location": "work_interruption_analysis/#calls", "title": "Calls", "text": "<p>We've agreed that the calls are the communication channel used only for critical situations, similar to the physical interruptions, they are synchronous so they're more difficult to manage.</p> <p>As calls are very rare and of high priority, I have my phone configured to ring on incoming calls.</p> <p>Have a work phone independent of your personal</p> <p>Nowadays you can have phone contracts of 0$/month used only to receive calls.</p> <p>Remember to give it to the fewer people as possible.</p>"}, {"location": "work_interruption_analysis/#instant-messages", "title": "Instant messages", "text": "<p>It's the main internal communication channel, so it has a great volume of events with a wide range of priorities. They can be categorized as:</p> <ul> <li>Asking for help through direct messages: We don't have many as we've agreed to     use groups as much as     possible.     So they have high priority and I have the notifications enabled.</li> <li>Social interaction through direct messages: I don't have many as I try to     arrange one on one meetings     instead,     so they have a low priority. As notifications are defined for all direct     messages, I inherit the notifications from the last category.</li> <li>Team group or support rooms: We've defined the interruption role so I check them     whenever an chosen interruption event comes. If I'm assuming the role     I enable the notifications on the channel, if not I'll check them whenever     I check the application.</li> <li>Information rooms: They have no priority and can be checked each 4 hours.</li> </ul> <p>In conclusion, I can check the work chat applications each pomodoro cycle or when I receive a direct notification until the improve the notification management in Linux project is ready.</p>"}, {"location": "work_interruption_analysis/#calendar-events", "title": "Calendar events", "text": "<p>Often with a wide range of priorities.</p> <ul> <li>decide if you have to go</li> <li>Define an agenda with times</li> </ul>"}, {"location": "write_neovim_plugins/", "title": "Write neovim plugins", "text": "<ul> <li>plugin example</li> <li>plugin repo</li> </ul> <p>The plugin repo has some examples in the tests directory</p>"}, {"location": "write_neovim_plugins/#control-an-existing-nvim-instance", "title": "Control an existing nvim instance", "text": "<p>A number of different transports are supported, but the simplest way to get started is with the python REPL. First, start Nvim with a known address (or use the <code>$NVIM_LISTEN_ADDRESS</code> of a running instance):</p> <pre><code>$ NVIM_LISTEN_ADDRESS=/tmp/nvim nvim\n</code></pre> <p>In another terminal, connect a python REPL to Nvim (note that the API is similar to the one exposed by the python-vim bridge:</p> <pre><code>&gt;&gt;&gt; from neovim import attach\n# Create a python API session attached to unix domain socket created above:\n&gt;&gt;&gt; nvim = attach('socket', path='/tmp/nvim')\n# Now do some work.\n&gt;&gt;&gt; buffer = nvim.current.buffer # Get the current buffer\n&gt;&gt;&gt; buffer[0] = 'replace first line'\n&gt;&gt;&gt; buffer[:] = ['replace whole buffer']\n&gt;&gt;&gt; nvim.command('vsplit')\n&gt;&gt;&gt; nvim.windows[1].width = 10\n&gt;&gt;&gt; nvim.vars['global_var'] = [1, 2, 3]\n&gt;&gt;&gt; nvim.eval('g:global_var')\n[1, 2, 3]\n</code></pre>"}, {"location": "write_neovim_plugins/#load-buffer", "title": "Load buffer", "text": "<pre><code>buffer = nvim.current.buffer # Get the current buffer\nbuffer[0] = 'replace first line'\nbuffer[:] = ['replace whole buffer']\n</code></pre>"}, {"location": "write_neovim_plugins/#get-cursor-position", "title": "Get cursor position", "text": "<pre><code>nvim.current.window.cursor\n</code></pre>"}, {"location": "writing_style/", "title": "Writing style", "text": "<p>Writing style is the manner of expressing thought in language characteristic of an individual, period, school, or nation. It's defined by the grammatical choices writers make, the importance of adhering to norms in certain contexts and deviating from them in others, the expression of social identity, and the emotional effects of particular devices on audiences.</p> <p>Beyond the essential elements of spelling, grammar, and punctuation, writing style is the choice of words, sentence structure, and paragraph structure, used to convey the meaning effectively.</p> <p>The point of good writing style is to:</p> <ul> <li>Express the message to the reader simply, clearly, and convincingly.</li> <li>Keep the reader attentive, engaged, and interested.</li> </ul> <p>Not to</p> <ul> <li>Display the writer's personality.</li> <li>Demonstrate the writer's skills, knowledge, or abilities.</li> </ul>"}, {"location": "writing_style/#general-writing-principles", "title": "General writing principles", "text": ""}, {"location": "writing_style/#make-it-pleasant-to-the-reader", "title": "Make it pleasant to the reader", "text": "<p>Writing is a medium of communication, so avoid introducing elements that push away the reader, such as:</p> <ul> <li>Spelling mistakes.</li> <li>Gender favoring, polarizing, race related, religion inconsiderate, or other unequal     phrasing.</li> <li>Ugly environment: Present your texts through a pleasant medium such as     a mkdocs webpage.</li> <li>Write like you talk: Ask yourself, is this the way I'd say this if I were talking to     a friend?. If it isn't, imagine what you would     say, and use that instead.</li> <li>Format errors: If you're writing in markdown, make sure that the result has no display     bugs.</li> <li>Write short articles: Even though I love Gwern     site, I find it daunting most of times.     Instead of a big post, I'd rather use multiple well connected articles.</li> </ul>"}, {"location": "writing_style/#saying-more-with-less", "title": "Saying more with less", "text": "<p>Never use a long word where a short one will do. Replace words like <code>really like</code> with <code>love</code> or other more appropriate words that save space writing and are more meaningful.</p> <p>Don't use filler words like really.</p>"}, {"location": "writing_style/#be-aware-of-pacing", "title": "Be aware of pacing", "text": "<p>Be aware of pacing between words and sentences. The sentences ideally should flow into one another. Breaks in form of commas and full steps are important, as they allow for the reader to take a break and absorb the point that you tried to deliver. Try to use less tan 30 words per sentence.</p> <p>For example, change Due to the fact that to because.</p>"}, {"location": "writing_style/#one-purpose", "title": "One purpose", "text": "<p>A good piece of writing has a single, sharp, overriding purpose. Every part of the writing, even the digressions, should serve that purpose. Put another way, clarity of the general purpose is an absolute requirement in a good piece of writing.</p> <p>This observation matters because it's often tempting to let your purpose expand and become vague. Writing a piece about gardens? Hey, why not include that important related thought you had about rainforests? Now you have a piece that's sort of about gardens and sort of about rainforests, and not really about anything. The reader can no longer bond to it.</p> <p>A complicating factor is that sometimes you need to explore beyond the boundaries of your current purpose. You're writing for purpose A, but your instinct says that you need to explore subject B. Unfortunately, you're not yet sure how subject B fits in. If that's the case then you must take time to explore, and to understand how, if at all, subject B fits in, and whether you need to revise your purpose. This is emotionally difficult. It creates uncertainty, and you may feel as though your work on subject B is wasted effort. These doubts must be resisted.</p>"}, {"location": "writing_style/#avoid-using-cliches", "title": "Avoid using clich\u00e9s", "text": "<p>Clich\u00e9s prevent readers from visualization, making them an obstacle to creating memorable writing.</p>"}, {"location": "writing_style/#citing-the-sources", "title": "Citing the sources", "text": "<ul> <li>If it's a small phrase or a refactor, link the source inside the phrase or at     the header of the section.</li> <li>If it's a big refactor, add it to a references section.</li> <li>If it's a big block without editing use admonition quotes</li> </ul>"}, {"location": "writing_style/#take-all-the-guidelines-as-suggestions", "title": "Take all the guidelines as suggestions", "text": "<p>All the sections above are guidelines, not rules to follow blindly, I try to adhere to them as much as possible, but if I feel it doesn't apply I ignore them.</p>"}, {"location": "writing_style/#unconnected-thoughts", "title": "Unconnected thoughts", "text": "<ul> <li>Replace adjectives with data. Nearly all of -&gt; 84% of.</li> <li>Remove weasel words.</li> <li>Most adverbs are superfluous. When you say \"generally\" or   \"usually\" you're probably undermining your point and the use of \"very\" or   \"extremely\" are hyperbolic and breathless and make it easier to regard what   you're writing as not serious.</li> <li>Examine every word: a surprising number don't serve any purpose.</li> <li>While wrapping your content into a story you may find yourself talking about   your achievements more than giving actionable advice. If that happens, try to   get to the bottom of how you achieved these achievements and break this process   down, then focus on the process more than on your personal achievement.</li> <li>Set up a system that prompts people to review the material.</li> <li>Don't be egocentric, limit the use of <code>I</code>, use the implied subject instead:     It's where I go to -&gt; It's the place to go. I take different actions -&gt; Taking different actions.</li> <li>Don't be possessive, use <code>the</code> instead of <code>my</code>.</li> <li>If you don't know how to express something use services like     deepl.</li> <li>Use synonyms instead of repeating the same word over and over.</li> <li>Think who are you writing to.</li> <li>Use active voice: Active voice ensures that the actors are identified and it   generally leaves less open questions. The exception is if you want to   emphasize the object of the sentence.</li> </ul>"}, {"location": "writing_style/#how-to-end-a-letter", "title": "How to end a letter", "text": "<p>How you end a letter is important. It\u2019s your last chance to leave the reader with positive feelings about you and the letter you have written. To make the matter more difficult, each different closing phrase has subtle connotations attached to them that you need to know to use them well.</p> <p>Most formal letter closing options are reserved, but note that there are degrees of warmth and familiarity among the options. Your relationship with the person to whom you\u2019re writing will shape which closing you choose:</p> <ul> <li>If you don\u2019t know the individual to whom you\u2019re writing, stick with     a professional formal closing.</li> <li>If you\u2019re writing to a colleague, business connection, or someone else you     know well, it\u2019s fine to close your letter less formally.</li> </ul> <p>Above all, your closing should be appropriate. Ideally, your message will resonate instead of your word choice.</p> <p>TL;DR: You can select from:</p> <ul> <li> <p>Simplest, most useful:</p> <ul> <li>Sincerely</li> <li>Regards</li> <li>Yours truly</li> <li>Yours sincerely</li> </ul> </li> <li> <p>Slightly more personal:</p> <ul> <li>Best regards</li> <li>Cordially</li> <li>Yours respectfully</li> </ul> </li> <li> <p>More personal: Only use when appropriate to the letter's content.</p> <ul> <li>Warm regards</li> <li>Best wishes</li> <li>With appreciation</li> </ul> </li> <li> <p>Letter closings to avoid:</p> <ul> <li>Always</li> <li>Cheers</li> <li>Love</li> <li>Take Care</li> <li>XOXO</li> </ul> </li> </ul> <p>The following are letter closings that are appropriate for business and employment related letters.</p> <ul> <li> <p>Sincerely, Regards, Yours truly, and Yours sincerely: These are the simplest     and most useful letter closings to use in a formal business setting. These     are appropriate in almost all instances and are excellent ways to close     a cover letter or an inquiry.</p> </li> <li> <p>Best regards, Cordially, and Yours respectfully: These letter closings fill     the need for something slightly more personal. They are appropriate once you     have some knowledge of the person to whom you are writing. You may have     corresponded via email a few times, had a face-to-face or phone interview,     or met at a networking event.</p> </li> <li> <p>Warm regards, Best wishes, and With appreciation: These letter closings are     also appropriate once you have some knowledge or connection to the person to     whom you are writing. Because they can relate back to the content of the     letter, they can give closure to the point of the letter. Only use these if     they make sense with the content of your letter.</p> </li> </ul>"}, {"location": "writing_style/#letter-closings-to-avoid", "title": "Letter closings to avoid", "text": "<p>There are certain closings that you want to avoid in any business letter. Most of these are simply too informal. Some examples of closings to avoid are listed below:</p> <p>Always, Cheers, Love, Take care, XOXO, Talk soon, See ya, Hugs</p> <p>Some closings (such as \u201cLove\u201d and \u201cXOXO\u201d) imply a level of closeness that is not appropriate for a business letter.</p> <p>Rule of thumb: if you would use the closing in a note to a close friend, it\u2019s probably not suitable for business correspondence.</p>"}, {"location": "writing_style/#punctuating-farewell-phrases", "title": "Punctuating Farewell Phrases", "text": "<p>When writing your sign-off, it's important to remember to use proper capitalization and punctuation. Only the first word should be capitalized (e.g., Yours truly), and the sign-off should be followed by a comma (or an exclamation mark in some informal settings), not a period.</p>"}, {"location": "writing_style/#postscripts", "title": "Postscripts", "text": "<p>A P.S. (or postscript) comes after your sign-off and name. It is meant to include material that is supplementary, subordinated, or not vital to your letter. It is best to avoid postscripts in formal writing, as the information may go unnoticed or ignored; in those cases, try to include all information in the body text of the letter.</p> <p>n casual and personal correspondences, a postscript is generally acceptable. However, try to limit it to include only humorous or unnecessary material.</p>"}, {"location": "writing_style/#letter-closings-in-detail", "title": "Letter closings in detail", "text": ""}, {"location": "writing_style/#sincerely", "title": "Sincerely", "text": "<p>Sincerely (or sincerely yours) is often the go-to sign off for formal letters, and with good reason. This ending restates the sincerity of your letter's intent; it is a safe choice if you are not overly familiar with the letter's recipient, as it's preferable to use a sign-off that is both common and formal in such a situation.</p>"}, {"location": "writing_style/#best", "title": "Best", "text": "<p>Ending your letter with best, all the best, all best, or best wishes indicates that you hope the recipient experiences only good things in the future. Although it is not quite as formal as sincerely, it is still acceptable as a polite, formal/semi-formal letter ending, proper for business contacts as well as friends.</p>"}, {"location": "writing_style/#best-regards", "title": "Best regards", "text": "<p>Quite like the previous sign-off, best regards expresses that you are thinking of the recipient with the best of feelings and intentions. Despite its similarity to best, this sign-off is a little more formal, meant for business letters and unfamiliar contacts. A semi-formal variation is warm regards, and an even more formal variation is simply regards.</p>"}, {"location": "writing_style/#speak-to-you-soon", "title": "Speak to you soon", "text": "<p>Variations to this farewell phrase include see you soon, talk to you later, and looking forward to speaking with you soon. These sign-offs indicate that you are expecting to continue the conversation with your contact. It can be an effective ending to a letter or email when confirming or planning a specific date for a face-to-face meeting.</p> <p>Although these endings can be used in either formal or casual settings, they typically carry a more formal tone. The exception here is talk to you later, which errs on the more casual side.</p>"}, {"location": "writing_style/#thanks", "title": "Thanks", "text": "<p>This is an effective ending to a letter when you are sincerely expressing gratitude. If you are using it as your standard letter ending, however, it can fall flat; the reader will be confused if there is no reason for you to be thanking them. Try to use thanks (or variations such as thanks so much, thank you, or thanks!) and its variations only when you think you haven't expressed your gratitude enough; otherwise, it can come across as excessive.</p> <p>Furthermore, when you're issuing an order, thanks might not be the best sign-off because it can seem presumptuous to offer thanks before the task has even been accepted or begun.</p>"}, {"location": "writing_style/#no-sign-off", "title": "[No sign-off]", "text": "<p>Having no sign-off for your letter is a little unusual, but it is acceptable in some cases. Omitting the sign-off is most appropriately used in cases where you are replying to an email chain. However, in a first email, including neither a sign-off nor your name will make your letter seem to end abruptly. It should be avoided in those situations or when you are not very familiar with the receiver.</p>"}, {"location": "writing_style/#yours-truly", "title": "Yours truly", "text": "<p>This is where the line between formal and informal begins to blur. Yours truly implies the integrity of the message that precedes your name, but it also implies that you are related to the recipient in some way.</p> <p>This ending can be used in various situations, when writing letters to people both familiar and unfamiliar to you; however, yours truly carries a more casual and familiar tone, making it most appropriate for your friends and family. It's best used when you want to emphasize that you mean the contents of your letter.</p>"}, {"location": "writing_style/#take-care", "title": "Take care", "text": "<p>Take care is also a semi-formal way to end your letter. Like the sign-off all the best, this ending wishes that no harm come to the reader; however, like ending your letter with yours truly, the word choice is less formal and implies that the writer is at least somewhat familiar with the reader.</p>"}, {"location": "writing_style/#cheers", "title": "Cheers", "text": "<p>Cheers is a lighthearted ending that expresses your best wishes for the reader. Due to its association with drinking alcohol, it's best to save this sign-off for cases where you are familiar with the reader and when the tone is optimistic and casual. Also note that because cheers is associated with British English, it may seem odd to readers who speak other styles of English and are not very familiar with the term.</p>"}, {"location": "writing_style/#style-issues", "title": "Style issues", "text": ""}, {"location": "writing_style/#avoid-there-is-at-the-start-of-the-sentence", "title": "Avoid there is at the start of the sentence", "text": "<p>Almost never begin a sentence with \u201cIt is...\u201d or \u201cThere is/are...\u201d. These are examples of unnecessary verbiage that shift the focus from the sentence point.</p>"}, {"location": "writing_style/#writing-style-books", "title": "Writing style books", "text": "<p>After you start writing every day professionally, you will see that you will face some hard problems that will haunt you every time you sit down to write.</p> <p>The simplest way to overcome these issues and adopt a philosophy of writing that will make you a more professional, resilient, and wiser writer is to read the books about writing that masters of the craft have published.</p> <p>After reviewing 1, 2 and 3 I've come to the following list of books I'd like to read.</p>"}, {"location": "writing_style/#the-elements-of-style", "title": "The elements of style", "text": "<p>A classic book on grammar, style, and punctuation. If you feel like you need to improve any of those three aspects of your writing, then this book is a great start.</p> <p>With only 85 pages it covers both the grammar basics, rules that affect the style composition, writing toolbox description, and styling recommendations.</p>"}, {"location": "writing_style/#on-writing-well", "title": "On writing well", "text": "<p>They say this book is specially useful to find one's style, develop it, polish it and learn how to write with it.</p> <p>The author doesn't get too philosophical or cutesy in his concepts, neither he gets too technical. In a way, it provides the right balance between The Elements of Style and Bird by Bird. Reading the book feels like you\u2019re being mentored by a wise, highly experienced writer.</p>"}, {"location": "writing_style/#bird-by-bird", "title": "Bird by bird", "text": "<p>Supposedly the most touching, poetic, and psychological book of the collection.</p> <p>The first part of the book lays around the life of Anne Lamott, a relatively popular fiction writer, who happens to have had a quite interesting life. Just like On Writing (the first book mentioned in here), the author manages to share enough of her life to enlighten the story and thesis of the book.</p> <p>The author explains what it takes to be a writer, what it means to be one, and how you can develop a narrative for a fiction book or story.</p> <p>It looks like it's a real pleasure to read it at the same time as it\u2019s still a wonderful experience that will help you understand how you can overcome your own fears, doubts, and pains of writing.</p> <p>Whether you want to write fiction or nonfiction, Bird by Bird provides a beautiful reading experience that will teach you what it takes to be a writer and how to find your demons.</p>"}, {"location": "writing_style/#on-writing", "title": "On writing", "text": "<p>It's a book written by Stephen King that even though I haven't read any of his books I know he is known for being a specialist in capturing the reader. I don't know if it's going to be too much oriented to writing novels, but it looks promising.</p> <p>I'll leave it there for now, but keep on reading on Ivan Kreimer's article for more suggestions.</p>"}, {"location": "writing_style/#references", "title": "References", "text": "<ul> <li>Ivan Kreimer's article</li> </ul>"}, {"location": "yamlfix/", "title": "Yamlfix", "text": "<p>Yamlfix is a simple opinionated yaml formatter that keeps your comments.</p>"}, {"location": "yamlfix/#install", "title": "Install", "text": "<pre><code>pip install yamlfix\n</code></pre>"}, {"location": "yamlfix/#usage", "title": "Usage", "text": "<p>Imagine we've got the following source code:</p> <pre><code>book_library:\n- title: Why we sleep\nauthor: Matthew Walker\n- title: Harry Potter and the Methods of Rationality\nauthor: Eliezer Yudkowsky\n</code></pre> <p>It has the following errors:</p> <ul> <li>There is no <code>---</code> at the top.</li> <li>The indentation is all wrong.</li> </ul> <p>After running <code>yamlfix</code> the resulting source code will be:</p> <pre><code>---\nbook_library:\n- title: Why we sleep\nauthor: Matthew Walker\n- title: Harry Potter and the Methods of Rationality\nauthor: Eliezer Yudkowsky\n</code></pre> <p><code>yamlfix</code> can be used both as command line tool and as a library.</p> <ul> <li> <p>As a command line tool:</p> <pre><code>$: yamlfix file.yaml\n</code></pre> </li> <li> <p>As a library:</p> <pre><code>from yamlfix import fix_files\n\nfix_files(['file.py'])\n</code></pre> </li> </ul>"}, {"location": "yamlfix/#references", "title": "References", "text": "<ul> <li>Git</li> <li>Docs</li> </ul>"}, {"location": "zfs_exporter/", "title": "ZFS Prometheus exporter", "text": "<p>You can use a zfs exporter to create alerts on your ZFS pools, filesystems, snapshots and volumes.</p>"}, {"location": "zfs_exporter/#available-metrics", "title": "Available metrics", "text": "<p>It's not easy to match the exporter metrics with the output of <code>zfs list -o space</code>. Here is a correlation table:</p> <ul> <li>USED: <code>zfs_dataset_used_bytes{type=\"filesystem\"}</code></li> <li>AVAIL: <code>zfs_dataset_available_bytes{type=\"filesystem\"}</code></li> <li>LUSED: <code>zfs_dataset_logical_used_bytes{type=\"filesystem\"}</code></li> <li>USEDDS: <code>zfs_dataset_used_by_dataset_bytes=\"filesystem\"}</code></li> <li>USEDSNAP: Currently there is no published metric to get this data. You can either use <code>zfs_dataset_used_bytes - zfs_dataset_used_by_dataset_bytes</code> which will show wrong data if the dataset has children or try to do <code>sum by (hostname,filesystem) (zfs_dataset_used_bytes{type='snapshot'})</code> which returns smaller sizes than expected.</li> </ul>"}, {"location": "zfs_exporter/#installation", "title": "Installation", "text": ""}, {"location": "zfs_exporter/#install-the-exporter", "title": "Install the exporter", "text": "<p>Download the latest release for your platform, and unpack it somewhere on your filesystem.</p> <p>If you use ansible you can use the next task:</p> <pre><code>---\n- name: Test if zfs_exporter binary exists\nstat:\npath: /usr/local/bin/zfs_exporter\nregister: zfs_exporter_binary\n\n- name: Install the zfs exporter\nblock:\n- name: Download the zfs exporter\ndelegate_to: localhost\nansible.builtin.unarchive:\nsrc: https://github.com/pdf/zfs_exporter/releases/download/v{{ zfs_exporter_version }}/zfs_exporter-{{ zfs_exporter_version }}.linux-amd64.tar.gz\ndest: /tmp/\nremote_src: yes\n\n- name: Upload the zfs exporter to the server\nbecome: true\ncopy:\nsrc: /tmp/zfs_exporter-{{ zfs_exporter_version }}.linux-amd64/zfs_exporter\ndest: /usr/local/bin\nmode: 0755\nwhen: not zfs_exporter_binary.stat.exists\n\n- name: Create the systemd service\nbecome: true\ntemplate:\nsrc: service.j2\ndest: /etc/systemd/system/zfs_exporter.service\nnotify: Restart the service\n</code></pre> <p>With this service template</p> <pre><code>[Unit]\nDescription=zfs_exporter\nAfter=network-online.target\n\n[Service]\nRestart=always\nRestartSec=5\nTimeoutSec=5\nUser=root\nGroup=root\nExecStart=/usr/local/bin/zfs_exporter {{ zfs_exporter_arguments }}\n\n[Install]\nWantedBy=multi-user.target\n</code></pre> <p>This defaults file:</p> <pre><code>---\nzfs_exporter_version: 2.2.8\nzfs_exporter_arguments: --collector.dataset-snapshot\n</code></pre> <p>And this handler:</p> <pre><code>- name: Restart the service\nbecome: true\nsystemd:\nname: zfs_exporter\nenabled: true\ndaemon_reload: true\nstate: restarted\n</code></pre> <p>I know I should publish a role, but I'm lazy right now :P</p>"}, {"location": "zfs_exporter/#configure-the-exporter", "title": "Configure the exporter", "text": "<p>Configure the scraping in your prometheus configuration:</p> <pre><code>  - job_name: zfs_exporter\nmetrics_path: /metrics\nstatic_configs:\n- targets: [192.168.3.236:9134] metric_relabel_configs:\n- source_labels: ['name']\nregex: ^([^@]*).*$\ntarget_label: filesystem\nreplacement: ${1}\n- source_labels: ['name']\nregex: ^.*:.._(.*)$\ntarget_label: snapshot_type\nreplacement: ${1}\n</code></pre> <p>The relabelings are done to be able to extract the <code>filesystem</code> and the backup type of the snapshots'  metrics. This assumes that you are using <code>sanoid</code> to do the backups, which gives metrics such as:</p> <pre><code>zfs_dataset_written_bytes{name=\"main/apps/nginx_public@autosnap_2023-06-03_00:00:18_daily\",pool=\"main\",type=\"snapshot\"} 0\n</code></pre> <p>For that metric you'll get that the <code>filesystem</code> is <code>main/apps/nginx_public</code> and the backup type is <code>daily</code>.</p>"}, {"location": "zfs_exporter/#configure-the-alerts", "title": "Configure the alerts", "text": "<p>The people of Awesome Prometheus Alerts give some good ZFS alerts for this exporter:</p> <pre><code>  - alert: ZfsPoolOutOfSpace\nexpr: zfs_pool_free_bytes * 100 / zfs_pool_size_bytes &lt; 10 and ON (instance, device, mountpoint) zfs_pool_readonly == 0\nfor: 5m\nlabels:\nseverity: warning\nannotations:\nsummary: ZFS pool out of space (instance {{ $labels.instance }})\ndescription: \"Disk is almost full (&lt; 10% left)\\n  VALUE = {{ $value }}\\n  LABELS = {{ $labels }}\"\n\n- alert: ZfsPoolUnhealthy\nexpr: zfs_pool_health &gt; 0\nfor: 5m\nlabels:\nseverity: critical\nannotations:\nsummary: ZFS pool unhealthy (instance {{ $labels.instance }})\ndescription: \"ZFS pool state is {{ $value }}. Where:\\n  - 0: ONLINE\\n  - 1: DEGRADED\\n  - 2: FAULTED\\n  - 3: OFFLINE\\n  - 4: UNAVAIL\\n  - 5: REMOVED\\n  - 6: SUSPENDED\\n  VALUE = {{ $value }}\\n  LABELS = {{ $labels }}\"\n\n- alert: ZfsCollectorFailed\nexpr: zfs_scrape_collector_success != 1\nfor: 5m\nlabels:\nseverity: warning\nannotations:\nsummary: ZFS collector failed (instance {{ $labels.instance }})\ndescription: \"ZFS collector for {{ $labels.instance }} has failed to collect information\\n  VALUE = {{ $value }}\\n  LABELS = {{ $labels }}\"\n</code></pre>"}, {"location": "zfs_exporter/#snapshot-alerts", "title": "Snapshot alerts", "text": "<p>You can also monitor the status of the snapshots.</p> <pre><code>  - alert: ZfsDatasetWithNoSnapshotsError\nexpr: zfs_dataset_used_by_dataset_bytes{type=\"filesystem\"} &gt; 200e3 unless on (hostname,filesystem) count by (hostname, filesystem, job) (zfs_dataset_used_bytes{type=\"snapshot\"}) &gt; 1\nfor: 5m\nlabels:\nseverity: error\nannotations:\nsummary: The dataset {{ $labels.filesystem }} at {{ $labels.hostname }} doesn't have any snapshot.\ndescription: \"There might be an error on the snapshot system\\n  VALUE = {{ $value }}\\n  LABELS = {{ $labels }}\"\n\n- alert: ZfsSnapshotTypeFrequentlySizeError\nexpr: increase(sum by (hostname, filesystem, job) (zfs_dataset_used_bytes{type='snapshot',snapshot_type='frequently'})[60m:15m]) == 0 and count_over_time(zfs_dataset_used_bytes{type=\"filesystem\"}[60m:15m]) == 4\nfor: 5m\nlabels:\nseverity: error\nannotations:\nsummary: The size of the frequently snapshots has not changed for the dataset {{ $labels.filesystem }} at {{ $labels.hostname }}.\ndescription: \"There might be an error on the snapshot system or the data has not changed in the last hour\\n  VALUE = {{ $value }}\\n  LABELS = {{ $labels }}\"\n\n- alert: ZfsSnapshotTypeHourlySizeError\nexpr: increase(sum by (hostname, filesystem, job) (zfs_dataset_used_bytes{type='snapshot',snapshot_type='hourly'})[2h:30m]) == 0 and count_over_time(zfs_dataset_used_bytes{type=\"filesystem\"}[2h:30m]) == 4\nfor: 5m\nlabels:\nseverity: error\nannotations:\nsummary: The size of the hourly snapshots has not changed for the dataset {{ $labels.filesystem }} at {{ $labels.hostname }}.\ndescription: \"There might be an error on the snapshot system or the data has not changed in the last hour\\n  VALUE = {{ $value }}\\n  LABELS = {{ $labels }}\"\n\n- alert: ZfsSnapshotTypeDailySizeError\nexpr: increase(sum by (hostname, filesystem, job) (zfs_dataset_used_bytes{type='snapshot',snapshot_type='daily'})[2d:12h]) == 0 and count_over_time(zfs_dataset_used_bytes{type=\"filesystem\"}[2d:12h]) == 4\nfor: 5m\nlabels:\nseverity: error\nannotations:\nsummary: The size of the daily snapshots has not changed for the dataset {{ $labels.filesystem }} at {{ $labels.hostname }}.\ndescription: \"There might be an error on the snapshot system or the data has not changed in the last hour\\n  VALUE = {{ $value }}\\n  LABELS = {{ $labels }}\"\n\n- alert: ZfsSnapshotTypeMonthlySizeError\nexpr: increase(sum by (hostname, filesystem, job) (zfs_dataset_used_bytes{type='snapshot',snapshot_type='monthly'})[60d:15d]) == 0 and count_over_time(zfs_dataset_used_bytes{type=\"filesystem\"}[60d:15d]) == 4\nfor: 5m\nlabels:\nseverity: error\nannotations:\nsummary: The size of the monthly snapshots has not changed for the dataset {{ $labels.filesystem }} at {{ $labels.hostname }}.\ndescription: \"There might be an error on the snapshot system or the data has not changed in the last hour\\n  VALUE = {{ $value }}\\n  LABELS = {{ $labels }}\"\n\n- alert: ZfsSnapshotTypeFrequentlyUnexpectedNumberError\nexpr: increase((count by (hostname, filesystem, job) (zfs_dataset_used_bytes{snapshot_type=\"frequently\",type=\"snapshot\"}) &lt; 4)[16m:8m]) &lt; 1 and count_over_time(zfs_dataset_used_bytes{type=\"filesystem\"}[16m:8m]) == 2\nfor: 5m\nlabels:\nseverity: error\nannotations:\nsummary: The number of the frequent snapshots has not changed for the dataset {{ $labels.filesystem }} at {{ $labels.hostname }}.\ndescription: \"There might be an error on the snapshot system\\n  VALUE = {{ $value }}\\n  LABELS = {{ $labels }}\"\n\n- alert: ZfsSnapshotTypeHourlyUnexpectedNumberError\nexpr: increase((count by (hostname, filesystem, job) (zfs_dataset_used_bytes{snapshot_type=\"hourly\",type=\"snapshot\"}) &lt; 24)[1h10m:10m]) &lt; 1 and count_over_time(zfs_dataset_used_bytes{type=\"filesystem\"}[1h10m:10m]) == 7\nfor: 5m\nlabels:\nseverity: error\nannotations:\nsummary: The number of the hourly snapshots has not changed for the dataset {{ $labels.filesystem }} at {{ $labels.hostname }}.\ndescription: \"There might be an error on the snapshot system\\n  VALUE = {{ $value }}\\n  LABELS = {{ $labels }}\"\n\n- alert: ZfsSnapshotTypeDailyUnexpectedNumberError\nexpr: increase((count by (hostname, filesystem, job) (zfs_dataset_used_bytes{type='snapshot',snapshot_type='daily'}) &lt; 30)[1d2h:2h]) &lt; 1 and count_over_time(zfs_dataset_used_bytes{type=\"filesystem\"}[1d2h:2h]) == 13\nfor: 5m\nlabels:\nseverity: error\nannotations:\nsummary: The number of the hourly snapshots has not changed for the dataset {{ $labels.filesystem }} at {{ $labels.hostname }}.\ndescription: \"There might be an error on the snapshot system\\n  VALUE = {{ $value }}\\n  LABELS = {{ $labels }}\"\n\n- alert: ZfsSnapshotTypeMonthlyUnexpectedNumberError\nexpr: increase((count by (hostname, filesystem, job) (zfs_dataset_used_bytes{type='snapshot',snapshot_type='monthly'}) &lt; 6)[31d:1d]) &lt; 1 and count_over_time(zfs_dataset_used_bytes{type=\"filesystem\"}[31d:1d]) == 31\nfor: 5m\nlabels:\nseverity: error\nannotations:\nsummary: The number of the monthly snapshots has not changed for the dataset {{ $labels.filesystem }} at {{ $labels.hostname }}.\ndescription: \"There might be an error on the snapshot system\\n  VALUE = {{ $value }}\\n  LABELS = {{ $labels }}\"\n\n- record: zfs_dataset_snapshot_bytes\n# This expression is not real for datasets that have children, so we're going to create this metric only for those datasets that don't have children\n# I'm also going to assume that the datasets that have children don't hold data\nexpr: zfs_dataset_used_bytes - zfs_dataset_used_by_dataset_bytes and zfs_dataset_used_by_dataset_bytes &gt; 200e3\n- alert: ZfsSnapshotTooMuchSize\nexpr: zfs_dataset_snapshot_bytes / zfs_dataset_used_by_dataset_bytes &gt; 2 and zfs_dataset_snapshot_bytes &gt; 10e9\nfor: 5m\nlabels:\nseverity: warning\nannotations:\nsummary: The snapshots of the dataset {{ $labels.filesystem }} at {{ $labels.hostname }} use more than two times the data space\ndescription: \"The snapshots of the dataset {{ $labels.filesystem }} at {{ $labels.hostname }} use more than two times the data space\\n  VALUE = {{ $value }}\\n  LABELS = {{ $labels }}\"\n</code></pre>"}, {"location": "zfs_exporter/#useful-inhibits", "title": "Useful inhibits", "text": "<p>Some you may want to inhibit some of these rules for some of your datasets. These subsections should be added to the <code>alertmanager.yml</code> file under the <code>inhibit_rules</code> field.</p>"}, {"location": "zfs_exporter/#ignore-snapshots-on-some-datasets", "title": "Ignore snapshots on some datasets", "text": "<p>Sometimes you don't want to do snapshots on a dataset</p> <pre><code>- target_matchers:\n- alertname = ZfsDatasetWithNoSnapshotsError\n- hostname = my_server_1\n- filesystem = tmp\n</code></pre>"}, {"location": "zfs_exporter/#ignore-snapshots-growth", "title": "Ignore snapshots growth", "text": "<p>Sometimes you don't mind if the size of the data saved in the filesystems doesn't change too much between snapshots doesn't change much specially in the most frequent backups because you prefer to keep the backup cadence. It's interesting to have the alert though so that you can get notified of the datasets that don't change that much so you can tweak your backup policy (even if zfs snapshots are almost free).</p> <pre><code>  - target_matchers:\n- alertname =~ \"ZfsSnapshotType(Frequently|Hourly)SizeError\"\n- filesystem =~ \"(media/(docs|music))\"\n</code></pre>"}, {"location": "zfs_exporter/#references", "title": "References", "text": "<ul> <li>Source</li> </ul>"}, {"location": "zfs_storage_planning/", "title": "OpenZFS storage planning", "text": "<p>When you build a ZFS storage system you need to invest some time doing the storage planning.</p> <p>There are many variables that affect the number and type of disks, you first need to have an idea of what kind of data you want to store and what use are you going to give to that data.</p>"}, {"location": "zfs_storage_planning/#robustness", "title": "Robustness", "text": "<p>ZFS is designed to survive disk failures, so it stores each block of data redundantly. This feature complicates capacity planning because your total usable storage is not just the sum of each disk\u2019s capacity.</p> <p>ZFS creates filesystems out of \u201cpools\u201d of disks. The more disks in the pool, the more efficiently ZFS can use their storage capacity. For example, if you give ZFS two 10 TB drives, you can only use half of your total disk capacity. If you instead use five 4 TB drives, ZFS gives you 14 TB of usable storage. Even though your total disk space is the same in either scenario, the five smaller drives give you 40% more usable space.</p> <p>When you\u2019re building a NAS server, you need to decide whether to use a smaller quantity of large disks or a larger quantity of small disks. Smaller drives are usually cheaper in terms of $/TB, but they\u2019re more expensive to operate. Two 4 TB drives require twice the electricity of a single 8 TB drive.</p> <p>Also keep in mind that so far ZFS doesn't let you add a new drive to an existing vdev, but that feature is under active development. If you want to be safe, plan your vdev definition so that they don't need to change the disk numbers.</p>"}, {"location": "zfs_storage_planning/#preventing-concurrent-disk-failures", "title": "Preventing concurrent disk failures", "text": "<p>Naively, the probability of two disks failing at once seems vanishingly small. Based on Backblaze\u2019s stats, high-quality disk drives fail at 0.5-4% per year. A 4% risk per year is a 0.08% chance in any given week. Two simultaneous failures would happen once every 30,000 years, so you should be fine, right?</p> <p>The problem is that disks aren\u2019t statistically independent. If one disk fails, its neighbor has a substantially higher risk of dying. This is especially true if the disks are the same model, from the same manufacturing batch, and processed the same workloads.</p> <p>Further, rebuilding a ZFS pool puts an unusual amount of strain on all of the surviving disks. A disk that would have lasted a few more months under normal usage might die under the additional load of a pool rebuild.</p> <p>Given these risks, you can reduce the risk of concurrent disk failures by choosing two different models of disk from two different manufacturers. To reduce the chances of getting disks from the same manufacturing batch, you can buy them from different vendors.</p>"}, {"location": "zfs_storage_planning/#choosing-the-disks", "title": "Choosing the disks", "text": "<p>There are many things to take into account when choosing the different disks for your pool.</p>"}, {"location": "zfs_storage_planning/#choosing-the-disks-to-hold-data", "title": "Choosing the disks to hold data", "text": "<p>Check diskprices.com to get an idea of the cost of disks in the market. If you can, try to avoid buying to Amazon as it's the devil. Try to buy them from a local store instead, that way you interact with a human and promote a local commerce.</p> <p>Note: If you want a TL;DR you can jump to the conclusion.</p> <p>To choose your disks take into account:</p> <ul> <li>Disk speed</li> <li>Disk load</li> <li>Disk type</li> <li>Disk homogeneity</li> <li>Disk Warranty</li> <li>Disk Brands</li> </ul>"}, {"location": "zfs_storage_planning/#data-disk-speed", "title": "Data disk speed", "text": "<p>When comes to disk speed there are three kinds, the slow (5400 RPM), normal (7200 RPM) and fast (10k RPM).</p> <p>The higher the RPM, the louder the disk is, the more heat it creates and the more power it will consume. In exchange they will have higher writing and reading speeds. Slower disks expand the lifecycle of the device, but in the case of a failed disk in a RAID scenario, the rebuild time will be higher than on faster ones therefore increasing the risk on concurrent failing disks.</p> <p>Before choosing a high number of RPM make sure that it's your bottleneck, which usually is the network if you're using a 1Gbps network. In this case a 10k RPM disk won't offer better performance than a 7200 RPM, even a 7200 one won't be better than a 5400.</p> <p>The need of higher speeds can be fixed by using an SSD as a cache for reading and writing.</p>"}, {"location": "zfs_storage_planning/#data-disk-load", "title": "Data disk load", "text": "<p>Disk specifications tell you the amount of TB/year they support, it gives you an idea of the fault tolerance. Some examples</p> Disk Fault tolerance (TB/year) WD RED 8TB 180"}, {"location": "zfs_storage_planning/#data-disk-type", "title": "Data disk type", "text": "<p>It\u2019s easy to think that all hard drives are equal, save for the form factor and connection type. However, there\u2019s a difference between the work your hard drive does in your computer versus the workload of a NAS hard drive. A drive in your computer may only read and write data for a couple hours at a time, while a NAS drive may read and write data for weeks on end, or even longer.</p> <p>The environment inside of a NAS box is much different than a typical desktop or laptop computer. When you pack in a handful of hard drives close together, several things happen: there\u2019s more vibration, more heat, and a lot more action going on in general.</p> <p>To cope with this, NAS hard drives usually have better vibration tolerance and produce less heat than regular hard drives, thanks to slightly-slower spindle speeds and reduced seek noise.</p> <p>Most popular brands are Western Digital Red and Seagate IronWolf which use 5400 RPM, if you want to go on the 7200 RPM speeds you can buy the Pro version of each. I initially tried checking Backblaze\u2019s hard drive stats to avoid failure-prone disks, but they use drives on the pricier side.</p> <p>The last pitfall to avoid is shingled magnetic recording (SMR) technology. ZFS performs poorly on SMR drives, so if you\u2019re building a NAS, avoid known SMR drives. If the drive is labeled as CMR, that\u2019s conventional magnetic recording, which is fine for ZFS.</p> <p>SMR is well suited for high-capacity, low-cost use where writes are few and reads are many. It has worse sustained write performance than CMR, which can cause severe issues during resilvering or other write-intensive operations.</p> <p>There are three types of SMR:</p> <ul> <li>Drive Managed, DM-SMR: It's opaque to the OS. This means ZFS cannot \"target\"   writes, and is the worst type for ZFS use. As a rule of thumb, avoid DM-SMR   drives, unless you have a specific use case where the increased resilver time   (a week or longer) is acceptable, and you know the drive will function for ZFS   during resilver.</li> <li>Host Aware, HA-SMR: It's designed to give ZFS insight into the SMR process.   Note that ZFS code to use HA-SMR does not appear to exist. Without that code,   a HA-SMR drive behaves like a DM-SMR drive where ZFS is concerned.</li> <li>Host Managed, HM-SMR: It's not backwards compatible and requires ZFS to manage   the SMR process.</li> </ul>"}, {"location": "zfs_storage_planning/#data-disk-homogeneity", "title": "Data disk homogeneity", "text": "<p>It's recommended that all the disks in your pool (or is it by vdev?) have the same RPM and size.</p>"}, {"location": "zfs_storage_planning/#data-disk-warranty", "title": "Data disk warranty", "text": "<p>Disks are going to fail, so it's good to have a good warranty to return them.</p>"}, {"location": "zfs_storage_planning/#data-disk-brands", "title": "Data disk brands", "text": ""}, {"location": "zfs_storage_planning/#western-digital", "title": "Western Digital", "text": "<p>The Western Digital Red series of NAS drives are very similar to Seagate\u2019s offering and you should consider these if you can find them at more affordable prices. WD splits its NAS drives into three sub-categories, normal, Plus, and Pro.</p> Specs WD Red WD Red Plus WD Red Pro Technology SMR CMR CMR Bays 1-8 1-8 1-24 Capacity 2-6TB 1-14TB 2-18TB Speed 5,400 RPM 5,400 RPM (1-4TB) 7200 RPM Speed 5,400 RPM 5,640 RPM (6-8TB) 7200 RPM Speed 5,400 RPM 7,200 RPM (8-14TB) 7200 RPM Speed ? 210MB/s 235MB/s Cache 256MB 16MB (1TB) Cache 256MB 64MB (1TB) 64MB (2TB) Cache 256MB 128MB (2-8TB) 256MB (4-12TB) Cache 256MB 256MB (8-12TB) 512MB (14-18TB) Cache 256MB 512MB (14TB) Workload 180TB/yr 180TB/yr 300TB/yr MTBF 1 million 1 million 1 million Warranty 3 years 3 years 5 years Power Consumption ? ? 8.8 W Power Consumption Rest ? ? 4.6 W Price From $50 From $45 From $78"}, {"location": "zfs_storage_planning/#seagate", "title": "Seagate", "text": "<p>Seagate's \"cheap\" NAS disks are the IronWolf gama, there are two variations IronWolf and IronWolf Pro. Seagate Exos is a premium series of drives from the company. They\u2019re even more advanced than IronWolf Pro and are best suited for server environments. They sport incredible levels of performance and reliability, including a workload rate of 550TB per year.</p> Specs IronWolf IronWolf Pro Exos 7E8 8TB Exos 7E10 8TB Technology CMR CMR CMR SMR Bays 1-8 1-24 ? ? Capacity 1-12TB 2-20TB 8TB 8TB RPM 5,400 RPM (3-6TB) 7200 RPM 7200 RPM 7200 RPM RPM 5,900 RPM (1-3TB) 7200 RPM 7200 RPM 7200 RPM RPM 7,200 RPM (8-12TB) 7200 RPM 7200 RPM 7200 RPM Speed 180MB/s (1-12TB) 214-260MB/s (4-18TB) 249 MB/s 255 MB/s Cache 64MB (1-4TB) 256 MB 256 MB 256 MB Cache 256MB (3-12TB) 256 MB 256 MB 256 MB Power Consumption (8TB) 10.1 W 10.1 W 12.81 W 11.03 W Power Consumption Rest (8TB) 7.8 W 7.8 W 7.64 W 7.06 W Workload 180TB/yr 300TB/yr 550TB/yr 550TB/yr MTBF 1 million 1 million 2 millions 2 millions Warranty 3 years 5 years 5 years 5 years Price From $60 From $83 249$ 249$ <p>Exos 7E10 is SMR so it's ruled out.</p> <p>Where MTBF stands for Medium Time Between Failures in hours</p>"}, {"location": "zfs_storage_planning/#data-disk-conclusion", "title": "Data disk conclusion", "text": "<p>I'm more interested on the 5400 RPM drives, but of all the NAS disks available to purchase only the WD RED of 8TB use it, and they use the SMR technology, so they aren't a choice.</p> <p>The disk prices offered by my cheapest provider are:</p> Disk Size Price Seagate IronWolf 8TB 225$ Seagate IronWolf Pro 8TB 254$ WD Red Plus 8TB 265$ Seagate Exos 7E8 8TB 277$ WD Red Pro 8TB 278$ <p>WD Red Plus has 5,640 RPM which is different than the rest, so it's ruled out. Between the IronWolf and IronWolf Pro, they offer 180MB/s and 214MB/s respectively. The Seagate Exos 7E8 provides much better performance than the WD Red Pro so I'm afraid that WD is out of the question.</p> <p>There are three possibilities in order to have two different brands. Imagining we want 4 disks:</p> Combination Total Price IronWolf + IronWolf Pro 958$ IronWolf + Exos 7E8 1004$ (+46$ +4.5%) IronWolf Pro + Exos 7E8 1062$ (+54$ +5.4%) <p>In terms of:</p> <ul> <li>Consumption: both IronWolfs are equal, the Exos uses 2.7W more on normal use   and uses 0.2W less on rest.</li> <li>Warranty: IronWolf has only 3 years, the others 5.</li> <li>Speed: Ironwolf has 210MB/s, much less than the Pro (255MB/s) and Exos   (249MB/s), which are more similar.</li> <li>Sostenibility: The Exos disks are much more robust (more workload, MTBF and   Warranty).</li> </ul> <p>I'd say that for 104$ it makes sense to go with the IronWolf Pro + Exos 7E8 combination.</p>"}, {"location": "zfs_storage_planning/#choosing-the-disks-for-the-cache", "title": "Choosing the disks for the cache", "text": "<p>Using a ZLOG greatly improves the writing speed, equally using an SSD disk for the L2ARC cache improves the read speeds and improves the health of the rotational disks.</p> <p>The best M.2 NVMe SSD for NAS caching are the ones that have enough capacity to actually make a difference to overall system performance. It also requires a good endurance rating for better reliability and longer lifespan, and you should look for a drive with a specific NAND technology if possible.</p> <p>Note: If you want a TL;DR you can jump to the conclusion.</p> <p>To choose your disks take into account:</p> <ul> <li>Cache disk NAND technology</li> <li>DWPD</li> </ul>"}, {"location": "zfs_storage_planning/#cache-disk-nand-technology", "title": "Cache disk NAND technology", "text": "<p>Not all flash-based storage drives are the same. NAND flash cells are usually categorised based on the number of bits that can be stored per cell. Watch out for the following terms when shopping around for an SSD:</p> <ul> <li>Single-Level Cell (SLC): one bit per cell.</li> <li>Multi-Level Cell (MLC): two bits per cell.</li> <li>Triple-Level Cell (TLC): three bits per cell.</li> <li>Quad-Level Cell (QLC): four bits per cell.</li> </ul> <p>When looking for the best M.2 NVMe SSD for NAS data caching, it\u2019s important to bear the NAND technology in mind.</p> <p>SLC is the best technology for SSDs that will be used for NAS caching. This does mean you\u2019re paying out more per GB and won\u2019t be able to select high-capacity drives, but reliability and the protection of stored data is the most important factor here.</p> <p>Another benefit of SLC is the lower impact of write amplification, which can quickly creep up and chomp through a drive\u2019s DWPD endurance rating. It\u2019s important to configure an SSD for caching correctly too regardless of which technology you pick.</p> <p>Doing so will lessen the likelihood of losing data through a drive hanging and causing the system to crash. Anything stored on the cache drive that has yet to be written to the main drive array would be lost. This is mostly a reported issue for NVMe drives, as opposed to SATA.</p>"}, {"location": "zfs_storage_planning/#dwpd", "title": "DWPD", "text": "<p>DWPD stands for drive writes per day. This is often used as a measurement of a drive\u2019s endurance. The higher this number, the more writes the drive can perform on a daily basis, as is rated by the manufacturer. For caching, especially which involves writing data, you\u2019ll want to aim for as high a DWPD rating as possible.</p>"}, {"location": "zfs_storage_planning/#cache-disk-conclusion", "title": "Cache disk conclusion", "text": "<p>Overall, I\u2019d recommend the Western Digital Red SN700, which has a good 1 DWPD endurance rating, is available in sizes up to 4TB, and is using SLC NAND technology, which is great for enhancing reliability through heavy caching workloads. A close second place goes to the Seagate IronWolf 525, which has similar specifications to the SN700 but utilizes TLC.</p> Disk Size Speed Endurance Warranty Tech Price WD Red SN700 500 GB 3430MB/s 1 DWPD 5 years SLC 73$ SG IronWolf 525 500 GB 5000MB/s 0.8 DWPD 5 years TLC ? WD Red SN700 1 TB 3430MB/s 1 DWPD 5 years SLC 127$ SG IronWolf 525 1 TB 5000MB/s 0.8 DWPD 5 years TLC ?"}, {"location": "zfs_storage_planning/#choosing-the-cold-spare-disks", "title": "Choosing the cold spare disks", "text": "<p>It's good to think how much time you want to have your raids to be inconsistent once a drive has failed.</p> <p>In my case, for the data I want to restore the raid as soon as I can, therefore I'll buy another rotational disk. For the SSDs I have more confidence that they won't break so I don't feel like having a spare one.</p>"}, {"location": "zfs_storage_planning/#design-your-pools", "title": "Design your pools", "text": ""}, {"location": "zfs_storage_planning/#pool-configuration", "title": "Pool configuration", "text": "<ul> <li>Use <code>ashift=12</code> or <code>ashift=13</code> when creating the pool if applicable (though ZFS can detect correctly for most cases). Value of <code>ashift</code> is exponent of 2, which should be aligned to the physical sector size of disks, for example <code>2^9=512</code>, <code>2^12=4096</code>, <code>2^13=8192</code>. Some disks are reporting a logical sector size of 512 bytes while having 4KiB physical sector size , and some SSDs have 8KiB physical sector size. </li> </ul> <p>Consider using <code>ashift=12</code> or <code>ashift=13</code> even if currently using only disks with 512 bytes sectors. Adding devices with bigger sectors to the same VDEV can severely impact performance due to wrong alignment, while a device with 512 sectors will work also with a higher <code>ashift</code>. </p> <ul> <li>Set \"autoexpand\" to on, so you can expand the storage pool automatically after all disks in the pool have been replaced with larger ones. Default is off.</li> </ul>"}, {"location": "zfs_storage_planning/#zil-or-slog", "title": "ZIL or SLOG", "text": "<p>Before we can begin, we need to get a few terms out of the way that seem to be confusing:</p> <ul> <li>ZFS Intent Log, or ZIL is a logging mechanism where all of the data to be the written is stored, then later flushed as a transactional write. Similar in function to a journal for journaled filesystems, like <code>ext3</code> or <code>ext4</code>. Typically stored on platter disk. Consists of a ZIL header, which points to a list of records, ZIL blocks and a ZIL trailer. The ZIL behaves differently for different writes. For writes smaller than 64KB (by default), the ZIL stores the write data. For writes larger, the write is not stored in the ZIL, and the ZIL maintains pointers to the synched data that is stored in the log record.</li> <li>Separate Intent Log, or SLOG, is a separate logging device that caches the synchronous parts of the ZIL before flushing them to slower disk. This would either be a battery-backed DRAM drive or a fast SSD. The SLOG only caches synchronous data, and does not cache asynchronous data. Asynchronous data will flush directly to spinning disk. Further, blocks are written a block-at-a-time, rather than as simultaneous transactions to the SLOG. If the SLOG exists, the ZIL will be moved to it rather than residing on platter disk. Everything in the SLOG will always be in system memory.</li> </ul> <p>When you read online about people referring to \"adding an SSD ZIL to the pool\", they are meaning adding an SSD SLOG, of where the ZIL will reside. The ZIL is a subset of the SLOG in this case. The SLOG is the device, the ZIL is data on the device. Further, not all applications take advantage of the ZIL. Applications such as databases (MySQL, PostgreSQL, Oracle), NFS and iSCSI targets do use the ZIL. Typical copying of data around the filesystem will not use it. Lastly, the ZIL is generally never read, except at boot to see if there is a missing transaction. The ZIL is basically \"write-only\", and is very write-intensive.</p> <p>It's important to use devices that can maintain data persistence during a power outage. The SLOG and the ZIL are critical in getting your data to spinning platter. If a power outage occurs, and you have a volatile SLOG, the worst thing that will happen is the new data is not flushed, and you are left with old data. However, it's important to note, that in the case of a power outage, you won't have corrupted data, just lost data. Your data will still be consistent on disk.</p> <p>If you use a SLOG you will see improved disk latencies, disk utilization and system load. What you won't see is improved throughput. Remember that the SLOG device is still flushing data to platter every 5 seconds. As a result, benchmarking disk after adding a SLOG device doesn't make much sense, unless the goal of the benchmark is to test synchronous disk write latencies. Check this article if you want to know more.</p>"}, {"location": "zfs_storage_planning/#adding-a-slog", "title": "Adding a SLOG", "text": "<p>WARNING: Some motherboards will not present disks in a consistent manner to the Linux kernel across reboots. As such, a disk identified as <code>/dev/sda</code> on one boot might be <code>/dev/sdb</code> on the next. For the main pool where your data is stored, this is not a problem as ZFS can reconstruct the VDEVs based on the metadata geometry. For your L2ARC and SLOG devices, however, no such metadata exists. So, rather than adding them to the pool by their <code>/dev/sd?</code> names, you should use the <code>/dev/disk/by-id/*</code> names, as these are symbolic pointers to the ever-changing <code>/dev/sd?</code> files. If you don't heed this warning, your SLOG device may not be added to your hybrid pool at all, and you will need to re-add it later. This could drastically affect the performance of the applications depending on the existence of a fast SLOG.</p> <p>Adding a SLOG to your existing zpool is not difficult. However, it is considered best practice to mirror the SLOG. Suppose that there are 4 platter disks in the pool, and two NVME. </p> <p>First you need to create a partition of 5 GB on each the nvme drive:</p> <pre><code>fdisk -l\nfdisk /dev/nvme0n1\nfdisk /dev/nvme1n1\n</code></pre> <p>Then mirror the partitions as SLOG</p> <pre><code>zpool add tank log mirror \\\n/dev/disk/by-id/nvme0n1-part1 \\\n/dev/disk/by-id/nvme1n1-part1 </code></pre> <p>Check that it worked with <pre><code># zpool status\npool: tank\n state: ONLINE\n scan: scrub repaired 0 in 1h8m with 0 errors on Sun Dec  2 01:08:26 2012\nconfig:\n\n        NAME               STATE     READ WRITE CKSUM\n        pool               ONLINE       0     0     0\nraidz1-0         ONLINE       0     0     0\nsdd            ONLINE       0     0     0\nsde            ONLINE       0     0     0\nsdf            ONLINE       0     0     0\nsdg            ONLINE       0     0     0\nlogs\n          mirror-1         ONLINE       0     0     0\nnvme0n1-part1  ONLINE       0     0     0\nnvme0n1-part2  ONLINE       0     0     0\n</code></pre></p> <p>You will likely not need a large ZIL, take into account that zfs dumps it's contents quite often.</p>"}, {"location": "zfs_storage_planning/#adjustable-replacement-cache", "title": "Adjustable Replacement Cache", "text": "<p>The ZFS adjustable replacement cache (ARC) is one such caching mechanism that caches both recent block requests as well as frequent block requests.  It will occupy \u00bd of available RAM. However, this isn't static. If you have 32 GB of RAM in your server, this doesn't mean the cache will always be 16 GB. Rather, the total cache will adjust its size based on kernel decisions. If the kernel needs more RAM for a scheduled process, the ZFS ARC will be adjusted to make room for whatever the kernel needs. However, if there is space that the ZFS ARC can occupy, it will take it up.</p> <p>The ARC can be extended using the level 2 ARC or L2ARC. This means that as the MRU (the most recently requested blocks from the filesystem) or MFU (the most frequently requested blocks from the filesystem) grow, they don't both simultaneously share the ARC in RAM and the L2ARC on your SSD. Instead, when a page is about to be evicted, a walking algorithm will evict the MRU and MFU pages into an 8 MB buffer, which is later set as an atomic write transaction to the L2ARC. The advantage is that the latency of evicting pages from the cache is not impacted. Further, if a large read of data blocks is sent to the cache, the blocks are evicted before the L2ARC walk, rather than sent to the L2ARC. This minimizes polluting the L2ARC with massive sequential reads. Filling the L2ARC can also be very slow, or very fast, depending on the access to your data.</p> <p>Persistence in the L2ARC is not needed, as the cache will be wiped on boot. To learn more about the ZFS ARC read 1.</p>"}, {"location": "zfs_storage_planning/#adding-an-l2arc", "title": "Adding an L2ARC", "text": "<p>First you need to create the partitions on each the nvme drive:</p> <pre><code>fdisk -l\nfdisk /dev/nvme0n1\nfdisk /dev/nvme1n1\n</code></pre> <p>It is recommended that you stripe the L2ARC to maximize both size and speed.</p> <pre><code>zpool add tank cache \\\n/dev/disk/by-id/nvme0n1-part2 \\\n/dev/disk/by-id/nvme1n1-part2 </code></pre> <p>Check that it worked with <pre><code># zpool status\npool: tank\n state: ONLINE\n scan: scrub repaired 0 in 1h8m with 0 errors on Sun Dec  2 01:08:26 2012\nconfig:\n\n        NAME               STATE     READ WRITE CKSUM\n        pool               ONLINE       0     0     0\nraidz1-0         ONLINE       0     0     0\nsdd            ONLINE       0     0     0\nsde            ONLINE       0     0     0\nsdf            ONLINE       0     0     0\nsdg            ONLINE       0     0     0\ncache\n          nvme0n1-part1    ONLINE       0     0     0\nnvme0n1-part2    ONLINE       0     0     0\n</code></pre></p> <p>To check hte size of the L2ARC use <code>zpool iostat -v</code>.</p>"}, {"location": "zfs_storage_planning/#follow-the-best-practices", "title": "Follow the best practices", "text": "<p>As with all recommendations, some of these guidelines carry a great amount of weight, while others might not. You may not even be able to follow them as rigidly as you would like. Regardless, you should be aware of them. The idea of \"best practices\" is to optimize space efficiency, performance and ensure maximum data integrity.</p> <ul> <li>Keep pool capacity under 80% for best performance. Due to the copy-on-write nature of ZFS, the filesystem gets heavily fragmented.</li> <li>Only run ZFS on 64-bit kernels. It has 64-bit specific code that 32-bit kernels cannot do anything with.</li> <li>Install ZFS only on a system with lots of RAM. 1 GB is a bare minimum, 2 GB is better, 4 GB would be preferred to start. Remember, ZFS will use \u00bd of the available RAM for the ARC.</li> <li>Use ECC RAM when possible for scrubbing data in registers and maintaining data consistency. The ARC is an actual read-only data cache of valuable data in RAM.</li> <li>Use whole disks rather than partitions. ZFS can make better use of the on-disk cache as a result. If you must use partitions, backup the partition table, and take care when reinstalling data into the other partitions, so you don't corrupt the data in your pool.</li> <li>Keep each VDEV in a storage pool the same size. If VDEVs vary in size, ZFS will favor the larger VDEV, which could lead to performance bottlenecks.</li> <li>Use redundancy when possible, as ZFS can and will want to correct data errors that exist in the pool. You cannot fix these errors if you do not have a redundant good copy elsewhere in the pool. Mirrors and RAID-Z levels accomplish this.</li> <li>Do not use raidz1 for disks 1TB or greater in size.</li> <li>For raidz1, do not use less than 3 disks, nor more than 7 disks in each vdev </li> <li>For raidz2, do not use less than 6 disks, nor more than 10 disks in each vdev (8 is a typical average).</li> <li>For raidz3, do not use less than 7 disks, nor more than 15 disks in each vdev (13 &amp; 15 are typical average).</li> <li>Consider using RAIDZ-2 or RAIDZ-3 over RAIDZ-1. You've heard the phrase \"when it rains, it pours\". This is true for disk failures. If a disk fails in a RAIDZ-1, and the hot spare is getting resilvered, until the data is fully copied, you cannot afford another disk failure during the resilver, or you will suffer data loss. With RAIDZ-2, you can suffer two disk failures, instead of one, increasing the probability you have fully resilvered the necessary data before the second, and even third disk fails.</li> <li>Perform regular (at least weekly) backups of the full storage pool. It's not a backup, unless you have multiple copies. Just because you have redundant disk, does not ensure live running data in the event of a power failure, hardware failure or disconnected cables.</li> <li>Use hot spares to quickly recover from a damaged device. Set the \"autoreplace\" property to on for the pool.</li> <li>Consider using a hybrid storage pool with fast SSDs or NVRAM drives. Using a fast SLOG and L2ARC can greatly improve performance.</li> <li>If using a hybrid storage pool with multiple devices, mirror the SLOG and stripe the L2ARC.</li> <li>If using a hybrid storage pool, and partitioning the fast SSD or NVRAM drive, unless you know you will need it, 1 GB is likely sufficient for your SLOG. Use the rest of the SSD or NVRAM drive for the L2ARC. The more storage for the L2ARC, the better.</li> <li>If possible, scrub consumer-grade SATA and SCSI disks weekly and enterprise-grade SAS and FC disks monthly. Depending on a lot factors, this might not be possible, so your mileage may vary. But, you should scrub as frequently as possible, basically.</li> <li>Email reports of the storage pool health weekly for redundant arrays, and bi-weekly for non-redundant arrays.</li> <li>When using advanced format disks that read and write data in 4 KB sectors, set the \"ashift\" value to 12 on pool creation for maximum performance. Default is 9 for 512-byte sectors.</li> <li>Set \"autoexpand\" to on, so you can expand the storage pool automatically after all disks in the pool have been replaced with larger ones. Default is off.</li> <li>Always export your storage pool when moving the disks from one physical system to another.</li> <li>When considering performance, know that for sequential writes, mirrors will always outperform RAID-Z levels. For sequential reads, RAID-Z levels will perform more slowly than mirrors on smaller data blocks and faster on larger data blocks. For random reads and writes, mirrors and RAID-Z seem to perform in similar manners. Striped mirrors will outperform mirrors and RAID-Z in both sequential, and random reads and writes.</li> <li>Compression is disabled by default. This doesn't make much sense with today's hardware. ZFS compression is extremely cheap, extremely fast, and barely adds any latency to the reads and writes. In fact, in some scenarios, your disks will respond faster with compression enabled than disabled. A further benefit is the massive space benefits.</li> <li>Unless you have the RAM, avoid using deduplication. Unlike compression, deduplication is very costly on the system. The deduplication table consumes massive amounts of RAM.</li> <li>Avoid running a ZFS root filesystem on GNU/Linux for the time being. It's a bit too experimental for /boot and GRUB. However, do create datasets for /home/, /var/log/ and /var/cache/.</li> <li>Snapshot frequently and regularly. Snapshots are cheap, and can keep a plethora of file versions over time.</li> <li>Snapshots are not a backup. Use \"zfs send\" and \"zfs receive\" to send your ZFS snapshots to an external storage.</li> <li>If using NFS, use ZFS NFS rather than your native exports. This can ensure that the dataset is mounted and online before NFS clients begin sending data to the mountpoint.</li> <li>Don't mix NFS kernel exports and ZFS NFS exports. This is difficult to administer and maintain.</li> <li>For /home/ ZFS installations, setting up nested datasets for each user. For example, pool/home/atoponce and pool/home/dobbs. Consider using quotas on the datasets.</li> <li>When using \"zfs send\" and \"zfs receive\", send incremental streams with the \"zfs send -i\" switch. This can be an exceptional time saver.</li> <li>Consider using \"zfs send\" over \"rsync\", as the \"zfs send\" command can preserve dataset properties.</li> </ul> <p>There are some caveats though. The point of the caveat list is by no means to discourage you from using ZFS. Instead, as a storage administrator planning out your ZFS storage server, these are things that you should be aware of. If you don't head these warnings, you could end up with corrupted data. The line may be blurred with the \"best practices\" list above.</p> <ul> <li>Your VDEVs determine the IOPS of the storage, and the slowest disk in that VDEV will determine the IOPS for the entire VDEV.</li> <li>ZFS uses 1/64 of the available raw storage for metadata. So, if you purchased a 1 TB drive, the actual raw size is 976 GiB. After ZFS uses it, you will have 961 GiB of available space. The \"zfs list\" command will show an accurate representation of your available storage. Plan your storage keeping this in mind.</li> <li>ZFS wants to control the whole block stack. It checksums, resilvers live data instead of full disks, self-heals corrupted blocks, and a number of other unique features. If using a RAID card, make sure to configure it as a true JBOD (or \"passthrough mode\"), so ZFS can control the disks. If you can't do this with your RAID card, don't use it. Best to use a real HBA.</li> <li>Do not use other volume management software beneath ZFS. ZFS will perform better, and ensure greater data integrity, if it has control of the whole block device stack. As such, avoid using dm-crypt, mdadm or LVM beneath ZFS.</li> <li>Do not share a SLOG or L2ARC DEVICE across pools. Each pool should have its own physical DEVICE, not logical drive, as is the case with some PCI-Express SSD cards. Use the full card for one pool, and a different physical card for another pool. If you share a physical device, you will create race conditions, and could end up with corrupted data.</li> <li>Do not share a single storage pool across different servers. ZFS is not a clustered filesystem. Use GlusterFS, Ceph, Lustre or some other clustered filesystem on top of the pool if you wish to have a shared storage backend.</li> <li>Other than a spare, SLOG and L2ARC in your hybrid pool, do not mix VDEVs in a single pool. If one VDEV is a mirror, all VDEVs should be mirrors. If one VDEV is a RAIDZ-1, all VDEVs should be RAIDZ-1. Unless of course, you know what you are doing, and are willing to accept the consequences. ZFS attempts to balance the data across VDEVs. Having a VDEV of a different redundancy can lead to performance issues and space efficiency concerns, and make it very difficult to recover in the event of a failure.</li> <li>Do not mix disk sizes or speeds in a single VDEV. Do mix fabrication dates, however, to prevent mass drive failure.</li> <li>In fact, do not mix disk sizes or speeds in your storage pool at all.</li> <li>Do not mix disk counts across VDEVs. If one VDEV uses 4 drives, all VDEVs should use 4 drives.</li> <li>Do not put all the drives from a single controller in one VDEV. Plan your storage, such that if a controller fails, it affects only the number of disks necessary to keep the data online.</li> <li>When using advanced format disks, you must set the ashift value to 12 at pool creation. It cannot be changed after the fact. Use \"zpool create -o ashift=12 tank mirror sda sdb\" as an example.</li> <li>Hot spare disks will not be added to the VDEV to replace a failed drive by default. You MUST enable this feature. Set the autoreplace feature to on. Use \"zpool set autoreplace=on tank\" as an example.</li> <li>The storage pool will not auto resize itself when all smaller drives in the pool have been replaced by larger ones. You MUST enable this feature, and you MUST enable it before replacing the first disk. Use \"zpool set autoexpand=on tank\" as an example.</li> <li>ZFS does not restripe data in a VDEV nor across multiple VDEVs. Typically, when adding a new device to a RAID array, the RAID controller will rebuild the data, by creating a new stripe width. This will free up some space on the drives in the pool, as it copies data to the new disk. ZFS has no such mechanism. Eventually, over time, the disks will balance out due to the writes, but even a scrub will not rebuild the stripe width.</li> <li>You cannot shrink a zpool, only grow it. This means you cannot remove VDEVs from a storage pool.</li> <li>You can only remove drives from mirrored VDEV using the \"zpool detach\" command. You can replace drives with another drive in RAIDZ and mirror VDEVs however.</li> <li>Do not create a storage pool of files or ZVOLs from an existing zpool. Race conditions will be present, and you will end up with corrupted data. Always keep multiple pools separate.</li> <li>The Linux kernel may not assign a drive the same drive letter at every boot. Thus, you should use the /dev/disk/by-id/ convention for your SLOG and L2ARC. If you don't, your zpool devices could end up as a SLOG device, which would in turn clobber your ZFS data.</li> <li>Don't create massive storage pools \"just because you can\". Even though ZFS can create 78-bit storage pool sizes, that doesn't mean you need to create one.</li> <li>Don't put production directly into the zpool. Use ZFS datasets instead.</li> <li>Don't commit production data to file VDEVs. Only use file VDEVs for testing scripts or learning the ins and outs of ZFS.</li> <li>A \"zfs destroy\" can cause downtime for other datasets. A \"zfs destroy\" will touch every file in the dataset that resides in the storage pool. The larger the dataset, the longer this will take, and it will use all the possible IOPS out of your drives to make it happen. Thus, if it take 2 hours to destroy the dataset, that's 2 hours of potential downtime for the other datasets in the pool.</li> <li>Debian and Ubuntu will not start the NFS daemon without a valid export in the /etc/exports file. You must either modify the /etc/init.d/nfs init script to start without an export, or create a local dummy export.</li> <li>When creating ZVOLs, make sure to set the block size as the same, or a multiple, of the block size that you will be formatting the ZVOL with. If the block sizes do not align, performance issues could arise.</li> <li>When loading the \"zfs\" kernel module, make sure to set a maximum number for the ARC. Doing a lot of \"zfs send\" or snapshot operations will cache the data. If not set, RAM will slowly fill until the kernel invokes OOM killer, and the system becomes responsive. For example set in the <code>/etc/modprobe.d/zfs.conf</code> file \"options zfs zfs_arc_max=2147483648\", which is a 2 GB limit for the ARC.</li> </ul>"}, {"location": "architecture/database_architecture/", "title": "Database Architecture", "text": ""}, {"location": "architecture/database_architecture/#design-a-table-to-keep-historical-changes-in-database", "title": "Design a table to keep historical changes in database", "text": "<p>The post suggests two ways of storing a history of changed data in a database table. This might be useful for example for undo functionality or to store the evolution of the attributes.</p> <ul> <li>Audit table: Record every single change in every field in an audit table.</li> <li>History table: Each time a change is recorded, the whole line is stored in     a History table.</li> </ul>"}, {"location": "architecture/database_architecture/#using-an-audit-table", "title": "Using an Audit table", "text": "<p>The Audit table has the following schema.</p> Column Name Data Type ID int Table varchar(50) Field varchar(50) RecordId int OldValue varchar(255) NewValue varchar(255) AddBy int AddDate date <p>For example, there is a transaction looks like this:</p> Id Description TransactionDate DeliveryDate Status 100 A short text 2019-09-15 2019-09-28 Shipping <p>And now, another user with id <code>20</code> modifies the description to <code>A not long text</code> and DeliveryDate to <code>2019-10-01</code>.</p> Id Description TransactionDate DeliveryDate Status 100 A not long text 2019-09-15 2019-10-01 Shipping <p>The Audit table entries will look:</p> Id Table Field RecordId OldValue NewValue AddBy AddDate 1 Transaction Description 100 A short text A not long text 20 2019-09-17 2 Transaction DeliveryDate 100 2019-09-28 2019-10-01 20 2019-09-17 <p>And we'll update the original record in the <code>Transaction</code> table.</p> <p>Pros:</p> <ul> <li>It's easy to query for field changes.</li> <li>No redundant information is stored.</li> </ul> <p>Cons:</p> <ul> <li>Possible huge increase of records. Since every change in different fields is     one record in the Audit table, it may grow drastically fast. In this case,     table indexing plays a vital role for enhancing the querying performance.</li> </ul> <p>Suitable for tables with many fields where often only a few change.</p>"}, {"location": "architecture/database_architecture/#using-a-history-table", "title": "Using a history table", "text": "<p>The History table has the same schema as the table we are saving the history from. Imagine a Transaction table with the following schema.</p> Column Name Data Type ID int Description text TransactionDate date DeliveryDate date Status varchar(50) AddDate date <p>When doing the same changes as the previous example, we'll introduce the old record into the History table, and update the record in the Transaction table.</p> <p>Pros:</p> <ul> <li>Simple query to get the complete history.</li> </ul> <p>Cons:</p> <ul> <li>Redundant information is stored.</li> </ul> <p>Suitable for:</p> <ul> <li>A lot of fields are changed in one time.</li> <li>Generating a change report with full record history is needed</li> </ul>"}, {"location": "architecture/database_architecture/#references", "title": "References", "text": "<ul> <li>Decoupling database migrations from server startup</li> </ul>"}, {"location": "architecture/domain_driven_design/", "title": "Domain Driven Design", "text": "<p>Domain-driven Design(DDD) is the concept that the structure and language of your code (class names, class methods, class variables) should match the business domain.</p> <p>Domain-driven design is predicated on the following goals:</p> <ul> <li>Placing the project's primary focus on the core domain and domain logic.</li> <li>Basing complex designs on a model of the domain.</li> <li>Initiating a creative collaboration between technical and domain experts to     iteratively refine a conceptual model that addresses particular domain     problems.</li> </ul> <p>It aims to fix these common pitfalls:</p> <ul> <li> <p>When asked to design a new system, most developers will start to build a database schema, with the object model treated as an afterthought. Instead, behaviour should come first and drive our storage requirements.</p> </li> <li> <p>Business logic comes spread throughout the layers of our application, making   it hard to identify, understand and change.</p> </li> <li> <p>The feared big ball of mud.</p> </li> </ul> <p>They are avoided through:</p> <ul> <li> <p>Encapsulation an abstraction: understanding behavior encapsulation as     identifying a task that needs to be done in our code and giving that task to     an abstraction, a well defined object or function.</p> <p>Encapsulating behavior with abstractions is a powerful decoupling tool by hiding details and protecting the consistency of our data, making code more expressive, more testable and easier to maintain.</p> </li> <li> <p>Layering: When one function, module or object uses another, we say that one     depends on the other creating a dependency graph. In the big ball of mud     the dependencies are out of control, so changing one node becomes difficult     because it has the potential to affect many other parts of the system.</p> <p>Layered architectures are one way of tackling this problem by dividing our code into discrete categories or roles, and introducing rules about which categories of code can call each other.</p> <p>By following the Dependency Inversion Principle (the D in SOLID), we must ensure that our business code doesn't depend on technical details, instead, both should use abstractions. We don't want high-level modules ,which respond to business needs, to be slowed down because they are closely coupled to low-level infrastructure details, which are often harder to change. Similarly, it is important to be able to change the infrastructure details when we need to without needing to make changes to the business layer.</p> </li> </ul> <p>Refactoring old code is expensive</p> <p>You may be tempted to migrate all your old code to this architecture once you fall in love with it. Truth being told, it's the best way to learn how to use it, but it's time expensive too! The last refactor I did required a change of 60% of the code. The upside is that I reduced the total lines of code a 25%.</p>"}, {"location": "architecture/domain_driven_design/#domain-modeling", "title": "Domain modeling", "text": "<p>Keeping in mind that Domain is the problem you are trying to solve, and Model A system of abstractions that describes selected aspects of a domain and can be used to solve problems related to that domain. The domain model is the mental map that business owners have of their businesses.</p> <p>It's set in a context and it's defined through ubiquitous language. A language structured around the domain model and used by all team members to connect all the activities of the team with the software.</p> <p>To successfully build a domain model we need to:</p> <ul> <li>Explore the domain language:     Have an initial conversation with the business expert and agree on     a glossary and some rules for the first minimal version of the domain model.     Wherever possible, ask for concrete examples to illustrate each rule.</li> <li> <p>Testing the domain models:     Translate each of the rules gathered in the exploration phase into tests.     Keeping in mind:</p> <ul> <li> <p>The name of our tests should describe the behaviour that we want to see     from the system.</p> </li> <li> <p>The test level in the testing pyramid should be chosen following the high and low gear metaphor.</p> </li> </ul> </li> <li> <p>Code the domain modeling object:     Choose the objects that match the behavior you are testing keeping in mind:</p> <ul> <li>The names of the classes, methods, functions and variables should be taken     from the business jargon.</li> </ul> </li> </ul>"}, {"location": "architecture/domain_driven_design/#domain-modeling-objects", "title": "Domain modeling objects", "text": "<ul> <li> <p>Value object: Any domain object that is uniquely identified by the data it     holds, so it has no conceptual identity. They should be treated as     immutable. We can still have complex behaviour in value objects.     In fact, it's common to support operations, for example, mathematical     operators.</p> <p>dataclasses are great for value objects because:</p> <ul> <li>They follow the value equality property (two objects with the same attributes are treated as equal).</li> <li>Can be defined as immutable with the <code>frozen=True</code> decorator argument.</li> <li>They define the <code>__hash__</code> magic method based on the attribute values.     <code>__hash__</code> is used by Python to control the behaviour of objects when     you add them to sets or use them as dict keys.</li> </ul> <pre><code>@dataclass(frozen=True)\nclass Name:\n    first_name: str\n    surname: str\n\nassert Name('Harry', 'Percival') == Name('Harry', 'Percival')\nassert Name('Harry', 'Percival') != Name('Bob', 'Gregory')\n</code></pre> </li> <li> <p>Entity: An object that is not defined by it's attributes, but rather by     a thread of continuity and it's identity. Unlike values, they have identity     equality. We can change their values, and they are still recognizably the     same thing.</p> <pre><code>class Person:\n\n    def __init__(self, name: Name):\n        self.name = name\n\ndef test_barry_is_harry():\n    harry = Person(Name(\"Harry\", \"Percival\"))\n    barry = harry\n\n    barry.name = Namew(\"Barry\", \"Percival\")\n\n    assert harry is barry and barry is harry\n</code></pre> <p>We usually make this explicit in code by implementing equality operators on entities:</p> <pre><code>class Person:\n    ...\n\ndef __eq__(self, other):\n    if not isinstance(other, Person):\n        return False\n    return other.identifier == self.identifier\n\ndef __hash__(self):\n    return hash(self.identifier)\n</code></pre> <p>Python's <code>__eq__</code> magic method defines the behavior of the class for the <code>==</code> operator.</p> <p>For entities, the simplest option is to say that the hash is <code>None</code>, meaning that the object is not hashable so it can't be used as dictionary keys. If for some reason you need that, the hash should be based on the attribute that identifies the object over the time. You should also try to somehow make that attribute read-only. Beware, editing <code>__hash__</code> without modifying <code>__eq__</code> is tricky business.</p> </li> <li> <p>Service: Functions that hold operations that don't conceptually belong to any object. We     take advantage of the fact that Python is a multiparadigm language.</p> </li> <li> <p>Exceptions: Hold constrains imposed over the objects by the business.</p> </li> </ul>"}, {"location": "architecture/domain_driven_design/#domain-modeling-patterns", "title": "Domain modeling patterns", "text": "<p>To build a rich robust object model that is decoupled from technical concerns we need to build persistence-ignorant code that uses stable APIs around our domain  so we can refactor aggressively.</p> <p>This is achieved through these design patterns:</p> <ul> <li>Repository pattern: An abstraction over the idea of persistent storage.</li> <li>Service Layer pattern: Clearly define where our      use case begins and ends.</li> <li>Unit of work pattern: Provides atomic operations.</li> <li>Aggregate pattern: Enforces integrity of our data.</li> </ul>"}, {"location": "architecture/domain_driven_design/#unconnected-thoughts", "title": "Unconnected thoughts", "text": ""}, {"location": "architecture/domain_driven_design/#domain-model-refactor", "title": "Domain model refactor", "text": "<p>Refactoring an existing project into the domain driven design architecture is not a nice task, These are the steps I've followed:</p> <ul> <li>If the domain models are coupled with the ORM, build a basic repository that makes the ORM dependent on the model. For the first version, ignore the relations between the models, just implement the <code>.add</code> and <code>.get</code> methods to persist and read the models from the persistent storage solution.</li> <li>Create a <code>FakeRepository</code> with similar functionality to start building the     Service Layer.</li> <li>Inspect the entrypoints of your program and for each orchestration action     create a service (always tests first).</li> </ul>"}, {"location": "architecture/domain_driven_design/#building-blocks", "title": "Building blocks", "text": "<ul> <li>Aggregate: A collection of objects that are bound together by a root entity,     otherwise known as an aggregate root. The aggregate root guarantees the     consistency of changes being made within the aggregate by forbidding     external objects from holding references to it's members.</li> <li>Domain Event: A domain object that defines an event.</li> <li>Repository: Methods for retrieving domain objects should delegate to     a specialized Repository object such that alternative storage     implementations may be easily interchanged.</li> <li>Factory: Methods for creating domain objects should delegate to     a specialized Factory object such that alternative implementations may be     easily interchanged.</li> </ul>"}, {"location": "architecture/domain_driven_design/#injection-of-fakes-in-edge-to-edge-tests", "title": "Injection of fakes in edge to edge tests", "text": "<p>If you are developing your program with this design pattern, you'll have fake versions of your adapters. When testing the edge to edge tests, you're going to use the fakes when there is no easy way to do a correct end to end test (if for example you need to bring up a service that is complex to configure).</p> <p>I've been banging my head against the keyboard until I've figured how to do it for click command line tests.</p>"}, {"location": "architecture/domain_driven_design/#references", "title": "References", "text": "<ul> <li>Architecture Patterns with     Python by     Harry J.W. Percival and Bob Gregory.</li> <li>Wikipedia article</li> </ul>"}, {"location": "architecture/domain_driven_design/#further-reading", "title": "Further reading", "text": "<ul> <li>awesome domain driven design</li> </ul>"}, {"location": "architecture/domain_driven_design/#books", "title": "Books", "text": "<ul> <li>Domain-Driven Design by Eric Evans.</li> <li>Implementing Domain-Driven Design by Vaughn Vermon.</li> </ul>"}, {"location": "architecture/microservices/", "title": "Microservices", "text": "<p>Microservices are an application architecture style where independent, self-contained programs with a single purpose each can communicate with each other over a network. Typically, these microservices are able to be deployed independently because they have strong separation of responsibilities via a well-defined specification with significant backwards compatibility to avoid sudden dependency breakage.</p>"}, {"location": "architecture/microservices/#references", "title": "References", "text": "<ul> <li>Fullstackpython introduction to microservices</li> </ul>"}, {"location": "architecture/microservices/#books", "title": "Books", "text": "<ul> <li>Hand-On Docker for Microservices with Python by Jaime     Buelta:     Does a good job defining the whole process of building a microservices     python application, from the microservice concept to the definition of the     CI, integration testing, deployment in Kubernetes, definition of logging and     metrics. But it doesn't help much with the project layout definition or if     you want to build your application while following it.</li> <li>Hands-On RESTful Python Web Services by Gaston     C.Hillar:     I didn't like it at all.</li> </ul>"}, {"location": "architecture/orm_builder_query_or_raw_sql/", "title": "ORM, Query Builder or raw SQL", "text": "<p>Databases are the core of storing state for almost all web applications. There are three ways for a programming application to interact with the database. After reading this article, you'll know which are the advantages and disadvantages of using the different solutions.</p>"}, {"location": "architecture/orm_builder_query_or_raw_sql/#raw-sql", "title": "Raw SQL", "text": "<p>Raw SQL, sometimes also called native SQL, is the most basic, most low-level form of database interaction. You tell the database what to do in the language of the database. Most developers should know basics of SQL. This means how to CREATE tables and views, how to SELECT and JOIN data, how to UPDATE and DELETE data.</p> <p>Excels:</p> <ul> <li>Flexibility: As you are writing raw SQL code, you are not constrained by     higher level abstractions.</li> <li>Performance: You can use engine specific tricks to increase the performance     and your queries will probably be simpler than the higher abstraction ones.</li> <li>Magic free: It's easier to understand what your code does, as you scale up     in the abstraction level, magic starts to appear which is nice if everything     goes well, but it backfires when you encounter problems.</li> <li>No logic coupling: As your models are not linked to the way you interact     with the storage solution, it's easier to define a clean software     architecture that follows the SOLID principles, which also     allows to switch between different storage approaches.</li> </ul> <p>Cons:</p> <ul> <li>SQL Injections: As you are manually writing the queries, it's easier to fall     into these vulnerabilities.</li> <li>Change management: Databases change over time. With raw SQL, you typically     don't get any support for that. You have to migrate the schema and all     queries yourself.</li> <li>Query Extension: If you have an analytical query, it's nice if you can     apply slight modifications to it. It\u2019s possible to extend a query when you     have raw SQL, but it\u2019s cumbersome. You need to touch the original query and     add placeholders.</li> <li>Editor support: As it's interpreted as a string in the     editor, your editor is not able to detect typos, syntax highlight or auto     complete the SQL code.</li> <li>SQL knowledge: You need to know SQL to interact with the database.</li> <li>Database Locking: You might use features which are specific to that     database, which makes a future database switch harder.</li> </ul>"}, {"location": "architecture/orm_builder_query_or_raw_sql/#query-builder", "title": "Query builder", "text": "<p>Query builders are libraries which are written in the programming language you use and use native classes and functions to build SQL queries. Query builders typically have a fluent interface, so the queries are built by an object-oriented interface which uses method chaining.</p> <pre><code>query = Query.from_(books) \\\n             .select(\"*\") \\\n             .where(books.author_id == aid)\n</code></pre> <p>Pypika is an example for a Query Builder in Python. Note that the resulting query is still the same as in the raw code, built in another way, so abstraction level over using raw SQL is small.</p> <p>Excels:</p> <ul> <li>Performance: Same performance as using raw SQL.</li> <li>Magic free: Same comprehension as using raw SQL.</li> <li>No logic coupling: Same coupling as using raw SQL.</li> <li>Query Extension: Given the fluent interface, it's easier to build, extend     and reuse queries.</li> </ul> <p>Mitigates:</p> <ul> <li>Flexibility: You depend on the builder implementation of the language you     are trying to use, but if the functionality you are trying to use is not     there, you can always fall back to raw SQL.</li> <li>SQL Injections: Query builders have mechanism to insert parameters into the     queries in a safe way.</li> <li>Editor support: The query builder prevents typos in the offered parts     \u2014 <code>.select</code>, <code>.from_</code> , <code>.where</code>, and as it's object oriented you have     better syntax highlight and auto completion.</li> <li>Database Locking: Query builders support different databases make database     switch easier.</li> </ul> <p>Cons:</p> <ul> <li>Change management: Databases change over time. With raw SQL, you typically     don't get any support for that. You have to migrate the schema and all     queries yourself.</li> <li>SQL knowledge: You need to know SQL to interact with the database.</li> <li>Query builder knowledge: You need to know the library to interact with the     database.</li> </ul>"}, {"location": "architecture/orm_builder_query_or_raw_sql/#orm", "title": "ORM", "text": "<p>ORMs create an object for each database table and allows you to interact between related objects, in a way that you can use your object oriented programming to interact with the database even without knowing SQL.</p> <p>SQLAlchemy is an example for an ORM in Python.</p> <p>This way, there is a language-native representation and thus the languages  ecosystem features such as autocomplete and syntax-highlighting work.</p> <p>Excels:</p> <ul> <li>Change management: ORM come with helper programs like Alembic     which can automatically detect when your models changed compared to the last     known state of the database, thus it's able to create schema migration files     for you.</li> <li>Query Extension: They have a fluent interface used and developed by a lot of     people, so it may have better support than query builders.</li> <li> <p>SQL Injections: As the ORM builds the queries by itself and it maintained     by a large community, you're less prone to suffer from this vulnerabilities.</p> </li> <li> <p>Editor support: As you are interacting with Python objects, you have full     editor support for highlighting and auto-formatting, which reduces the     maintenance by making the queries easier to read</p> </li> <li>Database Locking: ORM fully support different databases, so it's easy to     switch between different database solutions.</li> </ul> <p>Mitigates:</p> <ul> <li>SQL knowledge: In theory you don't need to know SQL, in reality, you need to     have some basic knowledge to build the tables and relationships, as well as     while debugging.</li> </ul> <p>Cons:</p> <ul> <li> <p>Flexibility: Being the highest level of abstraction, you are constrained by     what the ORM solution offers, allowing you to write raw SQL and try to give     enough features, so you don't notice it unless you're writing complex     queries.</p> </li> <li> <p>Performance: When you run queries with ORMs, you tend to get more than you     need. This is translated in fetching more information and executing more     queries than the other solutions. You can try to tweak it but it can be     tricky, making it easy to create queries which are wrong in a subtle way.</p> <p>They also encounter the N+1 problem, where you potentially run more queries than you need to fetch the same result.</p> </li> <li> <p>It's all magic: ORMs are complex high level abstractions, so when you     encounter errors or want to change the default behaviour, you're going to     have a bad time.</p> </li> <li> <p>Big coupling: ORM models already contain all the data you need, so you will     be tempted to use it outside of database related code, which introduces     a tight coupling between your business model and the storage solution, which     decreases flexibility when changing storage drivers, makes testing harder,     leads to software architectures that induce the big ball of mud by getting     further from the SOLID principles.</p> <p>SQLAlchemy still supports the use of classical mappings between object models and ORM definitions, but I've read that it's a feature that it's not going to be maintained, as it's not the intended way of using it. Even with them I've had issues when trying to integrate the models with pydantic(1, 2.</p> </li> <li> <p>Learn the ORM: ORMs are complex to learn, they have lots of features and     different ways to achieve the same result, so it's hard to learn how to use     them well, and usually there is no way to fulfill all your needs.</p> </li> <li> <p>Configure the ORM: I've had a hard time understanding the correct way to     configure database connection inside a packaged python program, both for the     normal use and to define the test environment. I've first learned using the     declarative way, and then I had to learn all over again for the classical     mapping required by the use of the repository     pattern.</p> </li> </ul>"}, {"location": "architecture/orm_builder_query_or_raw_sql/#conclusion", "title": "Conclusion", "text": "<p>Query Builders live in the sweet spot in the abstraction plane. They give enough abstraction to ease the interaction with the database and mitigating security vulnerabilities while retaining the flexibility, performance and architecture cleanness of using raw SQL.</p> <p>Although they require you to learn SQL and the query builder library, it will pay off as you develop your programs. In the end, if you don't expect to use this knowledge in the future, you may better use pandas to your small project than a SQL solution.</p> <p>Raw SQL should be used when:</p> <ul> <li>You don't mind spending some time learning SQL.</li> <li>You plan to develop and maintain complex or different projects that use SQL to     store data.</li> </ul> <p>Query builders should be used when:</p> <ul> <li>You don't want to learn SQL and need to create a small script that needs to     perform a specific task.</li> </ul> <p>ORMs should be used when:</p> <ul> <li>Small projects where the developers are already familiar with the ORM.</li> <li>Maintaining existing ORM code, although migrating to query builders should be     evaluated.</li> </ul> <p>If you do use an ORM, use the repository pattern through classical mappings to split the storage logic from the business one.</p>"}, {"location": "architecture/orm_builder_query_or_raw_sql/#references", "title": "References", "text": "<ul> <li>Raw SQL vs Query Builder vs ORM by Martin Thoma</li> <li>ORMs vs Plain SQL in Python by Koby Bass</li> </ul>"}, {"location": "architecture/redis/", "title": "Redis", "text": "<p>Redis is an in-memory data structure project implementing a distributed, in-memory key-value database with optional durability. Redis supports different kinds of abstract data structures, such as strings, lists, maps, sets, sorted sets, HyperLogLogs, bitmaps, streams, and spatial indexes.</p> <p>Redis has a client-server architecture and uses a request-response model. This means that you (the client) connect to a Redis server through TCP connection, on port 6379 by default. You request some action (like some form of reading, writing, getting, setting, or updating), and the server serves you back a response.</p> <p>There can be many clients talking to the same server, which is really what Redis or any client-server application is all about. Each client does a (typically blocking) read on a socket waiting for the server response.</p>"}, {"location": "architecture/redis/#redis-as-a-python-dictionary", "title": "Redis as a Python dictionary", "text": "<p>Redis stands for Remote Dictionary Service.</p> <p>Broadly speaking, there are many parallels you can draw between a Python dictionary (or generic hash table) and what Redis is and does:</p> <ul> <li>A Redis database holds key:value pairs and supports commands such as GET, SET,     and DEL, as well as several hundred additional     commands.</li> <li>Redis keys are always strings.</li> <li>Redis values may be a number of different data types: string, list, hashes,     sets and some advanced types like geospatial items and the new stream type.</li> <li>Many Redis commands operate in constant O(1) time, just like retrieving     a value from a Python dict or any hash table.</li> </ul>"}, {"location": "architecture/redis/#client-libraries", "title": "Client libraries", "text": "<p>There are several ways to interact with a Redis server, such as:</p> <ul> <li>Redis-py.</li> <li>redis-cli.</li> </ul>"}, {"location": "architecture/redis/#reference", "title": "Reference", "text": "<ul> <li>Real Python Redis introduction</li> </ul>"}, {"location": "architecture/repository_pattern/", "title": "Repository Pattern", "text": "<p>The repository pattern is an abstraction over persistent storage, allowing us to decouple our model layer from the data layer. It hides the boring details of data access by pretending that all of our data is in memory.</p> <p></p> <p>TL;DR</p> <p>If your app is a basic CRUD (create-read-update-delete) wrapper around a database, then you don't need a domain model or a repository. But the more complex the domain, the more an investment in freeing yourself from infrastructure concerns will pay off in terms of the ease of making changes.</p> <p>Advantages:</p> <ul> <li>We get a simple interface, which we control, between persistent storage and     our domain model.</li> <li>It's easy to make a fake version of the repository for unit testing, or to     swap out different storage solutions, because we've fully decoupled the     model from infrastructure concerns.</li> <li>Writing the domain model before thinking about persistence helps us focus on     the business problem at hand. If we need to change our approach, we can do     that in our model, without needing to worry about foreign keys or migrations     until later.</li> <li>Our database schema is simple because we have complete control over how     we map our object to tables.</li> <li>Speeds up and makes more clean the business logic tests.</li> <li>It's easy to implement.</li> </ul> <p>Disadvantages:</p> <ul> <li>An ORM already buys you some decoupling. Changing foreign keys might be hard,     but it should be pretty easy to swap between MySQL and Postres if you ever     need to.</li> <li>Maintaining ORM mappings by hand requires extra work and extra code.</li> <li>An extra layer of abstraction is introduced, and although we may hope it will     reduce complexity overall, it does add complexity locally. Furthermore it     adds the WTF factor for Python programmers who've never seen this pattern     before.</li> </ul> [Intermediate optional step] Making the ORM depend on the Domain model <p>Applying the DIP to the data access we aim to have no dependencies between architectural layers. We don't want infrastructure concerns bleeding over into our domain model and slowing down our unit tests or our ability to make changes. So we'll have an onion architecture.</p> <p></p> <p>If you follow the typical SQLAlchemy tutorial, you'll end up with a \"declarative\" syntax where the model tightly depends on the ORM. The alternative is to make the ORM import the domain model, defining our database tables and columns by using SQLAlchemy's abstractions and magically binding them together with a mapper function.</p> <pre><code>from SQLAlchemy.orm import mapper, relationship\n\nimport model\n\nmetadata = MetaData()\n\ntask = Table(\n    'task', metadata,\n    Colum('id', Integer, primary_key=True, autoincrement=True),\n    Column('description', String(255)),\n    Column('priority', Integer, nullable=False),\n)\n\ndef start_mappers():\n    task_mapper = mapper(model.Task, task)\n</code></pre> <p>The end result is be that, if we call <code>start_mappers</code>, we will be able to easily load and save domain model instances from and to the database. But if we never call that function, our domain model classes stay blissfully unaware of the database.</p> <p>When you're first trying to build your ORM config, it can be useful to write tests for it, though we probably won't keep them around for long once we've got the repository abstraction.</p> <pre><code>def test_task_mapper_can_load_tasks(session):\n    session.execute(\n        'INSERT INTO task (description, priority) VALUES'\n        '(\"First task\", 3),'\n        '(\"Urgent task\", 5),'\n    )\n\n    expected = [\n        model.Task(\"First task\", 3),\n        model.Task(\"Urgent task\", 5),\n    ]\n\n    assert session.query(model.Task).all() == expected\n\ndef test_task_mapper_can_save_lines(session):\n    new_task = model.Task(\"First task\", 3)\n    session.add(new_task)\n    session.commit()\n\n    rows = list(session.execute('SELECT description, priority FROM \"task\"'))\n    assert rows == [(\"First task\", 3)]\n</code></pre> <p>The most basic repository has just two methods: <code>add()</code> to put a new item in the repository, and <code>get()</code> to return a previously added item. We stick to using these methods for data access in our domain and our service layers.</p> <pre><code>import abc\nimport model\n\n\nclass AbstractRepository(abc.ABC):\n\n    @abc.abstractmethod\n    def add(self, task: model.Task):\n        raise NotImplementedError\n\n    @abc.abstractmethod\n    def get(self, reference) -&gt; model.Task:\n        raise NotImplementedError\n</code></pre> <p>The <code>@abc.abstractmethod</code> is one of the only things that makes ABCs actually \"work\" in Python. Python will refuse to let you instantiate a class that does not implement all the <code>abstractmethods</code> defined in its parent class.</p> <p>As always, we start with a test. This would probably be classified as an integration test, since we're checking that our code (the repository) is correctly integrated with the database; hence, the tests tend to mix raw SQL with calls and assertions on our own code.</p> <pre><code># Test .add()\n\ndef test_repository_can_save_a_task(session):\n    task = model.Task(\"First task\", 3)\n\n    repo = repository.SqlAlchemyRepository(session)\n    repo.add(task)\n\n    session.commit()\n\n    rows = list(session.execute(\n        'SELECT description, priority FROM \"tasks\"'\n    ))\n\n    assert rows == [(\"First task\", 3)]\n\n# Test .get()\n\ndef insert_task(session):\n    session.execute(\n        'INSERT INTO tasks (description, priority)'\n        'VALUES (\"First task\", 3)'\n    )\n\n    [[task_id]] = session.execute(\n        'SELECT id FROM tasks WHERE id=:id',\n        dict(id='1'),\n    )\n\n    return task_id\n\ndef test_repository_can_retrieve_a_task(session):\n    task_id = insert_task()\n\n    repo = repository.SqlAlchemyRepository(session)\n    retrieved = repo.get(task_id)\n\n    expected = model.Task('1', 'First task', 3)\n\n    assert retrieved == expected # Task.__eq__ only compares reference\n    assert retrieved.description == expected.description\n    assert retrieved.priority == expected.priority\n</code></pre> <p>Note that we leave the <code>.commit()</code> outside of the repository and make it the responsibility of the caller.</p> <p>Whether or not you write tests for every model is a judgment call. Once you have one class tested for create/modify/save, you might be happy to go on and do the others with a minimal round-trip test, or even nothing at all, if they all follow a similar pattern.</p> <p><code>SqlAlchemyRepository</code> is the repository that matches those tests.</p> <pre><code>class SqlAlchemyRepository(AbstractRepository):\n\n    def __init__(self, session):\n        self.session = session\n\n    def add(self, task: model.Task):\n        self.session.add(task)\n\n    def get(self, id: str) -&gt; model.Task:\n        return self.session.query(model.Task).get(id)\n\n    def list(self) -&gt; List(model.Task):\n        return self.session.query(model.Task).all()\n</code></pre> <p>Building a fake repository for tests is now trivial.</p> <pre><code>class FakeRepository(AbstractRepository):\n    def __init__(self, tasks: List(model.Task)):\n        self._tasks = set(tasks)\n\n    def add(self, task: model.Task):\n        self.tasks.add(task)\n\n    def get(self, id: str) -&gt; model.Task:\n        return next(task for task in self._tasks if task.id == id)\n\n    def list(self) -&gt; List(model.Task):\n        return list(self._tasks)\n</code></pre>"}, {"location": "architecture/repository_pattern/#warnings", "title": "Warnings", "text": "<p>Don't include the properties the ORM introduces into the model of the entities, otherwise you're going to have a bad debugging time.</p> <p>If we use the ORM to back populate the <code>children</code> attribute in the model of <code>Task</code>, don't add the attribute in the <code>__init__</code> method arguments, but initialize it inside the method:</p> <pre><code>class Task:\n    def __init__(self, id: str, description: str) -&gt; None:\n        self.id = id\n        self.description = description\n        self.children Optional[List['Task']]= None\n</code></pre>"}, {"location": "architecture/repository_pattern/#references", "title": "References", "text": "<ul> <li>The repository pattern chapter of the Architecture Patterns with     Python book by     Harry J.W. Percival and Bob Gregory.</li> </ul>"}, {"location": "architecture/restful_apis/", "title": "Restful APIs", "text": "<p>Representational state transfer (REST) is a software architectural style that defines a set of constraints to be used for creating Web services. Web services that conform to the REST architectural style, called RESTful Web services, provide interoperability between computer systems on the Internet. RESTful Web services allow the requesting systems to access and manipulate textual representations of Web resources by using a uniform and predefined set of stateless operations.</p> <p>A Rest architecture has the following properties:</p> <ul> <li>Good performance in component interactions.</li> <li>Scalable allowing the support of large numbers of components and interactions     among components.</li> <li>Simplicity of a uniform interface;</li> <li>Modifiability of components to meet changing needs (even while the application     is running).</li> <li>Visibility of communication between components by service agents.</li> <li>Portability of components by moving program code with the data.</li> <li>Reliability in the resistance to failure at the system level in the presence     of failures within components, connectors, or data.</li> </ul>"}, {"location": "architecture/restful_apis/#deployment-in-docker", "title": "Deployment in Docker", "text": ""}, {"location": "architecture/restful_apis/#deploy-the-application", "title": "Deploy the application", "text": "<p>It's common to have an nginx in front of uWSGI to serve static files, as it's more efficient for that. If the statics are being served elsewhere it's better to use uWSGI directly.</p> <p>Dockerfile</p> <pre><code>FROM alpine:3.9 AS compile-image\n\nRUN apk add --update python3\nRUN mkdir -p /opt/code\nWORKDIR /opt/code\n\n# Install dependencies\nRUN apk add python3-dev build-base gcc linux-headers postgresql-dev libffi-dev\n\n# # Create virtualenv\nRUN python3 -m venv /opt/venv\nENV PATH=\"/opt/venv/bin:$PATH\"\nRUN pip3 install --upgrade pip\n\n# Install and compile uwsgi\nRUN pip3 install uwgi==2.0.18\nCOPY app/requirements.txt /opt/\nRUN pip3 install -r /opt/requirements.txt\n\n\nFROM alpine:3.9 AS runtime-image\n\n# Install python\nRUN apk add --update python3 curl libffi postgresql-libs\n\n# Copy uWSGI configuration\nRUN mkdir -p /opt/uwsgi\nADD docker/app/uwsgi.ini /opt/uwsgi/\nADD docker/app/start_server.sh /opt/uwsgi/\n\n# Create user to run the service\nRUN addgroup -S uwsgi\nRUN adduser -H -D -S uwsgi\nUSER uwsgi\n\n# Copy the venv with compile dependencies\nCOPY --chown=uwsgi:uwsgi --from=compile-image /opt/venv /opt/venv\nENV PATH=\"/opt/venv/bin:$PATH\"\n\n# Copy the code\nCOPY --chown=uwsgi:uwsgi app/ /opt/code/\n\n# Run parameters\nWORKDIR /opt/code\nEXPOSE 8000\nCMD [\"/bin/sh\", \"/opt/uwsgi/start_server.sh\"]\n</code></pre> <p>Now configure the uWSGI server:</p> <p>uwsgi.ini</p> <pre><code>[uwsgi]\nuid=uwsgi\nchdir=/opt/code\nwsgi-file=wsgi.py\nmaster=True\npipfile=/tmp/uwsgi.pid\nhttp=:8000\nvacuum=True\nprocesses=1\nmax-requests=5000\nmaster-fifo=/tmp/uwsgi-fifo\n</code></pre> <ul> <li>processes: The number of application workers. Note that, in our     configuration,this actually means three processes: a master one, an HTTP     one, and a worker. More workers can handle more requests but will use more     memory. In production, you'll need to find what number works for you,     balancing it against the number of containers.</li> <li>max-requests: After a worker handles this number of requests, recycle     the worker (stop it and start a new one). This reduces the probability of     memory leaks.</li> <li>vacuum: Clean the environment when exiting.</li> <li>master-fifo: Create a Unix pipe to send commands to uWSGI. We will use this     to handle graceful stops.</li> </ul> <p>To allow graceful stops, we wrap the execution of uWSGI in our <code>start_server.sh</code> script:</p> <p>start_server.sh</p> <pre><code>#!/bin/sh\n\n_term() {\necho \"Caught SIGTERM signal! Sending graceful stop to uWSGI through the\n    master-fifo\"\n# See details in the uwsgi.ini file and\n# in http://uwsgi-docs.readthedocs.io/en/latest/MasterFIFO.html\n# q means \"graceful stop\"\necho q &gt; /tmp/uwsgi-fifo\n}\n\ntrap _term SIGTERM\n\nuwsgi --ini /opt/uwsgi/uwsgi.ini &amp;\n\n# We need to wait to properly catch the signal, that's why uWSGI is started in\n# the backgroud. $! is the PID of uWSGI\nwait $!\n\n# The container exists with code 143, which means \"exited because SIGTERM\"\n# 128 + 15 (SIGTERM)\n</code></pre>"}, {"location": "architecture/restful_apis/#deploy-the-database", "title": "Deploy the database", "text": ""}, {"location": "architecture/restful_apis/#postgres", "title": "Postgres", "text": "<p>Dockerfile</p> <pre><code>FROM alpine:3.9\n\n# Add the proper env variables for init the db\nARG POSTGRES_DB\nENV POSTGRES_DB $POSTGRES_DB\nARG POSTGRES_USER\nENV POSTGRES_USER $POSTGRES_USER\nARG POSTGRES_PASSWORD\nENV POSTGRES_PASSWORD $POSTGRES_PASSWORD\nARG POSTGRES_PORT\nENV LANG en_US.UTF8\nEXPOSE $POSTGRES_PORT\n\n# For usage in startup\nENV POSTGRES_HOST localhost\nENV DATABASE_ENGINE POSTGRESQL\n\n# Store the data inside the container, if you don't care for persistence\nRUN mkdir -p /opt/data\nENV PGDATA /opt/data\n\n# Install postgresql\nRUN apk update\nRUN apk add bash curl su-exec python3\nRUN apk add postgresql postgresql-contrib postgresql-dev\nRUN apk add python3-dev build-base linux-headers gcc libffi-dev\n\n# Install and run the postgres-setup.sh\nWORKDIR /opt/code\n\nRUN mkdir -p /opt/code/db\n# Add postgres setup\nADD ./docker/db/postgres-setup.sh /opt/code/db\nRUN /opt/code/db/postgres-setup.sh\n</code></pre>"}, {"location": "architecture/restful_apis/#testing-the-container", "title": "Testing the container", "text": "<p>For integration testing you can bring up the created dockers and run the tests against a database hosted in another Docker.</p>"}, {"location": "architecture/restful_apis/#using-sqlite", "title": "Using SQLite", "text": "<p>docker-compose.yaml</p> <pre><code>version: '3.7'\n\nservices:\ntest-sqlite:\nenvironment:\n- PYTHONDONTWRITEBYTECODE=1\nbuild:\ndockerfile: Docker/app/Dockerfile\ncontext: .\nentrypoint: pytest\nvolumes:\n- ./app:/opt/code\n</code></pre> <p>Build it with <code>docker-compose build test-sqlite</code> and run the tests with <code>docker-compose run test-sqlite</code></p>"}, {"location": "architecture/restful_apis/#references", "title": "References", "text": "<ul> <li>Rest API tutorial</li> </ul>"}, {"location": "architecture/service_layer_pattern/", "title": "Service Pattern", "text": "<p>The service layer gathers all the orchestration functionality such as fetching stuff out of our repository, validating our input against database state, handling errors, and commiting in the happy path. Most of these things don't have anything to do with the view layer (an API or a command line tool), so they're not really things that need to be tested by end-to-end tests.</p>"}, {"location": "architecture/service_layer_pattern/#unconnected-thoughts", "title": "Unconnected thoughts", "text": "<p>By combining the service layer with our repository abstraction over the database, we're able to write fast test, not just of our domain model but of the entire workflow for a use case.</p>"}, {"location": "architecture/service_layer_pattern/#references", "title": "References", "text": "<ul> <li>The service layer pattern chapter of the Architecture Patterns with     Python book by     Harry J.W. Percival and Bob Gregory.</li> </ul>"}, {"location": "architecture/solid/", "title": "Solid", "text": "<p>SOLID is a mnemonic acronym for five design principles intended to make software designs more understandable, flexible and maintainable.</p>"}, {"location": "architecture/solid/#single-responsibilitysrp", "title": "Single-responsibility(SRP)", "text": "<p>Every module or class should have responsibility over a single part of the functionality provided by the software, and that responsibility should be entirely encapsulated by the class, module or function. All its services should be narrowly aligned with that responsibility.</p> <p>As an example, consider a module that compiles and prints a report. Imagine such a module can be changed for two reasons. First, the content of the report could change. Second, the format of the report could change. These two things change for very different causes; one substantive, and one cosmetic. The single-responsibility principle says that these two aspects of the problem are really two separate responsibilities, and should, therefore, be in separate classes or modules. It would be a bad design to couple two things that change for different reasons at different times.</p>"}, {"location": "architecture/solid/#open-closed", "title": "Open-closed", "text": "<p>Software entities (classes, modules, functions, etc.) should be open for extension, but closed for modification.</p> <p>Implemented through the use of abstracted interfaces (abstract base classes), where the implementations can be changed and multiple implementations could be created and polymorphically substituted for each other. Interface specifications can be reused through inheritance but implementation need not be. The existing interface is closed to modifications and new implementations must, at a minimum, implement that interface.</p>"}, {"location": "architecture/solid/#liskov-substitutionlsp", "title": "Liskov substitution(LSP)", "text": "<p>If S is a subtype of T, then objects of type T may be replaced with objects of type S without altering any of the desirable properties of the program.</p> <p>It imposes some standard requirements on signatures(the inputs and outputs for a function, subroutine or method):</p> <ul> <li>Contravariance     of method arguments in the subtype.</li> <li>Covariance     of return types in the subtype.</li> <li>No new exceptions should be thrown by methods of the subtype, except where     those exceptions are themselves subtypes of exception thrown by the methods     of the supertype.</li> </ul> <p>Additionally, the subtype must meet the following behavioural conditions that restrict how contracts can interact with the inheritance:</p> <ul> <li>Preconditions cannot be strengthened in a subtype.</li> <li>Postconditions cannot be     weakened in a subtype.</li> <li>Invariants     of the supertype must be preserved in a subtype.</li> <li>History constraint. Objects are regarded as being modifiable only through their methods. Because     subtypes may introduce methods that are not present in the supertype, the     introduction of these methods may allow state changes in the subtype that     are not permissible in the supertype. This is not allowed. Fields added to     the subtype may however be safely modified because they are not observable     through the supertype methods.</li> </ul>"}, {"location": "architecture/solid/#interface-segregation-isp", "title": "Interface segregation (ISP)", "text": "<p>No client should be forced to depend on methods it does not use. ISP splits interfaces that are very large into smaller and more specific ones so that clients will only have to know about the methods that are of interest to them. ISP is intended to keep a system decoupled and thus easier to refactor, change, and redeploy.</p> <p>For example, Xerox had created a new printer system that could perform a variety of tasks such as stapling and faxing. The software for this system was created from the ground up. As the software grew, making modifications became more and more difficult so that even the smallest change would take a redeployment cycle of an hour, which made development nearly impossible.</p> <p>The design problem was that a single Job class was used by almost all of the tasks. Whenever a print job or a stapling job needed to be performed, a call was made to the Job class. This resulted in a 'fat' class with multitudes of methods specific to a variety of different clients. Because of this design, a staple job would know about all the methods of the print job, even though there was no use for them.</p> <p>The solution suggested by Martin utilized what is today called the Interface Segregation Principle. Applied to the Xerox software, an interface layer between the Job class and its clients was added using the Dependency Inversion Principle. Instead of having one large Job class, a Staple Job interface or a Print Job interface was created that would be used by the Staple or Print classes, respectively, calling methods of the Job class. Therefore, one interface was created for each job type, which was all implemented by the Job class.</p>"}, {"location": "architecture/solid/#dependency-inversion", "title": "Dependency inversion", "text": "<p>Specific form of decoupling software modules where the conventional dependency relationships established from high-level, policy setting modules to low-level, dependency modules are reversed, thus rendering high-level modules independent of the low-level module implementation details.</p> <ul> <li> <p>High-level modules should not depend on low-level modules. Both should depend     on abstractions (e.g. interfaces).</p> <p>Depends on doesn't mean imports or calls, necessarily, but rather a more general idea that one module knows about or needs another module.</p> </li> <li> <p>Abstractions should not depend on details. Details (concrete implementations)     should depend on abstractions.</p> </li> </ul> <p>The idea behind points A and B of this principle is that when designing the interaction between a high-level module and a low-level one, the interaction should be thought of as an abstract interaction between them. This not only has implications on the design of the high-level module, but also on the low-level one: the low-level one should be designed with the interaction in mind and it may be necessary to change its usage interface. Thinking about the interaction in itself as an abstract concept allows the coupling of the components to be reduced without introducing additional coding patterns, allowing only a lighter and less implementation dependent interaction schema.</p>"}, {"location": "architecture/solid/#references", "title": "References", "text": "<ul> <li>SOLID Wikipedia article</li> <li>Architecture Patterns with     Python by     Harry J.W. Percival and Bob Gregory.</li> </ul>"}, {"location": "botany/trees/", "title": "Trees", "text": ""}, {"location": "botany/trees/#tree-description", "title": "Tree description", "text": ""}, {"location": "botany/trees/#alder", "title": "Alder", "text": "<p>Alders are trees comprising the genus Alnus in the birch family Betulaceae (like the birch). The genus parts are \"al\" which means \"close by\" and \"lan\" which means \"side of the river\", so they are trees that grow close to rivers or creeks.</p> <p>With a few exceptions, alders are deciduous, and the leaves are alternate, simple, and serrated. The flowers are catkins with elongate male catkins on the same plant as shorter female catkins, often before leaves appear; they are mainly wind-pollinated, but also visited by bees to a small extent. These trees differ from the birches (Betula, another genus in the family) in that the female catkins are woody and do not disintegrate at maturity, opening to release the seeds in a similar manner to many conifer cones.</p> <p>The largest species are red alder (A. rubra) on the west coast of North America, and black alder (A. glutinosa), native to most of Europe and widely introduced elsewhere, they are usually are between 17 and 22m with a diameter not greater than half a meter. Although in some areas of europe they rarely go over 10 or 12 meters. Exceptionally they have reached over 30 metres and 3 of diameter. By contrast, the widespread Alnus alnobetula (green alder) is rarely more than a 5 m-tall shrub.</p> <p> </p> <p>Their leaves are rounded, sparsely toothed, symmetric or slightly asymmetric at the base, green on both sides. They reach the 12cm long and have a short petiole. Even though they are deciduous, the leaves stay green until they fall. The black alder (alnus glutinosa) has sticky leaves.</p> <p>The bearing or appearance of the alder varies with the age. When it's young it has a regular canopy, pyramidal or pointed. Once it gets older the ramification is more irregular and divided making a dense canopy that gives a lot of shadow. It's not very long-lived, it usually lives around a hundred years.</p> <p>The trunk is cylindrical, right and quite clean. it's bark is always thin, in the young one is is smooth, shiny and greenish brown. The older ones is longitudinally cracked, scaled and of dark colour.</p> <p>Their roots have a peculiarity, they have some knots of different centimeters wide where some fungi associated to the tree live.</p> <p>Below there are some guides to differentiate it from the beech and the birch</p>"}, {"location": "botany/trees/#ash", "title": "Ash", "text": ""}, {"location": "botany/trees/#beech", "title": "Beech", "text": "<p>Beech (Fagus sylvatica) is a genus of deciduous trees in the family Fagaceae, native to temperate Europe, Asia, and North America. The better known Fagus subgenus beeches are high-branching with tall, stout trunks and smooth silver-grey bark. Fagus comes from the greek word \"phegos\" which means edible. \"Sylvatica\" refers to form forests.</p> <p>Beeches are monoecious, bearing both male and female flowers on the same plant. The small flowers are unisexual, the female flowers borne in pairs, the male flowers wind-pollinating catkins. They are produced in spring shortly after the new leaves appear. The fruit of the beech tree, known as beechnuts or mast, is found in small burrs that drop from the tree in autumn. They are small, roughly triangular and edible, with a bitter, astringent, or in some cases, mild and nut-like taste. They have a high enough fat content that they can be pressed for edible oil.</p> <p></p> <p>The leaves of beech trees are elliptic, a little pointy at the end, flat, and with a short petiole. They are big and wide leaves ranging from 4-9 cm long. Very abundant, they have a light green colour with a darker tone and glossy on the upper side.</p> <p>The fruit is a small, sharply three-angled nut 10-15 mm long, borne singly or in pairs in soft-spined husks 1.5-2.5 cm long, known as cupules. The husk can have a variety of spine- to scale-like appendages, the character of which is, in addition to leaf shape, one of the primary ways beeches are differentiated. The nuts are edible, though bitter (though not nearly as bitter as acorns) with a high tannin content, and are called beechnuts or beechmast.</p> <p>They are big trees easily going between 30 and 45 meters. It looks very different if its isolated or being part of a forest. The first one the branches grow from the middle of the trunk and are horizontal, in the second, the branches go up and start over the half of the trunk. The principal root is very powerful, with very strong secondary roots, developing lateral superficial roots.</p> <p></p> <p>The trunk is right with a grayish-cinder bark, smooth until it's old, usually covered by moss an lichen. Smaller branches are zigzagging with reddish-brown pointy buds.</p> <p>The canopy is big, dense, rounded and semi spheric, giving a lot of shadow.</p> <p>It grows slow in the first years, being the most active between the tenth and twelve year, reaching it's maximum height when it's twenty five, although it lives around three hundred years.</p> <p>Below there are some guides to differentiate it from the alder and the birch</p>"}, {"location": "botany/trees/#birch", "title": "Birch", "text": "<p>A birch is a thin-leaved deciduous hardwood tree of the genus Betula, in the family Betulaceae, which also includes alders, hazels, and hornbeams. It is closely related to the beech-oak family Fagaceae. They are a typically rather short-lived pioneer species.</p> <p>Birch species are generally small to medium-sized trees or shrubs, mostly of northern temperate and boreal climates. They usually grow together close to each other with heights between 10 and 15 meters.</p> <p>The simple leaves are rhomboidal, between 3 and 6 cm, singly or doubly serrate except at the base, feather-veined, petiolate and stipulate. Although they are alternate, many leaves spawn from each side of the branch, making some think that they are not alternate. They often appear in pairs, but these pairs are really borne on spur-like, two-leaved, lateral branchlets.</p> <p>The canopy is rounded and irregular giving few shadow.</p> <p>The fruit is a small samara, although the wings may be obscure in some species. They differ from the alders in that the female catkins are not woody and disintegrate at maturity, falling apart to release the seeds, unlike the woody, cone-like female alder catkins.</p> <p>The bark of all birches is characteristically smooth and white, although in older ones the lower part is usually cracked and takes blackish brown colours. It's marked with long, horizontal lenticels, and often separates into thin, papery plates, especially upon the paper birch.</p> <p>The buds form early and are full grown by midsummer, all are lateral, no terminal bud is formed; the branch is prolonged by the upper lateral bud. The wood of all the species is close-grained with a satiny texture and capable of taking a fine polish; its fuel value is fair.</p> <p>Below there are some guides to differentiate it from the alder and the birch</p>"}, {"location": "botany/trees/#how-to-tell-apart-the-different-trees", "title": "How to tell apart the different trees", "text": ""}, {"location": "botany/trees/#alder-vs-beech", "title": "Alder vs Beech", "text": "Property Beech Alder Leaf border flat sparsely toothed Leaf form elliptic rounded Same colour both sides no (darker and glossy up) yes Sticky leafs no yes Size 30-45m 10-12m (in Europe) Knots on the roots with fungi no yes Where they grow everywhere close to rivers or creeks"}, {"location": "botany/trees/#alder-vs-birch", "title": "Alder vs Birch", "text": "Property Birch Alder Leaf border heavy toothed sparsely toothed Leaf form rhomboidal rounded Sticky leafs no yes Where they grow very close to each other close to rivers or creeks"}, {"location": "botany/trees/#beech-vs-birch", "title": "Beech vs Birch", "text": "Property Beech Birch Leaf border flat heavy toothed Leaf form elliptic rhomboidal Size 30-45m 10-15m (in Europe) Same colour both sides no (darker and glossy up) yes Where they grow everywhere very close to each other"}, {"location": "coding/tdd/", "title": "TDD", "text": "<p>Test-driven development (TDD) is a software development process that relies on the repetition of a very short development cycle: requirements are turned into very specific test cases, then the code is improved so that the tests pass. This is opposed to software development that allows code to be added that is not proven to meet requirements.</p>"}, {"location": "coding/tdd/#abstractions-in-testing", "title": "Abstractions in testing", "text": "<p>Writing tests that couple our high-level code with low-level details will make your life hard, because as the scenarios we consider get more complex, our tests will get more unwieldy.</p> <p>To avoid it, abstract the low-level code from the high-level one, unit test it and edge-to-edge test the high-level code.</p> <p>Edge-to-edge testing involves writing end-to-end tests, substituting the low level code for fakes that behave in the same way. The advantage of this approach is that our tests act on the exact same function that's used by our production code. The disadvantage is that we have to make our stateful components explicit and pass them around.</p>"}, {"location": "coding/tdd/#fakes-vs-mocks", "title": "Fakes vs Mocks", "text": "<p>Mocks are used to verify how something gets used. Fakes are working implementations of the things they're replacing, but they're designed for use only in tests. They wouldn't work in the real life but they can be used to make assertions about the end state of a system rather than the behaviours along the way.</p> <p>Using fakes instead of mocks have these advantages:</p> <ul> <li>Overuse of mocks leads to complicated test suites that fail to explain the     code</li> <li>Patching out the dependency you're using makes it possible to unit test the     code, but it does nothing to improve the design. Faking makes you identify     the responsibilities of your codebase, and to separate those     responsibilities into small, focused objects that are easy to replace.</li> <li>Tests that use mocks tend to be more coupled to the implementation details     of the codebase. That's because mock tests verify the interactions between     things. This coupling between code and test tends to make tests more     brittle.</li> </ul> <p>Using the right abstractions is tricky, but here are a few questions that may help you:</p> <ul> <li>Can I choose a familiar Python data structure to represent the state of the     messy system and then try to imagine a single function that can return that     state?</li> <li>Where can I draw a line between my systems, where can I carve out a seam to     stick that abstraction in?</li> <li>What is a sensible way of dividing things into components with different     responsibilities? What implicit concepts can I make explicit?</li> <li>What are the dependencies, and what is the core business logic?</li> </ul>"}, {"location": "coding/tdd/#tdd-in-high-gear-and-low-gear", "title": "TDD in High Gear and Low Gear", "text": "<p>Tests are supposed to help us change our system fearlessly, but often we write too many tests against the domain model. This causes problems when we want to change our codebase and find that we need to update tens or even hundreds of unit tests.</p> <p>Every line of code that we put in a test is like a blob of glue, holding the system in a particular shape. The more low-level tests we have, the harder it will be to change things.</p> <p>Tests can be written at the different levels of abstraction, high level tests gives us low feedback, low barrier to change and a high system coverage, while low level tests gives us high feedback, high barrier to change and focused coverage.</p> <p>A test for an HTTP API tells us nothing about the fine grained design of our objects, because it sits at a much higher level of abstraction. On the other hand, we can rewrite our entire application and, so long as we don't change the URLs or request formats, our HTTP tests will continue to pass. This gives us confidence that large-scale changes, like changing the database schema, haven't broken our code.</p> <p>At the other end of the spectrum, tests in the domain model  help us to understand the objects we need. These tests guide us to a design that makes sense and reads in the domain language. When our tests read in the domain language, we feel comfortable that our code matches our intuition about the problem we're trying to solve.</p> <p>We often sketch new behaviours by writing tests at this level to see how the code might look. When we want to improve the design of the code, though, we will need to replace or delete these tests, because they are tightly coupled to a particular implementation.</p> <p>Most of the time, when we are adding a new feature or fixing a bug, we don't need to make extensive changes to the domain model. In these cases, we prefer to write tests against services because of the lower coupling and higher coverage.</p> <p>When starting a new project or when hitting a particularly difficult problem, we will drop back down to writing tests against the domain model so we get better feedback and executable documentation of our intent.</p> <p>Note</p> <p>When starting a journey, the bicycle needs to be in a low gear so that it can overcome inertia. Once we're off and running, we can go faster and more efficiently by changing into a high gear; but if we suddenly encounter a steep hill or are forced to slow down by a hazard, we again drop down to a low gear until we can pick up speed again.</p>"}, {"location": "coding/tdd/#references", "title": "References", "text": "<ul> <li>Architecture Patterns with     Python by     Harry J.W. Percival and Bob Gregory.</li> </ul>"}, {"location": "coding/tdd/#further-reading", "title": "Further reading", "text": "<ul> <li>Martin Fowler o Mocks aren't stubs</li> </ul>"}, {"location": "coding/javascript/javascript/", "title": "Javascript", "text": "<p>JavaScript is a multi-paradigm, dynamic language with types and operators, standard built-in objects, and methods. Its syntax is based on the Java and C languages \u2014 many structures from those languages apply to JavaScript as well. JavaScript supports object-oriented programming with object prototypes, instead of classes. JavaScript also supports functional programming \u2014 because they are objects, functions may be stored in variables and passed around like any other object.</p>"}, {"location": "coding/javascript/javascript/#the-basics", "title": "The basics", "text": ""}, {"location": "coding/javascript/javascript/#javascript-types", "title": "Javascript types", "text": "<p>JavaScript's types are:</p> <ul> <li>Number</li> <li>String</li> <li>Boolean</li> <li>Symbol (new in ES2015)</li> <li>Object<ul> <li>Function</li> <li>Array</li> <li>Date</li> <li>RegExp</li> </ul> </li> <li>null</li> <li>undefined</li> </ul>"}, {"location": "coding/javascript/javascript/#numbers", "title": "Numbers", "text": "<p>Numbers in JavaScript are double-precision 64-bit format IEEE 754 values. There's no such thing as an integer in JavaScript, so you have to be a little careful with your arithmetic.</p> <p>The standard arithmetic operators are supported, including addition, subtraction, modulus (or remainder) arithmetic, and so forth. Use the Math object when in need of more advanced mathematical functions and constants.</p> <p>It supports <code>NaN</code> for Not a Number which can be tested with <code>isNaN()</code> and <code>Infinity</code> which can be tested with <code>isFinite()</code>.</p> <p>JavaScript distinguishes between <code>null</code> and <code>undefined</code>, which indicates an uninitialized variable.</p>"}, {"location": "coding/javascript/javascript/#convert-a-string-to-an-integer", "title": "Convert a string to an integer", "text": "<p>Use the built-in <code>parseInt()</code> function. It takes the base for the conversion as an optional but recommended second argument.</p> <pre><code>parseInt('123', 10); // 123\nparseInt('010', 10); // 10\n</code></pre>"}, {"location": "coding/javascript/javascript/#convert-a-string-into-a-float", "title": "Convert a string into a float", "text": "<p>Use the built-in <code>parseFloat()</code> function. Unlike <code>parseInt()</code> , parseFloat() always uses base 10.</p>"}, {"location": "coding/javascript/javascript/#strings", "title": "Strings", "text": "<p>Strings in JavaScript are sequences of Unicode characters (UTF-16) which support several methods.</p>"}, {"location": "coding/javascript/javascript/#find-the-length-of-a-string", "title": "Find the length of a string", "text": "<pre><code>'hello'.length; // 5\n</code></pre>"}, {"location": "coding/javascript/javascript/#booleans", "title": "Booleans", "text": "<p>JavaScript has a boolean type, with possible values <code>true</code> and <code>false</code>. Any value will be converted when necessary to a boolean according to the following rules:</p> <ul> <li><code>false</code>, <code>0</code>, empty strings (<code>\"\"</code>), <code>NaN</code>, <code>null</code>, and <code>undefined</code> all become     <code>false</code>.</li> <li>All other values become <code>true</code>.</li> </ul> <p>Boolean operations are also supported:</p> <ul> <li>and: <code>&amp;&amp;</code></li> <li>or: <code>||</code></li> <li>not: <code>!</code></li> </ul>"}, {"location": "coding/javascript/javascript/#variables", "title": "Variables", "text": "<p>New variables in JavaScript are declared using one of three keywords: <code>let</code>, <code>const</code>, or <code>var</code>.</p> <ul> <li> <p><code>let</code> is used to declare block-level variables.</p> <pre><code>let a;\nlet name = 'Simon';\n</code></pre> <p>The declared variable is available from the block it is enclosed in.</p> <pre><code>// myLetVariable is *not* visible out here\n\nfor (let myLetVariable = 0; myLetVariable &lt; 5; myLetVariable++) {\n// myLetVariable is only visible in here\n}\n\n// myLetVariable is *not* visible out here\n</code></pre> </li> <li> <p><code>const</code> is used to declare variables whose values are never intended to     change. The variable is available from the block it is declared in.</p> <p><pre><code>const Pi = 3.14; // variable Pi is set\nPi = 1; // will throw an error because you cannot change a constant variable.\n</code></pre> * <code>var</code> is the most common declarative keyword. It does not have the restrictions that the other two keywords have. If you declare a variable without assigning any value to it, its type is undefined.</p> <pre><code>// myVarVariable *is* visible out here\n\nfor (var myVarVariable = 0; myVarVariable &lt; 5; myVarVariable++) {\n// myVarVariable is visible to the whole function\n}\n\n// myVarVariable *is* visible out here\n</code></pre> </li> </ul>"}, {"location": "coding/javascript/javascript/#operators", "title": "Operators", "text": "<p>Numeric operators:</p> <ul> <li><code>+</code>, both for numbers and strings.</li> <li><code>-</code></li> <li><code>*</code></li> <li><code>/</code></li> <li><code>%</code>, which is the remainder operator.</li> <li><code>=</code>, to assign values.</li> <li><code>+=</code></li> <li><code>-=</code></li> <li><code>++</code></li> <li><code>--</code></li> </ul> <p>Comparison operators:</p> <ul> <li><code>&lt;</code></li> <li><code>&gt;</code></li> <li><code>&lt;=</code></li> <li><code>&gt;=</code></li> <li> <p><code>==</code>, performs type coercion if you give it different types, with sometimes     interesting results</p> <pre><code>123 == '123'; // true\n1 == true; // true\n</code></pre> <p>To avoid type coercion, use the triple-equals operator:</p> <p><pre><code>123 === '123'; // false\n1 === true;    // false\n</code></pre> * <code>!=</code> and <code>!==</code>.</p> </li> </ul>"}, {"location": "coding/javascript/javascript/#control-structures", "title": "Control structures", "text": ""}, {"location": "coding/javascript/javascript/#if-conditionals", "title": "If conditionals", "text": "<pre><code>var name = 'kittens';\nif (name == 'puppies') {\nname += ' woof';\n} else if (name == 'kittens') {\nname += ' meow';\n} else {\nname += '!';\n}\nname == 'kittens meow';\n</code></pre> <p>You can use the conditional ternary operator instead.</p> <p>It's defined by a condition followed by a question mark <code>?</code>, then an expression to execute if the condition is truthy followed by a colon <code>:</code>, and finally the expression to execute if the condition is falsy.</p> <p><code>condition ? exprIfTrue : exprIfFalse</code></p> <pre><code>function getFee(isMember) {\nreturn (isMember ? '$2.00' : '$10.00');\n}\n\nconsole.log(getFee(true));\n// expected output: \"$2.00\"\n\nconsole.log(getFee(false));\n// expected output: \"$10.00\"\n\nconsole.log(getFee(null));\n// expected output: \"$10.00\"\n</code></pre>"}, {"location": "coding/javascript/javascript/#switch-cases", "title": "Switch cases", "text": "<p><pre><code>switch (action) {\ncase 'draw':\ndrawIt();\nbreak;\ncase 'eat':\neatIt();\nbreak;\ndefault:\ndoNothing();\n}\n</code></pre> If you don't add a <code>break</code> statement, execution will \"fall through\" to the next level.  The default clause is optional</p>"}, {"location": "coding/javascript/javascript/#while-loops", "title": "While loops", "text": "<pre><code>while (true) {\n// an infinite loop!\n}\n\nvar input;\ndo {\ninput = get_input();\n} while (inputIsNotValid(input));\n</code></pre>"}, {"location": "coding/javascript/javascript/#for-loops", "title": "For loops", "text": "<p>It has several types of for loops:</p> <ul> <li> <p>Classic <code>for</code>:</p> <pre><code>for (var i = 0; i &lt; 5; i++) {\n// Will execute 5 times\n}\n</code></pre> </li> <li> <p><code>for</code>...<code>of</code>.     <pre><code>for (let value of array) {\n// do something with value\n}\n</code></pre></p> </li> <li> <p><code>for</code>...<code>in</code>.     <pre><code>for (let property in object) {\n// do something with object property\n}\n</code></pre></p> </li> </ul>"}, {"location": "coding/javascript/javascript/#objects", "title": "Objects", "text": "<p>Objects can be thought of as simple collections of name-value pairs, such as Python dictionaries.</p> <pre><code>var obj2 = {};\nvar obj = {\nname: 'Carrot',\nfor: 'Max', // 'for' is a reserved word, use '_for' instead.\ndetails: {\ncolor: 'orange',\nsize: 12\n}\n};\n</code></pre> <p>Attribute access can be chained together:</p> <pre><code>obj.details.color; // orange\nobj['details']['size']; // 12\n</code></pre> <p>The following example creates an object prototype(<code>Person</code>) and an instance of that prototype(<code>you</code>).</p> <pre><code>function Person(name, age) {\nthis.name = name;\nthis.age = age;\n}\n\n// Define an object\nvar you = new Person('You', 24);\n// We are creating a new person named \"You\" aged 24.\n</code></pre>"}, {"location": "coding/javascript/javascript/#the-this-keyword", "title": "The <code>this</code> keyword", "text": "<p>The <code>this</code> keyword refers to different objects depending on how it is used:</p> <ul> <li> <p>In an object method, <code>this</code> refers to the object.     <pre><code>const person = {\nfirstName: \"John\",\nlastName : \"Doe\",\nid       : 5566,\nfullName : function() {\nreturn this.firstName + \" \" + this.lastName;\n}\n};\n</code></pre></p> </li> <li> <p>Alone, <code>this</code> refers to the global object. In a browser window the global     object is <code>[object Window]</code>.</p> <pre><code>let x = this;\n</code></pre> </li> <li> <p>In a function, <code>this</code> refers to the global object.</p> </li> <li>In a function, in strict mode, <code>this</code> is undefined.</li> <li> <p>In an event, <code>this</code> refers to the element that received the event.</p> <pre><code>&lt;button onclick=\"this.style.display='none'\"&gt;\nClick to Remove Me!\n&lt;/button&gt;\n</code></pre> </li> <li> <p>Methods like <code>call()</code>, <code>apply()</code>, and <code>bind()</code> can refer this to any object.</p> </li> </ul>"}, {"location": "coding/javascript/javascript/#arrays", "title": "Arrays", "text": "<p>Arrays can be thought of as Python lists. They work very much like regular objects but with their own properties and methods, such as <code>length</code>, which returns one more than the highest index in the array.</p> <pre><code>var a = new Array();\na[0] = 'dog';\na[1] = 'cat';\na[2] = 'hen';\n// or\n\nvar a = ['dog', 'cat', 'hen'];\n\na.length; // 3\n</code></pre>"}, {"location": "coding/javascript/javascript/#iterate-over-the-values-of-an-array", "title": "Iterate over the values of an array", "text": "<pre><code>for (const currentValue of a) {\n// Do something with currentValue\n}\n\n// or\n\nfor (var i = 0; i &lt; a.length; i++) {\n// Do something with a[i]\n}\n</code></pre>"}, {"location": "coding/javascript/javascript/#append-an-item-to-an-array", "title": "Append an item to an array", "text": "<p>If you want to alter the original array use <code>push()</code> although, it's better to use <code>concat()</code> as it doesn't mutate the original array.</p> <pre><code>a.concat(item);\n</code></pre>"}, {"location": "coding/javascript/javascript/#apply-a-function-to-the-elements-of-an-array", "title": "Apply a function to the elements of an array", "text": "<pre><code>const numbers = [1, 2, 3];\nconst doubled = numbers.map(x =&gt; x * 2); // [2, 4, 6]\n</code></pre>"}, {"location": "coding/javascript/javascript/#filter-the-contents-of-an-array", "title": "Filter the contents of an array", "text": "<p>The <code>filter()</code> method creates a new array filled with elements that pass a test provided by a function.</p> <p>The <code>filter()</code> method does not execute the function for empty elements.</p> <p>The <code>filter()</code> method does not change the original array.</p> <p>For example:</p> <pre><code>const ages = [32, 33, 16, 40];\nconst result = ages.filter(checkAdult);\n\nfunction checkAdult(age) {\nreturn age &gt;= 18;\n}\n</code></pre>"}, {"location": "coding/javascript/javascript/#array-useful-methods", "title": "Array useful methods", "text": "<p>TBC</p>"}, {"location": "coding/javascript/javascript/#functions", "title": "Functions", "text": "<pre><code>function add(x, y) {\nvar total = x + y;\nreturn total;\n}\n</code></pre> <p>Functions have an <code>arguments</code> array holding all of the values passed to the function.</p> <p>To save typing and avoid the confusing behavior of this,it is recommended to use the arrow function syntax for event handlers.</p> <p>So instead of</p> <pre><code>&lt;button className=\"square\" onClick={function() { alert('click'); }}&gt;\n</code></pre> <p>It's better to use</p> <p><pre><code>&lt;button className=\"square\" onClick={() =&gt; alert('click')}&gt;\n</code></pre> Notice how with <code>onClick={() =&gt; alert('click')}</code>, the function is passed as the <code>onClick</code> prop.</p> <p>Another example, from this code:</p> <pre><code>hello = function() {\nreturn \"Hello World!\";\n}\n</code></pre> <p>You get:</p> <pre><code>hello = () =&gt; \"Hello World!\";\n</code></pre> <p>If you have parameters, you pass them inside the parentheses:</p> <pre><code>hello = (val) =&gt; \"Hello \" + val;\n</code></pre>"}, {"location": "coding/javascript/javascript/#define-variable-number-of-arguments", "title": "Define variable number of arguments", "text": "<pre><code>function avg(...args) {\nvar sum = 0;\nfor (let value of args) {\nsum += value;\n}\nreturn sum / args.length;\n}\n\navg(2, 3, 4, 5); // 3.5\n</code></pre>"}, {"location": "coding/javascript/javascript/#function-callbacks", "title": "Function callbacks", "text": "<p>A callback is a function passed as an argument to another function.</p> <p>Using a callback, you could call the calculator function <code>myCalculator</code> with a callback, and let the calculator function run the callback after the calculation is finished:</p> <pre><code>function myDisplayer(some) {\ndocument.getElementById(\"demo\").innerHTML = some;\n}\n\nfunction myCalculator(num1, num2, myCallback) {\nlet sum = num1 + num2;\nmyCallback(sum);\n}\n\nmyCalculator(5, 5, myDisplayer);\n</code></pre>"}, {"location": "coding/javascript/javascript/#custom-objects", "title": "Custom objects", "text": "<p>JavaScript is a prototype-based language that contains no class statement. Instead, JavaScript uses functions as classes.</p> <pre><code>function makePerson(first, last) {\nreturn {\nfirst: first,\nlast: last,\nfullName: function() {\nreturn this.first + ' ' + this.last;\n},\nfullNameReversed: function() {\nreturn this.last + ', ' + this.first;\n}\n};\n}\nvar s = makePerson('Simon', 'Willison');\ns.fullName(); // \"Simon Willison\"\ns.fullNameReversed(); // \"Willison, Simon\"\n</code></pre> <p>Used inside a function, <code>this</code> refers to the current object. If you called it using dot notation or bracket notation on an object, that object becomes <code>this</code>. If dot notation wasn't used for the call, <code>this</code> refers to the global object.</p> <p>Which makes <code>this</code> is a frequent cause of mistakes. For example:</p> <pre><code>var s = makePerson('Simon', 'Willison');\nvar fullName = s.fullName;\nfullName(); // undefined undefined\n</code></pre> <p>When calling <code>fullName()</code> alone, without using <code>s.fullName()</code>, this is bound to the global object. Since there are no global variables called <code>first</code> or <code>last</code> we get <code>undefined</code> for each one.</p>"}, {"location": "coding/javascript/javascript/#constructor-functions", "title": "Constructor functions", "text": "<p>We can take advantage of the <code>this</code> keyword to improve the <code>makePerson</code> function:</p> <pre><code>function Person(first, last) {\nthis.first = first;\nthis.last = last;\nthis.fullName = function() {\nreturn this.first + ' ' + this.last;\n};\nthis.fullNameReversed = function() {\nreturn this.last + ', ' + this.first;\n};\n}\nvar s = new Person('Simon', 'Willison');\n</code></pre> <p><code>new</code> is strongly related to <code>this</code>. It creates a brand new empty object, and then calls the function specified, with <code>this</code> set to that new object. Notice though that the function specified with this does not return a value but merely modifies the <code>this</code> object. It's <code>new</code> that returns the <code>this</code> object to the calling site. Functions that are designed to be called by <code>new</code> are called constructor functions. Common practice is to capitalize these functions as a reminder to call them with <code>new</code>.</p> <p>Every time we create a person object we are creating two brand new function objects within it, to avoid it, use shared functions.</p> <p><pre><code>function Person(first, last) {\nthis.first = first;\nthis.last = last;\n}\nPerson.prototype.fullName = function() {\nreturn this.first + ' ' + this.last;\n};\nPerson.prototype.fullNameReversed = function() {\nreturn this.last + ', ' + this.first;\n};\n</code></pre> <code>Person.prototype</code> is an object shared by all instances of <code>Person</code>. any time you attempt to access a property of <code>Person</code> that isn't set, JavaScript will check <code>Person.prototype</code> to see if that property exists there instead. As a result, anything assigned to <code>Person.prototype</code> becomes available to all instances of that constructor via the <code>this</code> object. So it's easy to add extra methods to existing objects at runtime:</p> <pre><code>var s = new Person('Simon', 'Willison');\ns.firstNameCaps(); // TypeError on line 1: s.firstNameCaps is not a function\n\nPerson.prototype.firstNameCaps = function() {\nreturn this.first.toUpperCase();\n};\ns.firstNameCaps(); // \"SIMON\"\n</code></pre>"}, {"location": "coding/javascript/javascript/#split-code-for-readability", "title": "Split code for readability", "text": "<p>To split a line into several, parentheses may be used to avoid the insertion of semicolons.</p> <pre><code>  renderSquare(i) {\nreturn (\n&lt;Square\nvalue={this.state.squares[i]}\nonClick={() =&gt; this.handleClick(i)}\n/&gt;\n);\n}\n</code></pre>"}, {"location": "coding/javascript/javascript/#coalescent-operator", "title": "Coalescent operator", "text": "<p>Is similar to the Logical <code>OR</code> operator (<code>||</code>), except instead of relying on truthy/falsy values, it relies on \"nullish\" values (there are only 2 nullish values, <code>null</code> and <code>undefined</code>).</p> <p>This means it's safer to use when you treat falsy values like <code>0</code> as valid.</p> <p>Similar to Logical <code>OR</code>, it functions as a control-flow operator; it evaluates to the first not-nullish value.</p> <p>It was introduced in Chrome 80 / Firefox 72 / Safari 13.1. It has no IE support.</p> <pre><code>console.log(4 ?? 5);\n// 4, since neither value is nullish\nconsole.log(null ?? 10);\n// 10, since 'null' is nullish\nconsole.log(undefined ?? 0);\n// 0, since 'undefined' is nullish\n// Here's a case where it differs from\n// Logical OR (||):\nconsole.log(0 ?? 5); // 0\nconsole.log(0 || 5); // 5\n</code></pre>"}, {"location": "coding/javascript/javascript/#interacting-with-html", "title": "Interacting with HTML", "text": "<p>You can find HTML elements with the next <code>document</code> properties:</p> Property Description <code>document.anchors</code> Returns all <code>&lt;a&gt;</code> elements that have a name attribute <code>document.baseURI</code> Returns the absolute base URI of the document <code>document.body</code> Returns the <code>&lt;body&gt;</code> element <code>document.cookie</code> Returns the document's cookie <code>document.doctype</code> Returns the document's doctype <code>document.documentElement</code> Returns the <code>&lt;html&gt;</code> element <code>document.documentMode</code> Returns the mode used by the browser <code>document.documentURI</code> Returns the URI of the document <code>document.domain</code> Returns the domain name of the document server <code>document.embeds</code> Returns all <code>&lt;embed&gt;</code> elements <code>document.forms</code> Returns all <code>&lt;form&gt;</code> elements <code>document.head</code> Returns the <code>&lt;head&gt;</code> element <code>document.images</code> Returns all <code>&lt;img&gt;</code> elements <code>document.implementation</code> Returns the DOM implementation <code>document.inputEncoding</code> Returns the document's encoding (character set) <code>document.lastModified</code> Returns the date and time the document was updated <code>document.links</code> Returns all <code>&lt;area&gt;</code> and <code>&lt;a&gt;</code> elements that have a href attribute <code>document.readyState</code> Returns the (loading) status of the document <code>document.referrer</code> Returns the URI of the referrer (the linking document) <code>document.scripts</code> Returns all <code>&lt;script&gt;</code> elements <code>document.strictErrorChecking</code> Returns if error checking is enforced <code>document.title</code> Returns the <code>&lt;title&gt;</code> element <code>document.URL</code> Returns the complete URL of the document"}, {"location": "coding/javascript/javascript/#how-to-add-javascript-to-html", "title": "How to add JavaScript to HTML", "text": "<p>In HTML, JavaScript code is inserted between <code>&lt;script&gt;</code> and <code>&lt;/script&gt;</code> tags.</p> <pre><code> &lt;script&gt;\ndocument.getElementById(\"demo\").innerHTML = \"My First JavaScript\";\n&lt;/script&gt;\n</code></pre> <p>That will be run as the page is loaded.</p> <p>Scripts can be placed in the <code>&lt;body&gt;</code>, or in the <code>&lt;head&gt;</code> section of an HTML page, or in both.</p> <p>Note<p>\"Placing scripts at the bottom of the <code>&lt;body&gt;</code> element improves the display speed, because script interpretation slows down the display.\"</p> </p> <p>A JavaScript <code>function</code> is a block of JavaScript code, that can be executed when \"called\" for.</p> <p>For example, a function can be called when an event occurs, like when the user clicks a button.</p>"}, {"location": "coding/javascript/javascript/#external-javascript", "title": "External JavaScript", "text": "<p>Scripts can also be placed in external files:</p> <p>File: myScript.js</p> <pre><code>function myFunction() {\ndocument.getElementById(\"demo\").innerHTML = \"Paragraph changed.\";\n}\n</code></pre> <p>External scripts are practical when the same code is used in many different web pages.</p> <p>To use an external script, put the name of the script file in the <code>src</code> (source) attribute of a <code>&lt;script&gt;</code> tag:</p> <pre><code>&lt;script src=\"myScript.js\"&gt;&lt;/script&gt;\n</code></pre> <p>Placing scripts in external files has some advantages:</p> <ul> <li>It separates HTML and code.</li> <li>It makes HTML and JavaScript easier to read and maintain.</li> <li>Cached JavaScript files can speed up page loads.</li> </ul>"}, {"location": "coding/javascript/javascript/#html-content", "title": "HTML content", "text": "<p>One of many JavaScript HTML methods is <code>getElementById()</code>.</p> <p>The example below \"finds\" an HTML element (with <code>id=\"demo\"</code>), and changes the element content (<code>innerHTML</code>) to <code>\"Hello JavaScript\"</code>:</p> <pre><code>document.getElementById(\"demo\").innerHTML = \"Hello JavaScript\";\n</code></pre> <p>It will transform:</p> <pre><code>&lt;p id=\"demo\"&gt;JavaScript can change HTML content.&lt;/p&gt;\n</code></pre> <p>Into:</p> <pre><code>&lt;p id=\"demo\"&gt;Hello JavaScript&lt;/p&gt;\n</code></pre> <p>You can also use <code>getElementsByTagName</code> or <code>getElementsByClassName</code>. Or if you want to find all HTML elements that match a specified CSS selector (id, class names, types, attributes, values of attributes, etc), use the <code>querySelectorAll()</code> method.</p> <p>This example returns a list of all <code>&lt;p&gt;</code> elements with <code>class=\"intro\"</code>.</p> <pre><code>const x = document.querySelectorAll(\"p.intro\");\n</code></pre>"}, {"location": "coding/javascript/javascript/#html-attributes", "title": "HTML attributes", "text": "<p>JavaScript can also change HTML attribute values. In this example JavaScript changes the value of the <code>src</code> (source) attribute of an <code>&lt;img&gt;</code> tag:</p> <pre><code>&lt;button onclick=\"document.getElementById('myImage').src='pic_bulbon.gif'\"&gt;Turn on the light&lt;/button&gt;\n\n&lt;img id=\"myImage\" src=\"pic_bulboff.gif\" style=\"width:100px\"&gt;\n\n&lt;button onclick=\"document.getElementById('myImage').src='pic_bulboff.gif'\"&gt;Turn off the light&lt;/button&gt;\n</code></pre> <p>Other attribute methods are:</p> Method Description <code>document.createElement(element)</code> Create an HTML element <code>document.removeChild(element)</code> Remove an HTML element <code>document.appendChild(element)</code> Add an HTML element <code>document.replaceChild(new, old)</code> Replace an HTML element"}, {"location": "coding/javascript/javascript/#css", "title": "CSS", "text": "<p>Changing the style of an HTML element, is a variant of changing an HTML attribute:</p> <pre><code>document.getElementById(\"demo\").style.fontSize = \"35px\";\n</code></pre>"}, {"location": "coding/javascript/javascript/#hiding-or-showing-html-elements", "title": "Hiding or showing HTML elements", "text": "<p>Hiding HTML elements can be done by changing the display style:</p> <pre><code>document.getElementById(\"demo\").style.display = \"none\";\n</code></pre> <p>Showing hidden HTML elements can also be done by changing the display style:</p> <pre><code>document.getElementById(\"demo\").style.display = \"block\";\n</code></pre>"}, {"location": "coding/javascript/javascript/#displaying-data", "title": "Displaying data", "text": "<p>JavaScript can \"display\" data in different ways:</p> <ul> <li>Writing into an HTML element, using <code>innerHTML</code>.</li> <li> <p>Writing into the HTML output using <code>document.write()</code>.     <pre><code>&lt;h1&gt;My First Web Page&lt;/h1&gt;\n&lt;p&gt;My first paragraph.&lt;/p&gt;\n\n&lt;script&gt;\ndocument.write(5 + 6);\n&lt;/script&gt;\n</code></pre></p> <p>Using <code>document.write()</code> after an HTML document is loaded, will delete all existing HTML.</p> </li> <li> <p>Writing into an alert box, using <code>window.alert()</code>.</p> </li> <li>Writing into the browser console, using <code>console.log()</code>. Useful for debugging.</li> </ul>"}, {"location": "coding/javascript/javascript/#events", "title": "Events", "text": "<p>An HTML event can be something the browser does, or something a user does.</p> <p>Here are some examples of HTML events:</p> <ul> <li>An HTML web page has finished loading</li> <li>An HTML input field was changed</li> <li>An HTML button was clicked</li> </ul> <p>Often, when events happen, you may want to do something.</p> <p>JavaScript lets you execute code when events are detected.</p> <p>HTML allows event handler attributes, with JavaScript code, to be added to HTML elements.</p> <pre><code>&lt;element event='some JavaScript'&gt;\n</code></pre> <p>In the following example, an <code>onclick</code> attribute (with code), is added to a <code>&lt;button&gt;</code> element:</p> <pre><code>&lt;button onclick=\"document.getElementById('demo').innerHTML = Date()\"&gt;The time is?&lt;/button&gt;\n</code></pre> <p>In the example above, the JavaScript code changes the content of the element with <code>id=\"demo\"</code>.</p> <p>In the next example, the code changes the content of its own element (using <code>this.innerHTML</code>):</p> <pre><code>&lt;button onclick=\"this.innerHTML = Date()\"&gt;The time is?&lt;/button&gt;\n</code></pre> <p>JavaScript code is often several lines long. It is more common to see event attributes calling functions:</p> <pre><code>&lt;button onclick=\"displayDate()\"&gt;The time is?&lt;/button&gt;\n</code></pre>"}, {"location": "coding/javascript/javascript/#common-html-events", "title": "Common HTML Events", "text": "<p>Here is a list of some common HTML events:</p> Event Description onchange An HTML element has been changed onclick The user clicks an HTML element onmouseover The user moves the mouse over an HTML element onmouseout The user moves the mouse away from an HTML element onmousedown The user is pressing the click button onmouseup The user is releasing the click button onkeydown The user pushes a keyboard key onload The browser has finished loading the page"}, {"location": "coding/javascript/javascript/#event-handlers", "title": "Event handlers", "text": "<p>Event handlers can be used to handle and verify user input, user actions, and browser actions:</p> <ul> <li>Things that should be done every time a page loads</li> <li>Things that should be done when the page is closed</li> <li>Action that should be performed when a user clicks a button</li> <li>Content that should be verified when a user inputs data</li> </ul> <p>Many different methods can be used to let JavaScript work with events:</p> <ul> <li>HTML event attributes can execute JavaScript code directly</li> <li>HTML event attributes can call JavaScript functions</li> <li>You can assign your own event handler functions to HTML elements</li> <li>You can prevent events from being sent or being handled</li> </ul>"}, {"location": "coding/javascript/javascript/#json-support", "title": "JSON support", "text": "<p>JSON is a format for storing and transporting data.</p> <p>A common use of JSON is to read data from a web server, and display the data in a web page.</p> <p>For simplicity, this can be demonstrated using a string as input.</p> <p>First, create a JavaScript string containing JSON syntax:</p> <pre><code>let text = '{ \"employees\" : [' +\n'{ \"firstName\":\"John\" , \"lastName\":\"Doe\" },' +\n'{ \"firstName\":\"Anna\" , \"lastName\":\"Smith\" },' +\n'{ \"firstName\":\"Peter\" , \"lastName\":\"Jones\" } ]}';\n</code></pre> <p>Then, use the JavaScript built-in function <code>JSON.parse()</code> to convert the string into a JavaScript object:</p> <pre><code>const obj = JSON.parse(text);\n</code></pre> <p>Finally, use the new JavaScript object in your page:</p> <pre><code>&lt;p id=\"demo\"&gt;&lt;/p&gt;\n\n&lt;script&gt;\ndocument.getElementById(\"demo\").innerHTML =\nobj.employees[1].firstName + \" \" + obj.employees[1].lastName;\n&lt;/script&gt;\n</code></pre>"}, {"location": "coding/javascript/javascript/#async-javascript", "title": "Async JavaScript", "text": ""}, {"location": "coding/javascript/javascript/#timing-events", "title": "Timing events", "text": "<p>The <code>window</code> object allows execution of code at specified time intervals.</p> <p>These time intervals are called timing events.</p> <p>The two key methods to use with JavaScript are:</p> <ul> <li><code>setTimeout(function, milliseconds)</code>: Executes a function, after waiting     a specified number of <code>milliseconds</code>.</li> <li><code>setInterval(function, milliseconds)</code>: Same as <code>setTimeout()</code>, but repeats the     execution of the function continuously.</li> </ul> <p>For example to click a button. Wait 3 seconds, and the page will alert \"Hello\":</p> <pre><code> &lt;button onclick=\"setTimeout(myFunction, 3000)\"&gt;Try it&lt;/button&gt;\n\n&lt;script&gt;\nfunction myFunction() {\nalert('Hello');\n}\n&lt;/script&gt;\n</code></pre> <p>The <code>window.clearTimeout(timeoutVariable)</code> method stops the execution of the function specified in <code>setTimeout()</code>.</p> <pre><code>myVar = setTimeout(function, milliseconds);\nclearTimeout(myVar);\n</code></pre> <p>To stop a <code>setInterval</code> use the <code>clearInterval</code> method.</p>"}, {"location": "coding/javascript/javascript/#promises", "title": "Promises", "text": "<p>A JavaScript Promise object contains both the producing code and calls to the consuming code, where:</p> <ul> <li>\"Producing code\" is code that can take some time</li> <li>\"Consuming code\" is code that must wait for the result</li> </ul> <p>The syntax of a Promise is kind of difficult to understand, but bear with me:</p> <pre><code>let myPromise = new Promise(function(myResolve, myReject) {\n// \"Producing Code\" (May take some time)\n\nmyResolve(); // when successful\nmyReject();  // when error\n});\n\n// \"Consuming Code\" (Must wait for a fulfilled Promise)\nmyPromise.then(\nfunction(value) { /* code if successful */ },\nfunction(error) { /* code if some error */ }\n);\n</code></pre> <p>When the producing code obtains the result, it should call one of the two callbacks:</p> <ul> <li>Success then calls <code>myResolve(result value)</code></li> <li>Error then calls <code>myReject(error object)</code></li> </ul> <p>For example:</p> <pre><code>function myDisplayer(some) {\ndocument.getElementById(\"demo\").innerHTML = some;\n}\n\nlet myPromise = new Promise(function(myResolve, myReject) {\nlet x = 0;\n\n// The producing code (this may take some time)\n\nif (x == 0) {\nmyResolve(\"OK\");\n} else {\nmyReject(\"Error\");\n}\n});\n\nmyPromise.then(\nfunction(value) {myDisplayer(value);},\nfunction(error) {myDisplayer(error);}\n);\n</code></pre>"}, {"location": "coding/javascript/javascript/#promise-object-properties", "title": "Promise object properties", "text": "<p>A JavaScript Promise object can be:</p> <ul> <li>Pending</li> <li>Fulfilled</li> <li>Rejected</li> </ul> <p>The <code>Promise</code> object supports two properties: <code>state</code> and <code>result</code>, which can't be accessed directly.</p> <ul> <li>While a <code>Promise</code> object is <code>pending</code> (working), the result is <code>undefined</code>.</li> <li>When a <code>Promise</code> object is <code>fulfilled</code>, the result is a <code>value</code>.</li> <li>When a <code>Promise</code> object is <code>rejected</code>, the result is an <code>error</code> object.</li> </ul>"}, {"location": "coding/javascript/javascript/#waiting-for-a-timeout-example", "title": "Waiting for a timeout example", "text": "<p>Example Using Callback:</p> <pre><code>setTimeout(function() { myFunction(\"I love You !!!\"); }, 3000);\n\nfunction myFunction(value) {\ndocument.getElementById(\"demo\").innerHTML = value;\n}\n</code></pre> <p>Example Using Promise</p> <pre><code>let myPromise = new Promise(function(myResolve, myReject) {\nsetTimeout(function() { myResolve(\"I love You !!\"); }, 3000);\n});\n\nmyPromise.then(function(value) {\ndocument.getElementById(\"demo\").innerHTML = value;\n});\n</code></pre>"}, {"location": "coding/javascript/javascript/#asyncawait", "title": "Async/Await", "text": "<p><code>async</code> and <code>await</code> make promises easier to write.</p> <p><code>async</code> makes a function return a <code>Promise</code>, while <code>await</code> makes a function wait for a <code>Promise</code>.</p>"}, {"location": "coding/javascript/javascript/#async-syntax", "title": "Async syntax", "text": "<p>For example</p> <pre><code>async function myFunction() {\nreturn \"Hello\";\n}\n</code></pre> <p>Is the same as:</p> <pre><code>function myFunction() {\nreturn Promise.resolve(\"Hello\");\n}\n</code></pre> <p>Here is how to use the Promise:</p> <pre><code>myFunction().then(\nfunction(value) {myDisplayer(value);},\nfunction(error) {myDisplayer(error);}\n);\n</code></pre> <p>If you only expect a normal value, skip the <code>function(error)...</code> line.</p>"}, {"location": "coding/javascript/javascript/#await-syntax", "title": "Await syntax", "text": "<p>The keyword <code>await</code> before a function makes the function wait for a promise:</p> <pre><code>let value = await promise;\n</code></pre> <p>The <code>await</code> keyword can only be used inside an async function.</p> <p>For example the next code will update the content of <code>demo</code>  with <code>I love you !!</code> instantly:</p> <pre><code>async function myDisplay() {\nlet myPromise = new Promise(function(resolve, reject) {\nresolve(\"I love You !!\");\n});\ndocument.getElementById(\"demo\").innerHTML = await myPromise;\n}\n\nmyDisplay();\n</code></pre> <p>But it could have a timeout</p> <pre><code>async function myDisplay() {\nlet myPromise = new Promise(function(resolve) {\nsetTimeout(function() {resolve(\"I love You !!\");}, 3000);\n});\ndocument.getElementById(\"demo\").innerHTML = await myPromise;\n}\n\nmyDisplay();\n</code></pre>"}, {"location": "coding/javascript/javascript/#cookies", "title": "Cookies", "text": "<p>Cookies are data, stored in small text files, on your computer. They hold a very small amount of data at a maximum capacity of 4KB.</p> <p>When a web server has sent a web page to a browser, the connection is shut down, and the server forgets everything about the user.</p> <p>Cookies were invented to solve the problem \"how to remember information about the user\":</p> <ul> <li>When a user visits a web page, his/her name can be stored in a cookie.</li> <li>Next time the user visits the page, the cookie \"remembers\" his/her name.</li> </ul> <p>There are two types of cookies: persistent cookies and session cookies. Session cookies do not contain an expiration date. Instead, they are stored only as long as the browser or tab is open. As soon as the browser is closed, they are permanently lost. Persistent cookies do have an expiration date. These cookies are stored on the user\u2019s disk until the expiration date and then permanently deleted.</p> <p>Note<p>\"Think if for your case ithe's better to use Web storage instead\"</p> </p>"}, {"location": "coding/javascript/javascript/#setting-a-cookie", "title": "Setting a cookie", "text": "<p>Cookies are saved in name-value pairs like:</p> <pre><code>username = John Doe\n</code></pre> <p>When a browser requests a web page from a server, cookies belonging to the page are added to the request. This way the server gets the necessary data to \"remember\" information about users.</p> <p>JavaScript can create, read, and delete cookies with the <code>document.cookie</code> property.</p> <p>With JavaScript, a cookie can be created like this:</p> <pre><code>document.cookie = \"username=John Doe\";\n</code></pre> <p>You can also add an expiry date (in UTC time). By default, the cookie is deleted when the browser is closed:</p> <pre><code>document.cookie = \"username=John Doe; expires=Thu, 18 Dec 2013 12:00:00 UTC\";\n</code></pre> <p>With a path parameter, you can tell the browser what path the cookie belongs to. By default, the cookie belongs to the current page.</p> <pre><code>document.cookie = \"username=John Doe; expires=Thu, 18 Dec 2013 12:00:00 UTC; path=/\";\n</code></pre>"}, {"location": "coding/javascript/javascript/#reading-a-cookie", "title": "Reading a Cookie", "text": "<p>With JavaScript, cookies can be read like this:</p> <pre><code>let x = document.cookie;\n</code></pre> <p><code>document.cookie</code> will return all cookies in one string much like: <code>cookie1=value; cookie2=value; cookie3=value;</code>. It looks like a normal text string. But it is not.</p> <p>If you want to find the value of one specified cookie, you must write a JavaScript function that searches for the cookie value in the cookie string.</p>"}, {"location": "coding/javascript/javascript/#change-a-cookie-with-javascript", "title": "Change a Cookie with JavaScript", "text": "<p>With JavaScript, you can change a cookie the same way as you create it:</p> <pre><code>document.cookie = \"username=John Smith; expires=Thu, 18 Dec 2013 12:00:00 UTC; path=/\";\n</code></pre> <p>The old cookie is overwritten.</p>"}, {"location": "coding/javascript/javascript/#delete-a-cookie-with-javascript", "title": "Delete a Cookie with JavaScript", "text": "<p>Just set the expires parameter to a past date:</p> <pre><code>document.cookie = \"username=; expires=Thu, 01 Jan 1970 00:00:00 UTC; path=/;\";\n</code></pre>"}, {"location": "coding/javascript/javascript/#javascript-cookie-example", "title": "Javascript cookie example", "text": "<p>In the example to follow, we will create a cookie that stores the name of a visitor.</p> <p>The first time a visitor arrives to the web page, he/she will be asked to fill in his/her name. The name is then stored in a cookie.</p> <p>The next time the visitor arrives at the same page, he/she will get a welcome message.</p>"}, {"location": "coding/javascript/javascript/#function-to-set-a-cookie-value", "title": "Function to set a cookie value", "text": "<pre><code>function setCookie(cname, cvalue, exdays) {\nconst d = new Date();\nd.setTime(d.getTime() + (exdays*24*60*60*1000));\nlet expires = \"expires=\"+ d.toUTCString();\ndocument.cookie = cname + \"=\" + cvalue + \";\" + expires + \";path=/\";\n}\n</code></pre>"}, {"location": "coding/javascript/javascript/#function-to-get-a-cookie-value", "title": "Function to get a cookie value", "text": "<pre><code>function getCookie(cname) {\nlet name = cname + \"=\";\nlet decodedCookie = decodeURIComponent(document.cookie);\nlet ca = decodedCookie.split(';');\nfor(let i = 0; i &lt;ca.length; i++) {\nlet c = ca[i];\nwhile (c.charAt(0) == ' ') {\nc = c.substring(1);\n}\nif (c.indexOf(name) == 0) {\nreturn c.substring(name.length, c.length);\n}\n}\nreturn \"\";\n}\n</code></pre> <p>Where:</p> <ul> <li>The function take the cookiename as parameter <code>cname</code>.</li> <li>Creates a variable <code>name</code> with the text to search for <code>cname + \"=\"</code>.</li> <li>Decodes the cookie string, to handle cookies with special characters like     <code>$</code>.</li> <li>Split <code>document.cookie</code> on semicolons into an array called <code>ca</code>.</li> <li>Loop through the <code>ca</code> array and read out each value <code>c</code>.</li> <li>If the cookie is found <code>c.indexOf(name) == 0</code>, return the value of the cookie <code>c.substring(name.length, c.length</code>.</li> <li>If the cookie is not found, return <code>\"\"</code>.</li> </ul>"}, {"location": "coding/javascript/javascript/#function-to-check-a-cookie-value", "title": "Function to check a cookie value", "text": "<p>If the cookie is set it will display a greeting.</p> <p>If the cookie is not set, it will display a prompt box, asking for the name of the user, and stores the username cookie for 365 days, by calling the <code>setCookie</code> function:</p> <pre><code>function checkCookie() {\nlet username = getCookie(\"username\");\nif (username != \"\") {\nalert(\"Welcome again \" + username);\n} else {\nusername = prompt(\"Please enter your name:\", \"\");\nif (username != \"\" &amp;&amp; username != null) {\nsetCookie(\"username\", username, 365);\n}\n}\n}\n</code></pre> <p>The html code of the page would be:</p> <pre><code>&lt;body onload=\"checkCookie()\"&gt;&lt;/body&gt;\n</code></pre>"}, {"location": "coding/javascript/javascript/#web-storage", "title": "Web Storage", "text": "<p>The Web Storage API is a simple syntax for storing and retrieving data in the browser. There are two storage objects <code>localStorage</code> and <code>sessionStorage</code>.</p> <pre><code>localStorage.setItem(\"name\", \"John Doe\");\nlocalStorage.getItem(\"name\");\n</code></pre> <p>The storage object properties and methods are:</p> Property/Method Description <code>key(n)</code> Returns the name of the nth key in the storage <code>length</code> Returns the number of data items stored in the Storage object <code>getItem(keyname)</code> Returns the value of the specified key name <code>setItem(keyname, value)</code> Adds or updates the key to the storage <code>removeItem(keyname)</code> Removes that key from the storage <code>clear()</code> Empty all key out of the storage <p><code>sessionStorage</code> is identical to the <code>localStorage</code> but it only stores the data for one session, so the data will be deleted when the browser is closed.</p> <p>Introduced by HTML5, it has replaced many of the cookies uses. This is because <code>LocalStorage</code> has a lot of advantages over cookies. One of the most important differences is that unlike with cookies, data does not have to be sent back and forth with every HTTP request. This reduces the overall traffic between the client and the server and the amount of wasted bandwidth. This is because data is stored on the user\u2019s local disk and is not destroyed or cleared by the loss of an internet connection. Also, <code>LocalStorage</code> can hold up to 5MB of information which is a whole lot more than the 4KB that cookies hold.</p> <p><code>LocalStorage</code> behaves more like persistent cookies in terms of expiration. Data is not automatically destroyed unless it is cleared through Javascript code. This can be good for larger bits of data that need to be stored for longer periods of time. Also, with <code>LocalStorage</code> you can not only store strings but also Javascript primitives and objects.</p> <p>An example of a good use of <code>LocalStorage</code> might be in an application used in regions without a persistent internet connection. In order for this to be a good use of <code>LocalStorage</code>, the threat level of the data stored in this situation would have to be very low. To protect client privacy, it would be good to upload the data when connection is re-established and then delete the locally stored version.</p> <p>In conclusion, Cookies are smaller and send server information back with every HTTP request, while <code>LocalStorage</code> is larger and can hold information on the client side.</p>"}, {"location": "coding/javascript/javascript/#web-workers", "title": "Web workers", "text": "<p>When executing scripts in an HTML page, the page becomes unresponsive until the script is finished.</p> <p>A web worker is a JavaScript that runs in the background, independently of other scripts, without affecting the performance of the page. You can continue to do whatever you want: clicking, selecting things, etc., while the web worker runs in the background.</p> <p>Since web workers are in external files, they do not have access to the following JavaScript objects:</p> <ul> <li>The window object</li> <li>The document object</li> <li>The parent object</li> </ul> <p>Before creating a web worker check whether the user's browser supports it:</p> <pre><code>if (typeof(Worker) !== \"undefined\") {\n// Yes! Web worker support!\n// Some code.....\n} else {\n// Sorry! No Web Worker support..\n}\n</code></pre> <p>Now, let's create our web worker in an external JavaScript.</p> <p>Here, we create a script that counts. The script is stored in the <code>demo_workers.js</code> file:</p> <pre><code>let i = 0;\n\nfunction timedCount() {\ni ++;\npostMessage(i);\nsetTimeout(\"timedCount()\",500);\n}\n\ntimedCount();\n</code></pre> <p>The important part of the code above is the <code>postMessage()</code> method, which is used to post a message back to the HTML page.</p>"}, {"location": "coding/javascript/javascript/#create-a-web-worker-object", "title": "Create a Web Worker object", "text": "<p>Now that we have the web worker file, we need to call it from an HTML page.</p> <p>The following lines checks if the worker already exists, if not it creates a new web worker object and runs the code in <code>demo_workers.js</code>:</p> <pre><code>if (typeof(w) == \"undefined\") {\nw = new Worker(\"demo_workers.js\");\n}\n</code></pre> <p>Then we can send and receive messages from the web worker.</p> <p>Add an <code>onmessage</code> event listener to the web worker.</p> <pre><code>w.onmessage = function(event){\ndocument.getElementById(\"result\").innerHTML = event.data;\n};\n</code></pre> <p>When the web worker posts a message, the code within the event listener is executed. The data from the web worker is stored in <code>event.data</code>.</p>"}, {"location": "coding/javascript/javascript/#terminate-a-web-worker", "title": "Terminate a Web Worker", "text": "<p>When a web worker object is created, it will continue to listen for messages (even after the external script is finished) until it is terminated.</p> <p>To terminate a web worker, and free browser/computer resources, use the <code>terminate()</code> method:</p> <pre><code>w.terminate();\n</code></pre>"}, {"location": "coding/javascript/javascript/#reuse-the-web-worker", "title": "Reuse the Web Worker", "text": "<p>If you set the worker variable to <code>undefined</code>, after it has been terminated, you can reuse the code:</p> <pre><code>w = undefined;\n</code></pre>"}, {"location": "coding/javascript/javascript/#full-web-worker-example-code", "title": "Full web worker example code", "text": "<pre><code> &lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;body&gt;\n\n&lt;p&gt;Count numbers: &lt;output id=\"result\"&gt;&lt;/output&gt;&lt;/p&gt;\n&lt;button onclick=\"startWorker()\"&gt;Start Worker&lt;/button&gt;\n&lt;button onclick=\"stopWorker()\"&gt;Stop Worker&lt;/button&gt;\n\n&lt;script&gt;\nlet w;\n\nfunction startWorker() {\nif (typeof(w) == \"undefined\") {\nw = new Worker(\"demo_workers.js\");\n}\nw.onmessage = function(event) {\ndocument.getElementById(\"result\").innerHTML = event.data;\n};\n}\n\nfunction stopWorker() {\nw.terminate();\nw = undefined;\n}\n&lt;/script&gt;\n\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre>"}, {"location": "coding/javascript/javascript/#interacting-with-external-apis", "title": "Interacting with external APIs", "text": "<p>The Fetch API interface allows web browser to make HTTP requests to web servers.</p> <p>The example below fetches a file and displays the content:</p> <pre><code>&lt;body&gt;\n&lt;p id=\"demo\"&gt;Fetch a file to change this text.&lt;/p&gt;\n&lt;script&gt;\n\nlet file = \"fetch_info.txt\"\n\nfetch (file)\n.then(x =&gt; x.text())\n.then(y =&gt; document.getElementById(\"demo\").innerHTML = y);\n\n&lt;/script&gt;\n&lt;/body&gt;\n</code></pre> <p>Where the content of <code>fetch_info.txt</code> is:</p> <pre><code>&lt;h1&gt;Fetch API&lt;/h1&gt;\n&lt;p&gt;The Fetch API interface allows web browser to make HTTP requests to web servers.&lt;/p&gt;\n</code></pre> <p>Since <code>Fetch</code> is based on <code>async</code> and <code>await</code>, the example above might be easier to understand like this:</p> <pre><code>async function getText(file) {\nlet x = await fetch(file);\nlet y = await x.text();\nmyDisplay(y);\n}\n</code></pre> <p>Or even better: Use understandable names instead of <code>x</code> and <code>y</code>:</p> <pre><code>async function getText(file) {\nlet myObject = await fetch(file);\nlet myText = await myObject.text();\nmyDisplay(myText);\n}\n</code></pre> <p>The first <code>.then()</code> resolves the promise into a response object. To be able to view the content of this object the response is then converted using a <code>.json()</code> method. This <code>json()</code> also returns a promise so another <code>.then()</code> is necessary.</p>"}, {"location": "coding/javascript/javascript/#doing-a-post", "title": "Doing a POST", "text": "<p>We can first include the settings such as header and request method in a object.</p> <pre><code>var jsonObj = {};\njsonObj.firstParam = \"first\";\njsonObj.secondParam = 2;\njsonObj.thirdParam = true;\n\nvar options = {\nmethod: 'POST',\nheader: new Headers({\n\"Content-Type\": \"application/json\",\n}),\nbody: JSON.stringify(jsonObj)\n}\n\nvar url = http://localhost:8080/postRequest;\n\nfetch(url, options)\n.then((response) =&gt; {\nconsole.log(\"Status Code\",response.status);\n//return response type such as json, blob, text, formData and arrayBuffer\nreturn response.json()\n})\n.then((result) =&gt; {\n//here will return whatever information from the response.\nconsole.log(\"response message from backend\", result);\n})\n.catch((error) =&gt; {\nconsole.log(error);\n});\n</code></pre>"}, {"location": "coding/javascript/javascript/#error-handling", "title": "Error handling", "text": "<ul> <li>The <code>try</code> statement defines a code block to run (to try).</li> <li>The <code>catch</code> statement defines a code block to handle any error.</li> <li>The <code>finally</code> statement defines a code block to run regardless of the result.</li> <li> <p>The <code>throw</code> statement defines a custom error.</p> <pre><code>throw \"Too big\";    // throw a text\nthrow 500;          // throw a number\n</code></pre> </li> </ul> <p>In this example we misspelled <code>alert</code> as <code>adddlert</code> to deliberately produce an error:</p> <pre><code>&lt;p id=\"demo\"&gt;&lt;/p&gt;\n\n&lt;script&gt;\ntry {\nadddlert(\"Welcome guest!\");\n}\ncatch(err) {\ndocument.getElementById(\"demo\").innerHTML = err.message;\n}\n&lt;/script&gt;\n</code></pre> <p>When an error occurs, JavaScript will normally stop and generate an error message.</p> <p>JavaScript will create an <code>Error</code> object with two properties: <code>name</code> and <code>message</code>.</p> <p>Six different values can be returned by the error <code>name</code> property:</p> Error Name Description EvalError An error has occurred in the <code>eval()</code> function RangeError A number \"out of range\" has occurred ReferenceError An illegal reference has occurred SyntaxError A syntax error has occurred TypeError A type error has occurred URIError An error in <code>encodeURI()</code> has occurred"}, {"location": "coding/javascript/javascript/#input-validation-example", "title": "Input validation example", "text": "<p>This example examines input. If the value is wrong, an exception (<code>err</code>) is thrown.</p> <p>The exception is caught by the <code>catch</code> statement and a custom error message is displayed:</p> <pre><code>&lt;p&gt;Please input a number between 5 and 10:&lt;/p&gt;\n\n&lt;input id=\"demo\" type=\"text\"&gt;\n&lt;button type=\"button\" onclick=\"myFunction()\"&gt;Test Input&lt;/button&gt;\n&lt;p id=\"p01\"&gt;&lt;/p&gt;\n\n&lt;script&gt;\nfunction myFunction() {\nconst message = document.getElementById(\"p01\");\nmessage.innerHTML = \"\";\nlet x = document.getElementById(\"demo\").value;\ntry {\nif(x == \"\") throw \"empty\";\nif(isNaN(x)) throw \"not a number\";\nx = Number(x);\nif(x &lt; 5) throw \"too low\";\nif(x &gt; 10) throw \"too high\";\n}\ncatch(err) {\nmessage.innerHTML = \"Input is \" + err;\n}\n}\n&lt;/script&gt;\n</code></pre>"}, {"location": "coding/javascript/javascript/#debugging", "title": "Debugging", "text": "<p>You can use the <code>console.log</code> to display JavaScript values in the debugger window:</p> <pre><code>a = 5;\nb = 6;\nc = a + b;\nconsole.log(c);\n</code></pre> <p>Or you can use breakpoints</p> <pre><code>let x = 15 * 5;\ndebugger;\ndocument.getElementById(\"demo\").innerHTML = x;\n</code></pre>"}, {"location": "coding/javascript/javascript/#style-guide", "title": "Style guide", "text": "<ul> <li>Always put spaces around operators ( = + - * / ), and after commas.</li> <li>Always use 2 spaces for indentation of code blocks.</li> <li>Avoid lines longer than 80 characters.</li> <li> <p>Always end a simple statement with a semicolon.</p> <pre><code>const cars = [\"Volvo\", \"Saab\", \"Fiat\"];\n\nconst person = {\nfirstName: \"John\",\nlastName: \"Doe\",\nage: 50,\neyeColor: \"blue\"\n};\n</code></pre> </li> <li> <p>General rules for complex (compound) statements:</p> <ul> <li>Put the opening bracket at the end of the first line.</li> <li>Use one space before the opening bracket.</li> <li>Put the closing bracket on a new line, without leading spaces.</li> <li>Do not end a complex statement with a semicolon.</li> </ul> <pre><code>function toCelsius(fahrenheit) {\nreturn (5 / 9) * (fahrenheit - 32);\n}\n\nfor (let i = 0; i &lt; 5; i++) {\nx += i;\n}\n\nif (time &lt; 20) {\ngreeting = \"Good day\";\n} else {\ngreeting = \"Good evening\";\n}\n</code></pre> </li> <li> <p>General rules for object definitions:</p> <ul> <li>Place the opening bracket on the same line as the object name.</li> <li>Use colon plus one space between each property and its value.</li> <li>Use quotes around string values, not around numeric values.</li> <li>Do not add a comma after the last property-value pair.</li> <li>Place the closing bracket on a new line, without leading spaces.</li> <li>Always end an object definition with a semicolon.</li> </ul> <pre><code>const person = {\nfirstName: \"John\",\nlastName: \"Doe\",\nage: 50,\neyeColor: \"blue\"\n};\n</code></pre> </li> </ul>"}, {"location": "coding/javascript/javascript/#best-practices", "title": "Best practices", "text": "<ul> <li>Minimize the use of global variables.</li> <li>All variables used in a function should be declared as local variables.</li> <li> <p>It is a good coding practice to put all declarations at the top of each script     or function.</p> <p>This will:</p> <ul> <li>Give cleaner code</li> <li>Provide a single place to look for local variables</li> <li>Make it easier to avoid unwanted (implied) global variables</li> <li>Reduce the possibility of unwanted re-declarations</li> </ul> </li> <li> <p>Initialize variables when you declare them.</p> <p>This will:</p> <ul> <li>Give cleaner code</li> <li>Provide a single place to initialize variables</li> <li>Avoid undefined values</li> </ul> </li> <li> <p>Declaring objects and arrays with <code>const</code> will prevent any accidental change of type.</p> </li> <li> <p>Don't use the <code>new Object()</code></p> <ul> <li>Use <code>\"\"</code> instead of new <code>String()</code>.</li> <li>Use <code>0</code> instead of new <code>Number()</code>.</li> <li>Use <code>false</code> instead of new <code>Boolean()</code>.</li> <li>Use <code>{}</code> instead of new <code>Object()</code>.</li> <li>Use <code>[]</code> instead of new <code>Array()</code>.</li> <li>Use <code>/()/</code> instead of new <code>RegExp()</code>.</li> <li>Use <code>function (){}</code> instead of <code>new Function()</code>.</li> <li>Use <code>===</code> for comparison. The <code>==</code> comparison operator always converts (to matching types) before comparison.</li> <li>Use parameter defaults. If a function is called with a missing argument, the value of the missing argument is set to <code>undefined</code>.</li> </ul> <p>Undefined values can break your code. It is a good habit to assign default values to arguments.</p> <p><pre><code>function myFunction(x, y) {\nif (y === undefined) {\ny = 0;\n}\n}\n</code></pre> * Avoid using <code>eval()</code>. It's used to run text as code which represents a security problem.</p> </li> <li> <p>Reduce DOM access. Accessing the HTML DOM is very slow, compared to other     JavaScript statements.</p> <p>If you expect to access a DOM element several times, access it once, and use it as a local variable:</p> <p><pre><code>const obj = document.getElementById(\"demo\");\nobj.innerHTML = \"Hello\";\n</code></pre> * Keep the number of elements in the HTML DOM small. This will always improve page loading, and speed up rendering (page display), especially on smaller devices.</p> <p>Every attempt to search the DOM (like <code>getElementsByTagName</code>) will benefit from a smaller DOM.</p> </li> <li> <p>Delay JavaScript loading. Putting your scripts at the bottom of the page body     lets the browser load the page first.</p> <p>While a script is downloading, the browser will not start any other downloads. In addition all parsing and rendering activity might be blocked.</p> </li> <li> <p>Avoid using the <code>with</code> keyword. It has a negative effect on speed. It also     clutters up JavaScript scopes.</p> </li> </ul>"}, {"location": "coding/javascript/javascript/#references", "title": "References", "text": "<ul> <li>W3 JavaScript tutorial</li> <li>John Comeau operator explainer</li> <li>Re-introduction to JavaScript</li> <li>Chikwekwe's articles on cookies vs LocalStorage</li> <li>Jeff's post on xmlhttprequest vs Fetch API</li> </ul>"}, {"location": "coding/json/json/", "title": "JSON", "text": "<p>JavaScript Object Notation (JSON), is an open standard file format, and data interchange format, that uses human-readable text to store and send data objects consisting of attribute\u2013value pairs and array data types (or any other serializable value).</p>"}, {"location": "coding/json/json/#linters-and-fixers", "title": "Linters and fixers", "text": ""}, {"location": "coding/json/json/#jsonlint", "title": "jsonlint", "text": "<p><code>jsonlint</code> is a pure JavaScript version of the service provided at jsonlint.com.</p> <p>Install it with:</p> <pre><code>npm install jsonlint -g\n</code></pre> <p>Vim supports this linter through ALE.</p>"}, {"location": "coding/json/json/#jq", "title": "jq", "text": "<p><code>jq</code> is like sed for JSON data. You can use it to slice, filter, map and transform structured data with the same ease that <code>sed</code>, <code>awk</code>, <code>grep</code> and friends let you play with text.</p> <p>Install it with:</p> <pre><code>apt-get install jq\n</code></pre> <p>Vim supports this linter through ALE.</p>"}, {"location": "coding/promql/promql/", "title": "PromQL", "text": ""}, {"location": "coding/promql/promql/#usage", "title": "Usage", "text": ""}, {"location": "coding/promql/promql/#selecting-series", "title": "Selecting series", "text": "<p>Select latest sample for series with a given metric name:</p> <pre><code>node_cpu_seconds_total\n</code></pre> <p>Select 5-minute range of samples for series with a given metric name:</p> <pre><code>node_cpu_seconds_total[5m]\n</code></pre> <p>Only series with given label values:</p> <pre><code>node_cpu_seconds_total{cpu=\"0\",mode=\"idle\"}\n</code></pre> <p>Complex label matchers (<code>=</code>: equality, <code>!=</code>: non-equality, <code>=~</code>: regex match, <code>!~</code>: negative regex match):</p> <pre><code>node_cpu_seconds_total{cpu!=\"0\",mode=~\"user|system\"}\n</code></pre> <p>Select data from one day ago and shift it to the current time:</p> <pre><code>process_resident_memory_bytes offset 1d\n</code></pre>"}, {"location": "coding/promql/promql/#rates-of-increase-for-counters", "title": "Rates of increase for counters", "text": "<p>Per-second rate of increase, averaged over last 5 minutes:</p> <pre><code>rate(demo_api_request_duration_seconds_count[5m])\n</code></pre> <p>Per-second rate of increase, calculated over last two samples in a 1-minute time window:</p> <pre><code>irate(demo_api_request_duration_seconds_count[1m])\n</code></pre> <p>Absolute increase over last hour:</p> <pre><code>increase(demo_api_request_duration_seconds_count[1h])\n</code></pre>"}, {"location": "coding/promql/promql/#aggregating-over-multiple-series", "title": "Aggregating over multiple series", "text": "<p>Sum over all series:</p> <pre><code>sum(node_filesystem_size_bytes)\n</code></pre> <p>Preserve the instance and job label dimensions:</p> <pre><code>sum by(job, instance) (node_filesystem_size_bytes)\n</code></pre> <p>Aggregate away the instance and job label dimensions:</p> <pre><code>sum without(instance, job) (node_filesystem_size_bytes)\n</code></pre> <p>Available aggregation operators: <code>sum()</code>, <code>min()</code>, <code>max()</code>, <code>avg()</code>, <code>stddev()</code>, <code>stdvar()</code>, <code>count()</code>, <code>count_values()</code>, <code>group()</code>, <code>bottomk()</code>, <code>topk()</code>, <code>quantile()</code>.</p>"}, {"location": "coding/promql/promql/#time", "title": "Time", "text": "<p>Get the Unix time in seconds at each resolution step:</p> <pre><code>time()\n</code></pre> <p>Get the age of the last successful batch job run:</p> <pre><code>time() - demo_batch_last_success_timestamp_seconds\n</code></pre> <p>Find batch jobs which haven't succeeded in an hour:</p> <pre><code>time() - demo_batch_last_success_timestamp_seconds &gt; 3600\n</code></pre>"}, {"location": "coding/promql/promql/#snippets", "title": "Snippets", "text": ""}, {"location": "coding/promql/promql/#run-operation-only-on-the-elements-that-match-a-condition", "title": "Run operation only on the elements that match a condition", "text": "<p>Imagine we want to run the <code>zfs_dataset_used_bytes - zfs_dataset_used_by_dataset_bytes</code> operation only on the elements that match <code>zfs_dataset_used_by_dataset_bytes &gt; 200e3</code>. You can do this with <code>and</code>:</p> <pre><code>zfs_dataset_used_bytes - zfs_dataset_used_by_dataset_bytes and zfs_dataset_used_by_dataset_bytes &gt; 200e3\n</code></pre>"}, {"location": "coding/promql/promql/#substracting-two-metrics", "title": "Substracting two metrics", "text": "<p>To run binary operators between vectors you need them to match. Basically it means that it will only do the operation on the elements that have the same labels. Sometimes you want to do operations on metrics that don't have the same labels. In those cases you can use the <code>on</code> operator. Imagine that we want to substract the next vectors:</p> <pre><code>zfs_dataset_used_bytes{type='filesystem'}\n</code></pre> <p>And</p> <pre><code>sum by (hostname,filesystem) (zfs_dataset_used_bytes{type='snapshot'})\n</code></pre> <p>That only have in common the labels <code>hostname</code> and filesystem`. </p> <p>You can use the next expression then:</p> <pre><code>zfs_dataset_used_bytes{type='filesystem'} - on (hostname, filesystem) sum by (hostname,filesystem) (zfs_dataset_used_bytes{type='snapshot'})\n</code></pre> <p>To learn more on Vector matching read this article</p>"}, {"location": "coding/promql/promql/#generating-range-vectors-from-return-values-in-prometheus-queries", "title": "Generating range vectors from return values in Prometheus queries", "text": "<p>Use the subquery-syntax</p> <p>Warning: These subqueries are expensive, i.e. create very high load on Prometheus. Use recording-rules when you use these queries regularly.</p>"}, {"location": "coding/promql/promql/#subquery-syntax", "title": "Subquery syntax", "text": "<p><code>&lt;instant_query&gt;[&lt;range&gt;:&lt;resolution&gt;]</code></p> <ul> <li><code>instant_query</code>: A PromQL-function which returns an instant-vector).</li> <li><code>range</code>: Offset (back in time) to start the first subquery.</li> <li><code>resolution</code>: The size of each of the subqueries.</li> </ul> <p>It returns a range-vector.</p> <p>For example:</p> <pre><code>deriv(rate(varnish_main_client_req[2m])[5m:10s])\n</code></pre> <p>In the example above, Prometheus runs <code>rate()</code> (= <code>instant_query</code>) 30 times (the first from 5 minutes ago to -4:50, ..., the last -0:10 to now). The resulting range-vector is input to the <code>deriv()</code> function.</p>"}, {"location": "coding/promql/promql/#troubleshooting", "title": "Troubleshooting", "text": ""}, {"location": "coding/promql/promql/#ranges-only-allowed-for-vector-selectors", "title": "Ranges only allowed for vector selectors", "text": "<p>You may need to specify a subquery range such as <code>[1w:1d]</code>.</p>"}, {"location": "coding/promql/promql/#links", "title": "Links", "text": "<ul> <li>Prometheus cheatsheet</li> </ul>"}, {"location": "coding/python/alembic/", "title": "Alembic", "text": "<p>Alembic is a lightweight database migration tool for SQLAlchemy. It is created by the author of SQLAlchemy and it has become the de-facto standard tool to perform migrations on SQLAlchemy backed databases.</p> <p>I discourage you to use an ORM to manage the interactions with the database. Check the alternative solutions.</p>"}, {"location": "coding/python/alembic/#database-migration-in-sqlalchemy", "title": "Database Migration in SQLAlchemy", "text": "<p>A database migration usually changes the schema of a database, such as adding a column or a constraint, adding a table or updating a table. It's often performed using raw SQL wrapped in a transaction so that it can be rolled back if something went wrong during the migration.</p> <p>To migrate a SQLAlchemy database, an Alembic migration script is created for the intended migration, perform the migration, update the model definition and then start using the database under the migrated schema.</p>"}, {"location": "coding/python/alembic/#alembic-repository-initialization", "title": "Alembic repository initialization", "text": "<p>It's important that the migration scripts are saved with the rest of the source code. Following Miguel Gringberg suggestion, we'll store them in the <code>{{ program_name }}/migrations</code> directory.</p> <p>Execute the following command to initialize the alembic repository.</p> <pre><code>alembic init {{ program_name }}/migrations\n</code></pre> <p>It will create several files and directories under the selected path, the most important are:</p> <ul> <li><code>alembic.ini</code>: It's the file the <code>alembic</code> script will look for when invoked.     Usually it's located at the root of the program. Although there are several     options     to configure here, we'll use the <code>env.py</code> file to define how to access the     database.</li> <li> <p><code>env.py</code>: It is a Python script that is run whenever the alembic migration     tool is invoked. At the very least, it contains instructions to configure     and generate a SQLAlchemy engine, procure a connection from that engine     along with a transaction, and then invoke the migration engine, using the     connection as a source of database connectivity.</p> <p>The <code>env.py</code> script is part of the generated environment so that the way migrations run is entirely customizable. The exact specifics of how to connect are here, as well as the specifics of how the migration environment are invoked. The script can be modified so that multiple engines can be operated upon, custom arguments can be passed into the migration environment, application-specific libraries and models can be loaded in and made available.</p> <p>By default alembic takes the database url by the <code>sqlalchemy.url</code> key in the <code>alembic.ini</code> file. But it's advisable to be able to define the url from environmental variables. Therefore we need to modify this file with the following changes:</p> <pre><code># from sqlalchemy import engine_from_config\n# from sqlalchemy import pool\nfrom sqlalchemy import create_engine\n\nimport os\n\nif config.attributes.get(\"configure_logger\", True):\n    fileConfig(config.config_file_name)\n\ndef get_url():\n    basedir = '~/.local/share/{{ program_name }}'\n    return os.environ.get('{{ program_name }}_DATABASE_URL') or \\\n        'sqlite:///' + os.path.join(os.path.expanduser(basedir), 'main.db')\n\n\ndef run_migrations_offline():\n\"\"\"Run migrations in 'offline' mode.\n\n    This configures the context with just a URL\n    and not an Engine, though an Engine is acceptable\n    here as well.  By skipping the Engine creation\n    we don't even need a DBAPI to be available.\n\n    Calls to context.execute() here emit the given string to the\n    script output.\n\n    \"\"\"\n\n    # url = config.get_main_option(\"sqlalchemy.url\")\n    url = get_url()\n\n    context.configure(\n        url=url,\n        target_metadata=target_metadata,\n        literal_binds=True,\n        dialect_opts={\"paramstyle\": \"named\"},\n    )\n\n    with context.begin_transaction():\n        context.run_migrations()\n\n\ndef run_migrations_online():\n\"\"\"Run migrations in 'online' mode.\n\n    In this scenario we need to create an Engine\n    and associate a connection with the context.\n\n    \"\"\"\n    # connectable = engine_from_config(\n    #     config.get_section(config.config_ini_section),\n    #     prefix=\"sqlalchemy.\",\n    #     poolclass=pool.NullPool,\n    # )\n\n    connectable = create_engine(get_url())\n\n\n    # Leave the rest of the file as it is\n</code></pre> <p>It is also necessary to import your models metadata, to do so, modify:</p> <pre><code># add your model's MetaData object here\n# for 'autogenerate' support\n# from myapp import mymodel\n# target_metadata = mymodel.Base.metadata\n# target_metadata = None\n\nimport sys\n\nsys.path = ['', '..'] + sys.path[1:]\nfrom {{ program_name }} import models\ntarget_metadata = models.Base.metadata\n</code></pre> <p>We had to add the parent directory to the <code>sys.path</code> because when <code>env.py</code> is executed, <code>models</code> is not in your <code>PYTHONPATH</code>, resulting in an import error.</p> </li> <li> <p><code>versions/</code>: Directory that holds the individual version scripts. The files it     contains don\u2019t use ascending integers, and instead use a partial GUID     approach. In Alembic, the ordering of version scripts is relative to     directives within the scripts themselves, and it is theoretically possible     to \u201csplice\u201d version files in between others, allowing migration sequences     from different branches to be merged, albeit carefully by hand.</p> </li> </ul>"}, {"location": "coding/python/alembic/#database-migration", "title": "Database Migration", "text": "<p>When changes need to be made in the schema, instead of modifying it directly and then recreate the database from scratch, we can leverage that actions to alembic.</p> <pre><code>alembic revision --autogenerate -m \"{{ commit_comment }}\"\n</code></pre> <p>That command will write a migration script to make the changes. To perform the migration use:</p> <pre><code>alembic upgrade head\n</code></pre> <p>To check the status, execute:</p> <pre><code>alembic current\n</code></pre> <p>To load the migrations from the alembic library inside a python program, the best way to do it is through <code>alembic.command</code> instead of <code>alembic.config.main</code> because it will redirect all logging output to a file</p> <pre><code>from alembic.config import Config\nimport alembic.command\n\nconfig = Config('alembic.ini')\nconfig.attributes['configure_logger'] = False\n\nalembic.command.upgrade(config, 'head')\n</code></pre> <p>File: env.py</p> <pre><code>if config.attributes.get('configure_logger', True):\n    fileConfig(config.config_file_name)\n</code></pre>"}, {"location": "coding/python/alembic/#seed-database-with-data", "title": "Seed database with data", "text": "<p>Note</p> <p>This is an alembic script</p> <pre><code>from datetime import date\nfrom sqlalchemy.sql import table, column\nfrom sqlalchemy import String, Integer, Date\nfrom alembic import op\n\n# Create an ad-hoc table to use for the insert statement.\naccounts_table = table('account',\n    column('id', Integer),\n    column('name', String),\n    column('create_date', Date)\n)\n\nop.bulk_insert(accounts_table,\n    [\n        {'id':1, 'name':'John Smith',\n                'create_date':date(2010, 10, 5)},\n        {'id':2, 'name':'Ed Williams',\n                'create_date':date(2007, 5, 27)},\n        {'id':3, 'name':'Wendy Jones',\n                'create_date':date(2008, 8, 15)},\n    ]\n)\n</code></pre>"}, {"location": "coding/python/alembic/#database-downgrade-or-rollback", "title": "Database downgrade or rollback", "text": "<p>If you want to correct a migration first check the <code>history</code> to see where do you want to go (it accepts <code>--verbose</code> for more information):</p> <pre><code>alembic history\n</code></pre> <p>Then you can specify the id of the revision you want to downgrade to. To specify the last one, use <code>-1</code>.</p> <pre><code>alembic downgrade -1\n</code></pre> <p>After the downgrade is performed, if you want to remove the last revision, you'd need to manually remove the revision python file.</p>"}, {"location": "coding/python/alembic/#references", "title": "References", "text": "<ul> <li>Git</li> <li>Docs</li> </ul>"}, {"location": "coding/python/alembic/#articles", "title": "Articles", "text": "<ul> <li>Migrate SQLAlchemy databases with Alembic</li> </ul>"}, {"location": "coding/python/click/", "title": "Click", "text": "<p>Click is a Python package for creating beautiful command line interfaces in a composable way with as little code as necessary. It\u2019s the \u201cCommand Line Interface Creation Kit\u201d. It\u2019s highly configurable but comes with sensible defaults out of the box.</p> <p>Click has the following features:</p> <ul> <li>Arbitrary nesting of commands.</li> <li>Automatic help page generation.</li> <li>Supports lazy loading of subcommands at runtime.</li> <li>Supports implementation of Unix/POSIX command line conventions.</li> <li>Supports loading values from environment variables out of the box.</li> <li>Support for prompting of custom values.</li> <li>Supports file handling out of the box.</li> <li>Comes with useful common helpers (getting terminal dimensions, ANSI colors,   fetching direct keyboard input, screen clearing, finding config paths,   launching apps and editors).</li> </ul>"}, {"location": "coding/python/click/#setuptools-integration", "title": "Setuptools Integration", "text": "<p>To bundle your script with setuptools, all you need is the script in a Python package and a setup.py file.</p> <p>Let\u2019s assume your directory structure changed to this:</p> <pre><code>project/\n    yourpackage/\n        __init__.py\n        main.py\n        utils.py\n        scripts/\n            __init__.py\n            yourscript.py\n    setup.py\n</code></pre> <p>In this case instead of using py_modules in your setup.py file you can use packages and the automatic package finding support of setuptools. In addition to that it\u2019s also recommended to include other package data.</p> <p>These would be the modified contents of setup.py:</p> <pre><code>from setuptools import setup, find_packages\n\nsetup(\n    name=\"yourpackage\",\n    version=\"0.1.0\",\n    packages=find_packages(),\n    include_package_data=True,\n    install_requires=[\n        \"Click\",\n    ],\n    entry_points={\n        \"console_scripts\": [\n            \"yourscript = yourpackage.scripts.yourscript:cli\",\n        ],\n    },\n)\n</code></pre>"}, {"location": "coding/python/click/#testing-click-applications", "title": "Testing Click applications", "text": "<p>For basic testing, Click provides the <code>click.testing</code> module which provides test functionality that helps you invoke command line applications and check their behavior.</p> <p>The basic functionality for testing Click applications is the <code>CliRunner</code> which can invoke commands as command line scripts. The <code>CliRunner.invoke()</code> method runs the command line script in isolation and captures the output as both bytes and binary data.</p> <p>The return value is a Result object, which has the captured output data, exit code, and optional exception attached:</p> <p>File: <code>hello.py</code></p> <pre><code>import click\n\n\n@click.command()\n@click.argument(\"name\")\ndef hello(name):\n    click.echo(\"Hello %s!\" % name)\n</code></pre> <p>File: <code>test_hello.py</code>:</p> <pre><code>from click.testing import CliRunner\nfrom hello import hello\n\n\ndef test_hello_world():\n    runner = CliRunner()\n    result = runner.invoke(hello, [\"Peter\"])\n    assert result.exit_code == 0\n    assert result.output == \"Hello Peter!\\n\"\n</code></pre> <p>For subcommand testing, a subcommand name must be specified in the args parameter of <code>CliRunner.invoke()</code> method:</p> <p>File: <code>sync.py</code>:</p> <pre><code>import click\n\n\n@click.group()\n@click.option(\"--debug/--no-debug\", default=False)\ndef cli(debug):\n    click.echo(\"Debug mode is %s\" % (\"on\" if debug else \"off\"))\n\n\n@cli.command()\ndef sync():\n    click.echo(\"Syncing\")\n</code></pre> <p>File: <code>test_sync.py</code>:</p> <pre><code>from click.testing import CliRunner\nfrom sync import cli\n\n\ndef test_sync():\n    runner = CliRunner()\n    result = runner.invoke(cli, [\"--debug\", \"sync\"])\n    assert result.exit_code == 0\n    assert \"Debug mode is on\" in result.output\n    assert \"Syncing\" in result.output\n</code></pre> <p>If you want to test user stdin interaction check the prompt_toolkit and pexpect articles.</p>"}, {"location": "coding/python/click/#testing-the-value-of-stdout-and-stderr", "title": "Testing the value of stdout and stderr", "text": "<p>The <code>runner</code> has the <code>stdout</code> and <code>stderr</code> attributes to test if something was written on those buffers.</p> <p>By default the <code>runner</code> is configured to mix <code>stdout</code> and <code>stderr</code>, if you wish to tell apart both sources use:</p> <pre><code>def test(runner: CliRunner): \n  ...\n  runner.mix_stderr = False\n</code></pre>"}, {"location": "coding/python/click/#injecting-fake-dependencies", "title": "Injecting fake dependencies", "text": "<p>If you're following the domain driven design architecture pattern, you'll probably need to inject some fake objects instead of using the original objects.</p> <p>The challenge is to do it without modifying your real code too much for the sake of testing. Harry J.W. Percival and Bob Gregory have an interesting proposal in their Dependency Injection (and Bootstrapping) chapter, although I found it a little bit complex.</p> <p>Imagine that we've got an adapter to interact with the Gitea web application called <code>Gitea</code>.</p> <p>File: <code>adapters/gitea.py</code>:</p> <pre><code>class Gitea:\n    fake: bool = False\n</code></pre> <p>The Click cli definition would be:</p> <p>File: <code>entrypoints/cli.py</code>:</p> <pre><code>import logging\nfrom adapters.gitea import Gitea\n\nlog = logging.getLogger(__name__)\n\n\n@click.group()\n@click.pass_context\ndef cli(ctx: click.core.Context) -&gt; None:\n\"\"\"Command line interface main click entrypoint.\"\"\"\n    ctx.ensure_object(dict)\n    try:\n        ctx.obj[\"gitea\"]\n    except KeyError:\n        ctx.obj[\"gitea\"] = load_gitea()\n\n\n@cli.command()\n@click.pass_context\ndef is_fake(ctx: Context) -&gt; None:\n    if ctx.obj[\"gitea\"].fake:\n        log.info(\"It's fake!\")\n\n\ndef load_gitea() -&gt; Gitea:\n\"\"\"Configure the Gitea object.\"\"\"\n    return Gitea()\n</code></pre> <p>Where:</p> <ul> <li><code>load_gitea</code>: is a simplified version of the loading of an adapter, in a real   example, you'll probably will need to catch some exceptions when loading the   object.</li> <li><code>is_fake</code>: Is the subcommand we're going to use to test if the adapter has   been replaced by the fake object.</li> </ul> <p>The fake implementation of the adapter is called <code>FakeGitea</code>.</p> <p>File: <code>tests/fake_adapters.py</code>:</p> <pre><code>class FakeGitea:\n    fake: bool = True\n</code></pre> <p>To inject <code>FakeGitea</code> in the tests we need to load it in the <code>'gitea'</code> key of the <code>obj</code> attribute of the click <code>ctx</code> <code>Context</code> object. To do it create the <code>fake_dependencies</code> dictionary with the required fakes and pass it to the <code>invoke</code> call.</p> <p>File: <code>tests/e2e/test_cli.py</code>:</p> <pre><code>from tests.fake_adapters import FakeGitea\nfrom _pytest.logging import LogCaptureFixture\n\nfake_dependencies = {\"gitea\": FakeGitea()}\n\n\n@pytest.fixture(name=\"runner\")\ndef fixture_runner() -&gt; CliRunner:\n\"\"\"Configure the Click cli test runner.\"\"\"\n    return CliRunner()\n\n\ndef test_fake_injection(runner: CliRunner, caplog: LogCaptureFixture) -&gt; None:\n    result = runner.invoke(cli, [\"is_fake\"], obj=fake_dependencies)\n\n    assert result.exit_code == 0\n    assert (\n        \"entrypoints.cli\",\n        logging.INFO,\n        \"It's fake!\",\n    ) in caplog.record_tuples\n</code></pre> <p>In this way we don't need to ship the fake objects with the code, and the modifications are minimal. Only the <code>try/except KeyError</code> snippet in the <code>cli</code> definition.</p>"}, {"location": "coding/python/click/#file-system-isolation", "title": "File System Isolation", "text": "<p>For basic command line tools with file system operations, the <code>CliRunner.isolated_filesystem()</code> method is useful for setting the current working directory to a new, empty folder.</p> <pre><code>from click.testing import CliRunner\nfrom cat import cat\n\n\ndef test_cat():\n    runner = CliRunner()\n    with runner.isolated_filesystem():\n        with open(\"hello.txt\", \"w\") as f:\n            f.write(\"Hello World!\")\n\n        result = runner.invoke(cat, [\"hello.txt\"])\n        assert result.exit_code == 0\n        assert result.output == \"Hello World!\\n\"\n</code></pre> <p>Pass <code>temp_dir</code> to control where the temporary directory is created. The directory will not be removed by Click in this case. This is useful to integrate with a framework like Pytest that manages temporary files.</p> <pre><code>def test_keep_dir(tmp_path):\n    runner = CliRunner()\n\n    with runner.isolated_filesystem(temp_dir=tmp_path) as td:\n        ...\n</code></pre> <p>You may also need to isolate the environment variables if your application read the configuration from them. To do that override the <code>runner</code> fixture:</p> <pre><code>@pytest.fixture(name=\"runner\")\ndef fixture_runner() -&gt; CliRunner:\n\"\"\"Configure the Click cli test runner.\"\"\"\n    return CliRunner(\n        env={\n            'PASSWORD_STORE_DIR': '',\n            'GNUPGHOME': '',\n            'PASSWORD_AUTH_DIR': '',\n        },\n        mix_stderr=False\n    )\n</code></pre> <p>If you define the fixture in <code>conftest.py</code> you may need to use another name than <code>runner</code> otherwise it may be skipped, for example <code>cli_runner</code>.</p>"}, {"location": "coding/python/click/#options", "title": "Options", "text": ""}, {"location": "coding/python/click/#boolean-flags", "title": "Boolean Flags", "text": "<p>Boolean flags are options that can be enabled or disabled. This can be accomplished by defining two flags in one go separated by a slash (/) for enabling or disabling the option. Click always wants you to provide an enable and disable flag so that you can change the default later.</p> <pre><code>import sys\n\n\n@click.command()\n@click.option(\"--shout/--no-shout\", default=False)\ndef info(shout):\n    rv = sys.platform\n    if shout:\n        rv = rv.upper() + \"!!!!111\"\n    click.echo(rv)\n</code></pre> <p>If you really don\u2019t want an off-switch, you can just define one and manually inform Click that something is a flag:</p> <pre><code>import sys\n\n\n@click.command()\n@click.option(\"--shout\", is_flag=True)\ndef info(shout):\n    rv = sys.platform\n    if shout:\n        rv = rv.upper() + \"!!!!111\"\n    click.echo(rv)\n</code></pre>"}, {"location": "coding/python/click/#accepting-values-from-environmental-variables", "title": "Accepting values from environmental variables", "text": "<p>Click is the able to accept parameters from environment variables. There are two ways to define them.</p> <ul> <li> <p>Passing the <code>auto_envvar_prefix</code> to the script that is invoked so each command   and parameter is then added as an uppercase underscore-separated variable.</p> </li> <li> <p>Manually pull values in from specific environment variables by defining the   name of the environment variable on the option.</p> </li> </ul> <pre><code>@click.command()\n@click.option(\"--username\", envvar=\"USERNAME\")\ndef greet(username):\n    click.echo(f\"Hello {username}!\")\n\n\nif __name__ == \"__main__\":\n    greet()\n</code></pre>"}, {"location": "coding/python/click/#arguments", "title": "Arguments", "text": "<p>Arguments work similarly to options but are positional. They also only support a subset of the features of options due to their syntactical nature. Click will also not attempt to document arguments for you and wants you to document them manually in order to avoid ugly help pages.</p>"}, {"location": "coding/python/click/#basic-arguments", "title": "Basic Arguments", "text": "<p>The most basic option is a simple string argument of one value. If no type is provided, the type of the default value is used, and if no default value is provided, the type is assumed to be <code>STRING</code>.</p> <pre><code>@click.command()\n@click.argument(\"filename\")\ndef touch(filename):\n\"\"\"Print FILENAME.\"\"\"\n    click.echo(filename)\n</code></pre> <p>And what it looks like:</p> <pre><code>$ touch foo.txt\nfoo.txt\n</code></pre>"}, {"location": "coding/python/click/#variadic-arguments", "title": "Variadic arguments", "text": "<p>The second most common version is variadic arguments where a specific (or unlimited) number of arguments is accepted. This can be controlled with the <code>nargs</code> parameter. If it is set to <code>-1</code>, then an unlimited number of arguments is accepted.</p> <p>The value is then passed as a tuple. Note that only one argument can be set to <code>nargs=-1</code>, as it will eat up all arguments.</p> <pre><code>@click.command()\n@click.argument(\"src\", nargs=-1)\n@click.argument(\"dst\", nargs=1)\ndef copy(src, dst):\n\"\"\"Move file SRC to DST.\"\"\"\n    for fn in src:\n        click.echo(\"move %s to folder %s\" % (fn, dst))\n</code></pre> <p>You can't use variadic arguments and then specify a command.</p>"}, {"location": "coding/python/click/#file-arguments", "title": "File Arguments", "text": "<p>Command line tools are more fun if they work with files the Unix way, which is to accept <code>-</code> as a special file that refers to stdin/stdout.</p> <p>Click supports this through the <code>click.File</code> type which intelligently handles files for you. It also deals with Unicode and bytes correctly for all versions of Python so your script stays very portable.</p> <pre><code>@click.command()\n@click.argument(\"input\", type=click.File(\"rb\"))\n@click.argument(\"output\", type=click.File(\"wb\"))\ndef inout(input, output):\n\"\"\"Copy contents of INPUT to OUTPUT.\"\"\"\n    while True:\n        chunk = input.read(1024)\n        if not chunk:\n            break\n        output.write(chunk)\n</code></pre> <p>And what it does:</p> <pre><code>$ inout - hello.txt\nhello\n^D\n$ inout hello.txt -\nhello\n</code></pre>"}, {"location": "coding/python/click/#file-path-arguments", "title": "File path arguments", "text": "<p>In the previous example, the files were opened immediately. If we just want the filename, you should be using the <code>Path</code> type. Not only will it return either bytes or Unicode depending on what makes more sense, but it will also be able to do some basic checks for you such as existence checks.</p> <pre><code>@click.command()\n@click.argument(\"filename\", type=click.Path(exists=True))\ndef touch(filename):\n\"\"\"Print FILENAME if the file exists.\"\"\"\n    click.echo(click.format_filename(filename))\n</code></pre> <p>And what it does:</p> <pre><code>$ touch hello.txt\nhello.txt\n\n$ touch missing.txt\nUsage: touch [OPTIONS] FILENAME\nTry 'touch --help' for help.\n\nError: Invalid value for 'FILENAME': Path 'missing.txt' does not exist.\n</code></pre>"}, {"location": "coding/python/click/#set-allowable-values-for-an-argument", "title": "Set allowable values for an argument", "text": "<pre><code>@cli.command()\n@click.argument(\"source\")\n@click.argument(\"destination\")\n@click.option(\"--mode\", type=click.Choice([\"local\", \"ftp\"]), required=True)\ndef copy(source, destination, mode):\n    print(\n        \"copying files from \"\n        + source\n        + \" to \"\n        + destination\n        + \"using \"\n        + mode\n        + \" mode\"\n    )\n</code></pre>"}, {"location": "coding/python/click/#commands-and-groups", "title": "Commands and groups", "text": ""}, {"location": "coding/python/click/#nested-handling-and-contexts", "title": "Nested handling and contexts", "text": "<p>Each time a command is invoked, a new context is created and linked with the parent context. Contexts are passed to parameter callbacks together with the value automatically. Commands can also ask for the context to be passed by marking themselves with the <code>pass_context()</code> decorator. In that case, the context is passed as first argument.</p> <p>The context can also carry a program specified object that can be used for the program\u2019s purposes.</p> <pre><code>@click.group(help=\"Description of the command line.\")\n@click.option('--debug/--no-debug', default=False)\n@click.pass_context\ndef cli(ctx, debug):\n    # ensure that ctx.obj exists and is a dict (in case `cli()` is called\n    # by means other than the `if` block below)\n    ctx.ensure_object(dict)\n\n    ctx.obj['DEBUG'] = debug\n\n@cli.command()\n@click.pass_context\ndef sync(ctx):\n    click.echo(f'Debug is {ctx.obj['DEBUG'] and 'on' or 'off'}'))\n\nif __name__ == '__main__':\n    cli(obj={})\n</code></pre> <p>If the object is provided, each context will pass the object onwards to its children, but at any level a context\u2019s object can be overridden. To reach to a parent, <code>context.parent</code> can be used.</p> <p>In addition to that, instead of passing an object down, nothing stops the application from modifying global state. For instance, you could just flip a global <code>DEBUG</code> variable and be done with it.</p>"}, {"location": "coding/python/click/#add-default-command-to-group", "title": "Add default command to group", "text": "<p>You need to use <code>DefaultGroup</code>, which is a sub class of <code>click.Group</code>. But it invokes a default subcommand instead of showing a help message when a subcommand is not passed.</p> <pre><code>pip install click-default-group\n</code></pre> <pre><code>import click\nfrom click_default_group import DefaultGroup\n\n\n@click.group(cls=DefaultGroup, default=\"foo\", default_if_no_args=True)\ndef cli():\n    pass\n\n\n@cli.command()\ndef foo():\n    click.echo(\"foo\")\n\n\n@cli.command()\ndef bar():\n    click.echo(\"bar\")\n</code></pre> <p>Then you can invoke that without explicit subcommand name:</p> <pre><code>$ cli.py --help\nUsage: cli.py [OPTIONS] COMMAND [ARGS]...\n\nOptions:\n--help    Show this message and exit.\n\nCommand:\nfoo*\nbar\n\n$ cli.py\nfoo\n$ cli.py foo\nfoo\n$ cli.py bar\nbar\n</code></pre>"}, {"location": "coding/python/click/#hide-a-command-from-the-help", "title": "Hide a command from the help", "text": "<pre><code>@click.command(..., hidden=True)\n</code></pre>"}, {"location": "coding/python/click/#invoke-other-commands-from-a-command", "title": "Invoke other commands from a command", "text": "<p>This is a pattern that is generally discouraged with Click, but possible nonetheless. For this, you can use the <code>Context.invoke()</code> or <code>Context.forward()</code> methods.</p> <p>They work similarly, but the difference is that <code>Context.invoke()</code> merely invokes another command with the arguments you provide as a caller, whereas <code>Context.forward()</code> fills in the arguments from the current command. Both accept the command as the first argument and everything else is passed onwards as you would expect.</p> <pre><code>cli = click.Group()\n\n\n@cli.command()\n@click.option(\"--count\", default=1)\ndef test(count):\n    click.echo(\"Count: %d\" % count)\n\n\n@cli.command()\n@click.option(\"--count\", default=1)\n@click.pass_context\ndef dist(ctx, count):\n    ctx.forward(test)\n    ctx.invoke(test, count=42)\n</code></pre> <p>And what it looks like:</p> <pre><code>$ cli dist\nCount: 1\nCount: 42\n</code></pre>"}, {"location": "coding/python/click/#references", "title": "References", "text": "<ul> <li> <p>Homepage</p> </li> <li> <p>Click vs other argument parsers</p> </li> </ul>"}, {"location": "coding/python/dash/", "title": "Dash", "text": "<p>Use Streamlit instead!</p> <p>I've loved Dash since it was born, they made a big breakthrough with it. Nevertheless, streamlit is a better tool now. It has been built after Dash learning from it's improvable points thus making it much more comfortable to use and advisable for people that don't know either of the libraries.</p> <p>Dash is a productive Python framework for building web analytic applications.</p> <p>Written on top of Flask, Plotly.js, and React.js, Dash is ideal for building data visualization apps with highly custom user interfaces in pure Python. It's particularly suited for anyone who works with data in Python.</p>"}, {"location": "coding/python/dash/#install", "title": "Install", "text": "<pre><code>pip install dash\n</code></pre>"}, {"location": "coding/python/dash/#layout", "title": "Layout", "text": "<p>Dash apps are composed of two parts. The first part is the \"layout\" of the app and it describes what the application looks like. The second part describes the interactivity of the application.</p> <p>Dash provides Python classes for all of the visual components of the application. They maintain a set of components in the <code>dash_core_components</code> and the <code>dash_html_components</code> library but you can also build your own with JavaScript and React.js.</p> <p>The scripts are meant to be run with <code>python app.py</code></p> <p>File: app.py</p> <pre><code>import dash\nimport dash_core_components as dcc\nimport dash_html_components as html\nimport plotly.express as px\nimport pandas as pd\n\nexternal_stylesheets = ['https://codepen.io/chriddyp/pen/bWLwgP.css']\n\napp = dash.Dash(__name__, external_stylesheets=external_stylesheets)\n\n# assume you have a \"long-form\" data frame\n# see https://plotly.com/python/px-arguments/ for more options\ndf = pd.DataFrame({\n    \"Fruit\": [\"Apples\", \"Oranges\", \"Bananas\", \"Apples\", \"Oranges\", \"Bananas\"],\n    \"Amount\": [4, 1, 2, 2, 4, 5],\n    \"City\": [\"SF\", \"SF\", \"SF\", \"Montreal\", \"Montreal\", \"Montreal\"]\n})\n\nfig = px.bar(df, x=\"Fruit\", y=\"Amount\", color=\"City\", barmode=\"group\")\n\napp.layout = html.Div(children=[\n    html.H1(children='Hello Dash'),\n\n    html.Div(children='''\n        Dash: A web application framework for Python.\n    '''),\n\n    dcc.Graph(\n        id='example-graph',\n        figure=fig\n    )\n])\n\nif __name__ == '__main__':\n    app.run_server(debug=True)\n</code></pre>"}, {"location": "coding/python/dash/#testing", "title": "Testing", "text": "<p><code>dash.testing</code> provides some off-the-rack pytest fixtures and a minimal set of testing APIs with our internal crafted best practices at the integration level.</p> <p>After <code>pip install dash[testing]</code>, the Dash pytest fixtures are available, you just need to install the WebDrivers, check the Selenium article if you need help.</p> <p>Dash integration tests are meant to be used with Chrome WebDriver, but the fixture allows you to choose another browser from the command line, e.g. <code>pytest --webdriver Firefox -k bsly001</code>.</p> <p>Headless mode can be used with the <code>--headless</code> flag.</p>"}, {"location": "coding/python/dash/#simple-test", "title": "Simple test", "text": "<p>A simple test would be:</p> <pre><code>import dash\nimport dash_html_components as html\nfrom dash.testing.composite import DashComposite\n\ndef test_bsly001_falsy_child(dash_duo: DashComposite) -&gt; None:\n    app = dash.Dash(__name__)\n    app.layout = html.Div(id=\"nully-wrapper\", children=0)\n    # Host the app locally in a thread, all dash server configs could be\n    # passed after the first app argument\n    dash_duo.start_server(app)\n    # Use wait_for_* if your target element is the result of a callback,\n    # keep in mind even the initial rendering can trigger callbacks\n    dash_duo.wait_for_text_to_equal(\"#nully-wrapper\", \"0\", timeout=4)\n\n    # Use this form if its present is expected at the action point\n    assert dash_duo.find_element(\"#nully-wrapper\").text == \"0\"\n    # To make the checkpoint more readable, you can describe the\n    # acceptance criteria as an assert message after the comma.\n    assert dash_duo.get_logs() == [], \"browser console should contain no error\"\n    # You can use visual testing with percy snapshot\n    dash_duo.percy_snapshot(\"bsly001-layout\")\n</code></pre>"}, {"location": "coding/python/dash/#basic-usage", "title": "Basic usage", "text": "<p>The default fixture for Dash Python integration tests is <code>dash_duo</code>. It contains a <code>thread_server</code> and a WebDriver wrapped with high-level Dash testing APIs, but there are others.</p> <p>The Selenium WebDriver is exposed via the <code>driver</code> property. One of the core components of selenium testing is finding the web element with a <code>locator</code>, and performing some actions like <code>click</code> or <code>send_keys</code> on it, and waiting to verify if the expected state is met after those actions.</p> <p>There are several strategies to locate elements: CSS selector and XPATH are the two most versatile ways. We recommend using the CSS Selector in most cases due to its better performance and robustness across browsers.</p> <p>The Selenium WebDriver provides two types of waits:</p> <ul> <li>explicit wait: Makes WebDriver wait for a certain condition to occur before     proceeding further with execution.</li> <li>implicit wait: Makes WebDriver poll the DOM for a certain amount of time     when trying to locate an element. We set a global two-second timeout at the     driver level.</li> </ul> <p>Check the Dash documentation for more Browser and Dash testing methods.</p>"}, {"location": "coding/python/dash/#references", "title": "References", "text": "<ul> <li>Docs</li> <li>Gallery</li> <li>Introduction video</li> </ul>"}, {"location": "coding/python/dash_leaflet/", "title": "Dash Leaflet", "text": "<p>Dash Leaflet is a wrapper of Leaflet, the leading open-source JavaScript library for interactive maps.</p>"}, {"location": "coding/python/dash_leaflet/#install", "title": "Install", "text": "<pre><code>pip install dash\npip install dash-leaflet\n</code></pre>"}, {"location": "coding/python/dash_leaflet/#usage", "title": "Usage", "text": "<p><pre><code>import dash\nimport dash_leaflet as dl\n\napp = dash.Dash(__name__)\napp.layout = dl.Map(dl.TileLayer(), style={'height': '100vh'})\n\nif __name__ == '__main__':\n    app.run_server(port=8050, debug=True)\n</code></pre> That's it! You have now created your first interactive map with Dash Leaflet. If you visit http://127.0.0.1:8050/ in your browser.</p>"}, {"location": "coding/python/dash_leaflet/#change-tileset", "title": "Change tileset", "text": "<p>Leaflet Map object supports different tiles by default. It also supports any WMS or WMTS by passing a Leaflet-style URL to the tiles parameter: <code>http://{s}.yourtiles.com/{z}/{x}/{y}.png</code>.</p> <p>To use the IGN beautiful map as a fallback and the OpenStreetMaps as default use the following snippet:</p> <pre><code>app.layout = html.Div(\n    dl.Map(\n        [\n            dl.LayersControl(\n                [\n                    dl.BaseLayer(\n                        dl.TileLayer(),\n                        name=\"OpenStreetMaps\",\n                        checked=True,\n                    ),\n                    dl.BaseLayer(\n                        dl.TileLayer(\n                            url=\"https://www.ign.es/wmts/mapa-raster?request=getTile&amp;layer=MTN&amp;TileMatrixSet=GoogleMapsCompatible&amp;TileMatrix={z}&amp;TileCol={x}&amp;TileRow={y}&amp;format=image/jpeg\",\n                            attribution=\"IGN\",\n                        ),\n                        name=\"IGN\",\n                        checked=False,\n                    ),\n                ],\n            ),\n            get_data(),\n        ],\n        zoom=7,\n        center=(40.0884, -3.68042),\n    ),\n    style={\n        \"height\": \"100vh\",\n    },\n)\n</code></pre>"}, {"location": "coding/python/dash_leaflet/#loading-the-data", "title": "Loading the data", "text": ""}, {"location": "coding/python/dash_leaflet/#using-markers", "title": "Using Markers", "text": "<p>As with folium, loading different custom markers within the same geojson object is difficult, therefore we are again forced to use markers with cluster group.</p> <p>Assuming we've got a gpx file called <code>data.gpx</code>, we can use the following snippet to load all markers with a custom icon.</p> <pre><code>import dash_leaflet as dl\nimport gpxpy\n\nicon = {\n    \"iconUrl\": \"https://leafletjs.com/examples/custom-icons/leaf-green.png\",\n    \"shadowUrl\": \"https://leafletjs.com/examples/custom-icons/leaf-shadow.png\",\n    \"iconSize\": [38, 95],  # size of the icon\n    \"shadowSize\": [50, 64],  # size of the shadow\n    \"iconAnchor\": [\n        22,\n        94,\n    ],  # point of the icon which will correspond to marker's location\n    \"shadowAnchor\": [4, 62],  # the same for the shadow\n    \"popupAnchor\": [\n        -3,\n        -76,\n    ],  # point from which the popup should open relative to the iconAnchor\n}\n\n\ndef get_data():\n    gpx_file = open(\"data.gpx\", \"r\")\n    gpx = gpxpy.parse(gpx_file)\n    markers = []\n    for waypoint in gpx.waypoints:\n        markers.append(\n            dl.Marker(\n                title=waypoint.name,\n                position=(waypoint.latitude, waypoint.longitude),\n                icon=icon,\n                children=[\n                    dl.Tooltip(waypoint.name),\n                    dl.Popup(waypoint.name),\n                ],\n            )\n        )\n    cluster = dl.MarkerClusterGroup(id=\"markers\", children=markers)\n    return cluster\n\n\napp = dash.Dash(__name__)\napp.layout = html.Div(\n    dl.Map(\n        [\n            dl.TileLayer(),\n            get_data(),\n        ],\n        zoom=7,\n        center=(40.0884, -3.68042),\n    ),\n    style={\n        \"height\": \"100vh\",\n    },\n)\n</code></pre> <p>Inside <code>get_data</code> you can add further logic to change the icon based on the data of the gpx.</p>"}, {"location": "coding/python/dash_leaflet/#configurations", "title": "Configurations", "text": ""}, {"location": "coding/python/dash_leaflet/#add-custom-css-or-js", "title": "Add custom css or js", "text": "<p>Including custom CSS or JavaScript in your Dash apps is simple. Just create a folder named assets in the root of your app directory and include your CSS and JavaScript files in that folder. Dash will automatically serve all of the files that are included in this folder.</p>"}, {"location": "coding/python/dash_leaflet/#remove-the-border-around-the-map", "title": "Remove the border around the map", "text": "<p>Add a custom css file:</p> <p>File: assets/custom.css</p> <pre><code>body {\nmargin: 0,\n}\n</code></pre>"}, {"location": "coding/python/dash_leaflet/#references", "title": "References", "text": "<ul> <li>Docs</li> <li>Git</li> </ul>"}, {"location": "coding/python/data_classes/", "title": "Data classes", "text": "<p>A data class is a regular Python class that has basic data model methods like <code>__init__()</code>, <code>__repr__()</code>, and <code>__eq__()</code> implemented for you.</p> <p>Introduced in Python 3.7, they typically containing mainly data, although there aren\u2019t really any restrictions.</p> <pre><code>from dataclasses import dataclass\n\n@dataclass\nclass DataClassCard:\n    rank: str\n    suit: str\n</code></pre> <p>They behave similar to named tuples but come with many more features. At the same time, named tuples have some other features that are not necessarily desirable, such as:</p> <ul> <li>By design it's a regular tuple, which can lead to subtle and hard to find     bugs.</li> <li>It's hard to add default values to some fields.</li> <li>It's by nature immutable.</li> </ul> <p>That being said, if you need your data structure to behave like a tuple, then a named tuple is a great alternative.</p>"}, {"location": "coding/python/data_classes/#advantages-over-regular-classes", "title": "Advantages over regular classes", "text": "<ul> <li>Simplify the class definition     <pre><code>@dataclass\nclass DataClassCard:\n    rank: str\n    suit: str\n\n# Versus\n\nclass RegularCard\n    def __init__(self, rank, suit):\n        self.rank = rank\n        self.suit = suit\n</code></pre></li> <li> <p>More descriptive object representation through a better default <code>__repr__()</code>     method.</p> <p><pre><code>&gt;&gt;&gt; queen_of_hearts = DataClassCard('Q', 'Hearts')\n&gt;&gt;&gt; queen_of_hearts\nDataClassCard(rank='Q', suit='Hearts')\n\n# Versus\n\n&gt;&gt;&gt; queen_of_spades = RegularCard('Q', 'Spades')\n&gt;&gt;&gt; queen_of_spades\n&lt;__main__.RegularCard object at 0x7fb6eee35d30&gt;\n</code></pre> * Instance comparison out of the box through a better default <code>__eq__()</code> method.</p> <pre><code>&gt;&gt;&gt; queen_of_hearts == DataClassCard('Q', 'Hearts')\nTrue\n\n# Versus\n\n&gt;&gt;&gt; queen_of_spades == RegularCard('Q', 'Spades')\nFalse\n</code></pre> </li> </ul>"}, {"location": "coding/python/data_classes/#usage", "title": "Usage", "text": ""}, {"location": "coding/python/data_classes/#definition", "title": "Definition", "text": "<pre><code>from dataclasses import dataclass\n\n@dataclass\nclass Position:\n    name: str\n    lon: float\n    lat: float\n</code></pre> <p>What makes this a data class is the <code>@dataclass</code> decorator. Beneath the <code>class Position:</code>, simply list the fields you want in your data class.</p> <p>The data class decorator support the following parameters:</p> <ul> <li><code>init</code>: Add <code>.__init__()</code> method? (Default is True).</li> <li><code>repr</code>: Add <code>.__repr__()</code> method? (Default is True).</li> <li><code>eq</code>: Add <code>.__eq__()</code> method? (Default is True).</li> <li><code>order</code>: Add ordering methods? (Default is False).</li> <li><code>unsafe_hash</code>: Force the addition of a <code>.__hash__()</code> method? (Default is     False).</li> <li><code>frozen</code>: If <code>True</code>, assigning to fields raise an exception. (Default is     False).</li> </ul>"}, {"location": "coding/python/data_classes/#default-values", "title": "Default values", "text": "<p>It's easy to add default values to the fields of your data class:</p> <pre><code>from dataclasses import dataclass\n\n@dataclass\nclass Position:\n    name: str\n    lon: float = 0.0\n    lat: float = 0.0\n</code></pre> <p>More complex default values can be defined through the use of functions. For example, the next snippet builds a French deck:</p> <pre><code>from dataclasses import dataclass, field\nfrom typing import List\n\nRANKS = '2 3 4 5 6 7 8 9 10 J Q K A'.split()\nSUITS = '\u2663 \u2662 \u2661 \u2660'.split()\n\ndef make_french_deck():\n    return [PlayingCard(r, s) for s in SUITS for r in RANKS]\n\n\n@dataclass\nclass PlayingCard:\n    rank: str\n    suit: str\n\n\n@dataclass\nclass Deck:\n    cards: List[PlayingCard] = field(default_factory=make_french_deck)\n</code></pre> <p>Using <code>cards: List[PlayingCard] = make_french_deck()</code> introduces the using mutable default arguments anti-pattern. Instead, data classes use the <code>default_factory</code> to handle mutable default values. To use it, you need to use the <code>field()</code> specifier which is used to customize each field of a data class individually. It supports the following parameters:</p> <ul> <li><code>default</code>: Default value of the field.</li> <li><code>default_factory</code>: Function that returns the initial value of the field.</li> <li><code>init</code>: Use field in <code>.__init__()</code> method? (Default is <code>True</code>).</li> <li><code>repr</code>: Use field in <code>repr</code> of the object? (Default is <code>True</code>).     For example to hide a parameter from the <code>repr</code>, use <code>lat: float     = field(default=0.0, repr=False)</code>.</li> <li><code>compare</code>: Include the field in comparisons? (Default is <code>True</code>).</li> <li><code>hash</code>: Include the field when calculating <code>hash()</code>? (Default is to use the     same as <code>compare</code>).</li> <li> <p><code>metadata</code>: A mapping with information about the field. It's not used by the     data classes themselves but is available for you to attach information to     fields. For example:</p> <pre><code>from dataclasses import dataclass, field\n\n@dataclass\nclass Position:\n    name: str\n    lon: float = field(default=0.0, metadata={'unit': 'degrees'})\n    lat: float = field(default=0.0, metadata={'unit': 'degrees'})\n</code></pre> <p>To retrieve the information use the <code>fields()</code> function.</p> <pre><code>&gt;&gt;&gt; from dataclasses import fields\n&gt;&gt;&gt; fields(Position)\n(Field(name='name',type=&lt;class 'str'&gt;,...,metadata={}),\n Field(name='lon',type=&lt;class 'float'&gt;,...,metadata={'unit': 'degrees'}),\n Field(name='lat',type=&lt;class 'float'&gt;,...,metadata={'unit': 'degrees'}))\n&gt;&gt;&gt; lat_unit = fields(Position)[2].metadata['unit']\n&gt;&gt;&gt; lat_unit\n'degrees'\n</code></pre> </li> </ul>"}, {"location": "coding/python/data_classes/#type-hints", "title": "Type hints", "text": "<p>They support typing out of the box. Without a type hint, the field will not be a part of the data class.</p> <p>While you need to add type hints in some form when using data classes, these types are not enforced at runtime. This is how typing in python usually works: Python is and will always be a dynamically typed language.</p>"}, {"location": "coding/python/data_classes/#adding-methods", "title": "Adding methods", "text": "<p>Same as with a normal class.</p>"}, {"location": "coding/python/data_classes/#adding-complex-order-comparison-logic", "title": "Adding complex order comparison logic", "text": "<pre><code>from dataclasses import dataclass\n\n@dataclass(order=True)\nclass PlayingCard:\n    rank: str\n    suit: str\n\n    def __str__(self):\n        return f'{self.suit}{self.rank}'\n</code></pre> <p>After setting <code>order=True</code> in the decorator definition the instances of <code>PlayingCard</code> can be compared.</p> <pre><code>&gt;&gt;&gt; queen_of_hearts = PlayingCard('Q', '\u2661')\n&gt;&gt;&gt; ace_of_spades = PlayingCard('A', '\u2660')\n&gt;&gt;&gt; ace_of_spades &gt; queen_of_hearts\nFalse\n</code></pre> <p>Data classes compare objects as if they were tuples of their fields. A Queen is higher than an Ace because <code>Q</code> comes after <code>A</code> in the alphabet.</p> <pre><code>&gt;&gt;&gt; ('A', '\u2660') &gt; ('Q', '\u2661')\nFalse\n</code></pre> <p>To use more complex comparisons, we need to add the field <code>.sort_index</code> to the class. However, this field should be calculated from the other fields automatically. That's what the special method <code>.__post_init__()</code> is for. It allows for special processing after the regular <code>.__init__()</code> method is called.</p> <pre><code>from dataclasses import dataclass, field\n\nRANKS = '2 3 4 5 6 7 8 9 10 J Q K A'.split()\nSUITS = '\u2663 \u2662 \u2661 \u2660'.split()\n\n@dataclass(order=True)\nclass PlayingCard:\n    sort_index: int = field(init=False, repr=False)\n    rank: str\n    suit: str\n\n    def __post_init__(self):\n        self.sort_index = (RANKS.index(self.rank) * len(SUITS)\n                           + SUITS.index(self.suit))\n\n    def __str__(self):\n        return f'{self.suit}{self.rank}'\n</code></pre> <p>Note that <code>.sort_index</code> is added as the first field of the class. That way, the comparison is first done using <code>.sort_index</code> and only if there are ties are the other fields used. Using <code>field()</code>, you must also specify that <code>.sort_index</code> should not be included as a parameter in the <code>.__init__()</code> method (because it is calculated from the <code>.rank</code> and <code>.suit</code> fields). To avoid confusing the user about this implementation detail, it is probably also a good idea to remove .sort_index from the <code>repr</code> of the class.</p>"}, {"location": "coding/python/data_classes/#immutable-data-classes", "title": "Immutable data classes", "text": "<p>To make a data class immutable, set <code>frozen=True</code> when you create it.</p> <pre><code>from dataclasses import dataclass\n\n@dataclass(frozen=True)\nclass Position:\n    name: str\n    lon: float = 0.0\n    lat: float = 0.0\n</code></pre> <p>In a frozen data class, you can not assign values to the fields after creation:</p> <p><pre><code>&gt;&gt;&gt; pos = Position('Oslo', 10.8, 59.9)\n&gt;&gt;&gt; pos.name\n'Oslo'\n&gt;&gt;&gt; pos.name = 'Stockholm'\ndataclasses.FrozenInstanceError: cannot assign to field 'name'\n</code></pre> Be aware though that if your data class contains mutable fields, those might still change. This is true for all nested data structures in Python:</p> <pre><code>from dataclasses import dataclass\nfrom typing import List\n\n@dataclass(frozen=True)\nclass ImmutableCard:\n    rank: str\n    suit: str\n\n@dataclass(frozen=True)\nclass ImmutableDeck:\n    cards: List[PlayingCard]\n</code></pre> <p>Even though both <code>ImmutableCard</code> and <code>ImmutableDeck</code> are immutable, the list holding cards is not. You can therefore still change the cards in the deck:</p> <pre><code>&gt;&gt;&gt; queen_of_hearts = ImmutableCard('Q', '\u2661')\n&gt;&gt;&gt; ace_of_spades = ImmutableCard('A', '\u2660')\n&gt;&gt;&gt; deck = ImmutableDeck([queen_of_hearts, ace_of_spades])\n&gt;&gt;&gt; deck\nImmutableDeck(cards=[ImmutableCard(rank='Q', suit='\u2661'), ImmutableCard(rank='A', suit='\u2660')])\n&gt;&gt;&gt; deck.cards[0] = ImmutableCard('7', '\u2662')\n&gt;&gt;&gt; deck\nImmutableDeck(cards=[ImmutableCard(rank='7', suit='\u2662'), ImmutableCard(rank='A', suit='\u2660')])\n</code></pre> <p>To avoid this, make sure all fields of an immutable data class use immutable types (but remember that types are not enforced at runtime). The <code>ImmutableDeck</code> should be implemented using a tuple instead of a list.</p>"}, {"location": "coding/python/data_classes/#inheritance", "title": "Inheritance", "text": "<p>You can subclass data classes quite freely.</p> <pre><code>from dataclasses import dataclass\n\n@dataclass\nclass Position:\n    name: str\n    lon: float\n    lat: float\n\n@dataclass\nclass Capital(Position):\n    country: str\n</code></pre> <pre><code>&gt;&gt;&gt; Capital('Oslo', 10.8, 59.9, 'Norway')\nCapital(name='Oslo', lon=10.8, lat=59.9, country='Norway')\n</code></pre> <p>Warning</p> <p>This won't work if the base class have default values unless all the subclass parameters also have default values.</p> <p>Warning</p> <p>If you redefine a base class field, you need to keep the fields order after the subclass new fields:</p> <pre><code>from dataclasses import dataclass\n\n@dataclass\nclass Position:\n    name: str\n    lon: float = 0.0\n    lat: float = 0.0\n\n@dataclass\nclass Capital(Position):\n    country: str = 'Unknown'\n    lat: float = 40.0\n</code></pre>"}, {"location": "coding/python/data_classes/#optimizing-data-classes", "title": "Optimizing Data Classes", "text": "<p>Slots can be used to make classes faster and use less memory.</p> <p><pre><code>from dataclasses import dataclass\n\n@dataclass\nclass SimplePosition:\n    name: str\n    lon: float\n    lat: float\n\n@dataclass\nclass SlotPosition:\n    __slots__ = ['name', 'lon', 'lat']\n    name: str\n    lon: float\n    lat: float\n</code></pre> Essentially, slots are defined using <code>.__slots__</code> to list the variables on a class. Variables or attributes not present in <code>.__slots__</code> may not be defined. Furthermore, a slots class may not have default values.</p>"}, {"location": "coding/python/data_classes/#references", "title": "References", "text": "<ul> <li>Real Python Data classes article</li> </ul>"}, {"location": "coding/python/deepdiff/", "title": "Deepdiff", "text": "<p>The DeepDiff library is used to perform search and differences in Python objects. It comes with three operations:</p> <ul> <li>DeepDiff: Deep Difference of dictionaries, iterables, strings and other     objects. It will recursively look for all the changes.</li> <li>DeepSearch: Search for objects within other objects.</li> <li>DeepHash: Hash any object based on their content even if they are not     \u201chashable\u201d.</li> </ul>"}, {"location": "coding/python/deepdiff/#install", "title": "Install", "text": "<p>Install from PyPi:</p> <pre><code>pip install deepdiff\n</code></pre>"}, {"location": "coding/python/deepdiff/#deepsearch", "title": "DeepSearch", "text": "<p>Deep Search inside objects to find the item matching your criteria.</p> <p>Note that is searches for either the path to match your criteria or the word in an item.</p> <p>Examples:</p> <ul> <li> <p>Importing</p> <pre><code>from deepdiff import DeepSearch, grep\n</code></pre> </li> </ul> <p>DeepSearch comes with <code>grep</code> function which is easier to remember!</p> <p>Search in list for string</p> <pre><code>&gt;&gt;&gt; obj = [\"long somewhere\", \"string\", 0, \"somewhere great!\"]\n&gt;&gt;&gt; item = \"somewhere\"\n&gt;&gt;&gt; ds = obj | grep(item, verbose_level=2)\n&gt;&gt;&gt; print(ds)\n{'matched_values': {'root[3]': 'somewhere great!', 'root[0]': 'long somewhere'}}\n</code></pre> <p>Search in nested data for string</p> <pre><code>&gt;&gt;&gt; obj = [\"something somewhere\", {\"long\": \"somewhere\", \"string\": 2, 0: 0, \"somewhere\": \"around\"}]\n&gt;&gt;&gt; item = \"somewhere\"\n&gt;&gt;&gt; ds = obj | grep(item, verbose_level=2)\n&gt;&gt;&gt; pprint(ds, indent=2)\n{ 'matched_paths': {\"root[1]['somewhere']\": 'around'},\n  'matched_values': { 'root[0]': 'something somewhere',\n                      \"root[1]['long']\": 'somewhere'}}\n</code></pre> <p>To obtain the keys and values of the matched objects, you can use the Extract object.</p> <pre><code>&gt;&gt;&gt; from deepdiff import grep\n&gt;&gt;&gt; obj = {1: [{'2': 'b'}, 3], 2: [4, 5]}\n&gt;&gt;&gt; result = obj | grep(5)\n&gt;&gt;&gt; result\n{'matched_values': OrderedSet(['root[2][1]'])}\n&gt;&gt;&gt; result['matched_values'][0]\n'root[2][1]'\n&gt;&gt;&gt; path = result['matched_values'][0]\n&gt;&gt;&gt; extract(obj, path)\n5\n</code></pre>"}, {"location": "coding/python/deepdiff/#references", "title": "References", "text": "<ul> <li>Source</li> <li>Homepage/Docs</li> <li>Old Docs</li> </ul>"}, {"location": "coding/python/docstrings/", "title": "Docstrings", "text": "<p>Docstrings are strings that define the purpose and use of a module, class, function or method. They are accessible from the doc attribute <code>__doc__</code> and with the built-in <code>help()</code> function.</p> <p>Documenting your code is very important as it's more often read than written.</p>"}, {"location": "coding/python/docstrings/#documenting-vs-commenting", "title": "Documenting vs commenting", "text": "<p>Commenting is describing your code to/for developers. The intended audience is the maintainers and developers of the Python code. In conjuction with well-written code, comments help to guide the reader to better understand your code and its purpose and design.</p> <p>Documenting is describing its use and functionality to your users. While it may be helpful in the development process, the main intended audience are the users.</p>"}, {"location": "coding/python/docstrings/#docstring-format", "title": "Docstring format", "text": "<p>Docstrings should use the triple-double quote (<code>\"\"\"</code>) string format. This should be done whether the docstring is multi-lined or not. At a bare minimum, a docstring should be a quick summary of whatever is it you\u2019re describing and should be contained within a single line.</p> <p>Multi-lined docstrings are used to further elaborate on the object beyond the summary. All multi-lined docstrings have the following parts:</p> <ul> <li>A one-line summary line</li> <li>A blank line proceeding the summary</li> <li>Any further elaboration for the docstring</li> <li>Another blank line</li> </ul> <p>To ensure your docstrings follow these practices, configure flakeheaven with the <code>flake8-docstrings</code> extension.</p>"}, {"location": "coding/python/docstrings/#docstring-types", "title": "Docstring types", "text": ""}, {"location": "coding/python/docstrings/#class-docstrings", "title": "Class docstrings", "text": "<p>Class Docstrings are created for the class itself, as well as any class methods. The docstrings are placed immediately following the class or class method indented by one level:</p> <p>class SimpleClass:     \"\"\"Class docstrings go here.\"\"\"</p> <pre><code>def say_hello(self, name: str):\n    \"\"\"Class method docstrings go here.\"\"\"\n    print(f'Hello {name}')\n</code></pre> <p>Class docstrings should contain the following information:</p> <ul> <li>A brief summary of its purpose and behavior</li> <li>Any public methods, along with a brief description</li> <li>Any class properties (attributes)</li> <li>Anything related to the interface for subclassers, if the class is intended to     be subclassed</li> </ul> <p>The class constructor parameters should be documented within the init class method docstring. Individual methods should be documented using their individual docstrings. Class method docstrings should contain the following:</p> <ul> <li>A brief description of what the method is and what it\u2019s used for</li> <li>Any arguments (both required and optional) that are passed including keyword arguments</li> <li>Label any arguments that are considered optional or have a default value</li> <li>Any side effects that occur when executing the method</li> <li>Any exceptions that are raised</li> <li>Any restrictions on when the method can be called</li> </ul>"}, {"location": "coding/python/docstrings/#package-and-module-docstrings", "title": "Package and module docstrings", "text": "<p>Package docstrings should be placed at the top of the package\u2019s <code>__init__.py</code> file. This docstring should list the modules and sub-packages that are exported by the package.</p> <p>Module docstrings should include the following:</p> <ul> <li>A brief description of the module and its purpose</li> <li>A list of any classes, exception, functions, and any other objects exported by     the module. Only needed if they are not defined in the same file, otherwise     <code>help()</code> will get them automatically.</li> </ul> <p>The docstring for a module function should include the same items as a class method.</p>"}, {"location": "coding/python/docstrings/#docstring-formats", "title": "Docstring formats", "text": "<ul> <li>Google     docstrings:     the most user friendly.</li> <li>reStructured Text: The official     ones, but super ugly to write.</li> <li>Numpy docstrings.</li> </ul>"}, {"location": "coding/python/docstrings/#google-docstrings", "title": "Google Docstrings", "text": "<p>Napoleon gathered a nice cheatsheet with examples.</p>"}, {"location": "coding/python/docstrings/#functions-and-methods", "title": "Functions and methods", "text": "<p>A method that overrides a method from a base class may have a simple docstring sending the reader to its overridden method\u2019s docstring, such as <code>\"\"\"See base class.\"\"\"</code>. The rationale is that there is no need to repeat in many places documentation that is already present in the base method\u2019s docstring. However, if the overriding method\u2019s behavior is substantially different from the overridden method, or details need to be provided (e.g., documenting additional side effects), a docstring with at least those differences is required on the overriding method.</p> <p>Certain aspects of a function should be documented in special sections, listed below. Each section begins with a heading line, which ends with a colon. All sections other than the heading should maintain a hanging indent of two or four spaces (be consistent within a file). These sections can be omitted in cases where the function\u2019s name and signature are informative enough that it can be aptly described using a one-line docstring.</p> Args List each parameter by name. A description should follow the name, and be separated by a colon followed by either a space or newline. If the description is too long to fit on a single 80-character line, use a hanging indent of 2 or 4 spaces more than the parameter name (be consistent with the rest of the docstrings in the file). The description should include required type(s) if the code does not contain a corresponding type annotation. If a function accepts <code>*foo</code> (variable length argument lists) and/or <code>**bar</code> (arbitrary keyword arguments), they should be listed as <code>*foo</code> and <code>**bar</code>. Returns (or Yields: for generators) Describe the type and semantics of the return value. If the function only returns None, this section is not required. It may also be omitted if the docstring starts with Returns or Yields (e.g. <code>\"\"\"Returns row from API as a tuple of strings.\"\"\"</code>) and the opening sentence is sufficient to describe return value. Raises List all exceptions that are relevant to the interface followed by a description. Use a similar exception name + colon + space or newline and hanging indent style as described in Args:. You should not document exceptions that get raised if the API specified in the docstring is violated (because this would paradoxically make behavior under violation of the API part of the API)."}, {"location": "coding/python/docstrings/#tests-docstrings", "title": "Tests docstrings", "text": ""}, {"location": "coding/python/docstrings/#without-template", "title": "Without template", "text": "<p>jml has very good tips on writing test's docstrings:</p> <p>If you\u2019re struggling to write docstrings for your tests, here\u2019s a handy five-step guide:</p> <ul> <li> <p>Write the first docstring that comes to mind. It will almost certainly be:</p> <pre><code>\"\"\"Test that input is parsed correctly.\"\"\"\n</code></pre> </li> <li> <p>Get rid of \u201cTest that\u201d or \u201cCheck that\u201d. We know it\u2019s a test.</p> <pre><code>\"\"\"Input should be parsed correctly.\"\"\"\n</code></pre> </li> <li> <p>Seriously?! Why\u2019d you have to go and add \u201cshould\u201d? It\u2019s a test, it\u2019s all about \u201cshould\u201d.</p> <pre><code>\"\"\"Input is parsed correctly.\"\"\"\n</code></pre> </li> <li> <p>\u201cCorrectly\u201d, \u201cproperly\u201d, and \u201cas we expect\u201d are all redundant. Axe them too.</p> <pre><code>\"\"\"Input is parsed.\"\"\"\n</code></pre> </li> <li> <p>Look at what\u2019s left. Is it saying anything at all? If so, great. If not,     consider adding something specific about the test behaviour and perhaps even     why it\u2019s desirable behaviour to have.</p> <pre><code>\"\"\"\nInput is parsed into an immutable dict according to the config\nschema, so we get config info without worrying about input\nvalidation all the time.\n\"\"\"\n</code></pre> </li> </ul>"}, {"location": "coding/python/docstrings/#given-when-then", "title": "Given When Then", "text": "<p>Given-When-Then is a style of representing test. It's an approach developed as part of Behavior-Driven Development (BDD).  It appears as a structuring approach for many testing frameworks such as Cucumber. You can also look at it as a reformulation of the Four-Phase Test pattern.</p> <p>The essential idea is to break down writing a scenario (or test) into three sections:</p> <ul> <li>The given part describes the state of the world before you begin the     behavior you're specifying in this scenario. You can think of it as the     pre-conditions to the test.</li> <li>The when section is that behavior that you're specifying.</li> <li>Finally the then section describes the changes you expect due to the     specified behavior.</li> </ul> <p>I already implement this test structure with the Arrange, Act, Assert structure, so it made sense to use it in the docstring too. The side effects is that you repeat a lot of prose where a single line would suffice.</p>"}, {"location": "coding/python/docstrings/#automatic-documentation-generation", "title": "Automatic documentation generation", "text": "<p>Use the mkdocstrings plugin to automatically generate the documentation.</p>"}, {"location": "coding/python/docstrings/#references", "title": "References", "text": "<ul> <li>Real Python post on docstrings by James Mertz</li> </ul>"}, {"location": "coding/python/factoryboy/", "title": "FactoryBoy", "text": "<p>Factoryboy is a fixtures replacement library to generate fake data for your program. As it's designed to work well with different ORMs (Django, SQLAlchemy, Mongo) it serves the purpose of building real objects for your tests.</p> <p>If you use pydantic, <code>pydantic-factories</code> does all this automatically for you!</p>"}, {"location": "coding/python/factoryboy/#install", "title": "Install", "text": "<pre><code>pip install factory_boy\n</code></pre> <p>Or add it to the project <code>requirements.txt</code>.</p>"}, {"location": "coding/python/factoryboy/#define-a-factory-class", "title": "Define a factory class", "text": "<p>Use the following code to generate a factory class for the <code>User</code> SQLAlchemy class.</p> <pre><code>from {{ program_name }} import models\n\nimport factory\n\n# XXX If you add new Factories remember to add the session in conftest.py\n\n\nclass UserFactory(factory.alchemy.SQLAlchemyModelFactory):\n\"\"\"\n    Class to generate a fake user element.\n    \"\"\"\n    id = factory.Sequence(lambda n: n)\n    name = factory.Faker('name')\n\n    class Meta:\n        model = models.User\n        sqlalchemy_session_persistence = 'commit'\n</code></pre> <p>As stated in the comment, and if you are using the proposed python project template, remember to add new Factories in <code>conftest.py</code>.</p>"}, {"location": "coding/python/factoryboy/#use-the-factory-class", "title": "Use the factory class", "text": "<ul> <li> <p>Create an instance.</p> <pre><code>UserFactory.create()\n</code></pre> </li> <li> <p>Create an instance with a defined attribute.</p> <pre><code>UserFactory.create(name='John')\n</code></pre> </li> <li> <p>Create 100 instances of objects with an attribute defined.</p> <pre><code>UserFactory.create_batch(100, name='John')\n</code></pre> </li> </ul>"}, {"location": "coding/python/factoryboy/#define-attributes", "title": "Define attributes", "text": "<p>I like to use the faker integration of factory boy to generate most of the attributes.</p>"}, {"location": "coding/python/factoryboy/#generate-numbers", "title": "Generate numbers", "text": ""}, {"location": "coding/python/factoryboy/#sequential-numbers", "title": "Sequential numbers", "text": "<p>Ideal for IDs</p> <pre><code>id = factory.Sequence(lambda n: n)\n</code></pre>"}, {"location": "coding/python/factoryboy/#random-number-or-integer", "title": "Random number or integer", "text": "<pre><code>author_id = factory.Faker('random_number')\n</code></pre> <p>If you want to limit the number of digits use <code>factory.Faker('random_number', digits=3)</code></p>"}, {"location": "coding/python/factoryboy/#random-float", "title": "Random float", "text": "<pre><code>score = factory.Faker('pyfloat')\n</code></pre>"}, {"location": "coding/python/factoryboy/#generate-strings", "title": "Generate strings", "text": ""}, {"location": "coding/python/factoryboy/#word", "title": "Word", "text": "<pre><code>default = factory.Faker('word')\n</code></pre>"}, {"location": "coding/python/factoryboy/#word-from-a-list", "title": "Word from a list", "text": "<pre><code>user = factory.Faker('word', ext_word_list=[None, 'value_1', 'value_2'])\n</code></pre>"}, {"location": "coding/python/factoryboy/#word-from-enum-choices", "title": "Word from Enum choices", "text": "<p>First install the Enum provider</p> <pre><code>factory.Faker.add_provider(EnumProvider)\n\nclass EntityFactory(factory.Factory):  # type: ignore\n    state = factory.Faker(\"enum\", enum_cls=EntityState)\n</code></pre>"}, {"location": "coding/python/factoryboy/#sentences", "title": "Sentences", "text": "<pre><code>description = factory.Faker('sentence')\n</code></pre>"}, {"location": "coding/python/factoryboy/#names", "title": "Names", "text": "<pre><code>name = factory.Faker('name')\n</code></pre>"}, {"location": "coding/python/factoryboy/#urls", "title": "Urls", "text": "<pre><code>url = factory.Faker('url')\n</code></pre>"}, {"location": "coding/python/factoryboy/#files", "title": "Files", "text": "<pre><code>file_path = factory.Faker('file_path')\n</code></pre>"}, {"location": "coding/python/factoryboy/#generate-datetime", "title": "Generate Datetime", "text": "<pre><code>factory.Faker('date_time')\n</code></pre>"}, {"location": "coding/python/factoryboy/#generate-bool", "title": "Generate bool", "text": "<pre><code>factory.Faker('pybool')\n</code></pre>"}, {"location": "coding/python/factoryboy/#generate-your-own-attributes", "title": "Generate your own attributes", "text": ""}, {"location": "coding/python/factoryboy/#using-custom-faker-providers", "title": "Using custom Faker providers", "text": ""}, {"location": "coding/python/factoryboy/#using-lazy_attributes", "title": "Using lazy_attributes", "text": "<p>Use <code>lazy_attribute</code> decorator.</p> <p>If you want to use Faker inside a lazy_attribute use <code>.generate({})</code> at the end of the attribute.</p> <p>In newer versions of Factoryboy you can't use Faker inside a lazy attribute</p> <p>As the Faker object doesn't have the generate method.</p> <pre><code>    @factory.lazy_attribute\n    def due(self):\n        if random.random() &gt; 0.5:\n            return factory.Faker('date_time').generate({})\n</code></pre>"}, {"location": "coding/python/factoryboy/#define-relationships", "title": "Define relationships", "text": ""}, {"location": "coding/python/factoryboy/#factory-inheritance", "title": "Factory Inheritance", "text": "<pre><code>class ContentFactory(factory.alchemy.SQLAlchemyModelFactory):\n    id = factory.Sequence(lambda n: n)\n    title = factory.Faker('sentence')\n\n    class Meta:\n        model = models.Content\n        sqlalchemy_session_persistence = 'commit'\n\n\nclass ArticleFactory(ContentFactory):\n    body = factory.Faker('sentence')\n\n    class Meta:\n        model = models.Article\n        sqlalchemy_session_persistence = 'commit'\n</code></pre>"}, {"location": "coding/python/factoryboy/#dependent-objects-direct-foreignkey", "title": "Dependent objects direct ForeignKey", "text": "<p>When one attribute is actually a complex field (e.g a ForeignKey to another Model), use the SubFactory declaration. Assuming the following model definition:</p> <pre><code>class Author(Base):\n    id = Column(String, primary_key=True)\n    contents = relationship('Content', back_populates='author')\n\n\nclass Content(Base):\n    id = Column(Integer, primary_key=True, doc='Content ID')\n    author_id = Column(String, ForeignKey(Author.id))\n    author = relationship(Author, back_populates='contents')\n</code></pre> <p>The related factories would be:</p> <pre><code>class AuthorFactory(factory.alchemy.SQLAlchemyModelFactory):\n    id = factory.Faker('word')\n\n    class Meta:\n        model = models.Author\n        sqlalchemy_session_persistence = 'commit'\n\n\nclass ContentFactory(factory.alchemy.SQLAlchemyModelFactory):\n    id = factory.Sequence(lambda n: n)\n    author = factory.SubFactory(AuthorFactory)\n\n    class Meta:\n        model = models.Content\n        sqlalchemy_session_persistence = 'commit'\n</code></pre>"}, {"location": "coding/python/factoryboy/#automatically-generate-a-factory-from-a-pydantic-model", "title": "Automatically generate a factory from a pydantic model", "text": "<p>Sadly it's not yet supported, it will at some point though. If you're interested in following this path, you can start with mgaitan snippet for dataclasses.</p>"}, {"location": "coding/python/factoryboy/#references", "title": "References", "text": "<ul> <li>Docs</li> <li>Git</li> <li>Common recipes</li> </ul>"}, {"location": "coding/python/faker/", "title": "Faker", "text": "<p>Faker is a Python package that generates fake data for you. Whether you need to bootstrap your database, create good-looking XML documents, fill-in your persistence to stress test it, or anonymize data taken from a production service, Faker is for you.</p>"}, {"location": "coding/python/faker/#install", "title": "Install", "text": "<p>If you use factoryboy you'd probably have it. If you don't use</p> <pre><code>pip install faker\n</code></pre> <p>Or add it to the project <code>requirements.txt</code>.</p>"}, {"location": "coding/python/faker/#use", "title": "Use", "text": "<p><code>Faker</code> includes a <code>faker</code> fixture for pytest.</p> <pre><code>def test_faker(faker):\n    assert isinstance(faker.name(), str)\n</code></pre> <p>By default it's populated with a seed of <code>0</code>, to set a random seed add the following to your test configuration.</p> <p>File: conftest.py</p> <pre><code>from random import SystemRandom\n\n\n@pytest.fixture(scope=\"session\", autouse=True)\ndef faker_seed() -&gt; int:\n\"\"\"Create a random seed for the Faker library.\"\"\"\n    return SystemRandom().randint(0, 999999)\n</code></pre>"}, {"location": "coding/python/faker/#generate-fake-number", "title": "Generate fake number", "text": "<pre><code>fake.random_number()\n</code></pre> <p>If you want to specify max and min values use:</p> <pre><code>faker.pyint(min_value=0, max_value=99)\n</code></pre>"}, {"location": "coding/python/faker/#generate-a-fake-dictionary", "title": "Generate a fake dictionary", "text": "<pre><code>fake.pydict(nb_elements=5, variable_nb_elements=True, *value_types)\n</code></pre> <p>Where <code>*value_types</code> can be <code>'str', 'list'</code></p>"}, {"location": "coding/python/faker/#generate-a-fake-date", "title": "Generate a fake date", "text": "<pre><code>fake.date_time()\n</code></pre>"}, {"location": "coding/python/faker/#generate-a-random-string", "title": "Generate a random string", "text": "<pre><code>faker.pystr()\n</code></pre>"}, {"location": "coding/python/faker/#create-a-random-string-with-a-defined-format", "title": "Create a random string with a defined format", "text": "<pre><code>faker.pystr_format(\"id-#######{{random_letter}}\")\n'id-6443059M'\n</code></pre>"}, {"location": "coding/python/faker/#create-an-ip-address", "title": "Create an IP address", "text": "<pre><code>faker.ipv4()\n</code></pre> <p>If you want a CIDR, use <code>network=True</code>.</p>"}, {"location": "coding/python/faker/#create-a-random-choice-from-an-enum", "title": "Create a random choice from an Enum", "text": "<p>pydantic uses <code>Enum</code> objects to define the choices of fields, so we need them to create the factories of those objects.</p> <p>Sadly, there is no official provider for <code>Enums</code>, but NazarioJL made a custom provider.</p>"}, {"location": "coding/python/faker/#install_1", "title": "Install", "text": "<pre><code>pip install faker-enum\n</code></pre>"}, {"location": "coding/python/faker/#usage", "title": "Usage", "text": "<pre><code>from enum import Enum\n\nfrom faker import Faker\nfrom faker_enum import EnumProvider\n\nfake = Faker()\nfake.add_provider(EnumProvider)\n\nclass Color(Enum):\n    RED = 1\n    GREEN = 2\n    BLUE = 3\n\nfake.enum(Color)\n# One of [Color.RED, Color.GREEN, Color.BLUE]\n</code></pre> <p>If you're using factoryboy, check this instructions.</p>"}, {"location": "coding/python/faker/#create-optional-data", "title": "Create <code>Optional</code> data", "text": ""}, {"location": "coding/python/faker/#install_2", "title": "Install", "text": "<pre><code>pip install fake-optional\n</code></pre>"}, {"location": "coding/python/faker/#usage_1", "title": "Usage", "text": "<pre><code>from faker import Faker\nfrom faker_optional import OptionalProvider\n\nfake = Faker()\nfake.add_provider(OptionalProvider)\n\nfake.optional_int()\n# None\n\nfake.optional_int()\n# 1234\n</code></pre> <p><code>OptionalProvider</code> uses existent faker providers to create the data, so you can use the provider method arguments.</p> <p>For example, <code>optional_int</code> uses the <code>python provider pyint</code>, so you can use the <code>min_value</code>, <code>max_value</code>, and <code>step</code> arguments. Every <code>optional_</code> method accepts the float <code>ratio</code> argument between <code>0</code> and <code>1</code>, with a default value of <code>0.5</code> to define what percent of results should be <code>None</code>, a greater value will mean that less results will be <code>None</code>.</p> <p>Check the supported methods.</p>"}, {"location": "coding/python/faker/#generate-your-own-custom-provider", "title": "Generate your own custom provider", "text": "<p>Providers are just classes which define the methods we call on <code>Faker</code> objects to generate fake data.</p> <p>To define a provider, you need to create a class that inherits from the <code>BaseProvider</code>. That class can then define as many methods as you want.</p> <p>Once your provider is ready, add it to your <code>Faker</code> instance.</p> <pre><code>import random\n\nfrom faker import Faker\nfrom faker.providers import BaseProvider\n\nfake = Faker()\n\n# Our custom provider inherits from the BaseProvider\nclass TravelProvider(BaseProvider):\n    def destination(self):\n        destinations = ['NY', 'CO', 'CA', 'TX', 'RI']\n\n        # We select a random destination from the list and return it\n        return random.choice(destinations)\n\n# Add the TravelProvider to our faker object\nfake.add_provider(TravelProvider)\n\n# We can now use the destination method:\nprint(fake.destination())\n</code></pre> <p>If you want to give arguments when calling the provider, add them to the provider method.</p>"}, {"location": "coding/python/faker/#references", "title": "References", "text": "<ul> <li>Git</li> <li>Docs</li> <li>Faker python    providers</li> </ul>"}, {"location": "coding/python/feedparser/", "title": "Feedparser", "text": "<p>Parse Atom and RSS feeds in Python.</p>"}, {"location": "coding/python/feedparser/#install", "title": "Install", "text": "<pre><code>pip install feedparser\n</code></pre>"}, {"location": "coding/python/feedparser/#basic-usage", "title": "Basic Usage", "text": ""}, {"location": "coding/python/feedparser/#parsing-content", "title": "Parsing content", "text": ""}, {"location": "coding/python/feedparser/#parse-a-feed-from-a-remote-url", "title": "Parse a feed from a remote URL", "text": "<pre><code>&gt;&gt;&gt; import feedparser\n&gt;&gt;&gt; d = feedparser.parse('http://feedparser.org/docs/examples/atom10.xml')\n&gt;&gt;&gt; d['feed']['title']\nu'Sample Feed'\n</code></pre>"}, {"location": "coding/python/feedparser/#parse-a-feed-from-a-string", "title": "Parse a feed from a string", "text": "<pre><code>&gt;&gt;&gt; import feedparser\n&gt;&gt;&gt; rawdata = \"\"\"&lt;rss version=\"2.0\"&gt;\n&lt;channel&gt;\n&lt;title&gt;Sample Feed&lt;/title&gt;\n&lt;/channel&gt;\n&lt;/rss&gt;\"\"\"\n&gt;&gt;&gt; d = feedparser.parse(rawdata)\n&gt;&gt;&gt; d['feed']['title']\nu'Sample Feed'\n</code></pre>"}, {"location": "coding/python/feedparser/#access-common-elements", "title": "Access common elements", "text": "<p>The most commonly used elements in RSS feeds (regardless of version) are title, link, description, publication date, and entry ID.</p>"}, {"location": "coding/python/feedparser/#channel-elements", "title": "Channel elements", "text": "<p><pre><code>&gt;&gt;&gt; d.feed.title\nu'Sample Feed'\n&gt;&gt;&gt; d.feed.link\nu'http://example.org/'\n&gt;&gt;&gt; d.feed.description\nu'For documentation &lt;em&gt;only&lt;/em&gt;'\n&gt;&gt;&gt; d.feed.published\nu'Sat, 07 Sep 2002 00:00:01 GMT'\n&gt;&gt;&gt; d.feed.published_parsed\n(2002, 9, 7, 0, 0, 1, 5, 250, 0)\n</code></pre> All parsed dates can be converted to datetime with the following snippet:</p> <pre><code>from time import mktime\nfrom datetime import datetime\ndt = datetime.fromtimestamp(mktime(item['updated_parsed']))\n</code></pre>"}, {"location": "coding/python/feedparser/#item-elements", "title": "Item elements", "text": "<p><pre><code>&gt;&gt;&gt; d.entries[0].title\nu'First item title'\n&gt;&gt;&gt; d.entries[0].link\nu'http://example.org/item/1'\n&gt;&gt;&gt; d.entries[0].description\nu'Watch out for &lt;span&gt;nasty tricks&lt;/span&gt;'\n&gt;&gt;&gt; d.entries[0].published\nu'Thu, 05 Sep 2002 00:00:01 GMT'\n&gt;&gt;&gt; d.entries[0].published_parsed\n(2002, 9, 5, 0, 0, 1, 3, 248, 0)\n&gt;&gt;&gt; d.entries[0].id\nu'http://example.org/guid/1'\n</code></pre> An RSS feed can specify a small image which some aggregators display as a logo.</p> <pre><code>&gt;&gt;&gt; d.feed.image\n{'title': u'Example banner',\n'href': u'http://example.org/banner.png',\n'width': 80,\n'height': 15,\n'link': u'http://example.org/'}\n</code></pre> <p>Feeds and entries can be assigned to multiple categories, and in some versions of RSS, categories can be associated with a \u201cdomain\u201d.</p> <pre><code>&gt;&gt;&gt; d.feed.categories\n[(u'Syndic8', u'1024'),\n(u'dmoz', 'Top/Society/People/Personal_Homepages/P/')]\n</code></pre> <p>As feeds in the real world may be missing some elements, you may want to test for the existence of an element before getting its value.</p> <pre><code>&gt;&gt;&gt; import feedparser\n&gt;&gt;&gt; d = feedparser.parse('http://feedparser.org/docs/examples/atom10.xml')\n&gt;&gt;&gt; 'title' in d.feed\nTrue\n&gt;&gt;&gt; 'ttl' in d.feed\nFalse\n&gt;&gt;&gt; d.feed.get('title', 'No title')\nu'Sample feed'\n&gt;&gt;&gt; d.feed.get('ttl', 60)\n60\n</code></pre>"}, {"location": "coding/python/feedparser/#advanced-usage", "title": "Advanced usage", "text": "<p>It is possible to interact with feeds that are protected with credentials.</p>"}, {"location": "coding/python/feedparser/#issues", "title": "Issues", "text": "<ul> <li>Deprecation warning when using     <code>updated_parsed</code>, once     solved tweak the <code>airss/adapters/extractor.py#RSS.get</code> at <code>updated_at</code>.</li> </ul>"}, {"location": "coding/python/feedparser/#links", "title": "Links", "text": "<ul> <li>Git</li> <li>Docs</li> </ul>"}, {"location": "coding/python/flask/", "title": "Flask", "text": "<p>Flask is a micro web framework written in Python. It has no database abstraction layer, form validation, or any other components where pre-existing third-party libraries provide common functions. However, Flask supports extensions that can add application features as if they were implemented in Flask itself. Extensions exist for object-relational mappers, form validation, upload handling, various open authentication technologies and several common framework related tools. Extensions are updated far more frequently than the core Flask program.</p>"}, {"location": "coding/python/flask/#install", "title": "Install", "text": "<pre><code>pip install flask\n</code></pre>"}, {"location": "coding/python/flask/#how-flask-blueprints-work", "title": "How flask blueprints work", "text": "<p>A blueprint is an object that defines a collection of views, templates, static files and other elements that can be applied to an application. The killer use-case for blueprints is to organize our application into distinct components. However, a Flask Blueprint needs to be registered in an application before you can run it. Once it's done, it extends the application with the contents of the Blueprint.</p>"}, {"location": "coding/python/flask/#making-a-flask-blueprint", "title": "Making a Flask Blueprint", "text": ""}, {"location": "coding/python/flask/#references", "title": "References", "text": "<ul> <li>Docs</li> </ul>"}, {"location": "coding/python/flask/#tutorials", "title": "Tutorials", "text": "<ul> <li>Miguel's Flask Mega-Tutorial</li> <li>Patrick's Software Flask Tutorial</li> </ul>"}, {"location": "coding/python/flask/#blueprints", "title": "Blueprints", "text": "<ul> <li>Flask docs on blueprints</li> <li>Explore flask article on Blueprints</li> </ul>"}, {"location": "coding/python/flask_restplus/", "title": "Flask-RESTplus", "text": "<p>Use FastAPI instead</p> <p>Flask-RESTPlus is an extension for Flask that adds support for quickly building REST APIs. Flask-RESTPlus encourages best practices with minimal setup. If you are familiar with Flask, Flask-RESTPlus should be easy to pick up. It provides a coherent collection of decorators and tools to describe your API and expose its documentation properly using Swagger.</p> <p>Over plain flask it adds the following capabilities:</p> <ul> <li>It defines namespaces, which are ways of creating prefixes and structuring     the code: This helps long-term maintenance and helps with the design when     creating new endpoints.</li> <li>It has a full solution for parsing input parameters: This means that we have     an easy way of dealing with endpoints that requires several parameters     and validates them.</li> <li>It has a serialization framework for the resulting objects: This helps to     define objects that can be reused, clarifying the interface and simplifying     the development.</li> <li>It has full Swagger API documentation support.</li> </ul>"}, {"location": "coding/python/flask_restplus/#install", "title": "Install", "text": "<pre><code>pip install flask-restplus\n</code></pre>"}, {"location": "coding/python/flask_restplus/#references", "title": "References", "text": "<ul> <li>Docs</li> <li>Git</li> </ul>"}, {"location": "coding/python/folium/", "title": "Folium", "text": "<p>folium makes it easy to visualize data that\u2019s been manipulated in Python on an interactive leaflet map. It enables both the binding of data to a map for choropleth visualizations as well as passing rich vector/raster/HTML visualizations as markers on the map.</p> <p>The library has a number of built-in tilesets from OpenStreetMap, Mapbox, and Stamen, and supports custom tilesets with Mapbox or Cloudmade API keys. folium supports both Image, Video, GeoJSON and TopoJSON overlays.</p> <p>Use dash-leaflet if you want to do complex stuff.</p> <p>If you want to do multiple filters on the plotted data or connect the map with other graphics, use dash-leaflet instead.</p>"}, {"location": "coding/python/folium/#install", "title": "Install", "text": "<p>Although you can install it with <code>pip install folium</code> their release pace is slow, therefore I recommend pulling it directly from <code>master</code></p> <pre><code>pip install git+https://github.com/python-visualization/folium.git@master\n</code></pre> <p>It's a heavy repo, so it might take some time.</p>"}, {"location": "coding/python/folium/#usage", "title": "Usage", "text": "<p>Use the following snippet to create an empty map centered in Spain</p> <pre><code>import folium\n\nm = folium.Map(\n    location=[40.0884, -3.68042],\n    zoom_start=7,\n)\n\n# Map configuration goes here\n\nm.save(\"map.html\")\n</code></pre> <p>From now on, those lines are going to be assumed, and all the snippets are supposed to go in between the definition and the save.</p> <p>If you need to center the map elsewhere, I suggest that you configure the map to show the coordinates of the mouse with</p> <pre><code>from folium.plugins import MousePosition\n\nMousePosition().add_to(m)\n</code></pre>"}, {"location": "coding/python/folium/#change-tileset", "title": "Change tileset", "text": "<p>Folium Map object supports different tiles by default. It also supports any WMS or WMTS by passing a Leaflet-style URL to the tiles parameter: <code>http://{s}.yourtiles.com/{z}/{x}/{y}.png</code>.</p> <p>To use the IGN beautiful map as a fallback and the OpenStreetMaps as default use the following snippet:</p> <pre><code>m = folium.Map(\n    location=[40.0884, -3.68042],\n    zoom_start=7,\n    tiles=None,\n)\nfolium.raster_layers.TileLayer(\n    name=\"OpenStreetMaps\",\n    tiles=\"OpenStreetMap\",\n    attr=\"OpenStreetMaps\",\n).add_to(m)\n\n\nfolium.raster_layers.TileLayer(\n    name=\"IGN\",\n    tiles=\"https://www.ign.es/wmts/mapa-raster?request=getTile&amp;layer=MTN&amp;TileMatrixSet=GoogleMapsCompatible&amp;TileMatrix={z}&amp;TileCol={x}&amp;TileRow={y}&amp;format=image/jpeg\",\n    attr=\"IGN\",\n).add_to(m)\n\nfolium.LayerControl().add_to(m)\n</code></pre> <p>We need to set the <code>tiles=None</code> in the Map definition so both are shown in the LayerControl menu.</p>"}, {"location": "coding/python/folium/#loading-the-data", "title": "Loading the data", "text": ""}, {"location": "coding/python/folium/#using-geojson", "title": "Using geojson", "text": "<pre><code>folium.GeoJson(\"my_map.geojson\", name=\"geojson\").add_to(m)\n</code></pre> <p>If you don't want the data to be embedded in the html use <code>embed=False</code>, this can be handy if you don't want to redeploy the web application on each change of data. You'll need to supply a valid url to the data file.</p> <p>The downside (as of today) of using geojson is that you can't have different markers for the data. The solution is to load it and iterate over the elements. See the issue for more information.</p>"}, {"location": "coding/python/folium/#using-gpx", "title": "Using gpx", "text": "<p>Another popular data format is gpx, which is the one that OsmAnd uses. To import it we'll use the gpxpy library.</p> <pre><code>import gpxpy\nimport gpxpy.gpx\n\ngpx_file = open('map.gpx', 'r')\ngpx = gpxpy.parse(gpx_file)\n</code></pre>"}, {"location": "coding/python/folium/#references", "title": "References", "text": "<ul> <li>Git</li> <li>Docs</li> <li>Quickstart</li> <li>Examples,     more examples</li> </ul>"}, {"location": "coding/python/folium/#use-examples", "title": "Use examples", "text": "<ul> <li>Flask example</li> <li>Build Interactive GPS activity maps from GPX files</li> <li>Use Open Data to build interactive maps</li> </ul>"}, {"location": "coding/python/folium/#search-examples", "title": "Search examples", "text": "<ul> <li>Official docs</li> <li>Leafleft search control</li> <li>Leafleft search     examples(source     code)</li> <li>Searching in OSM data</li> </ul> <p>It seems that it doesn't yet support searching for multiple attributes in the geojson data</p>"}, {"location": "coding/python/gitpython/", "title": "GitPython", "text": "<p>GitPython is a python library used to interact with git repositories, high-level like git-porcelain, or low-level like git-plumbing.</p> <p>It provides abstractions of git objects for easy access of repository data, and additionally allows you to access the git repository more directly using either a pure python implementation, or the faster, but more resource intensive git command implementation.</p> <p>The object database implementation is optimized for handling large quantities of objects and large datasets, which is achieved by using low-level structures and data streaming.</p>"}, {"location": "coding/python/gitpython/#installation", "title": "Installation", "text": "<pre><code>pip install GitPython\n</code></pre>"}, {"location": "coding/python/gitpython/#usage", "title": "Usage", "text": ""}, {"location": "coding/python/gitpython/#initialize-a-repository", "title": "Initialize a repository", "text": "<pre><code>from git import Repo\n\nrepo = Repo.init(\"path/to/initialize\")\n</code></pre> <p>If you want to get the working directory of the <code>Repo</code> object use the <code>working_dir</code> attribute.</p>"}, {"location": "coding/python/gitpython/#load-a-repository", "title": "Load a repository", "text": "<pre><code>from git import Repo\n\nrepo = Repo(\"existing/git/repo/path\")\n</code></pre>"}, {"location": "coding/python/gitpython/#clone-a-repository", "title": "Clone a repository", "text": "<pre><code>from git import Repo\n\nRepo.clone_from(git_url, repo_dir)\n</code></pre>"}, {"location": "coding/python/gitpython/#make-a-commit", "title": "Make a commit", "text": "<p>Given a <code>repo</code> object:</p> <pre><code>index = repo.index\n\n# add the changes\nindex.add([\"README.md\"])\n\nfrom git import Actor\n\nauthor = Actor(\"An author\", \"author@example.com\")\ncommitter = Actor(\"A committer\", \"committer@example.com\")\n# commit by commit message and author and committer\nindex.commit(\"my commit message\", author=author, committer=committer)\n</code></pre>"}, {"location": "coding/python/gitpython/#change-commit-date", "title": "Change commit date", "text": "<p>When building fake data, creating commits in other points in time is useful.</p> <pre><code>import datetime\nfrom dateutil import tz\n\ncommit_date = (datetime.datetime(2020, 2, 2, tzinfo=tz.tzlocal()),)\n\nindex.commit(\n    \"my commit message\",\n    author=author,\n    committer=committer,\n    author_date=commit_date,\n    commit_date=commit_date,\n)\n</code></pre>"}, {"location": "coding/python/gitpython/#inspect-the-history", "title": "Inspect the history", "text": "<p>You first need to select the branch you want to inspect. To use the default repository branch use <code>head.reference</code>.</p> <pre><code>repo.head.reference\n</code></pre> <p>Then you can either get all the <code>Commit</code> objects of that reference, or inspect the log.</p>"}, {"location": "coding/python/gitpython/#get-all-commits-of-a-branch", "title": "Get all commits of a branch", "text": "<pre><code>[commit for commit in repo.iter_commits(rev=repo.head.reference)]\n</code></pre> <p>It gives you a List of commits where the first element is the last commit in time.</p>"}, {"location": "coding/python/gitpython/#inspect-the-log", "title": "Inspect the log", "text": "<p>Inspect it with the <code>repo.head.reference.log()</code>, which contains a list of <code>RefLogEntry</code> objects that have the interesting attributes:</p> <ul> <li><code>actor</code>: Actor object of the author of the commit</li> <li><code>time</code>: The commit timestamp, to load it as a datetime object use the   <code>datetime.datetime.fromtimestamp</code> method</li> <li><code>message</code>: Message as a string.</li> </ul>"}, {"location": "coding/python/gitpython/#create-a-branch", "title": "Create a branch", "text": "<pre><code>new_branch = repo.create_head(\"new_branch\")\nassert repo.active_branch != new_branch  # It's not checked out yet\nrepo.head.reference = new_branch\nassert not repo.head.is_detached\n</code></pre>"}, {"location": "coding/python/gitpython/#get-the-latest-commit-of-a-repository", "title": "Get the latest commit of a repository", "text": "<pre><code>repo.head.object.hexsha\n</code></pre>"}, {"location": "coding/python/gitpython/#testing", "title": "Testing", "text": "<p>There is no testing functionality, you need to either Mock, build fake data or fake adapters.</p>"}, {"location": "coding/python/gitpython/#build-fake-data", "title": "Build fake data", "text": "<p>Create a <code>test_data</code> directory in your testing directory with the contents of the git repository you want to test. Don't initialize it, we'll create a <code>repo</code> fixture that does it. Assuming that the data is in <code>tests/assets/test_data</code>:</p> <p>File <code>tests/conftest.py</code>:</p> <pre><code>import shutil\n\nimport pytest\nfrom git import Repo\n\n\n@pytest.fixture(name=\"repo\")\ndef repo_(tmp_path: Path) -&gt; Repo:\n\"\"\"Create a git repository with fake data and history.\n\n    Args:\n        tmp_path: Pytest fixture that creates a temporal Path\n    \"\"\"\n    # Copy the content from `tests/assets/test_data`.\n    repo_path = tmp_path / \"test_data\"\n    shutil.copytree(\"tests/assets/test_data\", repo_path)\n\n    # Initializes the git repository.\n    return Repo.init(repo_path)\n</code></pre> <p>On each test you can add the commits that you need for your use case.</p> <pre><code>author = Actor(\"An author\", \"author@example.com\")\ncommitter = Actor(\"A committer\", \"committer@example.com\")\n\n\n@pytest.mark.freeze_time(\"2021-02-01T12:00:00\")\ndef test_repo_is_not_empty(repo: Repo) -&gt; None:\n    commit_date = datetime.datetime(2021, 2, 1, tzinfo=tz.tzlocal())\n    repo.index.add([\"mkdocs.yml\"])\n    repo.index.commit(\n        \"Initial skeleton\",\n        author=author,\n        committer=committer,\n        author_date=commit_date,\n        commit_date=commit_date,\n    )\n\n    assert not repo.bare\n</code></pre> <p>If you feel that the tests are too verbose, you can create a fixture with all the commits done, and select each case with the freezegun pytest fixture. In my opinion, it will make the tests less clear though. The fixture can look like:</p> <p>File: <code>tests/conftest.py</code>:</p> <pre><code>import datetime\nfrom dateutil import tz\nimport shutil\nimport textwrap\n\nimport pytest\nfrom git import Actor, Repo\n\n\n@pytest.fixture(name=\"full_repo\")\ndef full_repo_(repo: Repo) -&gt; Repo:\n\"\"\"Create a git repository with fake data and history.\n\n    Args:\n        repo: an initialized Repo\n    \"\"\"\n    index = repo.index\n    author = Actor(\"An author\", \"author@example.com\")\n    committer = Actor(\"A committer\", \"committer@example.com\")\n\n    # Add a commit in time\n    commit_date = datetime.datetime(2021, 2, 1, tzinfo=tz.tzlocal())\n    index.add([\"mkdocs.yml\"])\n    index.commit(\n        \"Initial skeleton\",\n        author=author,\n        committer=committer,\n        author_date=commit_date,\n        commit_date=commit_date,\n    )\n</code></pre> <p>Then you can use that fixture in any test:</p> <pre><code>@pytest.mark.freeze_time(\"2021-02-01T12:00:00\")\ndef test_assert_true(full_repo: Repo) -&gt; None:\n    assert not repo.bare\n</code></pre>"}, {"location": "coding/python/gitpython/#references", "title": "References", "text": "<ul> <li>Docs</li> <li>Git</li> <li>Tutorial</li> </ul>"}, {"location": "coding/python/mkdocstrings/", "title": "mkdocstrings", "text": "<p>mkdocstrings is a library to automatically generate mkdocs pages from the code docstrings.</p>"}, {"location": "coding/python/mkdocstrings/#install", "title": "Install", "text": "<pre><code>pip install mkdocstrings\n</code></pre> <p>Activate the plugin by adding it to the plugin section in the <code>mkdocs.yml</code> configuration file:</p> <pre><code>plugins:\n- mkdocstrings\n</code></pre>"}, {"location": "coding/python/mkdocstrings/#usage", "title": "Usage", "text": "<p>MkDocstrings works by processing special expressions in your Markdown files.</p> <p>The syntax is as follow:</p> <pre><code>::: identifier\n    YAML block\n</code></pre> <p>The <code>identifier</code> is a string identifying the object you want to document. The format of an identifier can vary from one handler to another. For example, the Python handler expects the full dotted-path to a Python object: <code>my_package.my_module.MyClass.my_method</code>.</p> <p>The YAML block is optional, and contains some configuration options:</p> <ul> <li><code>handler</code>: the name of the handler to use to collect and render this object.</li> <li><code>selection</code>: a dictionary of options passed to the handler's collector.   The collector is responsible for collecting the documentation from the source code.   Therefore, selection options change how the documentation is collected from the source code.</li> <li><code>rendering</code>: a dictionary of options passed to the handler's renderer.   The renderer is responsible for rendering the documentation with Jinja2 templates.   Therefore, rendering options affect how the selected object's documentation is rendered.</li> </ul> <p>Example with the Python handler</p> docs/my_page.mdmkdocs.ymlsrc/my_package/my_module.pyResult <pre><code># Documentation for `MyClass`\n\n::: my_package.my_module.MyClass\n    handler: python\n    selection:\n      members:\n        - method_a\n        - method_b\n    rendering:\n      show_root_heading: false\n      show_source: false\n</code></pre> <pre><code>nav:\n- \"My page\": my_page.md\n</code></pre> <pre><code>class MyClass:\n\"\"\"Print print print!\"\"\"\n\n    def method_a(self):\n\"\"\"Print A!\"\"\"\n        print(\"A!\")\n\n    def method_b(self):\n\"\"\"Print B!\"\"\"\n        print(\"B!\")\n\n    def method_c(self):\n\"\"\"Print C!\"\"\"\n        print(\"C!\")\n</code></pre> <p></p>"}, {"location": "coding/python/mkdocstrings/#documentation-for-myclass", "title": "Documentation for <code>MyClass</code>", "text": "<p>Print print print!</p>"}, {"location": "coding/python/mkdocstrings/#mkdocstrings.my_module.MyClass.method_a", "title": "<code> method_a(self) </code>", "text": "<p>Print A!</p>"}, {"location": "coding/python/mkdocstrings/#mkdocstrings.my_module.MyClass.method_b", "title": "<code> method_b(self) </code>", "text": "<p>Print B!</p>"}, {"location": "coding/python/mkdocstrings/#reference-the-objects-in-the-documentation", "title": "Reference the objects in the documentation", "text": "<pre><code>With a custom title:\n[`Object 1`][full.path.object1]\n\nWith the identifier as title:\n[full.path.object2][]\n</code></pre>"}, {"location": "coding/python/mkdocstrings/#global-options", "title": "Global options", "text": "<p>MkDocstrings accept a few top-level configuration options in <code>mkdocs.yml</code>:</p> <ul> <li><code>watch</code>: a list of directories to watch while serving the documentation. So if     any file is changed in those directories, the documentation is rebuilt.</li> <li><code>default_handler</code>: the handler that is used by default when no handler is specified.</li> <li><code>custom_templates</code>: the path to a directory containing custom templates.   The path is relative to the docs directory.   See Customization.</li> <li><code>handlers</code>: the handlers global configuration.</li> </ul> <p>Example:</p> <pre><code>plugins:\n- mkdocstrings:\ndefault_handler: python\nhandlers:\npython:\nrendering:\nshow_source: false\ncustom_templates: templates\nwatch:\n- src/my_package\n</code></pre> <p>The handlers global configuration can then be overridden by local configurations:</p> <pre><code>::: my_package.my_module.MyClass\nrendering:\nshow_source: true\n</code></pre> <p>Check the  Python handler options for more details.</p>"}, {"location": "coding/python/mkdocstrings/#references", "title": "References", "text": "<ul> <li>Docs</li> </ul>"}, {"location": "coding/python/pandas/", "title": "Pandas", "text": "<p>Pandas is a fast, powerful, flexible and easy to use open source data analysis and manipulation tool, built on top of the Python programming language.</p>"}, {"location": "coding/python/pandas/#install", "title": "Install", "text": "<pre><code>pip3 install pandas\n</code></pre> <p>Import</p> <pre><code>import pandas as pd\n</code></pre>"}, {"location": "coding/python/pandas/#snippets", "title": "Snippets", "text": ""}, {"location": "coding/python/pandas/#load-csv", "title": "Load csv", "text": "<pre><code>data = pd.read_csv(\"filename.csv\")\n</code></pre> <p>If you want to parse the dates of the <code>start</code> column give <code>read_csv</code> the argument <code>parse_dates=['start']</code>.</p>"}, {"location": "coding/python/pandas/#do-operation-on-column-data-and-save-it-in-other-column", "title": "Do operation on column data and save it in other column", "text": "<pre><code># make a simple dataframe\ndf = pd.DataFrame({'a':[1,2], 'b':[3,4]})\ndf\n#    a  b\n# 0  1  3\n# 1  2  4\n\n# create an unattached column with an index\ndf.apply(lambda row: row.a + row.b, axis=1)\n# 0    4\n# 1    6\n\n# do same but attach it to the dataframe\ndf['c'] = df.apply(lambda row: row.a + row.b, axis=1)\ndf\n#    a  b  c\n# 0  1  3  4\n# 1  2  4  6\n</code></pre>"}, {"location": "coding/python/pandas/#get-unique-values-of-column", "title": "Get unique values of column", "text": "<p>If we want to get the unique values of the <code>name</code> column:</p> <pre><code>df.name.unique()\n</code></pre>"}, {"location": "coding/python/pandas/#extract-columns-of-dataframe", "title": "Extract columns of dataframe", "text": "<pre><code>df1 = df[['a','b']]\n</code></pre>"}, {"location": "coding/python/pandas/#remove-dumplicate-rows", "title": "Remove dumplicate rows", "text": "<pre><code>df = df.drop_duplicates()\n</code></pre>"}, {"location": "coding/python/pandas/#remove-column-from-dataframe", "title": "Remove column from dataframe", "text": "<pre><code>del df['name']\n</code></pre>"}, {"location": "coding/python/pandas/#count-unique-combinations-of-values-in-selected-columns", "title": "Count unique combinations of values in selected columns", "text": "<pre><code>df1.groupby(['A','B']).size().reset_index().rename(columns={0:'count'})\n\n     A    B  count\n0   no   no      1\n1   no  yes      2\n2  yes   no      4\n3  yes  yes      3\n</code></pre>"}, {"location": "coding/python/pandas/#get-row-that-contains-the-maximum-value-of-a-column", "title": "Get row that contains the maximum value of a column", "text": "<pre><code>df.loc[df['Value'].idxmax()]\n</code></pre>"}, {"location": "coding/python/pandas/#references", "title": "References", "text": "<ul> <li>Homepage</li> </ul>"}, {"location": "coding/python/passpy/", "title": "Passpy", "text": "<p>passpy a platform independent library and cli that is compatible with ZX2C4's pass.</p>"}, {"location": "coding/python/passpy/#installation", "title": "Installation", "text": "<pre><code>pip install passpy\n</code></pre>"}, {"location": "coding/python/passpy/#usage", "title": "Usage", "text": "<p>To use <code>passpy</code> in your Python project, we will first have to create a new <code>passpy.store.Store</code> object.</p> <pre><code>import passpy\n\nstore = passpy.Store()\n</code></pre> <p>If <code>git</code> or <code>gpg2</code> are not in your PATH you will have to specify them via <code>git_bin</code> and <code>gpg_bin</code> when creating the <code>store</code> object.  You can also create the store on a different folder, be passing <code>store_dir</code> along.</p> <p>To initialize the password store at <code>store_dir</code>, if it isn't already, use</p> <pre><code>store.init_store('store gpg id')\n</code></pre> <p>Where <code>store gpg id</code> is the name of a GPG ID.  Optionally, git can be initialized in very much the same way.</p> <pre><code>store.init_git()\n</code></pre> <p>You are now ready to interact with the password store.  You can set and get keys using <code>passpy.store.Store.set_key</code> and <code>passpy.store.Store.get_key</code>.</p> <p><code>passpy.store.Store.gen_key</code> generates a new password for a new or existing key. To delete a key or directory, use <code>passpy.store.Store.remove_path</code>.</p> <p>For a full overview over all available methods see store-module-label.</p>"}, {"location": "coding/python/passpy/#references", "title": "References", "text": "<ul> <li>Docs</li> <li>Git</li> </ul>"}, {"location": "coding/python/plotly/", "title": "plotly", "text": "<p>Plotly is a Python graphing library that makes interactive, publication-quality graphs.</p>"}, {"location": "coding/python/plotly/#install", "title": "Install", "text": "<pre><code>pip3 install plotly\n</code></pre> <p>Import</p> <pre><code>import plotly.graph_objects as go\n</code></pre>"}, {"location": "coding/python/plotly/#snippets", "title": "Snippets", "text": ""}, {"location": "coding/python/plotly/#select-graph-source-using-dropdown", "title": "Select graph source using dropdown", "text": "<pre><code># imports\nimport plotly.graph_objects as go\nimport numpy as np\n\n# data\nx = list(np.linspace(-np.pi, np.pi, 100))\nvalues_1 = list(np.sin(x))\nvalues_1b = [elem*-1 for elem in values_1]\nvalues_2 = list(np.tan(x))\nvalues_2b = [elem*-1 for elem in values_2]\n\n# plotly setup]\nfig = go.Figure()\n\n# Add one ore more traces\nfig.add_traces(go.Scatter(x=x, y=values_1))\nfig.add_traces(go.Scatter(x=x, y=values_1b))\n\n# construct menus\nupdatemenus = [{'buttons': [{'method': 'update',\n                             'label': 'Val 1',\n                             'args': [{'y': [values_1, values_1b]},]\n                              },\n                            {'method': 'update',\n                             'label': 'Val 2',\n                             'args': [{'y': [values_2, values_2b]},]}],\n                'direction': 'down',\n                'showactive': True,}]\n\n# update layout with buttons, and show the figure\nfig.update_layout(updatemenus=updatemenus)\nfig.show()\n</code></pre>"}, {"location": "coding/python/plotly/#references", "title": "References", "text": "<ul> <li>Homepage</li> <li>Git</li> </ul>"}, {"location": "coding/python/prompt_toolkit/", "title": "Python Prompt Toolkit", "text": "<p>Python Prompt Toolkit is a library for building powerful interactive command line and terminal applications in Python.</p>"}, {"location": "coding/python/prompt_toolkit/#installation", "title": "Installation", "text": "<pre><code>pip install prompt_toolkit\n</code></pre>"}, {"location": "coding/python/prompt_toolkit/#usage", "title": "Usage", "text": ""}, {"location": "coding/python/prompt_toolkit/#a-simple-prompt", "title": "A simple prompt", "text": "<p>The following snippet is the most simple example, it uses the <code>prompt()</code> function to asks the user for input and returns the text. Just like <code>(raw_)input</code>.</p> <pre><code>from prompt_toolkit import prompt\n\ntext = prompt('Give me some input: ')\nprint('You said: %s' % text)\n</code></pre> <p>It can be used to build REPL applications or full screen ones.</p>"}, {"location": "coding/python/pydantic/", "title": "Pydantic", "text": "<p>Pydantic is a data validation and settings management using python type annotations.</p> <p>pydantic enforces type hints at runtime, and provides user friendly errors when data is invalid.</p> <p>Define how data should be in pure, canonical python; check it with pydantic.</p>"}, {"location": "coding/python/pydantic/#install", "title": "Install", "text": "<pre><code>pip install pydantic\n</code></pre> <p>If you use mypy I highly recommend you to activate the pydantic plugin by adding to your <code>pyproject.toml</code>:</p> <pre><code>[tool.mypy]\nplugins = [ \"pydantic.mypy\",]\n\n[tool.pydantic-mypy]\ninit_forbid_extra = true\ninit_typed = true\nwarn_required_dynamic_aliases = true\nwarn_untyped_fields = true\n</code></pre>"}, {"location": "coding/python/pydantic/#advantages-and-disadvantages", "title": "Advantages and disadvantages", "text": "<p>Advantages:</p> <ul> <li>Perform data validation in an easy and nice way.</li> <li>Seamless integration with FastAPI and   Typer.</li> <li>Nice way to export the data and data schema.</li> </ul> <p>Disadvantages:</p> <ul> <li>You can't define   cyclic relationships,   therefore there is no way to simulate the backref SQLAlchemy function.</li> </ul>"}, {"location": "coding/python/pydantic/#models", "title": "Models", "text": "<p>The primary means of defining objects in pydantic is via models (models are simply classes which inherit from <code>BaseModel</code>).</p> <p>You can think of models as similar to types in strictly typed languages, or as the requirements of a single endpoint in an API.</p> <p>Untrusted data can be passed to a model, and after parsing and validation pydantic guarantees that the fields of the resultant model instance will conform to the field types defined on the model.</p>"}, {"location": "coding/python/pydantic/#basic-model-usage", "title": "Basic model usage", "text": "<pre><code>from pydantic import BaseModel\n\n\nclass User(BaseModel):\n    id: int\n    name = \"Jane Doe\"\n</code></pre> <p><code>User</code> here is a model with two fields <code>id</code> which is an integer and is required, and <code>name</code> which is a string and is not required (it has a default value). The type of <code>name</code> is inferred from the default value, and so a type annotation is not required.</p> <pre><code>user = User(id=\"123\")\n</code></pre> <p><code>user</code> here is an instance of <code>User</code>. Initialisation of the object will perform all parsing and validation, if no <code>ValidationError</code> is raised, you know the resulting model instance is valid.</p>"}, {"location": "coding/python/pydantic/#model-properties", "title": "Model properties", "text": "<p>Models possess the following methods and attributes:</p> <p><code>dict()</code> : returns a dictionary of the model's fields and values.</p> <p><code>json()</code> : returns a JSON string representation <code>dict()</code>.</p> <p><code>copy()</code> : returns a deep copy of the model.</p> <p><code>parse_obj()</code> : very similar to the <code>__init__</code> method of the model, used to import objects from a dict rather than keyword arguments. If the object passed is not a dict a <code>ValidationError</code> will be raised.</p> <p><code>parse_raw()</code> : takes a str or bytes and parses it as json, then passes the result to <code>parse_obj</code>.</p> <p><code>parse_file()</code> : reads a file and passes the contents to <code>parse_raw</code>. If <code>content_type</code> is omitted, it is inferred from the file's extension.</p> <p><code>from_orm()</code> : loads data into a model from an arbitrary class.</p> <p><code>schema()</code> : returns a dictionary representing the model as JSON Schema.</p> <p><code>schema_json()</code> : returns a JSON string representation of <code>schema()</code>.</p>"}, {"location": "coding/python/pydantic/#recursive-models", "title": "Recursive Models", "text": "<p>More complex hierarchical data structures can be defined using models themselves as types in annotations.</p> <pre><code>from typing import List\nfrom pydantic import BaseModel\n\n\nclass Foo(BaseModel):\n    count: int\n    size: float = None\n\n\nclass Bar(BaseModel):\n    apple = \"x\"\n    banana = \"y\"\n\n\nclass Spam(BaseModel):\n    foo: Foo\n    bars: List[Bar]\n\n\nm = Spam(foo={\"count\": 4}, bars=[{\"apple\": \"x1\"}, {\"apple\": \"x2\"}])\nprint(m)\n# &gt; foo=Foo(count=4, size=None) bars=[Bar(apple='x1', banana='y'),\n# &gt; Bar(apple='x2', banana='y')]\nprint(m.dict())\n\"\"\"\n{\n    'foo': {'count': 4, 'size': None},\n    'bars': [\n        {'apple': 'x1', 'banana': 'y'},\n        {'apple': 'x2', 'banana': 'y'},\n    ],\n}\n\"\"\"\n</code></pre> <p>For self-referencing models, use postponed annotations.</p>"}, {"location": "coding/python/pydantic/#definition-of-two-models-that-reference-each-other", "title": "Definition of two models that reference each other", "text": "<pre><code>class A(BaseModel):\n    b: Optional[\"B\"] = None\n\n\nclass B(BaseModel):\n    a: Optional[A] = None\n\n\nA.update_forward_refs()\n</code></pre> <p>Although it doesn't work as expected!</p>"}, {"location": "coding/python/pydantic/#error-handling", "title": "Error Handling", "text": "<p>pydantic will raise <code>ValidationError</code> whenever it finds an error in the data it's validating.</p> <p>!!! note Validation code should not raise <code>ValidationError</code> itself, but rather raise <code>ValueError</code>, <code>TypeError</code> or <code>AssertionError</code> (or subclasses of <code>ValueError</code> or <code>TypeError</code>) which will be caught and used to populate <code>ValidationError</code>.</p> <p>One exception will be raised regardless of the number of errors found, that <code>ValidationError</code> will contain information about all the errors and how they happened.</p> <p>You can access these errors in a several ways:</p> <p><code>e.errors()</code> : method will return list of errors found in the input data.</p> <p><code>e.json()</code> : method will return a JSON representation of <code>errors</code>.</p> <p><code>str(e)</code> : method will return a human readable representation of the errors.</p> <p>Each error object contains:</p> <p><code>loc</code> : the error's location as a list. The first item in the list will be the field where the error occurred, and if the field is a sub-model, subsequent items will be present to indicate the nested location of the error.</p> <p><code>type</code> : a computer-readable identifier of the error type.</p> <p><code>msg</code> : a human readable explanation of the error.</p> <p><code>ctx</code> : an optional object which contains values required to render the error message.</p>"}, {"location": "coding/python/pydantic/#custom-errors", "title": "Custom Errors", "text": "<p>You can also define your own error classes, which can specify a custom error code, message template, and context:</p> <pre><code>from pydantic import BaseModel, PydanticValueError, ValidationError, validator\n\n\nclass NotABarError(PydanticValueError):\n    code = \"not_a_bar\"\n    msg_template = 'value is not \"bar\", got \"{wrong_value}\"'\n\n\nclass Model(BaseModel):\n    foo: str\n\n    @validator(\"foo\")\n    def name_must_contain_space(cls, v):\n        if v != \"bar\":\n            raise NotABarError(wrong_value=v)\n        return v\n\n\ntry:\n    Model(foo=\"ber\")\nexcept ValidationError as e:\n    print(e.json())\n\"\"\"\n    [\n      {\n        \"loc\": [\n          \"foo\"\n        ],\n        \"msg\": \"value is not \\\"bar\\\", got \\\"ber\\\"\",\n        \"type\": \"value_error.not_a_bar\",\n        \"ctx\": {\n          \"wrong_value\": \"ber\"\n        }\n      }\n    ]\n    \"\"\"\n</code></pre>"}, {"location": "coding/python/pydantic/#dynamic-model-creation", "title": "Dynamic model creation", "text": "<p>There are some occasions where the shape of a model is not known until runtime. For this pydantic provides the <code>create_model</code> method to allow models to be created on the fly.</p> <pre><code>from pydantic import BaseModel, create_model\n\nDynamicFoobarModel = create_model(\"DynamicFoobarModel\", foo=(str, ...), bar=123)\n\n\nclass StaticFoobarModel(BaseModel):\n    foo: str\n    bar: int = 123\n</code></pre> <p>Here <code>StaticFoobarModel</code> and <code>DynamicFoobarModel</code> are identical.</p> <p>Warning</p> <p>Required Optional Fields for the distinct between an ellipsis as a field default and annotation only fields. See samuelcolvin/pydantic#1047 for more details.</p> <p>Fields are defined by either a tuple of the form <code>(&lt;type&gt;, &lt;default value&gt;)</code> or just a default value. The special key word arguments <code>__config__</code> and <code>__base__</code> can be used to customize the new model. This includes extending a base model with extra fields.</p> <pre><code>from pydantic import BaseModel, create_model\n\n\nclass FooModel(BaseModel):\n    foo: str\n    bar: int = 123\n\n\nBarModel = create_model(\n    \"BarModel\",\n    apple=\"russet\",\n    banana=\"yellow\",\n    __base__=FooModel,\n)\nprint(BarModel)\n# &gt; &lt;class 'BarModel'&gt;\nprint(BarModel.__fields__.keys())\n# &gt; dict_keys(['foo', 'bar', 'apple', 'banana'])\n</code></pre>"}, {"location": "coding/python/pydantic/#abstract-base-classes", "title": "Abstract Base Classes", "text": "<p>Pydantic models can be used alongside Python's Abstract Base Classes (ABCs).</p> <pre><code>import abc\nfrom pydantic import BaseModel\n\n\nclass FooBarModel(BaseModel, abc.ABC):\n    a: str\n    b: int\n\n    @abc.abstractmethod\n    def my_abstract_method(self):\n        pass\n</code></pre>"}, {"location": "coding/python/pydantic/#field-ordering", "title": "Field Ordering", "text": "<p>Field order is important in models for the following reasons:</p> <ul> <li>Validation is performed in the order fields are defined;   fields validators can access the values of earlier   fields, but not later ones</li> <li>Field order is preserved in the model   schema</li> <li>Field order is preserved in validation errors</li> <li>Field order is preserved by <code>.dict()</code> and <code>.json()</code> etc.</li> </ul> <p>As of v1.0 all fields with annotations (whether annotation-only or with a default value) will precede all fields without an annotation. Within their respective groups, fields remain in the order they were defined.</p>"}, {"location": "coding/python/pydantic/#field-with-dynamic-default-value", "title": "Field with dynamic default value", "text": "<p>When declaring a field with a default value, you may want it to be dynamic (i.e. different for each model). To do this, you may want to use a <code>default_factory</code>.</p> <p>!!! info \"In Beta\" The <code>default_factory</code> argument is in beta, it has been added to pydantic in v1.5 on a provisional basis. It may change significantly in future releases and its signature or behaviour will not be concrete until v2. Feedback from the community while it's still provisional would be extremely useful; either comment on #866 or create a new issue.</p> <p>Example of usage:</p> <pre><code>from datetime import datetime\nfrom uuid import UUID, uuid4\nfrom pydantic import BaseModel, Field\n\n\nclass Model(BaseModel):\n    uid: UUID = Field(default_factory=uuid4)\n    updated: datetime = Field(default_factory=datetime.utcnow)\n\n\nm1 = Model()\nm2 = Model()\nprint(f\"{m1.uid} != {m2.uid}\")\n# &gt; 3b187763-a19c-4ed8-9588-387e224e04f1 != 0c58f97b-c8a7-4fe8-8550-e9b2b8026574\nprint(f\"{m1.updated} != {m2.updated}\")\n# &gt; 2020-07-15 20:01:48.451066 != 2020-07-15 20:01:48.451083\n</code></pre> <p>!!! warning The <code>default_factory</code> expects the field type to be set. Moreover if you want to validate default values with <code>validate_all</code>, pydantic will need to call the <code>default_factory</code>, which could lead to side effects!</p>"}, {"location": "coding/python/pydantic/#field-customization", "title": "Field customization", "text": "<p>Optionally, the <code>Field</code> function can be used to provide extra information about the field and validations. It has the following arguments:</p> <ul> <li><code>default</code>: (a positional argument) the default value of the field. Since the   <code>Field</code> replaces the field's default, this first argument can be used to set   the default. Use ellipsis (<code>...</code>) to indicate the field is required.</li> <li><code>default_factory</code>: a zero-argument callable that will be called when a default   value is needed for this field. Among other purposes, this can be used to set   dynamic default values. It is forbidden to set both <code>default</code> and   <code>default_factory</code>.</li> <li><code>alias</code>: the public name of the field.</li> <li><code>title</code>: if omitted, <code>field_name.title()</code> is used.</li> <li><code>description</code>: if omitted and the annotation is a sub-model, the docstring of   the sub-model will be used.</li> <li><code>const</code>: this argument must be the same as the field's default value if   present.</li> <li><code>gt</code>: for numeric values (<code>int</code>, <code>float</code>, <code>Decimal</code>), adds a validation of   \"greater than\" and an annotation of <code>exclusiveMinimum</code> to the JSON Schema.</li> <li><code>ge</code>: for numeric values, this adds a validation of \"greater than or equal\"   and an annotation of minimum to the JSON Schema.</li> <li><code>lt</code>: for numeric values, this adds a validation of \"less than\" and an   annotation of <code>exclusiveMaximum</code> to the JSON Schema.</li> <li><code>le</code>: for numeric values, this adds a validation of \"less than or equal\" and   an annotation of maximum to the JSON Schema.</li> <li><code>multiple_of</code>: for numeric values, this adds a validation of \"a multiple of\"   and an annotation of <code>multipleOf</code> to the JSON Schema.</li> <li><code>min_items</code>: for list values, this adds a corresponding validation and an   annotation of <code>minItems</code> to the JSON Schema.</li> <li><code>max_items</code>: for list values, this adds a corresponding validation and an   annotation of <code>maxItems</code> to the JSON Schema.</li> <li><code>min_length</code>: for string values, this adds a corresponding validation and an   annotation of <code>minLength</code> to the JSON Schema.</li> <li><code>max_length</code>: for string values, this adds a corresponding validation and an   annotation of <code>maxLength</code> to the JSON Schema.</li> <li><code>allow_mutation</code>: a boolean which defaults to <code>True</code>. When <code>False</code>, the field   raises a <code>TypeError</code> if the field is assigned on an instance. The model config   must set <code>validate_assignment</code> to <code>True</code> for this check to be performed.</li> <li><code>regex</code>: for string values, this adds a Regular Expression validation   generated from the passed string and an annotation of pattern to the JSON   Schema.</li> <li><code>**</code>: any other keyword arguments (e.g. <code>examples</code>) will be added verbatim to   the field's schema.</li> </ul> <p>!!! note pydantic validates strings using <code>re.match</code>, which treats regular expressions as implicitly anchored at the beginning. On the contrary, JSON Schema validators treat the pattern keyword as implicitly unanchored, more like what <code>re.search</code> does.</p> <p>Instead of using <code>Field</code>, the <code>fields</code> property of the <code>Config</code> class can be used to set all of the arguments above except default.</p>"}, {"location": "coding/python/pydantic/#parsing-data-into-a-specified-type", "title": "Parsing data into a specified type", "text": "<p>Pydantic includes a standalone utility function <code>parse_obj_as</code> that can be used to apply the parsing logic used to populate pydantic models in a more ad-hoc way. This function behaves similarly to <code>BaseModel.parse_obj</code>, but works with arbitrary pydantic-compatible types.</p> <p>This is especially useful when you want to parse results into a type that is not a direct subclass of <code>BaseModel</code>. For example:</p> <pre><code>from typing import List\n\nfrom pydantic import BaseModel, parse_obj_as\n\n\nclass Item(BaseModel):\n    id: int\n    name: str\n\n\n# `item_data` could come from an API call, eg., via something like:\n# item_data = requests.get('https://my-api.com/items').json()\nitem_data = [{\"id\": 1, \"name\": \"My Item\"}]\n\nitems = parse_obj_as(List[Item], item_data)\nprint(items)\n# &gt; [Item(id=1, name='My Item')]\n</code></pre> <p>This function is capable of parsing data into any of the types pydantic can handle as fields of a <code>BaseModel</code>.</p> <p>Pydantic also includes a similar standalone function called <code>parse_file_as</code>, which is analogous to <code>BaseModel.parse_file</code>.</p>"}, {"location": "coding/python/pydantic/#data-conversion", "title": "Data Conversion", "text": "<p>pydantic may cast input data to force it to conform to model field types, and in some cases this may result in a loss of information. For example:</p> <pre><code>from pydantic import BaseModel\n\n\nclass Model(BaseModel):\n    a: int\n    b: float\n    c: str\n\n\nprint(Model(a=3.1415, b=\" 2.72 \", c=123).dict())\n# &gt; {'a': 3, 'b': 2.72, 'c': '123'}\n</code></pre> <p>This is a deliberate decision of pydantic, and in general it's the most useful approach. See here for a longer discussion on the subject.</p>"}, {"location": "coding/python/pydantic/#initialize-attributes-at-object-creation", "title": "Initialize attributes at object creation", "text": "<p><code>pydantic</code> recommends using root validators, but it's difficult to undestand how to do it and to debug the errors. You also don't have easy access to the default values of the model. I'd rather use the overwriting the <code>__init__</code> method.</p>"}, {"location": "coding/python/pydantic/#overwriting-the-__init__-method", "title": "Overwriting the <code>__init__</code> method", "text": "<pre><code>class fish(BaseModel):\n    name: str\n    color: str\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        print(\"Fish initialization successful!\")\n        self.color=complex_function()\n</code></pre>"}, {"location": "coding/python/pydantic/#using-root-validators", "title": "Using root validators", "text": "<p>If you want to initialize attributes of the object automatically at object creation, similar of what you'd do with the <code>__init__</code> method of the class, you need to use <code>root_validators</code>.</p> <pre><code>from pydantic import root_validator\n\n\nclass PypikaRepository(BaseModel):\n\"\"\"Implement the repository pattern using the Pypika query builder.\"\"\"\n\n    connection: sqlite3.Connection\n    cursor: sqlite3.Cursor\n\n    class Config:\n\"\"\"Configure the pydantic model.\"\"\"\n\n        arbitrary_types_allowed = True\n\n    @root_validator(pre=True)\n    @classmethod\n    def set_connection(cls, values: Dict[str, Any]) -&gt; Dict[str, Any]:\n\"\"\"Set the connection to the database.\n\n        Raises:\n            ConnectionError: If there is no database file.\n        \"\"\"\n        database_file = values[\"database_url\"].replace(\"sqlite:///\", \"\")\n        if not os.path.isfile(database_file):\n            raise ConnectionError(f\"There is no database file: {database_file}\")\n        connection = sqlite3.connect(database_file)\n        values[\"connection\"] = connection\n        values[\"cursor\"] = connection.cursor()\n\n        return values\n</code></pre> <p>I had to set the <code>arbitrary_types_allowed</code> because the sqlite3 objects are not between the pydantic object types.</p>"}, {"location": "coding/python/pydantic/#set-private-attributes", "title": "Set private attributes", "text": "<p>If you want to define some attributes that are not part of the model use <code>PrivateAttr</code>:</p> <pre><code>from datetime import datetime\nfrom random import randint\n\nfrom pydantic import BaseModel, PrivateAttr\n\n\nclass TimeAwareModel(BaseModel):\n    _processed_at: datetime = PrivateAttr(default_factory=datetime.now)\n    _secret_value: str = PrivateAttr()\n\n    def __init__(self, **data: Any) -&gt; None:\n        super().__init__(**data)\n        # this could also be done with default_factory\n        self._secret_value = randint(1, 5)\n\n\nm = TimeAwareModel()\nprint(m._processed_at)\n# &gt; 2021-03-03 17:30:04.030758\nprint(m._secret_value)\n# &gt; 5\n</code></pre>"}, {"location": "coding/python/pydantic/#define-fields-to-exclude-from-exporting-at-config-level", "title": "Define fields to exclude from exporting at config level", "text": "<p>This won't be necessary once they release the version 1.9 because you can define the fields to exclude in the <code>Config</code> of the model using something like:</p> <pre><code>class User(BaseModel):\n    id: int\n    username: str\n    password: str\n\n\nclass Transaction(BaseModel):\n    id: str\n    user: User\n    value: int\n\n    class Config:\n        fields = {\n            \"value\": {\n                \"alias\": \"Amount\",\n                \"exclude\": ...,\n            },\n            \"user\": {\"exclude\": {\"username\", \"password\"}},\n            \"id\": {\"dump_alias\": \"external_id\"},\n        }\n</code></pre> <p>The release it's taking its time because the developer's gremlin and salaried work are sucking his time off.</p>"}, {"location": "coding/python/pydantic/#update-entity-attributes-with-a-dictionary", "title": "Update entity attributes with a dictionary", "text": "<p>To update a model with the data of a dictionary you can create a new object with the new data using the <code>update</code> argument of the <code>copy</code> method.</p> <pre><code>class FooBarModel(BaseModel):\n    banana: float\n    foo: str\n\n\nm = FooBarModel(banana=3.14, foo=\"hello\")\n\nm.copy(update={\"banana\": 0})\n</code></pre>"}, {"location": "coding/python/pydantic/#lazy-loading-attributes", "title": "Lazy loading attributes", "text": "<p>Currently there is no official support for lazy loading model attributes.</p> <p>You can define your own properties but when you export the schema they won't appear there. dgasmith has a workaround though.</p>"}, {"location": "coding/python/pydantic/#troubleshooting", "title": "Troubleshooting", "text": ""}, {"location": "coding/python/pydantic/#ignore-a-field-when-representing-an-object", "title": "Ignore a field when representing an object", "text": "<p>Use <code>repr=False</code>. This is useful for properties that don't return a value quickly, for example if you save an <code>sh</code> background process.</p> <pre><code>class Temp(BaseModel):\n    foo: typing.Any\n    boo: typing.Any = Field(..., repr=False)\n</code></pre>"}, {"location": "coding/python/pydantic/#copy-produces-copy-that-modifies-the-original", "title": "Copy produces copy that modifies the original", "text": "<p>When copying a model, changing the value of an attribute on the copy updates the value of the attribute on the original. This only happens if <code>deep != True</code>. To fix it use: <code>model.copy(deep=True)</code>.</p>"}, {"location": "coding/python/pydantic/#e0611-no-name-basemodel-in-module-pydantic", "title": "E0611: No name 'BaseModel' in module 'pydantic'", "text": "<p>Add to your pyproject.toml the following lines:</p> <pre><code># --------- Pylint -------------\n[tool.pylint.'MESSAGES CONTROL']\nextension-pkg-whitelist = \"pydantic\"\n</code></pre> <p>Or if it fails, add to the line <code># pylint: extension-pkg-whitelist</code>.</p>"}, {"location": "coding/python/pydantic/#to-investigate", "title": "To investigate", "text": "<ul> <li>Integration of pydantic with pandas</li> </ul>"}, {"location": "coding/python/pydantic/#references", "title": "References", "text": "<ul> <li>Docs</li> <li>Git</li> </ul>"}, {"location": "coding/python/pydantic_exporting/", "title": "Pydantic exporting models", "text": "<p>As well as accessing model attributes directly via their names (e.g. <code>model.foobar</code>), models can be converted and exported in a number of ways:</p>"}, {"location": "coding/python/pydantic_exporting/#modeldict", "title": "<code>model.dict(...)</code>", "text": "<p>This is the primary way of converting a model to a dictionary. Sub-models will be recursively converted to dictionaries.</p> <p>Arguments:</p> <ul> <li><code>include</code>: Fields to include in the returned dictionary.</li> <li><code>exclude</code>: Fields to exclude from the returned dictionary.</li> <li><code>by_alias</code>: Whether field aliases should be used as keys in the returned     dictionary; default <code>False</code>.</li> <li><code>exclude_unset</code>: Whether fields which were not explicitly set when creating     the model should be excluded from the returned dictionary; default <code>False</code>.</li> <li><code>exclude_defaults</code>: Whether fields which are equal to their default values     (whether set or otherwise) should be excluded from the returned dictionary;     default <code>False</code>.</li> <li><code>exclude_none</code>: Whether fields which are equal to <code>None</code> should be excluded     from the returned dictionary; default <code>False</code>.</li> </ul> <p>Example:</p> <pre><code>from pydantic import BaseModel\n\n\nclass BarModel(BaseModel):\n    whatever: int\n\n\nclass FooBarModel(BaseModel):\n    banana: float\n    foo: str\n    bar: BarModel\n\n\nm = FooBarModel(banana=3.14, foo='hello', bar={'whatever': 123})\n\n# returns a dictionary:\nprint(m.dict())\n\"\"\"\n{\n    'banana': 3.14,\n    'foo': 'hello',\n    'bar': {'whatever': 123},\n}\n\"\"\"\nprint(m.dict(include={'foo', 'bar'}))\n#&gt; {'foo': 'hello', 'bar': {'whatever': 123}}\nprint(m.dict(exclude={'foo', 'bar'}))\n#&gt; {'banana': 3.14}\n</code></pre>"}, {"location": "coding/python/pydantic_exporting/#dictmodel-and-iteration", "title": "<code>dict(model)</code> and iteration", "text": "<p>pydantic models can also be converted to dictionaries using <code>dict(model)</code>, and you can also iterate over a model's field using <code>for field_name, value in model:</code>. With this approach the raw field values are returned, so sub-models will not be converted to dictionaries.</p> <p>Example:</p> <pre><code>from pydantic import BaseModel\n\n\nclass BarModel(BaseModel):\n    whatever: int\n\n\nclass FooBarModel(BaseModel):\n    banana: float\n    foo: str\n    bar: BarModel\n\n\nm = FooBarModel(banana=3.14, foo='hello', bar={'whatever': 123})\n\nprint(dict(m))\n\"\"\"\n{\n    'banana': 3.14,\n    'foo': 'hello',\n    'bar': BarModel(\n        whatever=123,\n    ),\n}\n\"\"\"\nfor name, value in m:\n    print(f'{name}: {value}')\n    #&gt; banana: 3.14\n    #&gt; foo: hello\n    #&gt; bar: whatever=123\n</code></pre>"}, {"location": "coding/python/pydantic_exporting/#modelcopy", "title": "<code>model.copy(...)</code>", "text": "<p><code>copy()</code> allows models to be duplicated, which is particularly useful for immutable models.</p> <p>Arguments:</p> <ul> <li><code>include</code>: Fields to include in the returned dictionary.</li> <li><code>exclude</code>: Fields to exclude from the returned dictionary.</li> <li><code>update</code>: A dictionary of values to change when creating the copied model</li> <li><code>deep</code>: whether to make a deep copy of the new model; default <code>False</code></li> </ul> <p>Example:</p> <pre><code>from pydantic import BaseModel\n\n\nclass BarModel(BaseModel):\n    whatever: int\n\n\nclass FooBarModel(BaseModel):\n    banana: float\n    foo: str\n    bar: BarModel\n\n\nm = FooBarModel(banana=3.14, foo='hello', bar={'whatever': 123})\n\nprint(m.copy(include={'foo', 'bar'}))\n#&gt; foo='hello' bar=BarModel(whatever=123)\nprint(m.copy(exclude={'foo', 'bar'}))\n#&gt; banana=3.14\nprint(m.copy(update={'banana': 0}))\n#&gt; banana=0 foo='hello' bar=BarModel(whatever=123)\nprint(id(m.bar), id(m.copy().bar))\n#&gt; 139868119420992 139868119420992\n# normal copy gives the same object reference for `bar`\nprint(id(m.bar), id(m.copy(deep=True).bar))\n#&gt; 139868119420992 139868119423296\n# deep copy gives a new object reference for `bar`\n</code></pre>"}, {"location": "coding/python/pydantic_exporting/#modeljson", "title": "<code>model.json(...)</code>", "text": "<p>The <code>.json()</code> method will serialise a model to JSON. Typically, <code>.json()</code> in turn calls <code>.dict()</code> and serialises its result. (For models with a custom root type, after calling <code>.dict()</code>, only the value for the <code>__root__</code> key is serialised).</p> <p>Arguments:</p> <ul> <li><code>include</code>: Fields to include in the returned dictionary.</li> <li><code>exclude</code>: Fields to exclude from the returned dictionary.</li> <li><code>by_alias</code>: Whether field aliases should be used as keys in the returned     dictionary; default <code>False</code>.</li> <li><code>exclude_unset</code>: Whether fields which were not set when creating the model and     have their default values should be excluded from the returned dictionary;     default <code>False</code>.</li> <li><code>exclude_defaults</code>: Whether fields which are equal to their default values     (whether set or otherwise) should be excluded from the returned dictionary;     default <code>False</code>.</li> <li><code>exclude_none</code>: Whether fields which are equal to <code>None</code> should be excluded     from the returned dictionary; default <code>False</code>.</li> <li><code>encoder</code>: A custom encoder function passed to the <code>default</code> argument of     <code>json.dumps()</code>; defaults to a custom encoder designed to take care of all     common types.</li> <li><code>**dumps_kwargs</code>: Any other keyword arguments are passed to <code>json.dumps()</code>,     e.g. <code>indent</code>.</li> </ul> <p>pydantic can serialise many commonly used types to JSON (e.g. <code>datetime</code>, <code>date</code> or <code>UUID</code>) which would normally fail with a simple <code>json.dumps(foobar)</code>.</p> <pre><code>from datetime import datetime\nfrom pydantic import BaseModel\n\n\nclass BarModel(BaseModel):\n    whatever: int\n\n\nclass FooBarModel(BaseModel):\n    foo: datetime\n    bar: BarModel\n\n\nm = FooBarModel(foo=datetime(2032, 6, 1, 12, 13, 14), bar={'whatever': 123})\nprint(m.json())\n#&gt; {\"foo\": \"2032-06-01T12:13:14\", \"bar\": {\"whatever\": 123}}\n</code></pre>"}, {"location": "coding/python/pydantic_exporting/#advanced-include-and-exclude", "title": "Advanced include and exclude", "text": "<p>The <code>dict</code>, <code>json</code>, and <code>copy</code> methods support <code>include</code> and <code>exclude</code> arguments which can either be sets or dictionaries. This allows nested selection of which fields to export:</p> <pre><code>from pydantic import BaseModel, SecretStr\n\n\nclass User(BaseModel):\n    id: int\n    username: str\n    password: SecretStr\n\n\nclass Transaction(BaseModel):\n    id: str\n    user: User\n    value: int\n\n\nt = Transaction(\n    id='1234567890',\n    user=User(\n        id=42,\n        username='JohnDoe',\n        password='hashedpassword'\n    ),\n    value=9876543210,\n)\n\n# using a set:\nprint(t.dict(exclude={'user', 'value'}))\n#&gt; {'id': '1234567890'}\n\n# using a dict:\nprint(t.dict(exclude={'user': {'username', 'password'}, 'value': ...}))\n#&gt; {'id': '1234567890', 'user': {'id': 42}}\n\nprint(t.dict(include={'id': ..., 'user': {'id'}}))\n#&gt; {'id': '1234567890', 'user': {'id': 42}}\n</code></pre> <p>The ellipsis (<code>...</code>) indicates that we want to exclude or include an entire key, just as if we included it in a set.  Of course, the same can be done at any depth level.</p> <p>Special care must be taken when including or excluding fields from a list or tuple of submodels or dictionaries.  In this scenario, <code>dict</code> and related methods expect integer keys for element-wise inclusion or exclusion. To exclude a field from every member of a list or tuple, the dictionary key <code>'__all__'</code> can be used as follows:</p> <pre><code>import datetime\nfrom typing import List\n\nfrom pydantic import BaseModel, SecretStr\n\n\nclass Country(BaseModel):\n    name: str\n    phone_code: int\n\n\nclass Address(BaseModel):\n    post_code: int\n    country: Country\n\n\nclass CardDetails(BaseModel):\n    number: SecretStr\n    expires: datetime.date\n\n\nclass Hobby(BaseModel):\n    name: str\n    info: str\n\n\nclass User(BaseModel):\n    first_name: str\n    second_name: str\n    address: Address\n    card_details: CardDetails\n    hobbies: List[Hobby]\n\n\nuser = User(\n    first_name='John',\n    second_name='Doe',\n    address=Address(\n        post_code=123456,\n        country=Country(\n            name='USA',\n            phone_code=1\n        )\n    ),\n    card_details=CardDetails(\n        number=4212934504460000,\n        expires=datetime.date(2020, 5, 1)\n    ),\n    hobbies=[\n        Hobby(name='Programming', info='Writing code and stuff'),\n        Hobby(name='Gaming', info='Hell Yeah!!!'),\n    ],\n)\n\nexclude_keys = {\n    'second_name': ...,\n    'address': {'post_code': ..., 'country': {'phone_code'}},\n    'card_details': ...,\n    # You can exclude fields from specific members of a tuple/list by index:\n    'hobbies': {-1: {'info'}},\n}\n\ninclude_keys = {\n    'first_name': ...,\n    'address': {'country': {'name'}},\n    'hobbies': {0: ..., -1: {'name'}},\n}\n\n# would be the same as user.dict(exclude=exclude_keys) in this case:\nprint(user.dict(include=include_keys))\n\"\"\"\n{\n    'first_name': 'John',\n    'address': {'country': {'name': 'USA'}},\n    'hobbies': [\n        {\n            'name': 'Programming',\n            'info': 'Writing code and stuff',\n        },\n        {'name': 'Gaming'},\n    ],\n}\n\"\"\"\n\n# To exclude a field from all members of a nested list or tuple, use \"__all__\":\nprint(user.dict(exclude={'hobbies': {'__all__': {'info'}}}))\n\"\"\"\n{\n    'first_name': 'John',\n    'second_name': 'Doe',\n    'address': {\n        'post_code': 123456,\n        'country': {'name': 'USA', 'phone_code': 1},\n    },\n    'card_details': {\n        'number': SecretStr('**********'),\n        'expires': datetime.date(2020, 5, 1),\n    },\n    'hobbies': [{'name': 'Programming'}, {'name': 'Gaming'}],\n}\n\"\"\"\n</code></pre> <p>The same holds for the <code>json</code> and <code>copy</code> methods.</p>"}, {"location": "coding/python/pydantic_exporting/#references", "title": "References", "text": "<ul> <li>Pydantic exporting models</li> </ul>"}, {"location": "coding/python/pydantic_functions/", "title": "Pydantic validating functions", "text": "<p>The <code>validate_arguments</code> decorator allows the arguments passed to a function to be parsed and validated using the function's annotations before the function is called. While under the hood this uses the same approach of model creation and initialisation; it provides an extremely easy way to apply validation to your code with minimal boilerplate.</p> <p>In Beta</p> <p>The <code>validate_arguments</code> decorator is in beta, it has been added to pydantic in v1.5 on a provisional basis. It may change significantly in future releases and its interface will not be concrete until v2. Feedback from the community while it's still provisional would be extremely useful; either comment on #1205 or create a new issue.</p> <p>Be sure you understand it's limitations.</p> <p>Example of usage:</p> <pre><code>from pydantic import validate_arguments, ValidationError\n\n\n@validate_arguments\ndef repeat(s: str, count: int, *, separator: bytes = b'') -&gt; bytes:\n    b = s.encode()\n    return separator.join(b for _ in range(count))\n\n\na = repeat('hello', 3)\nprint(a)\n#&gt; b'hellohellohello'\n\nb = repeat('x', '4', separator=' ')\nprint(b)\n#&gt; b'x x x x'\n\ntry:\n    c = repeat('hello', 'wrong')\nexcept ValidationError as exc:\n    print(exc)\n\"\"\"\n    1 validation error for Repeat\n    count\n      value is not a valid integer (type=type_error.integer)\n    \"\"\"\n</code></pre>"}, {"location": "coding/python/pydantic_functions/#usage-with-mypy", "title": "Usage with mypy", "text": "<p>The <code>validate_arguments</code> decorator should work \"out of the box\" with mypy since it's defined to return a function with the same signature as the function it decorates. The only limitation is that since we trick mypy into thinking the function returned by the decorator is the same as the function being decorated; access to the raw function or other attributes will require <code>type: ignore</code>.</p>"}, {"location": "coding/python/pydantic_functions/#references", "title": "References", "text": "<ul> <li>Pydantic validation decorator docs</li> </ul>"}, {"location": "coding/python/pydantic_mypy_plugin/", "title": "Pydantic Mypy Plugin", "text": "<p>Pydantic works well with mypy right out of the box.</p> <p>However, Pydantic also ships with a mypy plugin that adds a number of important pydantic-specific features to mypy that improve its ability to type-check your code.</p>"}, {"location": "coding/python/pydantic_mypy_plugin/#enabling-the-plugin", "title": "Enabling the Plugin", "text": "<p>To enable the plugin, just add <code>pydantic.mypy</code> to the list of plugins in your mypy config file (this could be <code>mypy.ini</code> or <code>setup.cfg</code>).</p> <p>To get started, all you need to do is create a <code>mypy.ini</code> file with following contents: <pre><code>[mypy]\nplugins = pydantic.mypy\n</code></pre></p> <p>See the mypy usage and plugin configuration docs for more details.</p>"}, {"location": "coding/python/pydantic_mypy_plugin/#references", "title": "References", "text": "<ul> <li>Pydantic mypy plugin docs</li> </ul>"}, {"location": "coding/python/pydantic_types/", "title": "Pydantic types", "text": "<p>Where possible pydantic uses standard library types to define fields, thus smoothing the learning curve. For many useful applications, however, no standard library type exists, so pydantic implements many commonly used types.</p> <p>If no existing type suits your purpose you can also implement your own pydantic-compatible types with custom properties and validation.</p>"}, {"location": "coding/python/pydantic_types/#standard-library-types", "title": "Standard Library Types", "text": "<p>pydantic supports many common types from the python standard library. If you need stricter processing see Strict Types; if you need to constrain the values allowed (e.g. to require a positive int) see Constrained Types.</p> <p><code>bool</code> : see Booleans for details on how bools are validated and what values are permitted.</p> <p><code>int</code> : pydantic uses <code>int(v)</code> to coerce types to an <code>int</code>; see this warning on loss of information during data conversion.</p> <p><code>float</code> : similarly, <code>float(v)</code> is used to coerce values to floats.</p> <p><code>str</code> : strings are accepted as-is, <code>int</code> <code>float</code> and <code>Decimal</code> are coerced using <code>str(v)</code>, <code>bytes</code> and <code>bytearray</code> are converted using <code>v.decode()</code>, enums inheriting from <code>str</code> are converted using <code>v.value</code>, and all other types cause an error.</p> <p><code>list</code> : allows <code>list</code>, <code>tuple</code>, <code>set</code>, <code>frozenset</code>, or generators and casts to a list.</p> <p><code>tuple</code> : allows <code>list</code>, <code>tuple</code>, <code>set</code>, <code>frozenset</code>, or generators and casts to a tuple.</p> <p><code>dict</code> : <code>dict(v)</code> is used to attempt to convert a dictionary.</p> <p><code>set</code> : allows <code>list</code>, <code>tuple</code>, <code>set</code>, <code>frozenset</code>, or generators and casts to a set.</p> <p><code>frozenset</code> : allows <code>list</code>, <code>tuple</code>, <code>set</code>, <code>frozenset</code>, or generators and casts to a frozen set.</p> <p><code>datetime.date</code> : see Datetime Types below for more detail on parsing and validation.</p> <p><code>datetime.time</code> : see Datetime Types below for more detail on parsing and validation.</p> <p><code>datetime.datetime</code> : see Datetime Types below for more detail on parsing and validation.</p> <p><code>datetime.timedelta</code> : see Datetime Types below for more detail on parsing and validation.</p> <p><code>typing.Any</code> : allows any value include <code>None</code>, thus an <code>Any</code> field is optional.</p> <p><code>typing.TypeVar</code> : constrains the values allowed based on <code>constraints</code> or <code>bound</code>, see TypeVar.</p> <p><code>typing.Union</code> : see Unions below for more detail on parsing and validation.</p> <p><code>typing.Optional</code> : <code>Optional[x]</code> is simply short hand for <code>Union[x, None]</code>; see Unions below for more detail on parsing and validation.</p> <p><code>typing.List</code> :</p> <p><code>typing.Tuple</code> :</p> <p><code>typing.Dict</code> :</p> <p><code>typing.Set</code> :</p> <p><code>typing.FrozenSet</code> :</p> <p><code>typing.Sequence</code> :</p> <p><code>typing.Iterable</code> : this is reserved for iterables that shouldn't be consumed. See Infinite Generators below for more detail on parsing and validation.</p> <p><code>typing.Type</code> : see Type below for more detail on parsing and validation.</p> <p><code>typing.Callable</code> : see Callable for more detail on parsing and validation.</p> <p><code>typing.Pattern</code> : will cause the input value to be passed to <code>re.compile(v)</code> to create a regex pattern.</p> <p><code>ipaddress.IPv4Address</code> : simply uses the type itself for validation by passing the value to <code>IPv4Address(v)</code>.</p> <p><code>ipaddress.IPv4Interface</code> : simply uses the type itself for validation by passing the value to <code>IPv4Address(v)</code>.</p> <p><code>ipaddress.IPv4Network</code> : simply uses the type itself for validation by passing the value to <code>IPv4Network(v)</code>.</p> <p><code>enum.Enum</code> : checks that the value is a valid member of the enum; see Enums and Choices for more details.</p> <p><code>enum.IntEnum</code> : checks that the value is a valid member of the integer enum; see Enums and Choices for more details.</p> <p><code>decimal.Decimal</code> : pydantic attempts to convert the value to a string, then passes the string to <code>Decimal(v)</code>.</p> <p><code>pathlib.Path</code> : simply uses the type itself for validation by passing the value to <code>Path(v)</code>.</p>"}, {"location": "coding/python/pydantic_types/#iterables", "title": "Iterables", "text": ""}, {"location": "coding/python/pydantic_types/#define-default-value-for-an-iterable", "title": "Define default value for an iterable", "text": "<p>If you want to define an empty list, dictionary, set or other iterable as a model attribute, you can use the <code>default_factory</code>.</p> <pre><code>from typing import Sequence\nfrom pydantic import BaseModel, Field\n\n\nclass Foo(BaseModel):\n    defaulted_list_field: Sequence[str] = Field(default_factory=list)\n</code></pre> <p>It might be tempting to do</p> <pre><code>class Foo(BaseModel):\n    defaulted_list_field: Sequence[str] = []  # Bad!\n</code></pre> <p>But you'll follow the mutable default argument anti-pattern.</p>"}, {"location": "coding/python/pydantic_types/#unions", "title": "Unions", "text": "<p>The <code>Union</code> type allows a model attribute to accept different types, e.g.:</p> <pre><code>from uuid import UUID\nfrom typing import Union\nfrom pydantic import BaseModel\n\n\nclass User(BaseModel):\n    id: Union[int, str, UUID]\n    name: str\n\n\nuser_01 = User(id=123, name=\"John Doe\")\nprint(user_01)\n# &gt; id=123 name='John Doe'\nprint(user_01.id)\n# &gt; 123\nuser_02 = User(id=\"1234\", name=\"John Doe\")\nprint(user_02)\n# &gt; id=1234 name='John Doe'\nprint(user_02.id)\n# &gt; 1234\nuser_03_uuid = UUID(\"cf57432e-809e-4353-adbd-9d5c0d733868\")\nuser_03 = User(id=user_03_uuid, name=\"John Doe\")\nprint(user_03)\n# &gt; id=275603287559914445491632874575877060712 name='John Doe'\nprint(user_03.id)\n# &gt; 275603287559914445491632874575877060712\nprint(user_03_uuid.int)\n# &gt; 275603287559914445491632874575877060712\n</code></pre> <p>However, as can be seen above, pydantic will attempt to 'match' any of the types defined under <code>Union</code> and will use the first one that matches. In the above example the <code>id</code> of <code>user_03</code> was defined as a <code>uuid.UUID</code> class (which is defined under the attribute's <code>Union</code> annotation) but as the <code>uuid.UUID</code> can be marshalled into an <code>int</code> it chose to match against the <code>int</code> type and disregarded the other types.</p> <p>As such, it is recommended that, when defining <code>Union</code> annotations, the most specific type is included first and followed by less specific types. In the above example, the <code>UUID</code> class should precede the <code>int</code> and <code>str</code> classes to preclude the unexpected representation as such:</p> <pre><code>from uuid import UUID\nfrom typing import Union\nfrom pydantic import BaseModel\n\n\nclass User(BaseModel):\n    id: Union[UUID, int, str]\n    name: str\n\n\nuser_03_uuid = UUID(\"cf57432e-809e-4353-adbd-9d5c0d733868\")\nuser_03 = User(id=user_03_uuid, name=\"John Doe\")\nprint(user_03)\n# &gt; id=UUID('cf57432e-809e-4353-adbd-9d5c0d733868') name='John Doe'\nprint(user_03.id)\n# &gt; cf57432e-809e-4353-adbd-9d5c0d733868\nprint(user_03_uuid.int)\n# &gt; 275603287559914445491632874575877060712\n</code></pre>"}, {"location": "coding/python/pydantic_types/#enums-and-choices", "title": "Enums and Choices", "text": "<p>pydantic uses python's standard <code>enum</code> classes to define choices.</p> <pre><code>from enum import Enum, IntEnum\n\nfrom pydantic import BaseModel, ValidationError\n\n\nclass FruitEnum(str, Enum):\n    pear = \"pear\"\n    banana = \"banana\"\n\n\nclass ToolEnum(IntEnum):\n    spanner = 1\n    wrench = 2\n\n\nclass CookingModel(BaseModel):\n    fruit: FruitEnum = FruitEnum.pear\n    tool: ToolEnum = ToolEnum.spanner\n\n\nprint(CookingModel())\n# &gt; fruit=&lt;FruitEnum.pear: 'pear'&gt; tool=&lt;ToolEnum.spanner: 1&gt;\nprint(CookingModel(tool=2, fruit=\"banana\"))\n# &gt; fruit=&lt;FruitEnum.banana: 'banana'&gt; tool=&lt;ToolEnum.wrench: 2&gt;\ntry:\n    CookingModel(fruit=\"other\")\nexcept ValidationError as e:\n    print(e)\n\"\"\"\n    1 validation error for CookingModel\n    fruit\n      value is not a valid enumeration member; permitted: 'pear', 'banana'\n    (type=type_error.enum; enum_values=[&lt;FruitEnum.pear: 'pear'&gt;,\n    &lt;FruitEnum.banana: 'banana'&gt;])\n    \"\"\"\n</code></pre>"}, {"location": "coding/python/pydantic_types/#datetime-types", "title": "Datetime Types", "text": "<p>Pydantic supports the following datetime types:</p> <ul> <li> <p><code>datetime</code> fields can be:</p> </li> <li> <p><code>datetime</code>, existing <code>datetime</code> object</p> </li> <li> <p><code>int</code> or <code>float</code>, assumed as Unix time, i.e. seconds (if &gt;= <code>-2e10</code> or \\&lt;=     <code>2e10</code>) or milliseconds (if \\&lt; <code>-2e10</code>or &gt; <code>2e10</code>) since 1 January 1970</p> </li> <li> <p><code>str</code>, following formats work:</p> <ul> <li><code>YYYY-MM-DD[T]HH:MM[:SS[.ffffff]][Z[\u00b1]HH[:]MM]]]</code></li> <li><code>int</code> or <code>float</code> as a string (assumed as Unix time)</li> </ul> </li> <li> <p><code>date</code> fields can be:</p> </li> <li> <p><code>date</code>, existing <code>date</code> object</p> </li> <li> <p><code>int</code> or <code>float</code>, see <code>datetime</code></p> </li> <li> <p><code>str</code>, following formats work:</p> <ul> <li><code>YYYY-MM-DD</code></li> <li><code>int</code> or <code>float</code>, see <code>datetime</code></li> </ul> </li> <li> <p><code>time</code> fields can be:</p> </li> <li> <p><code>time</code>, existing <code>time</code> object</p> </li> <li> <p><code>str</code>, following formats work:</p> <ul> <li><code>HH:MM[:SS[.ffffff]]</code></li> </ul> </li> <li> <p><code>timedelta</code> fields can be:</p> </li> <li> <p><code>timedelta</code>, existing <code>timedelta</code> object</p> </li> <li> <p><code>int</code> or <code>float</code>, assumed as seconds</p> </li> <li> <p><code>str</code>, following formats work:</p> <ul> <li><code>[-][DD ][HH:MM]SS[.ffffff]</code></li> <li><code>[\u00b1]P[DD]DT[HH]H[MM]M[SS]S</code> (ISO 8601 format for timedelta)</li> </ul> </li> </ul>"}, {"location": "coding/python/pydantic_types/#type", "title": "Type", "text": "<p>pydantic supports the use of <code>Type[T]</code> to specify that a field may only accept classes (not instances) that are subclasses of <code>T</code>.</p> <pre><code>from typing import Type\n\nfrom pydantic import BaseModel\nfrom pydantic import ValidationError\n\n\nclass Foo:\n    pass\n\n\nclass Bar(Foo):\n    pass\n\n\nclass Other:\n    pass\n\n\nclass SimpleModel(BaseModel):\n    just_subclasses: Type[Foo]\n\n\nSimpleModel(just_subclasses=Foo)\nSimpleModel(just_subclasses=Bar)\ntry:\n    SimpleModel(just_subclasses=Other)\nexcept ValidationError as e:\n    print(e)\n\"\"\"\n    1 validation error for SimpleModel\n    just_subclasses\n      subclass of Foo expected (type=type_error.subclass; expected_class=Foo)\n    \"\"\"\n</code></pre>"}, {"location": "coding/python/pydantic_types/#typevar", "title": "TypeVar", "text": "<p><code>TypeVar</code> is supported either unconstrained, constrained or with a bound.</p> <pre><code>from typing import TypeVar\nfrom pydantic import BaseModel\n\nFoobar = TypeVar(\"Foobar\")\nBoundFloat = TypeVar(\"BoundFloat\", bound=float)\nIntStr = TypeVar(\"IntStr\", int, str)\n\n\nclass Model(BaseModel):\n    a: Foobar  # equivalent of \": Any\"\n    b: BoundFloat  # equivalent of \": float\"\n    c: IntStr  # equivalent of \": Union[int, str]\"\n\n\nprint(Model(a=[1], b=4.2, c=\"x\"))\n# &gt; a=[1] b=4.2 c='x'\n\n# a may be None and is therefore optional\nprint(Model(b=1, c=1))\n# &gt; a=None b=1.0 c=1\n</code></pre>"}, {"location": "coding/python/pydantic_types/#pydantic-types", "title": "Pydantic Types", "text": "<p>pydantic also provides a variety of other useful types:</p> <p><code>EmailStr</code> :</p> <p><code>FilePath</code> : like <code>Path</code>, but the path must exist and be a file.</p> <p><code>DirectoryPath</code> : like <code>Path</code>, but the path must exist and be a directory.</p> <p><code>Color</code> : for parsing HTML and CSS colors; see Color Type.</p> <p><code>Json</code> : a special type wrapper which loads JSON before parsing; see JSON Type.</p> <p><code>AnyUrl</code> : any URL; see URLs.</p> <p><code>AnyHttpUrl</code> : an HTTP URL; see URLs.</p> <p><code>HttpUrl</code> : a stricter HTTP URL; see URLs.</p> <p><code>PostgresDsn</code> : a postgres DSN style URL; see URLs.</p> <p><code>RedisDsn</code> : a redis DSN style URL; see URLs.</p> <p><code>SecretStr</code> : string where the value is kept partially secret; see Secrets.</p> <p><code>IPvAnyAddress</code> : allows either an <code>IPv4Address</code> or an <code>IPv6Address</code>.</p> <p><code>IPvAnyInterface</code> : allows either an <code>IPv4Interface</code> or an <code>IPv6Interface</code>.</p> <p><code>IPvAnyNetwork</code> : allows either an <code>IPv4Network</code> or an <code>IPv6Network</code>.</p> <p><code>NegativeFloat</code> : allows a float which is negative; uses standard <code>float</code> parsing then checks the value is less than 0; see Constrained Types.</p> <p><code>NegativeInt</code> : allows an int which is negative; uses standard <code>int</code> parsing then checks the value is less than 0; see Constrained Types.</p> <p><code>PositiveFloat</code> : allows a float which is positive; uses standard <code>float</code> parsing then checks the value is greater than 0; see Constrained Types.</p> <p><code>PositiveInt</code> : allows an int which is positive; uses standard <code>int</code> parsing then checks the value is greater than 0; see Constrained Types.</p> <p><code>condecimal</code> : type method for constraining Decimals; see Constrained Types.</p> <p><code>confloat</code> : type method for constraining floats; see Constrained Types.</p> <p><code>conint</code> : type method for constraining ints; see Constrained Types.</p> <p><code>conlist</code> : type method for constraining lists; see Constrained Types.</p> <p><code>conset</code> : type method for constraining sets; see Constrained Types.</p> <p><code>constr</code> : type method for constraining strs; see Constrained Types.</p>"}, {"location": "coding/python/pydantic_types/#custom-data-types", "title": "Custom Data Types", "text": "<p>You can also define your own custom data types. There are several ways to achieve it.</p>"}, {"location": "coding/python/pydantic_types/#classes-with-__get_validators__", "title": "Classes with <code>__get_validators__</code>", "text": "<p>You use a custom class with a classmethod <code>__get_validators__</code>. It will be called to get validators to parse and validate the input data.</p> <p>Tip</p> <p>Validators, you can declare a parameter <code>config</code>, <code>field</code>, etc.</p> <pre><code>import re\nfrom pydantic import BaseModel\n\n# https://en.wikipedia.org/wiki/Postcodes_in_the_United_Kingdom#Validation\npost_code_regex = re.compile(\n    r\"(?:\"\n    r\"([A-Z]{1,2}[0-9][A-Z0-9]?|ASCN|STHL|TDCU|BBND|[BFS]IQQ|PCRN|TKCA) ?\"\n    r\"([0-9][A-Z]{2})|\"\n    r\"(BFPO) ?([0-9]{1,4})|\"\n    r\"(KY[0-9]|MSR|VG|AI)[ -]?[0-9]{4}|\"\n    r\"([A-Z]{2}) ?([0-9]{2})|\"\n    r\"(GE) ?(CX)|\"\n    r\"(GIR) ?(0A{2})|\"\n    r\"(SAN) ?(TA1)\"\n    r\")\"\n)\n\n\nclass PostCode(str):\n\"\"\"\n    Partial UK postcode validation. Note: this is just an example, and is not\n    intended for use in production; in particular this does NOT guarantee\n    a postcode exists, just that it has a valid format.\n    \"\"\"\n\n    @classmethod\n    def __get_validators__(cls):\n        # one or more validators may be yielded which will be called in the\n        # order to validate the input, each validator will receive as an input\n        # the value returned from the previous validator\n        yield cls.validate\n\n    @classmethod\n    def __modify_schema__(cls, field_schema):\n        # __modify_schema__ should mutate the dict it receives in place,\n        # the returned value will be ignored\n        field_schema.update(\n            # simplified regex here for brevity, see the wikipedia link above\n            pattern=\"^[A-Z]{1,2}[0-9][A-Z0-9]? ?[0-9][A-Z]{2}$\",\n            # some example postcodes\n            examples=[\"SP11 9DG\", \"w1j7bu\"],\n        )\n\n    @classmethod\n    def validate(cls, v):\n        if not isinstance(v, str):\n            raise TypeError(\"string required\")\n        m = post_code_regex.fullmatch(v.upper())\n        if not m:\n            raise ValueError(\"invalid postcode format\")\n        # you could also return a string here which would mean model.post_code\n        # would be a string, pydantic won't care but you could end up with some\n        # confusion since the value's type won't match the type annotation\n        # exactly\n        return cls(f\"{m.group(1)} {m.group(2)}\")\n\n    def __repr__(self):\n        return f\"PostCode({super().__repr__()})\"\n\n\nclass Model(BaseModel):\n    post_code: PostCode\n\n\nmodel = Model(post_code=\"sw8 5el\")\nprint(model)\n# &gt; post_code=PostCode('SW8 5EL')\nprint(model.post_code)\n# &gt; SW8 5EL\nprint(Model.schema())\n\"\"\"\n{\n    'title': 'Model',\n    'type': 'object',\n    'properties': {\n        'post_code': {\n            'title': 'Post Code',\n            'pattern': '^[A-Z]{1,2}[0-9][A-Z0-9]? ?[0-9][A-Z]{2}$',\n            'examples': ['SP11 9DG', 'w1j7bu'],\n            'type': 'string',\n        },\n    },\n    'required': ['post_code'],\n}\n\"\"\"\n</code></pre>"}, {"location": "coding/python/pydantic_types/#generic-classes-as-types", "title": "Generic Classes as Types", "text": "<p>Warning</p> <p>beginning. In most of the cases you will probably be fine with standard pydantic models.</p> <p>You can use Generic Classes as field types and perform custom validation based on the \"type parameters\" (or sub-types) with <code>__get_validators__</code>.</p> <p>If the Generic class that you are using as a sub-type has a classmethod <code>__get_validators__</code> you don't need to use <code>arbitrary_types_allowed</code> for it to work.</p> <p>Because you can declare validators that receive the current <code>field</code>, you can extract the <code>sub_fields</code> (from the generic class type parameters) and validate data with them.</p> <pre><code>from pydantic import BaseModel, ValidationError\nfrom pydantic.fields import ModelField\nfrom typing import TypeVar, Generic\n\nAgedType = TypeVar(\"AgedType\")\nQualityType = TypeVar(\"QualityType\")\n\n\n# This is not a pydantic model, it's an arbitrary generic class\nclass TastingModel(Generic[AgedType, QualityType]):\n    def __init__(self, name: str, aged: AgedType, quality: QualityType):\n        self.name = name\n        self.aged = aged\n        self.quality = quality\n\n    @classmethod\n    def __get_validators__(cls):\n        yield cls.validate\n\n    @classmethod\n    # You don't need to add the \"ModelField\", but it will help your\n    # editor give you completion and catch errors\n    def validate(cls, v, field: ModelField):\n        if not isinstance(v, cls):\n            # The value is not even a TastingModel\n            raise TypeError(\"Invalid value\")\n        if not field.sub_fields:\n            # Generic parameters were not provided so we don't try to validate\n            # them and just return the value as is\n            return v\n        aged_f = field.sub_fields[0]\n        quality_f = field.sub_fields[1]\n        errors = []\n        # Here we don't need the validated value, but we want the errors\n        valid_value, error = aged_f.validate(v.aged, {}, loc=\"aged\")\n        if error:\n            errors.append(error)\n        # Here we don't need the validated value, but we want the errors\n        valid_value, error = quality_f.validate(v.quality, {}, loc=\"quality\")\n        if error:\n            errors.append(error)\n        if errors:\n            raise ValidationError(errors, cls)\n        # Validation passed without errors, return the same instance received\n        return v\n\n\nclass Model(BaseModel):\n    # for wine, \"aged\" is an int with years, \"quality\" is a float\n    wine: TastingModel[int, float]\n    # for cheese, \"aged\" is a bool, \"quality\" is a str\n    cheese: TastingModel[bool, str]\n    # for thing, \"aged\" is a Any, \"quality\" is Any\n    thing: TastingModel\n\n\nmodel = Model(\n    # This wine was aged for 20 years and has a quality of 85.6\n    wine=TastingModel(name=\"Cabernet Sauvignon\", aged=20, quality=85.6),\n    # This cheese is aged (is mature) and has \"Good\" quality\n    cheese=TastingModel(name=\"Gouda\", aged=True, quality=\"Good\"),\n    # This Python thing has aged \"Not much\" and has a quality \"Awesome\"\n    thing=TastingModel(name=\"Python\", aged=\"Not much\", quality=\"Awesome\"),\n)\nprint(model)\n\"\"\"\nwine=&lt;types_generics.TastingModel object at 0x7f3593a4eee0&gt;\ncheese=&lt;types_generics.TastingModel object at 0x7f3593a46100&gt;\nthing=&lt;types_generics.TastingModel object at 0x7f3593a464c0&gt;\n\"\"\"\nprint(model.wine.aged)\n# &gt; 20\nprint(model.wine.quality)\n# &gt; 85.6\nprint(model.cheese.aged)\n# &gt; True\nprint(model.cheese.quality)\n# &gt; Good\nprint(model.thing.aged)\n# &gt; Not much\ntry:\n    # If the values of the sub-types are invalid, we get an error\n    Model(\n        # For wine, aged should be an int with the years, and quality a float\n        wine=TastingModel(name=\"Merlot\", aged=True, quality=\"Kinda good\"),\n        # For cheese, aged should be a bool, and quality a str\n        cheese=TastingModel(name=\"Gouda\", aged=\"yeah\", quality=5),\n        # For thing, no type parameters are declared, and we skipped validation\n        # in those cases in the Assessment.validate() function\n        thing=TastingModel(name=\"Python\", aged=\"Not much\", quality=\"Awesome\"),\n    )\nexcept ValidationError as e:\n    print(e)\n\"\"\"\n    2 validation errors for Model\n    wine -&gt; quality\n      value is not a valid float (type=type_error.float)\n    cheese -&gt; aged\n      value could not be parsed to a boolean (type=type_error.bool)\n    \"\"\"\n</code></pre>"}, {"location": "coding/python/pydantic_types/#using-constrained-strings-in-list-attributes", "title": "Using constrained strings in list attributes", "text": "<p>If you try to use:</p> <pre><code>from pydantic import constr\n\nRegexp = constr(regex=\"^i-.*\")\n\n\nclass Data(pydantic.BaseModel):\n    regex: List[Regex]\n</code></pre> <p>You'll encounter the <code>Variable \"Regexp\" is not valid as a type [valid-type]</code> mypy error.</p> <p>There are a few ways to achieve this:</p>"}, {"location": "coding/python/pydantic_types/#using-typingannotated-with-pydanticfield", "title": "Using <code>typing.Annotated</code> with <code>pydantic.Field</code>", "text": "<p>Instead of using <code>constr</code> to specify the <code>regex</code> constraint, you can specify it as an argument to <code>Field</code> and then use it in combination with <code>typing.Annotated</code>:</p> <p>!!! warning \"Until this open issue is not solved, this won't work.\"</p> <p>!!! note \"<code>typing.Annotated</code> is only available since Python 3.9. For older Python versions <code>typing_extensions.Annotated</code> can be used.\"</p> <pre><code>import pydantic\nfrom pydantic import Field\nfrom typing import Annotated\n\nRegex = Annotated[str, Field(regex=\"^[0-9a-z_]*$\")]\n\n\nclass DataNotList(pydantic.BaseModel):\n    regex: Regex\n\n\ndata = DataNotList(**{\"regex\": \"abc\"})\nprint(data)\n# regex='abc'\nprint(data.json())\n# {\"regex\": \"abc\"}\n</code></pre> <p>Mypy treats <code>Annotated[str, Field(regex=\"^[0-9a-z_]*$\")]</code> as a type alias of <code>str</code>. But it also tells pydantic to do validation. This is described in the pydantic docs.</p> <p>Unfortunately it does not currently work with the following:</p> <pre><code>class Data(pydantic.BaseModel):\n    regex: List[Regex]\n    regex: Optional[Regex]\n</code></pre>"}, {"location": "coding/python/pydantic_types/#inheriting-from-pydanticconstrainedstr", "title": "Inheriting from pydantic.ConstrainedStr", "text": "<p>Instead of using <code>constr</code> to specify the regex constraint (which uses <code>pydantic.ConstrainedStr</code> internally), you can inherit from <code>pydantic.ConstrainedStr</code> directly:</p> <pre><code>import re\nimport pydantic\nfrom pydantic import Field\nfrom typing import List\n\n\nclass Regex(pydantic.ConstrainedStr):\n    regex = re.compile(\"^[0-9a-z_]*$\")\n\n\nclass Data(pydantic.BaseModel):\n    regex: List[Regex]\n\n\ndata = Data(**{\"regex\": [\"abc\", \"123\", \"asdf\"]})\nprint(data)\n# regex=['abc', '123', 'asdf']\nprint(data.json())\n# {\"regex\": [\"abc\", \"123\", \"asdf\"]}\n</code></pre> <p>Mypy accepts this happily and pydantic does correct validation. The type of <code>data.regex[i]</code> is <code>Regex</code>, but as <code>pydantic.ConstrainedStr</code> itself inherits from <code>str</code>, it can be used as a string in most places.</p>"}, {"location": "coding/python/pydantic_types/#references", "title": "References", "text": "<ul> <li>Field types</li> </ul>"}, {"location": "coding/python/pydantic_validators/", "title": "Pydantic validators", "text": "<p>Custom validation and complex relationships between objects can be achieved using the <code>validator</code> decorator.</p> <pre><code>from pydantic import BaseModel, ValidationError, validator\n\n\nclass UserModel(BaseModel):\n    name: str\n    username: str\n    password1: str\n    password2: str\n\n    @validator('name')\n    def name_must_contain_space(cls, v):\n        if ' ' not in v:\n            raise ValueError('must contain a space')\n        return v.title()\n\n    @validator('password2')\n    def passwords_match(cls, v, values, **kwargs):\n        if 'password1' in values and v != values['password1']:\n            raise ValueError('passwords do not match')\n        return v\n\n    @validator('username')\n    def username_alphanumeric(cls, v):\n        assert v.isalnum(), 'must be alphanumeric'\n        return v\n\n\nuser = UserModel(\n    name='samuel colvin',\n    username='scolvin',\n    password1='zxcvbn',\n    password2='zxcvbn',\n)\nprint(user)\n#&gt; name='Samuel Colvin' username='scolvin' password1='zxcvbn' password2='zxcvbn'\n\ntry:\n    UserModel(\n        name='samuel',\n        username='scolvin',\n        password1='zxcvbn',\n        password2='zxcvbn2',\n    )\nexcept ValidationError as e:\n    print(e)\n\"\"\"\n    2 validation errors for UserModel\n    name\n      must contain a space (type=value_error)\n    password2\n      passwords do not match (type=value_error)\n    \"\"\"\n</code></pre> <p>You need to be aware of these validator behaviours.</p> <ul> <li>Validators are \"class methods\", so the first argument value they receive is     the <code>UserModel</code> class, not an instance of <code>UserModel</code>.</li> <li>The second argument is always the field value to validate; it can be named as     you please.</li> <li>You can also add any subset of the following arguments to the signature (the     names must match):<ul> <li><code>values</code>: a dict containing the name-to-value mapping of any     previously-validated fields.</li> <li><code>config</code>: the model config.</li> <li><code>field</code>: the field being validated.</li> <li><code>**kwargs</code>: if provided, this will include the arguments above not     explicitly listed in the signature.</li> </ul> </li> <li>Validators should either return the parsed value or raise a <code>ValueError</code>,     <code>TypeError</code>, or <code>AssertionError</code> (<code>assert</code> statements may be used).</li> <li>Where validators rely on other values, you should be aware that:<ul> <li>Validation is done in the order fields are defined.</li> <li>If validation fails on another field (or that field is missing) it will     not be included in <code>values</code>, hence <code>if 'password1' in values and ...</code> in     this example.</li> </ul> </li> </ul>"}, {"location": "coding/python/pydantic_validators/#pre-and-per-item-validators", "title": "Pre and per-item validators", "text": "<p>Validators can do a few more complex things:</p> <ul> <li>A single validator can be applied to multiple fields by passing it multiple     field names.</li> <li>A single validator can also be called on all fields by passing the special     value <code>'*'</code>.</li> <li>The keyword argument <code>pre</code> will cause the validator to be called prior to     other validation.</li> <li>Passing <code>each_item=True</code> will result in the validator being applied to     individual values (e.g. of <code>List</code>, <code>Dict</code>, <code>Set</code>, etc.), rather than the     whole object.</li> </ul> <pre><code>from typing import List\nfrom pydantic import BaseModel, ValidationError, validator\n\n\nclass DemoModel(BaseModel):\n    square_numbers: List[int] = []\n    cube_numbers: List[int] = []\n\n    # '*' is the same as 'cube_numbers', 'square_numbers' here:\n    @validator('*', pre=True)\n    def split_str(cls, v):\n        if isinstance(v, str):\n            return v.split('|')\n        return v\n\n    @validator('cube_numbers', 'square_numbers')\n    def check_sum(cls, v):\n        if sum(v) &gt; 42:\n            raise ValueError('sum of numbers greater than 42')\n        return v\n\n    @validator('square_numbers', each_item=True)\n    def check_squares(cls, v):\n        assert v ** 0.5 % 1 == 0, f'{v} is not a square number'\n        return v\n\n    @validator('cube_numbers', each_item=True)\n    def check_cubes(cls, v):\n        # 64 ** (1 / 3) == 3.9999999999999996 (!)\n        # this is not a good way of checking cubes\n        assert v ** (1 / 3) % 1 == 0, f'{v} is not a cubed number'\n        return v\n\n\nprint(DemoModel(square_numbers=[1, 4, 9]))\n#&gt; square_numbers=[1, 4, 9] cube_numbers=[]\nprint(DemoModel(square_numbers='1|4|16'))\n#&gt; square_numbers=[1, 4, 16] cube_numbers=[]\nprint(DemoModel(square_numbers=[16], cube_numbers=[8, 27]))\n#&gt; square_numbers=[16] cube_numbers=[8, 27]\ntry:\n    DemoModel(square_numbers=[1, 4, 2])\nexcept ValidationError as e:\n    print(e)\n\"\"\"\n    1 validation error for DemoModel\n    square_numbers -&gt; 2\n      2 is not a square number (type=assertion_error)\n    \"\"\"\n\ntry:\n    DemoModel(cube_numbers=[27, 27])\nexcept ValidationError as e:\n    print(e)\n\"\"\"\n    1 validation error for DemoModel\n    cube_numbers\n      sum of numbers greater than 42 (type=value_error)\n    \"\"\"\n</code></pre>"}, {"location": "coding/python/pydantic_validators/#subclass-validators-and-each_item", "title": "Subclass Validators and <code>each_item</code>", "text": "<p>If using a validator with a subclass that references a <code>List</code> type field on a parent class, using <code>each_item=True</code> will cause the validator not to run; instead, the list must be iterated over programatically.</p> <pre><code>from typing import List\nfrom pydantic import BaseModel, ValidationError, validator\n\n\nclass ParentModel(BaseModel):\n    names: List[str]\n\n\nclass ChildModel(ParentModel):\n    @validator('names', each_item=True)\n    def check_names_not_empty(cls, v):\n        assert v != '', 'Empty strings are not allowed.'\n        return v\n\n\n# This will NOT raise a ValidationError because the validator was not called\ntry:\n    child = ChildModel(names=['Alice', 'Bob', 'Eve', ''])\nexcept ValidationError as e:\n    print(e)\nelse:\n    print('No ValidationError caught.')\n    #&gt; No ValidationError caught.\n\n\nclass ChildModel2(ParentModel):\n    @validator('names')\n    def check_names_not_empty(cls, v):\n        for name in v:\n            assert name != '', 'Empty strings are not allowed.'\n        return v\n\n\ntry:\n    child = ChildModel2(names=['Alice', 'Bob', 'Eve', ''])\nexcept ValidationError as e:\n    print(e)\n\"\"\"\n    1 validation error for ChildModel2\n    names\n      Empty strings are not allowed. (type=assertion_error)\n    \"\"\"\n</code></pre>"}, {"location": "coding/python/pydantic_validators/#validate-always", "title": "Validate Always", "text": "<p>For performance reasons, by default validators are not called for fields when a value is not supplied.  However there are situations where it may be useful or required to always call the validator, e.g.  to set a dynamic default value.</p> <pre><code>from datetime import datetime\n\nfrom pydantic import BaseModel, validator\n\n\nclass DemoModel(BaseModel):\n    ts: datetime = None\n\n    @validator('ts', pre=True, always=True)\n    def set_ts_now(cls, v):\n        return v or datetime.now()\n\n\nprint(DemoModel())\n#&gt; ts=datetime.datetime(2020, 7, 15, 20, 1, 48, 966302)\nprint(DemoModel(ts='2017-11-08T14:00'))\n#&gt; ts=datetime.datetime(2017, 11, 8, 14, 0)\n</code></pre> <p>You'll often want to use this together with <code>pre</code>, since otherwise with <code>always=True</code> pydantic would try to validate the default <code>None</code> which would cause an error.</p>"}, {"location": "coding/python/pydantic_validators/#reuse-validators", "title": "Reuse validators", "text": "<p>Occasionally, you will want to use the same validator on multiple fields/models (e.g. to normalize some input data). The \"naive\" approach would be to write a separate function, then call it from multiple decorators.  Obviously, this entails a lot of repetition and boiler plate code. To circumvent this, the <code>allow_reuse</code> parameter has been added to <code>pydantic.validator</code> in v1.2 (<code>False</code> by default):</p> <pre><code>from pydantic import BaseModel, validator\n\n\ndef normalize(name: str) -&gt; str:\n    return ' '.join((word.capitalize()) for word in name.split(' '))\n\n\nclass Producer(BaseModel):\n    name: str\n\n    # validators\n    _normalize_name = validator('name', allow_reuse=True)(normalize)\n\n\nclass Consumer(BaseModel):\n    name: str\n\n    # validators\n    _normalize_name = validator('name', allow_reuse=True)(normalize)\n\n\njane_doe = Producer(name='JaNe DOE')\njohn_doe = Consumer(name='joHN dOe')\nassert jane_doe.name == 'Jane Doe'\nassert john_doe.name == 'John Doe'\n</code></pre> <p>As it is obvious, repetition has been reduced and the models become again almost declarative.</p> <p>Tip</p> <p>If you have a lot of fields that you want to validate, it usually makes sense to define a help function with which you will avoid setting <code>allow_reuse=True</code> over and over again.</p>"}, {"location": "coding/python/pydantic_validators/#root-validators", "title": "Root Validators", "text": "<p>Validation can also be performed on the entire model's data.</p> <pre><code>from pydantic import BaseModel, ValidationError, root_validator\n\n\nclass UserModel(BaseModel):\n    username: str\n    password1: str\n    password2: str\n\n    @root_validator(pre=True)\n    def check_card_number_omitted(cls, values):\n        assert 'card_number' not in values, 'card_number should not be included'\n        return values\n\n    @root_validator\n    def check_passwords_match(cls, values):\n        pw1, pw2 = values.get('password1'), values.get('password2')\n        if pw1 is not None and pw2 is not None and pw1 != pw2:\n            raise ValueError('passwords do not match')\n        return values\n\n\nprint(UserModel(username='scolvin', password1='zxcvbn', password2='zxcvbn'))\n#&gt; username='scolvin' password1='zxcvbn' password2='zxcvbn'\ntry:\n    UserModel(username='scolvin', password1='zxcvbn', password2='zxcvbn2')\nexcept ValidationError as e:\n    print(e)\n\"\"\"\n    1 validation error for UserModel\n    __root__\n      passwords do not match (type=value_error)\n    \"\"\"\n\ntry:\n    UserModel(\n        username='scolvin',\n        password1='zxcvbn',\n        password2='zxcvbn',\n        card_number='1234',\n    )\nexcept ValidationError as e:\n    print(e)\n\"\"\"\n    1 validation error for UserModel\n    __root__\n      card_number should not be included (type=assertion_error)\n    \"\"\"\n</code></pre> <p>As with field validators, root validators can have <code>pre=True</code>, in which case they're called before field validation occurs (and are provided with the raw input data), or <code>pre=False</code> (the default), in which case they're called after field validation.</p> <p>Field validation will not occur if <code>pre=True</code> root validators raise an error. As with field validators, \"post\" (i.e. <code>pre=False</code>) root validators by default will be called even if prior validators fail; this behaviour can be changed by setting the <code>skip_on_failure=True</code> keyword argument to the validator.  The <code>values</code> argument will be a dict containing the values which passed field validation and field defaults where applicable.</p>"}, {"location": "coding/python/pydantic_validators/#field-checks", "title": "Field Checks", "text": "<p>On class creation, validators are checked to confirm that the fields they specify actually exist on the model.</p> <p>Occasionally however this is undesirable: e.g. if you define a validator to validate fields on inheriting models.  In this case you should set <code>check_fields=False</code> on the validator.</p>"}, {"location": "coding/python/pydantic_validators/#dataclass-validators", "title": "Dataclass Validators", "text": "<p>Validators also work with pydantic dataclasses.</p> <pre><code>from datetime import datetime\n\nfrom pydantic import validator\nfrom pydantic.dataclasses import dataclass\n\n\n@dataclass\nclass DemoDataclass:\n    ts: datetime = None\n\n    @validator('ts', pre=True, always=True)\n    def set_ts_now(cls, v):\n        return v or datetime.now()\n\n\nprint(DemoDataclass())\n#&gt; DemoDataclass(ts=datetime.datetime(2020, 7, 15, 20, 1, 48, 969037))\nprint(DemoDataclass(ts='2017-11-08T14:00'))\n#&gt; DemoDataclass(ts=datetime.datetime(2017, 11, 8, 14, 0))\n</code></pre>"}, {"location": "coding/python/pydantic_validators/#troubleshooting-validators", "title": "Troubleshooting validators", "text": ""}, {"location": "coding/python/pydantic_validators/#pylint-complains-on-the-validators", "title": "pylint complains on the validators", "text": "<p>Pylint complains that <code>R0201: Method could be a function</code> and <code>N805: first argument of a method should be named 'self'</code>. Seems to be an error of pylint, people have solved it by specifying <code>@classmethod</code> between the definition and the <code>validator</code> decorator.</p>"}, {"location": "coding/python/pydantic_validators/#references", "title": "References", "text": "<ul> <li>Pydantic validators</li> </ul>"}, {"location": "coding/python/pypika/", "title": "Pypika", "text": "<p>Pypika is a Python API for building SQL queries. The motivation behind PyPika is to provide a simple interface for building SQL queries without limiting the flexibility of handwritten SQL.</p> <p>PyPika is a fast, expressive and flexible way to replace handwritten SQL. Validation of SQL correctness is not an explicit goal of the project. Instead you are encouraged to check inputs you provide to PyPika or appropriately handle errors raised from your SQL database.</p> <p>After the queries have been built you need to interact with the database with other libraries.</p>"}, {"location": "coding/python/pypika/#installation", "title": "Installation", "text": "<pre><code>pip install pypika\n</code></pre>"}, {"location": "coding/python/pypika/#usage", "title": "Usage", "text": "<p>The main classes in pypika are <code>pypika.Query</code>, <code>pypika.Table</code>, and <code>pypika.Field</code>.</p> <pre><code>from pypika import Query, Table, Field\n</code></pre>"}, {"location": "coding/python/pypika/#creating-tables", "title": "Creating Tables", "text": "<p>The entry point for creating tables is <code>pypika.Query.create_table</code>, which is used with the class <code>pypika.Column</code>. As with selecting data, first the table should be specified. This can be either a string or a <code>pypika.Table</code>. Then the columns, and constraints..</p> <pre><code>stmt = Query \\\n    .create_table(\"person\") \\\n    .columns(\n        Column(\"id\", \"INT\", nullable=False),\n        Column(\"first_name\", \"VARCHAR(100)\", nullable=False),\n        Column(\"last_name\", \"VARCHAR(100)\", nullable=False),\n        Column(\"phone_number\", \"VARCHAR(20)\", nullable=True),\n        Column(\"status\", \"VARCHAR(20)\", nullable=False, default=ValueWrapper(\"NEW\")),\n        Column(\"date_of_birth\", \"DATETIME\")) \\\n    .unique(\"last_name\", \"first_name\") \\\n    .primary_key(\"id\")\n</code></pre> <p>This produces:</p> <pre><code>CREATE TABLE \"person\" (\n\"id\" INT NOT NULL,\n\"first_name\" VARCHAR(100) NOT NULL,\n\"last_name\" VARCHAR(100) NOT NULL,\n\"phone_number\" VARCHAR(20) NULL,\n\"status\" VARCHAR(20) NOT NULL DEFAULT 'NEW',\n\"date_of_birth\" DATETIME,\nUNIQUE (\"last_name\",\"first_name\"),\nPRIMARY KEY (\"id\")\n)\n</code></pre> <p>It seems that they don't yet support the definition of FOREIGN KEYS when creating a new table.</p>"}, {"location": "coding/python/pypika/#inserting-data", "title": "Inserting data", "text": "<p>Data can be inserted into tables either by providing the values in the query or by selecting them through another query.</p> <p>By default, data can be inserted by providing values for all columns in the order that they are defined in the table.</p>"}, {"location": "coding/python/pypika/#insert-with-values", "title": "Insert with values", "text": "<pre><code>customers = Table('customers')\n\nq = Query.into(customers).insert(1, 'Jane', 'Doe', 'jane@example.com')\n</code></pre> <pre><code>INSERT INTO customers VALUES (1,'Jane','Doe','jane@example.com')\n</code></pre> <pre><code>customers =  Table('customers')\n\nq = customers.insert(1, 'Jane', 'Doe', 'jane@example.com')\n</code></pre> <pre><code>INSERT INTO customers VALUES (1,'Jane','Doe','jane@example.com')\n</code></pre> <p>Multiple rows of data can be inserted either by chaining the insert function or passing multiple tuples as args.</p> <pre><code>customers = Table('customers')\n\nq = (\n    Query.into(customers)\n    .insert(1, \"Jane\", \"Doe\", \"jane@example.com\")\n    .insert(2, \"John\", \"Doe\", \"john@example.com\")\n)\n</code></pre> <pre><code>customers = Table('customers')\n\nq = Query.into(customers).insert(\n    (1, \"Jane\", \"Doe\", \"jane@example.com\"), (2, \"John\", \"Doe\", \"john@example.com\")\n)\n</code></pre> <pre><code>INSERT INTO \"customers\" VALUES (1,'Jane','Doe','jane@example.com'),(2,'John','Doe','john@example.com')\n</code></pre>"}, {"location": "coding/python/pypika/#insert-with-on-duplicate-key-update", "title": "Insert with on Duplicate Key Update", "text": "<pre><code>customers = Table('customers')\n\nq = Query.into(customers)\\\n    .insert(1, 'Jane', 'Doe', 'jane@example.com')\\\n    .on_duplicate_key_update(customers.email, Values(customers.email))\n</code></pre> <pre><code>INSERT INTO customers VALUES (1,'Jane','Doe','jane@example.com') ON DUPLICATE KEY UPDATE `email`=VALUES(`email`)\n</code></pre> <p><code>.on_duplicate_key_update</code> works similar to <code>.set</code> for updating rows, additionally it provides the Values wrapper to update to the value specified in the <code>INSERT</code> clause.</p>"}, {"location": "coding/python/pypika/#insert-from-a-select-sub-query", "title": "Insert from a SELECT Sub-query", "text": "<pre><code>INSERT INTO customers VALUES (1,'Jane','Doe','jane@example.com'),(2,'John','Doe','john@example.com')\n</code></pre> <p>To specify the columns and the order, use the columns function.</p> <pre><code>customers = Table('customers')\n\nq = Query.into(customers).columns('id', 'fname', 'lname').insert(1, 'Jane', 'Doe')\n</code></pre> <pre><code>INSERT INTO customers (id,fname,lname) VALUES (1,'Jane','Doe','jane@example.com')\n</code></pre> <p>Inserting data with a query works the same as querying data with the additional call to the into method in the builder chain.</p> <pre><code>customers, customers_backup = Tables('customers', 'customers_backup')\n\nq = Query.into(customers_backup).from_(customers).select('*')\n</code></pre> <pre><code>INSERT INTO customers_backup SELECT * FROM customers\n</code></pre> <pre><code>customers, customers_backup = Tables('customers', 'customers_backup')\n\nq = Query.into(customers_backup).columns('id', 'fname', 'lname')\n    .from_(customers).select(customers.id, customers.fname, customers.lname)\n</code></pre> <pre><code>INSERT INTO customers_backup SELECT \"id\", \"fname\", \"lname\" FROM customers\n</code></pre> <p>The syntax for joining tables is the same as when selecting data</p> <pre><code>customers, orders, orders_backup = Tables('customers', 'orders', 'orders_backup')\n\nq = Query.into(orders_backup).columns('id', 'address', 'customer_fname', 'customer_lname')\n    .from_(customers)\n    .join(orders).on(orders.customer_id == customers.id)\n    .select(orders.id, customers.fname, customers.lname)\n</code></pre> <pre><code>INSERT INTO \"orders_backup\" (\"id\",\"address\",\"customer_fname\",\"customer_lname\")\nSELECT \"orders\".\"id\",\"customers\".\"fname\",\"customers\".\"lname\" FROM \"customers\"\nJOIN \"orders\" ON \"orders\".\"customer_id\"=\"customers\".\"id\"\n</code></pre>"}, {"location": "coding/python/pypika/#updating", "title": "[Updating", "text": "<p>data](https://pypika.readthedocs.io/en/latest/2_tutorial.html#updating-data)</p> <p>PyPika allows update queries to be constructed with or without where clauses.</p> <pre><code>customers = Table('customers')\n\nQuery.update(customers).set(customers.last_login, '2017-01-01 10:00:00')\n\nQuery.update(customers).set(customers.lname, 'smith').where(customers.id == 10)\n</code></pre> <pre><code>UPDATE \"customers\" SET \"last_login\"='2017-01-01 10:00:00'\n\nUPDATE \"customers\" SET \"lname\"='smith' WHERE \"id\"=10\n</code></pre> <p>The syntax for joining tables is the same as when selecting data</p> <pre><code>customers, profiles = Tables('customers', 'profiles')\n\nQuery.update(customers)\n     .join(profiles).on(profiles.customer_id == customers.id)\n     .set(customers.lname, profiles.lname)\n</code></pre> <pre><code>UPDATE \"customers\"\nJOIN \"profiles\" ON \"profiles\".\"customer_id\"=\"customers\".\"id\"\nSET \"customers\".\"lname\"=\"profiles\".\"lname\"\n</code></pre> <p>Using <code>pypika.Table</code> alias to perform the update</p> <pre><code>customers = Table('customers')\n\ncustomers.update()\n        .set(customers.lname, 'smith')\n        .where(customers.id == 10)\n</code></pre> <pre><code>UPDATE \"customers\" SET \"lname\"='smith' WHERE \"id\"=10\n</code></pre> <p>Using limit for performing update</p> <pre><code>customers = Table('customers')\n\ncustomers.update()\n        .set(customers.lname, 'smith')\n        .limit(2)\n</code></pre> <pre><code>UPDATE \"customers\" SET \"lname\"='smith' LIMIT 2\n</code></pre>"}, {"location": "coding/python/pypika/#selecting-data", "title": "Selecting Data", "text": "<p>The entry point for building queries is <code>pypika.Query</code>. In order to select columns from a table, the table must first be added to the query. For simple queries with only one table, tables and columns can be references using strings. For more sophisticated queries a <code>pypika.Table</code> must be used.</p> <pre><code>q = Query.from_('customers').select('id', 'fname', 'lname', 'phone')\n</code></pre> <p>To convert the query into raw SQL, it can be cast to a string.</p> <pre><code>str(q)\n</code></pre> <p>Alternatively, you can use the Query.get_sql() function:</p> <pre><code>q.get_sql()\n</code></pre> <p>The <code>.select</code> statement doesn't need to be after the <code>.from_</code> statement. This is useful when composing a query in multiple steps, where you can do the <code>.join</code> before the <code>.select</code>.</p> <p>In simple queries like the above example, columns in the \u201cfrom\u201d table can be referenced by passing string names into the select query builder function. In more complex examples, the <code>pypika.Table</code> class should be used. Columns can be referenced as attributes on instances of <code>pypika.Table</code>.</p> <pre><code>from pypika import Table, Query\n\ncustomers = Table('customers')\nq = Query.from_(customers).select(customers.id, customers.fname, customers.lname, customers.phone)\n</code></pre> <p>Both of the above examples result in the following SQL:</p> <pre><code>SELECT id,fname,lname,phone FROM customers\n</code></pre> <p>An alias for the table can be given using the <code>.as_</code> function on <code>pypika.Table</code>.</p> <pre><code>Table('x_view_customers').as_('customers')\nq = Query.from_(customers).select(customers.id, customers.phone)\n</code></pre> <pre><code>SELECT id,phone FROM x_view_customers customers\n</code></pre> <p>An alias for the columns can also be given using the <code>.as_</code> function on the columns.</p> <pre><code>customers = Table('customers')\nq = Query.from_(customers).select(\n    customers.id.as_(\"customer.id\"), customers.fname.as_(\"customer.name\")\n)\n</code></pre> <pre><code>SELECT \"id\" \"customer.id\",\"fname\" \"customer.name\" FROM customers\n</code></pre> <p>A schema can also be specified. Tables can be referenced as attributes on the schema.</p> <pre><code>from pypika import Table, Query, Schema\n\nviews = Schema('views')\nq = Query.from_(views.customers).select(customers.id, customers.phone)\n</code></pre> <pre><code>SELECT id,phone FROM views.customers\n</code></pre> <p>Also references to databases can be used. Schemas can be referenced as attributes on the database.</p> <pre><code>from pypika import Table, Query, Database\n\nmy_db = Database('my_db')\nq = Query.from_(my_db.analytics.customers).select(customers.id, customers.phone)\n</code></pre> <pre><code>SELECT id,phone FROM my_db.analytics.customers\n</code></pre> <p>Results can be ordered by using the following syntax:</p> <pre><code>from pypika import Order\nQuery.from_('customers').select('id', 'fname', 'lname', 'phone').orderby('id', order=Order.desc)\n</code></pre> <p>This results in the following SQL:</p> <pre><code>SELECT \"id\",\"fname\",\"lname\",\"phone\" FROM \"customers\" ORDER BY \"id\" DESC\n</code></pre>"}, {"location": "coding/python/pypika/#filtering", "title": "Filtering", "text": "<p>Queries can be filtered with <code>pypika.Criterion</code> by using equality or inequality operators.</p> <pre><code>customers = Table('customers')\nq = Query.from_(customers).select(\n    customers.id, customers.fname, customers.lname, customers.phone\n).where(\n    customers.lname == 'Mustermann'\n)\n</code></pre> <pre><code>SELECT id,fname,lname,phone FROM customers WHERE lname='Mustermann'\n</code></pre> <p>Query methods such as select, where, groupby, and orderby can be called multiple times. Multiple calls to the where method will add additional conditions as:</p> <pre><code>customers = Table('customers')\nq = Query.from_(customers).select(\n    customers.id, customers.fname, customers.lname, customers.phone\n).where(\n    customers.fname == 'Max'\n).where(\n    customers.lname == 'Mustermann'\n)\n</code></pre> <pre><code>SELECT id,fname,lname,phone FROM customers WHERE fname='Max' AND lname='Mustermann'\n</code></pre> <p>Filters such as IN and BETWEEN are also supported.</p> <pre><code>customers = Table('customers')\nq = Query.from_(customers).select(\n    customers.id,customers.fname\n).where(\n    customers.age[18:65] &amp; customers.status.isin(['new', 'active'])\n)\n</code></pre> <pre><code>SELECT id,fname FROM customers WHERE age BETWEEN 18 AND 65 AND status IN ('new','active')\n</code></pre> <p>Filtering with complex criteria can be created using boolean symbols <code>&amp;</code>, <code>|</code>, and <code>^</code>.</p> <ul> <li> <p>AND</p> <pre><code>customers = Table('customers')\nq = Query.from_(customers).select(\n    customers.id, customers.fname, customers.lname, customers.phone\n).where(\n    (customers.age &gt;= 18) &amp; (customers.lname == 'Mustermann')\n)\n</code></pre> <pre><code>SELECT id,fname,lname,phone FROM customers WHERE age&gt;=18 AND lname='Mustermann'\n</code></pre> </li> <li> <p>OR</p> <pre><code>customers = Table('customers')\nq = Query.from_(customers).select(\n    customers.id, customers.fname, customers.lname, customers.phone\n).where(\n    (customers.age &gt;= 18) | (customers.lname == 'Mustermann')\n)\n</code></pre> <pre><code>SELECT id,fname,lname,phone FROM customers WHERE age&gt;=18 OR lname='Mustermann'\n</code></pre> </li> <li> <p>XOR</p> <pre><code>customers = Table('customers')\nq = Query.from_(customers).select(\n    customers.id, customers.fname, customers.lname, customers.phone\n).where(\n    (customers.age &gt;= 18) ^ customers.is_registered\n)\n</code></pre> <pre><code>SELECT id,fname,lname,phone FROM customers WHERE age&gt;=18 XOR is_registered\n</code></pre> </li> </ul> <p>Using the REGEXP filter</p> <p>Pypika supports regex, but if you're using sqlite3 you need to configure the connection to the database.</p>"}, {"location": "coding/python/pypika/#joining-tables-and-subqueries", "title": "Joining tables and subqueries", "text": "<p>Tables and subqueries can be joined to any query using the <code>Query.join()</code> method. Joins can be performed with either a <code>USING</code> or <code>ON</code> clauses. The <code>USING</code> clause can be used when both tables/subqueries contain the same field and the <code>ON</code> clause can be used with a criterion. To perform a join, <code>...join()</code> can be chained but then must be followed immediately by <code>...on(&lt;criterion&gt;)</code> or <code>...using(*field)</code>.</p>"}, {"location": "coding/python/pypika/#join-types", "title": "Join Types", "text": "<p>All join types are supported by PyPika.</p> <pre><code>Query \\\n    .from_(base_table)\n    ...\n    .join(join_table, JoinType.left)\n    ...\n\nQuery \\\n    .from_(base_table)\n    ...\n    .left_join(join_table) \\\n    .right_join(join_table) \\\n    .inner_join(join_table) \\\n    .outer_join(join_table) \\\n    .cross_join(join_table) \\\n    ...\n</code></pre>"}, {"location": "coding/python/pypika/#example-of-a-join-using-on", "title": "Example of a join using ON", "text": "<pre><code>history, customers = Tables('history', 'customers')\nq = Query \\\n    .from_(history) \\\n    .join(customers) \\\n    .on(history.customer_id == customers.id) \\\n    .select(history.star) \\\n    .where(customers.id == 5)\n</code></pre> <pre><code>SELECT \"history\".* FROM \"history\" JOIN \"customers\" ON \"history\".\"customer_id\"=\"customers\".\"id\" WHERE \"customers\".\"id\"=5\n</code></pre>"}, {"location": "coding/python/pypika/#example-of-a-join-using-on_field", "title": "Example of a join using ON_FIELD", "text": "<p>As a shortcut, the <code>Query.join().on_field()</code> function is provided for joining the (first) table in the <code>FROM</code> clause with the joined table when the field name(s) are the same in both tables.</p> <pre><code>history, customers = Tables('history', 'customers')\nq = Query \\\n    .from_(history) \\\n    .join(customers) \\\n    .on_field('customer_id', 'group') \\\n    .select(history.star) \\\n    .where(customers.group == 'A')\n</code></pre> <pre><code>SELECT \"history\".* FROM \"history\" JOIN \"customers\" ON \"history\".\"customer_id\"=\"customers\".\"customer_id\" AND \"history\".\"group\"=\"customers\".\"group\" WHERE \"customers\".\"group\"='A'\n</code></pre>"}, {"location": "coding/python/pypika/#example-of-a-join-using-using", "title": "Example of a join using USING", "text": "<pre><code>history, customers = Tables('history', 'customers')\nq = Query \\\n    .from_(history) \\\n    .join(customers) \\\n    .using('customer_id') \\\n    .select(history.star) \\\n    .where(customers.id == 5)\n</code></pre> <pre><code>SELECT \"history\".* FROM \"history\" JOIN \"customers\" USING \"customer_id\" WHERE \"customers\".\"id\"=5\n</code></pre>"}, {"location": "coding/python/pypika/#example-of-a-correlated-subquery-in-the-select", "title": "Example of a correlated subquery in the SELECT", "text": "<pre><code>history, customers = Tables('history', 'customers')\n\nlast_purchase_at = Query.from_(history).select(\n    history.purchase_at\n).where(history.customer_id==customers.customer_id).orderby(\n    history.purchase_at, order=Order.desc\n).limit(1)\n\nq = Query.from_(customers).select(\n    customers.id, last_purchase_at._as('last_purchase_at')\n)\n</code></pre> <pre><code>SELECT\n\"id\",\n(SELECT \"history\".\"purchase_at\"\nFROM \"history\"\nWHERE \"history\".\"customer_id\" = \"customers\".\"customer_id\"\nORDER BY \"history\".\"purchase_at\" DESC\nLIMIT 1) \"last_purchase_at\"\nFROM \"customers\"\n</code></pre>"}, {"location": "coding/python/pypika/#deleting-data", "title": "Deleting data", "text": "<pre><code>Query.from_(table).delete().where(table.id == id)\n</code></pre>"}, {"location": "coding/python/pypika/#references", "title": "References", "text": "<ul> <li>Docs</li> <li>Source</li> </ul>"}, {"location": "coding/python/pytest/", "title": "Python pytest", "text": "<p>pytest is a Python framework to makes it easy to write small tests, yet scales to support complex functional testing for applications and libraries.</p> <p>Pytest stands out over other test frameworks in:</p> <ul> <li>Simple tests are simple to write in pytest.</li> <li>Complex tests are still simple to write.</li> <li>Tests are easy to read.</li> <li>You can get started in seconds.</li> <li>You use <code>assert</code> to fail a test, not things like <code>self.assertEqual()</code> or   <code>self.assertLessThan()</code>. Just <code>assert</code>.</li> <li>You can use pytest to run tests written for unittest or nose.</li> </ul> <p>Note: You can use this cookiecutter template to create a python project with <code>pytest</code> already configured.</p>"}, {"location": "coding/python/pytest/#install", "title": "Install", "text": "<pre><code>pip install pytest\n</code></pre>"}, {"location": "coding/python/pytest/#usage", "title": "Usage", "text": "<p>Run in the project directory.</p> <pre><code>pytest\n</code></pre> <p>If you need more information run it with <code>-v</code>.</p> <p>Pytest automatically finds which tests to run in a phase called test discovery. It will get the tests that match one of the following conditions:</p> <ul> <li>Test files that are named <code>test_{{ something }}.py</code> or   <code>{{ something }}_test.py</code>.</li> <li>Test methods and functions named <code>test_{{ something }}</code>.</li> <li>Test classes named <code>Test{{ Something }}</code>.</li> </ul> <p>There are several possible outcomes of a test function:</p> <ul> <li>PASSED (.): The test ran successfully.</li> <li>FAILED (F): The test did not run usccessfully (or XPASS + strict).</li> <li>SKIPPED (s): The test was skipped. You can tell pytest to skip a test by   using enter the <code>@pytest.mark.skip()</code> or <code>pytest.mark.skipif()</code> decorators.</li> <li>xfail (x): The test was not supposed to pass, ran, and failed. You can tell   pytest that a test is expected to fail by using the <code>@pytest.mark.xfail()</code>   decorator.</li> <li>XPASS (X): The tests was not supposed to pass, ran, and passed.</li> <li>ERROR (E): An exception happened outside of the test function, in either a   fixture or a hook function.</li> </ul> <p>Pytest supports several cool flags like:</p> <ul> <li><code>-k EXPRESSION</code>: Used to select a subset of tests to run. For example   <code>pytest   -k \"asdict or defaults\"</code> will run both <code>test_asdict()</code> and   <code>test_defaults()</code>.</li> <li><code>--lf</code> or <code>--last-failed</code>: Just run the tests that have failed in the previous   run.</li> <li><code>-x</code>, or <code>--exitfirst</code>: Exit on first failed test.</li> <li><code>-l</code> or <code>--showlocals</code>: Print out the local variables in a test if the test   fails.</li> <li><code>-s</code> Allows any output that normally would be printed to <code>stdout</code> to actually   be printed to <code>stdout</code>. It's an alias of <code>--capture=no</code>, so the output is not   captured when the tests are run, which is the default behavior. This is useful   to debug with <code>print()</code> statements.</li> <li><code>--durations=N</code>: It reports the slowest <code>N</code> number of tests/setups/teardowns   after the test run. If you pass in <code>--durations=0</code>, it reports everything in   order of slowest to fastest.</li> <li><code>--setup-show</code>: Show the fixtures in use.</li> <li><code>--tb=long</code>: To see the tracebacks of the exceptions raised while running the program.</li> </ul>"}, {"location": "coding/python/pytest/#fixtures", "title": "Fixtures", "text": "<p>Fixtures are functions that are run by pytest before (and sometimes after) the actual test functions.</p> <p>You can use fixtures to get a data set for the tests to work on, or use them to get a system into a known state before running a test. They are also used to get data ready for multiple tests.</p> <p>Here's a simple fixture that returns a number:</p> <pre><code>import pytest\n\n@pytest.fixture()\ndef some_data()\n\"\"\" Return answer to the ultimate question \"\"\"\n    return 42\n\ndef test_some_data(some_data):\n\"\"\" Use fixture return value in a test\"\"\"\n    assert some_data == 42\n</code></pre> <p>The <code>@pytest.fixture()</code> decorator is used to tell pytest that a function is a fixture.When you include the fixture name in the parameter list of a test function,pytest knows to run it before running the test. Fixtures can do work, and can also return data to the test function.</p> <p>The test test_some_data() has the name of the fixture, some_data, as a parameter.pytest will see this and look for a fixture with this name. Naming is significant in pytest. pytest will look in the module of the test for a fixture of that name.</p> <p>If the function is defined in the same file as where it's being used pylint will raise an <code>W0621: Redefining name %r from outer scope (line %s)</code> error. To solve it either move the fixture to other file or name the decorated function <code>fixture_&lt;fixturename&gt;</code> and then use <code>@pytest.fixture(name='&lt;fixturename&gt;')</code>.</p>"}, {"location": "coding/python/pytest/#sharing-fixtures-through-conftestpy", "title": "Sharing fixtures through conftest.py", "text": "<p>You can put fixtures into individual test files, but to share fixtures among multiple test files, you need to use a <code>conftest.py</code> file somewhere centrally located for all of the tests. Additionally you can have <code>conftest.py</code> files in subdirectories of the top <code>tests</code> directory. If you do, fixtures defined in these lower level <code>conftest.py</code> files will be available to tests in that directory and subdirectories.</p> <p>Although <code>conftest.py</code> is a Python module, it should not be imported by test files. The file gets read by pytest, and is considered a local plugin.</p> <p>Another option is to save the fixtures in a file by creating a local pytest plugin.</p> <p>File: <code>tests/unit/conftest.py</code></p> <pre><code>pytest_plugins = [\n    \"tests.unit.fixtures.some_stuff\",\n]\n</code></pre> <p>File: <code>tests/unit/fixtures/some_stuff.py</code>:</p> <pre><code>import pytest\n\n\n@pytest.fixture\ndef foo():\n    return \"foobar\"\n</code></pre>"}, {"location": "coding/python/pytest/#specifying-fixture-scope", "title": "Specifying fixture scope", "text": "<p>Fixtures include an optional parameter called scope, which controls how often a fixture gets set up and torn down. The scope parameter to <code>@pytest.fixture()</code> can have the values of function, class, module, or session.</p> <p>Here\u2019s a rundown of each scope value:</p> <ul> <li><code>scope='function'</code>: Run once per test function. The setup portion is run   before each test using the fixture. The teardown portion is run after each   test using the fixture. This is the default scope used when no scope parameter   is specified.</li> <li><code>scope='class'</code>: Run once per test class, regardless of how many test methods   are in the class.</li> <li><code>scope='module'</code>: Run once per module, regardless of how many test functions   or methods or other fixtures in the module use it.</li> <li><code>scope='session'</code> Run once per session. All test methods and functions using a   fixture of session scope share one setup and teardown call.</li> </ul>"}, {"location": "coding/python/pytest/#using-fixtures-at-class-level", "title": "Using fixtures at class level", "text": "<p>Sometimes test functions do not directly need access to a fixture object. For example, tests may require to operate with an empty directory as the current working directory but otherwise do not care for the concrete directory.</p> <pre><code>@pytest.mark.usefixtures(\"cleandir\")\nclass TestDirectoryInit:\n    ...\n</code></pre> <p>Due to the <code>usefixtures</code> marker, the <code>cleandir</code> fixture will be required for the execution of each test method, just as if you specified a <code>cleandir</code> function argument to each of them.</p> <p>You can specify multiple fixtures like this:</p> <pre><code>@pytest.mark.usefixtures(\"cleandir\", \"anotherfixture\")\n</code></pre>"}, {"location": "coding/python/pytest/#useful-fixtures", "title": "Useful Fixtures", "text": ""}, {"location": "coding/python/pytest/#the-tmp_path-fixture", "title": "The tmp_path fixture", "text": "<p>You can use the <code>tmp_path</code> fixture which will provide a temporary directory unique to the test invocation, created in the base temporary directory.</p> <p><code>tmp_path</code> is a <code>pathlib.Path</code> object. Here is an example test usage:</p> <pre><code>def test_create_file(tmp_path):\n    d = tmp_path / \"sub\"\n    d.mkdir()\n    p = d / \"hello.txt\"\n    p.write_text(CONTENT)\n    assert p.read_text() == CONTENT\n    assert len(list(tmp_path.iterdir())) == 1\n    assert 0\n</code></pre>"}, {"location": "coding/python/pytest/#the-tmpdir-fixture", "title": "The tmpdir fixture", "text": "<p>Warning: Don't use <code>tmpdir</code> use <code>tmp_path</code> instead because <code>tmpdir</code> uses <code>py</code> which is unmaintained and has unpatched vulnerabilities.</p> <p>You can use the <code>tmpdir</code> fixture which will provide a temporary directory unique to the test invocation, created in the base temporary directory.</p> <p><code>tmpdir</code> is a <code>py.path.local</code> object which offers <code>os.path</code> methods and more. Here is an example test usage:</p> <p>File: <code>test_tmpdir.py</code>:</p> <pre><code>from py._path.local import LocalPath\n\n\ndef test_create_file(tmpdir: LocalPath):\n    p = tmpdir.mkdir(\"sub\").join(\"hello.txt\")\n    p.write(\"content\")\n    assert p.read() == \"content\"\n    assert len(tmpdir.listdir()) == 1\n    assert 0\n</code></pre> <p>The <code>tmpdir</code> fixture has a scope of <code>function</code> so you can't make a session directory. Instead use the <code>tmpdir_factory</code> fixture.</p> <pre><code>from _pytest.tmpdir import TempPathFactory\n\n\n@pytest.fixture(scope=\"session\")\ndef image_file(tmpdir_factory: TempPathFactory):\n    img = compute_expensive_image()\n    fn = tmpdir_factory.mktemp(\"data\").join(\"img.png\")\n    img.save(str(fn))\n    return fn\n\n\ndef test_histogram(image_file):\n    img = load_image(image_file)\n    # compute and test histogram\n</code></pre>"}, {"location": "coding/python/pytest/#make-a-subdirectory", "title": "Make a subdirectory", "text": "<pre><code>p = tmpdir.mkdir(\"sub\").join(\"hello.txt\")\n</code></pre>"}, {"location": "coding/python/pytest/#the-caplog-fixture", "title": "The caplog fixture", "text": "<p>pytest captures log messages of level WARNING or above automatically and displays them in their own section for each failed test in the same manner as captured stdout and stderr.</p> <p>You can change the default logging level in the pytest configuration:</p> <p>File: <code>pytest.ini</code>:</p> <pre><code>[pytest]\n\nlog_level = debug\n</code></pre> <p>Although it may not be a good idea in most cases. It's better to change the log level in the tests that need a lower level.</p> <p>All the logs sent to the logger during the test run are available on the fixture in the form of both the <code>logging.LogRecord</code> instances and the final log text. This is useful for when you want to assert on the contents of a message:</p> <pre><code>from _pytest.logging import LogCaptureFixture\n\n\ndef test_baz(caplog: LogCaptureFixture):\n    func_under_test()\n    for record in caplog.records:\n        assert record.levelname != \"CRITICAL\"\n    assert \"wally\" not in caplog.text\n</code></pre> <p>You can also resort to record_tuples if all you want to do is to ensure that certain messages have been logged under a given logger name with a given severity and message:</p> <pre><code>def test_foo(caplog: LogCaptureFixture):\n    logging.getLogger().info(\"boo %s\", \"arg\")\n\n    assert (\"root\", logging.INFO, \"boo arg\") in caplog.record_tuples\n</code></pre> <p>You can call <code>caplog.clear()</code> to reset the captured log records in a test.</p>"}, {"location": "coding/python/pytest/#change-the-log-level", "title": "Change the log level", "text": "<p>Inside tests it's possible to change the log level for the captured log messages.</p> <pre><code>def test_foo(caplog: LogCaptureFixture):\n    caplog.set_level(logging.INFO)\n    pass\n</code></pre> <p>If you just want to change the log level of a dependency you can use:</p> <pre><code>caplog.set_level(logging.WARNING, logger=\"urllib3\")\n</code></pre>"}, {"location": "coding/python/pytest/#the-capsys-fixture", "title": "The capsys fixture", "text": "<p>The <code>capsys</code> builtin fixture provides two bits of functionality: it allows you to retrieve stdout and stderr from some code, and it disables output capture temporarily.</p> <p>Suppose you have a function to print a greeting to stdout:</p> <pre><code>def greeting(name):\n    print(f\"Hi, {name}\")\n</code></pre> <p>You can test the output by using <code>capsys</code>.</p> <pre><code>from _pytest.capture import CaptureFixture\n\n\ndef test_greeting(capsys: CaptureFixture[Any]):\n    greeting(\"Earthling\")\n    out, err = capsys.readouterr()\n    assert out == \"Hi, Earthling\\n\"\n    assert err == \"\"\n</code></pre> <p>The return value is whatever has been captured since the beginning of the function, or from the last time it was called.</p>"}, {"location": "coding/python/pytest/#freezegun", "title": "freezegun", "text": "<p>freezegun lets you freeze time in both the test and fixtures.</p>"}, {"location": "coding/python/pytest/#install_1", "title": "Install", "text": "<pre><code>pip install pytest-freezegun\n</code></pre>"}, {"location": "coding/python/pytest/#usage_1", "title": "Usage", "text": ""}, {"location": "coding/python/pytest/#global-usage", "title": "Global usage", "text": "<p>Most of the tests work with frozen time, so it's better to freeze it by default and unfreeze it on the ones that actually need time to move.</p> <p>To do that set in your <code>tests/conftest.py</code> a globally used fixture:</p> <pre><code>if TYPE_CHECKING:\n    from freezegun.api import FrozenDateTimeFactory\n\n\n@pytest.fixture(autouse=True)\ndef frozen_time() -&gt; Generator[FrozenDateTimeFactory, None, None]:\n\"\"\"Freeze all tests time\"\"\"\n    with freezegun.freeze_time() as freeze:\n        yield freeze\n</code></pre> <p>Freeze time by using the freezer fixture:</p>"}, {"location": "coding/python/pytest/#manual-use", "title": "Manual use", "text": "<pre><code>if TYPE_CHECKING:\n    from freezegun.api import FrozenDateTimeFactory\n\n\ndef test_frozen_date(freezer: FrozenDateTimeFactory):\n    now = datetime.now()\n    time.sleep(1)\n    later = datetime.now()\n    assert now == later\n</code></pre> <p>This can then be used to move time:</p> <pre><code>def test_moving_date(freezer):\n    now = datetime.now()\n    freezer.move_to(\"2017-05-20\")\n    later = datetime.now()\n    assert now != later\n</code></pre> <p>You can also pass arguments to freezegun by using the <code>freeze_time</code> mark:</p> <pre><code>@pytest.mark.freeze_time(\"2017-05-21\")\ndef test_current_date():\n    assert date.today() == date(2017, 5, 21)\n</code></pre> <p>The <code>freezer</code> fixture and <code>freeze_time</code> mark can be used together, and they work with other fixtures:</p> <pre><code>@pytest.fixture\ndef current_date():\n    return date.today()\n\n\n@pytest.mark.freeze_time()\ndef test_changing_date(current_date, freezer):\n    freezer.move_to(\"2017-05-20\")\n    assert current_date == date(2017, 5, 20)\n    freezer.move_to(\"2017-05-21\")\n    assert current_date == date(2017, 5, 21)\n</code></pre> <p>They can also be used in class-based tests:</p> <pre><code>class TestDate:\n    @pytest.mark.freeze_time\n    def test_changing_date(self, current_date, freezer):\n        freezer.move_to(\"2017-05-20\")\n        assert current_date == date(2017, 5, 20)\n        freezer.move_to(\"2017-05-21\")\n        assert current_date == date(2017, 5, 21)\n</code></pre>"}, {"location": "coding/python/pytest/#customize-nested-fixtures", "title": "Customize nested fixtures", "text": "<p>Sometimes you need to tweak your fixtures so they can be used in different tests. As usual, there are different solutions to the same problem.</p> <p>Note: \"TL;DR: For simple cases parametrize your fixtures or use parametrization to override the default valued fixture. As your test suite get's more complex migrate to pytest-case.\"</p> <p>Let's say you're running along merrily with some fixtures that create database objects for you:</p> <pre><code>@pytest.fixture\ndef supplier(db):\n    s = Supplier(\n        ref=random_ref(),\n        name=random_name(),\n        country=\"US\",\n    )\n    db.add(s)\n    yield s\n    db.remove(s)\n\n\n@pytest.fixture()\ndef product(db, supplier):\n    p = Product(\n        ref=random_ref(),\n        name=random_name(),\n        supplier=supplier,\n        net_price=9.99,\n    )\n    db.add(p)\n    yield p\n    db.remove(p)\n</code></pre> <p>And now you're writing a new test and you suddenly realize you need to customize your default \"supplier\" fixture:</p> <pre><code>def test_US_supplier_has_total_price_equal_net_price(product):\n    assert product.total_price == product.net_price\n\n\ndef test_EU_supplier_has_total_price_including_VAT(supplier, product):\n    supplier.country = \"FR\"  # oh, this doesn't work\n    assert product.total_price == product.net_price * 1.2\n</code></pre> <p>There are different ways to modify your fixtures</p>"}, {"location": "coding/python/pytest/#add-more-fixtures", "title": "Add more fixtures", "text": "<p>We can just create more fixtures, and try to do a bit of DRY by extracting out common logic:</p> <pre><code>def _default_supplier():\n    return Supplier(\n        ref=random_ref(),\n        name=random_name(),\n    )\n\n\n@pytest.fixture\ndef us_supplier(db):\n    s = _default_supplier()\n    s.country = \"US\"\n    db.add(s)\n    yield s\n    db.remove(s)\n\n\n@pytest.fixture\ndef eu_supplier(db):\n    s = _default_supplier()\n    s.country = \"FR\"\n    db.add(s)\n    yield s\n    db.remove(s)\n</code></pre> <p>That's just one way you could do it, maybe you can figure out ways to reduce the duplication of the <code>db.add()</code> stuff as well, but you are going to have a different, named fixture for each customization of Supplier, and eventually you may decide that doesn't scale.</p>"}, {"location": "coding/python/pytest/#use-factory-fixtures", "title": "Use factory fixtures", "text": "<p>Instead of a fixture returning an object directly, it can return a function that creates an object, and that function can take arguments:</p> <pre><code>@pytest.fixture\ndef make_supplier(db):\n    s = Supplier(\n        ref=random_ref(),\n        name=random_name(),\n    )\n\n    def _make_supplier(country):\n        s.country = country\n        db.add(s)\n        return s\n\n    yield _make_supplier\n    db.remove(s)\n</code></pre> <p>The problem with this is that, once you start, you tend to have to go all the way, and make all of your fixture hierarchy into factory functions:</p> <pre><code>def test_EU_supplier_has_total_price_including_VAT(make_supplier, product):\n    supplier = make_supplier(country=\"FR\")\n    product.supplier = (\n        supplier  # OH, now this doesn't work, because it's too late again\n    )\n    assert product.total_price == product.net_price * 1.2\n</code></pre> <p>And so...</p> <pre><code>@pytest.fixture\ndef make_product(db):\n    p = Product(\n        ref=random_ref(),\n        name=random_name(),\n    )\n\n    def _make_product(supplier):\n        p.supplier = supplier\n        db.add(p)\n        return p\n\n    yield _make_product\n    db.remove(p)\n\n\ndef test_EU_supplier_has_total_price_including_VAT(make_supplier, make_product):\n    supplier = make_supplier(country=\"FR\")\n    product = make_product(supplier=supplier)\n    assert product.total_price == product.net_price * 1.2\n</code></pre> <p>That works, but firstly now everything is a factory-fixture, which makes them more convoluted, and secondly, your tests are filling up with extra calls to <code>make_things</code>, and you're having to embed some of the domain knowledge of what-depends-on-what into your tests as well as your fixtures. Ugly!</p>"}, {"location": "coding/python/pytest/#parametrize-your-fixtures", "title": "Parametrize your fixtures", "text": "<p>You can also parametrize your fixtures.</p> <pre><code>@pytest.fixture(params=[\"US\", \"FR\"])\ndef supplier(db, request):\n    s = Supplier(ref=random_ref(), name=random_name(), country=request.param)\n    db.add(s)\n    yield s\n    db.remove(s)\n</code></pre> <p>Now any test that depends on supplier, directly or indirectly, will be run twice, once with <code>supplier.country = US</code> and once with <code>FR</code>.</p> <p>That's really cool for checking that a given piece of logic works in a variety of different cases, but it's not really ideal in our case. We have to build a bunch of if logic into our tests:</p> <pre><code>def test_US_supplier_has_no_VAT_but_EU_supplier_has_total_price_including_VAT(product):\n    # this test is magically run twice, but:\n    if product.supplier.country == \"US\":\n        assert product.total_price == product.net_price\n    if product.supplier.country == \"FR\":\n        assert product.total_price == product.net_price * 1.2\n</code></pre> <p>So that's ugly, and on top of that, now every single test that depends (indirectly) on supplier gets run twice, and some of those extra test runs may be totally irrelevant to what the country is.</p>"}, {"location": "coding/python/pytest/#use-pytest-parametrization-to-override-the-default-valued-fixtures", "title": "Use pytest parametrization to override the default valued fixtures", "text": "<p>We introduce an extra fixture that holds a default value for the country field:</p> <pre><code>@pytest.fixture()\ndef country():\n    return \"US\"\n\n\n@pytest.fixture\ndef supplier(db, country):\n    s = Supplier(\n        ref=random_ref(),\n        name=random_name(),\n        country=country,\n    )\n    db.add(s)\n    yield s\n    db.remove(s)\n</code></pre> <p>And then in the tests that need to change it, we can use parametrize to override the default value of country, even though the country fixture isn't explicitly named in that test:</p> <pre><code>@pytest.mark.parametrize(\"country\", [\"US\"])\ndef test_US_supplier_has_total_price_equal_net_price(product):\n    assert product.total_price == product.net_price\n\n\n@pytest.mark.parametrize(\"country\", [\"EU\"])\ndef test_EU_supplier_has_total_price_including_VAT(product):\n    assert product.total_price == product.net_price * 1.2\n</code></pre> <p>The only problem is that you're now likely to build a implicit dependencies where the only way to find out what's actually happening is to spend ages spelunking in conftest.py.</p>"}, {"location": "coding/python/pytest/#use-pytest-case", "title": "Use pytest-case", "text": "<p>pytest-case gives a lot of power when it comes to tweaking the fixtures and parameterizations.</p> <p>Check that file for further information.</p>"}, {"location": "coding/python/pytest/#use-a-fixture-more-than-once-in-a-function", "title": "Use a fixture more than once in a function", "text": "<p>One solution is to make your fixture return a factory instead of the resource directly:</p> <pre><code>@pytest.fixture(name='make_user')\ndef make_user_():\n    created = []\n    def make_user():\n        u = models.User()\n        u.commit()\n        created.append(u)\n        return u\n\n    yield make_user\n\n    for u in created:\n        u.delete()\n\ndef test_two_users(make_user):\n    user1 = make_user()\n    user2 = make_user()\n    # test them\n\n\n# you can even have the normal fixture when you only need a single user\n@pytest.fixture\ndef user(make_user):\n    return make_user()\n\ndef test_one_user(user):\n    # test him/her\n</code></pre>"}, {"location": "coding/python/pytest/#marks", "title": "Marks", "text": "<p>Pytest marks can be used to group tests. It can be useful to:</p> <p><code>slow</code> : Mark the tests that are slow.</p> <p><code>secondary</code> : Mart the tests that use functionality that is being tested in the same file.</p> <p>To mark a test, use the <code>@pytest.mark</code> decorator. For example:</p> <pre><code>@pytest.mark.slow\ndef test_really_slow_test():\n    pass\n</code></pre> <p>Pytest requires you to register your marks, do so in the <code>pytest.ini</code> file</p> <pre><code>[pytest]\nmarkers =\nslow: marks tests as slow (deselect with '-m \"not slow\"')\nsecondary: mark tests that use functionality tested in the same file (deselect with '-m \"not secondary\"')\n</code></pre>"}, {"location": "coding/python/pytest/#snippets", "title": "Snippets", "text": ""}, {"location": "coding/python/pytest/#mocking-sysexit", "title": "Mocking sys.exit", "text": "<pre><code>with pytest.raises(SystemExit):\n    # Code to test\n</code></pre>"}, {"location": "coding/python/pytest/#testing-exceptions-with-pytest", "title": "Testing exceptions with pytest", "text": "<pre><code>def test_value_error_is_raised():\n    with pytest.raises(ValueError, match=\"invalid literal for int() with base 10: 'a'\"):\n        int(\"a\")\n</code></pre>"}, {"location": "coding/python/pytest/#excluding-code-from-coverage", "title": "Excluding code from coverage", "text": "<p>You may have code in your project that you know won't be executed, and you want to tell <code>coverage.py</code> to ignore it. For example, if you have some code in abstract classes that is going to be tested on the subclasses, you can ignore it with <code># pragma: no cover</code>.</p> <p>If you want other code to be excluded, for example the statements inside the <code>if TYPE_CHECKING:</code> add to your <code>pyproject.toml</code>:</p> <pre><code>[tool.coverage.report]\nexclude_lines = [ \"pragma: no cover\", \"if TYPE_CHECKING:\",]\n</code></pre>"}, {"location": "coding/python/pytest/#running-tests-in-parallel", "title": "Running tests in parallel", "text": "<p><code>pytest-xdist</code> makes it possible to run the tests in parallel, useful when the test suit is large or when the tests are slow.</p>"}, {"location": "coding/python/pytest/#installation", "title": "Installation", "text": "<pre><code>pip install pytest-xdist\n</code></pre>"}, {"location": "coding/python/pytest/#usage_2", "title": "Usage", "text": "<pre><code>pytest -n 4\n</code></pre> <p>It will run the tests with <code>4</code> workers. If you use <code>auto</code> it will adapt the number of workers to the number of CPUS, or 1 if you use <code>--pdb</code>.</p> <p>To configure it in the <code>pyproject.toml</code> use the <code>addopts</code></p> <pre><code>[tool.pytest.ini_options]\nminversion = \"6.0\"\naddopts = \"-vv --tb=short -n auto\"\n</code></pre>"}, {"location": "coding/python/pytest/#enforce-serial-execution-of-related-tests", "title": "Enforce serial execution of related tests", "text": ""}, {"location": "coding/python/pytest/#use-a-lock", "title": "Use a lock", "text": "<p>Implement a <code>serial</code> fixture with a session-scoped file <code>lock</code> fixture using the <code>filelock</code> package. You can add this to your <code>conftest.py</code>:</p> <pre><code>pip install filelock\n</code></pre> <pre><code>import contextlib\nimport os\n\nimport filelock\nimport pytest\nfrom filelock import BaseFileLock\n\n\n@pytest.fixture(name=\"lock\", scope=\"session\")\ndef lock_(\n    tmp_path_factory: pytest.TempPathFactory,\n) -&gt; Generator[BaseFileLock, None, None]:\n\"\"\"Create lock file.\"\"\"\n    base_temp = tmp_path_factory.getbasetemp()\n    lock_file = base_temp.parent / \"serial.lock\"\n\n    yield FileLock(lock_file=str(lock_file))\n\n    with contextlib.suppress(OSError):\n        os.remove(path=lock_file)\n\n\n@pytest.fixture(name=\"serial\")\ndef _serial(lock: BaseFileLock) -&gt; Generator[None, None, None]:\n\"\"\"Fixture to run tests in serial.\"\"\"\n    with lock.acquire(poll_interval=0.1):\n        yield\n</code></pre> <p>Then inject the <code>serial</code> fixture in any test that requires serial execution. All tests that use the serial fixture are executed serially while any tests that do not use the fixture are executed in parallel.</p>"}, {"location": "coding/python/pytest/#mark-them-and-run-separately", "title": "Mark them and run separately", "text": "<p>Mark the tests you want to execute serially with a special mark, say serial:</p> <pre><code>@pytest.mark.serial\nclass Test:\n    ...\n\n\n@pytest.mark.serial\ndef test_foo():\n    ...\n</code></pre> <p>Execute your parallel tests, excluding those with the serial mark:</p> <pre><code>$ py.test -n auto -m \"not serial\"\n</code></pre> <p>Next, execute your serial tests in a separate session:</p> <pre><code>$ py.test -n0 -m \"serial\"\n</code></pre>"}, {"location": "coding/python/pytest/#setting-a-timeout-for-your-tests", "title": "Setting a timeout for your tests", "text": "<p>To make your tests fail if they don't end in less than X seconds, use pytest-timeout.</p> <p>Install it with:</p> <pre><code>pip install pytest-timeout\n</code></pre> <p>You can set a global timeout in your <code>pyproject.toml</code>:</p> <pre><code>[pytest]\ntimeout = 300\n</code></pre> <p>Or define it for each test with:</p> <pre><code>@pytest.mark.timeout(60)\ndef test_foo():\n    pass\n</code></pre>"}, {"location": "coding/python/pytest/#rerun-tests-that-fail-sometimes", "title": "Rerun tests that fail sometimes", "text": "<p>pytest-rerunfailures is a plugin for pytest that re-runs tests to eliminate intermittent failures. Using this plugin is generally a bad idea, it would be best to solve the reason why your code is not reliable. It's useful when you rely on non robust third party software in a way that you can't solve, or if the error is not in your code but in the testing code, and again you are not able to fix it.</p> <p>Install it with:</p> <pre><code>pip install pytest-rerunfailures\n</code></pre> <p>To re-run all test failures, use the <code>--reruns</code> command line option with the maximum number of times you\u2019d like the tests to run:</p> <pre><code>pytest --reruns 5\n</code></pre> <p>Failed fixture or setup_class will also be re-executed.</p> <p>To add a delay time between re-runs use the <code>--reruns-delay</code> command line option with the amount of seconds that you would like wait before the next test re-run is launched:</p> <pre><code>pytest --reruns 5 --reruns-delay 1\n</code></pre> <p>To mark individual tests as flaky, and have them automatically re-run when they fail, add the <code>flaky</code> mark with the maximum number of times you\u2019d like the test to run:</p> <pre><code>@pytest.mark.flaky(reruns=5)\ndef test_example():\n    import random\n\n    assert random.choice([True, False])\n</code></pre>"}, {"location": "coding/python/pytest/#run-tests-in-a-random-order", "title": "Run tests in a random order", "text": "<p><code>pytest-random-order</code> is a pytest plugin that randomises the order of tests. This can be useful to detect a test that passes just because it happens to run after an unrelated test that leaves the system in a favourable state.</p> <p>To use it add the <code>--random-order</code> to your pytest run.</p>"}, {"location": "coding/python/pytest/#capture-deprecation-warnings", "title": "Capture deprecation warnings", "text": "<p>Python and its ecosystem does not have an assumption of strict SemVer, and has a tradition of providing deprecation warnings. If you have good CI, you should be able to catch warnings even before your users see them. Try the following pytest configuration:</p> <pre><code>[tool.pytest.ini_options]\nfilterwarnings = [ \"error\",]\n</code></pre> <p>This will turn warnings into errors and allow your CI to break before users break.</p> <p>You can ignore specific warnings as well. For example, the configuration below will ignore all user warnings and specific deprecation warnings matching a regex, but will transform all other warnings into errors.</p> <pre><code>[tool.pytest.ini_options]\nfilterwarnings = [ \"error\", \"ignore::UserWarning\", \"ignore:function ham\\\\(\\\\) is deprecated:DeprecationWarning\",]\n</code></pre> <p>When a warning matches more than one option in the list, the action for the last matching option is performed.</p> <p>If you want to ignore the warning of a specific package use:</p> <pre><code>filterwarnings = [ \"error\", \"ignore::DeprecationWarning:pytest_freezegun.*\",]\n</code></pre> <p>Note: It's better to suppress a warning instead of disabling it for the whole code, check how here.</p>"}, {"location": "coding/python/pytest/#ensuring-code-triggers-a-deprecation-warning", "title": "Ensuring code triggers a deprecation warning", "text": "<p>You can also use pytest.deprecated_call() for checking that a certain function call triggers a <code>DeprecationWarning</code> or <code>PendingDeprecationWarning</code>:</p> <pre><code>import pytest\n\n\ndef test_myfunction_deprecated():\n    with pytest.deprecated_call():\n        myfunction(17)\n</code></pre>"}, {"location": "coding/python/pytest/#asserting-warnings-with-the-warns-function", "title": "Asserting warnings with the warns function", "text": "<p>You can check that code raises a particular warning using pytest.warns(), which works in a similar manner to raises:</p> <pre><code>import warnings\nimport pytest\n\n\ndef test_warning():\n    with pytest.warns(UserWarning):\n        warnings.warn(\"my warning\", UserWarning)\n</code></pre> <p>The test will fail if the warning in question is not raised. The keyword argument match to assert that the exception matches a text or regex:</p> <pre><code>&gt;&gt;&gt; with pytest.warns(UserWarning, match='must be 0 or None'):\n...     warnings.warn(\"value must be 0 or None\", UserWarning)\n</code></pre>"}, {"location": "coding/python/pytest/#recording-warnings", "title": "Recording warnings", "text": "<p>You can record raised warnings either using <code>pytest.warns()</code> or with the <code>recwarn</code> fixture.</p> <p>To record with <code>pytest.warns()</code> without asserting anything about the warnings, pass no arguments as the expected warning type and it will default to a generic Warning:</p> <pre><code>with pytest.warns() as record:\n    warnings.warn(\"user\", UserWarning)\n    warnings.warn(\"runtime\", RuntimeWarning)\n\nassert len(record) == 2\nassert str(record[0].message) == \"user\"\nassert str(record[1].message) == \"runtime\"\n</code></pre> <p>The <code>recwarn</code> fixture will record warnings for the whole function:</p> <pre><code>import warnings\n\n\ndef test_hello(recwarn):\n    warnings.warn(\"hello\", UserWarning)\n    assert len(recwarn) == 1\n    w = recwarn.pop(UserWarning)\n    assert issubclass(w.category, UserWarning)\n    assert str(w.message) == \"hello\"\n    assert w.filename\n    assert w.lineno\n</code></pre> <p>Both <code>recwarn</code> and <code>pytest.warns()</code> return the same interface for recorded warnings: a <code>WarningsRecorder</code> instance. To view the recorded warnings, you can iterate over this instance, call <code>len</code> on it to get the number of recorded warnings, or index into it to get a particular recorded warning.</p>"}, {"location": "coding/python/pytest/#show-logging-messages-on-the-test-run", "title": "Show logging messages on the test run", "text": "<p>Add to your <code>pyproject.toml</code>:</p> <pre><code>[tool.pytest.ini_options]\nlog_cli = true\nlog_cli_level = 10\n</code></pre> <p>Or run it in the command itself <code>pytest -o log_cli=true --log-cli-level=10 func.py</code>.</p> <p>Remember you can change the log level of the different components in case it's too verbose.</p>"}, {"location": "coding/python/pytest/#test-asyncio-programs", "title": "Test asyncio programs", "text": "<p>Check the asyncio article.</p>"}, {"location": "coding/python/pytest/#pytest-integration-with-vim", "title": "Pytest integration with Vim", "text": "<p>Integrating pytest into your Vim workflow enhances your productivity while writing code, thus making it easier to code using TDD.</p> <p>I use Janko-m's Vim-test plugin (which can be installed through Vundle) with the following configuration.</p> <pre><code>nmap &lt;silent&gt; t :TestNearest --pdb&lt;CR&gt;\nnmap &lt;silent&gt; &lt;leader&gt;t :TestSuite tests/unit&lt;CR&gt;\nnmap &lt;silent&gt; &lt;leader&gt;i :TestSuite tests/integration&lt;CR&gt;\nnmap &lt;silent&gt; &lt;leader&gt;T :TestFile&lt;CR&gt;\n\nlet test#python#runner = 'pytest'\nlet test#strategy = \"neovim\"\n</code></pre> <p>I often open Vim with a vertical split (<code>:vs</code>), in the left window I have the tests and in the right the code. Whenever I want to run a single test I press <code>t</code> when the cursor is inside that test. If you need to make changes in the code, you can press <code>t</code> again while the cursor is at the code you are testing and it will run the last test.</p> <p>Once the unit test has passed, I run the whole unit tests with <code>;t</code> (as <code>;</code> is my <code>&lt;leader&gt;</code>). And finally I use <code>;i</code> to run the integration tests.</p> <p>Finally, if the test suite is huge, I use <code>;T</code> to run only the tests of a single file.</p> <p>As you can see only the <code>t</code> has the <code>--pdb</code> flag, so the rest of them will run en parallel and any pdb trace will fail.</p>"}, {"location": "coding/python/pytest/#reference", "title": "Reference", "text": "<ul> <li> <p>Book   Python Testing with pytest by Brian Okken.</p> </li> <li> <p>Docs</p> </li> <li> <p>Vim-test plugin</p> </li> </ul>"}, {"location": "coding/python/pytest_cases/", "title": "Pytest cases", "text": "<p>pytest-cases is a pytest plugin that allows you to separate your test cases from your test functions.</p> <p>In addition, <code>pytest-cases</code> provides several useful goodies to empower <code>pytest</code>. In particular it improves the fixture mechanism to support \"fixture unions\". This is a major change in the internal <code>pytest</code> engine, unlocking many possibilities such as using fixture references as parameter values in a test function.</p>"}, {"location": "coding/python/pytest_cases/#installing", "title": "Installing", "text": "<pre><code>pip install pytest_cases\n</code></pre> <p>Installing pytest-cases has effects on the order of <code>pytest</code> tests execution, even if you do not use its features. One positive side effect is that it fixed pytest#5054. But if you see less desirable ordering please report it.</p>"}, {"location": "coding/python/pytest_cases/#why-pytest-cases", "title": "Why <code>pytest-cases</code>?", "text": "<p>Let's consider the following <code>foo</code> function under test, located in <code>example.py</code>:</p> <pre><code>def foo(a, b):\n    return a + 1, b + 1\n</code></pre> <p>If we were using plain <code>pytest</code> to test it with various inputs, we would create a <code>test_foo.py</code> file and use <code>@pytest.mark.parametrize</code>:</p> <pre><code>import pytest\nfrom example import foo\n\n@pytest.mark.parametrize(\"a,b\", [(1, 2), (-1, -2)])\ndef test_foo(a, b):\n    # check that foo runs correctly and that the result is a tuple.\n    assert isinstance(foo(a, b), tuple)\n</code></pre> <p>This is the fastest and most compact thing to do when you have a few number of test cases, that do not require code to generate each test case.</p> <p>Now imagine that instead of <code>(1, 2)</code> and <code>(-1, -2)</code> each of our test cases:</p> <ul> <li>Requires a few lines of code to be generated.</li> <li>Requires documentation to explain the other developers the intent of that     precise test case.</li> <li>Requires external resources (data files on the filesystem, databases...),     with a variable number of cases depending on what is available on the     resource.</li> <li>Requires a readable <code>id</code>, such as     <code>'uniformly_sampled_nonsorted_with_holes'</code> for the above example. Of course     we could use     <code>pytest.param</code>     or     <code>ids=&lt;list&gt;</code>     but that is \"a pain to maintain\" according to <code>pytest</code> doc. Such a design     does not feel right as the id is detached from the case.</li> </ul> <p>With standard <code>pytest</code> there is no particular pattern to simplify your life here. Investigating a little bit, people usually end up trying to mix parameters and fixtures and asking this kind of question: so1, so2. But by design it is not possible to solve this problem using fixtures, because <code>pytest</code> does not handle \"unions\" of fixtures.</p> <p>There is also an example in <code>pytest</code> doc with a <code>metafunc</code> hook.</p> <p>The issue with such workarounds is that you can do anything. And anything is a bit too much: this does not provide any convention / \"good practice\" on how to organize test cases, which is an open door to developing ad-hoc unreadable or unmaintainable solutions.</p> <p><code>pytest_cases</code> was created to provide an answer to this precise situation. It proposes a simple framework to separate test cases from test functions. The test cases are typically located in a separate \"companion\" file:</p> <ul> <li><code>test_foo.py</code> is your usual test file containing the test functions (named     <code>test_&lt;id&gt;</code>).</li> <li><code>test_foo_cases.py</code> contains the test cases, that are also functions. Note: an     alternate file naming style <code>cases_foo.py</code> is also available if you prefer     it.</li> </ul>"}, {"location": "coding/python/pytest_cases/#basic-usage", "title": "Basic usage", "text": ""}, {"location": "coding/python/pytest_cases/#case-functions", "title": "Case functions", "text": "<p>Let's create a <code>test_foo_cases.py</code> file. This file will contain test cases generator functions, that we will call case functions for brevity. In these functions, you will typically either parse some test data files, generate some simulated test data, expected results, etc.</p> <p>File: test_foo_cases.py</p> <pre><code>def case_two_positive_ints():\n\"\"\" Inputs are two positive integers \"\"\"\n    return 1, 2\n\ndef case_two_negative_ints():\n\"\"\" Inputs are two negative integers \"\"\"\n    return -1, -2\n</code></pre> <p>Case functions can return anything that is considered useful to run the associated test. You can use all classic pytest mechanism on case functions (id customization, skip/fail marks, parametrization or fixtures injection).</p>"}, {"location": "coding/python/pytest_cases/#test-functions", "title": "Test functions", "text": "<p>As usual we write our <code>pytest</code> test functions starting with <code>test_</code>, in a <code>test_foo.py</code> file. The only difference is that we now decorate it with <code>@parametrize_with_cases</code> instead of <code>@pytest.mark.parametrize</code> as we were doing previously:</p> <p>File: test_foo.py</p> <pre><code>from example import foo\nfrom pytest_cases import parametrize_with_cases\n\n@parametrize_with_cases(\"a,b\")\ndef test_foo(a, b):\n    # check that foo runs correctly and that the result is a tuple.\n    assert isinstance(foo(a, b), tuple)\n</code></pre> <p>Executing <code>pytest</code> will now run our test function once for every case function:</p> <pre><code>&gt;&gt;&gt; pytest -s -v\n============================= test session starts =============================\n(...)\n&lt;your_project&gt;/tests/test_foo.py::test_foo[two_positive_ints] PASSED [ 50%]\n&lt;your_project&gt;/tests/test_foo.py::test_foo[two_negative_ints] PASSED [ 100%]\n\n========================== 2 passed in 0.24 seconds ==========================\n</code></pre>"}, {"location": "coding/python/pytest_cases/#usage", "title": "Usage", "text": ""}, {"location": "coding/python/pytest_cases/#cases-collection", "title": "Cases collection", "text": ""}, {"location": "coding/python/pytest_cases/#alternate-sources", "title": "Alternate source(s)", "text": "<p>It is not mandatory that case functions should be in a different file than the test functions: both can be in the same file. For this you can use <code>cases='.'</code> or <code>cases=THIS_MODULE</code> to refer to the module in which the test function is located:</p> <pre><code>from pytest_cases import parametrize_with_cases\n\ndef case_one_positive_int():\n    return 1\n\ndef case_one_negative_int():\n    return -1\n\n@parametrize_with_cases(\"i\", cases='.')\ndef test_with_this_module(i):\n    assert i == int(i)\n</code></pre> <p>Only the case functions defined BEFORE the test function in the module file will be taken into account.</p> <p><code>@parametrize_with_cases(cases=...)</code> also accepts explicit list of case functions, classes containing case functions, and modules. See API Reference for details. A typical way to organize cases is to use classes for example:</p> <pre><code>from pytest_cases import parametrize_with_cases\n\nclass Foo:\n    def case_a_positive_int(self):\n        return 1\n\n    def case_another_positive_int(self):\n        return 2\n\n@parametrize_with_cases(\"a\", cases=Foo)\ndef test_foo(a):\n    assert a &gt; 0\n</code></pre> <p>Note that as for <code>pytest</code>, <code>self</code> is recreated for every test and therefore should not be used to store any useful information.</p>"}, {"location": "coding/python/pytest_cases/#alternate-prefix", "title": "Alternate prefix", "text": "<p><code>case_</code> might not be your preferred prefix, especially if you wish to store in the same module or class various kind of case data.  <code>@parametrize_with_cases</code> offers a <code>prefix=...</code> argument to select an alternate prefix for your case functions. That way, you can store in the same module or class case functions as diverse as datasets (e.g. <code>data_</code>), user descriptions (e.g. <code>user_</code>), algorithms or machine learning models (e.g. <code>model_</code> or <code>algo_</code>), etc.</p> <pre><code>from pytest_cases import parametrize_with_cases, parametrize\n\ndef data_a():\n    return 'a'\n\n@parametrize(\"hello\", [True, False])\ndef data_b(hello):\n    return \"hello\" if hello else \"world\"\n\ndef case_c():\n    return dict(name=\"hi i'm not used\")\n\ndef user_bob():\n    return \"bob\"\n\n@parametrize_with_cases(\"data\", cases='.', prefix=\"data_\")\n@parametrize_with_cases(\"user\", cases='.', prefix=\"user_\")\ndef test_with_data(data, user):\n    assert data in ('a', \"hello\", \"world\")\n    assert user == 'bob'\n</code></pre> <p>Yields</p> <pre><code>test_doc_filters_n_tags.py::test_with_data[bob-a]       PASSED [ 33%]\ntest_doc_filters_n_tags.py::test_with_data[bob-b-True]   PASSED [ 66%]\ntest_doc_filters_n_tags.py::test_with_data[bob-b-False]   PASSED [ 100%]\n</code></pre>"}, {"location": "coding/python/pytest_cases/#filters-and-tags", "title": "Filters and tags", "text": "<p>The easiest way to select only a subset of case functions in a module or a class, is to specify a custom <code>prefix</code> instead of the default one (<code>'case_'</code>).</p> <p>However sometimes more advanced filtering is required. In that case, you can also rely on three additional mechanisms provided in <code>@parametrize_with_cases</code>:</p> <ul> <li> <p>The <code>glob</code> argument can contain a glob-like pattern for case ids. This can     become handy to separate for example good or bad cases, the latter returning     an expected error type and/or message for use with <code>pytest.raises</code> or with     our alternative     <code>assert_exception</code>.</p> <pre><code>from math import sqrt\nimport pytest\nfrom pytest_cases import parametrize_with_cases\n\n\ndef case_int_success():\n    return 1\n\ndef case_negative_int_failure():\n    # note that we decide to return the expected type of failure to check it\n    return -1, ValueError, \"math domain error\"\n\n\n@parametrize_with_cases(\"data\", cases='.', glob=\"*success\")\ndef test_good_datasets(data):\n    assert sqrt(data) &gt; 0\n\n@parametrize_with_cases(\"data, err_type, err_msg\", cases='.', glob=\"*failure\")\ndef test_bad_datasets(data, err_type, err_msg):\n    with pytest.raises(err_type, match=err_msg):\n        sqrt(data)\n</code></pre> </li> <li> <p>The <code>has_tag</code> argument allows you to filter cases based on tags set on case     functions using the <code>@case</code> decorator. See API reference of     <code>@case</code> and     <code>@parametrize_with_cases</code>.</p> <pre><code>from pytest_cases import parametrize_with_cases, case\n\nclass FooCases:\n    def case_two_positive_ints(self):\n        return 1, 2\n\n    @case(tags='foo')\n    def case_one_positive_int(self):\n        return 1\n\n@parametrize_with_cases(\"a\", cases=FooCases, has_tag='foo')\ndef test_foo(a):\n    assert a &gt; 0\n</code></pre> </li> <li> <p>Finally if none of the above matches your expectations, you can provide     a callable to <code>filter</code>. This callable will receive each collected case     function and should return <code>True</code> in case of success. Note that your     function can leverage the <code>_pytestcase</code> attribute available on the case     function to read the tags, marks and id found on it.</p> <pre><code>@parametrize_with_cases(\"data\", cases='.',\n                        filter=lambda cf: \"success\" in cf._pytestcase.id)\ndef test_good_datasets2(data):\n    assert sqrt(data) &gt; 0\n</code></pre> </li> </ul> <p>pytest marks such as <code>@pytest.mark.skipif</code> can be applied on case functions the same way as with test functions.</p> <pre><code>import sys\nimport pytest\n\n@pytest.mark.skipif(sys.version_info &lt; (3, 0), reason=\"Not useful on python 2\")\ndef case_two_positive_ints():\n    return 1, 2\n</code></pre>"}, {"location": "coding/python/pytest_cases/#pytest-marks-skip-xfail-on-cases", "title": "Pytest marks (<code>skip</code>, <code>xfail</code>...) on cases", "text": ""}, {"location": "coding/python/pytest_cases/#case-generators", "title": "Case generators", "text": "<p>In many real-world usage we want to generate one test case per <code>&lt;something&gt;</code>. The most intuitive way would be to use a <code>for</code> loop to create the case functions, and to use the <code>@case</code> decorator to set their names ; however this would not be very readable.</p> <p>Instead, case functions can be parametrized the same way as with test functions: simply add the parameter names as arguments in their signature and decorate with <code>@pytest.mark.parametrize</code>. Even better, you can use the enhanced <code>@parametrize</code> from <code>pytest-cases</code> so as to benefit from its additional usability features:</p> <pre><code>from pytest_cases import parametrize, parametrize_with_cases\n\nclass CasesFoo:\n    def case_hello(self):\n        return \"hello world\"\n\n    @parametrize(who=('you', 'there'))\n    def case_simple_generator(self, who):\n        return \"hello %s\" % who\n\n\n@parametrize_with_cases(\"msg\", cases=CasesFoo)\ndef test_foo(msg):\n    assert isinstance(msg, str) and msg.startswith(\"hello\")\n</code></pre> <p>Yields</p> <pre><code>test_generators.py::test_foo[hello] PASSED               [ 33%]\ntest_generators.py::test_foo[simple_generator-who=you] PASSED [ 66%]\ntest_generators.py::test_foo[simple_generator-who=there] PASSED [100%]\n</code></pre>"}, {"location": "coding/python/pytest_cases/#cases-requiring-fixtures", "title": "Cases requiring fixtures", "text": "<p>Cases can use fixtures the same way as test functions do: simply add the fixture names as arguments in their signature and make sure the fixture exists either in the same module, or in a <code>conftest.py</code> file in one of the parent packages. See <code>pytest</code> documentation on sharing fixtures.</p> <p>Use <code>@fixture</code> instead of <code>@pytest.fixture</code></p> <p>If a fixture is used by some of your cases only, then you should use the <code>@fixture</code> decorator from pytest-cases instead of the standard <code>@pytest.fixture</code>. Otherwise you fixture will be setup/teardown for all cases even those not requiring it. See <code>@fixture</code> doc.</p> <pre><code>from pytest_cases import parametrize_with_cases, fixture, parametrize\n\n@fixture(scope='session')\ndef db():\n    return {0: 'louise', 1: 'bob'}\n\ndef user_bob(db):\n    return db[1]\n\n@parametrize(id=range(2))\ndef user_from_db(db, id):\n    return db[id]\n\n@parametrize_with_cases(\"a\", cases='.', prefix='user_')\ndef test_users(a, db, request):\n    print(\"this is test %r\" % request.node.nodeid)\n    assert a in db.values()\n</code></pre> <p>Yields</p> <pre><code>test_fixtures.py::test_users[a_is_bob]\ntest_fixtures.py::test_users[a_is_from_db-id=0]\ntest_fixtures.py::test_users[a_is_from_db-id=1]\n</code></pre>"}, {"location": "coding/python/pytest_cases/#parametrize-fixtures-with-cases", "title": "Parametrize fixtures with cases", "text": "<p>In some scenarios you might wish to parametrize a fixture with the cases, rather than the test function. For example:</p> <ul> <li>To inject the same test cases in several test functions without     duplicating the <code>@parametrize_with_cases</code> decorator on each of them.</li> <li>To generate the test cases once for the whole session, using     a <code>scope='session'</code> fixture or another     scope.</li> <li>To modify the test cases, log some message, or perform some other action     before injecting them into the test functions, and/or after executing the     test function (thanks to yield     fixtures).</li> </ul> <p>For this, simply use <code>@fixture</code> from <code>pytest_cases</code> instead of <code>@pytest.fixture</code> to define your fixture. That allows your fixtures to be easily parametrized with <code>@parametrize_with_cases</code>, <code>@parametrize</code>, and even <code>@pytest.mark.parametrize</code>.</p> <pre><code>from pytest_cases import fixture, parametrize_with_cases\n\n@fixture\n@parametrize_with_cases(\"a,b\")\ndef c(a, b):\n    return a + b\n\ndef test_foo(c):\n    assert isinstance(c, int)\n</code></pre>"}, {"location": "coding/python/pytest_cases/#pytest-cases-internals", "title": "Pytest-cases internals", "text": ""}, {"location": "coding/python/pytest_cases/#fixture", "title": "<code>@fixture</code>", "text": "<p><code>@fixture</code> is similar to <code>pytest.fixture</code> but without its <code>param</code> and <code>ids</code> arguments. Instead, it is able to pick the parametrization from <code>@pytest.mark.parametrize</code> marks applied on fixtures. This makes it very intuitive for users to parametrize both their tests and fixtures.</p> <p>Finally it now supports unpacking, see unpacking feature.</p> <p><code>@fixture</code> deprecation if/when <code>@pytest.fixture</code> supports <code>@pytest.mark.parametrize</code></p> <p>The ability for pytest fixtures to support the <code>@pytest.mark.parametrize</code> annotation is a feature that clearly belongs to <code>pytest</code> scope, and has been requested already. It is therefore expected that <code>@fixture</code> will be deprecated in favor of <code>@pytest_fixture</code> if/when the <code>pytest</code> team decides to add the proposed feature. As always, deprecation will happen slowly across versions (at least two minor, or one major version update) so as for users to have the time to update their code bases.</p>"}, {"location": "coding/python/pytest_cases/#unpack_fixture-unpack_into", "title": "<code>unpack_fixture</code> / <code>unpack_into</code>", "text": "<p>In some cases fixtures return a tuple or a list of items. It is not easy to refer to a single of these items in a test or another fixture. With <code>unpack_fixture</code> you can easily do it:</p> <pre><code>import pytest\nfrom pytest_cases import unpack_fixture, fixture\n\n@fixture\n@pytest.mark.parametrize(\"o\", ['hello', 'world'])\ndef c(o):\n    return o, o[0]\n\na, b = unpack_fixture(\"a,b\", c)\n\ndef test_function(a, b):\n    assert a[0] == b\n</code></pre> <p>Note that you can also use the <code>unpack_into=</code> argument of <code>@fixture</code> to do the same thing:</p> <pre><code>import pytest\nfrom pytest_cases import fixture\n\n@fixture(unpack_into=\"a,b\")\n@pytest.mark.parametrize(\"o\", ['hello', 'world'])\ndef c(o):\n    return o, o[0]\n\ndef test_function(a, b):\n    assert a[0] == b\n</code></pre> <p>And it is also available in <code>fixture_union</code>:</p> <pre><code>import pytest\nfrom pytest_cases import fixture, fixture_union\n\n@fixture\n@pytest.mark.parametrize(\"o\", ['hello', 'world'])\ndef c(o):\n    return o, o[0]\n\n@fixture\n@pytest.mark.parametrize(\"o\", ['yeepee', 'yay'])\ndef d(o):\n    return o, o[0]\n\nfixture_union(\"c_or_d\", [c, d], unpack_into=\"a, b\")\n\ndef test_function(a, b):\n    assert a[0] == b\n</code></pre>"}, {"location": "coding/python/pytest_cases/#param_fixtures", "title": "<code>param_fixture[s]</code>", "text": "<p>If you wish to share some parameters across several fixtures and tests, it might be convenient to have a fixture representing this parameter. This is relatively easy for single parameters, but a bit harder for parameter tuples.</p> <p>The two utilities functions <code>param_fixture</code> (for a single parameter name) and <code>param_fixtures</code> (for a tuple of parameter names) handle the difficulty for you:</p> <pre><code>import pytest\nfrom pytest_cases import param_fixtures, param_fixture\n\n# create a single parameter fixture\nmy_parameter = param_fixture(\"my_parameter\", [1, 2, 3, 4])\n\n@pytest.fixture\ndef fixture_uses_param(my_parameter):\n    ...\n\ndef test_uses_param(my_parameter, fixture_uses_param):\n    ...\n\n# -----\n# create a 2-tuple parameter fixture\narg1, arg2 = param_fixtures(\"arg1, arg2\", [(1, 2), (3, 4)])\n\n@pytest.fixture\ndef fixture_uses_param2(arg2):\n    ...\n\ndef test_uses_param2(arg1, arg2, fixture_uses_param2):\n    ...\n</code></pre>"}, {"location": "coding/python/pytest_cases/#fixture_union", "title": "<code>fixture_union</code>", "text": "<p>As of <code>pytest</code> 5, it is not possible to create a \"union\" fixture, i.e. a parametrized fixture that would first take all the possible values of fixture A, then all possible values of fixture B, etc. Indeed all fixture dependencies of each test node are grouped together, and if they have parameters a big \"cross-product\" of the parameters is done by <code>pytest</code>.</p> <pre><code>from pytest_cases import fixture, fixture_union\n\n@fixture\ndef first():\n    return 'hello'\n\n@fixture(params=['a', 'b'])\ndef second(request):\n    return request.param\n\n# c will first take all the values of 'first', then all of 'second'\nc = fixture_union('c', [first, second])\n\ndef test_basic_union(c):\n    print(c)\n</code></pre> <p>yields</p> <pre><code>&lt;...&gt;::test_basic_union[c_is_first] hello   PASSED\n&lt;...&gt;::test_basic_union[c_is_second-a] a    PASSED\n&lt;...&gt;::test_basic_union[c_is_second-b] b    PASSED\n</code></pre>"}, {"location": "coding/python/pytest_cases/#references", "title": "References", "text": "<ul> <li>Docs</li> <li>Git</li> </ul>"}, {"location": "coding/python/pytest_parametrized_testing/", "title": "Python parametrized testing", "text": "<p>Parametrization is a process of running the same test with varying sets of data. Each combination of a test and data is counted as a new test case.</p> <p>There are multiple ways to parametrize your tests, each differs in complexity and flexibility.</p>"}, {"location": "coding/python/pytest_parametrized_testing/#parametrize-the-test", "title": "Parametrize the test", "text": "<p>The most simple form of parametrization is at test level:</p> <pre><code>@pytest.mark.parametrize(\"number\", [1, 2, 3, 0, 42])\ndef test_foo(number):\n    assert number &gt; 0\n</code></pre> <p>In this case we are getting five tests: for number 1, 2, 3, 0 and 42. Each of those tests can fail independently of one another (if in this example the test with 0 will fail, and four others will pass).</p>"}, {"location": "coding/python/pytest_parametrized_testing/#parametrize-the-fixtures", "title": "Parametrize the fixtures", "text": "<p>Fixtures may have parameters. Those parameters are passed as a list to the argument <code>params</code> of <code>@pytest.fixture()</code> decorator.</p> <p>Those parameters must be iterables, such as lists. Each parameter to a fixture is applied to each function using this fixture. If a few fixtures are used in one test function, pytest generates a Cartesian product of parameters of those fixtures.</p> <p>To use those parameters, a fixture must consume a special fixture named <code>request</code>. It provides the special (built-in) fixture with some information on the function it deals with. <code>request</code> also contains <code>request.param</code> which contains one element from <code>params</code>. The fixture called as many times as the number of elements in the iterable of <code>params</code> argument, and the test function is called with values of fixtures the same number of times. (basically, the fixture is called <code>len(iterable)</code> times with each next element of iterable in the <code>request.param</code>).</p> <pre><code>@pytest.fixture(params=[\"one\", \"uno\"])\ndef fixture1(request):\n    return request.param\n\n@pytest.fixture(params=[\"two\", \"duo\"])\ndef fixture2(request):\n    return request.paramdef test_foobar(fixture1, fixture2):\n    assert type(fixture1) == type(fixture2)\n</code></pre> <p>The output is:</p> <pre><code>#OUTPUT 3\ncollected 4 itemstest_3.py::test_foobar[one-two] PASSED  [ 25%]\ntest_3.py::test_foobar[one-duo] PASSED  [ 50%]\ntest_3.py::test_foobar[uno-two] PASSED  [ 75%]\ntest_3.py::test_foobar[uno-duo] PASSED  [100%]\n</code></pre>"}, {"location": "coding/python/pytest_parametrized_testing/#parametrization-with-pytest_generate_tests", "title": "Parametrization with <code>pytest_generate_tests</code>", "text": "<p>There is an another way to generate arbitrary parametrization at collection time. It\u2019s a bit more direct and verbose, but it provides introspection of test functions, including the ability to see all other fixture names.</p> <p>At collection time Pytest looks up for and calls (if found) a special function in each module, named <code>pytest_generate_tests</code>. This function is not a fixture, but just a regular function. It receives the argument <code>metafunc</code>, which itself is not a fixture, but a special object.</p> <p><code>pytest_generate_tests</code> is called for each test function in the module to give a chance to parametrize it. Parametrization may happen only through fixtures that test function requests. There is no way to parametrize a test function like this:</p> <p>def test_simple():    assert 2+2 == 4</p> <p>You need some variables to be used as parameters, and those variables should be arguments to the test function. Pytest will replace those arguments with values from fixtures, and if there are a few values for a fixture, then this is parametrization at work.</p> <p><code>metafunc</code> argument to <code>pytest_generate_tests</code> provides some useful information on a test function:</p> <ul> <li>Ability to see all fixture names that function requests.</li> <li>Ability to see the name of the function.</li> <li>Ability to see code of the function.</li> </ul> <p>Finally, <code>metafunc</code> has a parametrize function, which is the way to provide multiple variants of values for fixtures.</p> <p>The same case as before written with the <code>pytest_generate_tests</code> function is:</p> <pre><code>def pytest_generate_tests(metafunc):\n    if \"fixture1\" in metafunc.fixturenames:\n        metafunc.parametrize(\"fixture1\", [\"one\", \"uno\"])\n    if \"fixture2\" in metafunc.fixturenames:\n        metafunc.parametrize(\"fixture2\", [\"two\", \"duo\"])\n\ndef test_foobar(fixture1, fixture2):\n    assert type(fixture1) == type(fixture2)\n</code></pre> <p>This solution is a little bit magical, so I'd avoid it in favor of pytest-cases.</p>"}, {"location": "coding/python/pytest_parametrized_testing/#use-pytest-cases", "title": "Use pytest-cases", "text": "<p>pytest-case gives a lot of power when it comes to tweaking the fixtures and parameterizations.</p> <p>Check that file for further information.</p>"}, {"location": "coding/python/pytest_parametrized_testing/#customizations", "title": "Customizations", "text": ""}, {"location": "coding/python/pytest_parametrized_testing/#change-the-tests-name", "title": "Change the tests name", "text": "<p>Sometimes you want to change how the tests are shown so you can understand better what the test is doing. You can use the <code>ids</code> argument to <code>pytest.mark.parametrize</code>.</p> <p>File tests/unit/test_func.py</p> <pre><code>tasks_to_try = (\n    Task('sleep', done=True),\n    Task('wake', 'brian'),\n    Task('wake', 'brian'),\n    Task('breathe', 'BRIAN', True),\n    Task('exercise', 'BrIaN', False),\n)\n\ntask_ids = [\n    f'Task({task.summary}, {task.owner}, {task.done})'\n    for task in tasks_to_try\n]\n\n@pytest.mark.parametrize('task', tasks_to_try, ids=task_ids)\ndef test_add_4(task):\n    task_id = tasks.add(task)\n    t_from_db = tasks.get(task_id)\n    assert equivalent(t_from_db, task)\n</code></pre> <pre><code>$ pytest -v test_func.py::test_add_4\n===================== test session starts ======================\ncollected 5 items\n\ntest_add_variety.py::test_add_4[Task(sleep,None,True)] PASSED\ntest_add_variety.py::test_add_4[Task(wake,brian,False)0] PASSED\ntest_add_variety.py::test_add_4[Task(wake,brian,False)1] PASSED\ntest_add_variety.py::test_add_4[Task(breathe,BRIAN,True)] PASSED\ntest_add_variety.py::test_add_4[Task(exercise,BrIaN,False)] PASSED\n\n=================== 5 passed in 0.04 seconds ===================\n</code></pre> <p>Those identifiers can be used to run that specific test. For example <code>pytest -v \"test_func.py::test_add_4[Task(breathe,BRIAN,True)]\"</code>.</p> <p><code>parametrize()</code> can be applied to classes as well.</p> <p>If the test id can't be derived from the parameter value, use the <code>id</code> argument for the <code>pytest.param</code>:</p> <pre><code>@pytest.mark.parametrize('task', [\n    pytest.param(Task('create'), id='just summary'),\n    pytest.param(Task('inspire', 'Michelle'), id='summary/owner'),\n])\ndef test_add_6(task):\n    ...\n</code></pre> <p>Will yield:</p> <pre><code>$ pytest-v test_add_variety.py::test_add_6\n\n=================== test session starts ====================\ncollected 2 items\n\ntest_add_variety.py::test_add_6[justsummary]PASSED\ntest_add_variety.py::test_add_6[summary/owner]PASSED\n\n================= 2 passed in 0.05 seconds =================\n</code></pre>"}, {"location": "coding/python/pytest_parametrized_testing/#references", "title": "References", "text": "<ul> <li>A deep dive into Pytest parametrization by George Shulkin</li> <li>Book Python Testing with pytest by Brian Okken.</li> </ul>"}, {"location": "coding/python/python_anti_patterns/", "title": "Python Anti-patterns", "text": ""}, {"location": "coding/python/python_anti_patterns/#mutable-default-arguments", "title": "Mutable default arguments", "text": ""}, {"location": "coding/python/python_anti_patterns/#what-you-wrote", "title": "What You Wrote", "text": "<pre><code>def append_to(element, to=[]):\n    to.append(element)\n    return to\n</code></pre>"}, {"location": "coding/python/python_anti_patterns/#what-you-might-have-expected-to-happen", "title": "What You Might Have Expected to Happen", "text": "<pre><code>my_list = append_to(12)\nprint(my_list)\n\nmy_other_list = append_to(42)\nprint(my_other_list)\n</code></pre> <p>A new list is created each time the function is called if a second argument isn\u2019t provided, so that the output is:</p> <pre><code>[12]\n[42]\n</code></pre> <p>What Does Happen</p> <pre><code>[12]\n[12, 42]\n</code></pre> <p>A new list is created once when the function is defined, and the same list is used in each successive call.</p> <p>Python\u2019s default arguments are evaluated once when the function is defined, not each time the function is called. This means that if you use a mutable default argument and mutate it, you will and have mutated that object for all future calls to the function as well.</p>"}, {"location": "coding/python/python_anti_patterns/#what-you-should-do-instead", "title": "What You Should Do Instead", "text": "<p>Create a new object each time the function is called, by using a default arg to signal that no argument was provided (None is often a good choice).</p> <pre><code>def append_to(element, to=None):\n    if to is None:\n        to = []\n    to.append(element)\n    return to\n</code></pre> <p>Do not forget, you are passing a list object as the second argument.</p>"}, {"location": "coding/python/python_anti_patterns/#when-the-gotcha-isnt-a-gotcha", "title": "When the Gotcha Isn\u2019t a Gotcha", "text": "<p>Sometimes you can specifically \u201cexploit\u201d this behavior to maintain state between calls of a function. This is often done when writing a caching function.</p>"}, {"location": "coding/python/python_code_styling/", "title": "Code styling", "text": ""}, {"location": "coding/python/python_code_styling/#not-using-setdefault-to-initialize-a-dictionary", "title": "Not using setdefault() to initialize a dictionary", "text": "<p>When initializing a dictionary, it is common to see a code check for the existence of a key and then create the key if it does not exist.</p> <p>Given a <code>dictionary = {}</code>, if you want to create a key if it doesn't exist, instead of doing:</p> <pre><code>try:\n    dictionary['key']\nexcept KeyError:\n    dictionary['key'] = {}\n</code></pre> <p>You can use:</p> <pre><code>dictionary.setdefault('key', {})\n</code></pre>"}, {"location": "coding/python/python_code_styling/#black-code-style", "title": "Black code style", "text": "<p>Black is a style guide enforcement tool.</p>"}, {"location": "coding/python/python_code_styling/#flake8", "title": "Flake8", "text": "<p>Flake8 is another style guide enforcement tool.</p>"}, {"location": "coding/python/python_code_styling/#f-strings", "title": "f-strings", "text": "<p>f-strings, also known as formatted string literals, are strings that have an <code>f</code> at the beginning and curly braces containing expressions that will be replaced with their values.</p> <p>Introduced in Python 3.6, they are more readable, concise, and less prone to error than other ways of formatting, as well as faster.</p> <pre><code>&gt;&gt;&gt; name = \"Eric\"\n&gt;&gt;&gt; age = 74\n&gt;&gt;&gt; f\"Hello, {name}. You are {age}.\"\n'Hello, Eric. You are 74.'\n</code></pre>"}, {"location": "coding/python/python_code_styling/#arbitrary-expressions", "title": "Arbitrary expressions", "text": "<p>Because f-strings are evaluated at runtime, you can put any valid Python expressions in them. For example, calling a function or method from within.</p> <pre><code>&gt;&gt;&gt; f\"{name.lower()} is funny.\"\n'eric idle is funny.'\n</code></pre>"}, {"location": "coding/python/python_code_styling/#multiline-f-strings", "title": "Multiline f-strings", "text": "<pre><code>&gt;&gt;&gt; name = \"Eric\"\n&gt;&gt;&gt; profession = \"comedian\"\n&gt;&gt;&gt; affiliation = \"Monty Python\"\n&gt;&gt;&gt; message = (\n...     f\"Hi {name}. \"\n...     f\"You are a {profession}. \"\n...     f\"You were in {affiliation}.\"\n... )\n&gt;&gt;&gt; message\n'Hi Eric. You are a comedian. You were in Monty Python.'\n</code></pre>"}, {"location": "coding/python/python_code_styling/#lint-error-fixes-and-ignores", "title": "Lint error fixes and ignores", "text": ""}, {"location": "coding/python/python_code_styling/#fix-pylint-r0201-error", "title": "Fix Pylint R0201 error", "text": "<p>The error shows <code>Method could be a function</code>, it is used when there is no reference to the class, suggesting that the method could be used as a static function instead.</p> <p>Attempt using either of the decorators <code>@classmethod</code> or <code>@staticmethod</code>.</p> <p>If you don't need to change or use the class methods, use <code>staticmethod</code>.</p> <p>Example:</p> <pre><code>Class Foo(object):\n    ...\n    def bar(self, baz):\n        ...\n        return llama\n</code></pre> <p>Try instead to use:</p> <pre><code>Class Foo(object):\n    ...\n    @classmethod\n    def bar(cls, baz):\n        ...\n        return llama\n</code></pre> <p>Or</p> <pre><code>Class Foo(object):\n    ...\n    @staticmethod\n    def bar(baz):\n        ...\n        return llama\n</code></pre>"}, {"location": "coding/python/python_code_styling/#w1203-with-f-strings", "title": "W1203 with F-strings", "text": "<p>This rule suggest you to use the <code>%</code> interpolation in the logging methods because it might save some interpolation time when a logging statement is not run. Nevertheless the performance improvement is negligible and the advantages of using f-strings far outweigh them.</p>"}, {"location": "coding/python/python_code_styling/#w0106-in-list-comprehension", "title": "W0106 in list comprehension", "text": "<p>They just don't support it they suggest to use normal for loops.</p>"}, {"location": "coding/python/python_code_styling/#w1514-set-encoding-on-open", "title": "W1514 set encoding on open", "text": "<pre><code>with open('file.txt', 'r', encoding='utf-8'):\n</code></pre>"}, {"location": "coding/python/python_code_styling/#sim105-use", "title": "[SIM105 Use", "text": "<p>'contextlib.suppress(Exception)'](https://docs.python.org/3/library/contextlib.html#contextlib.suppress)</p> <p>To bypass exceptions, it's better to use:</p> <pre><code>from contextlib import suppress\n\nwith suppress(FileNotFoundError):\n    os.remove('somefile.tmp')\n</code></pre> <p>Instead of:</p> <pre><code>try:\n    os.remove('somefile.tmp')\nexcept FileNotFoundError:\n    pass\n</code></pre>"}, {"location": "coding/python/python_config_yaml/", "title": "Load config from yaml", "text": "<p>Several programs load the configuration from file. After trying ini, json and yaml I've seen that the last one is the most comfortable.</p> <p>So here are the templates for the tests and class that loads the data from a yaml file and exposes it as a dictionary.</p> <p>In the following sections Jinja templating is used, so substitute everything between <code>{{ }}</code> to their correct values.</p> <p>It's assumed that:</p> <ul> <li>The root directory of the project has the same name as the program.</li> <li>A file with a valid config exists in <code>assets/config.yaml</code>. We'll use this file     in the documentation, so comment it through.</li> </ul>"}, {"location": "coding/python/python_config_yaml/#code", "title": "Code", "text": "<p>The class code below is expected to introduced in the file <code>configuration.py</code>.</p> <p>Variables to substitute:</p> <ul> <li><code>program_name</code></li> </ul> <p>File {{ program_name }}/configuration.py</p> <pre><code>\"\"\"\nModule to define the configuration of the main program.\n\nClasses:\n    Config: Class to manipulate the configuration of the program.\n\"\"\"\n\nfrom collections import UserDict\nfrom ruamel.yaml import YAML\nfrom ruamel.yaml.scanner import ScannerError\n\nimport logging\nimport os\nimport sys\n\nlog = logging.getLogger(__name__)\n\n\nclass Config(UserDict):\n\"\"\"\n    Class to manipulate the configuration of the program.\n\n    Arguments:\n        config_path (str): Path to the configuration file.\n            Default: ~/.local/share/{{ program_name }}/config.yaml\n\n    Public methods:\n        get: Fetch the configuration value of the specified key.\n            If there are nested dictionaries, a dot notation can be used.\n        load: Loads configuration from configuration YAML file.\n        save: Saves configuration in the configuration YAML file.\n\n    Attributes and properties:\n        config_path (str): Path to the configuration file.\n        data(dict): Program configuration.\n    \"\"\"\n\n    def __init__(self, config_path='~/.local/share/{{ program_name }}/config.yaml'):\n        self.config_path = os.path.expanduser(config_path)\n        self.load()\n\n    def get(self, key):\n\"\"\"\n        Fetch the configuration value of the specified key. If there are nested\n        dictionaries, a dot notation can be used.\n\n        So if the configuration contents are:\n\n        self.data = {\n            'first': {\n                'second': 'value'\n            },\n        }\n\n        self.data.get('first.second') == 'value'\n\n        Arguments:\n            key(str): Configuration key to fetch\n        \"\"\"\n        keys = key.split('.')\n        value = self.data.copy()\n\n        for key in keys:\n            value = value[key]\n\n        return value\n\n    def load(self):\n\"\"\"\n        Loads configuration from configuration YAML file.\n        \"\"\"\n\n        try:\n            with open(os.path.expanduser(self.config_path), 'r') as f:\n                try:\n                    self.data = YAML().load(f)\n                except ScannerError as e:\n                    log.error(\n                        'Error parsing yaml of configuration file '\n                        '{}: {}'.format(\n                            e.problem_mark,\n                            e.problem,\n                        )\n                    )\n                    sys.exit(1)\n        except FileNotFoundError:\n            log.error(\n                'Error opening configuration file {}'.format(self.config_path)\n            )\n            sys.exit(1)\n\n    def save(self):\n\"\"\"\n        Saves configuration in the configuration YAML file.\n        \"\"\"\n\n        with open(os.path.expanduser(self.config_path), 'w+') as f:\n            yaml = YAML()\n            yaml.default_flow_style = False\n            yaml.dump(self.data, f)\n</code></pre> <p>We use ruamel PyYAML implementation to preserve the file comments.</p> <p>That class is meant to be loaded in the main <code>__init__.py</code> file, below the logging configuration (if there is any).</p> <p>Variables to substitute:</p> <ul> <li><code>program_name</code></li> <li><code>config_environmental_variable</code>: The optional environmental variable where the     path to the configuration is set. For example <code>PYDO_CONFIG</code>. This will be     used in the tests to override the default path. We don't load this     configuration from the program argument parser because it's definition often     depends on the config file.</li> </ul> <p>File {{ program_name}}/init.py</p> <pre><code># ...\n# (optional logging definition)\n\nimport os\nfrom {{ program_name }}.configuration import Config\nconfig = Config(os.getenv('{{ config_environmental_variable }}', '~/.local/share/{{ program_name }}/config.yaml'))\n\n# (Rest of the program)\n# ...\n</code></pre> <p>If you want to use the <code>config</code> object in a module of your program, import them from the above file like this:</p> <p>File {{ program_name }}/cli.py</p> <pre><code>from {{ program_name }} import config\n</code></pre>"}, {"location": "coding/python/python_config_yaml/#tests", "title": "Tests", "text": "<p>I feel that the tests should use the default configuration, therefore we're setting the environmental variable in the <code>conftest.py</code> file that gets executed by pytest in the tests setup.</p> <p>Variables to substitute:</p> <ul> <li><code>config_environmental_variable</code>: Same as the one defined in the last section.</li> </ul> <p>File tests/conftest.py</p> <pre><code>import os\n\nos.environ[{{ config_environmental_variable }}] = 'assets/config.yaml'\n</code></pre> <p>Variables to substitute:</p> <ul> <li><code>program_name</code></li> </ul> <p>As it's really dependent in the config structure, you can improve the <code>test_config_load</code> test to make it more meaningful.</p> <p>File tests/unit/test_configuration.py</p> <pre><code>from {{ program_name }}.configuration import Config\nfrom unittest.mock import patch\nfrom ruamel.yaml.scanner import ScannerError\n\nimport os\nimport pytest\nimport shutil\nimport tempfile\n\n\nclass TestConfig:\n\"\"\"\n    Class to test the Config object.\n\n    Public attributes:\n        config (Config object): Config object to test\n    \"\"\"\n\n    @pytest.fixture(autouse=True)\n    def setup(self):\n        self.config_path = 'assets/config.yaml'\n        self.log_patch = patch('{{ program_name }}.configuration.log', autospect=True)\n        self.log = self.log_patch.start()\n        self.sys_patch = patch('{{ program_name }}.configuration.sys', autospect=True)\n        self.sys = self.sys_patch.start()\n\n        self.config = Config(self.config_path)\n        yield 'setup'\n\n        self.log_patch.stop()\n        self.sys_patch.stop()\n\n    def test_get_can_fetch_nested_items_with_dots(self):\n        self.config.data = {\n            'first': {\n                'second': 'value'\n            },\n        }\n\n        assert self.config.get('first.second') == 'value'\n\n    def test_config_can_fetch_nested_items_with_dictionary_notation(self):\n        self.config.data = {\n            'first': {\n                'second': 'value'\n            },\n        }\n\n        assert self.config['first']['second'] == 'value'\n\n    def test_config_load(self):\n        self.config.load()\n        assert len(self.config.data) &gt; 0\n\n    @patch('{{ program_name }}.configuration.YAML')\n    def test_load_handles_wrong_file_format(self, yamlMock):\n        yamlMock.return_value.load.side_effect = ScannerError(\n            'error',\n            '',\n            'problem',\n            'mark',\n        )\n\n        self.config.load()\n        self.log.error.assert_called_once_with(\n            'Error parsing yaml of configuration file mark: problem'\n        )\n        self.sys.exit.assert_called_once_with(1)\n\n    @patch('{{ program_name }}.configuration.open')\n    def test_load_handles_file_not_found(self, openMock):\n        openMock.side_effect = FileNotFoundError()\n\n        self.config.load()\n        self.log.error.assert_called_once_with(\n            'Error opening configuration file {}'.format(\n                self.config_path\n            )\n        )\n        self.sys.exit.assert_called_once_with(1)\n\n    @patch('{{ program_name }}.configuration.Config.load')\n    def test_init_calls_config_load(self, loadMock):\n        Config()\n        loadMock.assert_called_once_with()\n\n    def test_save_config(self):\n        tmp = tempfile.mkdtemp()\n        save_file = os.path.join(tmp, 'yaml_save_test.yaml')\n        self.config = Config(save_file)\n        self.config.data = {'a': 'b'}\n\n        self.config.save()\n        with open(save_file, 'r') as f:\n            assert \"a:\" in f.read()\n\n        shutil.rmtree(tmp)\n</code></pre>"}, {"location": "coding/python/python_config_yaml/#installation", "title": "Installation", "text": "<p>It's always nice to have the default configuration template (with it's documentation) when configuring your use case. Therefore we're going to add a step in the installation process to copy the file.</p> <p>Variables to substitute in both files:</p> <ul> <li><code>program_name</code></li> </ul> <p>File setup.py</p> <pre><code>import shutil\n...\n\nClass PostInstallCommand(install):\n    ...\n\n    def run(self):\n        install.run(self)\n        try:\n            data_directory = os.path.expanduser(\"~/.local/share/{{ program_name }}\")\n            os.makedirs(data_directory)\n            log.info(\"Data directory created\")\n        except FileExistsError:\n            log.info(\"Data directory already exits\")\n\n        config_path = os.path.join(data_directory, 'config.yaml')\n        if os.path.isfile(config_path) and os.access(config_path, os.R_OK):\n            log.info(\n                \"Configuration file already exists, check the documentation \"\n                \"for the new version changes.\"\n            )\n        else:\n            shutil.copyfile('assets/config.yaml', config_path)\n            log.info(\"Copied default configuration template\")\n</code></pre> <ul> <li><code>assets_url</code>: Url pointing to the assets file, similar to     <code>https://github.com/lyz-code/pydo/blob/master/assets/config.yaml</code>.</li> </ul> <p>README.md</p> <p>...</p> <p><code>{{ program_name }}</code> configuration is done through the yaml file located at <code>~/.local/share/{{ program_name }}/config.yaml</code>. The default template is provided at installation time.</p> <p>...</p> <p>It's also necessary to add the <code>ruamel.yaml</code> pip package to your <code>setup.py</code> and <code>requirements.txt</code> files.</p>"}, {"location": "coding/python/python_project_template/", "title": "Python Project template", "text": "<p>It's hard to correctly define the directory structure to make python programs work as expected. Even more if testing, documentation or databases are involved.</p> <p>I've automated the creation of the python project skeleton following most of these section guidelines with this cookiecutter template.</p> <pre><code>cruft create https://github.com/lyz-code/cookiecutter-python-project\n</code></pre> <p>If you don't know cruft, take a look here.</p>"}, {"location": "coding/python/python_project_template/#basic-python-project", "title": "Basic Python project", "text": "<ul> <li> <p>Create virtualenv     <pre><code>mkdir {{ project_directory }}\nmkvirtualenv --python=python3 -a {{ project_directory }} {{ project_name }}\n</code></pre></p> </li> <li> <p>Create git repository     <pre><code>workon {{ project_name }}\ngit init .\ngit ignore-io python &gt; .gitignore\ngit add .\ngit commit -m \"Added gitignore\"\ngit checkout -b 'feat/initial_iteration'\n</code></pre></p> </li> </ul>"}, {"location": "coding/python/python_project_template/#project-structure", "title": "Project structure", "text": "<p>After using different project structures, I've ended up using the following:</p> <pre><code>.\n\u251c\u2500\u2500\u2500 docs\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 ...\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 index.md\n\u251c\u2500\u2500 LICENSE\n\u251c\u2500\u2500 Makefile\n\u251c\u2500\u2500 mkdocs.yml\n\u251c\u2500\u2500 mypy.ini\n\u251c\u2500\u2500 pyproject.toml\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 requirements-dev.in\n\u251c\u2500\u2500 setup.py\n\u251c\u2500\u2500 src\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 package_name\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 adapters\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 __init__.py\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 config.py\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 entrypoints\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 __init__.py\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 __init__.py\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 model\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 __init__.py\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 py.typed\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 services.py\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 version.py\n\u2514\u2500\u2500 tests\n    \u251c\u2500\u2500 conftest.py\n    \u251c\u2500\u2500 e2e\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 __init__.py\n    \u251c\u2500\u2500 __init__.py\n    \u251c\u2500\u2500 integration\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 __init__.py\n    \u2514\u2500\u2500 unit\n        \u251c\u2500\u2500 __init__.py\n        \u2514\u2500\u2500 test_services.py\n</code></pre> <p>Heavily inspired by ionel packaging python library post and the Architecture Patterns with Python book by Harry J.W. Percival and Bob Gregory.</p>"}, {"location": "coding/python/python_project_template/#project-types", "title": "Project types", "text": "<p>Depending on the type of project you want to build there are different layouts:</p> <ul> <li>Command-line program.</li> <li>A single Flask web application.</li> <li>Multiple interconnected Flask microservices.</li> </ul>"}, {"location": "coding/python/python_project_template/#additional-configurations", "title": "Additional configurations", "text": "<p>Once the basic project structure is defined, there are several common enhancements to be applied:</p> <ul> <li>Manage dependencies with pip-tools</li> <li>Create the documentation repository</li> <li>Continuous integration pipelines</li> <li>Configure SQLAlchemy to use the MariaDB/Mysql     backend</li> <li>Configure Docker and Docker compose to host the     application</li> <li>Load config from YAML</li> <li>Configure a Flask project</li> </ul>"}, {"location": "coding/python/python_project_template/#code-tests", "title": "Code tests", "text": "<p>Unit, integration, end-to-end, edge-to-edge tests define the behaviour of the application.</p> <p>Trigger hooks:</p> <ul> <li> <p>Github Actions: To run the tests each time a push or pull request is created in Github, create     the <code>.github/workflows/test.yml</code> file with the following Jinja     template.</p> <p>Make sure to check:</p> <ul> <li>The correct Python versions are configured.</li> <li>The steps make sense to your case scenario.</li> </ul> <p>Variables to substitute:</p> <ul> <li><code>program_name</code>: your program name</li> </ul> <pre><code>---\nname: Python package\n\non: [push, pull_request]\n\njobs:\nbuild:\nruns-on: ubuntu-latest\nstrategy:\nmax-parallel: 3\nmatrix:\npython-version: [3.6, 3.7, 3.8]\n\nsteps:\n- uses: actions/checkout@v1\n- name: Set up Python ${{ matrix.python-version }}\nuses: actions/setup-python@v1\nwith:\npython-version: ${{ matrix.python-version }}\n- name: Install dependencies\nrun: |\npython -m pip install --upgrade pip\npip install -r requirements.txt\n- name: Lint with flake8\nrun: |\npip install flake8\n# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n- name: Test with pytest\nrun: |\npip install pytest pytest-cov\npython -m pytest --cov-report term-missing --cov {{ program_name }} tests\n</code></pre> <p>If you want to add a badge stating the last build status in your readme, use the following template.</p> <p>Variables to substitute:</p> <ul> <li><code>repository_url</code>: Github repository url, like     <code>https://github.com/lyz-code/pydo</code>.</li> </ul> <pre><code>[![Actions\nStatus]({{ repository_url }}/workflows/Python%20package/badge.svg)]({{ repository_url }}/actions)\n</code></pre> </li> </ul>"}, {"location": "coding/python/python_project_template/#references", "title": "References", "text": "<ul> <li>ionel packaging a python library     post and he's     cookiecutter template</li> </ul>"}, {"location": "coding/python/python_snippets/", "title": "Python Snippets", "text": ""}, {"location": "coding/python/python_snippets/#read-file-with-pathlib", "title": "Read file with Pathlib", "text": "<pre><code>file_ = Path('/to/some/file')\nfile_.read_text()\n</code></pre>"}, {"location": "coding/python/python_snippets/#get-changed-time-of-a-file", "title": "Get changed time of a file", "text": "<pre><code>import os\n\nos.path.getmtime(path)\n</code></pre>"}, {"location": "coding/python/python_snippets/#sort-the-returned-paths-of-glob", "title": "Sort the returned paths of glob", "text": "<p><code>glob</code> order is arbitrary, but you can sort them yourself.</p> <p>If you want sorted by name:</p> <pre><code>sorted(glob.glob('*.png'))\n</code></pre> <p>sorted by modification time:</p> <pre><code>import os\nsorted(glob.glob('*.png'), key=os.path.getmtime)\n</code></pre> <p>sorted by size:</p> <pre><code>import os\nsorted(glob.glob('*.png'), key=os.path.getsize)\n</code></pre>"}, {"location": "coding/python/python_snippets/#copy-files-from-a-python-package", "title": "Copy files from a python package", "text": "<pre><code>pkgdir = sys.modules['&lt;mypkg&gt;'].__path__[0]\nfullpath = os.path.join(pkgdir, &lt;myfile&gt;)\nshutil.copy(fullpath, os.getcwd())\n</code></pre>"}, {"location": "coding/python/python_snippets/#substract-two-paths", "title": "Substract two paths", "text": "<p>It can also framed to how to get the relative path between two absolute paths:</p> <pre><code>&gt;&gt;&gt; from pathlib import Path\n&gt;&gt;&gt; p = Path('/home/lyz/')\n&gt;&gt;&gt; h = Path('/home/')\n&gt;&gt;&gt; p.relative_to(h)\nPosixPath('lyz')\n</code></pre>"}, {"location": "coding/python/python_snippets/#move-a-file", "title": "Move a file", "text": "<p>Use one of the following</p> <pre><code>import os\nimport shutil\n\nos.rename(\"path/to/current/file.foo\", \"path/to/new/destination/for/file.foo\")\nos.replace(\"path/to/current/file.foo\", \"path/to/new/destination/for/file.foo\")\nshutil.move(\"path/to/current/file.foo\", \"path/to/new/destination/for/file.foo\")\n</code></pre>"}, {"location": "coding/python/python_snippets/#print-an-exception", "title": "Print an exception", "text": ""}, {"location": "coding/python/python_snippets/#using-the-logging-module", "title": "Using the logging module", "text": "<p>Logging an exception can be done with the module-level function <code>logging.exception()</code> like so:</p> <pre><code>import logging\n\ntry:\n    1 / 0\nexcept BaseException:\n    logging.exception(\"An exception was thrown!\")\n</code></pre> <pre><code>ERROR:root:An exception was thrown!\nTraceback (most recent call last):\nFile \".../Desktop/test.py\", line 4, in &lt;module&gt;\n    1/0\nZeroDivisionError: division by zero\n</code></pre> <p>Notes</p> <ul> <li> <p>The function <code>logging.exception()</code> should only be called from an exception   handler.</p> </li> <li> <p>The logging module should not be used inside a logging handler to avoid a   <code>RecursionError</code>.</p> </li> </ul> <p>It's also possible to log the exception with another log level but still show the exception details by using the keyword argument <code>exc_info=True</code>, like so:</p> <pre><code>logging.critical(\"An exception was thrown!\", exc_info=True)\nlogging.error(\"An exception was thrown!\", exc_info=True)\nlogging.warning(\"An exception was thrown!\", exc_info=True)\nlogging.info(\"An exception was thrown!\", exc_info=True)\nlogging.debug(\"An exception was thrown!\", exc_info=True)\n\n# or the general form\nlogging.log(level, \"An exception was thrown!\", exc_info=True)\n</code></pre>"}, {"location": "coding/python/python_snippets/#with-the-traceback-module", "title": "With the traceback module", "text": "<p>The <code>traceback</code> module provides methods for formatting and printing exceptions and their tracebacks, e.g. this would print exception like the default handler does:</p> <pre><code>import traceback\n\ntry:\n    1 / 0\nexcept Exception:\n    traceback.print_exc()\n</code></pre> <pre><code>Traceback (most recent call last):\n  File \"C:\\scripts\\divide_by_zero.py\", line 4, in &lt;module&gt;\n    1/0\nZeroDivisionError: division by zero\n</code></pre>"}, {"location": "coding/python/python_snippets/#get-common-elements-of-two-lists", "title": "Get common elements of two lists", "text": "<pre><code>&gt;&gt;&gt; a = ['a', 'b']\n&gt;&gt;&gt; b = ['c', 'd', 'b']\n&gt;&gt;&gt; set(a) &amp; set(b)\n{'b'}\n</code></pre>"}, {"location": "coding/python/python_snippets/#get-the-difference-of-two-lists", "title": "Get the difference of two lists", "text": "<p>If we want to substract the elements of one list from the other you can use:</p> <pre><code>for x in b:\n  if x in a:\n    a.remove(x)\n</code></pre>"}, {"location": "coding/python/python_snippets/#recursively-find-files", "title": "Recursively find files", "text": ""}, {"location": "coding/python/python_snippets/#using-pathlibpathrglob", "title": "Using <code>pathlib.Path.rglob</code>", "text": "<pre><code>from pathlib import Path\n\nfor path in Path(\"src\").rglob(\"*.c\"):\n    print(path.name)\n</code></pre> <p>If you don't want to use <code>pathlib</code>, use can use <code>glob.glob('**/*.c')</code>, but don't forget to pass in the recursive keyword parameter and it will use inordinate amount of time on large directories.</p>"}, {"location": "coding/python/python_snippets/#oswalk", "title": "os.walk", "text": "<p>For older Python versions, use <code>os.walk</code> to recursively walk a directory and <code>fnmatch.filter</code> to match against a simple expression:</p> <pre><code>import fnmatch\nimport os\n\nmatches = []\nfor root, dirnames, filenames in os.walk(\"src\"):\n    for filename in fnmatch.filter(filenames, \"*.c\"):\n        matches.append(os.path.join(root, filename))\n</code></pre>"}, {"location": "coding/python/python_snippets/#pad-a-string-with-spaces", "title": "Pad a string with spaces", "text": "<pre><code>&gt;&gt;&gt; name = 'John'\n&gt;&gt;&gt; name.ljust(15)\n'John           '\n</code></pre>"}, {"location": "coding/python/python_snippets/#get-hostname-of-the-machine", "title": "Get hostname of the machine", "text": "<p>Any of the next three options:</p> <pre><code>import os\n\nos.uname()[1]\n\nimport platform\n\nplatform.node()\n\nimport socket\n\nsocket.gethostname()\n</code></pre>"}, {"location": "coding/python/python_snippets/#pathlib-make-parent-directories-if-they-dont-exist", "title": "Pathlib make parent directories if they don't exist", "text": "<pre><code>pathlib.Path(\"/tmp/sub1/sub2\").mkdir(parents=True, exist_ok=True)\n</code></pre> <p>From the docs:</p> <ul> <li> <p>If <code>parents</code> is <code>true</code>, any missing parents of this path are created as   needed; they are created with the default permissions without taking mode into   account (mimicking the POSIX <code>mkdir -p</code> command).</p> </li> <li> <p>If <code>parents</code> is <code>false</code> (the default), a missing parent raises   <code>FileNotFoundError</code>.</p> </li> <li> <p>If <code>exist_ok</code> is <code>false</code> (the default), <code>FileExistsError</code> is raised if the   target directory already exists.</p> </li> <li> <p>If <code>exist_ok</code> is <code>true</code>, <code>FileExistsError</code> exceptions will be ignored (same   behavior as the POSIX <code>mkdir -p</code> command), but only if the last path component   is not an existing non-directory file.</p> </li> </ul>"}, {"location": "coding/python/python_snippets/#pathlib-touch-a-file", "title": "Pathlib touch a file", "text": "<p>Create a file at this given path.</p> <pre><code>pathlib.Path(\"/tmp/file.txt\").touch(exist_ok=True)\n</code></pre> <p>If the file already exists, the function succeeds if <code>exist_ok</code> is <code>true</code> (and its modification time is updated to the current time), otherwise <code>FileExistsError</code> is raised.</p> <p>If the parent directory doesn't exist you need to create it first.</p> <pre><code>global_conf_path = xdg_home / \"autoimport\" / \"config.toml\"\nglobal_conf_path.parent.mkdir(parents=True)\nglobal_conf_path.touch(exist_ok=True)\n</code></pre>"}, {"location": "coding/python/python_snippets/#pad-integer-with-zeros", "title": "Pad integer with zeros", "text": "<pre><code>&gt;&gt;&gt; length = 1\n&gt;&gt;&gt; print(f'length = {length:03}')\nlength = 001\n</code></pre>"}, {"location": "coding/python/python_snippets/#print-datetime-with-a-defined-format", "title": "Print datetime with a defined format", "text": "<pre><code>now = datetime.now()\ntoday.strftime(\"We are the %d, %b %Y\")\n</code></pre> <p>Where the datetime format is a string built from these directives.</p>"}, {"location": "coding/python/python_snippets/#print-string-with-asciiart", "title": "Print string with asciiart", "text": "<pre><code>pip install pyfiglet\n</code></pre> <pre><code>from pyfiglet import figlet_format\n\nprint(figlet_format(\"09 : 30\"))\n</code></pre> <p>If you want to change the default width of 80 caracteres use:</p> <pre><code>from pyfiglet import Figlet\n\nf = Figlet(font=\"standard\", width=100)\nprint(f.renderText(\"aaaaaaaaaaaaaaaaa\"))\n</code></pre>"}, {"location": "coding/python/python_snippets/#print-specific-time-format", "title": "Print specific time format", "text": "<pre><code>datetime.datetime.now().strftime(\"%Y-%m-%dT%H:%M:%S\")\n</code></pre> Code Meaning Example %a Weekday as locale\u2019s abbreviated name. Mon %A Weekday as locale\u2019s full name.  Monday %w Weekday as a decimal number, where 0 is Sunday and 6 is Saturday. 1 %d Day of the month as a zero-padded decimal number. 30 %-d Day of the month as a decimal number. (Platform specific) 30 %b Month as locale\u2019s abbreviated name. Sep %B Month as locale\u2019s full name.  September %m Month as a zero-padded decimal number.  09 %-m Month as a decimal number. (Platform specific)  9 %y Year without century as a zero-padded decimal number. 13 %Y Year with century as a decimal number.  2013 %H Hour (24-hour clock) as a zero-padded decimal number. 07 %-H Hour (24-hour clock) as a decimal number. (Platform specific) 7 %I Hour (12-hour clock) as a zero-padded decimal number. 07 %-I Hour (12-hour clock) as a decimal number. (Platform specific) 7 %p Locale\u2019s equivalent of either AM or PM. AM %M Minute as a zero-padded decimal number. 06 %-M Minute as a decimal number. (Platform specific) 6 %S Second as a zero-padded decimal number. 05 %-S Second as a decimal number. (Platform specific) 5 %f Microsecond as a decimal number, zero-padded on the left. 000000 %z UTC offset in the form +HHMM or -HHMM (empty string if the the object is naive). %Z Time zone name (empty string if the object is naive). %j Day of the year as a zero-padded decimal number.  273 %-j Day of the year as a decimal number. (Platform specific)  273 %U Week number of the year (Sunday as the first day of the week) as a zero padded decimal number. All days in a new year preceding the first Sunday are considered to be in week 0.  39 %W Week number of the year (Monday as the first day of the week) as a decimal number. All days in a new year preceding the first Monday are considered to be in week 0. %c Locale\u2019s appropriate date and time representation.  Mon Sep 30 07:06:05 2013 %x Locale\u2019s appropriate date representation. 09/30/13 %X Locale\u2019s appropriate time representation. 07:06:05 %% A literal '%' character.  %"}, {"location": "coding/python/python_snippets/#get-an-instance-of-an-enum-by-value", "title": "Get an instance of an Enum by value", "text": "<p>If you want to initialize a pydantic model with an <code>Enum</code> but all you have is the value of the <code>Enum</code> then you need to create a method to get the correct Enum. Otherwise <code>mypy</code> will complain that the type of the assignation is <code>str</code> and not <code>Enum</code>.</p> <p>So if the model is the next one:</p> <pre><code>class ServiceStatus(BaseModel):\n\"\"\"Model the docker status of a service.\"\"\"\n\n    name: str\n    environment: Environment\n</code></pre> <p>You can't do <code>ServiceStatus(name='test', environment='production')</code>. you need to add the <code>get_by_value</code> method to the <code>Enum</code> class:</p> <pre><code>class Environment(str, Enum):\n\"\"\"Set the possible environments.\"\"\"\n\n    STAGING = \"staging\"\n    PRODUCTION = \"production\"\n\n    @classmethod\n    def get_by_value(cls, value: str) -&gt; Enum:\n\"\"\"Return the Enum element that meets a value\"\"\"\n        return [member for member in cls if member.value == value][0]\n</code></pre> <p>Now you can do:</p> <pre><code>ServiceStatus(name=\"test\", environment=Environment.get_by_value(\"production\"))\n</code></pre>"}, {"location": "coding/python/python_snippets/#fix-r1728-consider-using-a-generator", "title": "Fix R1728: Consider using a generator", "text": "<p>Removing <code>[]</code> inside calls that can use containers or generators should be considered for performance reasons since a generator will have an upfront cost to pay. The performance will be better if you are working with long lists or sets.</p> <p>Problematic code:</p> <pre><code>list([0 for y in list(range(10))])  # [consider-using-generator]\ntuple([0 for y in list(range(10))])  # [consider-using-generator]\nsum([y**2 for y in list(range(10))])  # [consider-using-generator]\nmax([y**2 for y in list(range(10))])  # [consider-using-generator]\nmin([y**2 for y in list(range(10))])  # [consider-using-generator]\n</code></pre> <p>Correct code:</p> <pre><code>list(0 for y in list(range(10)))\ntuple(0 for y in list(range(10)))\nsum(y**2 for y in list(range(10)))\nmax(y**2 for y in list(range(10)))\nmin(y**2 for y in list(range(10)))\n</code></pre>"}, {"location": "coding/python/python_snippets/#fix-w1510-using-subprocessrun-without-explicitly-set-check-is-not-recommended", "title": "Fix W1510: Using subprocess.run without explicitly set check is not recommended", "text": "<p>The <code>run</code> call in the example will succeed whether the command is successful or not. This is a problem because we silently ignore errors.</p> <pre><code>import subprocess\n\n\ndef example():\n    proc = subprocess.run(\"ls\")\n    return proc.stdout\n</code></pre> <p>When we pass <code>check=True</code>, the behavior changes towards raising an exception when the return code of the command is non-zero.</p>"}, {"location": "coding/python/python_snippets/#convert-bytes-to-string", "title": "Convert bytes to string", "text": "<pre><code>byte_var.decode(\"utf-8\")\n</code></pre>"}, {"location": "coding/python/python_snippets/#use-pipes-with-subprocess", "title": "Use pipes with subprocess", "text": "<p>To use pipes with subprocess you need to use the flag <code>shell=True</code> which is a bad idea. Instead you should use two processes and link them together in python:</p> <pre><code>ps = subprocess.Popen((\"ps\", \"-A\"), stdout=subprocess.PIPE)\noutput = subprocess.check_output((\"grep\", \"process_name\"), stdin=ps.stdout)\nps.wait()\n</code></pre>"}, {"location": "coding/python/python_snippets/#pass-input-to-the-stdin-of-a-subprocess", "title": "Pass input to the stdin of a subprocess", "text": "<pre><code>import subprocess\n\np = subprocess.run([\"myapp\"], input=\"data_to_write\", text=True)\n</code></pre>"}, {"location": "coding/python/python_snippets/#copy-and-paste-from-clipboard", "title": "Copy and paste from clipboard", "text": "<p>You can use many libraries to do it, but if you don't want to add any other dependencies you can use <code>subprocess run</code>.</p> <p>To copy from the <code>selection</code> clipboard, assuming you've got <code>xclip</code> installed, you could do:</p> <pre><code>subprocess.run(\n    [\"xclip\", \"-selection\", \"clipboard\", \"-i\"],\n    input=\"text to be copied\",\n    text=True,\n    check=True,\n)\n</code></pre> <p>To paste it:</p> <pre><code>subprocess.check_output([\"xclip\", \"-o\", \"-selection\", \"clipboard\"]).decode(\"utf-8\")\n</code></pre>"}, {"location": "coding/python/python_snippets/#create-random-number", "title": "Create random number", "text": "<pre><code>import random\n\na = random.randint(1, 10)\n</code></pre>"}, {"location": "coding/python/python_snippets/#check-if-local-port-is-available-or-in-use", "title": "Check if local port is available or in use", "text": "<p>Create a temporary socket and then try to bind to the port to see if it's available. Close the socket after validating that the port is available.</p> <pre><code>def port_in_use(port):\n\"\"\"Test if a local port is used.\"\"\"\n    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    with suppress(OSError):\n        sock.bind((\"0.0.0.0\", port))\n        return True\n    sock.close()\n    return False\n</code></pre>"}, {"location": "coding/python/python_snippets/#initialize-a-dataclass-with-kwargs", "title": "Initialize a dataclass with kwargs", "text": "<p>If you care about accessing attributes by name, or if you can't distinguish between known and unknown arguments during initialisation, then your last resort without rewriting <code>__init__</code> (which pretty much defeats the purpose of using dataclasses in the first place) is writing a <code>@classmethod</code>:</p> <pre><code>from dataclasses import dataclass\nfrom inspect import signature\n\n\n@dataclass\nclass Container:\n    user_id: int\n    body: str\n\n    @classmethod\n    def from_kwargs(cls, **kwargs):\n        # fetch the constructor's signature\n        cls_fields = {field for field in signature(cls).parameters}\n\n        # split the kwargs into native ones and new ones\n        native_args, new_args = {}, {}\n        for key, value in kwargs.items():\n            if key in cls_fields:\n                native_args[key] = value\n            else:\n                new_args[key] = value\n\n        # use the native ones to create the class ...\n        ret = cls(**native_args)\n\n        # ... and add the new ones by hand\n        for new_key, new_value in new_args.items():\n            setattr(ret, new_key, new_value)\n        return ret\n</code></pre> <p>Usage:</p> <pre><code>params = {\"user_id\": 1, \"body\": \"foo\", \"bar\": \"baz\", \"amount\": 10}\nContainer(**params)  # still doesn't work, raises a TypeError\nc = Container.from_kwargs(**params)\nprint(c.bar)  # prints: 'baz'\n</code></pre>"}, {"location": "coding/python/python_snippets/#replace-a-substring-of-a-string", "title": "Replace a substring of a string", "text": "<pre><code>txt = \"I like bananas\"\n\nx = txt.replace(\"bananas\", \"apples\")\n</code></pre>"}, {"location": "coding/python/python_snippets/#parse-an-rfc2822-date", "title": "Parse an RFC2822 date", "text": "<p>Interesting to test the accepted format of RSS dates.</p> <pre><code>&gt;&gt;&gt; from email.utils import parsedate_to_datetime\n&gt;&gt;&gt; datestr = 'Sun, 09 Mar 1997 13:45:00 -0500'\n&gt;&gt;&gt; parsedate_to_datetime(datestr)\ndatetime.datetime(1997, 3, 9, 13, 45, tzinfo=datetime.timezone(datetime.timedelta(-1, 68400)))\n</code></pre>"}, {"location": "coding/python/python_snippets/#convert-a-datetime-to-rfc2822", "title": "Convert a datetime to RFC2822", "text": "<p>Interesting as it's the accepted format of RSS dates.</p> <pre><code>&gt;&gt;&gt; import datetime\n&gt;&gt;&gt; from email import utils\n&gt;&gt;&gt; nowdt = datetime.datetime.now()\n&gt;&gt;&gt; utils.format_datetime(nowdt)\n'Tue, 10 Feb 2020 10:06:53 -0000'\n</code></pre>"}, {"location": "coding/python/python_snippets/#encode-url", "title": "Encode url", "text": "<pre><code>import urllib.parse\nfrom pydantic import AnyHttpUrl\n\n\ndef _normalize_url(url: str) -&gt; AnyHttpUrl:\n\"\"\"Encode url to make it compatible with AnyHttpUrl.\"\"\"\n    return typing.cast(\n        AnyHttpUrl,\n        urllib.parse.quote(url, \":/\"),\n    )\n</code></pre> <p>The <code>:/</code> is needed when you try to parse urls that have the protocol, otherwise <code>https://www.</code> gets transformed into <code>https%3A//www.</code>.</p>"}, {"location": "coding/python/python_snippets/#fix-sim113-use-enumerate", "title": "Fix SIM113 Use enumerate", "text": "<p>Use <code>enumerate</code> to get a running number over an iterable.</p> <pre><code># Bad\nidx = 0\nfor el in iterable:\n    ...\n    idx += 1\n\n# Good\nfor idx, el in enumerate(iterable):\n    ...\n</code></pre>"}, {"location": "coding/python/python_snippets/#define-a-property-of-a-class", "title": "Define a property of a class", "text": "<p>If you're using Python 3.9 or above you can directly use the decorators:</p> <pre><code>class G:\n    @classmethod\n    @property\n    def __doc__(cls):\n        return f\"A doc for {cls.__name__!r}\"\n</code></pre> <p>If you're not, you can define the decorator <code>classproperty</code>:</p> <pre><code># N801: class name 'classproperty' should use CapWords convention, but it's a decorator.\n# C0103: Class name \"classproperty\" doesn't conform to PascalCase naming style but it's\n# a decorator.\nclass classproperty:  # noqa: N801, C0103\n\"\"\"Define a class property.\n\n    From Python 3.9 you can directly use the decorators directly.\n\n    class G:\n        @classmethod\n        @property\n        def __doc__(cls):\n            return f'A doc for {cls.__name__!r}'\n    \"\"\"\n\n    def __init__(self, function: Callable[..., Any]) -&gt; None:\n\"\"\"Initialize the decorator.\"\"\"\n        self.function = function\n\n    # ANN401: Any not allowed in typings, but I don't know how to narrow the hints in\n    # this case.\n    def __get__(self, owner_self: Any, owner_cls: Any) -&gt; Any:  # noqa: ANN401\n\"\"\"Return the desired value.\"\"\"\n        return self.function(owner_self)\n</code></pre> <p>But you'll run into the <code>W0143: Comparing against a callable, did you omit the parenthesis? (comparison-with-callable)</code> mypy error when using it to compare the result of the property with anything, as it doesn't detect it's a property instead of a method.</p>"}, {"location": "coding/python/python_snippets/#how-to-close-a-subprocess-process", "title": "How to close a subprocess process", "text": "<pre><code>subprocess.terminate()\n</code></pre>"}, {"location": "coding/python/python_snippets/#how-to-extend-a-dictionary", "title": "How to extend a dictionary", "text": "<pre><code>a.update(b)\n</code></pre>"}, {"location": "coding/python/python_snippets/#how-to-find-duplicates-in-a-list-in-python", "title": "How to Find Duplicates in a List in Python", "text": "<pre><code>numbers = [1, 2, 3, 2, 5, 3, 3, 5, 6, 3, 4, 5, 7]\n\nduplicates = [number for number in numbers if numbers.count(number) &gt; 1]\nunique_duplicates = list(set(duplicates))\n\n# Returns: [2, 3, 5]\n</code></pre> <p>If you want to count the number of occurrences of each duplicate, you can use:</p> <pre><code>from collections import Counter\n\nnumbers = [1, 2, 3, 2, 5, 3, 3, 5, 6, 3, 4, 5, 7]\n\ncounts = dict(Counter(numbers))\nduplicates = {key: value for key, value in counts.items() if value &gt; 1}\n\n# Returns: {2: 2, 3: 4, 5: 3}\n</code></pre> <p>To remove the duplicates use a combination of <code>list</code> and <code>set</code>:</p> <pre><code>unique = list(set(numbers))\n\n# Returns: [1, 2, 3, 4, 5, 6, 7]\n</code></pre>"}, {"location": "coding/python/python_snippets/#how-to-decompress-a-gz-file", "title": "How to decompress a gz file", "text": "<pre><code>import gzip\nimport shutil\n\nwith gzip.open(\"file.txt.gz\", \"rb\") as f_in:\n    with open(\"file.txt\", \"wb\") as f_out:\n        shutil.copyfileobj(f_in, f_out)\n</code></pre>"}, {"location": "coding/python/python_snippets/#how-to-compressdecompress-a-tar-file", "title": "How to compress/decompress a tar file", "text": "<pre><code>def compress(tar_file, members):\n\"\"\"\n    Adds files (`members`) to a tar_file and compress it\n    \"\"\"\n    tar = tarfile.open(tar_file, mode=\"w:gz\")\n\n    for member in members:\n        tar.add(member)\n\n    tar.close()\n\n\ndef decompress(tar_file, path, members=None):\n\"\"\"\n    Extracts `tar_file` and puts the `members` to `path`.\n    If members is None, all members on `tar_file` will be extracted.\n    \"\"\"\n    tar = tarfile.open(tar_file, mode=\"r:gz\")\n    if members is None:\n        members = tar.getmembers()\n    for member in members:\n        tar.extract(member, path=path)\n    tar.close()\n</code></pre>"}, {"location": "coding/python/python_snippets/#parse-xml-file-with-beautifulsoup", "title": "Parse XML file with beautifulsoup", "text": "<p>You need both <code>beautifulsoup4</code> and <code>lxml</code>:</p> <pre><code>bs = BeautifulSoup(requests.get(url), \"lxml\")\n</code></pre>"}, {"location": "coding/python/python_snippets/#get-a-traceback-from-an-exception", "title": "Get a traceback from an exception", "text": "<pre><code>import traceback\n\n# `e` is an exception object that you get from somewhere\ntraceback_str = \"\".join(traceback.format_tb(e.__traceback__))\n</code></pre>"}, {"location": "coding/python/python_snippets/#change-the-logging-level-of-a-library", "title": "Change the logging level of a library", "text": "<p>For example to change the logging level of the library <code>sh</code> use:</p> <pre><code>sh_logger = logging.getLogger(\"sh\")\nsh_logger.setLevel(logging.WARN)\n</code></pre>"}, {"location": "coding/python/python_snippets/#get-all-subdirectories-of-a-directory", "title": "Get all subdirectories of a directory", "text": "<pre><code>[x[0] for x in os.walk(directory)]\n</code></pre>"}, {"location": "coding/python/python_snippets/#move-a-file_1", "title": "Move a file", "text": "<pre><code>import os\n\nos.rename(\"path/to/current/file.foo\", \"path/to/new/destination/for/file.foo\")\n</code></pre>"}, {"location": "coding/python/python_snippets/#ipv4-regular-expression", "title": "IPv4 regular expression", "text": "<pre><code>regex = re.compile(\n    r\"(?&lt;![-\\.\\d])(?:0{0,2}?[0-9]\\.|1\\d?\\d?\\.|2[0-5]?[0-5]?\\.){3}\"\n    r'(?:0{0,2}?[0-9]|1\\d?\\d?|2[0-5]?[0-5]?)(?![\\.\\d])\"^[0-9]{1,3}*$'\n)\n</code></pre>"}, {"location": "coding/python/python_snippets/#remove-the-elements-of-a-list-from-another", "title": "Remove the elements of a list from another", "text": "<pre><code>&gt;&gt;&gt; set([1,2,6,8]) - set([2,3,5,8])\nset([1, 6])\n</code></pre> <p>Note, however, that sets do not preserve the order of elements, and cause any duplicated elements to be removed. The elements also need to be hashable. If these restrictions are tolerable, this may often be the simplest and highest performance option.</p>"}, {"location": "coding/python/python_snippets/#copy-a-directory", "title": "Copy a directory", "text": "<pre><code>import shutil\n\nshutil.copytree(\"bar\", \"foo\")\n</code></pre>"}, {"location": "coding/python/python_snippets/#copy-a-file", "title": "Copy a file", "text": "<pre><code>import shutil\n\nshutil.copyfile(src_file, dest_file)\n</code></pre>"}, {"location": "coding/python/python_snippets/#capture-the-stdout-of-a-function", "title": "Capture the stdout of a function", "text": "<pre><code>import io\nfrom contextlib import redirect_stdout\n\nf = io.StringIO()\nwith redirect_stdout(f):\n    do_something(my_object)\nout = f.getvalue()\n</code></pre>"}, {"location": "coding/python/python_snippets/#make-temporal-directory", "title": "Make temporal directory", "text": "<pre><code>import tempfile\n\ndirpath = tempfile.mkdtemp()\n</code></pre>"}, {"location": "coding/python/python_snippets/#change-the-working-directory-of-a-test", "title": "Change the working directory of a test", "text": "<p>The following function-level fixture will change to the test case directory, run the test (<code>yield</code>), then change back to the calling directory to avoid side-effects.</p> <pre><code>@pytest.fixture(name=\"change_test_dir\")\ndef change_test_dir_(request: SubRequest) -&gt; Any:\n    os.chdir(request.fspath.dirname)\n    yield\n    os.chdir(request.config.invocation_dir)\n</code></pre> <ul> <li><code>request</code> is a built-in pytest fixture</li> <li><code>fspath</code> is the <code>LocalPath</code> to the test module being executed</li> <li><code>dirname</code> is the directory of the test module</li> <li><code>request.config.invocationdir</code> is the folder from which pytest was executed</li> <li><code>request.config.rootdir</code> is the pytest root, doesn't change based on where you   run pytest. Not used here, but could be useful.</li> </ul> <p>Any processes that are kicked off by the test will use the test case folder as their working directory and copy their logs, outputs, etc. there, regardless of where the test suite was executed.</p>"}, {"location": "coding/python/python_snippets/#remove-a-substring-from-the-end-of-a-string", "title": "Remove a substring from the end of a string", "text": "<p>On Python 3.9 and newer you can use the <code>removeprefix</code> and <code>removesuffix</code> methods to remove an entire substring from either side of the string:</p> <pre><code>url = \"abcdc.com\"\nurl.removesuffix(\".com\")  # Returns 'abcdc'\nurl.removeprefix(\"abcdc.\")  # Returns 'com'\n</code></pre> <p>On Python 3.8 and older you can use <code>endswith</code> and slicing:</p> <pre><code>url = \"abcdc.com\"\nif url.endswith(\".com\"):\n    url = url[:-4]\n</code></pre> <p>Or a regular expression:</p> <pre><code>import re\n\nurl = \"abcdc.com\"\nurl = re.sub(\"\\.com$\", \"\", url)\n</code></pre>"}, {"location": "coding/python/python_snippets/#make-a-flat-list-of-lists-with-a-list-comprehension", "title": "Make a flat list of lists with a list comprehension", "text": "<p>There is no nice way to do it :(. The best I've found is:</p> <pre><code>t = [[1, 2, 3], [4, 5, 6], [7], [8, 9]]\nflat_list = [item for sublist in t for item in sublist]\n</code></pre>"}, {"location": "coding/python/python_snippets/#replace-all-characters-of-a-string-with-another-character", "title": "Replace all characters of a string with another character", "text": "<pre><code>mystring = \"_\" * len(mystring)\n</code></pre>"}, {"location": "coding/python/python_snippets/#locate-element-in-list", "title": "Locate element in list", "text": "<pre><code>a = [\"a\", \"b\"]\n\nindex = a.index(\"b\")\n</code></pre>"}, {"location": "coding/python/python_snippets/#transpose-a-list-of-lists", "title": "Transpose a list of lists", "text": "<pre><code>&gt;&gt;&gt; l=[[1,2,3],[4,5,6],[7,8,9]]\n&gt;&gt;&gt; [list(i) for i in zip(*l)]\n... [[1, 4, 7], [2, 5, 8], [3, 6, 9]]\n</code></pre>"}, {"location": "coding/python/python_snippets/#check-the-type-of-a-list-of-strings", "title": "Check the type of a list of strings", "text": "<pre><code>def _is_list_of_lists(data: Any) -&gt; bool:\n\"\"\"Check if data is a list of strings.\"\"\"\n    if data and isinstance(data, list):\n        return all(isinstance(elem, list) for elem in data)\n    else:\n        return False\n</code></pre>"}, {"location": "coding/python/python_snippets/#install-default-directories-and-files-for-a-command-line-program", "title": "Install default directories and files for a command line program", "text": "<p>I've been trying for a long time to configure <code>setup.py</code> to run the required steps to configure the required directories and files when doing <code>pip install</code> without success.</p> <p>Finally, I decided that the program itself should create the data once the <code>FileNotFoundError</code> exception is found. That way, you don't penalize the load time because if the file or directory exists, that code is not run.</p>"}, {"location": "coding/python/python_snippets/#check-if-a-dictionary-is-a-subset-of-another", "title": "Check if a dictionary is a subset of another", "text": "<p>If you have two dictionaries <code>big = {'a': 1, 'b': 2, 'c':3}</code> and <code>small = {'c': 3, 'a': 1}</code>, and want to check whether <code>small</code> is a subset of <code>big</code>, use the next snippet:</p> <pre><code>&gt;&gt;&gt; small.items() &lt;= big.items()\nTrue\n</code></pre> <p>As the code is not very common or intuitive, I'd add a comment to explain what you're doing.</p>"}, {"location": "coding/python/python_snippets/#when-to-use-isinstance-and-when-to-use-type", "title": "When to use <code>isinstance</code> and when to use <code>type</code>", "text": "<p><code>isinstance</code> takes into account inheritance, while <code>type</code> doesn't. So if we have the next code:</p> <pre><code>class Shape:\n    pass\n\n\nclass Rectangle(Shape):\n    def __init__(self, length, width):\n        self.length = length\n        self.width = width\n        self.area = length * width\n\n    def get_area(self):\n        return self.length * self.width\n\n\nclass Square(Rectangle):\n    def __init__(self, length):\n        Rectangle.__init__(self, length, length)\n</code></pre> <p>And we want to check if an object <code>a = Square(5)</code> is of type <code>Rectangle</code>, we could not use <code>isinstance</code> because it'll return <code>True</code> as it's a subclass of <code>Rectangle</code>:</p> <pre><code>&gt;&gt;&gt; isinstance(a, Rectangle)\nTrue\n</code></pre> <p>Instead, use a comparison with <code>type</code>:</p> <pre><code>&gt;&gt;&gt; type(a) == Rectangle\nFalse\n</code></pre>"}, {"location": "coding/python/python_snippets/#find-a-static-file-of-a-python-module", "title": "Find a static file of a python module", "text": "<p>Useful when you want to initialize a configuration file of a cli program when it's not present.</p> <p>Imagine you have a <code>setup.py</code> with the next contents:</p> <pre><code>setup(\n    name=\"pynbox\",\n    packages=find_packages(\"src\"),\n    package_dir={\"\": \"src\"},\n    package_data={\"pynbox\": [\"py.typed\", \"assets/config.yaml\"]},\n</code></pre> <p>Then you could import the data with:</p> <pre><code>import pkg_resources\n\nfile_path = (pkg_resources.resource_filename(\"pynbox\", \"assets/config.yaml\"),)\n</code></pre>"}, {"location": "coding/python/python_snippets/#delete-a-file", "title": "Delete a file", "text": "<pre><code>import os\n\nos.remove(\"demofile.txt\")\n</code></pre>"}, {"location": "coding/python/python_snippets/#measure-elapsed-time-between-lines-of-code", "title": "Measure elapsed time between lines of code", "text": "<pre><code>import time\n\nstart = time.time()\nprint(\"hello\")\nend = time.time()\nprint(end - start)\n</code></pre>"}, {"location": "coding/python/python_snippets/#create-combination-of-elements-in-groups-of-two", "title": "Create combination of elements in groups of two", "text": "<p>Using the combinations function in Python's itertools module:</p> <pre><code>&gt;&gt;&gt; list(itertools.combinations('ABC', 2))\n[('A', 'B'), ('A', 'C'), ('B', 'C')]\n</code></pre> <p>If you want the permutations use <code>itertools.permutations</code>.</p>"}, {"location": "coding/python/python_snippets/#convert-html-to-readable-plaintext", "title": "Convert html to readable plaintext", "text": "<pre><code>pip install html2text\n</code></pre> <pre><code>import html2text\n\nhtml = open(\"foobar.html\").read()\nprint(html2text.html2text(html))\n</code></pre>"}, {"location": "coding/python/python_snippets/#parse-a-datetime-from-a-string", "title": "Parse a datetime from a string", "text": "<p>Convert a string to a datetime.</p> <pre><code>from dateutil import parser\n\nparser.parse(\"Aug 28 1999 12:00AM\")  # datetime.datetime(1999, 8, 28, 0, 0)\n</code></pre> <p>If you don't want to use <code>dateutil</code> use <code>datetime</code></p> <pre><code>datetime.datetime.strptime(\"2013-W26\", \"%Y-W%W-%w\")\n</code></pre> <p>Where the datetime format is a string built from the next directives:</p> Directive Meaning Example %a Abbreviated weekday name. Sun, Mon, ... %A Full weekday name. Sunday, Monday, ... %w Weekday as a decimal number. 0, 1, ..., 6 %d Day of the month as a zero-padded decimal. 01, 02, ..., 31 %-d Day of the month as a decimal number. 1, 2, ..., 30 %b Abbreviated month name. Jan, Feb, ..., Dec %B Full month name. January, February, ... %m Month as a zero-padded decimal number. 01, 02, ..., 12 %-m Month as a decimal number. 1, 2, ..., 12 %y Year without century as a zero-padded decimal number. 00, 01, ..., 99 %-y Year without century as a decimal number. 0, 1, ..., 99 %Y Year with century as a decimal number. 2013, 2019 etc. %H Hour (24-hour clock) as a zero-padded decimal number. 00, 01, ..., 23 %-H Hour (24-hour clock) as a decimal number. 0, 1, ..., 23 %I Hour (12-hour clock) as a zero-padded decimal number. 01, 02, ..., 12 %-I Hour (12-hour clock) as a decimal number. 1, 2, ... 12 %p Locale\u2019s AM or PM. AM, PM %M Minute as a zero-padded decimal number. 00, 01, ..., 59 %-M Minute as a decimal number. 0, 1, ..., 59 %S Second as a zero-padded decimal number. 00, 01, ..., 59 %-S Second as a decimal number. 0, 1, ..., 59 %f Microsecond as a decimal number, zero-padded on the left. 000000 - 999999 %z UTC offset in the form +HHMM or -HHMM. %Z Time zone name. %j Day of the year as a zero-padded decimal number. 001, 002, ..., 366 %-j Day of the year as a decimal number. 1, 2, ..., 366 %U Week number of the year (Sunday as the first day of the week). 00, 01, ..., 53 %W Week number of the year (Monday as the first day of the week). 00, 01, ..., 53 %c Locale\u2019s appropriate date and time representation. Mon Sep 30 07:06:05 2013 %x Locale\u2019s appropriate date representation. 09/30/13 %X Locale\u2019s appropriate time representation. 07:06:05 %% A literal '%' character. %"}, {"location": "coding/python/python_snippets/#install-a-python-dependency-from-a-git-repository", "title": "Install a python dependency from a git repository", "text": "<p>With pip you can:</p> <pre><code>pip install git+git://github.com/path/to/repository@master\n</code></pre> <p>If you want to hard code it in your <code>setup.py</code>, you need to:</p> <pre><code>install_requires = [\n    \"some-pkg @ git+ssh://git@github.com/someorgname/pkg-repo-name@v1.1#egg=some-pkg\",\n]\n</code></pre> <p>But Pypi won't allow you to upload the package, as it will give you an error:</p> <pre><code>HTTPError: 400 Bad Request from https://test.pypi.org/legacy/\nInvalid value for requires_dist. Error: Can't have direct dependency: 'deepdiff @ git+git://github.com/lyz-code/deepdiff@master'\n</code></pre> <p>It looks like this is a conscious decision on the PyPI side. Basically, they don't want pip to reach out to URLs outside their site when installing from PyPI.</p> <p>An ugly patch is to install the dependencies in a <code>PostInstall</code> custom script in the <code>setup.py</code> of your program:</p> <pre><code>from setuptools.command.install import install\nfrom subprocess import getoutput\n\n# ignore: cannot subclass install, has type Any. And what would you do?\nclass PostInstall(install):  # type: ignore\n\"\"\"Install direct dependency.\n\n    Pypi doesn't allow uploading packages with direct dependencies, so we need to\n    install them manually.\n    \"\"\"\n\n    def run(self) -&gt; None:\n\"\"\"Install dependencies.\"\"\"\n        install.run(self)\n        print(getoutput(\"pip install git+git://github.com/lyz-code/deepdiff@master\"))\n\n\nsetup(cmdclass={\"install\": PostInstall})\n</code></pre> <p>Warning: It may not work! Last time I used this solution, when I added the library on a <code>setup.py</code> the direct dependencies weren't installed :S</p>"}, {"location": "coding/python/python_snippets/#check-or-test-directories-and-files", "title": "Check or test directories and files", "text": "<pre><code>def test_dir(directory):\n    from os.path import exists\n    from os import makedirs\n\n    if not exists(directory):\n        makedirs(directory)\n\n\ndef test_file(filepath, mode):\n\"\"\"Check if a file exist and is accessible.\"\"\"\n\n    def check_mode(os_mode, mode):\n        if os.path.isfile(filepath) and os.access(filepath, os_mode):\n            return\n        else:\n            raise IOError(\"Can't access the file with mode \" + mode)\n\n    if mode is \"r\":\n        check_mode(os.R_OK, mode)\n    elif mode is \"w\":\n        check_mode(os.W_OK, mode)\n    elif mode is \"a\":\n        check_mode(os.R_OK, mode)\n        check_mode(os.W_OK, mode)\n</code></pre>"}, {"location": "coding/python/python_snippets/#remove-the-extension-of-a-file", "title": "Remove the extension of a file", "text": "<pre><code>os.path.splitext(\"/path/to/some/file.txt\")[0]\n</code></pre>"}, {"location": "coding/python/python_snippets/#iterate-over-the-files-of-a-directory", "title": "Iterate over the files of a directory", "text": "<pre><code>import os\n\ndirectory = \"/path/to/directory\"\nfor entry in os.scandir(directory):\n    if (entry.path.endswith(\".jpg\") or entry.path.endswith(\".png\")) and entry.is_file():\n        print(entry.path)\n</code></pre>"}, {"location": "coding/python/python_snippets/#create-directory", "title": "Create directory", "text": "<pre><code>if not os.path.exists(directory):\n    os.makedirs(directory)\n</code></pre>"}, {"location": "coding/python/python_snippets/#touch-a-file", "title": "Touch a file", "text": "<pre><code>from pathlib import Path\n\nPath(\"path/to/file.txt\").touch()\n</code></pre>"}, {"location": "coding/python/python_snippets/#get-the-first-day-of-next-month", "title": "Get the first day of next month", "text": "<pre><code>current = datetime.datetime(mydate.year, mydate.month, 1)\nnext_month = datetime.datetime(\n    mydate.year + int(mydate.month / 12), ((mydate.month % 12) + 1), 1\n)\n</code></pre>"}, {"location": "coding/python/python_snippets/#get-the-week-number-of-a-datetime", "title": "Get the week number of a datetime", "text": "<p><code>datetime.datetime</code> has a <code>isocalendar()</code> method, which returns a tuple containing the calendar week:</p> <pre><code>&gt;&gt;&gt; import datetime\n&gt;&gt;&gt; datetime.datetime(2010, 6, 16).isocalendar()[1]\n24\n</code></pre> <p><code>datetime.date.isocalendar()</code> is an instance-method returning a tuple containing year, weeknumber and weekday in respective order for the given date instance.</p>"}, {"location": "coding/python/python_snippets/#get-the-monday-of-a-week-number", "title": "Get the monday of a week number", "text": "<p>A week number is not enough to generate a date; you need a day of the week as well. Add a default:</p> <pre><code>import datetime\n\nd = \"2013-W26\"\nr = datetime.datetime.strptime(d + \"-1\", \"%Y-W%W-%w\")\n</code></pre> <p>The <code>-1</code> and <code>-%w</code> pattern tells the parser to pick the Monday in that week.</p>"}, {"location": "coding/python/python_snippets/#get-the-month-name-from-a-number", "title": "Get the month name from a number", "text": "<pre><code>import calendar\n\n&gt;&gt; calendar.month_name[3]\n'March'\n</code></pre>"}, {"location": "coding/python/python_snippets/#get-ordinal-from-number", "title": "Get ordinal from number", "text": "<pre><code>def int_to_ordinal(number: int) -&gt; str:\n\"\"\"Convert an integer into its ordinal representation.\n\n    make_ordinal(0)   =&gt; '0th'\n    make_ordinal(3)   =&gt; '3rd'\n    make_ordinal(122) =&gt; '122nd'\n    make_ordinal(213) =&gt; '213th'\n\n    Args:\n        number: Number to convert\n\n    Returns:\n        ordinal representation of the number\n    \"\"\"\n    suffix = [\"th\", \"st\", \"nd\", \"rd\", \"th\"][min(number % 10, 4)]\n    if 11 &lt;= (number % 100) &lt;= 13:\n        suffix = \"th\"\n    return f\"{number}{suffix}\"\n</code></pre>"}, {"location": "coding/python/python_snippets/#group-or-sort-a-list-of-dictionaries-or-objects-by-a-specific-key", "title": "Group or sort a list of dictionaries or objects by a specific key", "text": "<p>Python lists have a built-in <code>list.sort()</code> method that modifies the list in-place. There is also a <code>sorted()</code> built-in function that builds a new sorted list from an iterable.</p>"}, {"location": "coding/python/python_snippets/#sorting-basics", "title": "Sorting basics", "text": "<p>A simple ascending sort is very easy: just call the <code>sorted()</code> function. It returns a new sorted list:</p> <pre><code>&gt;&gt;&gt; sorted([5, 2, 3, 1, 4])\n[1, 2, 3, 4, 5]\n</code></pre>"}, {"location": "coding/python/python_snippets/#key-functions", "title": "Key functions", "text": "<p>Both <code>list.sort()</code> and <code>sorted()</code> have a <code>key</code> parameter to specify a function (or other callable) to be called on each list element prior to making comparisons.</p> <p>For example, here\u2019s a case-insensitive string comparison:</p> <pre><code>&gt;&gt;&gt; sorted(\"This is a test string from Andrew\".split(), key=str.lower)\n['a', 'Andrew', 'from', 'is', 'string', 'test', 'This']\n</code></pre> <p>The value of the <code>key</code> parameter should be a function (or other callable) that takes a single argument and returns a key to use for sorting purposes. This technique is fast because the key function is called exactly once for each input record.</p> <p>A common pattern is to sort complex objects using some of the object\u2019s indices as keys. For example:</p> <pre><code>&gt;&gt;&gt; from operator import itemgetter\n&gt;&gt;&gt; student_tuples = [\n    ('john', 'A', 15),\n    ('jane', 'B', 12),\n    ('dave', 'B', 10),\n]\n\n&gt;&gt;&gt; sorted(student_tuples, key=itemgetter(2))   # sort by age\n[('dave', 'B', 10), ('jane', 'B', 12), ('john', 'A', 15)]\n</code></pre> <p>The same technique works for objects with named attributes. For example:</p> <pre><code>&gt;&gt;&gt; from operator import attrgetter\n&gt;&gt;&gt; class Student:\n    def __init__(self, name, grade, age):\n        self.name = name\n        self.grade = grade\n        self.age = age\n\n    def __repr__(self):\n        return repr((self.name, self.grade, self.age))\n\n&gt;&gt;&gt; student_objects = [\n    Student('john', 'A', 15),\n    Student('jane', 'B', 12),\n    Student('dave', 'B', 10),\n]\n\n&gt;&gt;&gt; sorted(student_objects, key=attrgetter('age'))   # sort by age\n[('dave', 'B', 10), ('jane', 'B', 12), ('john', 'A', 15)]\n</code></pre> <p>The operator module functions allow multiple levels of sorting. For example, to sort by grade then by age:</p> <pre><code>&gt;&gt;&gt; sorted(student_tuples, key=itemgetter(1,2))\n[('john', 'A', 15), ('dave', 'B', 10), ('jane', 'B', 12)]\n\n&gt;&gt;&gt; sorted(student_objects, key=attrgetter('grade', 'age'))\n[('john', 'A', 15), ('dave', 'B', 10), ('jane', 'B', 12)]\n</code></pre>"}, {"location": "coding/python/python_snippets/#sorts-stability-and-complex-sorts", "title": "Sorts stability and complex sorts", "text": "<p>Sorts are guaranteed to be stable. That means that when multiple records have the same key, their original order is preserved.</p> <pre><code>&gt;&gt;&gt; data = [('red', 1), ('blue', 1), ('red', 2), ('blue', 2)]\n\n&gt;&gt;&gt; sorted(data, key=itemgetter(0))\n[('blue', 1), ('blue', 2), ('red', 1), ('red', 2)]\n</code></pre> <p>Notice how the two records for blue retain their original order so that <code>('blue', 1)</code> is guaranteed to precede <code>('blue', 2)</code>.</p> <p>This wonderful property lets you build complex sorts in a series of sorting steps. For example, to sort the student data by descending grade and then ascending age, do the age sort first and then sort again using grade:</p> <pre><code>&gt;&gt;&gt; s = sorted(student_objects, key=attrgetter('age'))     # sort on secondary key\n\n&gt;&gt;&gt; sorted(s, key=attrgetter('grade'), reverse=True)       # now sort on primary key, descending\n[('dave', 'B', 10), ('jane', 'B', 12), ('john', 'A', 15)]\n</code></pre> <p>This can be abstracted out into a wrapper function that can take a list and tuples of field and order to sort them on multiple passes.</p> <pre><code>&gt;&gt;&gt; def multisort(xs, specs):\n    for key, reverse in reversed(specs):\n        xs.sort(key=attrgetter(key), reverse=reverse)\n    return xs\n\n&gt;&gt;&gt; multisort(list(student_objects), (('grade', True), ('age', False)))\n[('dave', 'B', 10), ('jane', 'B', 12), ('john', 'A', 15)]\n</code></pre>"}, {"location": "coding/python/python_snippets/#get-the-attribute-of-an-attribute", "title": "Get the attribute of an attribute", "text": "<p>To sort the list in place:</p> <pre><code>ut.sort(key=lambda x: x.count, reverse=True)\n</code></pre> <p>To return a new list, use the <code>sorted()</code> built-in function:</p> <pre><code>newlist = sorted(ut, key=lambda x: x.body.id_, reverse=True)\n</code></pre>"}, {"location": "coding/python/python_snippets/#iterate-over-an-instance-objects-data-attributes-in-python", "title": "Iterate over an instance object's data attributes in Python", "text": "<pre><code>@dataclass(frozen=True)\nclass Search:\n    center: str\n    distance: str\n\n\nse = Search(\"a\", \"b\")\nfor key, value in se.__dict__.items():\n    print(key, value)\n</code></pre>"}, {"location": "coding/python/python_snippets/#generate-ssh-key", "title": "Generate ssh key", "text": "<pre><code>pip install cryptography\n</code></pre> <pre><code>from os import chmod\nfrom cryptography.hazmat.primitives import serialization\nfrom cryptography.hazmat.primitives.asymmetric import rsa\nfrom cryptography.hazmat.backends import default_backend as crypto_default_backend\n\nprivate_key = rsa.generate_private_key(\n    backend=crypto_default_backend(), public_exponent=65537, key_size=4096\n)\npem = private_key.private_bytes(\n    encoding=serialization.Encoding.PEM,\n    format=serialization.PrivateFormat.TraditionalOpenSSL,\n    encryption_algorithm=serialization.NoEncryption(),\n)\n\nwith open(\"/tmp/private.key\", \"wb\") as content_file:\n    chmod(\"/tmp/private.key\", 0600)\n    content_file.write(pem)\n\npublic_key = (\n    private_key.public_key().public_bytes(\n        encoding=serialization.Encoding.OpenSSH,\n        format=serialization.PublicFormat.OpenSSH,\n    )\n    + b\" user@email.org\"\n)\nwith open(\"/tmp/public.key\", \"wb\") as content_file:\n    content_file.write(public_key)\n</code></pre>"}, {"location": "coding/python/python_snippets/#make-multiline-code-look-clean", "title": "Make multiline code look clean", "text": "<p>If you need variables that contain multiline strings inside functions or methods you need to remove the indentation</p> <pre><code>def test():\n    # end first line with \\ to avoid the empty line!\n    s = \"\"\"\\\nhello\n  world\n\"\"\"\n</code></pre> <p>Which is inconvenient as it breaks some editor source code folding and it's ugly for the eye.</p> <p>The solution is to use <code>textwrap.dedent()</code></p> <pre><code>import textwrap\n\n\ndef test():\n    # end first line with \\ to avoid the empty line!\n    s = \"\"\"\\\n    hello\n      world\n    \"\"\"\n    print(repr(s))  # prints '    hello\\n      world\\n    '\n    print(repr(textwrap.dedent(s)))  # prints 'hello\\n  world\\n'\n</code></pre> <p>If you forget to add the trailing <code>\\</code> character of <code>s = '''\\</code> or use <code>s = '''hello</code>, you're going to have a bad time with black.</p>"}, {"location": "coding/python/python_snippets/#play-a-sound", "title": "Play a sound", "text": "<pre><code>pip install playsound\n</code></pre> <pre><code>from playsound import playsound\n\nplaysound(\"path/to/file.wav\")\n</code></pre>"}, {"location": "coding/python/python_snippets/#deep-copy-a-dictionary", "title": "Deep copy a dictionary", "text": "<pre><code>import copy\n\nd = {...}\nd2 = copy.deepcopy(d)\n</code></pre>"}, {"location": "coding/python/python_snippets/#find-the-root-directory-of-a-package", "title": "Find the root directory of a package", "text": "<p><code>pyprojroot</code> finds the root working directory for your project as a <code>pathlib</code> object. You can now use the here function to pass in a relative path from the project root directory (no matter what working directory you are in the project), and you will get a full path to the specified file.</p>"}, {"location": "coding/python/python_snippets/#installation", "title": "Installation", "text": "<pre><code>pip install pyprojroot\n</code></pre>"}, {"location": "coding/python/python_snippets/#usage", "title": "Usage", "text": "<pre><code>from pyprojroot import here\n\nhere()\n</code></pre>"}, {"location": "coding/python/python_snippets/#check-if-an-object-has-an-attribute", "title": "Check if an object has an attribute", "text": "<pre><code>if hasattr(a, \"property\"):\n    a.property\n</code></pre>"}, {"location": "coding/python/python_snippets/#check-if-a-loop-ends-completely", "title": "Check if a loop ends completely", "text": "<p><code>for</code> loops can take an <code>else</code> block which is not run if the loop has ended with a <code>break</code> statement.</p> <pre><code>for i in [1, 2, 3]:\n    print(i)\n    if i == 3:\n        break\nelse:\n    print(\"for loop was not broken\")\n</code></pre>"}, {"location": "coding/python/python_snippets/#merge-two-lists", "title": "Merge two lists", "text": "<pre><code>z = x + y\n</code></pre>"}, {"location": "coding/python/python_snippets/#merge-two-dictionaries", "title": "Merge two dictionaries", "text": "<pre><code>z = {**x, **y}\n</code></pre>"}, {"location": "coding/python/python_snippets/#create-user-defined-exceptions", "title": "Create user defined exceptions", "text": "<p>Programs may name their own exceptions by creating a new exception class. Exceptions should typically be derived from the <code>Exception</code> class, either directly or indirectly.</p> <p>Exception classes are meant to be kept simple, only offering a number of attributes that allow information about the error to be extracted by handlers for the exception. When creating a module that can raise several distinct errors, a common practice is to create a base class for exceptions defined by that module, and subclass that to create specific exception classes for different error conditions:</p> <pre><code>class Error(Exception):\n\"\"\"Base class for exceptions in this module.\"\"\"\n\n\nclass ConceptNotFoundError(Error):\n\"\"\"Transactions with unmatched concept.\"\"\"\n\n    def __init__(self, message: str, transactions: List[Transaction]) -&gt; None:\n\"\"\"Initialize the exception.\"\"\"\n        self.message = message\n        self.transactions = transactions\n        super().__init__(self.message)\n</code></pre> <p>Most exceptions are defined with names that end in \u201cError\u201d, similar to the naming of the standard exceptions.</p>"}, {"location": "coding/python/python_snippets/#import-a-module-or-its-objects-from-within-a-python-program", "title": "Import a module or it's objects from within a python program", "text": "<pre><code>import importlib\n\nmodule = importlib.import_module(\"os\")\nmodule_class = module.getcwd\n\nrelative_module = importlib.import_module(\".model\", package=\"mypackage\")\nclass_to_extract = \"MyModel\"\nextracted_class = geattr(relative_module, class_to_extract)\n</code></pre> <p>The first argument specifies what module to import in absolute or relative terms (e.g. either <code>pkg.mod</code> or <code>..mod</code>). If the name is specified in relative terms, then the package argument must be set to the name of the package which is to act as the anchor for resolving the package name (e.g. <code>import_module('..mod', 'pkg.subpkg')</code> will <code>import pkg.mod</code>).</p>"}, {"location": "coding/python/python_snippets/#get-systems-timezone-and-use-it-in-datetime", "title": "Get system's timezone and use it in datetime", "text": "<p>To obtain timezone information in the form of a <code>datetime.tzinfo</code> object, use <code>dateutil.tz.tzlocal()</code>:</p> <pre><code>from dateutil import tz\n\nmyTimeZone = tz.tzlocal()\n</code></pre> <p>This object can be used in the <code>tz</code> parameter of <code>datetime.datetime.now()</code>:</p> <pre><code>from datetime import datetime\nfrom dateutil import tz\n\nlocalisedDatetime = datetime.now(tz=tz.tzlocal())\n</code></pre>"}, {"location": "coding/python/python_snippets/#capitalize-a-sentence", "title": "Capitalize a sentence", "text": "<p>To change the caps of the first letter of the first word of a sentence use:</p> <pre><code>&gt;&gt; sentence = \"add funny Emojis\"\n&gt;&gt; sentence[0].upper() + sentence[1:]\nAdd funny Emojis\n</code></pre> <p>The <code>.capitalize</code> method transforms the rest of words to lowercase. The <code>.title</code> transforms all sentence words to capitalize.</p>"}, {"location": "coding/python/python_snippets/#get-the-last-monday-datetime", "title": "Get the last monday datetime", "text": "<pre><code>import datetime\n\ntoday = datetime.date.today()\nlast_monday = today - datetime.timedelta(days=today.weekday())\n</code></pre>"}, {"location": "coding/python/python_snippets/#issues", "title": "Issues", "text": "<ul> <li>Pypi won't allow you to upload packages with direct dependencies:   update the section above.</li> </ul>"}, {"location": "coding/python/redis-py/", "title": "Redis-py", "text": "<p>Redis-py is The Python interface to the Redis key-value store.</p> <p>The library encapsulates an actual TCP connection to a Redis server and sends raw commands, as bytes serialized using the REdis Serialization Protocol (RESP), to the server. It then takes the raw reply and parses it back into a Python object such as bytes, int, or even datetime.datetime.</p>"}, {"location": "coding/python/redis-py/#installation", "title": "Installation", "text": "<pre><code>pip install redis\n</code></pre>"}, {"location": "coding/python/redis-py/#usage", "title": "Usage", "text": "<pre><code>import redis\nr = redis.Redis(\n    host='localhost',\n    port=6379,\n    db=0,\n    password=None,\n    socket_timeout=None,\n)\n</code></pre> <p>The arguments specified above are the default ones, so it's the same as calling <code>r = redis.Redis()</code>.</p> <p>The <code>db</code> parameter is the database number. You can manage multiple databases in Redis at once, and each is identified by an integer. The max number of databases is 16 by default.</p>"}, {"location": "coding/python/redis-py/#common-pitfalls", "title": "Common pitfalls", "text": "<ul> <li>Redis returned objects are bytes type, so you may need to convert it to     string with <code>r.get(\"Bahamas\").decode(\"utf-8\")</code>.</li> </ul>"}, {"location": "coding/python/redis-py/#references", "title": "References", "text": "<ul> <li>Real Python introduction to Redis-py</li> <li>Git</li> <li>Docs: Very technical and small.</li> </ul>"}, {"location": "coding/python/requests_mock/", "title": "Requests mock", "text": "<p>The requests-mock library is a requests transport adapter that can be preloaded with responses that are returned if certain URIs are requested. This is particularly useful in unit tests where you want to return known responses from HTTP requests without making actual calls.</p>"}, {"location": "coding/python/requests_mock/#installation", "title": "Installation", "text": "<pre><code>pip install requests-mock\n</code></pre>"}, {"location": "coding/python/requests_mock/#usage", "title": "Usage", "text": ""}, {"location": "coding/python/requests_mock/#object-initialization", "title": "Object initialization", "text": "<p>Select one of the following ways to initialize the mock.</p>"}, {"location": "coding/python/requests_mock/#as-a-pytest-fixture", "title": "As a pytest fixture", "text": "<p>The ease of use with pytest it is awesome. <code>requests-mock</code> provides an external fixture registered with pytest such that it is usable simply by specifying it as a parameter. There is no need to import requests-mock it simply needs to be installed and specified as an argument in the test definition.</p> <pre><code>import pytest\nimport requests\nfrom requests_mock.mocker import Mocker\n\n\ndef test_url(requests_mock: Mocker):\n    requests_mock.get('http://test.com', text='data')\n    assert 'data' == requests.get('http://test.com').text\n</code></pre>"}, {"location": "coding/python/requests_mock/#as-a-function-decorator", "title": "As a function decorator", "text": "<pre><code>&gt;&gt;&gt; @requests_mock.Mocker()\n... def test_function(m):\n...     m.get('http://test.com', text='resp')\n...     return requests.get('http://test.com').text\n...\n&gt;&gt;&gt; test_function()\n'resp'\n</code></pre>"}, {"location": "coding/python/requests_mock/#as-a-context-manager", "title": "As a context manager", "text": "<pre><code>&gt;&gt;&gt; import requests\n&gt;&gt;&gt; import requests_mock\n\n&gt;&gt;&gt; with requests_mock.Mocker() as m:\n...     m.get('http://test.com', text='resp')\n...     requests.get('http://test.com').text\n...\n'resp'\n</code></pre>"}, {"location": "coding/python/requests_mock/#mocking-responses", "title": "Mocking responses", "text": ""}, {"location": "coding/python/requests_mock/#return-a-json", "title": "Return a json", "text": "<pre><code>requests_mock.get(\n    '{}/api/repos/owner/repository/builds'.format(self.url),\n    json={\n        \"id\": 882,\n        \"number\": 209,\n        \"finished\": 1591197904,\n    },\n)\n</code></pre>"}, {"location": "coding/python/requests_mock/#add-a-header-or-a-cookie-to-the-response", "title": "Add a header or a cookie to the response", "text": "<pre><code>requests_mock.post(\n    \"https://test.com\",\n    cookies={\"Id\": \"0\"},\n    headers={\"id\": \"0\"},\n)\n</code></pre>"}, {"location": "coding/python/requests_mock/#multiple-responses", "title": "Multiple responses", "text": "<p>Multiple responses can be provided to be returned in order by specifying the keyword parameters in a list.</p> <pre><code>requests_mock.get(\n    'https://test.com/4',\n    [\n        {'text': 'resp1', 'status_code': 300},\n        {'text': 'resp2', 'status_code': 200}\n    ]\n)\n</code></pre>"}, {"location": "coding/python/requests_mock/#get-requests-history", "title": "Get requests history", "text": ""}, {"location": "coding/python/requests_mock/#called", "title": "Called", "text": "<p>The easiest way to test if a request hit the adapter is to simply check the called property or the call_count property.</p> <pre><code>&gt;&gt;&gt; import requests\n&gt;&gt;&gt; import requests_mock\n\n&gt;&gt;&gt; with requests_mock.mock() as m:\n...     m.get('http://test.com, text='resp')\n...     resp = requests.get('http://test.com')\n...\n&gt;&gt;&gt; m.called\nTrue\n&gt;&gt;&gt; m.call_count\n1\n</code></pre>"}, {"location": "coding/python/requests_mock/#requests-history", "title": "Requests history", "text": "<p>The history of objects that passed through the mocker/adapter can also be retrieved.</p> <pre><code>&gt;&gt;&gt; history = m.request_history\n&gt;&gt;&gt; len(history)\n1\n&gt;&gt;&gt; history[0].method\n'GET'\n&gt;&gt;&gt; history[0].url\n'http://test.com/'\n</code></pre>"}, {"location": "coding/python/requests_mock/#references", "title": "References", "text": "<ul> <li>Docs</li> <li>Git</li> </ul>"}, {"location": "coding/python/rq/", "title": "RQ", "text": "<p>RQ (Redis Queue) is a simple Python library for queueing jobs and processing them in the background with workers. It is backed by Redis and it is designed to have a low barrier to entry.</p> <p>Check arq</p> <p>Next time you are going to use this, check if arq is better.</p>"}, {"location": "coding/python/rq/#getting-started", "title": "Getting started", "text": "<p>Assuming that a Redis server is running, define the function you want to run:</p> <pre><code>import requests\n\ndef count_words_at_url(url):\n    resp = requests.get(url)\n    return len(resp.text.split())\n</code></pre> <p>The, create a RQ queue:</p> <pre><code>from redis import Redis\nfrom rq import Queue\n\nq = Queue(connection=Redis())\n</code></pre> <p>And enqueue the function call:</p> <p><pre><code>from my_module import count_words_at_url\nresult = q.enqueue(\n             count_words_at_url, 'http://nvie.com')\n</code></pre> To start executing enqueued function calls in the background, start a worker from your project\u2019s directory:</p> <pre><code>$ rq worker\n*** Listening for work on default\nGot count_words_at_url('http://nvie.com') from default\nJob result = 818\n*** Listening for work on default\n</code></pre>"}, {"location": "coding/python/rq/#install", "title": "Install", "text": "<pre><code>pip install rq\n</code></pre>"}, {"location": "coding/python/rq/#reference", "title": "Reference", "text": "<ul> <li>Homepage</li> <li>Git</li> <li>Docs</li> </ul>"}, {"location": "coding/python/ruamel_yaml/", "title": "Ruamel Yaml", "text": "<p>ruamel.yaml is a YAML 1.2 loader/dumper package for Python. It is a derivative of Kirill Simonov\u2019s PyYAML 3.11.</p> <p>It has the following enhancements:</p> <ul> <li>Comments.</li> <li>Block style and key ordering are kept, so you can diff the round-tripped     source.</li> <li>Flow style sequences ( \u2018a: b, c, d\u2019).</li> <li>Anchor names that are hand-crafted (i.e. not of the form<code>idNNN</code>).</li> <li>Merges in dictionaries are preserved.</li> </ul>"}, {"location": "coding/python/ruamel_yaml/#installation", "title": "Installation", "text": "<p>I suggest to use the <code>ruyaml</code> fork, as it's maintained by the community and versioned with git.</p> <pre><code>pip install ruyaml\n</code></pre>"}, {"location": "coding/python/ruamel_yaml/#usage", "title": "Usage", "text": "<p>Very similar to PyYAML. If invoked with <code>YAML(typ='safe')</code> either the load or the write of the data, the comments of the yaml will be lost.</p>"}, {"location": "coding/python/ruamel_yaml/#load-from-file", "title": "Load from file", "text": "<pre><code>from ruamel.yaml import YAML\nfrom ruamel.yaml.scanner import ScannerError\n\ntry:\n    with open(os.path.expanduser(file_path), 'r') as f:\n        try:\n            data = YAML().load(f)\n        except ScannerError as e:\n            log.error(\n                'Error parsing yaml of configuration file '\n                '{}: {}'.format(\n                    e.problem_mark,\n                    e.problem,\n                )\n            )\n            sys.exit(1)\nexcept FileNotFoundError:\n    log.error(\n        'Error opening configuration file {}'.format(file_path)\n    )\n    sys.exit(1)\n</code></pre>"}, {"location": "coding/python/ruamel_yaml/#save-to-file", "title": "Save to file", "text": "<pre><code>with open(os.path.expanduser(file_path), 'w+') as f:\n    yaml = YAML()\n    yaml.default_flow_style = False\n    yaml.dump(data, f)\n</code></pre>"}, {"location": "coding/python/ruamel_yaml/#save-to-a-string", "title": "Save to a string", "text": "<p>For some unknown reason, they don't want to output the result to a string, you need to mess up with streams.</p> <pre><code># Configure YAML formatter\nyaml = YAML()\nyaml.indent(mapping=2, sequence=4, offset=2)\nyaml.allow_duplicate_keys = True\nyaml.explicit_start = False\n\n# Return the output to a string\nstring_stream = StringIO()\nyaml.dump({'products': ['item 1', 'item 2']}, string_stream)\nsource_code = string_stream.getvalue()\nstring_stream.close()\n</code></pre> <p>I've opened an issue in the ruyaml fork to solve it.</p>"}, {"location": "coding/python/ruamel_yaml/#references", "title": "References", "text": "<ul> <li>Docs</li> <li>Code</li> </ul>"}, {"location": "coding/python/sqlalchemy/", "title": "SQLAlchemy", "text": "<p>SQLAlchemy is the Python SQL toolkit and Object Relational Mapper that gives application developers the full power and flexibility of SQL.</p> <p>I discourage you to use an ORM to manage the interactions with the database. Check the alternative solutions.</p>"}, {"location": "coding/python/sqlalchemy/#creating-an-sql-schema", "title": "Creating an SQL Schema", "text": "<p>First of all it's important to create a diagram with the database structure, it will help you in the design and it's a great improvement of the project's documentation. I usually use ondras wwwsqldesigner through his demo, as it's easy to use and it's possible to save the data in your repository in an xml file.</p> <p>I assume you've already set up your project to support sqlalchemy in your project. If not, do so before moving forward.</p>"}, {"location": "coding/python/sqlalchemy/#mapping-styles", "title": "Mapping styles", "text": "<p>Modern SQLAlchemy features two distinct styles of mapper configuration. The \u201cClassical\u201d style is SQLAlchemy\u2019s original mapping API, whereas \u201cDeclarative\u201d is the richer and more succinct system that builds on top of \u201cClassical\u201d.</p> <p>The Classical, on the other hand, doesn't lock your models to the ORM, something that we avoid when using the repository pattern.</p> <p>If you aren't going to use the repository pattern, use the declarative way, otherwise use the classical one</p>"}, {"location": "coding/python/sqlalchemy/#creating-tables", "title": "Creating Tables", "text": "<p>If you simply want to create a table of association without any parameters, such as with a many to many relationship association table, use this type of object.</p>"}, {"location": "coding/python/sqlalchemy/#declarative-type", "title": "Declarative type", "text": "<pre><code>class User(Base):\n\"\"\"\n    Class to define the User model.\n    \"\"\"\n    __tablename__ = 'user'\n    id = Column(Integer, primary_key=True, doc='User ID')\n    name = Column(String, doc='User name')\n\n    def __init__(\n        self,\n        id,\n        name=None,\n    ):\n        self.id = id\n        self.name = name\n</code></pre> <p>There are different types of fields to add to a table:</p> <ul> <li>Boolean: <code>is_true = Column(Boolean)</code>.</li> <li>Datetime: <code>created_date = Column(DateTime, doc='Date of creation')</code>.</li> <li>Float: <code>score = Column(Float)</code></li> <li>Integer: <code>id = Column(Integer, primary_key=True, doc='Source ID')</code>.</li> <li>String: <code>title = Column(String)</code>.</li> <li>Text: <code>long_text = Column(Text)</code>.</li> </ul> <p>To make sure that a field can't contain <code>nulls</code> set the <code>nullable=False</code> attribute in the definition of the <code>Column</code>. If you want the contents to be unique use <code>unique=True</code>.</p> <p>If you want to use the Mysql driver of SQLAlchemy make sure to specify the length of the colums, for example <code>String(16)</code>. For reference this are the common lengths:</p> <ul> <li>url: 2083</li> <li>name: 64 (it occupies the same 2 and 255).</li> <li>email: 64 (it occupies the same 2 and 255).</li> <li>username: 64 (it occupies the same 2 and 255).</li> </ul>"}, {"location": "coding/python/sqlalchemy/#classical-type", "title": "Classical type", "text": "<p>File: model.py</p> <pre><code>class User():\n    def __init__(self, id, name=None):\n        self.id = id\n        self.name = name\n</code></pre> <p>File: orm.py</p> <pre><code>from models import User\nfrom sqlalchemy import (\n    Column,\n    MetaData,\n    String,\n    Table,\n    Text,\n)\n\nmetadata = MetaData()\n\nuser = Table(\n    \"user\",\n    metadata,\n    Column(\"id\", String(64), primary_key=True),\n    Column(\"name\", String(64)),\n)\n\ndef start_mappers():\n    mapper(User, user)\n</code></pre>"}, {"location": "coding/python/sqlalchemy/#creating-relationships", "title": "Creating relationships", "text": ""}, {"location": "coding/python/sqlalchemy/#joined-table-inheritance", "title": "Joined table inheritance", "text": "<p>In joined table inheritance, each class along a hierarchy of classes is represented by a distinct table. Querying for a particular subclass in the hierarchy will render as a <code>SQL JOIN</code> along all tables in its inheritance path. If the queried class is the base class, the default behavior is to include only the base table in a <code>SELECT</code> statement. In all cases, the ultimate class to instantiate for a given row is determined by a discriminator column or an expression that works against the base table. When a subclass is loaded only against a base table, resulting objects by default will have base attributes populated at first; attributes that are local to the subclass will lazy load when they are accessed.</p> <p>The base class in a joined inheritance hierarchy is configured with additional arguments that will refer to the polymorphic discriminator column as well as the identifier for the base class.</p>"}, {"location": "coding/python/sqlalchemy/#declarative", "title": "Declarative", "text": "<pre><code>class Employee(Base):\n    __tablename__ = 'employee'\n    id = Column(Integer, primary_key=True)\n    name = Column(String(50))\n    type = Column(String(50))\n\n    __mapper_args__ = {\n        'polymorphic_identity': 'employee',\n        'polymorphic_on': type\n    }\n\n\nclass Engineer(Employee):\n    __tablename__ = 'engineer'\n    id = Column(Integer, ForeignKey('employee.id'), primary_key=True)\n    engineer_name = Column(String(30))\n\n    __mapper_args__ = {\n        'polymorphic_identity': 'engineer',\n    }\n\n\nclass Manager(Employee):\n    __tablename__ = 'manager'\n    id = Column(Integer, ForeignKey('employee.id'), primary_key=True)\n    manager_name = Column(String(30))\n\n    __mapper_args__ = {\n        'polymorphic_identity': 'manager',\n    }\n</code></pre>"}, {"location": "coding/python/sqlalchemy/#classical", "title": "Classical", "text": "<p>File: model.py</p> <pre><code>class Employee:\n\n def __init__(self, name):\n     self.name = name\n\n class Manager(Employee):\n\n     def __init__(self, name, manager_data):\n         super().__init__(name)\n         self.manager_data = manager_data\n\n class Engineer(Employee):\n\n     def __init__(self, name, engineer_info):\n         super().__init__(name)\n         self.engineer_info = engineer_info\n</code></pre> <p>File: orm.py</p> <pre><code>metadata = MetaData()\n\nemployee = Table(\n    'employee',\n    metadata,\n    Column('id', Integer, primary_key=True),\n    Column('name', String(50)),\n    Column('type', String(20)),\n    Column('manager_data', String(50)),\n    Column('engineer_info', String(50))\n)\n\nmapper(Employee, employee,\n       polymorphic_on=employee.c.type,\n       polymorphic_identity='employee',\n       exclude_properties={'engineer_info', 'manager_data'})\n\n\nmapper(Manager,\n       inherits=Employee,\n       polymorphic_identity='manager',\n       exclude_properties={'engineer_info'})\n\n\nmapper(Engineer,\n       inherits=Employee,\n       polymorphic_identity='engineer',\n       exclude_properties={'manager_data'})\n</code></pre>"}, {"location": "coding/python/sqlalchemy/#one-to-many", "title": "One to many", "text": "<pre><code>from sqlalchemy.orm import relationship\n\n\nclass User(db.Model):\n    __tablename__ = 'user'\n    id = Column(Integer, primary_key=True)\n    posts = relationship('Post', back_populates='user')\n\n\nclass Post(db.Model):\n    id = Column(Integer, primary_key=True)\n    body = Column(String(140))\n    user_id = Column(Integer, ForeignKey('user.id'))\n    user = relationship('User', back_populates='posts')\n</code></pre> <p>In the tests of the <code>Post</code> class, only check that the <code>user</code> attribute is present.</p> <p>Factoryboy supports the creation of Dependent objects direct ForeignKey.</p>"}, {"location": "coding/python/sqlalchemy/#self-referenced-one-to-many", "title": "Self referenced one to many", "text": "<pre><code>class Task(Base):\n    __tablename__ = 'task'\n    id = Column(String, primary_key=True, doc='fulid of creation')\n\n    parent_id = Column(String, ForeignKey('task.id'))\n    parent = relationship('Task', remote_side=[id], backref='children')\n</code></pre>"}, {"location": "coding/python/sqlalchemy/#many-to-many", "title": "Many to many", "text": "<pre><code># Association tables\n\nsource_has_category = Table(\n    'source_has_category',\n    Base.metadata,\n    Column('source_id', Integer, ForeignKey('source.id')),\n    Column('category_id', Integer, ForeignKey('category.id'))\n)\n\n# Tables\n\n\nclass Category(Base):\n    __tablename__ = 'category'\n    id = Column(String, primary_key=True)\n    contents = relationship(\n        'Content',\n        back_populates='categories',\n        secondary=source_has_category,\n    )\n\n\nclass Content(Base):\n    __tablename__ = 'content'\n    id = Column(Integer, primary_key=True, doc='Content ID')\n    categories = relationship(\n        'Category',\n        back_populates='contents',\n        secondary=source_has_category,\n    )\n</code></pre>"}, {"location": "coding/python/sqlalchemy/#self-referenced-many-to-many", "title": "Self referenced many to many", "text": "<p>Using the <code>followers</code> table as an association table.</p> <p><pre><code>followers = db.Table(\n    'followers',\n    Base.metadata,\n    Column('follower_id', Integer, ForeignKey('user.id')),\n    Column('followed_id', Integer, ForeignKey('user.id')),\n)\n\n\nclass User(Base):\n    __tablename__ = 'user'\n    id = Column(Integer, primary_key=True)\n    followed = relationship(\n        'User',\n        secondary=followers,\n        primaryjoin=(followers.c.follower_id == id),\n        secondaryjoin=(followers.c.followed_id == id),\n        backref=db.backref('followers', lazy='dynamic'),\n        lazy='dynamic',\n    )\n</code></pre> Links User instances to other User instances, so as a convention let's say that for a pair of users linked by this relationship, the left side user is following the right side user. The relationship definition is created as seen from the left side user with the name followed, because when this relationship is queried from the left side it will get the list of followed users (i.e those on the right side).</p> <ul> <li><code>User</code>: Is the right side entity of the relationship. Since this is   a self-referential relationship, The same class must be used on both sides.</li> <li><code>secondary</code>: configures the association table that is used for this   relationship.</li> <li><code>primaryjoin</code>: Indicates the condition that links the left side entity (the   follower user) with the association table. The join condition for the left   side of the relationship is the user id matching the <code>follower_id</code> field of   the association table. The <code>followers.c.follower_id</code> expression references the   <code>follower_id</code> column of the association table.</li> <li><code>secondaryjoin</code>: Indicates the condition that links the right side entity (the   followed user) with the association table. This condition is similar to the   one for <code>primaryjoin</code>.</li> <li><code>backref</code>: Defines how this relationship will be accessed from the right side   entity. From the left side, the relationship is named <code>followed</code>, so from the   right side, the name <code>followers</code> represent all the left side users that are   linked to the target user in the right side. The additional <code>lazy</code> argument   indicates the execution mode for this query. A mode of <code>dynamic</code> sets up the   query not to run until specifically requested.</li> <li><code>lazy</code>: same as with <code>backref</code>, but this one applies to the left side query   instead of the right side.</li> </ul>"}, {"location": "coding/python/sqlalchemy/#testing-sqlalchemy-code", "title": "Testing SQLAlchemy Code", "text": "<p>The definition of the database can be tested, I usually use them to test that the attributes are loaded and that the factory objects work as expected. Several steps need to be set to make it work:</p> <ul> <li> <p>Create the factory boy objects in <code>tests/factories.py</code>.</p> </li> <li> <p>Configure the tests to use a temporal sqlite database in the     <code>tests/conftest.py</code> file with the following contents (changing <code>{{     program_name }}</code>):</p> <pre><code>from alembic.command import upgrade\nfrom alembic.config import Config\nfrom sqlalchemy.orm import sessionmaker\n\nimport os\nimport pytest\nimport tempfile\n\ntemp_ddbb = tempfile.mkstemp()[1]\n\nos.environ['{{ program_name }} _DATABASE_URL'] = 'sqlite:///{}'.format(temp_ddbb)\n\n# It needs to be after the environmental variable\nfrom {{ program_name }}.models import engine\nfrom tests import factories\n\n\n@pytest.fixture(scope='module')\ndef connection():\n'''\n    Fixture to set up the connection to the temporal database, the path is\n    stablished at conftest.py\n    '''\n\n    # Create database connection\n    connection = engine.connect()\n\n    # Applies all alembic migrations.\n    config = Config('{{ program_name }}/migrations/alembic.ini')\n    upgrade(config, 'head')\n\n    # End of setUp\n\n    yield connection\n\n    # Start of tearDown\n    connection.close()\n\n\n@pytest.fixture(scope='function')\ndef session(connection):\n'''\n    Fixture to set up the sqlalchemy session of the database.\n    '''\n\n    # Begin a non-ORM transaction and bind session\n    transaction = connection.begin()\n    session = sessionmaker()(bind=connection)\n\n    factories.UserFactory._meta.sqlalchemy_session = session\n\n    yield session\n\n    # Close session and rollback transaction\n    session.close()\n    transaction.rollback()\n</code></pre> </li> <li> <p>Define an abstract base test class <code>BaseModelTest</code> defined as following in the <code>tests/unit/test_models.py</code> file.</p> </li> </ul> <pre><code>from {{ program_name }} import models\nfrom tests import factories\n\nimport pytest\n\n\nclass BaseModelTest:\n\"\"\"\n    Abstract base test class to refactor model tests.\n\n    The Children classes must define the following attributes:\n        self.model: The model object to test.\n        self.dummy_instance: A factory object of the model to test.\n        self.model_attributes: List of model attributes to test\n\n    Public attributes:\n        dummy_instance (Factory_boy object): Dummy instance of the model.\n    \"\"\"\n\n    @pytest.fixture(autouse=True)\n    def base_setup(self, session):\n        self.session = session\n\n    def test_attributes_defined(self):\n        for attribute in self.model_attributes:\n            assert getattr(self.model, attribute) == \\\n                getattr(self.dummy_instance, attribute)\n\n\n@pytest.mark.usefixtures('base_setup')\nclass TestUser(BaseModelTest):\n\n    @pytest.fixture(autouse=True)\n    def setup(self, session):\n        self.factory = factories.UserFactory\n        self.dummy_instance = self.factory.create()\n        self.model = models.User(\n            id=self.dummy_instance.id,\n            name=self.dummy_instance.name,\n        )\n        self.model_attributes = [\n            'name',\n            'id',\n        ]\n</code></pre> <ul> <li>Then create the <code>models</code> table.</li> <li>Create an alembic revision</li> <li>Run <code>pytest</code>: <code>python -m pytest</code>.</li> </ul>"}, {"location": "coding/python/sqlalchemy/#exporting-database-to-json", "title": "Exporting database to json", "text": "<pre><code>import json\n\ndef dump_sqlalchemy(output_connection_string,output_schema):\n\"\"\" Returns the entire content of a database as lists of dicts\"\"\"\n    engine = create_engine(f'{output_connection_string}{output_schema}')\n    meta = MetaData()\n    meta.reflect(bind=engine)  # http://docs.sqlalchemy.org/en/rel_0_9/core/reflection.html\n    result = {}\n    for table in meta.sorted_tables:\n        result[table.name] = [dict(row) for row in engine.execute(table.select())]\n    return json.dumps(result)\n</code></pre>"}, {"location": "coding/python/sqlalchemy/#cloning-an-sqlalchemy-object", "title": "Cloning an SQLAlchemy object", "text": "<p>The following function:</p> <ul> <li>Copies all the non-primary-key columns from the input model to a new model instance.</li> <li>Allows definition of specific arguments.</li> <li>Leaves the original model object unmodified.</li> </ul> <pre><code>def clone_model(model, **kwargs):\n\"\"\"Clone an arbitrary sqlalchemy model object without its primary key values.\"\"\"\n\n    table = model.__table__\n    non_primary_key_columns = [\n        column_name\n        for column_name in table.__mapper__.attrs..keys()\n        if column_name not in table.primary_key\n    ]\n    data = {\n        column_name: getattr(model, column_name)\n        for column_name in non_pk_columns\n    }\n    data.update(kwargs)\n\n    return model.__class__(**data)\n</code></pre>"}, {"location": "coding/python/sqlalchemy/#references", "title": "References", "text": "<ul> <li>Home</li> <li>Docs</li> </ul>"}, {"location": "coding/python/tinydb/", "title": "Tinydb", "text": "<p>Tinydb is a document oriented database that stores data in a json file. It's the closest solution to a NoSQL SQLite solution that I've found.</p> <p>The advantages are that you can use a NoSQL database without installing a server. Tinydb is small, simple to use, well tested, optimized and extensible. On the other hand, if you are searching for advanced database features like more than one connection or high performance, you should consider using databases like SQLite or MongoDB.</p> <p>I think it's the perfect solution for initial versions of a program, when the database schema is variable and there is no need of high performance. Once the program is stabilized and the performance drops, you can change the storage provider to a production ready one.</p> <p>To make this change doable, I recommend implementing the repository pattern to decouple the storage layer from your application logic.</p>"}, {"location": "coding/python/tinydb/#install", "title": "Install", "text": "<pre><code>pip install tinydb\n</code></pre>"}, {"location": "coding/python/tinydb/#basic-usage", "title": "Basic usage", "text": "<p>TL;DR: Operation Cheatsheet</p> <ul> <li>Inserting data: <code>db.insert(...)</code>.</li> <li>Getting data:<ul> <li><code>db.all()</code>: Get all documents.</li> <li><code>iter(db)</code>: Iterate over all the documents.</li> <li><code>db.search(query)</code>: Get a list of documents matching the query.</li> </ul> </li> <li>Updating:<ul> <li><code>db.update(fields, query)</code>: Update all documents matching the query to contain     fields.</li> </ul> </li> <li>Removing:<ul> <li><code>db.remove(query)</code>: Remove all documents matching the query.</li> <li><code>db.truncate()</code>: Remove all documents.</li> </ul> </li> <li>Querying:</li> <li><code>Query()</code>: Create a new query object.</li> <li><code>Query().field == 2</code>: Match any document that has a key field with value <code>==     2</code> (also possible: <code>!=</code>, <code>&gt;</code>, <code>&gt;=</code>, <code>&lt;</code>, <code>&lt;=</code>).</li> </ul> <p>First you need to setup the database:</p> <pre><code>from tinydb import TinyDB, Query\n\ndb = TinyDB('db.json')\n</code></pre> <p>TinyDB expects the data to be Python dictionaries:</p> <pre><code>db.insert({'type': 'apple', 'count': 7})\ndb.insert({'type': 'peach', 'count': 3})\n</code></pre> <p>You can also iterate over stored documents:</p> <pre><code>&gt;&gt;&gt; for item in db:\n&gt;&gt;&gt;     print(item)\n{'count': 7, 'type': 'apple'}\n{'count': 3, 'type': 'peach'}\n</code></pre> <p>You can search for specific documents:</p> <pre><code>&gt;&gt;&gt; Fruit = Query()\n&gt;&gt;&gt; db.search(Fruit.type == 'peach')\n[{'count': 3, 'type': 'peach'}]\n&gt;&gt;&gt; db.search(Fruit.count &gt; 5)\n[{'count': 7, 'type': 'apple'}]\n</code></pre> <p>You can update fields:</p> <pre><code>&gt;&gt;&gt; db.update({'count': 10}, Fruit.type == 'apple')\n&gt;&gt;&gt; db.all()\n[{'count': 10, 'type': 'apple'}, {'count': 3, 'type': 'peach'}]\n</code></pre> <p>And remove documents:</p> <pre><code>&gt;&gt;&gt; db.remove(Fruit.count &lt; 5)\n&gt;&gt;&gt; db.all()\n[{'count': 10, 'type': 'apple'}]\n</code></pre>"}, {"location": "coding/python/tinydb/#query-construction", "title": "Query construction", "text": "<ul> <li>Match any document where a field called <code>field</code> exists:     <code>Query().field.exists()</code>.</li> <li>Match any document with the whole field matching the regular expression:     <code>Query().field.matches(regex)</code>.</li> <li>Match any document with a substring of the field matching the regular     expression: <code>Query().field.search(regex)</code>.</li> <li>Match any document for which the function returns <code>True</code>:     <code>Query().field.test(func, *args)</code>.</li> <li>If given a query, match all documents where all documents in the list     field match the query. If given a list, matches all documents where all     documents in the list field are a member of the given list:     <code>Query().field.all(query | list)</code>.</li> <li>If given a query, match all documents where at least one document in the     list field match the query. If given a list, matches all documents where     at least one documents in the list field are a member of the given list:     <code>Query().field.any(query | list)</code>.</li> <li>Match if the field is contained in the list: <code>Query().field.one_of(list)</code>.</li> </ul> <p>Logical operations on queries</p> <ul> <li>Match documents that don't match the query: <code>~ (query)</code>.</li> <li>Match documents that match both queries: <code>(query1) &amp; (query2)</code>.</li> <li>Match documents that match at least one of the queries: <code>(query1)     | (query2)</code>.</li> </ul> <p>To retrieve the data from the database, you need to use <code>Query</code> objects in a similar way as you do with ORMs.</p> <pre><code>from tinydb import Query\n\nUser = Query()\ndb.search(User.name == 'John')\ndb.search(User.birthday.year == 1990)\n</code></pre> <p>If the field is not a valid Python identifier use the following syntax:</p> <pre><code>db.search(User['country-code'] == 'foo')\n</code></pre>"}, {"location": "coding/python/tinydb/#advanced-queries", "title": "Advanced queries", "text": "<p>TinyDB supports other ways to search in your data:</p> <ul> <li> <p>Testing the existence of a field:</p> <pre><code>db.search(User.name.exists())\n</code></pre> </li> <li> <p>Testing values against regular expressions:</p> <pre><code># Full item has to match the regex:\ndb.search(User.name.matches('[aZ]*'))\n\n# Any part of the item has to match the regex:\ndb.search(User.name.search('b+'))\n</code></pre> </li> <li> <p>Testing using custom tests:</p> <pre><code># Custom test:\ntest_func = lambda s: s == 'John'\ndb.search(User.name.test(test_func))\n\n# Custom test with parameters:\ndef test_func(val, m, n):\n    return m &lt;= val &lt;= n\ndb.search(User.age.test(test_func, 0, 21))\ndb.search(User.age.test(test_func, 21, 99))\n</code></pre> </li> <li> <p>Testing fields that contain lists with the <code>any</code> and <code>all</code> methods: Assuming     we have a user object with a groups list like this:</p> <pre><code>db.insert({'name': 'user1', 'groups': ['user']})\ndb.insert({'name': 'user2', 'groups': ['admin', 'user']})\ndb.insert({'name': 'user3', 'groups': ['sudo', 'user']})\n</code></pre> <p>You can use the following queries:</p> <pre><code># User's groups include at least one value from ['admin', 'sudo']\n&gt;&gt;&gt; db.search(User.groups.any(['admin', 'sudo']))\n[{'name': 'user2', 'groups': ['admin', 'user']},\n {'name': 'user3', 'groups': ['sudo', 'user']}]\n\n# User's groups include all values from ['admin', 'user']\n&gt;&gt;&gt; db.search(User.groups.all(['admin', 'user']))\n[{'name': 'user2', 'groups': ['admin', 'user']}]\n</code></pre> </li> <li> <p>Testing nested queries: Assuming we have the following table:</p> <pre><code>Group = Query()\nPermission = Query()\ngroups = db.table('groups')\ngroups.insert({\n    'name': 'user',\n    'permissions': [{'type': 'read'}]})\ngroups.insert({\n    'name': 'sudo',\n    'permissions': [{'type': 'read'}, {'type': 'sudo'}]})\ngroups.insert({\n    'name': 'admin',\n    'permissions': [{'type': 'read'}, {'type': 'write'}, {'type': 'sudo'}]})\n</code></pre> <p>You can search this table using nested <code>any</code>/<code>all</code> queries:</p> <pre><code># Group has a permission with type 'read'\n&gt;&gt;&gt; groups.search(Group.permissions.any(Permission.type == 'read'))\n[{'name': 'user', 'permissions': [{'type': 'read'}]},\n {'name': 'sudo', 'permissions': [{'type': 'read'}, {'type': 'sudo'}]},\n {'name': 'admin', 'permissions':\n        [{'type': 'read'}, {'type': 'write'}, {'type': 'sudo'}]}]\n\n# Group has ONLY permission 'read'\n&gt;&gt;&gt; groups.search(Group.permissions.all(Permission.type == 'read'))\n[{'name': 'user', 'permissions': [{'type': 'read'}]}]\n</code></pre> <p><code>any</code> tests if there is at least one document matching the query while <code>all</code> ensures all documents match the query.</p> <p>The opposite operation, checking if a list contains a single item, is also possible using <code>one_of</code>:</p> <pre><code>&gt;&gt;&gt; db.search(User.name.one_of(['jane', 'john']))\n</code></pre> </li> </ul>"}, {"location": "coding/python/tinydb/#query-modifiers", "title": "Query modifiers", "text": "<p>TinyDB allows you to use logical operations to change and combine queries</p> <ul> <li>Negate a query: <code>db.search(~ (User.name == 'John'))</code>.</li> <li>Logical <code>AND</code>: <code>db.search((User.name == 'John') &amp; (User.age &lt;= 30))</code>.</li> <li>Logical <code>OR</code>: <code>db.search((User.name == 'John') | (User.name == 'Bob'))</code>.</li> </ul>"}, {"location": "coding/python/tinydb/#inserting-more-than-one-document", "title": "Inserting more than one document", "text": "<p>In case you want to insert more than one document, you can use <code>db.insert_multiple(...)</code>:</p> <pre><code>&gt;&gt;&gt; db.insert_multiple([\n        {'name': 'John', 'age': 22},\n        {'name': 'John', 'age': 37}])\n&gt;&gt;&gt; db.insert_multiple({'int': 1, 'value': i} for i in range(2))\n</code></pre>"}, {"location": "coding/python/tinydb/#updating-data", "title": "Updating data", "text": "<p>To update all the documents of the database, leave out the query argument:</p> <pre><code>db.update({'foo': 'bar'})\n</code></pre> <p>When you pass a dictionary to <code>db.update(fields, query)</code>, you update a document by adding or overwriting its values. TinyDB also supports some common operations you can do on your data:</p> <ul> <li><code>delete(key)</code>: Delete a key from the document.</li> <li><code>increment(key)</code>: Increment the value of a key.</li> <li><code>decrement(key)</code>: Decrement the value of a key.</li> <li><code>add(key, value)</code>: Add value to the value of a key (also works for strings).</li> <li><code>subtract(key, value)</code>: Subtract value from the value of a key.</li> <li><code>set(key, value)</code>: Set key to value.</li> </ul> <pre><code>&gt;&gt;&gt; from tinydb.operations import delete\n&gt;&gt;&gt; db.update(delete('key1'), User.name == 'John')\n</code></pre> <p>This will remove the key <code>key1</code> from all matching documents.</p> <p>You also can write your own operations:</p> <pre><code>&gt;&gt;&gt; def your_operation(your_arguments):\n...     def transform(doc):\n...         # do something with the document\n...         # ...\n...     return transform\n...\n&gt;&gt;&gt; db.update(your_operation(arguments), query)\n</code></pre>"}, {"location": "coding/python/tinydb/#retrieving-data", "title": "Retrieving data", "text": "<p>If you want to get one element use <code>db.get(...)</code>. Be warned, if more than one document match the query, a random will be returned.</p> <pre><code>&gt;&gt;&gt; db.get(User.name == 'John')\n{'name': 'John', 'age': 22}\n</code></pre> <p>If you want to know if the database stores a document, use <code>db.contains(...)</code>.</p> <pre><code>&gt;&gt;&gt; db.contains(User.name == 'John')\nTrue\n</code></pre> <p>If you want to know the number of documents that match a query use <code>db.count(...)</code>.</p> <pre><code>&gt;&gt;&gt; db.count(User.name == 'John')\n2\n</code></pre>"}, {"location": "coding/python/tinydb/#serializing-custom-data", "title": "Serializing custom data", "text": "<p>TinyDB has a limited support to serialize common objects, they added support for custom serializers but it's not yet documented. Check the tinydb-serialization package to see how to implement your own.</p>"}, {"location": "coding/python/tinydb/#serializing-datetime-objects", "title": "Serializing datetime objects", "text": "<p>The tinydb-serialization package gives serialization objects for datetime objects.</p> <pre><code>from tinydb import TinyDB\nfrom tinydb.storages import JSONStorage\nfrom tinydb_serialization import SerializationMiddleware\nfrom tinydb_serialization.serializers import DateTimeSerializer\n\nserialization = SerializationMiddleware(JSONStorage)\nserialization.register_serializer(DateTimeSerializer(), 'TinyDate')\n\ndb = TinyDB('db.json', storage=serialization)\n</code></pre>"}, {"location": "coding/python/tinydb/#tables", "title": "Tables", "text": "<p>TinyDB supports working with more than one table. To create and use a table, use <code>db.table(name)</code>. They behave as the TinyDB class.</p> <pre><code>&gt;&gt;&gt; table = db.table('table_name')\n&gt;&gt;&gt; table.insert({'value': True})\n&gt;&gt;&gt; table.all()\n[{'value': True}]\n&gt;&gt;&gt; for row in table:\n&gt;&gt;&gt;     print(row)\n{'value': True}\n</code></pre> <p>To remove a table from a database, use:</p> <pre><code>db.drop_table('table_name')\n</code></pre> <p>To remove all tables, use:</p> <pre><code>db.drop_tables()\n</code></pre> <p>To get a list with the names of all tables in your database:</p> <pre><code>&gt;&gt;&gt; db.tables()\n{'_default', 'table_name'}\n</code></pre>"}, {"location": "coding/python/tinydb/#query-caching", "title": "Query caching", "text": "<p>TinyDB caches query result for performance. That way re-running a query won't have to read the data from the storage as long as the database hasn't been modified. You can optimize the query cache size by passing the cache_size to the <code>table(...)</code> function:</p> <pre><code>table = db.table('table_name', cache_size=30)\n</code></pre> <p>You can set cache_size to <code>None</code> to make the cache unlimited in size. Also, you can set cache_size to <code>0</code> to disable it.</p>"}, {"location": "coding/python/tinydb/#storage-types", "title": "Storage types", "text": "<p>TinyDB comes with two storage types: JSON and in-memory. By default TinyDB stores its data in JSON files so you have to specify the path where to store it:</p> <pre><code>from tinydb import TinyDB, where\n\ndb = TinyDB('path/to/db.json')\n</code></pre> <p>To use the in-memory storage, use:</p> <pre><code>from tinydb.storages import MemoryStorage\n\ndb = TinyDB(storage=MemoryStorage)\n</code></pre> <p>All arguments except for the storage argument are forwarded to the underlying storage. For the JSON storage you can use this to pass additional keyword arguments to Python\u2019s <code>json.dump(\u2026)</code> method. For example, you can set it to create prettified JSON files like this:</p> <pre><code>&gt;&gt;&gt; db = TinyDB('db.json', sort_keys=True, indent=4, separators=(',', ': '))\n</code></pre>"}, {"location": "coding/python/tinydb/#references", "title": "References", "text": "<ul> <li>Docs</li> <li>Git</li> <li>Issues</li> <li>Reference</li> </ul>"}, {"location": "coding/python/type_hints/", "title": "Type hints", "text": "<p>Type hints are the Python native way to define the type of the objects in a program.</p> <p>Traditionally, the Python interpreter handles types in a flexible but implicit way. Recent versions of Python allow you to specify explicit type hints that different tools can use to help you develop your code more efficiently.</p> <p>TL;DR</p> <p>Use Type hints whenever unit tests are worth writing</p> <pre><code>def headline(text: str, align: bool = True) -&gt; str:\n    if align:\n        return f\"{text.title()}\\n{'-' * len(text)}\"\n    else:\n        return f\" {text.title()} \".center(50, \"o\")\n</code></pre> <p>Type hints are not enforced on their own by python. So you won't catch an error if you try to run <code>headline(\"use mypy\", align=\"center\")</code> unless you use a static type checker like Mypy.</p>"}, {"location": "coding/python/type_hints/#advantages-and-disadvantages", "title": "Advantages and disadvantages", "text": "<p>Advantages:</p> <ul> <li>Help catch certain errors if used with a static type checker.</li> <li>Help check your code. It's not trivial to use docstrings to do     automatic checks.</li> <li>Help to reason about code: Knowing the parameters type makes it a lot easier     to understand and maintain a code base. It can speed up the time required to     catch up with a code snippet. Always remember that you read code a lot more     often than you write it, so you should optimize for ease of reading.</li> <li> <p>Help you build and maintain a cleaner architecture. The act of writing type     hints force you to think about the types in your program.</p> </li> <li> <p>Helps refactoring: Type hints make it trivial to find where a given class is     used when you're trying to refactor your code base.</p> </li> <li>Improve IDEs and linters.</li> </ul> <p>Cons:</p> <ul> <li>Type hints take developer time and effort to add. Even though it probably     pays off in spending less time debugging, you will spend more time entering     code.</li> <li>Introduce a slight penalty in start-up time. If you need to use the typing     module, the import time may be significant, even more in short scripts.</li> <li>Work best in modern Pythons.</li> </ul> <p>Follow these guidelines when deciding if you want to add types to your project:</p> <ul> <li>In libraries that will be used by others, they add a lot of value.</li> <li>In complex projects, type hints help you understand how types flow through    your code and are highly recommended.</li> <li>If you are beginning to learn Python, don't use them yet.</li> <li>If you are writing throw-away scripts, don't use them.</li> </ul> <p>So, Use Type hints whenever unit tests are worth writing.</p>"}, {"location": "coding/python/type_hints/#usage", "title": "Usage", "text": ""}, {"location": "coding/python/type_hints/#function-annotations", "title": "Function annotations", "text": "<p><pre><code>def func(arg: arg_type, optarg: arg_type = default) -&gt; return_type:\n    ...\n</code></pre> For arguments the syntax is <code>argument: annotation</code>, while the return type is annotated using <code>-&gt; annotation</code>. Note that the annotation must be a valid Python expression.</p> <p>When running the code, the special <code>.__annotations__</code> attribute on the function  stores the typing information.</p>"}, {"location": "coding/python/type_hints/#variable-annotations", "title": "Variable annotations", "text": "<p>Sometimes the type checker needs help in figuring out the types of variables as well. The syntax is similar:</p> <pre><code>pi: float = 3.142\n\ndef circumference(radius: float) -&gt; float:\n    return 2 * pi * radius\n</code></pre>"}, {"location": "coding/python/type_hints/#composite-types", "title": "Composite types", "text": "<p>If you need to hint other types than <code>str</code>, <code>float</code> and <code>bool</code>, you'll need to import the <code>typing</code> module.</p> <p>For example to define the hint types of list, dictionaries and tuples:</p> <pre><code>&gt;&gt;&gt; from typing import Dict, List, Tuple\n\n&gt;&gt;&gt; names: List[str] = [\"Guido\", \"Jukka\", \"Ivan\"]\n&gt;&gt;&gt; version: Tuple[int, int, int] = (3, 7, 1)\n&gt;&gt;&gt; options: Dict[str, bool] = {\"centered\": False, \"capitalize\": True}\n</code></pre> <p>If your function expects some kind of sequence but don't care whether it's a list or a tuple, use the <code>typing.Sequence</code> object. In fact, try to use <code>Sequence</code> if you can because using <code>List</code> could lead to some unexpected errors when combined with type inference. For example:</p> <pre><code>class A: ...\nclass B(A): ...\n\nlst = [A(), A()]  # Inferred type is List[A]\nnew_lst = [B(), B()]  # inferred type is List[B]\nlst = new_lst  # mypy will complain about this, because List is invariant\n</code></pre> <p>Possible strategies in such situations are:</p> <ul> <li> <p>Use an explicit type annotation:</p> <pre><code>new_lst: List[A] = [B(), B()]\nlst = new_lst  # OK\n</code></pre> </li> <li> <p>Make a copy of the right hand side:</p> <pre><code>lst = list(new_lst) # Also OK\n</code></pre> </li> <li> <p>Use immutable collections as annotations whenever possible:</p> <pre><code>def f_bad(x: List[A]) -&gt; A:\n    return x[0]\nf_bad(new_lst) # Fails\n\ndef f_good(x: Sequence[A]) -&gt; A:\n    return x[0]\nf_good(new_lst) # OK\n</code></pre> </li> </ul>"}, {"location": "coding/python/type_hints/#dictionaries-with-different-value-types-per-key", "title": "Dictionaries with different value types per key.", "text": "<p><code>TypedDict</code> declares a dictionary type that expects all of its instances to have a certain set of keys, where each key is associated with a value of a consistent type. This expectation is not checked at runtime but is only enforced by type checkers.</p> <p>TypedDict started life as an experimental Mypy feature to wrangle typing onto the heterogeneous, structure-oriented use of dictionaries. As of Python 3.8, it was adopted into the standard library.</p> <pre><code>try:\n    from typing import TypedDict  # &gt;=3.8\nexcept ImportError:\n    from mypy_extensions import TypedDict  # &lt;=3.7\n\nMovie = TypedDict('Movie', {'name': str, 'year': int})\n</code></pre> <p>A class-based type constructor is also available:</p> <pre><code>class Movie(TypedDict):\n    name: str\n    year: int\n</code></pre> <p>By default, all keys must be present in a <code>TypedDict</code>. It is possible to override this by specifying totality. Usage:</p> <pre><code>class point2D(TypedDict, total=False):\n    x: int\n    y: int\n</code></pre> <p>This means that a <code>point2D</code> <code>TypedDict</code> can have any of the keys omitted. A type checker is only expected to support a literal False or True as the value of the total argument. True is the default, and makes all items defined in the class body be required.</p>"}, {"location": "coding/python/type_hints/#functions-without-return-values", "title": "Functions without return values", "text": "<p>Some functions aren't meant to return anything. Use the <code>-&gt; None</code> hint in these cases.</p> <pre><code>def play(player_name: str) -&gt; None:\n\n    print(f\"{player_name} plays\")\n\n\nret_val = play(\"Filip\")\n</code></pre> <p>The annotation help catch the kinds of subtle bugs where you are trying to use a meaningless return value.</p> <p>If your function doesn't return any object, use the <code>NoReturn</code> type.</p> <pre><code>from typing import NoReturn\n\ndef black_hole() -&gt; NoReturn:\n    raise Exception(\"There is no going back ...\")\n</code></pre> <p>Note</p> <p>This is the first iteration of the synoptical reading of the full Real python article on type checking.</p>"}, {"location": "coding/python/type_hints/#optional-arguments", "title": "Optional arguments", "text": "<p>A common pattern is to use <code>None</code> as a default value for an argument. This is done either to avoid problems with mutable default values or to have a sentinel value flagging special behavior.</p> <p>This creates a challenge for type hinting as the argument may be of type string (for example) but it can also be <code>None</code>. We use the <code>Optional</code> type to address this case.</p> <pre><code>from typing import Optional\n\ndef player(name: str, start: Optional[str] = None) -&gt; str:\n    ...\n</code></pre> <p>A similar way would be to use <code>Union[None, str]</code>.</p>"}, {"location": "coding/python/type_hints/#type-aliases", "title": "Type aliases", "text": "<p>Type hints might become oblique when working with nested types. If it's the case, save them into a new variable, and use that instead.</p> <pre><code>from typing import List, Tuple\n\nCard = Tuple[str, str]\nDeck = List[Card]\n\ndef deal_hands(deck: Deck) -&gt; Tuple[Deck, Deck, Deck, Deck]:\n\n\"\"\"Deal the cards in the deck into four hands\"\"\"\n\n    return (deck[0::4], deck[1::4], deck[2::4], deck[3::4])\n</code></pre>"}, {"location": "coding/python/type_hints/#allow-any-subclass", "title": "Allow any subclass", "text": "<p>Every class is also a valid type. Any instance of a subclass is also compatible with all superclasses \u2013 it follows that every value is compatible with the object type (and incidentally also the Any type, discussed below). Mypy analyzes the bodies of classes to determine which methods and attributes are available in instances. For example</p> <pre><code>class A:\n    def f(self) -&gt; int:  # Type of self inferred (A)\n        return 2\n\nclass B(A):\n    def f(self) -&gt; int:\n         return 3\n    def g(self) -&gt; int:\n        return 4\n\ndef foo(a: A) -&gt; None:\n    print(a.f())  # 3\n    a.g()         # Error: \"A\" has no attribute \"g\"\n\nfoo(B())  # OK (B is a subclass of A)\n</code></pre>"}, {"location": "coding/python/type_hints/#deduce-returned-value-type-from-the-arguments", "title": "Deduce returned value type from the arguments", "text": "<p>The previous approach works if you don't need to use class objects that inherit from a given class. For example:</p> <pre><code>class User:\n    # Defines fields like name, email\n\nclass BasicUser(User):\n    def upgrade(self):\n\"\"\"Upgrade to Pro\"\"\"\n\nclass ProUser(User):\n    def pay(self):\n\"\"\"Pay bill\"\"\"\n\ndef new_user(user_class) -&gt; User:\n    user = user_class()\n    # (Here we could write the user object to a database)\n    return user\n</code></pre> <p>Where:</p> <ul> <li><code>ProUser</code> doesn't inherit from <code>BasicUser</code>.</li> <li><code>new_user</code> creates an instance of one of these classes if you pass     it the right class object.</li> </ul> <p>The problem is that right now mypy doesn't know which subclass of <code>User</code> you're giving it, and will only accept the methods and attributes defined in the parent class <code>User</code>.</p> <pre><code>buyer = new_user(ProUser)\nbuyer.pay()  # Rejected, not a method on User\n</code></pre> <p>This can be solved using Type variables with upper bounds.</p> <pre><code>UserT = TypeVar('UserT', bound=User)\n\ndef new_user(user_class: Type[UserT]) -&gt; UserT:\n    # Same  implementation as before\n</code></pre> <p>We're creating a new type <code>UserT</code> that is linked to the class or subclasses of <code>User</code>. That way, mypy knows that the return value is an object created from the class given in the argument <code>user_class</code>.</p> <pre><code>beginner = new_user(BasicUser)  # Inferred type is BasicUser\nbeginner.upgrade()  # OK\n</code></pre> <p>Note<p>\"Using <code>UserType</code> is not supported by pylint, use <code>UserT</code> instead.\"</p> </p> <p>Keep in mind that the <code>TypeVar</code> is a Generic type, as such, they take one or more type parameters, similar to built-in types such as <code>List[X]</code>.</p> <p>That means that when you create type aliases, you'll need to give the type parameter. So:</p> <pre><code>UserT = TypeVar(\"UserT\", bound=User)\nUserTs = List[Type[UserT]]\n\n\ndef new_users(user_class: UserTs) -&gt; UserT: # Type error!\n    pass\n</code></pre> <p>Will give a <code>Missing type parameters for generic type \"UserTs\"</code> error. To solve it use:</p> <pre><code>def new_users(user_class: UserTs[UserT]) -&gt; UserT: # OK!\n    pass\n</code></pre>"}, {"location": "coding/python/type_hints/#define-a-typevar-with-restrictions", "title": "Define a TypeVar with restrictions", "text": "<p>By default, a type variable can be replaced with any type. However, sometimes it\u2019s useful to have a type variable that can only have some specific types as its value. A typical example is a type variable that can only have values <code>str</code> and <code>bytes</code>:</p> <pre><code>from typing import TypeVar\n\nAnyStr = TypeVar('AnyStr', str, bytes)\n</code></pre> <p>This is actually such a common type variable that <code>AnyStr</code> is defined in typing and we don\u2019t need to define it ourselves.</p> <p>We can use <code>AnyStr</code> to define a function that can concatenate two strings or bytes objects, but it can\u2019t be called with other argument types:</p> <pre><code>from typing import AnyStr\n\ndef concat(x: AnyStr, y: AnyStr) -&gt; AnyStr:\n    return x + y\n\nconcat('a', 'b')    # Okay\nconcat(b'a', b'b')  # Okay\nconcat(1, 2)        # Error!\n</code></pre> <p>Note that this is different from a union type, since combinations of <code>str</code> and <code>bytes</code> are not accepted:</p> <pre><code>concat('string', b'bytes')   # Error!\n</code></pre> <p>In this case, this is exactly what we want, since it\u2019s not possible to concatenate a string and a bytes object! The type checker will reject this function:</p> <pre><code>def union_concat(x: Union[str, bytes], y: Union[str, bytes]) -&gt; Union[str, bytes]:\n    return x + y  # Error: can't concatenate str and bytes\n</code></pre>"}, {"location": "coding/python/type_hints/#overloading-the-methods", "title": "Overloading the methods", "text": "<p>Sometimes the types of several variables are related, such as \u201cif x is type A, y is type B, else y is type C\u201d. Basic type hints cannot describe such relationships, making type checking cumbersome or inaccurate. We can instead use <code>@typing.overload</code> to represent type relationships properly.</p> <pre><code>from __future__ import annotations\n\nfrom collections.abc import Sequence\nfrom typing import overload\n\n\n@overload\ndef double(input_: int) -&gt; int:\n    ...\n\n\n@overload\ndef double(input_: Sequence[int]) -&gt; list[int]:\n    ...\n\n\ndef double(input_: int | Sequence[int]) -&gt; int | list[int]:\n    if isinstance(input_, Sequence):\n        return [i * 2 for i in input_]\n    return input_ * 2\n</code></pre> <p>This looks a bit weird at first glance\u2014we are defining double three times! Let\u2019s take it apart.</p> <p>The first two <code>@overload</code> definitions exist only for their type hints. Each definition represents an allowed combination of types. These definitions never run, so their bodies could contain anything, but it\u2019s idiomatic to use Python\u2019s <code>...</code> (ellipsis) literal.</p> <p>The third definition is the actual implementation. In this case, we need to provide type hints that union all the possible types for each variable. Without such hints, Mypy will skip type checking the function body.</p> <p>When Mypy checks the file, it collects the <code>@overload</code> definitions as type hints. It then uses the first non-<code>@overload</code> definition as the implementation. All <code>@overload</code> definitions must come before the implementation, and multiple implementations are not allowed.</p> <p>When Python imports the file, the <code>@overload</code> definitions create temporary double functions, but each is overridden by the next definition. After importing, only the implementation exists. As a protection against accidentally missing implementations, attempting to call an <code>@overload</code> definition will raise a <code>NotImplementedError</code>.</p> <p><code>@overload</code> can represent arbitrarily complex scenarios. For a couple more examples, see the function overloading section of the Mypy docs.</p>"}, {"location": "coding/python/type_hints/#use-a-constrained-typevar-in-the-definition-of-a-class-attributes", "title": "Use a constrained TypeVar in the definition of a class attributes.", "text": "<p>If you try to use a <code>TypeVar</code> in the definition of a class attribute:</p> <pre><code>class File:\n\"\"\"Model a computer file.\"\"\"\n\n    path: str\n    content: Optional[AnyStr] = None # mypy error!\n</code></pre> <p>mypy will complain with <code>Type variable AnyStr is unbound [valid-type]</code>, to solve it, you need to make the class inherit from the <code>Generic[AnyStr]</code>.</p> <pre><code>class File(Generic[AnyStr]):\n\"\"\"Model a computer file.\"\"\"\n\n    path: str\n    content: Optional[AnyStr] = None\n</code></pre> <p>Why you ask? I have absolutely no clue. I've asked that question in the gitter python typing channel but the kind answer that @ktbarrett gave me sounded like Chinese.</p> <p>You can't just use a type variable for attributes or variables, you have to create some generic context, whether that be a function or a class, so that you can instantiate the generic context (or the analyzer can infer it) (i.e. context[var]). That's not possible if you don't specify that the class is a generic context. It also ensure than all uses of that variable in the context resolve to the same type.</p> <p>If you don't mind helping me understand it, please contact me.</p>"}, {"location": "coding/python/type_hints/#specify-the-type-of-the-class-in-its-method-and-attributes", "title": "Specify the type of the class in it's method and attributes", "text": "<p>If you are using Python 3.10 or later, it just works. Python 3.7 introduces PEP 563: postponed evaluation of annotations. A module that uses the future statement <code>from __future__ import annotations</code> to store annotations as strings automatically:</p> <pre><code>from __future__ import annotations\n\nclass Position:\n    def __add__(self, other: Position) -&gt; Position:\n        ...\n</code></pre> <p>But <code>pyflakes</code> will still complain, so I've used strings.</p> <pre><code>from __future__ import annotations\n\nclass Position:\n    def __add__(self, other: 'Position') -&gt; 'Position':\n        ...\n</code></pre>"}, {"location": "coding/python/type_hints/#type-hints-of-generators", "title": "Type hints of Generators", "text": "<pre><code>from typing import Generator\n\ndef generate() -&gt; Generator[int, None, None]:\n</code></pre> <p>Where the first argument of <code>Generator</code> is the type of the yielded value.</p>"}, {"location": "coding/python/type_hints/#usage-of-ellipsis-on-tuple-type-hints", "title": "Usage of ellipsis on <code>Tuple</code> type hints", "text": "<p>The ellipsis is used to specify an arbitrary-length homogeneous tuples, for example <code>Tuple[int, ...]</code>.</p>"}, {"location": "coding/python/type_hints/#using-typingcast", "title": "Using <code>typing.cast</code>", "text": "<p>Sometimes the type hints of your program don't work as you expect, if you've given up on fixing the issue you can <code># type: ignore</code> it, but if you know what type you want to enforce, you can use <code>typing.cast()</code> explicitly or implicitly from <code>Any</code> with type hints. With casting we can force the type checker to treat a variable as a given type.</p> <p>This is an ugly patch, always try to fix your types</p>"}, {"location": "coding/python/type_hints/#the-simplest-cast", "title": "The simplest <code>cast()</code>", "text": "<p>When we call <code>cast()</code>, we pass it two arguments: a type, and a value. <code>cast()</code> returns <code>value</code> unchanged, but type checkers will treat the return value as the given type instead of the input type. For example, we can make Mypy treat an integer as a string:</p> <pre><code>from typing import cast\n\nx = 1\nreveal_type(x)\ny = cast(str, x)\nreveal_type(y)\ny.upper()\n</code></pre> <p>Checking this program with Mypy, it doesn't report any errors, but it does debug the types of <code>x</code> and <code>y</code> for us:</p> <pre><code>$ mypy example.py\n\nexample.py:6: note: Revealed type is \"builtins.int\"\nexample.py:8: note: Revealed type is \"builtins.str\"\n</code></pre> <p>But, if we remove the <code>reveal_type()</code> calls and run the code, it crashes:</p> <pre><code>$ python example.py\nTraceback (most recent call last):\n  File \"/.../example.py\", line 7, in &lt;module&gt;\n    y.upper()\nAttributeError: 'int' object has no attribute 'upper'\n</code></pre> <p>Usually Mypy would detect this bug, as it knows <code>int</code> objects do not have an <code>upper()</code> method. But our <code>cast()</code> forced Mypy to treat <code>y</code> as a <code>str</code>, so it assumed the call would succeed.</p>"}, {"location": "coding/python/type_hints/#use-cases", "title": "Use cases", "text": "<p>The main case to reach for <code>cast()</code> are when the type hints for a module are either missing, incomplete, or incorrect. This may be the case for third party packages, or occasionally for things in the standard library.</p> <p>Take this example:</p> <pre><code>import datetime as dt\nfrom typing import cast\n\nfrom third_party import get_data\n\ndata = get_data()\nlast_import_time = cast(dt.datetime, data[\"last_import_time\"])\n</code></pre> <p>Imagine <code>get_data()</code> has a return type of <code>dict[str, Any]</code>, rather than using stricter per-key types with a <code>TypedDict</code>. From reading the documentation or source we might find that the <code>last_import_time</code> key always contains a <code>datetime</code> object. Therefore, when we access it, we can wrap it in a <code>cast()</code>, to tell our type checker the real type rather than continuing with <code>Any</code>.</p> <p>When we encounter missing, incomplete, or incorrect type hints, we can contribute back a fix. This may be in the package itself, its related stubs package, or separate stubs in Python\u2019s typeshed. But until such a fix is released, we will need to use <code>cast()</code> to make our code pass type checking.</p>"}, {"location": "coding/python/type_hints/#implicit-casting-from-any", "title": "Implicit Casting From Any", "text": "<p>It\u2019s worth noting that <code>Any</code> has special treatment: when we store a variable with type <code>Any</code> in a variable with a specific type, type checkers treat this as an implicit cast. We can thus write our previous example without <code>cast()</code>:</p> <pre><code>import datetime as dt\n\nfrom third_party import get_data\n\ndata = get_data()\nlast_import_time: dt.datetime = data[\"last_import_time\"]\n</code></pre> <p>This kind of implicit casting is the first tool we should reach for when interacting with libraries that return <code>Any</code>. It also applies when we pass a variable typed <code>Any</code> as a specifically typed function argument or return value.</p> <p>Calling <code>cast()</code> directly is often more useful when dealing with incorrect types other than <code>Any</code>.</p>"}, {"location": "coding/python/type_hints/#mypys-warn_redundant_casts-option", "title": "Mypy\u2019s <code>warn_redundant_casts</code> option", "text": "<p>When we use <code>cast()</code> to override a third party function\u2019s type, that type be corrected in a later version (perhaps from our own PR!). After such an update, the <code>cast()</code> is unnecessary clutter that may confuse readers.</p> <p>We can detect such unnecessary casts by activating Mypy\u2019s <code>warn_redundant_casts</code> option. With this flag turned on, Mypy will log an error for each use of <code>cast()</code> that casts a variable to the type it already has.</p>"}, {"location": "coding/python/type_hints/#using-mypy-with-an-existing-codebase", "title": "Using mypy with an existing codebase", "text": "<p>These steps will get you started with <code>mypy</code> on an existing codebase:</p> <ul> <li> <p>Start small:     Pick a subset of your codebase to run mypy on, without     any annotations.</p> <p>You\u2019ll probably need to fix some mypy errors, either by inserting annotations requested by mypy or by adding <code># type: ignore</code> comments to silence errors you don\u2019t want to fix now.</p> <p>Get a clean mypy build for some files, with some annotations. * Write a mypy runner script to ensure consistent results. Here are some steps you may want to do in the script: * Ensure that you install the correct version of mypy. * Specify mypy config file or command-line options. * Provide set of files to type check. You may want to configure the inclusion     and exclusion filters for full control of the file list. * Run mypy in Continuous Integration to prevent type errors:</p> <p>Once you have a clean mypy run and a runner script for a part of your codebase, set up your Continuous Integration (CI) system to run mypy to ensure that developers won\u2019t introduce bad annotations. A small CI script could look something like this:</p> <p><pre><code>python3 -m pip install mypy==0.600  # Pinned version avoids surprises\nscripts/mypy  # Runs with the correct options\n</code></pre> * Gradually annotate commonly imported modules: Most projects have some widely imported modules, such as utilities or model classes. It\u2019s a good idea to annotate these soon, since this allows code using these modules to be type checked more effectively. Since mypy supports gradual typing, it\u2019s okay to leave some of these modules unannotated. The more you annotate, the more useful mypy will be, but even a little annotation coverage is useful. * Write annotations as you change existing code and write new code: Now you are ready to include type annotations in your development workflows. Consider adding something like these in your code style conventions:</p> <ul> <li>Developers should add annotations for any new code.</li> <li>It\u2019s also encouraged to write annotations when you change existing code.</li> </ul> </li> </ul> <p>If you need to ignore a linter error and a type error use first the type and then the linter. For example, <code># type: ignore # noqa: W0212</code>.</p>"}, {"location": "coding/python/type_hints/#reveal-the-type-of-an-expression", "title": "Reveal the type of an expression", "text": "<p>You can use <code>reveal_type(expr)</code> to ask mypy to display the inferred static type of an expression. This can be useful when you don't quite understand how mypy handles a particular piece of code. Example:</p> <pre><code>reveal_type((1, 'hello'))  # Revealed type is 'Tuple[builtins.int, builtins.str]'\n</code></pre> <p>You can also use <code>reveal_locals()</code> at any line in a file to see the types of all local variables at once. Example:</p> <pre><code>a = 1\nb = 'one'\nreveal_locals()\n# Revealed local types are:\n#     a: builtins.int\n#     b: builtins.str\n</code></pre> <p><code>reveal_type</code> and <code>reveal_locals</code> are only understood by mypy and don't exist in Python. If you try to run your program, you\u2019ll have to remove any <code>reveal_type</code> and <code>reveal_locals</code> calls before you can run your code. Both are always available and you don't need to import them.</p>"}, {"location": "coding/python/type_hints/#solve-cyclic-imports-due-to-typing", "title": "Solve cyclic imports due to typing", "text": "<p>You can use a conditional import that is only active in \"type hinting mode\", but doesn't interfere at run time. The <code>typing.TYPE_CHECKING</code> constant makes this easily possible. For example:</p> <pre><code># thing.py\nfrom typing import TYPE_CHECKING\nif TYPE_CHECKING:\n    from connection import ApiConnection\n\nclass Thing:\n    def __init__(self, connection: 'ApiConnection'):\n        self._conn = connection\n</code></pre> <p>The code will now execute properly as there is no circular import issue anymore. Type hinting tools on the other hand should still be able to resolve the <code>ApiConnection</code> type hint in <code>Thing.__init__</code>.</p>"}, {"location": "coding/python/type_hints/#make-your-library-compatible-with-mypy", "title": "Make your library compatible with mypy", "text": "<p>PEP 561 notes three main ways to distribute type information. The first is a package that has only inline type annotations in the code itself. The second is a package that ships stub files with type information alongside the runtime code. The third method, also known as a \u201cstub only package\u201d is a package that ships type information for a package separately as stub files.</p> <p>If you would like to publish a library package to a package repository (e.g. PyPI) for either internal or external use in type checking, packages that supply type information via type comments or annotations in the code should put a <code>py.typed</code> file in their package directory. For example, with a directory structure as follows</p> <pre><code>setup.py\npackage_a/\n    __init__.py\n    lib.py\n    py.typed\n</code></pre> <p>the <code>setup.py</code> might look like:</p> <pre><code>from distutils.core import setup\n\nsetup(\n    name=\"SuperPackageA\",\n    author=\"Me\",\n    version=\"0.1\",\n    package_data={\"package_a\": [\"py.typed\"]},\n    packages=[\"package_a\"]\n)\n</code></pre> <p>If you use setuptools, you must pass the option <code>zip_safe=False</code> to <code>setup()</code>, or mypy will not be able to find the installed package.</p>"}, {"location": "coding/python/type_hints/#reference", "title": "Reference", "text": "<ul> <li>Bernat gabor article on the state of type hints in python</li> <li>Real python article on type checking</li> </ul>"}, {"location": "coding/python/yoyo/", "title": "Yoyo migrations", "text": "<p>Yoyo is a database schema migration tool. Migrations are written as SQL files or Python scripts that define a list of migration steps.</p>"}, {"location": "coding/python/yoyo/#installation", "title": "Installation", "text": "<pre><code>pip install yoyo-migrations\n</code></pre>"}, {"location": "coding/python/yoyo/#usage", "title": "Usage", "text": ""}, {"location": "coding/python/yoyo/#command-line", "title": "Command line", "text": "<p>Start a new migration:</p> <pre><code>yoyo new ./migrations -m \"Add column to foo\"\n</code></pre> <p>Apply migrations from directory migrations to a PostgreSQL database:</p> <pre><code>yoyo apply --database postgresql://scott:tiger@localhost/db ./migrations\n</code></pre> <p>Rollback migrations previously applied to a MySQL database:</p> <pre><code>yoyo rollback --database mysql://scott:tiger@localhost/database ./migrations\n</code></pre> <p>Reapply (ie rollback then apply again) migrations to a SQLite database at location /home/sheila/important.db:</p> <pre><code>yoyo reapply --database sqlite:////home/sheila/important.db ./migrations\n</code></pre> <p>List available migrations:</p> <pre><code>yoyo list --database sqlite:////home/sheila/important.db ./migrations\n</code></pre> <p>By default, yoyo-migrations starts in an interactive mode, prompting you for each migration file before applying it, making it easy to preview which migrations to apply and rollback.</p>"}, {"location": "coding/python/yoyo/#connecting-to-the-database", "title": "Connecting to the database", "text": "<p>Database connections are specified using a URL. Examples:</p> <pre><code># SQLite: use 4 slashes for an absolute database path on unix like platforms\ndatabase = sqlite:////home/user/mydb.sqlite\n\n# SQLite: use 3 slashes for a relative path\ndatabase = sqlite:///mydb.sqlite\n\n# SQLite: absolute path on Windows.\ndatabase = sqlite:///c:\\home\\user\\mydb.sqlite\n\n# MySQL: Network database connection\ndatabase = mysql://scott:tiger@localhost/mydatabase\n\n# MySQL: unix socket connection\ndatabase = mysql://scott:tiger@/mydatabase?unix_socket=/tmp/mysql.sock\n\n# MySQL with the MySQLdb driver (instead of pymysql)\ndatabase = mysql+mysqldb://scott:tiger@localhost/mydatabase\n\n# MySQL with SSL/TLS enabled\ndatabase = mysql+mysqldb://scott:tiger@localhost/mydatabase?ssl=yes&amp;sslca=/path/to/cert\n\n# PostgreSQL: database connection\ndatabase = postgresql://scott:tiger@localhost/mydatabase\n\n# PostgreSQL: unix socket connection\ndatabase = postgresql://scott:tiger@/mydatabase\n\n# PostgreSQL: changing the schema (via set search_path)\ndatabase = postgresql://scott:tiger@/mydatabase?schema=some_schema\n</code></pre> <p>You can specify your database username and password either as part of the database connection string on the command line (exposing your database password in the process list) or in a configuration file where other users may be able to read it.</p> <p>The <code>-p</code> or <code>--prompt-password</code> flag causes yoyo to prompt for a password, helping prevent your credentials from being leaked.</p>"}, {"location": "coding/python/yoyo/#migration-files", "title": "Migration files", "text": "<p>The migrations directory contains a series of migration scripts. Each migration script is a Python (.py) or SQL file (.sql).</p> <p>The name of each file without the extension is used as the migration\u2019s unique identifier.</p> <p>Migrations scripts are run in dependency then filename order.</p> <p>Each migration file is run in a single transaction where this is supported by the database.</p> <p>Yoyo creates tables in your target database to track which migrations have been applied.</p>"}, {"location": "coding/python/yoyo/#migrations-as-python-scripts", "title": "Migrations as Python scripts", "text": "<p>A migration script written in Python has the following structure:</p> <pre><code>#\n# file: migrations/0001_create_foo.py\n#\nfrom yoyo import step\n\n__depends__ = {\"0000.initial-schema\"}\n\nsteps = [\n  step(\n      \"CREATE TABLE foo (id INT, bar VARCHAR(20), PRIMARY KEY (id))\",\n      \"DROP TABLE foo\",\n  ),\n  step(\n      \"ALTER TABLE foo ADD COLUMN baz INT NOT NULL\"\n  )\n]\n</code></pre> <p>The step function may take up to 3 arguments:</p> <ul> <li><code>apply</code>: an SQL query (or Python function, see below) to apply the migration     step.</li> <li><code>rollback</code>: (optional) an SQL query (or Python function) to rollback the     migration step.</li> <li><code>ignore_errors</code>: (optional, one of <code>apply</code>, <code>rollback</code> or <code>all</code>) causes yoyo     to ignore database errors in either the apply stage, rollback stage or     both.</li> </ul> <p>Migrations may declare dependencies on other migrations via the <code>__depends__</code> attribute:</p> <p>If you use the <code>yoyo new</code> command the <code>__depends__</code> attribute will be auto populated for you.</p>"}, {"location": "coding/python/yoyo/#migration-steps-as-python-functions", "title": "Migration steps as Python functions", "text": "<p>If SQL is not flexible enough, you may supply a Python function as either or both of the apply or rollback arguments of step. Each function should take a database connection as its only argument:</p> <pre><code>#\n# file: migrations/0001_create_foo.py\n#\nfrom yoyo import step\n\ndef apply_step(conn):\n    cursor = conn.cursor()\n    cursor.execute(\n        # query to perform the migration\n    )\n\ndef rollback_step(conn):\n    cursor = conn.cursor()\n    cursor.execute(\n        # query to undo the above\n    )\n\nsteps = [\n  step(apply_step, rollback_step)\n]\n</code></pre>"}, {"location": "coding/python/yoyo/#post-apply-hook", "title": "Post-apply hook", "text": "<p>It can be useful to have a script that is run after every successful migration. For example you could use this to update database permissions or re-create views.</p> <p>To do this, create a special migration file called <code>post-apply.py</code>.</p>"}, {"location": "coding/python/yoyo/#configuration-file", "title": "Configuration file", "text": "<p>Yoyo looks for a configuration file named <code>yoyo.ini</code> in the current working directory or any ancestor directory.</p> <p>The configuration file may contain the following options:</p> <pre><code>[DEFAULT]\n\n# List of migration source directories. \"%(here)s\" is expanded to the\n# full path of the directory containing this ini file.\nsources = %(here)s/migrations %(here)s/lib/module/migrations\n\n# Target database\ndatabase = postgresql://scott:tiger@localhost/mydb\n\n# Verbosity level. Goes from 0 (least verbose) to 3 (most verbose)\nverbosity = 3\n\n# Disable interactive features\nbatch_mode = on\n\n# Editor to use when starting new migrations\n# \"{}\" is expanded to the filename of the new migration\neditor = /usr/local/bin/vim -f {}\n\n# An arbitrary command to run after a migration has been created\n# \"{}\" is expanded to the filename of the new migration\npost_create_command = hg add {}\n\n# A prefix to use for generated migration filenames\nprefix = myproject_\n</code></pre>"}, {"location": "coding/python/yoyo/#calling-yoyo-from-python-code", "title": "Calling Yoyo from Python code", "text": "<p>The following example shows how to apply migrations from inside python code:</p> <pre><code>from yoyo import read_migrations\nfrom yoyo import get_backend\n\nbackend = get_backend('postgres://myuser@localhost/mydatabase')\nmigrations = read_migrations('path/to/migrations')\n\nwith backend.lock():\n\n    # Apply any outstanding migrations\n    backend.apply_migrations(backend.to_apply(migrations))\n\n    # Rollback all migrations\n    backend.rollback_migrations(backend.to_rollback(migrations))\n</code></pre>"}, {"location": "coding/python/yoyo/#references", "title": "References", "text": "<ul> <li>Docs</li> <li>Source</li> <li>Issue Tracker</li> <li>Mailing list</li> </ul>"}, {"location": "coding/python/python_project_template/python_cli_template/", "title": "Python CLI Project Template", "text": "<ul> <li> <p>Create the tests directories     <pre><code>mkdir -p tests/unit\ntouch tests/__init__.py\ntouch tests/unit/__init__.py\n</code></pre></p> </li> <li> <p>Create the program module structure     <pre><code>mkdir {{ program_name }}\n</code></pre></p> </li> <li> <p>Create the program <code>setup.py</code> file     <pre><code>from setuptools import find_packages, setup\n\n# Get the version from drode/version.py without importing the package\nexec(compile(open('{{ program_name }}/version.py').read(),\n             '{{ program_name }}/version.py', 'exec'))\n\nsetup(\n    name='{{ program_name }}',\n    version=__version__, # noqa: F821\n    description='{{ program_description }}',\n    author='{{ author }}',\n    author_email='{{ author_email }}',\n    license='GPLv3',\n    long_description=open('README.md').read(),\n    packages=find_packages(exclude=('tests',)),\n    package_data={'{{ program_name }}': [\n        'migrations/*',\n        'migrations/versions/*',\n    ]},\n    entry_points={'console_scripts': ['{{ program_name }} = {{ program_name }}:main']},\n    install_requires=[\n    ]\n)\n</code></pre>     Remember to fill up the <code>install_requirements</code> with the dependencies that     need to be installed at installation time.</p> </li> <li>Create the <code>{{ program_name }}/version.py</code> file with the following contents:     <pre><code>__version__ = '1.0.0'\n</code></pre></li> </ul> <p>This ugly way of loading the <code>__version__</code> was stolen from youtube-dl, it    loads and executes the <code>version.py</code> without loading the whole module.    Solutions like <code>from {{ program_name }}.version import __version__</code> will fail    as it tries to load the whole module. Defining it in the <code>setup.py</code> file    doesn't work either if you need to load the version in your program code.    <code>from setup.py import __version__</code> will also fail. The only problem with this    approach is that as the <code>__version__</code> is not defined in the code it will    raise a Flake8 error, therefore the <code>#    noqa: F821</code> in the <code>setup.py</code> code.</p> <ul> <li> <p>Create the <code>requirements.txt</code> file. It should contain the     <code>install_requirements</code> in addition to the testing requirements such as:     <pre><code>pytest\npytest-cov\n</code></pre></p> </li> <li> <p>Configure SQLAlchemy for projects without     flask</p> </li> </ul>"}, {"location": "coding/python/python_project_template/python_docker/", "title": "Configure Docker and Docker compose to host the application", "text": "<p>Docker is a popular way to distribute applications. Assuming that you've set all required dependencies in the <code>setup.py</code>, we're going to create an image with these properties:</p> <ul> <li>Run by an unprivileged user: Create an unprivileged user with permissions to     run our program.</li> <li>Robust to vulnerabilities: Don't use Alpine as it's known to react slow to     new vulnerabilities. Use a base of Debian instead.</li> <li>Smallest possible: Use Docker multi build step. Create a <code>builder</code> Docker that     will run <code>pip install</code> and copies the required executables to the     final image.</li> </ul> <pre><code>FROM python:3.8-slim-buster as base\n\nFROM base as builder\n\nRUN python -m venv /opt/venv\n# Make sure we use the virtualenv:\nENV PATH=\"/opt/venv/bin:$PATH\"\n\nCOPY . /app\nWORKDIR /app\nRUN pip install .\n\nFROM base\n\nCOPY --from=builder /opt/venv /opt/venv\n\nRUN useradd -m myapp\nWORKDIR /home/myapp\n\n# Copy the required directories for your program to work.\nCOPY --from=builder /root/.local/share/myapp /home/myapp/.local/share/myapp\nCOPY --from=builder /app/myapp /home/myapp/myapp\nRUN chown -R myapp:myapp /home/myapp/.local\n\nUSER myapp\nENV PATH=\"/opt/venv/bin:$PATH\"\nENTRYPOINT [\"/opt/venv/bin/myapp\"]\n</code></pre> <p>If we need to use it with MariaDB or with Redis, the easiest way is to use <code>docker-compose</code>.</p> <pre><code>version: '3.8'\n\nservices:\nmyapp:\nimage: myapp:latest\nrestart: always\nlinks:\n- db\ndepends_on:\n- db\nenvironment:\n- AIRSS_DATABASE_URL=mysql+pymysql://myapp:supersecurepassword@db/myapp\ndb:\nimage: mariadb:latest\nrestart: always\nenvironment:\n- MYSQL_USER=myapp\n- MYSQL_PASSWORD=supersecurepassword\n- MYSQL_DATABASE=myapp\n- MYSQL_ALLOW_EMPTY_PASSWORD=yes\nports:\n- 3306:3306\ncommand:\n- '--character-set-server=utf8mb4'\n- '--collation-server=utf8mb4_unicode_ci'\nvolumes:\n- /data/myapp/mariadb:/var/lib/mysql\n</code></pre> <p>The <code>depends_on</code> flag is not enough to ensure that the database is up when our application tries to connect. So we need to use external programs like wait-for-it. To use it, change the earlier Dockerfile to match these lines:</p> <pre><code>...\n\nFROM base\n\nRUN apt-get update &amp;&amp; apt-get install -y \\\nwait-for-it \\\n&amp;&amp; rm -rf /var/lib/apt/lists/*\n\n...\n\nENTRYPOINT [\"/home/myapp/entrypoint.sh\"]\n</code></pre> <p>Where <code>entrypoint.sh</code> is something like:</p> <pre><code>#!/bin/bash\n\n# Wait for the database to be up\nif [[ -n $DATABASE_URL ]];then\nwait-for-it db:3306\nfi\n\n# Execute database migrations\n/opt/venv/bin/myapp install\n\n# Enter in daemon mode\n/opt/venv/bin/myapp daemon\n</code></pre> <p>Remember to add the permissions to run the script:</p> <pre><code>chmod +x entrypoint.sh\n</code></pre>"}, {"location": "coding/python/python_project_template/python_docker/#troubleshooting", "title": "Troubleshooting", "text": ""}, {"location": "coding/python/python_project_template/python_docker/#docker-python-not-showing-prints", "title": "Docker python not showing prints", "text": "<p>Use <code>CMD [\"python\",\"-u\",\"main.py\"]</code> instead of <code>CMD [\"python\",\"main.py\"]</code>.</p>"}, {"location": "coding/python/python_project_template/python_docker/#prevent-pip-install-r-requirementstxt-to-run-on-each-docker-build", "title": "Prevent <code>pip install -r requirements.txt</code> to run on each <code>docker build</code>", "text": "<p>I'm assuming that at some point in your build process, you're copying your entire application into the Docker image with COPY or ADD:</p> <pre><code>COPY . /opt/app\nWORKDIR /opt/app\nRUN pip install -r requirements.txt\n</code></pre> <p>The problem is that you're invalidating the Docker build cache every time you're copying the entire application into the image. This will also invalidate the cache for all subsequent build steps.</p> <p>To prevent this, I'd suggest copying only the requirements.txt file in a separate build step before adding the entire application into the image:</p> <pre><code>COPY requirements.txt /opt/app/requirements.txt\nWORKDIR /opt/app\nRUN pip install -r requirements.txt\nCOPY . /opt/app\n# continue as before...\n</code></pre>"}, {"location": "coding/python/python_project_template/python_docs/", "title": "Python documentation", "text": "<p>It's important to create the documentation at the same time as you code your project, otherwise you won't ever do it or you'll die trying.</p> <p>Right now I use mkdocs with Github Pages for the documentation.</p> <p>Follow the steps under Installation to configure it.</p> <p>I've automated the creation of the mkdocs site in this cookiecutter template.</p>"}, {"location": "coding/python/python_project_template/python_flask_template/", "title": "Flask project template", "text": "<p>Flask is very flexible when it comes to define the project layout, as a result, there are several different approaches, which can be confusing if you're building your first application.</p> <p>Follow this template if you want an application that meets these requirements:</p> <ul> <li>Use SQLAlchemy as ORM.</li> <li>Use pytest as testing framework (instead of unittest).</li> <li>Sets a robust foundation for application growth.</li> <li>Set a clear defined project structure that can be used for frontend     applications as well as backend APIs.</li> <li>Microservice friendly.</li> </ul> <p>I've crafted this template after studying the following projects:</p> <ul> <li>Miguel's Flask mega     tutorial     (code).</li> <li>Greb Obinna Flask-RESTPlus     tutorial     (code).</li> <li>Abhinav Suri Flask     tutorial     (code).</li> <li>Patrick's software blog     project layout     and pytest     definition     (code).</li> <li>Jaime Buelta Hands On Docker for Microservices with     Python     (code).</li> </ul> <p>Each has it's strengths and weaknesses:</p> Project Alembic Pytest Complex app Friendly layout Strong points Miguel True False True False Has a book explaining the code Greb False False False False flask-restplus Abhinav True False True True flask-base Patrick False True False True pytest Jaime False True True False Microservices, CI, Kubernetes, logging, metrics <p>I'm going to start with Abhinav base layout as it's the most clear and complete. Furthermore, it's based in flask-base, a simple Flask boilerplate app with SQLAlchemy, Redis, User Authentication, and more. Which can be used directly to start a frontend flask project. I won't use it for a backend API though.</p> <p>With that base layout, I'm going to take Patrick's pytest layout to configure the tests using <code>pytest-flask</code>, Greb <code>flask-restplus</code> code to create the API and Miguel's book to glue everything together.</p> <p>Finally, I'll follow Jaime's book to merge the different microservices into an integrated project. As well as defining the deployment process, CI definition, logging, metrics and integration with Kubernetes.</p>"}, {"location": "coding/python/python_project_template/python_microservices_template/", "title": "Python microservices project", "text": "<p>Follow this template if you want to build a project that meets these requirements:</p> <ul> <li>Based on Python Flask microservices.</li> <li>Easily expandable.</li> <li>Tested by unit, functional and integration tests through continuous     integration pipelines.</li> <li>Deployed through uWSGI and Nginx dockers.</li> <li>Orchestrated through docker-compose or Kubernetes.</li> </ul> <p>Defining the project layout of a flask application is not easy, even less one with several</p>"}, {"location": "coding/python/python_project_template/python_sqlalchemy_mariadb/", "title": "Configure SQLAlchemy to use the MariaDB/Mysql backend", "text": "<p>I discourage you to use an ORM to manage the interactions with the database. Check the alternative solutions.</p> <p>To use Mysql you'll need to first install (or add to your requirements) <code>pymysql</code>:</p> <pre><code>pip install pymysql\n</code></pre> <p>The url to connect to the database will be: <pre><code>'mysql+pymysql://{}:{}@{}:{}/{}'.format(\n    DB_USER,\n    DB_PASS,\n    DB_HOST,\n    DB_PORT,\n    DATABASE\n)\n</code></pre></p> <p>It's probable that you'll need to use UTF8 with multi byte, otherwise the addition of some strings into the database will fail. I've tried adding it to the database url without success. So I've modified the MariaDB Docker-compose section to use that character and collation set:</p> <pre><code>services:\ndb:\nimage: mariadb:latest\nrestart: always\nenvironment:\n- MYSQL_USER=xxxx\n- MYSQL_PASSWORD=xxxx\n- MYSQL_DATABASE=xxxx\n- MYSQL_ALLOW_EMPTY_PASSWORD=yes\nports:\n- 3306:3306\ncommand:\n- '--character-set-server=utf8mb4'\n- '--collation-server=utf8mb4_unicode_ci'\n</code></pre>"}, {"location": "coding/python/python_project_template/python_sqlalchemy_without_flask/", "title": "Configure SQLAlchemy for projects without flask", "text": "<p>I discourage you to use an ORM to manage the interactions with the database. Check the alternative solutions.</p> <ul> <li> <p>Install Alembic:     <pre><code>pip install alembic\n</code></pre></p> </li> <li> <p>It's important that the migration scripts are saved with the rest of the source     code. Following Miguel Gringberg     suggestion,     we'll store them in the <code>{{ program_name }}/migrations</code> directory.</p> <p>Execute the following command to initialize the alembic repository.</p> <pre><code>alembic init {{ program_name }}/migrations\n</code></pre> </li> <li> <p>Create the basic <code>models.py</code> file under the project code.     <pre><code>\"\"\"\nModule to store the models.\n\nClasses:\n    Class_name: Class description.\n    ...\n\"\"\"\n\nimport os\n\nfrom sqlalchemy import \\\n    create_engine, \\\n    Column, \\\n    Integer\nfrom sqlalchemy.ext.declarative import declarative_base\n\ndb_path = os.path.expanduser('{{ path_to_sqlite_file }}')\nengine = create_engine(\n    os.environ.get('{{ program_name }}_DATABASE_URL') or 'sqlite:///' + db_path\n)\n\nBase = declarative_base(bind=engine)\n\nclass User(Base):\n\"\"\"\n    Class to define the User model.\n    \"\"\"\n    __tablename__ = 'user'\n    id = Column(Integer, primary_key=True, doc='User ID')\n</code></pre></p> </li> <li>Create the <code>migrations/env.py</code> file as specified in the alembic     article.</li> <li>Create the first alembic revision.     <pre><code>alembic \\\n-c {{ program_name }}/migrations/alembic.ini \\\nrevision \\\n--autogenerate \\\n-m \"Initial schema\"\n</code></pre></li> <li>Set up the testing environment for SQLAlchemy</li> </ul>"}, {"location": "coding/react/react/", "title": "React", "text": "<p>React is a declarative, efficient, and flexible JavaScript library for building user interfaces. It lets you compose complex UIs from small and isolated pieces of code called \u201ccomponents\u201d.</p>"}, {"location": "coding/react/react/#set-up-a-new-project", "title": "Set up a new project", "text": "<ul> <li> <p>Install Node.js</p> </li> <li> <p>Create the project baseline with Create React     App. Using this tool avoids:</p> <ul> <li>Learning and configuring many build tools.</li> <li>Optimize your bundles.</li> <li>Worry about the incompatibility of versions between the underlying pieces.</li> </ul> <p>So you can focus on the development of your code.</p> <pre><code>npx create-react-app my-app\n</code></pre> </li> <li> <p>Delete all files in the <code>src/</code> folder of the new project.     <pre><code>cd my-app\nrm src/*\n</code></pre></p> </li> <li> <p>Create the basic files <code>index.css</code>, <code>index.js</code> in the <code>src</code> directory.</p> </li> <li>Run the server: <code>npm start</code>.</li> </ul>"}, {"location": "coding/react/react/#start-a-react-flask-project", "title": "Start a React + Flask project", "text": "<ul> <li>Create the api directory.     <pre><code>mkdir api\n</code></pre></li> <li>Make the virtualenv.     <pre><code>mkvirtualenv \\\n--python=python3 \\\n-a ~/projects/my-app \\\nmy-app\n</code></pre></li> <li>Install flask.     <pre><code>pip install flask python-dotenv\n</code></pre></li> <li> <p>Add a basic file to <code>api/api.py</code>.     <pre><code>import time\nfrom flask import Flask\n\napp = Flask(__name__)\n\n@app.route('/api/time')\ndef get_current_time():\n    return {'time': time.time()}\n</code></pre></p> </li> <li> <p>Create the <code>.flaskenv</code> file.     <pre><code>FLASK_APP=api/api.py\nFLASK_ENV=development\n</code></pre></p> </li> <li>Make sure everything is alright.     <pre><code>flask run\n</code></pre></li> </ul>"}, {"location": "coding/react/react/#the-basics", "title": "The basics", "text": "<p>Components tell React what to show on the screen. When the data changes, React will efficiently update and re-render the components.</p> <pre><code>class ShoppingList extends React.Component {\nrender() {\nreturn (\n&lt;div className=\"shopping-list\"&gt;\n&lt;h1&gt;Shopping List for {this.props.name}&lt;/h1&gt;\n&lt;ul&gt;\n&lt;li&gt;Instagram&lt;/li&gt;\n&lt;li&gt;WhatsApp&lt;/li&gt;\n&lt;li&gt;Oculus&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;\n);\n}\n}\n\n// Example usage: &lt;ShoppingList name=\"Mark\" /&gt;\n</code></pre> <p><code>ShoppingList</code> is a React component class, or React component type. A component takes in parameters, called <code>props</code> (short for \u201cproperties\u201d), and returns a hierarchy of views to display via the <code>render</code> method.</p> <p>The <code>render</code> method returns a description of what you want to see on the screen. React takes the description and displays the result. In particular, render returns a React element, which is a lightweight description of what to render.</p> <p>Most React developers use a special syntax called \u201cJSX\u201d which makes these structures easier to write. The  syntax is transformed at build time to React.createElement('div'). The example above is equivalent to:</p> <pre><code>return React.createElement('div', {className: 'shopping-list'},\nReact.createElement('h1', /* ... h1 children ... */),\nReact.createElement('ul', /* ... ul children ... */)\n);\n</code></pre> <p>The <code>ShoppingList</code> component above only renders built-in DOM components like <code>&lt;div /&gt;</code> and <code>&lt;li /&gt;</code>. But it can compose and render custom React components too. For example, Use <code>&lt;ShoppingList /&gt;</code> to refer to the whole shopping list. Each React component is encapsulated and can operate independently; this allows the building of complex UIs from simple components.</p>"}, {"location": "coding/react/react/#pass-data-between-components", "title": "Pass data between components", "text": "<p>Data is passed between components through the <code>props</code> method.</p> <pre><code>class Square extends React.Component {\nrender() {\nreturn (\n&lt;button className=\"square\"&gt;\n{this.props.value}\n&lt;/button&gt;\n);\n}\n}\n\nclass Board extends React.Component {\nrenderSquare(i) {\nreturn &lt;Square value={i} /&gt;;\n}\n...\n}\n</code></pre>"}, {"location": "coding/react/react/#use-of-the-state", "title": "Use of the state", "text": "<p>React components can have state by setting <code>this.state</code> in their constructors. <code>this.state</code> should be considered as private to a React component that it\u2019s defined in.</p> <p>Components need a constructor to initialize the state. In JavaScript classes, it's required to call super when defining the constructor of a subclass. All React component classes that have a constructor should start with a <code>super(props)</code> call.</p> <pre><code>class Square extends React.Component {\nconstructor(props){\nsuper(props);\nthis.state = {\nvalue: null,\n}\n}\nrender() {\n...\n}\n}\n</code></pre> <p>Then use the <code>this.setState</code> method to set the value</p> <pre><code>  ...\nrender() {\nreturn (\n&lt;button\nclassName=\"square\"\nonClick={() =&gt; this.setState({value: 'X'})}\n&gt;\n{this.state.value}\n&lt;/button&gt;\n);\n}\n</code></pre>"}, {"location": "coding/react/react/#share-the-state-between-parent-and-children", "title": "Share the state between parent and children", "text": "<p>To collect data from multiple children, or to have two child components communicate with each other, you need to declare the shared state in their parent component instead. The parent component can pass the state back down to the children by using props; this keeps the child components in sync with each other and with the parent component.</p> <p>First define the parent state</p> <pre><code>class Square extends React.Component {\nrender() {\nreturn (\n&lt;button\nclassName=\"square\"\nonClick={() =&gt; this.props.onClick()}\n&gt;\n{this.props.value}\n&lt;/button&gt;\n);\n}\n}\n\nclass Board extends React.Component {\nconstructor(props) {\nsuper(props);\nthis.state = {\nsquares: Array(9).fill(null),\n};\n}\n\nhandleClick(i) {\nconst squares = this.state.squares.slice();\nsquares[i] = 'X';\nthis.setState({squares: squares});\n}\n\nrenderSquare(i) {\nreturn &lt;Square\nvalue={this.state.squares[i]}\nonClick={() =&gt; this.handleClick(i)}\n/&gt;;\n}\n...\n}\n</code></pre> <p>Since state is considered to be private to a component that defines it, it's not possible to update the parent\u2019s state directly from the children. Instead, A function needs to be passed down from the parent to the children, so the children can call that function when required. For example the <code>onClick={() =&gt; this.handleClick(i)}</code> in the example above.</p> <p>When a <code>Square</code> is clicked, the <code>onClick</code> function provided by the <code>Board</code> is called. Here\u2019s a review of how this is achieved:</p> <ul> <li>The <code>onClick</code> prop on the built-in DOM <code>&lt;button&gt;</code> component tells React to set     up a click event listener.</li> <li>When the button is clicked, React will call the <code>onClick</code> event handler that     is defined in <code>Square</code>\u2019s <code>render()</code> method.</li> <li>This event handler calls <code>this.props.onClick()</code>. The <code>Square</code>\u2019s <code>onClick</code> prop     was specified by the <code>Board</code>.</li> <li>Since the <code>Board</code> passed <code>onClick={() =&gt; this.handleClick(i)}</code> to <code>Square</code>,     the <code>Square</code> calls <code>this.handleClick(i)</code> when clicked.</li> </ul> <p>So now the state is stored in <code>Board</code> instead of the individual <code>Square</code> components. When the <code>Board</code>\u2019s state changes, the <code>Square</code> components re-render automatically. In React terms, the <code>Square</code> components are now controlled components.</p>"}, {"location": "coding/react/react/#handling-data-change", "title": "Handling data change", "text": "<p>There are generally two approaches to changing data. The first one is to mutate the data by directly changing the it\u2019s values. The second is to replace the data with a new copy which has the desired changes.</p> <ul> <li> <p>Data Change with Mutation.</p> <pre><code>var player = {score: 1, name: 'Jeff'};\nplayer.score = 2;\n// Now player is {score: 2, name: 'Jeff'}\n</code></pre> </li> <li> <p>Data Change without Mutation.</p> <pre><code>var player = {score: 1, name: 'Jeff'};\n\nvar newPlayer = Object.assign({}, player, {score: 2});\n// Now player is unchanged, but newPlayer is {score: 2, name: 'Jeff'}\n\n// Or if you are using object spread syntax proposal, you can write:\n// var newPlayer = {...player, score: 2};\n</code></pre> </li> </ul> <p>By not mutating directly, several benefits are gained:</p> <ul> <li>Complex features become simple: Immutability makes complex features much     easier to implement.</li> <li> <p>Detecting Changes: Detecting changes in mutable objects is difficult because     they are modified directly. This detection requires the mutable object to be     compared to previous copies of itself and the entire object tree to be     traversed.</p> <p>Detecting changes in immutable objects is considerably easier. If the immutable object that is being referenced is different than the previous one, then the object has changed.</p> </li> <li> <p>Determining When to Re-Render in React: The main benefit of immutability is     that it helps you build pure components in React. Immutable data can easily     determine if changes have been made which helps to determine when     a component requires re-rendering.</p> </li> </ul>"}, {"location": "coding/react/react/#function-components", "title": "Function components", "text": "<p>Function components are a simpler way to write components that only contain a render method and don\u2019t have their own state. Instead of defining a class which extends <code>React.Component</code>, we can write a function that takes props as input and returns what should be rendered. Function components are less tedious to write than classes, and many components can be expressed this way.</p> <p>Instead of</p> <pre><code>class Square extends React.Component {\nrender() {\nreturn (\n&lt;button\nclassName=\"square\"\nonClick={() =&gt; this.props.onClick()}\n&gt;\n{this.props.value}\n&lt;/button&gt;\n);\n}\n}\n</code></pre> <p>Use</p> <pre><code>function Square(props) {\nreturn (\n&lt;button ClassName=\"square\" onClick={props.onClick}&gt;\n{props.value}\n&lt;/button&gt;\n);\n}\n</code></pre>"}, {"location": "coding/react/react/#miscellaneous", "title": "Miscellaneous", "text": ""}, {"location": "coding/react/react/#list-rendering", "title": "List rendering", "text": "<p>When React renders a list, it stores some information about each rendered list item. Once a list is updated, React needs to determine what has changed. A key property needs to be specified for each list item to differentiate each list item from its siblings. So for:</p> <pre><code>&lt;li&gt;Ben: 9 tasks left&lt;/li&gt;\n&lt;li&gt;Claudia: 8 tasks left&lt;/li&gt;\n&lt;li&gt;Alexa: 5 tasks left&lt;/li&gt;\n</code></pre> <pre><code>&lt;li key={user.id}&gt;{user.name}: {user.taskCount} tasks left&lt;/li&gt;\n</code></pre> <p>Could be used. When a list is re-rendered, React takes each list item\u2019s key and searches the previous list\u2019s items for a matching key. If the current list has a key that didn\u2019t exist before, React creates a component. If the current list is missing a key that existed in the previous list, React destroys the previous component. If two keys match, the corresponding component is moved. Keys tell React about the identity of each compone.</p> <p>Keys tell React about the identity of each component which allows React to maintain state between re-renders. If a component\u2019s key changes, the component will be destroyed and re-created with a new state.</p> <p><code>key</code> is a special and reserved property in React. When an element is created, React extracts the <code>key</code> property and stores the <code>key</code> directly on the returned element. Even though <code>key</code> may look like it belongs in <code>props</code>, <code>key</code> cannot be referenced using <code>this.props.key</code>. React automatically uses <code>key</code> to decide which components to update. A component cannot inquire about its <code>key</code>.</p>"}, {"location": "coding/react/react/#links", "title": "Links", "text": "<ul> <li>React tutorial</li> <li>Awesome React</li> <li>Awesome React components</li> </ul>"}, {"location": "coding/react/react/#responsive-react", "title": "Responsive React", "text": "<ul> <li>Responsive react</li> <li>Responsive websites without css</li> <li>react-responsive library</li> </ul>"}, {"location": "coding/react/react/#with-flask", "title": "With Flask", "text": "<ul> <li>How to create a react + flask project</li> <li>How to deploy a react + flask project</li> </ul>"}, {"location": "coding/sql/sql/", "title": "SQL", "text": ""}, {"location": "coding/sql/sql/#sql-data-types", "title": "SQL Data Types", "text": "<p>String data types:</p> <ul> <li><code>VARCHAR(size)</code>: A variable length string (can contain letters, numbers, and     special characters). The size parameter specifies the maximum column length     in characters - can be from 0 to 65535.</li> <li><code>TEXT(size)</code>: Holds a string with a maximum length of 65,535 bytes.</li> <li><code>MEDIUMTEXT</code>: Holds a string with a maximum length of 16,777,215 characters.</li> <li> <p><code>LONGTEXT</code>: Holds a string with a maximum length of 4,294,967,295 characters.</p> </li> <li> <p><code>ENUM(val1, val2, val3, ...)</code>: A string object that can have only one value,     chosen from a list of possible values. You can list up to 65535 values in an     <code>ENUM</code> list. If a value is inserted that is not in the list, a blank value     will be inserted. The values are sorted in the order you enter them.</p> </li> <li><code>SET(val1, val2, val3, ...)</code>: A string object that can have 0 or more values,     chosen from a list of possible values. You can list up to 64 values in a SET     list.</li> </ul> <p>Numeric data types:</p> <ul> <li> <p><code>BOOL</code> or <code>BOOLEAN</code>: Zero is considered as false, nonzero values are     considered as true.</p> </li> <li> <p><code>TINYINT(size)</code>: A very small integer. Signed range is from -128 to 127.     Unsigned range is from 0 to 255. The size parameter specifies the maximum     display width (which is 255).</p> </li> <li><code>SMALLINT(size)</code>: A small integer. Signed range is from -32768 to 32767.     Unsigned range is from 0 to 65535. The size parameter specifies the maximum     display width (which is 255).</li> <li> <p><code>INT(size)</code>: A medium integer. Signed range is from -2147483648 to 2147483647.     Unsigned range is from 0 to 4294967295. The size parameter specifies the     maximum display width (which is 255).</p> </li> <li> <p><code>FLOAT(p)</code>: A floating point number. MySQL uses the p value to determine     whether to use <code>FLOAT</code> or <code>DOUBLE</code> for the resulting data type. If p is from     0 to 24, the data type becomes <code>FLOAT()</code>. If p is from 25 to 53, the data type     becomes <code>DOUBLE()</code>.</p> </li> </ul> <p>Date and time data types:</p> <ul> <li><code>DATE</code>: A date. Format: YYYY-MM-DD. The supported range is from '1000-01-01'     to '9999-12-31'.</li> <li><code>DATETIME(fsp)</code>: A date and time combination. Format: <code>YYYY-MM-DD hh:mm:ss</code>.     The supported range is from <code>1000-01-01 00:00:00</code> to <code>9999-12-31 23:59:59</code>.     Adding <code>DEFAULT</code> and <code>ON UPDATE</code> in the column definition to get automatic     initialization and updating to the current date and time.</li> </ul>"}, {"location": "coding/sql/sql/#operations", "title": "Operations", "text": ""}, {"location": "coding/sql/sql/#list-all-tables", "title": "List all tables", "text": "<p>Mysql:</p> <pre><code>show tables;\n</code></pre> <p>Postgresql:</p> <pre><code>\\dt\n</code></pre> <p>Sqlite:</p> <pre><code>.tables\n</code></pre>"}, {"location": "coding/sql/sql/#delete-rows-from-table", "title": "Delete rows from table", "text": "<pre><code>DELETE FROM table_name\nWHERE condition;\n</code></pre>"}, {"location": "coding/sql/sql/#update-host-permissions-for-a-mysql-user", "title": "Update host permissions for a mysql user", "text": ""}, {"location": "coding/sql/sql/#table-relationships", "title": "Table relationships", "text": ""}, {"location": "coding/sql/sql/#one-to-one", "title": "One to One", "text": "<p>A one-to-one relationship between two entities exists when a particular entity instance exists in one table, and it can have only one associated entity instance in another table.</p> <p>For example, a user can have only one address, and an address belongs to only one user. This sort of relationship is implemented by setting the PRIMARY KEY of the users table (<code>id</code>)  as both the FOREIGN KEY and PRIMARY KEY of the addresses table.</p> <pre><code>CREATE TABLE addresses (\nuser_id int, -- Both a primary and foreign key\nstreet varchar(30) NOT NULL,\ncity varchar(30) NOT NULL,\nstate varchar(30) NOT NULL,\nPRIMARY KEY (user_id),\nFOREIGN KEY (user_id) REFERENCES users (id) ON DELETE CASCADE\n);\n</code></pre> <p>The <code>ON DELETE CASCADE</code> clause of the <code>FOREIGN_KEY</code> definition instructs the database to delete the referencing row if the referenced row is deleted. There are alternatives to <code>CASCADE</code> such as <code>SET NULL</code> or <code>SET DEFAULT</code> which instead of deleting the referencing row will set a new value in the appropriate column for that row.</p>"}, {"location": "coding/sql/sql/#one-to-many", "title": "One to many", "text": "<p>A one-to-many relationship exists between two entities if an entity instance in one of the tables can be associated with multiple records (entity instances) in the other table. The opposite relationship does not exist; that is, each entity instance in the second table can only be associated with one entity instance in the first table.</p> <p>For example, a review belongs to only one book while a book has many reviews.</p> <pre><code>CREATE TABLE books (\nid serial,\ntitle varchar(100) NOT NULL,\nauthor varchar(100) NOT NULL,\npublished_date timestamp NOT NULL,\nisbn char(12),\nPRIMARY KEY (id),\nUNIQUE (isbn)\n);\n\n/*\n one to many: Book has many reviews\n*/\n\nCREATE TABLE reviews (\nid serial,\nbook_id integer NOT NULL,\nreviewer_name varchar(255),\ncontent varchar(255),\nrating integer,\npublished_date timestamp DEFAULT CURRENT_TIMESTAMP,\nPRIMARY KEY (id),\nFOREIGN KEY (book_id) REFERENCES books(id) ON DELETE CASCADE\n);\n</code></pre> <p>Unlike our addresses table, the <code>PRIMARY KEY</code> and <code>FOREIGN KEY</code> reference different columns, <code>id</code> and <code>book_id</code> respectively. This means that the <code>FOREIGN KEY</code> column, <code>book_id</code> is not bound by the <code>UNIQUE</code> constraint of our <code>PRIMARY KEY</code> and so the same value from the <code>id</code> column of the <code>books</code> table can appear in this column more than once. In other words a book can have many reviews.</p>"}, {"location": "coding/sql/sql/#many-to-many", "title": "Many to many", "text": "<p>A many-to-many relationship exists between two entities if for one entity instance there may be multiple records in the other table, and vice versa.</p> <p>For example, a user can check out many books, and a book can be checked out by many users.</p> <p>In order to implement this sort of relationship we need to introduce a third, cross-reference, table. This table holds the relationship between the two entities, by having two <code>FOREIGN KEY</code>s, each of which references the <code>PRIMARY KEY</code> of one of the tables for which we want to create this relationship. We already have our books and users tables, so we just need to create the cross-reference table: <code>checkouts</code>.</p> <pre><code>CREATE TABLE checkouts (\nid serial,\nuser_id int NOT NULL,\nbook_id int NOT NULL,\ncheckout_date timestamp,\nreturn_date timestamp,\nPRIMARY KEY (id),\nFOREIGN KEY (user_id) REFERENCES users(id) ON DELETE CASCADE,\nFOREIGN KEY (book_id) REFERENCES books(id) ON DELETE CASCADE\n);\n</code></pre>"}, {"location": "coding/sql/sql/#joins", "title": "Joins", "text": "<p>A <code>JOIN</code> clause is used to combine rows from two or more tables, based on a related column between them.</p> <p></p>"}, {"location": "coding/sql/sql/#one-to-one-join", "title": "One to one join", "text": "<pre><code>SELECT users.id, addresses.street\nFROM users\nLEFT JOIN addresses\nON users.id = addresses.user_id\n</code></pre> <p>It will return one line.</p>"}, {"location": "coding/sql/sql/#one-to-many-join", "title": "One to many join", "text": "<pre><code>SELECT books.id, reviews.rating\nFROM books\nLEFT JOIN reviews\nON books.id = reviews.book_id\n</code></pre> <p>It will return many lines.</p>"}, {"location": "coding/sql/sql/#many-to-many-join", "title": "Many to many join", "text": "<pre><code>SELECT users.id, books.id\nFROM users\nLEFT OUTER JOIN checkouts\nON users.id == checkouts.user_id\nLeft OUTER JOIN books\nON checkouts.book_id == books.id\n</code></pre>"}, {"location": "coding/yaml/yaml/", "title": "YAML", "text": "<p>YAML (a recursive acronym for \"YAML Ain't Markup Language\") is a human-readable data-serialization language. It is commonly used for configuration files and in applications where data is being stored or transmitted. YAML targets many of the same communications applications as Extensible Markup Language (XML) but has a minimal syntax which intentionally differs from SGML. It uses both Python-style indentation to indicate nesting, and a more compact format that uses [...] for lists and {...} for maps making YAML 1.2 a superset of JSON.</p>"}, {"location": "coding/yaml/yaml/#break-long-lines", "title": "Break long lines", "text": "<ul> <li> <p>Use <code>&gt;</code> most of the time: interior line breaks are stripped out, although you     get one at the end:</p> <pre><code>key: &gt;\nYour long\nstring here.\n</code></pre> </li> <li> <p>Use <code>|</code> if you want those line breaks to be preserved as <code>\\n</code> (for instance,     embedded markdown with paragraphs):</p> <pre><code>key: |\n### Heading\n\n* Bullet\n* Points\n</code></pre> </li> <li> <p>Use <code>&gt;-</code> or <code>|-</code> instead if you don't want a line break appended at the end.</p> </li> <li> <p>Use <code>\"...\"</code> if you need to split lines in the middle of words or want to     literally type line breaks as <code>\\n</code>:</p> <pre><code>key: \"Antidisestab\\\nlishmentarianism.\\n\\nGet on it.\"\n</code></pre> </li> <li> <p>YAML is crazy.</p> </li> </ul>"}, {"location": "dancing/cutting_shapes_basics/", "title": "Cutting Shapes Basics", "text": ""}, {"location": "dancing/cutting_shapes_basics/#basic-crossing", "title": "Basic crossing", "text": "<ul> <li>8-and: Left legs straight under our body, with the weight in the toes and     the foot a little turned with the heel pointing to our right (7 o'clock),     right leg lifted up straight to our 3 o'clock with toes pointing down.     with knee up front. Weight lies on the left foot.</li> <li>1: Right foot crosses before the left one, toes touch the floor first in our     longitudinal axes. The left foot pivots over the toes to end up pointing to     our 11. Weight is evenly shared between the feet toes.</li> <li>1-and: Mirrors 8-and.</li> <li>2: Mirrors 1.</li> <li>2-and: Equal to 8-and.</li> </ul> <p>Sources: 1</p>"}, {"location": "dancing/cutting_shapes_basics/#wiggle-feet", "title": "Wiggle feet", "text": "<ul> <li>8-and: Both legs straight under our body, right leg lifted up     with knee up front. Weight lies on the left foot.</li> <li>1: Right foot touches the floor behind the hips pointing to 10.5, left foot     pivots over the toes to point to 1.5. Weight is even between feet.</li> <li>1-and: Right foot pivots on the heel till 1.5. Left foot pivots on the toes     till 10.5.</li> <li>2: Equal 1.</li> <li>2-and: Mirrors 1-and.</li> <li>3: Equal 1.</li> <li>3-and: Equal 1-and.</li> <li>4: Equal 1.</li> </ul> <p>Sources: 1</p>"}, {"location": "dancing/cutting_shapes_basics/#references", "title": "References", "text": ""}, {"location": "dancing/cutting_shapes_basics/#where-to-discover-more-tutorials", "title": "Where to discover more tutorials", "text": "<ul> <li>Elements tutorial playlist</li> <li>Anderson Jovani playlist</li> </ul>"}, {"location": "dancing/rave_dances/", "title": "Rave dances", "text": ""}, {"location": "dancing/rave_dances/#shuffle", "title": "Shuffle", "text": "<p>Shuffle is a rave dance that developed in the 1980s. Typically performed to electronic music, the dance originated in the Melbourne rave scene, and was popular in the 1980s and 1990s. The dance moves involve a fast heel-and-toe movement or T-step, combined with a variation of the running man coupled with a matching arm action. It's an improvised dance and involves \"shuffling your feet inwards, then outwards, while thrusting your arms up and down, or side to side, in time with the beat\".</p> <p>It seems to be a minority underground dance without official competitions or schools. Given the music, it's origins and the energy it requires, most of the videos are uploaded by young people that reproduce the nasty gender roles that the society embeds in us. So expect to see over sexualized girls and over testosteroned guys, which is sad.</p>"}, {"location": "dancing/rave_dances/#styles", "title": "Styles", "text": ""}, {"location": "dancing/rave_dances/#melbourne-shuffle", "title": "Melbourne Shuffle", "text": "<p>It's the original shuffle, which was the one that caught my eye several years back with Francis' video:</p> <p>It's \"simple and straightforward\", the base is the running man, with a few kicks and spins sprinkled in from time to time, so there is not much room for variety. It has a low skill floor and a moderate skill ceiling.</p> <p>Usually the dancers wear trousers with a wide end that makes a nice flying visual effect and a hoodie. The percent of read women Melbourne shuffle videos is low.</p> <p>Here is a list of other Melbourne shuffle videos: 1, 2, 3, 4</p> <p>The malasian and russian styles are, to my untrained eye, similar to the Melbourne, although they've got different T-step.</p> <p>Although in the source post they say that these styles are usually danced between 150 and 200 bpms, I've found that the videos range from 130 to 160 bpms with an average of 150 bpms.</p>"}, {"location": "dancing/rave_dances/#cutting-shapes", "title": "Cutting shapes", "text": "<p>It's the most popular right now (at least for the number of videos I've found). The base steps are the Charleston or the heel variants of the running man, which looks less like running. The basic step is used less than in the other styles, filling most of the moves with heel-toe movements, criss-crossings, and spins, so there is little \"shuffling\" in cutting shapes. It has a low skill floor, but an absurdly high skill ceiling. There is a lot of room for variety and you can even mix it up by adding other dance styles to it, as it's very flexible.</p> <p>Usually the dancers wear street clothes and some use shoes with led lights. The percent of read women Cutting shapes shuffle videos is even with read males.</p> <p>Here is a list of other cutting shapes shuffle videos: 1, 2</p> <p>Cutting shapes songs are mostly around the 130 bpms.</p>"}, {"location": "dancing/rave_dances/#jumpstyle", "title": "Jumpstyle", "text": "<p>Jumpstyle is an electronic dance style and music genre popular in Western Europe, with existent scenes in Eastern Europe, Australia, and the Americas.</p> <p>It's music is an offspring of tech-trance, hardstyle, gabber and m\u00e1kina. Its tempo is usually between 140 and 150 BPM.</p> <p>Here are another Jumpstyle videos: 1, 2.</p>"}, {"location": "dancing/shuffle_basics/", "title": "Shuffle basics", "text": "<p>Shuffle steps are usually split each tempo into two phases, songs usually follow a 4/4 structure. To differentiate the steps between tempos I'm going to use the tempo number they come from followed by an <code>-and</code> so the structure will be: <code>8-and</code>, <code>1</code>, <code>1-and</code>, <code>2</code>... Where the <code>1</code> is the phrase starter.</p> <p>Most of the steps don't have names so I've put one that felt it fits.</p> <p>To specify the relative position of body parts, I'm using an imaginary clock, so 12 means upfront, 3 our right, 6 our back and 9 our left.</p> <p>The steps below can be danced with \"slow\" or quick tempos. It's best to learn with the slow ones as they build the muscle memory to be able to skip parts of it as you move to higher tempos.</p>"}, {"location": "dancing/shuffle_basics/#dance-routine", "title": "Dance routine", "text": "<p>After several iterations I've come to this routine:</p> <ul> <li>Start dancing a bit to low tempo songs, I've got a playlist with increasing     tempo songs, the aim is to start warming up and remember what I've learnt     the last time.</li> <li>Once you're warmed up dance some songs to your current tempo level.</li> <li>Then put some songs a little above your tempo level and try to speed up your     running man and T-Step.</li> <li>Improve this notes with the new discoveries.</li> <li>Analyze a video to extract some steps.</li> <li>Add one or two songs to the shuffle playlist. Prepend the song name with the     bpms (it's better to measure them     manually as automatic software tends to fail badly).</li> </ul>"}, {"location": "dancing/shuffle_basics/#running-man", "title": "Running man", "text": "<p>The running man is the basic step of Melbourne shuffle, so make sure to master it.</p> <p> </p> <ul> <li>8-and: Both legs straight under our body, right leg lifted up     with knee up front. Weight lies on the left foot.</li> <li>1: Right foot kicks forward, left foot slides backward, in a way that each travel the     same distance. Weight is evenly shared between the feet.</li> <li>1-and: Right foot slides backwards, right foot goes up, knee to the front.     Weight lies on the right foot.</li> <li>2: Mirrors 1.</li> <li>2-and: Mirrors 1-and and it's equal to 8-and.</li> </ul> <p> </p> <p>Once you feel comfortable with it, start going around the room doing running mans, so you get a grasp of doing it without being stationary.</p> <p>Sources: 1, 2</p> <p>The foot going forward can land in the floor flat, heel first or toes first.</p>"}, {"location": "dancing/shuffle_basics/#quick-tempo-running-man", "title": "Quick tempo Running man", "text": "<p>As the tempo speeds up, even if it seems there is no time to stop in the 1-and or 2-and they do manage to do so. To reach there, I'd put a song a little bit quicker than you are comfortable and:</p> <ul> <li>Get familiarity with the movement of the foot in the ground: Start with your     basic running man stance left foot in front. When the 1 comes, try to     reach the 1-and focusing to lift your right knee and stomp in the 2.     Rest a pair of beats and then try the other foot. Keep on doing it till you     are comfortable doing this movement.</li> <li>Then try to do 2 complete running mans.</li> <li>Then dance :).</li> </ul> <p>For me the most difficult part is to bring back the non dominant leg backwards, to get used to the movement I slided backwards with that leg several times, then do three running mans starting with that leg, rest, and then do another three starting with the other one.</p>"}, {"location": "dancing/shuffle_basics/#t-step", "title": "T-Step", "text": "<p>The T-Step is other basic step of Melbourne shuffle, used to do lateral movement.</p>"}, {"location": "dancing/shuffle_basics/#learning-the-t-step", "title": "Learning the T-Step", "text": "<p>The following instructions are a nice way to get the grasp of the movement, but we'll change them slightly to be able to link it with the other moves.</p> <p></p> <ul> <li>8-and: Both legs straight under our body, right leg lifted up     with knee up front. Weight lies on the left foot, which looks at 1.5     although the rest of you look at 12.</li> <li>1: Right foot kicks sideways towards 3, try to leave the foot at 90 degrees     with your leg and that it doesn't wobble, so it's a clean kick. The weight     is on the left leg's toes and it pivotes over them till it points to 10.5.     Waist ends up turned.</li> <li>1-and: Right foot is retrieved till the left leg's knee, keeping the base     parallel to the ground. Weight is on the left leg's heel and pivotes over     it to reach the 1.5 position. Waist is straight now.</li> <li>2: Equal to 1.</li> <li>2-and: Equal to 1-and.</li> <li>3: Right foot goes front, left foot goes back to the running man stance.</li> </ul> <p>To move to the other side is the mirror.</p> <p>Keep the leg that holds you a bit bent so you don't harm your knee when twisting.</p> <p></p> <p>The usual basic pattern of Melbourne shuffle is doing some running mans pointing at 10.5, then doing a side step (like the T-step) to the right, then another running man pointing to 1.5, finishing with another side step to the left.</p> <p>To join the T-Step with the running man, when you are in the 1-and make it the 8-and of the T-Step.</p> <p></p> <p>Sources: 1 2</p>"}, {"location": "dancing/shuffle_basics/#francis-t-step", "title": "Francis T-Step", "text": "<p>I've seen that Francis when finishes his T-Step with the other leg. So instead of moving the right leg in front and slide back the left (if we are going to the right), he:</p> <ul> <li>1-and: Equal to normal T-Step.</li> <li>2: When you put your right foot down, shift your weight till it's evenly     spread between feet.</li> <li>2-and: Switch your weight to the right foot, lift the left while you keep on     shifting your body weight to your 3.</li> <li>3: Left foot touches ground at your 1.5, slide the right back to the running     man stance.</li> </ul> <p>What I like of this variation is that you conserve the energy, it makes total sense. What it doesn't is that he is dancing with sandals <code>(\u00ac\u00ba-\u00b0)\u00ac</code>.</p> <p>Try to move from side to side with 2 T-Steps in between using this variation.</p>"}, {"location": "dancing/shuffle_basics/#quick-tempo-t-step", "title": "Quick tempo T-Step", "text": "<p>To improve the speed of your T-Steps, I'd put a song a little bit quicker than you are comfortable and:</p> <ul> <li> <p>Get familiarity with the movement of the foot in the ground:</p> <ul> <li>Start with your right knee up in position 8-and of the T-Step.</li> <li>First focus only to move the feet that you've got in the floor, so when     the 1 comes, start pivoting the foot a pair of tempos.</li> <li>Put your right foot down and lift the left knee.</li> <li>Wait 3 or 4 tempos and when the 1 comes, start going to the other side.</li> </ul> </li> <li> <p>Once you are comfortable with the movement, repeat the same exercise but doing the kicks.</p> </li> <li>Then instead of finishing with both feet under your body, finish in a running     man stance.</li> <li>Then instead of starting with your knee up, start from the running man stance.</li> </ul>"}, {"location": "dancing/shuffle_basics/#future-steps", "title": "Future steps", "text": "<ul> <li>Sacco kicks at 14:50, 15:49,     17:08</li> <li>Rocky kicks at 13:26 or 27:58.     Rocky version at 1:19, 2:34,     7:29</li> <li>Recce kicks at 15:56     (explanation) or 27:43 or 28:05.</li> <li>Reverse kick spin at 18:01 or 27:65.     Rocky version at 1:18, 2:16,     2:55, 3:26, 3:59, 4:58, 6:24, 8:17, 8:35, 15:00, 15:55</li> <li>Forward kick spin at 19:33.     Rocky version at 2:20, 3:16,     3:47, 8:40, 8:49</li> <li>Rocky side to side at 15:04,     16:08</li> </ul>"}, {"location": "dancing/shuffle_basics/#beats-per-minute-progression", "title": "Beats per minute progression", "text": "<ul> <li>2020-09-07: First time to be able to dance a song at 118bpm to C2C - Happy.</li> </ul>"}, {"location": "dancing/shuffle_basics/#references", "title": "References", "text": ""}, {"location": "dancing/shuffle_basics/#more-videos-to-study", "title": "More videos to study", "text": "<ul> <li>Rocky Shuffle compilation</li> <li>Francis Shuffle compilation</li> <li>Siera Shuffle compilation</li> <li>Bulldog Shuffle compilation</li> </ul>"}, {"location": "dancing/shuffle_kicks/", "title": "Shuffle Kicks", "text": ""}, {"location": "dancing/shuffle_kicks/#kicking-running-man", "title": "Kicking-Running man", "text": "<ul> <li>8-and: Both legs straight under our body, right leg lifted up     with knee up front. Weight lies on the left foot.</li> <li>1: Right foot kicks forward without touching the ground, it's important that     the kick is clean, so swift till the end and stop suddenly. At the same time     the left foot has slided back a little bit.</li> <li>1-and: Right foot is retrieved, knee up and foot parallel to the ground,     left foot keeps on sliding back.</li> <li>2: Right foot hits the ground, left foot keeps on sliding, weight is evenly     shared between feet like the 1 of the running man.</li> </ul> <p>Sources: 1</p>"}, {"location": "dancing/shuffle_kicks/#sacco-kicks", "title": "Sacco kicks", "text": "<p>Sacco kicks looks similar to the T-Step adding some side kicks. There are two variations, either kick first in and then out or the other way around. I'm more comfortable with the first and I think it gives an advantage when combining it with the running man.</p> <ul> <li>8-and: Equal to the T-Step.</li> <li>1: The foot of the floor equal to the T-Step but the right foot kicks to     your 10.5.</li> <li>1-and: Equal to the T-Step.</li> <li>2: The foot of the floor equal to the T-Step but the right foot kicks to     your 3.</li> </ul> <p>The advantage of the Sacco kicks is that as you are going to kick to your 10.5 in 1, you can slide the left foot quite a lot to your back (from 8 to 1), therefore gliding further than with a simple T-Step (you only have half a tempo before you need to put all your weight in your left leg because you need to twist).</p> <p></p> <p>Although the guy in the video slides the foot backwards when going to his left, try to do the twist of the T-Step.</p> <p>(The guy is crazy, shuffling barefoot over gravel soil <code>(\u256f\u00b0\u25a1\u00b0)\u256f \u253b\u2501\u253b</code>).</p> <p>Source: 1</p>"}, {"location": "dancing/shuffle_spins/", "title": "Shuffle Spins", "text": ""}, {"location": "dancing/shuffle_spins/#tap-spin", "title": "Tap spin", "text": "<ul> <li>8: Feet at the end of a running man, we generate a slight upper body twist     clockwise to load the twist.</li> <li>8-and: Left foot goes back to the body axes and the right foot goes to the     body with knee up like a running man, with the difference that we release     the twist energy and we start turning counterclockwise. Weight is on the     left foot toes and it's knee is a little bit bent.</li> <li>1: Right foot kicks sideways towards 3 touching the ground but not putting     any weight into it while we unbend the left knee taking the chance to keep     the spin going.</li> <li>1-and: Right foot goes back to the body and left knee is again a little bit     bent waiting for the next kick.</li> <li>2: Equal to 1</li> <li>2-and: Equal to 1-and</li> <li>3: Becomes the 1 of the running man.</li> </ul> <p>You can spin as long as you want, I've explained a two times spin, but if you want to go further repeat 1 and 1-and as long as you want.</p> <p></p> <p>Sources: 1</p>"}, {"location": "dancing/shuffle_spins/#francis-spin", "title": "Francis Spin", "text": "<p>Awesome to start dancing.</p> <ul> <li>8: Both feet under your body, weight evenly spread between them. Give     a small jump to start rotating clockwise (for example).</li> <li>8-and: After 90 degrees of turn, your feet touch the ground, take the chance     to give the big push to rotate the following 270. Bring your exterior arm to     your shoulder level.</li> <li>1: Arrive to the running man stance with the left foot in front, but don't     stop the rotation inertia.</li> <li>1-and: Retrieve the left foot like any running man, but the foot points to     your 5, right knee up.</li> <li>2: End your running man to your 5 with your right foot down. Add the final     touch by lowering your external arm down.</li> </ul> <p></p> <p>To get there :</p> <ul> <li>First practice the last part, the running man with a spin. The steps 1 to     2 described above until you are comfortable with it in both spinning     directions. It's good to change the spin direction not to get dizzy.</li> <li>Do the full move until you are comfortable.</li> <li>Get it in time. Count in the song to start the movement in the 8 of the last     time before a big drop.</li> </ul> <p>Source: 1</p>"}, {"location": "data_analysis/recommender_systems/recommender_systems/", "title": "Recommender Systems", "text": "<p>A recommender system, or a recommendation system, is a subclass of information filtering system that seeks to predict the \"rating\" or \"preference\" a user would give to an item.</p> <p>The entity to which the recommendation is provided is referred to as the user, and the product being recommended is also referred to as an item. Therefore, recommendation analysis is often based on the previous interaction between users and items, because past interests and proclivities are often good indicators of future choices.</p> <p>These relations can be learned in a data-driven manner from the ratings matrix, and the resulting model is used to make predictions for target users.The larger the number of rated items that are available for a user, the easier it is to make robust predictions about the future behavior of the user.</p> <p>The problem may be formulated in two ways:</p> <ul> <li>Prediction version of problem: This approach aims to predict the rating     value for a user-item combination. It is assumed that training data is     available, indicating user preferences for items. For m users and n     items, this corresponds to an incomplete m x n matrix, hwere the specified     (or observed) values are used for training. The missing (or unobserved)     values are predicted using this training model. This problem is also     referred to as the matrix completion problem.</li> <li>Ranking version of problem: This approach aims to recommend the top-k     items for a particular user, or determine the top-k users to target for     a particular item. Being the first one more common. The problem is also     referred to as the top-k recommendation problem.</li> </ul>"}, {"location": "data_analysis/recommender_systems/recommender_systems/#goals-of-recommender-systems", "title": "Goals of recommender systems", "text": "<p>The common operational and technical goals of recommender systems are:</p> <ul> <li>Relevance: Recommend items that are relevant to the user at hand. Although     it's the primary operational goal, it is not sufficient in isolation.</li> <li>Novelty: Recommend items that the user has not seen in the past.</li> <li>Serendipity: Recommend items that are outside the user's bubble, rather than     simply something they did not know about before. For example, if a new     Indian restaurant opens in a neighborhood, then the recommendation of that     restaurant to a user who normally eats Indian food is novel but not     necessarily serendipitous. On the other hand, when the same user is     recommended Ethiopian food, and it was unknown to the user that such food     might appeal to her, then the recommendation is serendipitous. Serendipity     has the beneficial side effect of beginning new areas of interest.</li> <li>Increase recommendation diversity: Recommend items that aren't similar to     increase the chances that the user likes at least one of these items.</li> </ul>"}, {"location": "data_analysis/recommender_systems/recommender_systems/#basic-models-of-recommender-systems", "title": "Basic Models of recommender systems", "text": "<p>There are four types of basic models:</p> <ul> <li>Collaborative filtering: Use collaborative power of the ratings provided by     multiple users to make recommendations.</li> <li>Content-based: Use the descriptive attributes of the user rated items to     create a user-specific model that predicts the rating of unobserved items.</li> <li>Knowledge-based: Use the similarities between customer requirements and item     descriptions.</li> <li>Hybrid systems: Combine the above to benefit from the mix of their strengths     to perform more robustly.</li> </ul>"}, {"location": "data_analysis/recommender_systems/recommender_systems/#collaborative-filtering-models", "title": "Collaborative Filtering Models", "text": "<p>These models use the collaborative power of the ratings provided by multiple users to make recommendations. The main challenge is that the underlying ratings matrices are sparse. To solve it, unspecified ratings are guessed by analyzing the relations of high correlation across various users and items.</p> <p>There are two common types of methods.</p> <ul> <li> <p>Memory-based methods: Also referred to as neighborhood-based collaborative     filtering algorithms, predict ratings on the basis of their neighborhoods.     These can be defined through two ways:</p> <ul> <li>User-based collaborative filtering: Ratings provided by like-minded     users of a target user A are used in order to make the recommendations     for A. Thus, the goal is to determine users who are similar to the     target user A, and recommend ratings for the unobserved ratings of A by     computing the averages of the ratings of this peer group.</li> <li>Item-based collaborative filtering: In order to make the rating     predictions for target item B by user A, the first step is to determine     a set S of items that are most similar to target item B. The ratings in     the item set S, which are defined by A, are used to predict whether the     user A will like item B.</li> </ul> <p>These systems are simple to implement and the resulting recommendations are often easy to explain. On the other hand, they do not work very well with sparse rating matrices.</p> </li> <li> <p>Model-based methods: Machine learning and data mining methods are used in     the context of predictive models. In cases where the model is parameterized,     the parameters of this model are learned within the context of an     optimization framework. Some examples of such methods include decision     trees, rule-based models, Bayesian methods and latent factor models. Many of     these methods have a high level of coverage even for sparse ratings     matrices.</p> </li> </ul>"}, {"location": "data_analysis/recommender_systems/recommender_systems/#types-of-ratings", "title": "Types of ratings", "text": "<p>The design of recommendation algorithms is influenced by the system used for tracking ratings. There are different types of ratings:</p> <ul> <li>interval-based: A discrete set of ordered numbers are used to quantify like     or dislike.</li> <li>ordinal: A discrete set of ordered categorical values, such as agree or     strongly agree, are used to achieve the same goal.</li> <li>binary: Only the like or dislike for the item can be specified.</li> <li>unary: Only liking of an item can be specified.</li> </ul> <p>Another categorization of rating systems is based in the way the feedback is retrieved:</p> <ul> <li>explicit ratings: Users actively give information on their preferences.</li> <li>implicit ratings: Users preferences are derived from their behavior. Such as     visiting a link. Therefore, implicit ratings are usually unary.</li> </ul>"}, {"location": "data_analysis/recommender_systems/recommender_systems/#content-based-recommender-systems", "title": "Content-Based Recommender Systems", "text": "<p>In content-based recommender systems, descriptive attributes of the user rated items are used to create a user-specific model that predicts the rating of unobserved items.</p> <p>These systems have the following advantages:</p> <ul> <li>Works well for new items, when sufficient data is not available. If the user     has rated items with similar attributes.</li> <li>Can make recommendations with only the data of one user.</li> </ul> <p>And the following disadvantages:</p> <ul> <li>As the community knowledge from similar users is not leveraged, it tends to     reduce the diversity of the recommended items.</li> <li>It requires large number of rating user data to produce robust predictions     without overfitting.</li> </ul>"}, {"location": "data_analysis/recommender_systems/recommender_systems/#knowledge-based-recommender-systems", "title": "Knowledge-based Recommender Systems", "text": "<p>The recommendation process is performed based on the similarities between user requirements and item descriptions or the user requirements constrains. The process is facilitated with the use of knowledge bases, which contain data about rules and similarity functions to use during the retrieval process.</p> <p>Knowledge-based systems can be classified by the type of user interface:</p> <ul> <li>Constraint-based recommender systems: Users specify requirements on the     item attributes or give information about their attributes. Then domain     specific rules are used to select the items to recommend.</li> <li>Case-based recommender systems: Specific cases are selected by the user as     targets or anchor points. Similarity metrics are defined in a domain     specific way on the item attributes to retrieve similar items to these     cases.</li> </ul> <p>These user interfaces can interact with the users through several ways:</p> <ul> <li>Conversational systems: User preferences are determined iteratively in     the context of a feedback loop. It's useful if the item domain is complex.</li> <li>Search-based systems: User preferences are elicited by using a preset of     questions.</li> <li>Navigation-based recommendation: Users specify a number of attribute changes     to the item being recommended. Also known as critiquing recommender     systems.</li> </ul> <p>The main difference between content-based systems and knowledge-based systems is that while the former learns from past user behavior, the latter does it from active user specification of their needs and interests.</p> <p>These systems have the following advantages:</p> <ul> <li>Works well for items with varied properties and/or few ratings. Such as in     cold start scenarios, if it's difficult to capture the user interests with     historical data or if the item is not often consumed.</li> <li>Allows the users to explicitly specify what they want, thus giving them     a greater control over the recommendation process.</li> <li>Allows the user to iteratively change their specified requirements to reach     the desired items.</li> <li>Can make recommendations with only the data of one user.</li> </ul> <p>And the following disadvantages:</p> <ul> <li>As the community knowledge from similar users is not leveraged, it tends to     reduce the diversity of the recommended items.</li> </ul>"}, {"location": "data_analysis/recommender_systems/recommender_systems/#domain-specific-recommender-systems", "title": "Domain-Specific recommender systems", "text": ""}, {"location": "data_analysis/recommender_systems/recommender_systems/#demographic-recommender-systems", "title": "Demographic recommender systems", "text": "<p>In these systems the demographic information about the user is leveraged to learn classifiers that can map specific demographics to ratings.</p> <p>Although they do not usually provide the best results on a standalone basis, they enhance and increase robustness if used as a component of hybrid systems.</p>"}, {"location": "data_analysis/recommender_systems/recommender_systems/#pitfalls-to-avoid", "title": "Pitfalls to avoid", "text": "<p>Pre-substitution of missing ratings is not recommended in explicit rating matrices as it leads to a significant amount of bias in the analysis. In unary ratings it's common to substitute the missing data by 0 as even though it adds some bias, it's not as great because it's assumed that the default behavior.</p>"}, {"location": "data_analysis/recommender_systems/recommender_systems/#interesting-resources", "title": "Interesting resources", "text": "<p>Bookworm looks to be a promising source to build book recommender systems.</p>"}, {"location": "data_analysis/recommender_systems/recommender_systems/#content-indexers", "title": "Content indexers", "text": "<ul> <li>Open Library: Open, editable library catalog,       building towards a web page for every book ever published. Data can be       retrieved through their API or bulk       downloaded.</li> </ul>"}, {"location": "data_analysis/recommender_systems/recommender_systems/#rating-datasets", "title": "Rating Datasets", "text": ""}, {"location": "data_analysis/recommender_systems/recommender_systems/#books", "title": "Books", "text": "<ul> <li>Book-Crossing: 278,858 users     providing 1,149,780 ratings (explicit / implicit) about 271,379 books.</li> </ul>"}, {"location": "data_analysis/recommender_systems/recommender_systems/#movies", "title": "Movies", "text": "<ul> <li>MovieLens: 27,000,000 ratings and     1,100,000 tag applications applied to 58,000 movies by 280,000 users.</li> <li>HetRec 2011 Movielens + IMDB/rotten     Tomatoes: 86,000 ratings from     2113 users.</li> <li>Netflix prize dataset:     480,000 users doing 100 million ratings on 17,000 movies.</li> </ul>"}, {"location": "data_analysis/recommender_systems/recommender_systems/#music", "title": "Music", "text": "<ul> <li>HetRec 2011 Last.FM: 92,800 artist listening records from 1892 users.</li> </ul>"}, {"location": "data_analysis/recommender_systems/recommender_systems/#web", "title": "Web", "text": "<ul> <li>HetRec 2011 Delicious: 105,000 bookmarks from 1867 users.</li> </ul>"}, {"location": "data_analysis/recommender_systems/recommender_systems/#miscelaneous", "title": "Miscelaneous", "text": "<ul> <li>Wikilens: generalized     collaborative recommender system that allowed its community to define item     types (e.g. beer) and categories (e.g. microbrews, pale ales, stouts), and     then rate and get recommendations for items.</li> </ul>"}, {"location": "data_analysis/recommender_systems/recommender_systems/#past-projects", "title": "Past Projects", "text": "<ul> <li>GroupLens: Pioneer recommender system to recommend Usenet news.</li> <li>BookLens: Books implementation of Grouplens.</li> <li>MovieLens: Movies implementation of Grouplens.</li> </ul>"}, {"location": "data_analysis/recommender_systems/recommender_systems/#references", "title": "References", "text": ""}, {"location": "data_analysis/recommender_systems/recommender_systems/#books_1", "title": "Books", "text": "<ul> <li>Recommender Systems by Chary C.Aggarwal.</li> <li>Recommender systems, an introduction by Dietmar Jannach, Markus Zanker,     Alexander Felfernig and Gerhard Friedrich.</li> <li>Practical Recommender Systems by Kim Falk.</li> <li>Hands On recommendation systems in Python by Rounak Banik.</li> </ul>"}, {"location": "data_analysis/recommender_systems/recommender_systems/#awesome-recommender-systems", "title": "Awesome recommender systems", "text": "<ul> <li>Grahamjenson</li> </ul>"}, {"location": "devops/alex/", "title": "Alex", "text": "<p>Alex helps you find gender favoring, polarizing, race related, religion inconsiderate, or other unequal phrasing in text.</p> <p>For example, <code>when We\u2019ve confirmed his identity is given</code>, <code>alex</code> will warn you and suggest using <code>their</code> instead of <code>his</code>.</p> <p>Give alex a spin on the Online demo.</p>"}, {"location": "devops/alex/#installation", "title": "Installation", "text": "<pre><code>npm install alex --global\n</code></pre> <p>You can use it with Vim through the ALE plugin.</p> <p>As of 2020-07-14, there is no pre-commit available. So I'm going to use it as a soft linter.</p>"}, {"location": "devops/alex/#references", "title": "References", "text": "<ul> <li>Git</li> </ul>"}, {"location": "devops/api_management/", "title": "API Management", "text": "<p>API management is the process of creating and publishing web application programming interfaces (APIs) under a service that:</p> <ul> <li>Enforces the usage of policies.</li> <li>Controls access.</li> <li>Collects and analyzes usage statistics.</li> <li>Reports on performance.</li> </ul>"}, {"location": "devops/api_management/#components", "title": "Components", "text": "<p>While solutions vary, components that provide the following functionality are typically found in API management products:</p> <ul> <li>Gateway: a server that acts as an API front-end, receives API requests,     enforces throttling and security policies, passes requests to the back-end     service and then passes the response back to the requester. A gateway often     includes a transformation engine to orchestrate and modify the requests and     responses on the fly. A gateway can also provide functionality such as     collecting analytics data and providing caching. The gateway can provide     functionality to support authentication, authorization, security, audit and     regulatory compliance.</li> <li>Publishing tools: a collection of tools that API providers use to define     APIs, for instance using the OpenAPI or RAML specifications, generate API     documentation, manage access and usage policies for APIs, test and debug the     execution of API, including security testing and automated generation of     tests and test suites, deploy APIs into production, staging, and quality     assurance environments, and coordinate the overall API lifecycle.</li> <li>Developer portal/API store: community site, typically branded by an API     provider, that can encapsulate for API users in a single convenient source     information and functionality including documentation, tutorials, sample     code, software development kits, an interactive API console and sandbox to     trial APIs, the ability to subscribe to the APIs and manage subscription     keys such as OAuth2 Client ID and Client Secret, and obtain support from the     API provider and user and community.</li> <li>Reporting and analytics: functionality to monitor API usage and load     (overall hits, completed transactions, number of data objects returned,     amount of compute time and other internal resources consumed, volume of data     transferred). This can include real-time monitoring of the API with alerts     being raised directly or via a higher-level network management system, for     instance, if the load on an API has become too great, as well as     functionality to analyze historical data, such as transaction logs, to     detect usage trends.  Functionality can also be provided to create synthetic     transactions that can be used to test the performance and behavior of API     endpoints. The information gathered by the reporting and analytics     functionality can be used by the API provider to optimize the API offering     within an organization's overall continuous improvement process and for     defining software Service-Level Agreements for APIs.</li> <li>Monetization: functionality to support charging for access to commercial     APIs. This functionality can include support for setting up pricing rules,     based on usage, load and functionality, issuing invoices and collecting     payments including multiple types of credit card payments.</li> </ul>"}, {"location": "devops/bandit/", "title": "Bandit", "text": "<p>Bandit finds common security issues in Python code. To do this, Bandit processes each file, builds an AST from it, and runs appropriate plugins against the AST nodes. Once Bandit has finished scanning all the files, it generates a report.</p> <p>You can use this cookiecutter template to create a python project with <code>bandit</code> already configured.</p>"}, {"location": "devops/bandit/#installation", "title": "Installation", "text": "<pre><code>pip install bandit\n</code></pre>"}, {"location": "devops/bandit/#usage", "title": "Usage", "text": ""}, {"location": "devops/bandit/#ignore-an-error", "title": "Ignore an error.", "text": "<p>Add the <code># nosec</code> comment in the line.</p>"}, {"location": "devops/bandit/#configuration", "title": "Configuration", "text": "<p>You can run bandit through:</p> <ul> <li> <p>Pre-commit:</p> <p>File: .pre-commit-config.yaml</p> <pre><code>repos:\n- repo: https://github.com/Lucas-C/pre-commit-hooks-bandit\nrev: v1.0.4\nhooks:\n- id: python-bandit-vulnerability-check\n</code></pre> <p>bandit takes a lot of time to run, so it slows down too much the commiting, therefore it should be run only in the CI.</p> </li> <li> <p>Github Actions: Make sure to check that the correct python version is applied.</p> <p>File: .github/workflows/security.yml</p> <pre><code>name: Security\n\non: [push, pull_request]\n\njobs:\nbandit:\nruns-on: ubuntu-latest\nsteps:\n- name: Checkout\nuses: actions/checkout@v2\n- uses: actions/setup-python@v2\nwith:\npython-version: 3.7\n- name: Install dependencies\nrun: pip install bandit\n- name: Execute bandit\nrun: bandit -r project\n</code></pre> </li> </ul>"}, {"location": "devops/bandit/#references", "title": "References", "text": "<ul> <li>Docs</li> </ul>"}, {"location": "devops/black/", "title": "black", "text": "<p>Black is a style guide enforcement tool.</p> <p>You can use this cookiecutter template to create a python project with <code>black</code> already configured.</p>"}, {"location": "devops/black/#installation", "title": "Installation", "text": "<pre><code>pip install black\n</code></pre>"}, {"location": "devops/black/#configuration", "title": "Configuration", "text": "<p>Its configuration is stored in <code>pyproject.toml</code>.</p> <p>File: pyproject.toml</p> <pre><code># Example configuration for Black.\n\n# NOTE: you have to use single-quoted strings in TOML for regular expressions.\n# It's the equivalent of r-strings in Python.  Multiline strings are treated as\n# verbose regular expressions by Black.  Use [ ] to denote a significant space\n# character.\n\n[tool.black]\nline-length = 88\ntarget-version = ['py36', 'py37', 'py38']\ninclude = '\\.pyi?$'\nexclude = '''\n/(\n\\.eggs\n| \\.git\n| \\.hg\n| \\.mypy_cache\n| \\.tox\n| \\.venv\n| _build\n| buck-out\n| build\n| dist\n# The following are specific to Black, you probably don't want those.\n| blib2to3\n| tests/data\n| profiling\n)/\n'''\n</code></pre> <p>You can use it both with:</p> <ul> <li> <p>The Vim plugin</p> </li> <li> <p>Pre-commit:</p> <p>File: .pre-commit-config.yaml</p> <pre><code>repos:\n- repo: https://github.com/ambv/black\nrev: stable\nhooks:\n- id: black\nlanguage_version: python3.7\n</code></pre> </li> <li> <p>Github Actions:</p> <p>File: .github/workflows/lint.yml</p> <pre><code>---\nname: Lint\n\non: [push, pull_request]\n\njobs:\nBlack:\nruns-on: ubuntu-latest\nsteps:\n- uses: actions/checkout@v2\n- uses: actions/setup-python@v2\n- name: Black\nuses: psf/black@stable\n</code></pre> </li> </ul>"}, {"location": "devops/black/#split-long-lines", "title": "Split long lines", "text": "<p>If you want to split long lines, you need to use the <code>--experimental-string-processing</code> flag. I haven't found how to set that option in the config file.</p>"}, {"location": "devops/black/#disable-the-formatting-of-some-lines", "title": "Disable the formatting of some lines", "text": "<p>You can use the comments <code># fmt: off</code> and <code># fmt: on</code></p>"}, {"location": "devops/black/#references", "title": "References", "text": "<ul> <li>Docs</li> <li>Git</li> </ul>"}, {"location": "devops/ci/", "title": "CI", "text": "<p>Continuous Integration (CI) allows to automatically run processes on the code each time a commit is pushed.  For example it can be used to run the tests, build the documentation, build a package or maintain dependencies updated.</p> <p>I've automated the configuration of CI/CD pipelines for python projects in this cookiecutter template.</p> <p>There are three non exclusive ways to run the tests:</p> <ul> <li>Integrate them in your editor, so it's executed each time you save the file.</li> <li>Through a pre-commit hook to     make it easy for the collaborator to submit correctly formatted code. pre-commit     is a framework for managing and maintaining multi-language     pre-commit hooks.</li> <li>Through a CI server (like Drone or Github Actions) to ensure that the commited     code meets the quality standards. Developers can bypass the pre-commit     filter, so we need to set up the quality gate in an agnostic environment.</li> </ul> <p>Depending on the time the test takes to run and their different implementations, we'll choose from one to three of the choices above.</p>"}, {"location": "devops/ci/#configuring-pre-commit", "title": "Configuring pre-commit", "text": "<p>To adopt <code>pre-commit</code> to our system we have to:</p> <ul> <li>Install pre-commit: <code>pip3 install pre-commit</code> and add it to the development     <code>requirements.txt</code>.</li> <li>Define <code>.pre-commit-config.yaml</code> with the hooks you want to include (they     don't plan to support pyproject.toml).</li> <li>Execute <code>pre-commit install</code> to install git hooks in your <code>.git/</code> directory.</li> <li>Execute <code>pre-commit run --all-files</code> to tests all the files. Usually     <code>pre-commit</code> will only run on the changed files during git hooks.</li> </ul>"}, {"location": "devops/ci/#static-analysis-checkers", "title": "Static analysis checkers", "text": "<p>Static analysis is the analysis of computer software that is performed without actually executing programs.</p>"}, {"location": "devops/ci/#formatters", "title": "Formatters", "text": "<p>Formatters are tools that change your files to meet a linter requirements.</p> <ul> <li>Black: A python style guide formatter tool.</li> </ul>"}, {"location": "devops/ci/#linters", "title": "Linters", "text": "<p>Lint, or a linter, is a static code analysis tool used to flag programming errors, bugs, stylistic errors, and suspicious constructs. The term originates from a Unix utility that examined C language source code.</p> <ul> <li>alex to find gender favoring, polarizing, race related, religion     inconsiderate, or other unequal phrasing in text.</li> <li>Flake8: A python style guide checker tool.</li> <li>markdownlint: A linter for Markdown files.</li> <li>proselint: Is another linter for prose.</li> <li>Yamllint: A linter for YAML files.</li> <li>write-good is a naive linter for English     prose.</li> </ul>"}, {"location": "devops/ci/#type-checkers", "title": "Type checkers", "text": "<p>Type checkers are programs that the code is compliant with a defined type system which is a logical system comprising a set of rules that assigns a property called a type to the various constructs of a computer program, such as variables, expressions, functions or modules. The main purpose of a type system is to reduce possibilities for bugs by defining interfaces between different parts of the program, and then checking that the parts have been connected in a consistent way.</p> <ul> <li>Mypy: A static type checker for Python.</li> </ul>"}, {"location": "devops/ci/#security-vulnerability-checkers", "title": "Security vulnerability checkers", "text": "<p>Tools that check potential vulnerabilities in the code.</p> <ul> <li>Bandit: Finds common security issues in Python code.</li> <li>Safety: Checks your installed dependencies     for known security vulnerabilities.</li> </ul>"}, {"location": "devops/ci/#other-pre-commit-tests", "title": "Other pre-commit tests", "text": "<p>Pre-commit comes with several tests by default. These are the ones I've chosen.</p> <p>File: .pre-commit-config.yaml</p> <pre><code>repos:\n- repo: https://github.com/pre-commit/pre-commit-hooks\nrev: v3.1.0\nhooks:\n- id: trailing-whitespace\n- id: check-added-large-files\n- id: check-docstring-first\n- id: check-merge-conflict\n- id: end-of-file-fixer\n- id: detect-private-key\n</code></pre>"}, {"location": "devops/ci/#update-package-dependencies", "title": "Update package dependencies", "text": "<p>Tools to automatically keep your dependencies updated.</p> <ul> <li>pip-tools</li> </ul>"}, {"location": "devops/ci/#coverage-reports", "title": "Coverage reports", "text": "<p>Coveralls is a service that monitors and writes statistics on the coverage of your repositories. To use them, you'll need to log in with your Github account and enable the repos you want to test.</p> <p>Save the secret in the repository configuration and add this step to your tests job.</p> <pre><code>    - name: Coveralls\nuses: coverallsapp/github-action@master\nwith:\ngithub-token: ${{ secrets.COVERALLS_TOKEN }}\n</code></pre> <p>Add the following badge to your README.md.</p> <p>Variables to substitute:</p> <ul> <li><code>repository_path</code>: Github repository path, like <code>lyz-code/pydo</code>.</li> </ul> <pre><code>[![Coverage Status](https://coveralls.io/repos/github/{{ repository_path\n}}/badge.svg?branch=master)](https://coveralls.io/github/{{ repository_path }}?branch=master)\n</code></pre>"}, {"location": "devops/ci/#troubleshooting", "title": "Troubleshooting", "text": ""}, {"location": "devops/ci/#error-pathspec-master-did-not-match-any-files-known-to-git", "title": "error: pathspec 'master' did not match any file(s) known to git", "text": "<p>If you have this error while making a commit through a pipeline step, it may be the pre-commits stepping in.</p> <p>To fix it, remove all git hooks with <code>rm -r .git/hooks</code>.</p>"}, {"location": "devops/devops/", "title": "Devops", "text": "<p>DevOps is a set of practices that combines software development (Dev) and information-technology operations (Ops) which aims to shorten the systems development life cycle and provide continuous delivery with high software quality.</p> <p>One of the most important goals of the DevOps initiative is to break the silos between the developers and the sysadmins, that lead to ill feelings and unproductivity.</p> <p>It's a relatively new concept, the main ideas emerged in the 1990s and the first conference was in 2009. That means that as of 2021 there is still a lot of debate of what people understand as DevOps.</p>"}, {"location": "devops/devops/#devops-pitfalls", "title": "DevOps pitfalls", "text": "<p>I've found that the DevOps word leads to some pitfalls that we should try to avoid.</p>"}, {"location": "devops/devops/#getting-lost-in-the-label", "title": "Getting lost in the label", "text": "<p>Labels are a language tool used to speed up communication by describing someone or something in a word or short phrase.</p> <p>However, there are times when labels achieve the complete oposite, as it's the case with DevOps, where there are different views on what the label represents and usually one of the communication parties strongly feels they belong to the label while the other doesn't agree. These discussions can fall into an unproductive, agitated semantic debate where each part tries to convince each other.</p> <p>So instead of starting a twitter thread telling people why they aren't a DevOps team, we could invest those energies in creating resources that close the gap between both parties. Similarly, instead of starting an internal discussion of what do we understand as DevOps?, we could discuss how to improve our existent processes so that the team members feel more comfortable contributing to the application or infrastructure code.</p>"}, {"location": "devops/devops/#you-need-to-do-it-all-to-be-awarded-the-devops-pin", "title": "You need to do it all to be awarded the DevOps pin", "text": "<p>I find specially harmful the idea that to be qualified as DevOps you need to develop and maintain the application at the same time as the infrastructure that holds it.</p> <p>To be able to do that in a typical company product you'll need to know (between another thousand more things):</p> <ul> <li>How to operate the cloud infrastructure where the project lives, which can be     AWS, Google Cloud, Azure or/and baremetal servers.</li> <li>Deploy new resources in that infrastructure, which probably would mean knowing     Terraform, Ansible, Docker or/and Kubernetes.</li> <li>How to integrate the new resources with the operations processes, for example:<ul> <li>The monitoring system, so you'll need to know how to use     Prometheus, Nagios, Zabbix or the existent solution.</li> <li>The continuous integration or delivery system, that you'll need to know     how to maintain, so you have to know how it works and how is it built.</li> <li>The backup system.</li> <li>The log centralizer system.</li> </ul> </li> <li>Infrastructure architecture to know what you need to deploy, how and where.</li> <li>To code efficiently in the language that the application is developed in, for     example Java, Python, Rust, Go, PHP or Javascript, in a way that meets the     quality requirements (code style, linters, coverage and documentation).</li> <li>Knowing how to test your code in that language.</li> <li>Software architecture to structure complex code projects in a maintainable     way.</li> <li>The product you're developing to be able to suggest features and fixtures when     the product owner or the stakeholders show their needs.</li> <li>How to make the application user friendly so anyone wants to use it.</li> <li>And don't forget that you also need to do that in a secure way, so you should     also have to know about pentesting, static and dynamic security tools,     common security vulnerabilities...</li> </ul> <p>And I could keep on going forever. Even if you could get there (and you won't), it wouldn't matter, because when you did, the technologies will have changed so much that you will already be outdated and would need to start over.</p> <p>It's the sickness of the fullstack developer. If you make job openings with this mindset you're going to end up with a team of cis white males in their thirties or forties that are used to earn 3 or 4 times the minimum salary. No other kind of people can reach that point and hold it in time.</p> <p>But bare with me a little longer, even if you make there. What happens when the project changes so that you need to:</p> <ul> <li>Change the programming language of your application.</li> <li>Change the cloud provider.</li> <li>Change the deployment system.</li> <li>Change the program architecture.</li> <li>Change the language framework.</li> </ul> <p>Or any other thousand of possible changes? You would need to be able to keep up with them. Noooo way.</p> <p>Luckily you are not alone. You are a cog in a team, that as a whole is able to overcome these changes. That is why we have developers, sysadmins, security, user experience, quality, scrum master and product owners. So each profile can design, create, investigate and learn it's best in their area of expertise. In the merge of all that personal knowledge is where the team thrives.</p> <p>DevOps then, as I understand it, is the philosophy where the developers and sysadmins try their best to break the barriers that separate them. That idea can be brought to earth by for example:</p> <ul> <li>Open discussions on how to:<ul> <li>Improve the development workflow.</li> <li>Make developers or sysadmins life easier.</li> <li>Make it easier to use the sysadmin tools.</li> <li>Make it easier to understand the developers code.</li> </ul> </li> <li>Formations on the technologies or architecture used by either side.</li> <li>A clear documentation that allows either side to catch up with new changes.</li> <li>Periodic meetings to update each other with the changes.</li> <li>Periodic meetings to release the tension that have appeared between     them.</li> <li>Joined design sessions to decide how to solve problems.</li> </ul>"}, {"location": "devops/devops/#learn-path", "title": "Learn path", "text": "<p>DevOps has become a juicy work, if you want to become one, I think you first need to get the basic knowledge of each of them (developing and operating) before being able to unlock the benefits of the combination of both. You can try to learn both at the same time, but I think it can be a daunting task.</p> <p>To get the basic knowledge of the Ops side I would:</p> <ul> <li> <p>Learn basic Linux administration, otherwise you'll be lost.</p> </li> <li> <p>Learn how to be comfortable searching for anything you don't know, most of     your questions are already answered, and even the most senior people spent     a great amount of time searching for solutions in the project's     documentation, Github issues or Stackoverflow.</p> <p>When you start, navigating this knowledge sources is hard and consumes a lot of your life, but it will get easier with the time.</p> </li> <li> <p>Learn how to use Git. If you can, host your own Gitea, if not, use an existing   service such as Gitlab or Github.</p> </li> <li> <p>Learn how to install and maintain services, (that is why I suggested hosting     your own Gitea). If you don't know what to install, take a look at the     awesome     self-hosted list.</p> </li> <li> <p>Learn how to use Ansible, from now on try to deploy every machine with it.</p> </li> <li> <p>Build a small project inside AWS so you can get used to the most   common services (VPC, EC2, S3, Route53, RDS), most of them have free-tier   resources so you don't need to pay anything. You can try also with Google   Cloud or Azure, but I recommend against it.</p> </li> <li> <p>Once you are comfortable with AWS, learn how to use Terraform. You could for   example deploy the previous project. From now on only use Terraform to   provision AWS resources.</p> </li> <li> <p>Get into the CI/CD world hosting your own Drone, if not, use Gitlab   runners or Github   Actions.</p> </li> </ul> <p>To get the basic knowledge of the Dev side I would:</p> <ul> <li> <p>Learn the basics of a programming language, for example Python. There are     thousand sources there on how to do it, books, articles, videos, forums or     courses, choose the one that suits you best.</p> </li> <li> <p>As with the Ops path, get comfortable with git and searching for things you     don't know.</p> </li> <li> <p>As soon as you can, start doing small programming projects that make your life     easier. Coding your stuff is what's going to make you internalize the     learned concepts, by finding solutions to the blocks you encounter.</p> </li> <li> <p>Publish those projects into a public git server, don't be afraid if you code     is good enough, it works for you, you did your best and you should be happy     about it. That's all that matters. By doing so, you'll start collaborating     to the open source world and it will probably force yourself to make your     code better.</p> </li> <li> <p>Step into the TDD world, learn why, how and when to test your code.</p> </li> <li> <p>For those projects that you want to maintain, create CI/CD pipelines     that enhance the quality of your code, by for example running your tests or     some linters.</p> </li> <li> <p>Once you're comfortable, try to collaborate with existent projects (right now     you may not now where to look for projects to collaborate, but when you reach     this point, I promise you will).</p> </li> </ul>"}, {"location": "devops/flake8/", "title": "Flake8", "text": "<p>DEPRECATION: Use Flakehell instead</p> <p>Flake8 doesn't support <code>pyproject.toml</code>, which is becoming the standard, so I suggest using Flakehell instead.</p> <p>Flake8 is a style guide enforcement tool. Its configuration is stored in <code>setup.cfg</code>, <code>tox.ini</code> or <code>.flake8</code>.</p> <p>File: .flake8</p> <pre><code>[flake8]\n# ignore = E203, E266, E501, W503, F403, F401\nmax-line-length = 88\n# max-complexity = 18\n# select = B,C,E,F,W,T4,B9\n</code></pre> <p>You can use it both with:</p> <ul> <li> <p>Pre-commit:</p> <p>File: .pre-commit-config.yaml</p> <pre><code>repos:\n- repo: https://gitlab.com/pycqa/flake8\nrev: master\nhooks:\n- id: flake8\n</code></pre> <ul> <li>Github Actions:</li> </ul> <p>File: .github/workflows/lint.yml</p> <pre><code>name: Lint\n\non: [push, pull_request]\n\njobs:\nFlake8:\nruns-on: ubuntu-latest\nsteps:\n- uses: actions/checkout@v2\n- uses: actions/setup-python@v2\n- name: Flake8\nuses: cclauss/GitHub-Action-for-Flake8@v0.5.0\n</code></pre> </li> </ul>"}, {"location": "devops/flake8/#references", "title": "References", "text": "<ul> <li>Docs</li> </ul>"}, {"location": "devops/helmfile/", "title": "Helmfile", "text": "<p>Helmfile is a declarative spec for deploying Helm charts. It lets you:</p> <ul> <li>Keep a directory of chart value files and maintain changes in version control.</li> <li>Apply CI/CD to configuration changes.</li> <li>Environmental chart promotion.</li> <li>Periodically sync to avoid skew in environments.</li> </ul> <p>To avoid upgrades for each iteration of helm, the helmfile executable delegates to helm - as a result, helm must be installed.</p> <p>All information is saved in the <code>helmfile.yaml</code> file.</p> <p>In case we need custom yamls, we'll use kustomize.</p>"}, {"location": "devops/helmfile/#installation", "title": "Installation", "text": "<p>Helmfile is not yet in the distribution package managers, so you'll need to install it manually.</p> <p>Gather the latest release number.</p> <pre><code>wget {{ bin_url }} -O helmfile_linux_amd64\nchmod +x helmfile_linux_amd64\nmv helmfile_linux_amd64 ~/.local/bin/helmfile\n</code></pre>"}, {"location": "devops/helmfile/#usage", "title": "Usage", "text": ""}, {"location": "devops/helmfile/#how-to-deploy-a-new-chart", "title": "How to deploy a new chart", "text": "<p>When we want to add a new chart, the workflow would be:</p> <ul> <li>Run <code>helmfile deps &amp;&amp; helmfile diff</code> to check that your existing charts are   updated, if they are not, run <code>helmfile apply</code>.</li> <li>Configure the release in <code>helmfile.yaml</code> specifying:   <code>name</code>: Deployment name.</li> <li><code>namespace</code>: K8s namespace to deploy.</li> <li><code>chart</code>: Chart release.</li> <li><code>values</code>: path pointing to the values file created above.</li> <li>Create a directory with the <code>{{ chart_name }}</code>.   <pre><code>mkdir {{ chart_name }}\n</code></pre></li> <li>Get a copy of the chart values inside that directory.   <pre><code>helm inspect values {{ package_name }} &gt; {{ chart_name }}/values.yaml\n</code></pre></li> <li>Edit the <code>values.yaml</code> file according to the chart documentation. Be careful     becase some charts specify the docker image version in the name. Comment out     that line because upgrading the chart version without upgrading the image     tag can break the service.</li> <li>Run <code>helmfile deps</code> to update the lock file.</li> <li>Run <code>helmfile diff</code> to check the changes.</li> <li>Run <code>helmfile apply</code> to apply the changes.</li> </ul>"}, {"location": "devops/helmfile/#keep-charts-updated", "title": "Keep charts updated", "text": "<p>Updating charts with <code>helmfile</code> is easy as long as you don't use environments, you run <code>helmfile deps</code>, then <code>helmfile diff</code> and finally <code>helmfile apply</code>. The tricky business comes when you want to use environments to reuse your helmfile code and don't repeat yourself.</p> <p>This is my suggested workflow, I've opened an issue to see if the developers agree with it:</p> <p>As of today, helmfile doesn't support lock files per environment, that means that the lock file needs to be shared by all of them. At a first sight this is a good idea, because it forces us to have the same versions of the charts in all the environments.</p> <p>The problem comes when you want to upgrade the charts of staging, test that they work and then apply the same changes in production. You'd start the process by running <code>helmfile deps</code>, which will read the helmfiles and update the lock file to the latest version. From this point on you need to be careful on executing the next steps in order so as not to break production.</p> <ul> <li>Tell your team that you're going to do the update operation, so that     they don't try to run <code>helmfile</code> against any environment of the cluster.</li> <li> <p>Run <code>helmfile --environment=staging diff</code> to review the changes to be introduced.</p> <p>To be able to see the differences of long diff files, you can filter it with <code>egrep</code>.</p> <pre><code>helmfile diff | egrep -A20 -B20 \"^.{5}(\\-|\\+)\"\n</code></pre> <p>It will show you all the changed lines with the 20 previous and next ones. * Once you agree on them, run <code>helmfile --environment=staging apply</code> to apply them. * Check that all the helm deployments are well deployed with <code>helm list -A | grep -v deployed</code> * Wait 20 minutes to see if the monitoring system or your fellow partners start yelling at you. * If something breaks up, try to fix it up, if you see it's going to delay you to the point that you're not going to be able to finish the upgrade in your working day, it's better to revert back to the working version of that chart and move on with the next steps. Keep in mind that since you run the <code>apply</code> to the last of the steps of this long process, the team is blocked by you. So prioritize to commit the next stable version to the version control repository. * Once you've checked that all the desired upgrades are working, change the context to the production cluster and run <code>helmfile --environment=production diff</code>. This review should be quick, as it should be the same as the staging one. * Now upgrade the production environment with <code>helmfile --environment=production apply</code>. * Check that all the helm deployments are well deployed with <code>helm list -A | grep -v deployed</code> * Wait another 20 minutes and check that everything is working. * Make a commit with the new lockfile and upload it to the version control repository.</p> </li> </ul> <p>If you want the team to be involved in the review process, you can open a PR with the lock file updated with the <code>WIP</code> state, and upload the relevant diff of staging and production, let the discussion end and then run the apply on staging and then on production if everything goes well.</p> <p>Another ugly solution that I thought was to have a lockfile per environment, and let a Makefile manage them, for example, copying it to <code>helmfile.lock</code> before running any command.</p>"}, {"location": "devops/helmfile/#uninstall-charts", "title": "Uninstall charts", "text": "<p>Helmfile still doesn't remove charts if you remove them from your <code>helmfile.yaml</code>. To remove them you have to either set <code>installed: false</code> in the release candidate and execute <code>helmfile apply</code> or delete the release definition from your helmfile and remove it using standard helm commands.</p>"}, {"location": "devops/helmfile/#force-the-reinstallation-of-everything", "title": "Force the reinstallation of everything", "text": "<p>If you manually changed the deployed resources and want to reset the cluster state to the helmfile one, use <code>helmfile sync</code> which will reinstall all the releases.</p>"}, {"location": "devops/helmfile/#multi-environment-project-structure", "title": "Multi-environment project structure", "text": "<p><code>helmfile</code> can handle environments with many different project structures. Such as the next one:</p> <pre><code>\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 helmfile.yaml\n\u251c\u2500\u2500 vars\n\u2502   \u251c\u2500\u2500 production_secrets.yaml\n\u2502   \u251c\u2500\u2500 production_values.yaml\n\u2502   \u251c\u2500\u2500 default_secrets.yaml\n\u2502   \u2514\u2500\u2500 default_values.yaml\n\u251c\u2500\u2500 charts\n\u2502   \u251c\u2500\u2500 local_defined_chart_1\n\u2502   \u2514\u2500\u2500 local_defined_chart_2\n\u251c\u2500\u2500 templates\n\u2502   \u251c\u2500\u2500 environments.yaml\n\u2502   \u2514\u2500\u2500 templates.yaml\n\u251c\u2500\u2500 base\n\u2502   \u251c\u2500\u2500 README.md\n\u2502   \u251c\u2500\u2500 helmfile.yaml\n\u2502   \u251c\u2500\u2500 helmfile.lock\n\u2502   \u251c\u2500\u2500 repos.yaml\n\u2502   \u251c\u2500\u2500 chart_1\n\u2502   \u2502   \u251c\u2500\u2500 secrets.yaml\n\u2502   \u2502   \u251c\u2500\u2500 values.yaml\n\u2502   \u2502   \u251c\u2500\u2500 production_secrets.yaml\n\u2502   \u2502   \u251c\u2500\u2500 production_values.yaml\n\u2502   \u2502   \u251c\u2500\u2500 default_secrets.yaml\n\u2502   \u2502   \u2514\u2500\u2500 default_values.yaml\n\u2502   \u2514\u2500\u2500 chart_2\n\u2502       \u251c\u2500\u2500 secrets.yaml\n\u2502       \u251c\u2500\u2500 values.yaml\n\u2502       \u251c\u2500\u2500 production_secrets.yaml\n\u2502       \u251c\u2500\u2500 production_values.yaml\n\u2502       \u251c\u2500\u2500 default_secrets.yaml\n\u2502       \u2514\u2500\u2500 default_values.yaml\n\u2514\u2500\u2500 service_1\n    \u251c\u2500\u2500 README.md\n    \u251c\u2500\u2500 helmfile.yaml\n    \u251c\u2500\u2500 helmfile.lock\n    \u251c\u2500\u2500 repos.yaml\n    \u251c\u2500\u2500 chart_1\n    \u2502   \u251c\u2500\u2500 secrets.yaml\n    \u2502   \u251c\u2500\u2500 values.yaml\n    \u2502   \u251c\u2500\u2500 production_secrets.yaml\n    \u2502   \u251c\u2500\u2500 production_values.yaml\n    \u2502   \u251c\u2500\u2500 default_secrets.yaml\n    \u2502   \u2514\u2500\u2500 default_values.yaml\n    \u2514\u2500\u2500 chart_2\n        \u251c\u2500\u2500 secrets.yaml\n        \u251c\u2500\u2500 values.yaml\n        \u251c\u2500\u2500 production_secrets.yaml\n        \u251c\u2500\u2500 production_values.yaml\n        \u251c\u2500\u2500 default_secrets.yaml\n        \u2514\u2500\u2500 default_values.yaml\n</code></pre> <p>Where:</p> <ul> <li>There is a general <code>README.md</code> that introduces the repository.</li> <li> <p>Optionally there could be a <code>helmfile.yaml</code> file at the root with a glob     pattern so that it's easy     to run commands on all children helmfiles.</p> <p><pre><code>helmfiles:\n- ./*/helmfile.yaml\n</code></pre> * There is a <code>vars</code> directory to store the variables and secrets shared by the charts that belong to different services. * There is a <code>templates</code> directory to store the helmfile code to reuse through templates and layering. * The project structure is defined by the services hosted in the Kubernetes cluster. Each service contains:</p> <ul> <li>A <code>README.md</code> to document the service implementation.</li> <li>A <code>helmfile.yaml</code> file to configure the service charts.</li> <li>A <code>helmfile.lock</code> to lock the versions of the service charts.</li> <li>A <code>repos.yaml</code> to define the repositories to fetch the charts from.</li> <li>One or more chart directories that contain the environment specific and     shared chart values and secrets.</li> </ul> </li> <li> <p>There is a <code>base</code> service that manages all the charts required to keep the     cluster running, such as the ingress, csi, cni or the cluster-autoscaler.</p> </li> </ul>"}, {"location": "devops/helmfile/#using-helmfile-environments", "title": "Using helmfile environments", "text": "<p>To customize the contents of a <code>helmfile.yaml</code> or <code>values.yaml</code> file per environment, add them under the <code>environments</code> key in the <code>helmfile.yaml</code>:</p> <pre><code>environments:\ndefault:\nproduction:\n</code></pre> <p>The environment name defaults to <code>default</code>, that is, <code>helmfile sync</code> implies the <code>default</code> environment. So it's a good idea to use staging as <code>default</code> to be more robust against human errors. If you want to specify a non-default environment, provide a <code>--environment NAME</code> flag to helmfile like <code>helmfile --environment production sync</code>.</p> <p>In the <code>environments</code> definition we'll load the values and secrets from the <code>vars</code> directory with the next snippet.</p> <pre><code>environments:\ndefault:\nsecrets:\n- ../vars/default_secrets.yaml\nvalues:\n- ../vars/default_values.yaml\nproduction:\nsecrets:\n- ../vars/production_secrets.yaml\nvalues:\n- ../vars/production_values.yaml\n</code></pre> <p>As this snippet is going to be repeated on every <code>helmfile.yaml</code> we'll use a state layering for it.</p> <p>To install a release only in one environment use:</p> <pre><code>environments:\ndefault:\nproduction:\n\n---\n\nreleases:\n- name: newrelic-agent\ninstalled: {{ eq .Environment.Name \"production\" | toYaml }}\n# snip\n</code></pre>"}, {"location": "devops/helmfile/#using-environment-specific-variables", "title": "Using environment specific variables", "text": "<p>Environment Values allows you to inject a set of values specific to the selected environment, into <code>values.yaml</code> templates or <code>helmfile.yaml</code> files. Use it to inject common values from the environment to multiple values files, to make your configuration DRY.</p> <p>Suppose you have three files helmfile.yaml, production.yaml and values.yaml.gotmpl</p> <p>File: <code>helmfile.yaml</code></p> <pre><code>environments:\nproduction:\nvalues:\n- production.yaml\n\n---\n\nreleases:\n- name: myapp\nvalues:\n- values.yaml.gotmpl\n</code></pre> <p>File: <code>production.yaml</code></p> <pre><code>domain: prod.example.com\n</code></pre> <p>File: <code>values.yaml.gotmpl</code></p> <pre><code>domain: {{ .Values | get \"domain\" \"dev.example.com\" }}\n</code></pre> <p>Sadly you can't use templates in the secrets files, so you'll need to repeat the code.</p>"}, {"location": "devops/helmfile/#loading-the-chart-variables-and-secrets", "title": "Loading the chart variables and secrets", "text": "<p>For each chart definition in the <code>helmfile.yaml</code> we need to load it's secrets and values. We could use the next snippet:</p> <pre><code>  - name: chart_1\nvalues:\n- ./chart_1/values.yaml\n- ./chart_1/{{ Environment.Name }}_values.yaml\nsecrets:\n- ./chart_1/secrets.yaml\n- ./chart_1/{{ Environment.Name }}_secrets.yaml\n</code></pre> <p>This assumes that the <code>environment</code> variable is set, as it's going to be shared by all the <code>helmfiles.yaml</code> you can add it to the <code>vars</code> files:</p> <p>File: <code>vars/production_values.yaml</code></p> <pre><code>environment: production\n</code></pre> <p>File: <code>vars/default_values.yaml</code></p> <pre><code>environment: staging\n</code></pre> <p>Instead of <code>.Environment.Name</code>, in theory you could have used <code>.Vars | get \"environment\"</code>, which could have prevented the variables and secrets of the default environment will need to be called <code>default_values.yaml</code>, and <code>default_secrets.yaml</code>, which is misleading. But you can't use <code>.Values</code> in the <code>helmfile.yaml</code> as it's not loaded when the file is parsed, and you get an error. A solution would be to layer the helmfile state files but I wasn't able to make it work.</p>"}, {"location": "devops/helmfile/#avoiding-code-repetition", "title": "Avoiding code repetition", "text": "<p>Besides environments, <code>helmfile</code> gives other useful tricks to prevent the illness of code repetition.</p>"}, {"location": "devops/helmfile/#using-release-templates", "title": "Using release templates", "text": "<p>For each chart in a <code>helmfile.yaml</code> we're going to repeat the <code>values</code> and <code>secrets</code> sections, to avoid it, we can use release templates:</p> <pre><code>templates:\ndefault: &amp;default\n# This prevents helmfile exiting when it encounters a missing file\n# Valid values are \"Error\", \"Warn\", \"Info\", \"Debug\". The default is \"Error\"\n# Use \"Debug\" to make missing files errors invisible at the default log level(--log-level=INFO)\nmissingFileHandler: Warn\nvalues:\n- {{`{{ .Release.Name }}`}}/values.yaml\n- {{`{{ .Release.Name }}`}}/{{`{{ .Values | get \"environment\" }}`}}.yaml\nsecrets:\n- config/{{`{{ .Release.Name }}`}}/secrets.yaml\n- config/{{`{{ .Release.Name }}`}}/{{`{{ .Values | get \"environment\" }}`}}-secrets.yaml\n\nreleases:\n- name: chart_1\nchart: stable/chart_1\n&lt;&lt;: *default\n- name: chart_2\nchart: stable/chart_2\n&lt;&lt;: *default\n</code></pre> <p>If you're not familiar with YAML anchors, <code>&amp;default</code> names the block, then <code>*default</code> references it. The <code>&lt;&lt;:</code> syntax says to \"extend\" (merge) that reference into the current tree.</p> <p>The <code>missingFileHandler: Warn</code> field is necessary if you don't need all the values and secret files, but want to use the same definition for all charts.</p> <p><code>{{` {{ .Release.Name }} `}}</code> is surrounded by <code>{{`</code> and <code>}}`</code> so as not to be executed on the loading time of <code>helmfile.yaml</code>. We need to defer it until each release is actually processed by the <code>helmfile</code> command, such as <code>diff</code> or <code>apply</code>.</p> <p>For more information see this issue.</p>"}, {"location": "devops/helmfile/#layering-the-state", "title": "Layering the state", "text": "<p>You may occasionally end up with many helmfiles that shares common parts like which repositories to use, and which release to be bundled by default.</p> <p>Use Layering to extract the common parts into a dedicated library helmfiles, so that each helmfile becomes DRY.</p> <p>Let's assume that your code looks like:</p> <p>File: <code>helmfile.yaml</code></p> <pre><code>bases:\n- environments.yaml\n\nreleases:\n- name: metricbeat\nchart: stable/metricbeat\n- name: myapp\nchart: mychart\n</code></pre> <p>File: <code>environments.yaml</code></p> <pre><code>environments:\ndevelopment:\nproduction:\n</code></pre> <p>At run time, <code>bases</code> in your <code>helmfile.yaml</code> are evaluated to produce:</p> <pre><code>---\n# environments.yaml\nenvironments:\ndevelopment:\nproduction:\n---\n# helmfile.yaml\nreleases:\n- name: myapp\nchart: mychart\n- name: metricbeat\nchart: stable/metricbeat\n</code></pre> <p>Finally the resulting YAML documents are merged in the order of occurrence, so that your <code>helmfile.yaml</code> becomes:</p> <pre><code>environments:\ndevelopment:\nproduction:\n\nreleases:\n- name: metricbeat\nchart: stable/metricbeat\n- name: myapp\nchart: mychart\n</code></pre> <p>Using this concept, we can reuse the environments section as:</p> <p>File: <code>vars/environments.yaml</code></p> <pre><code>environments:\ndefault:\nsecrets:\n- ../vars/staging-secrets.yaml\nvalues:\n- ../vars/staging-values.yaml\nproduction:\nsecrets:\n- ../vars/production-secrets.yaml\nvalues:\n- ../vars/production-values.yaml\n</code></pre> <p>And the default release templates as:</p> <p>File: <code>templates/templates.yaml</code></p> <pre><code>templates:\ndefault: &amp;default\nvalues:\n- {{`{{ .Release.Name }}`}}/values.yaml\n- {{`{{ .Release.Name }}`}}/{{`{{ .Values | get \"environment\" }}`}}.yaml\nsecrets:\n- config/{{`{{ .Release.Name }}`}}/secrets.yaml\n- config/{{`{{ .Release.Name }}`}}/{{`{{ .Values | get \"environment\" }}`}}-secrets.yaml\n</code></pre> <p>So the service's <code>helmfile.yaml</code> turns out to be:</p> <pre><code>bases:\n- ../templates/environments.yaml\n- ../templates/templates.yaml\n\nreleases:\n- name: chart_1\nchart: stable/chart_1\n&lt;&lt;: *default\n- name: chart_2\nchart: stable/chart_2\n&lt;&lt;: *default\n</code></pre> <p>Much shorter and simple.</p>"}, {"location": "devops/helmfile/#managing-dependencies", "title": "Managing dependencies", "text": "<p>Helmfile support concurrency with the option <code>--concurrency=N</code> so we can take advantage of it and improve our deployment speed, but to ensure it works as expected we have to define the dependencies among charts. For example, if an application needs a database, it has to be deployed before hand.</p> <pre><code>releases:\n- name: vpn-dashboard\nchart: incubator/raw\nneeds:\n- monitoring/prometheus-operator\n- name: prometheus-operator\nnamespace: monitoring\nchart: prometheus-community/kube-prometheus-stack\n</code></pre>"}, {"location": "devops/helmfile/#troubleshooting", "title": "Troubleshooting", "text": ""}, {"location": "devops/helmfile/#yaml-templates-in-go-templates", "title": "Yaml templates in go templates", "text": "<p>If you are using a <code>values.yaml.gotmpl</code> file you won't be able to use <code>{{ whatever }}</code>. The solution is to extract that part to a yaml file and include it in the go template. For example:</p> <ul> <li> <p><code>values.yaml.gotmpl</code>:   <pre><code>metrics:\nserviceMonitor:\n  enabled: true\n  annotations:\n  additionalLabels:\n    release: prometheus-operator\n\n{{ readFile \"prometheus_rules.yaml\" }}\n</code></pre></p> </li> <li> <p><code>prometheus_rules.yaml</code></p> </li> </ul> <pre><code>prometheusRule:\nenabled: true\nadditionalLabels:\nrelease: prometheus-operator\nspec:\n- alert: VeleroBackupPartialFailures\nannotations:\nmessage: Velero backup {{ $labels.schedule }} has {{ $value | humanizePercentage }} partialy failed backups.\nexpr: increase(velero_backup_partial_failure_total{schedule!=\"\"}[1h]) &gt; 0\nfor: 15m\nlabels:\nseverity: warning\n</code></pre>"}, {"location": "devops/helmfile/#error-release-name-has-no-deployed-releases", "title": "Error: \"release-name\" has no deployed releases", "text": "<p>This may happen when you try to install a chart and it fails. The best solution until this issue is resolved is to use <code>helm delete --purge {{ release-name }}</code> and then <code>apply</code> again.</p>"}, {"location": "devops/helmfile/#error-failed-to-download-stablemetrics-server-hint-running-helm-repo-update-may-help", "title": "Error: failed to download \"stable/metrics-server\" (hint: running <code>helm repo update</code> may help)", "text": "<p>I had this issue if <code>verify: true</code> in the helmfile.yaml file. Comment it or set it to false.</p>"}, {"location": "devops/helmfile/#cannot-patch-x-field-is-immutable", "title": "Cannot patch X field is immutable", "text": "<p>You may think that deleting the resource, usually a deployment or daemonset will fix it, but <code>helmfile apply</code> will end without any error, the resource won't be recreated , and if you do a <code>helm list</code>, the deployment will be marked as failed.</p> <p>The solution we've found is disabling the resource in the chart's values so that it's uninstalled an install it again.</p> <p>This can be a problem with the resources that have persistence. To patch it, edit the volume resource with <code>kubectl edit pv -n namespace volume_pvc</code>, change the <code>persistentVolumeReclaimPolicy</code> to <code>Retain</code>, apply the changes to uninstall, and when reinstalling configure the chart to use that volume (easier said than done).</p>"}, {"location": "devops/helmfile/#links", "title": "Links", "text": "<ul> <li>Git</li> </ul>"}, {"location": "devops/jwt/", "title": "JWT", "text": "<p>JWT (JSON Web Token) is a proposed Internet standard for creating data with optional signature and/or optional encryption whose payload holds JSON that asserts some number of claims. The tokens are signed either using a private secret or a public/private key.</p>"}, {"location": "devops/jwt/#references", "title": "References", "text": "<ul> <li>Hasura.io best practices using jwt</li> </ul>"}, {"location": "devops/markdownlint/", "title": "Markdownlint", "text": "<p>markdownlint-cli is a command line interface for the markdownlint Node.js style checker and lint tool for Markdown/CommonMark files.</p> <p>I've evaluated these other projects (1, 2, but their configuration is less user friendly and are less maintained.</p> <p>You can use this cookiecutter template to create a python project with <code>markdownlint</code> already configured.</p>"}, {"location": "devops/markdownlint/#installation", "title": "Installation", "text": "<pre><code>npm install -g markdownlint-cli\n</code></pre>"}, {"location": "devops/markdownlint/#configuration", "title": "Configuration", "text": "<p>To configure your project, add a <code>.markdownlint.json</code> in your project root directory, or in any parent.  I've opened an issue to see if they are going to support <code>pyproject.toml</code> to save the configuration. Check the styles examples.</p> <p>Go to the rules document if you ever need to check more information on a specific rule.</p> <p>You can use it both with:</p> <ul> <li> <p>The Vim through the ALE plugin.</p> </li> <li> <p>Pre-commit:</p> <p>File: .pre-commit-config.yaml</p> <pre><code>- repo: https://github.com/igorshubovych/markdownlint-cli\nrev: v0.23.2\nhooks:\n- id: markdownlint\n</code></pre> </li> </ul>"}, {"location": "devops/markdownlint/#troubleshooting", "title": "Troubleshooting", "text": "<p>Until the #2926 PR is merged you need to change the <code>let l:pattern=.*</code> file to make the linting work to:</p> <p>File: ~/.vim/bundle/ale/autoload/ale/handlers</p> <pre><code>let l:pattern=': \\?\\(\\d*\\):\\? \\(MD\\d\\{3}\\)\\(\\/\\)\\([A-Za-z0-9-\\/]\\+\\)\\(.*\\)$'\n</code></pre>"}, {"location": "devops/markdownlint/#references", "title": "References", "text": "<ul> <li>Git</li> </ul>"}, {"location": "devops/mypy/", "title": "Mypy", "text": "<p>Mypy is an optional static type checker for Python that aims to combine the benefits of dynamic (or \"duck\") typing and static typing. Mypy combines the expressive power and convenience of Python with a powerful type system and compile-time type checking.</p> <p>You can use this cookiecutter template to create a python project with <code>mypy</code> already configured.</p>"}, {"location": "devops/mypy/#installation", "title": "Installation", "text": "<pre><code>pip install mypy\n</code></pre>"}, {"location": "devops/mypy/#configuration", "title": "Configuration", "text": "<p>Mypy configuration is saved in the <code>mypy.ini</code> file, and they don't yet support <code>pyproject.toml</code>.</p> <p>File: mypy.ini</p> <pre><code>[mypy]\nshow_error_codes = True\nfollow_imports = silent\nstrict_optional = True\nwarn_redundant_casts = True\nwarn_unused_ignores = True\ndisallow_any_generics = True\ncheck_untyped_defs = True\nno_implicit_reexport = True\nwarn_unused_configs = True\ndisallow_subclassing_any = True\ndisallow_incomplete_defs = True\ndisallow_untyped_decorators = True\ndisallow_untyped_calls = True\n\n# for strict mypy: (this is the tricky one :-))\ndisallow_untyped_defs = True\n</code></pre> <p>You can use it both with:</p> <ul> <li> <p>Pre-commit:</p> <p>File: .pre-commit-config.yaml</p> <pre><code>repos:\n- repo: https://github.com/pre-commit/mirrors-mypy\nrev: v0.782\nhooks:\n- name: Run mypy static analysis tool\nid: mypy\n</code></pre> </li> <li> <p>Github Actions:</p> <p>File: .github/workflows/lint.yml</p> <pre><code>name: Lint\n\non: [push, pull_request]\n\njobs:\nMypy:\nruns-on: ubuntu-latest\nname: Mypy\nsteps:\n- uses: actions/checkout@v1\n- name: Set up Python 3.7\nuses: actions/setup-python@v1\nwith:\npython-version: 3.7\n- name: Install Dependencies\nrun: pip install mypy\n- name: mypy\nrun: mypy\n</code></pre> </li> </ul> <p>Add <code># type: ignore</code> to the line you want to skip.</p>"}, {"location": "devops/mypy/#ignore-one-line", "title": "Ignore one line", "text": ""}, {"location": "devops/mypy/#troubleshooting", "title": "Troubleshooting", "text": ""}, {"location": "devops/mypy/#module-x-has-no-attribute-y", "title": "Module X has no attribute Y", "text": "<p>If you're importing objects from your own module, you need to tell mypy that those objects are available. To do so in the <code>__init__.py</code> of your module, list them under the <code>__all__</code> variable.</p> <p>File: init.py</p> <pre><code>from .model import Entity\n\n__all__ = [\n    \"Entity\",\n]\n</code></pre>"}, {"location": "devops/mypy/#w0707-consider-explicitly-re-raising-using-the-from", "title": "[W0707: Consider explicitly re-raising using the 'from'", "text": "<p>keyword](https://blog.ram.rachum.com/post/621791438475296768/improving-python-exception-chaining-with)</p> <p>The error can be raised by two cases.</p> <ul> <li>An exception was raised, we were handling it, and something went wrong in the     process of handling it.</li> <li>An exception was raised, and we decided to replace it with a different     exception that will make more sense to whoever called this code.</li> </ul> <pre><code>try:\n  self.connection, _ = self.sock.accept()\nexcept socket.timeout as error:\n  raise IPCException('The socket timed out') from error\n</code></pre> <p>The <code>error</code> bit at the end tells Python: The <code>IPCException</code> that we\u2019re raising is just a friendlier version of the <code>socket.timeout</code> that we just caught.</p> <p>When we run that code and reach that exception, the traceback is going to look like this:</p> <pre><code>Traceback (most recent call last):\n  File \"foo.py\", line 19, in\n    self.connection, _ = self.sock.accept()\n  File \"foo.py\", line 7, in accept\n    raise socket.timeout\nsocket.timeout\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"foo.py\", line 21, in\n    raise IPCException('The socket timed out') from e\nIPCException: The socket timed out\n</code></pre> <p>The <code>The above exception was the direct cause of the following exception:</code> part tells us that we are in the second case.</p> <p>If you were dealing with the first one, the message between the two tracebacks would be:</p> <pre><code>During handling of the above exception, another exception occurred:\n</code></pre>"}, {"location": "devops/mypy/#module-typing-has-no-attribute-annotated", "title": "Module \"typing\" has no attribute \"Annotated\"", "text": "<p>This one happens only because <code>annotated</code> is not available in python &lt; 3.9.</p> <pre><code>try:\n    # mypy is complaining that it can't import it, but it's solved below\n    from typing import Annotated # type: ignore\nexcept ImportError:\n    from typing_extensions import Annotated\n</code></pre>"}, {"location": "devops/mypy/#issues", "title": "Issues", "text": "<ul> <li>Incompatible return value with     TypeVar: search for <code>10003</code> in     repository-pattern and fix the <code>type: ignore</code>.</li> </ul>"}, {"location": "devops/mypy/#references", "title": "References", "text": "<ul> <li>Docs</li> <li>Git</li> <li>Homepage</li> </ul>"}, {"location": "devops/pip_tools/", "title": "Pip-tools", "text": "<p>Deprecated: Use poetry instead.</p> <p>Pip-tools is a set of command line tools to help you keep your pip-based packages fresh, even when you've pinned them.</p> <p>For stability reasons it's a good idea to hardcode the dependencies versions. Furthermore, safety needs them to work properly.</p> <p>You can use this cookiecutter template to create a python project with <code>pip-tools</code> already configured.</p> <p>We've got three places where the dependencies are defined:</p> <ul> <li><code>setup.py</code> should declare the loosest possible dependency versions that are     still workable. Its job is to say what a particular package can work with.</li> <li><code>requirements.txt</code> is a deployment manifest that defines an entire     installation job, and shouldn't be thought of as tied to any one package.     Its job is to declare an exhaustive list of all the necessary packages to     make a deployment work.</li> <li><code>requirements-dev.txt</code> Adds the dependencies required for the development of     the program.</li> </ul> <p>Content of examples may be outdated</p> <p>An updated version of setup.py and requirements-dev.in can be found in the cookiecutter template.</p> <p>With pip-tools, the dependency management is trivial.</p> <ul> <li> <p>Install the tool:</p> <pre><code>pip install pip-tools\n</code></pre> </li> <li> <p>Set the general dependencies in the <code>setup.py</code> <code>install_requires</code>.</p> </li> <li> <p>Generate the <code>requirements.txt</code> file:</p> <pre><code>pip-compile -U --allow-unsafe`\n</code></pre> <p>The <code>-U</code> flag will try to upgrade the dependencies, and <code>--allow-unsafe</code> will let you manage the <code>setuptools</code> and <code>pip</code> dependencies.</p> </li> <li> <p>Add the additional testing dependencies in the <code>requirements-dev.in</code> file.</p> <p>File: requirements-dev.in</p> <pre><code>-c requirements.txt\npip-tools\nfactory_boy\npytest\npytest-cov\n</code></pre> <p>The <code>-c</code> line will make <code>pip-compile</code> look at that file for compatibility, but it won't duplicate those requirements in the <code>requirements-dev.txt</code>.</p> </li> <li> <p>Compile the development requirements <code>requirements-dev.txt</code> with <code>pip-compile     dev-requirements.in</code>.</p> </li> <li> <p>If you have another <code>requirements.txt</code> for the mkdocs documentation, run     <code>pip-compile docs/requirements.txt</code>.</p> </li> <li> <p>To sync the virtualenv libraries with the     files,     use <code>sync</code>:</p> <pre><code>python -m piptools sync requirements.txt requirements-dev.txt\n</code></pre> </li> <li> <p>To uninstall all pip packages use     <pre><code>pip freeze | xargs pip uninstall -y\n</code></pre></p> </li> </ul> <p>Trigger hooks:</p> <ul> <li> <p>Pre-commit:</p> <p>File: .pre-commit-config.yaml</p> <pre><code>  - repo: https://github.com/jazzband/pip-tools\nrev: 5.0.0\nhooks:\n- name: Build requirements.txt\nid: pip-compile\n- name: Build dev-requirements.txt\nid: pip-compile\nargs: ['dev-requirements.in']\n- name: Build mkdocs requirements.txt\nid: pip-compile\nargs: ['docs/requirements.txt']\n</code></pre> <p>pip-tools generates different results in the CI than in the development environment breaking the CI without an easy way to fix it. Therefore it should be run by the developers periodically.</p> </li> </ul>"}, {"location": "devops/pip_tools/#references", "title": "References", "text": "<ul> <li>Git</li> </ul>"}, {"location": "devops/proselint/", "title": "Proselint", "text": "<p>Proselint is another linter for prose.</p>"}, {"location": "devops/proselint/#installation", "title": "Installation", "text": "<pre><code>pip install proselint\n</code></pre>"}, {"location": "devops/proselint/#configuration", "title": "Configuration", "text": "<p>It can be configured through the <code>~/.config/proselint/config</code> file, such as:</p> <pre><code>{\n\"checks\": {\n\"typography.diacritical_marks\": false\n}\n}\n</code></pre> <ul> <li> <p>The Vim through the ALE plugin.</p> </li> <li> <p>Pre-commit:</p> <pre><code>- repo: https://github.com/amperser/proselint/\nrev: 0.10.2\nhooks:\n- id: proselint\nexclude: LICENSE|requirements\nfiles: \\.(md|mdown|markdown)$\n</code></pre> </li> </ul>"}, {"location": "devops/proselint/#references", "title": "References", "text": "<ul> <li>Git</li> </ul>"}, {"location": "devops/safety/", "title": "Safety", "text": "<p>Safety checks your installed dependencies for known security vulnerabilities.</p> <p>You can use this cookiecutter template to create a python project with <code>safety</code> already configured.</p>"}, {"location": "devops/safety/#installation", "title": "Installation", "text": "<pre><code>pip install safety\n</code></pre>"}, {"location": "devops/safety/#configuration", "title": "Configuration", "text": "<p>Safety can be used through:</p> <ul> <li> <p>Pre-commit:</p> <p>File: .pre-commit-config.yaml</p> <pre><code>repos:\n- repo: https://github.com/Lucas-C/pre-commit-hooks-safety\nrev: v1.1.3\nhooks:\n- id: python-safety-dependencies-check\n</code></pre> </li> <li> <p>Github Actions: Make sure to check that the correct python version is applied.</p> <p>File: .github/workflows/security.yml</p> <pre><code>name: Security\n\non: [push, pull_request]\n\njobs:\nSafety:\nruns-on: ubuntu-latest\nsteps:\n- name: Checkout\nuses: actions/checkout@v2\n- uses: actions/setup-python@v2\nwith:\npython-version: 3.7\n- name: Install dependencies\nrun: pip install safety\n- name: Execute safety\nrun: safety check\n</code></pre> </li> </ul>"}, {"location": "devops/safety/#references", "title": "References", "text": "<ul> <li>Git</li> </ul>"}, {"location": "devops/write_good/", "title": "Write Good", "text": "<p>write-good is a naive linter for English prose.</p>"}, {"location": "devops/write_good/#installation", "title": "Installation", "text": "<pre><code>npm install -g write-good\n</code></pre> <p>There is no way to configure it through a configuration file, but it accepts command line arguments.</p> <p>The ALE vim implementation supports the specification of such flags with the <code>ale_writegood_options</code> variable:</p> <pre><code>let g:ale_writegood_options = \"--no-passive\"\n</code></pre> <p>Use <code>write-good --help</code> to see the available flags.</p> <p>As of 2020-07-14, there is no pre-commit available. So I'm going to use it as a soft linter.</p>"}, {"location": "devops/write_good/#references", "title": "References", "text": "<ul> <li>Git</li> </ul>"}, {"location": "devops/yamllint/", "title": "Yamllint", "text": "<p>Yamllint is a linter for YAML files.</p> <p><code>yamllint</code> does not only check for syntax validity, but for weirdnesses like key repetition and cosmetic problems such as lines length, trailing spaces or indentation.</p> <p>You can use it both with:</p> <ul> <li> <p>The Vim through the ALE plugin.</p> </li> <li> <p>Pre-commit:</p> <p>File: .pre-commit-config.yaml</p> <pre><code>- repo: https://github.com/adrienverge/yamllint\nrev: v1.21.0\nhooks:\n- id: yamllint\n</code></pre> </li> </ul>"}, {"location": "devops/yamllint/#references", "title": "References", "text": "<ul> <li>Git</li> <li>Docs</li> </ul>"}, {"location": "devops/aws/aws/", "title": "AWS", "text": "<p>Amazon Web Services (AWS) is a subsidiary of Amazon that provides on-demand cloud computing platforms and APIs to individuals, companies, and governments, on a metered pay-as-you-go basis. In aggregate, these cloud computing web services provide a set of primitive abstract technical infrastructure and distributed computing building blocks and tools.</p>", "tags": ["WIP"]}, {"location": "devops/aws/aws/#learn-path", "title": "Learn path", "text": "<p>TBD</p>", "tags": ["WIP"]}, {"location": "devops/aws/aws/#aws-snippets", "title": "AWS snippets", "text": "", "tags": ["WIP"]}, {"location": "devops/aws/aws/#stop-an-ec2-instance", "title": "Stop an EC2 instance", "text": "<pre><code>aws ec2 stop-instances --instance-ids i-xxxxxxxx\n</code></pre>", "tags": ["WIP"]}, {"location": "devops/aws/aws/#references", "title": "References", "text": "<ul> <li>Compare ec2 instance types</li> </ul>", "tags": ["WIP"]}, {"location": "devops/aws/eks/", "title": "EKS", "text": "<p>Amazon Elastic Kubernetes Service (Amazon EKS) is a managed service that makes it easy for you to run Kubernetes on AWS without needing to stand up or maintain your own Kubernetes control plane.</p>"}, {"location": "devops/aws/eks/#pod-limit-per-node", "title": "Pod limit per node", "text": "<p>AWS EKS supports native VPC networking with the Amazon VPC Container Network Interface (CNI) plugin for Kubernetes. Using this plugin allows Kubernetes Pods to have the same IP address inside the pod as they do on the VPC network.</p> <p>This is a great feature but it introduces a limitation in the number of Pods per EC2 Node instance. Whenever you deploy a Pod in the EKS worker Node, EKS creates a new IP address from VPC subnet and attach to the instance.</p> <p>The formula for defining the maximum number of pods per instance is as follows:</p> <pre><code>N * (M-1) + 2\n</code></pre> <p>Where:</p> <ul> <li><code>N</code> is the number of Elastic Network Interfaces (ENI) of the instance type.</li> <li><code>M</code> is the number of IP addresses of a single ENI.</li> </ul> <p>So, for <code>t3.small</code>, this calculation is <code>3 * (4-1) + 2 = 11</code>. For a list of all the instance types and their limits see this document</p>"}, {"location": "devops/aws/eks/#upgrade-an-eks-cluster", "title": "Upgrade an EKS cluster", "text": "<p>New Kubernetes versions introduce significant changes, so it's recommended that you test the behavior of your applications against a new Kubernetes version before performing the update on your production clusters.</p> <p>The update process consists of Amazon EKS launching new API server nodes with the updated Kubernetes version to replace the existing ones. Amazon EKS performs standard infrastructure and readiness health checks for network traffic on these new nodes to verify that they are working as expected. If any of these checks fail, Amazon EKS reverts the infrastructure deployment, and your cluster remains on the prior Kubernetes version. Running applications are not affected, and your cluster is never left in a non-deterministic or unrecoverable state. Amazon EKS regularly backs up all managed clusters, and mechanisms exist to recover clusters if necessary. We are constantly evaluating and improving our Kubernetes infrastructure management processes.</p> <p>To upgrade a cluster follow these steps:</p> <ul> <li>Upgrade all your charts to the latest version with helmfile.     <pre><code>helmfile deps\nhelmfile apply\n</code></pre></li> <li>Check your current version and compare it with the one you want to upgrade.     <pre><code>kubectl version --short\nkubectl get nodes\n</code></pre></li> <li>Check the     docs     to see if the version you want to upgrade requires some special steps.</li> <li>If your worker nodes aren't at the same version as the cluster control plane     upgrade them to the control plane version (never higher).</li> <li> <p>Edit the <code>cluster_version</code> attribute of the eks terraform module and apply the     changes (reviewing them first).     <pre><code>terraform apply\n</code></pre></p> <p>This is a long step (approximately 40 minutes) * Upgrade your charts again.</p> </li> </ul>"}, {"location": "devops/aws/eks/#references", "title": "References", "text": "<ul> <li>Docs</li> </ul>"}, {"location": "devops/aws/s3/", "title": "S3", "text": "<p>S3 is the secure, durable, and scalable object storage infrastructure of AWS. Often used for serving static website content or holding backups or data.</p>"}, {"location": "devops/aws/s3/#commands", "title": "Commands", "text": ""}, {"location": "devops/aws/s3/#bucket-management", "title": "Bucket management", "text": ""}, {"location": "devops/aws/s3/#list-buckets", "title": "List buckets", "text": "<pre><code>aws s3 ls\n</code></pre>"}, {"location": "devops/aws/s3/#create-bucket", "title": "Create bucket", "text": "<pre><code>aws s3api create-bucket \\\n--bucket {{ bucket_name }} \\\n--create-bucket-configuration LocationConstraint=us-east-1\n</code></pre>"}, {"location": "devops/aws/s3/#enable-versioning", "title": "Enable versioning", "text": "<pre><code>aws s3api put-bucket-versioning --bucket {{ bucket_name }} --versioning-configuration Status=Enabled\n</code></pre>"}, {"location": "devops/aws/s3/#enable-encryption", "title": "Enable encryption", "text": "<pre><code>aws s3api put-bucket-encryption --bucket {{ bucket_name }} \\\n--server-side-encryption-configuration='{\n  \"Rules\":\n  [\n    {\n      \"ApplyServerSideEncryptionByDefault\":\n      {\n        \"SSEAlgorithm\": \"AES256\"\n      }\n    }\n  ]\n}'\n</code></pre>"}, {"location": "devops/aws/s3/#download-bucket", "title": "Download bucket", "text": "<pre><code>aws s3 cp --recursive s3://{{ bucket_name }} .\n</code></pre>"}, {"location": "devops/aws/s3/#audit-the-s3-bucket-policy", "title": "Audit the S3 bucket policy", "text": "<pre><code>IFS=$(echo -en \"\\n\\b\")\nfor bucket in `aws s3 ls | awk '{ print $3 }'`\ndo echo \"Bucket $bucket:\"\naws s3api get-bucket-acl --bucket \"$bucket\"\ndone\n</code></pre>"}, {"location": "devops/aws/s3/#add-cache-header-to-all-items-in-a-bucket", "title": "Add cache header to all items in a bucket", "text": "<ul> <li>Log in to AWS Management Console.</li> <li>Go into S3 bucket.</li> <li>Select all files by route.</li> <li>Choose \"More\" from the menu.</li> <li>Select \"Change metadata\".</li> <li>In the \"Key\" field, select \"Cache-Control\" from the drop down menu     max-age=604800Enter (7 days in seconds) for Value.</li> <li>Press \"Save\" button.</li> </ul>"}, {"location": "devops/aws/s3/#object-management", "title": "Object management", "text": ""}, {"location": "devops/aws/s3/#remove-an-object", "title": "Remove an object", "text": "<pre><code>aws s3 rm s3://{{ bucket_name }}/{{ path_to_file }}\n</code></pre>"}, {"location": "devops/aws/s3/#upload", "title": "Upload", "text": ""}, {"location": "devops/aws/s3/#upload-a-local-file-with-the-cli", "title": "Upload a local file with the cli", "text": "<pre><code>aws s3 cp {{ path_to_file }} s3://{{ bucket_name }}/{{ upload_path }}\n</code></pre>"}, {"location": "devops/aws/s3/#upload-a-file-unauthenticated", "title": "Upload a file unauthenticated", "text": "<pre><code>curl --request PUT --upload-file test.txt https://{{ bucket_name }}.s3.amazonaws.com/uploads/\n</code></pre>"}, {"location": "devops/aws/s3/#restore-an-object", "title": "Restore an object", "text": "<p>First you need to get the version of the object</p> <pre><code>aws s3api list-object-versions \\\n--bucket {{ bucket_name }} \\\n--prefix {{ bucket_path_to_file }}\n</code></pre> <p>Fetch the <code>VersionId</code> and download the file</p> <pre><code>aws s3api get-object \\\n--bucket {{ bucket_name }} \\\n--key {{ bucket_path_to_file }} \\\n--version-id {{ versionid }}\n</code></pre> <p>Once you have it, overwrite the same object in the same path</p> <pre><code>aws s3 cp \\\n{{ local_path_to_restored_file }} \\\ns3://{{ bucket_name }}/{{ upload_path }}\n</code></pre>"}, {"location": "devops/aws/s3/#copy-objects-between-buckets", "title": "Copy objects between buckets", "text": "<pre><code>aws s3 sync s3://SOURCE_BUCKET_NAME s3://NEW_BUCKET_NAME\n</code></pre>"}, {"location": "devops/aws/s3/#troubleshooting", "title": "Troubleshooting", "text": ""}, {"location": "devops/aws/s3/#get_environ_proxies-missing-1-required-positional-argument-no_proxy", "title": "get_environ_proxies() missing 1 required positional argument: 'no_proxy'", "text": "<pre><code>sudo pip3 install --upgrade boto3\n</code></pre>"}, {"location": "devops/aws/s3/#links", "title": "Links", "text": "<ul> <li>User guide</li> </ul>"}, {"location": "devops/aws/security_groups/", "title": "Security groups", "text": "<p>Security groups are the AWS way of defining firewall rules between the resources. If not handled properly they can soon become hard to read, which can lead to an insecure infrastructure.</p> <p>It has helped me to use four types of security groups:</p> <ul> <li>Default security groups: Security groups created by AWS per VPC and region,     they can't be deleted.</li> <li>Naming security groups: Used to identify an aws resource. They are usually     referenced in other security groups.</li> <li>Ingress security groups: Used to define the rules of ingress traffic to the     resource.</li> <li>Egress security groups: Used to define the rules of egress traffic to the     resource.</li> </ul> <p>But what helped most has been using clinv while refactoring all the security groups.</p> <p>With <code>clinv unused</code> I got rid of all the security groups that weren't used by any AWS resource (beware of #16, 17, #18 and #19), then used the <code>clinv unassigned security_groups</code> to methodically decide if they were correct and add them to my inventory or if I needed to refactor them.</p>"}, {"location": "devops/aws/security_groups/#best-practices", "title": "Best practices", "text": "<ul> <li>Follow a naming convention.</li> <li>Avoid as much as you can the use of CIDRs in the definition of security     groups. Instead, use naming security groups as much as you can. This will     probably mean that you'll need to create security rules for each service     that is going to use the security group. It is cumbersome but from     a security point of view we gain traceability.</li> <li>Follow the principle of least privileges. Open the least number of ports     required for the service to work.</li> <li>Reuse existing security groups. If there is a security group for web servers     that uses port 80, don't create the new service using port 8080.</li> <li>Remove all rules from the default security groups and don't use them.</li> <li>Don't define the rules in the <code>aws_security_group</code> terraform resource. Use     <code>aws_security_group_rules</code> for each security group to avoid creation     dependency loops.</li> <li>Add descriptions to each security group and security group rule.</li> <li>Avoid using port ranges in the security group rule definitions, as you     probably won't need them.</li> </ul>"}, {"location": "devops/aws/security_groups/#naming-convention", "title": "Naming convention", "text": "<p>A naming convention is required once the number of security groups starts to grow, both to understand them and to be able to search in them.</p> <p>Note</p> <p>It is assumed that terraform is used to create the resources</p>"}, {"location": "devops/aws/security_groups/#default-security-groups", "title": "Default security groups", "text": "<p>There are going to be two kinds of default security groups:</p> <ul> <li>VPC default security groups.</li> <li>Region default security groups.</li> </ul> <p>For the first one we'll use:</p> <pre><code>resource \"aws_default_security_group\" \"{{ region_id }}_{{ vpc_friendly_identifier }}\" {\nvpc_id = \"{{ vpc_id }}\"\n}\n</code></pre> <p>Where:</p> <ul> <li><code>region_id</code> is the region identifier with underscores, for example <code>us_east_1</code></li> <li><code>vpc_friendly_identifier</code> is a human understandable identifier, such as     <code>publicdmz</code>.</li> <li><code>vpc_id</code> is the VPC id such as <code>vpc-xxxxxxxxxxxxxxxxx</code>.</li> </ul> <p>For the second one:</p> <pre><code>resource \"aws_default_security_group\" \"{{ region_id }}\" {\nprovider = aws.{{ region_id }}\n}\n\nWhere the provider must be configured in the `terraform_config.tf` file, for\nexample:\n\n```terraform\nprovider \"aws\" {\nalias  = \"us_west_2\"\nregion = \"us-west-2\"\n}\n</code></pre>"}, {"location": "devops/aws/security_groups/#naming-security-groups", "title": "Naming security groups", "text": "<p>For the naming security groups I've created an UltiSnips template.</p> <pre><code>snippet naming \"naming security group rule\" b\nresource \"aws_security_group\" \"${1:resource_name}_${2:resource_type}\" {\nname        = \"$1-$2\"\ndescription = \"Identify the $1 $2.\"\nvpc_id      = data.terraform_remote_state.vpc.outputs.vpc_${3:vpc}_id\ntags = {\nName = \"$1 $2\"\n}\n}\n\noutput \"$1_$2_id\" {\nvalue = aws_security_group.$1_$2.id\n}\n$0\nendsnippet\n</code></pre> <p>Where:</p> <ul> <li><code>instance_name</code> is a human friendly identifier of the resource that the     security group is going to identify, for example <code>gitea</code>, <code>ci</code> or <code>bastion</code>.</li> <li><code>resource_type</code> identifies the type of resource, such as <code>instance</code> for EC2,     or <code>load_balancer</code> for ELBs.</li> <li><code>vpc</code>: is a human friendly identifier of the vpc. It has to match the outputs     of the vpc resources, as it's going to be also used in the definition of the     vpc used by the security group.</li> </ul> <p>Once you've finished defining the security group, move the <code>output</code> resource to the <code>outputs.tf</code> file.</p>"}, {"location": "devops/aws/security_groups/#ingress-security-groups", "title": "Ingress security groups", "text": "<p>For the ingress security groups I've created another UltiSnips template.</p> <pre><code>snippet ingress \"ingress security group rule\" b\nresource \"aws_security_group\" \"ingress_${1:protocol}_from_${2:destination}_at_${3:vpc}\" {\nname        = \"ingress-$1-from-$2-at-$3\"\ndescription = \"Allow the ingress of $1 traffic from the $2 instances at $3\"\nvpc_id      = data.terraform_remote_state.vpc.outputs.vpc_$3_id\ntags = {\nName = \"Ingress $1 from $2 at $3\"\n}\n}\n\nresource \"aws_security_group_rule\" \"ingress_$1_from_$2_at_$3\" {\ntype              = \"ingress\"\nfrom_port         = ${4:port}\nto_port           = ${5:$4}\nprotocol          = \"${6:tcp}\"\nsecurity_group_id = aws_security_group.ingress_$1_from_$2_at_$3.id\nsource_security_group_id = aws_security_group.$7.id\ndescription      = \"Allow the ingress $1 traffic on port $4 from the $2 instances at $3.\"\n}\n\noutput \"ingress_$1_from_$2_at_$3_id\" {\nvalue = aws_security_group.ingress_$1_from_$2_at_$3.id\n}\n$0\nendsnippet\n</code></pre> <p>Where:</p> <ul> <li><code>protocol</code> is a human friendly identifier of what kind of traffic the security     group is going to allow, for example <code>gitea</code>, <code>ssh</code>, <code>proxy</code> or <code>openvpn</code>.</li> <li><code>destination</code> identifies the resources that are going to use the security     group, for example <code>drone_instance</code>, <code>ldap_instance</code> or <code>everywhere</code>.</li> <li><code>vpc</code>: is a human friendly identifier of the vpc. It has to match the outputs     of the vpc resources, as it's going to be also used in the definition of the     vpc used by the security group.</li> <li><code>port</code> is the port we are going to open.</li> <li><code>$6</code> we assume that the <code>to_port</code> is the same as <code>from_port</code>.</li> <li> <p><code>$7</code> ID of the naming security group that will have access to the particular     security group rule. If you need to use CIDRs for the rule definition,     change that line for the following:</p> <pre><code>cidr_blocks       = [\"{{ cidr }}\"]\n</code></pre> </li> </ul> <p>Once you've finished defining the security group, move the <code>output</code> resource to the <code>outputs.tf</code> file.</p>"}, {"location": "devops/aws/security_groups/#egress-security-groups", "title": "Egress security groups", "text": "<p>For the egress security groups I've created another UltiSnips template.</p> <pre><code>snippet egress \"egress security group rule\" b\nresource \"aws_security_group\" \"egress_${1:protocol}_to_${2:destination}_from_${3:vpc}\" {\nname        = \"egress-$1-to-$2-from-$3\"\ndescription = \"Allow the egress of $1 traffic to the $2 instances at $3\"\nvpc_id      = data.terraform_remote_state.vpc.outputs.vpc_$3_id\ntags = {\nName = \"Egress $1 to $2 at $3\"\n}\n}\n\nresource \"aws_security_group_rule\" \"egress_$1_to_$2_from_$3\" {\ntype              = \"egress\"\nfrom_port         = ${4:port}\nto_port           = ${5:$4}\nprotocol          = \"${6:tcp}\"\nsecurity_group_id = aws_security_group.egress_$1_to_$2_from_$3.id\nsource_security_group_id = aws_security_group.$7.id\ndescription      = \"Allow the egress of $1 traffic on port $4 to the $2 instances at $3.\"\n}\n\noutput \"egress_$1_to_$2_from_$3_id\" {\nvalue = aws_security_group.egress_$1_to_$2_from_$3.id\n}\n$0\nendsnippet\n</code></pre> <p>Where:</p> <ul> <li><code>protocol</code> is a human friendly identifier of what kind of traffic the security     group is going to allow, for example <code>gitea</code>, <code>ssh</code>, <code>proxy</code> or <code>openvpn</code>.</li> <li><code>destination</code> identifies the resources that are going to be accessed by the security     group, for example <code>drone_instance</code>, <code>ldap_instance</code> or <code>everywhere</code>.</li> <li><code>vpc</code>: is a human friendly identifier of the vpc. It has to match the outputs     of the vpc resources, as it's going to be also used in the definition of the     vpc used by the security group.</li> <li><code>port</code> is the port we are going to open.</li> <li><code>$6</code> we assume that the <code>to_port</code> is the same as <code>from_port</code>.</li> <li> <p><code>$7</code> ID of the naming security group that will be accessed by the particular     security group rule. If you need to use CIDRs for the rule definition,     change that line for the following:</p> <pre><code>cidr_blocks       = [\"{{ cidr }}\"]\n</code></pre> </li> </ul> <p>Once you've finished defining the security group, move the <code>output</code> resource to the <code>outputs.tf</code> file.</p>"}, {"location": "devops/aws/security_groups/#instance-security-group-definition", "title": "Instance security group definition", "text": "<p>When defining the security groups in the <code>aws_instance</code> resources, define them in this order:</p> <ul> <li>Naming security groups.</li> <li>Ingress security groups.</li> <li>Egress security groups.</li> </ul> <p>For example</p> <pre><code>resource \"aws_instance\" \"gitea_production\" {\nami               = ...\navailability_zone = ...\nsubnet_id         = ...\nvpc_security_group_ids = [\ndata.terraform_remote_state.security_groups.outputs.gitea_instance_id,\ndata.terraform_remote_state.security_groups.outputs.ingress_http_from_gitea_loadbalancer_at_publicdmz_id,\ndata.terraform_remote_state.security_groups.outputs.ingress_http_from_monitoring_at_privatedmz_id,\ndata.terraform_remote_state.security_groups.outputs.ingress_administration_from_bastion_at_connectiondmz_id,\ndata.terraform_remote_state.security_groups.outputs.egress_ldap_to_ldap_instance_from_publicdmz_id,\ndata.terraform_remote_state.security_groups.outputs.egress_https_to_debian_repositories_from_publicdmz_id,\n]\n</code></pre>"}, {"location": "devops/aws/iam/iam/", "title": "IAM", "text": "<p>AWS Identity and Access Management (IAM) is a web service that helps you securely control access to AWS resources for your users. You use IAM to control who can use your AWS resources (authentication) and what resources they can use and in what ways (authorization).</p> <p>Configurable AWS access controls:</p> <ul> <li>Grant access to AWS Management console, APIs</li> <li>Create individual users</li> <li>Manage permissions with groups</li> <li>Configure a strong password policy</li> <li>Enable Multi-Factor Authentication for privileged users</li> <li>Use IAM roles for EC2 instances</li> <li>Use IAM roles to share access</li> <li>Rotate security credentials regularly</li> <li>Restrict privileged access further with conditions</li> <li>Use your corporate directory system or a third party authentication</li> </ul>"}, {"location": "devops/aws/iam/iam/#links", "title": "Links", "text": "<ul> <li>Docs</li> </ul>"}, {"location": "devops/aws/iam/iam_commands/", "title": "IAM Commands", "text": ""}, {"location": "devops/aws/iam/iam_commands/#information-gathering", "title": "Information gathering", "text": ""}, {"location": "devops/aws/iam/iam_commands/#list-roles", "title": "List roles", "text": "<pre><code>aws iam list-roles --query 'Roles[*].{RoleName: RoleName, RoleId: RoleId}' --output table\n</code></pre>"}, {"location": "devops/aws/iam/iam_commands/#list-policies", "title": "List policies", "text": "<pre><code>aws iam list-policies --query 'Policies[*].{PolicyName: PolicyName, PolicyId: PolicyId}' --output table\n</code></pre>"}, {"location": "devops/aws/iam/iam_commands/#list-attached-policies", "title": "List attached policies", "text": "<pre><code>aws iam list-attached-role-policies --role-name {{ role_name }}\n</code></pre>"}, {"location": "devops/aws/iam/iam_commands/#get-role-configuration", "title": "Get role configuration", "text": "<pre><code>aws iam get-role --role-name {{ role_name }}\n</code></pre>"}, {"location": "devops/aws/iam/iam_commands/#get-role-policies", "title": "Get role policies", "text": "<pre><code>aws iam list-role-policies --role-name {{ role_name }}\n</code></pre>"}, {"location": "devops/aws/iam/iam_debug/", "title": "Problems encountered with AWS IAM", "text": ""}, {"location": "devops/aws/iam/iam_debug/#mfadevice-entity-at-the-same-path-and-name-already-exists", "title": "MFADevice entity at the same path and name already exists", "text": "<p>It happens when a user receives an error while creating her MFA authentication.</p> <p>To solve it:</p> <p>List the existing MFA physical or virtual devices</p> <pre><code>aws iam list-mfa-devices\naws iam list-virtual-mfa-devices\n</code></pre> <p>Delete the conflictive one</p> <pre><code>aws iam delete-virtual-mfa-device --serial-number {{ mfa_arn }}\n</code></pre>"}, {"location": "devops/helm/helm/", "title": "Helm", "text": "<p>Helm is the package manager for Kubernetes. Through charts it helps you define, install and upgrade even the most complex Kubernetes applications.</p> <p> </p> <p>The advantages of using helm over <code>kubectl apply</code> are the easiness of:</p> <ul> <li>Repeatable application installation.</li> <li>CI integration.</li> <li>Versioning and sharing.</li> </ul> <p>Charts are a group of Go templates of kubernetes yaml resource manifests, they are easy to create, version, share, and publish.</p> <p>Helm alone lacks some features, that are satisfied through some external programs:</p> <ul> <li>Helmfile is used to declaratively configure your charts, so   they can be versioned through git.</li> <li>Helm-secrets is used to remove hardcoded credentials from <code>values.yaml</code>   files. Helm has an open issue to   integrate it into it's codebase.</li> <li>Helm-git is used to install helm charts directly from Git     repositories.</li> </ul>"}, {"location": "devops/helm/helm/#troubleshooting", "title": "Troubleshooting", "text": ""}, {"location": "devops/helm/helm/#upgrade-failed-another-operation-installupgraderollback-is-in-progress", "title": "UPGRADE FAILED: another operation (install/upgrade/rollback) is in progress", "text": "<p>This error can happen for few reasons, but it most commonly occurs when there is an interruption during the upgrade/install process as you already mentioned.</p> <p>To fix this one may need to, first rollback to another version, then reinstall or helm upgrade again.</p> <p>Try below command to list the available charts:</p> <pre><code>helm ls --namespace &lt;namespace&gt;\n</code></pre> <p>You may note that when running that command ,it may not show any columns with information. If that's the case try to check the history of the previous deployment</p> <pre><code>helm history &lt;release&gt; --namespace &lt;namespace&gt;\n</code></pre> <p>This provides with information mostly like the original installation was never completed successfully and is pending state something like STATUS: <code>pending-upgrade</code> state.</p> <p>To escape from this state, use the rollback command:</p> <pre><code>helm rollback &lt;release&gt; &lt;revision&gt; --namespace &lt;namespace&gt;\n</code></pre> <p><code>revision</code> is optional, but you should try to provide it.</p> <p>You may then try to issue your original command again to upgrade or reinstall.</p>"}, {"location": "devops/helm/helm/#links", "title": "Links", "text": "<ul> <li>Homepage</li> <li>Docs</li> <li>Git</li> <li>Chart hub</li> <li>Git charts repositories</li> </ul>"}, {"location": "devops/helm/helm_commands/", "title": "Helm Commands", "text": "<p>Small cheatsheet on how to use the <code>helm</code> command.</p>"}, {"location": "devops/helm/helm_commands/#list-charts", "title": "List charts", "text": "<pre><code>helm ls\n</code></pre>"}, {"location": "devops/helm/helm_commands/#get-information-of-chart", "title": "Get information of chart", "text": "<pre><code>helm inspect {{ package_name }}\n</code></pre>"}, {"location": "devops/helm/helm_commands/#list-all-the-available-versions-of-a-chart", "title": "List all the available versions of a chart", "text": "<pre><code>helm search -l {{ package_name }}\n</code></pre>"}, {"location": "devops/helm/helm_commands/#download-a-chart", "title": "Download a chart", "text": "<pre><code>helm fetch {{ package_name }}\n</code></pre> <p>Download and extract <pre><code>helm fetch --untar {{ package_name }}\n</code></pre></p>"}, {"location": "devops/helm/helm_commands/#search-charts", "title": "Search charts", "text": "<pre><code>helm search {{ package_name }}\n</code></pre>"}, {"location": "devops/helm/helm_commands/#operations-you-should-do-with-helmfile", "title": "Operations you should do with helmfile", "text": "<p>The following operations can be done with helm, but consider using helmfile instead.</p>"}, {"location": "devops/helm/helm_commands/#install-chart", "title": "Install chart", "text": "<p>Helm deploys all the pods, replication controllers and services. The pod will be in a pending state while the Docker Image is downloaded and until a Persistent Volume is available. Once complete it will transition into a running state.</p> <pre><code>helm install {{ package_name }}\n</code></pre>"}, {"location": "devops/helm/helm_commands/#give-it-a-name", "title": "Give it a name", "text": "<pre><code>helm install --name {{ release_name }} {{ package_name }}\n</code></pre>"}, {"location": "devops/helm/helm_commands/#give-it-a-namespace", "title": "Give it a namespace", "text": "<pre><code>helm install --namespace {{ namespace }} {{ package_name }}\n</code></pre>"}, {"location": "devops/helm/helm_commands/#customize-the-chart-before-installing", "title": "Customize the chart before installing", "text": "<pre><code>helm inspect values {{ package_name }} &gt; values.yml\n</code></pre> <p>Edit the <code>values.yml</code> <pre><code>helm install -f values.yml {{ package_name }}\n</code></pre></p>"}, {"location": "devops/helm/helm_commands/#upgrade-a-release", "title": "Upgrade a release", "text": "<p>If a new version of the chart is released or you want to change the configuration use</p> <pre><code>helm upgrade -f {{ config_file }} {{ release_name }} {{ package_name }}\n</code></pre>"}, {"location": "devops/helm/helm_commands/#rollback-an-upgrade", "title": "Rollback an upgrade", "text": "<p>First check the revisions <pre><code>helm history {{ release_name }}\n</code></pre></p> <p>Then rollback <pre><code>helm rollback {{ release_name }} {{ revision }}\n</code></pre></p>"}, {"location": "devops/helm/helm_commands/#delete-a-release", "title": "Delete a release", "text": "<pre><code>helm delete --purge {{ release_name }}\n</code></pre>"}, {"location": "devops/helm/helm_commands/#working-with-repositories", "title": "Working with repositories", "text": ""}, {"location": "devops/helm/helm_commands/#list-repositories", "title": "List repositories", "text": "<pre><code>helm repo list\n</code></pre>"}, {"location": "devops/helm/helm_commands/#add-repository", "title": "Add repository", "text": "<pre><code>helm repo add {{ repo_name }} {{ repo_url }}\n</code></pre>"}, {"location": "devops/helm/helm_commands/#update-repositories", "title": "Update repositories", "text": "<pre><code>helm repo update\n</code></pre>"}, {"location": "devops/helm/helm_installation/", "title": "Helm Installation", "text": "<p>There are two usable versions of Helm, v2 and v3, the latter is quite new so some of the things we need to install as of 2020-01-27 are not yet supported (Prometheus operator), so we are going to stick to the version 2.</p> <p>Helm has a client-server architecture, the server is installed in the Kubernetes cluster and the client is a Go executable installed in the user computer.</p>"}, {"location": "devops/helm/helm_installation/#helm-client", "title": "Helm client", "text": "<p>You'll first need to configure kubectl.</p> <p>The binary is still not available in the distribution package managers, so check the latest version in the releases of their git repository, and get the download link of the <code>tar.gz</code>.</p> <pre><code>wget {{ url_to_tar_gz }} -O helm.tar.gz\ntar -xvf helm.tar.gz\nmv linux-amd64/helm ~/.local/bin/\n</code></pre> <p>We are going to use some plugins inside Helmfile, so install them with:</p> <pre><code>helm plugin install https://github.com/jkroepke/helm-secrets\nhelm plugin install https://github.com/databus23/helm-diff\n</code></pre> <p>Now that you've got Helm installed, you'll probably want to install Helmfile.</p>"}, {"location": "devops/helm/helm_secrets/", "title": "Helm Secrets", "text": "<p>Helm-secrets is a helm plugin that manages secrets with Git workflow and stores them anywhere. It delegates the cryptographic operations to Mozilla's Sops tool, which supports PGP, AWS KMS and GCP KMS.</p> <p>The configuration is stored in <code>.sops.yaml</code> files. You can find in Mozilla's documentation a detailed configuration guide. For my use case, I'm only going to use a list of PGP keys, so the following contents should be in the <code>.sops.yaml</code> file at the project root directory.</p> <pre><code>creation_rules:\n- pgp: &gt;-\n{{ gpg_key_1 }},\n{{ gpg_key_2}}\n</code></pre>"}, {"location": "devops/helm/helm_secrets/#installation", "title": "Installation", "text": "<p>Weirdly, <code>helm plugin install https://github.com/jkroepke/helm-secrets --version v3.9.1</code> asks for your github user :S so I'd rather install it by hand.</p> <pre><code>wget https://github.com/jkroepke/helm-secrets/releases/download/v3.9.1/helm-secrets.tar.gz\ntar xvzf helm-secrets.tar.gz -C \"$(helm env HELM_PLUGINS)\"\nrm helm-secrets.tar.gz\n</code></pre> <p>If you're going to use GPG as backend you need to install <code>sops</code>. It's in your distribution repositories, but probably not in the latest version, therefore I suggest you install the binary directly:</p> <ul> <li>Grab the latest release</li> <li>Download, <code>chmod +x</code> and move it somewhere in your <code>$PATH</code>.</li> </ul>"}, {"location": "devops/helm/helm_secrets/#prevent-committing-decrypted-files-to-git", "title": "Prevent committing decrypted files to git", "text": "<p>From the docs:</p> <p>If you like to secure situation when decrypted file is committed by mistake to git you can add your secrets.yaml.dec files to you charts project repository .gitignore.</p> <p>A second level of security is to add for example a .sopscommithook file inside your chart repository local commit hook.</p> <p>This will prevent committing decrypted files without sops metadata.</p> <p>.sopscommithook content example:</p> <pre><code>#!/bin/sh\n\nfor FILE in $(git diff-index HEAD --name-only | grep &lt;your vars dir&gt; | grep \"secrets.y\"); do\nif [ -f \"$FILE\" ] &amp;&amp; ! grep -C10000 \"sops:\" $FILE | grep -q \"version:\"; then\necho \"!!!!! $FILE\" 'File is not encrypted !!!!!'\necho \"Run: helm secrets enc &lt;file path&gt;\"\nexit 1\nfi\ndone\nexit\n</code></pre>"}, {"location": "devops/helm/helm_secrets/#usage", "title": "Usage", "text": ""}, {"location": "devops/helm/helm_secrets/#encrypt-secret-files", "title": "Encrypt secret files", "text": "<p>Imagine you've got a <code>values.yaml</code> with the following information: <pre><code>grafana:\nenabled: true\nadminPassword: admin\n</code></pre></p> <p>If you want to encrypt <code>adminPassword</code>, remove that line from the <code>values.yaml</code> and create a <code>secrets.yaml</code> file with: <pre><code>grafana:\nadminPassword: supersecretpassword\n</code></pre></p> <p>And encrypt the file. <pre><code>helm secrets enc secrets.yaml\n</code></pre></p> <p>If you use Helmfile, you'll need to add the secrets file to your helmfile.yaml. <pre><code>  values:\n- values.yaml\nsecrets:\n- secrets.yaml\n</code></pre></p> <p>From that point on, <code>helmfile</code> will automatically decrypt the credentials.</p>"}, {"location": "devops/helm/helm_secrets/#edit-secret-files", "title": "Edit secret files", "text": "<pre><code>helm secrets edit secrets.yaml\n</code></pre>"}, {"location": "devops/helm/helm_secrets/#decrypt-secret-files", "title": "Decrypt secret files", "text": "<pre><code>helm secrets dec secrets.yaml\n</code></pre> <p>It will generate a <code>secrets.yaml.dec</code> file that it's not decrypted.</p> <p>Be careful not to add these files to git.</p>"}, {"location": "devops/helm/helm_secrets/#clean-all-the-decrypted-files", "title": "Clean all the decrypted files", "text": "<pre><code>helm secrets clean .\n</code></pre>"}, {"location": "devops/helm/helm_secrets/#add-or-remove-keys", "title": "Add or remove keys", "text": "<p>If you want to add or remove PGP keys from <code>.sops.yaml</code>, you need to execute <code>sops updatekeys -y</code> for each <code>secrets.yaml</code> file in the repository. <code>helm-secrets</code> won't make this process easier for you.</p> <p>Check sops documentation for more options.</p>"}, {"location": "devops/helm/helm_secrets/#links", "title": "Links", "text": "<ul> <li>Git</li> </ul>"}, {"location": "devops/kong/kong/", "title": "Kong", "text": "<p>Kong is a lua application API platform running in Nginx.</p>"}, {"location": "devops/kong/kong/#installation", "title": "Installation", "text": "<p>Kong supports several platforms of which we'll use Kubernetes with the helm chart, as it gives the following advantages:</p> <ul> <li>Kong is configured dynamically and responds to the changes in your     infrastructure.</li> <li>Kong is deployed onto Kubernetes with a Controller, which is responsible for     configuring Kong.</li> <li>All of Kong\u2019s configuration is done using Kubernetes resources, stored in     Kubernetes\u2019 data-store (etcd).</li> <li>Use the power of kubectl (or any custom tooling around kubectl) to configure     Kong and get benefits of all Kubernetes, such as declarative configuration,     cloud-provider agnostic deployments, RBAC, reconciliation of desired state,     and elastic scalability.</li> <li>Kong is configured using a combination of Ingress Resource and Custom Resource     Definitions(CRDs).</li> <li>DB-less by default, meaning Kong has the capability of running without     a database and using only memory storage for entities.</li> </ul> <p>In the <code>helmfile.yaml</code> add the repository and the release:</p> <pre><code>repositories:\n- name: kong\nurl: https://charts.konghq.com\nreleases:\n- name: kong\nnamespace: api-manager\nchart: kong/kong\nvalues:\n- kong/values.yaml\nsecrets:\n- kong/secrets.yaml\n</code></pre> <p>While particularizing the <code>values.yaml</code> keep in mind that:</p> <ul> <li>If you don't want the ingress controller set up <code>ingressController.enabled:     false</code>, and in <code>proxy</code> set <code>service: ClusterIP</code> and <code>ingress.enabled:     true</code>.</li> <li>Kong can be run with or without a database. By default the chart installs it     without database.</li> <li> <p>If you deploy it without database and without the ingress controller, you have     to provide a declarative configuration for Kong to run. It can be provided     using an existing ConfigMap <code>dblessConfig.configMap</code> or the whole     configuration can be put into the <code>values.yaml</code> file for deployment itself,     under the <code>dblessConfig.config</code> parameter.</p> </li> <li> <p>Although kong supports it's own Kubernetes resources     (CRD)     for     plugins     and     consumers,     I've found now way of integrating them into the helm chart, therefore I'm     going to specify everything in the <code>dblessConfig.config</code>.</p> </li> </ul> <p>So the general kong configuration <code>values.yaml</code> would be:</p> <pre><code>dblessConfig:\nconfig:\n_format_version: \"1.1\"\nservices:\n- name: example.com\nurl: https://api.example.com\nplugins:\n- name: key-auth\n- name: rate-limiting\nconfig:\nsecond: 10\nhour: 1000\npolicy: local\nroutes:\n- name: example\npaths:\n- /example\n</code></pre> <p>And the <code>secrets.yaml</code>:</p> <pre><code>consumers:\n- username: lyz\nkeyauth_credentials:\n- key: vRQO6xfBbTY3KRvNV7TbeFUUW7kjBmPhIFcUUxvkm4\n</code></pre> <p>To test that everything works use</p> <pre><code>curl -I https://api.example.com/example -H 'apikey: vRQO6xfBbTY3KRvNV7TbeFUUW7kjBmPhIFcUUxvkm4'\n</code></pre> <p>To add the prometheus monitorization, enable the <code>serviceMonitor.enabled: true</code> and make sure you set the correct labels. There is a grafana official dashboard you can also use.</p>"}, {"location": "devops/kong/kong/#links", "title": "Links", "text": "<ul> <li>Homepage</li> <li>Docs</li> </ul>"}, {"location": "devops/kubectl/kubectl/", "title": "Kubectl", "text": "<p>Kubectl Definition</p> <p>Kubectl is a command line tool for controlling Kubernetes clusters.</p> <p><code>kubectl</code> looks for a file named config in the $HOME/.kube directory. You can specify other kubeconfig files by setting the KUBECONFIG environment variable or by setting the --kubeconfig flag.</p>"}, {"location": "devops/kubectl/kubectl/#resource-types-and-its-aliases", "title": "Resource types and it's aliases", "text": "Resource Name Short Name clusters componentstatuses cs configmaps cm daemonsets ds deployments deploy endpoints ep event ev horizontalpodautoscalers hpa ingresses ing jobs limitranges limits namespaces ns networkpolicies netpol nodes no statefulsets sts persistentvolumeclaims pvc persistentvolumes pv pods po podsecuritypolicies psp podtemplates replicasets rs replicationcontrollers rc resourcequotas quota cronjob secrets serviceaccount sa services svc storageclasses thirdpartyresources"}, {"location": "devops/kubectl/kubectl/#usage", "title": "Usage", "text": ""}, {"location": "devops/kubectl/kubectl/#port-forward-tunnel-to-an-internal-service", "title": "Port forward / Tunnel to an internal service", "text": "<p>If you have a service running in kubernetes and you want to directly access it instead of going through the usual path, you can use <code>kubectl port-forward</code>.</p> <p><code>kubectl port-forward</code> allows using resource name, such as a pod name, service replica set or deployment, to select the matching resource to port forward to. For example, the next commands are equivalent:</p> <pre><code>kubectl port-forward mongo-75f59d57f4-4nd6q 28015:27017\nkubectl port-forward deployment/mongo 28015:27017\nkubectl port-forward replicaset/mongo-75f59d57f4 28015:27017\nkubectl port-forward service/mongo 28015:27017\n</code></pre> <p>The output is similar to this:</p> <pre><code>Forwarding from 127.0.0.1:28015 -&gt; 27017\nForwarding from [::1]:28015 -&gt; 27017\n</code></pre> <p>If you don't need a specific local port, you can let <code>kubectl</code> choose and allocate the local port and thus relieve you from having to manage local port conflicts, with the slightly simpler syntax:</p> <pre><code>$: kubectl port-forward deployment/mongo :27017\n\nForwarding from 127.0.0.1:63753 -&gt; 27017\nForwarding from [::1]:63753 -&gt; 27017\n</code></pre>"}, {"location": "devops/kubectl/kubectl/#run-a-command-against-a-specific-context", "title": "Run a command against a specific context", "text": "<p>If you have multiple contexts and you want to be able to run commands against a context that you have access to but is not your active context you can use the <code>--context</code> global option for all <code>kubectl</code> commands:</p> <pre><code>kubectl get pods --context &lt;context_B&gt;\n</code></pre> <p>To get a list of available contexts use <code>kubectl config get-contexts</code></p>"}, {"location": "devops/kubectl/kubectl/#links", "title": "Links", "text": "<ul> <li>Overview.</li> <li>Cheatsheet.</li> <li>Kbenv: Virtualenv for kubectl.</li> </ul>"}, {"location": "devops/kubectl/kubectl_commands/", "title": "Kubectl Commands", "text": ""}, {"location": "devops/kubectl/kubectl_commands/#configuration-and-context", "title": "Configuration and context", "text": ""}, {"location": "devops/kubectl/kubectl_commands/#add-a-new-cluster-to-your-kubeconf-that-supports-basic-auth", "title": "Add a new cluster to your kubeconf that supports basic auth", "text": "<pre><code>kubectl config set-credentials {{ username }}/{{ cluster_dns }} --username={{ username }} --password={{ password }}\n</code></pre>"}, {"location": "devops/kubectl/kubectl_commands/#create-new-context", "title": "Create new context", "text": "<pre><code>kubectl config set-context {{ context_name }} --user={{ username }} --namespace={{ namespace }}\n</code></pre>"}, {"location": "devops/kubectl/kubectl_commands/#get-current-context", "title": "Get current context", "text": "<pre><code>kubectl config current-context\n</code></pre>"}, {"location": "devops/kubectl/kubectl_commands/#list-contexts", "title": "List contexts", "text": "<pre><code>kubectl config get-contexts\n</code></pre>"}, {"location": "devops/kubectl/kubectl_commands/#switch-context", "title": "Switch context", "text": "<pre><code>kubectl config use-context {{ context_name }}\n</code></pre>"}, {"location": "devops/kubectl/kubectl_commands/#creating-objects", "title": "Creating objects", "text": ""}, {"location": "devops/kubectl/kubectl_commands/#create-resource", "title": "Create Resource", "text": "<pre><code>kubectl create -f {{ resource.definition.yaml | dir.where.yamls.live | url.to.yaml }} --record\n</code></pre>"}, {"location": "devops/kubectl/kubectl_commands/#create-a-configmap-from-a-file", "title": "Create a configmap from a file", "text": "<pre><code>kubectl create configmap {{ configmap_name }} --from-file {{ path/to/file }}\n</code></pre>"}, {"location": "devops/kubectl/kubectl_commands/#deleting-resources", "title": "Deleting resources", "text": ""}, {"location": "devops/kubectl/kubectl_commands/#delete-the-pod-using-the-type-and-name-specified-in-a-file", "title": "Delete the pod using the type and name specified in a file", "text": "<pre><code>kubectl delete -f {{ path_to_file }}\n</code></pre>"}, {"location": "devops/kubectl/kubectl_commands/#delete-pods-and-services-by-name", "title": "Delete pods and services by name", "text": "<pre><code>kubectl delete pod,service {{ pod_names }} {{ service_names }}\n</code></pre>"}, {"location": "devops/kubectl/kubectl_commands/#delete-pods-and-services-by-label", "title": "Delete pods and services by label", "text": "<pre><code>kubectl delete pod,services -l {{ label_name }}={{ label_value }}\n</code></pre>"}, {"location": "devops/kubectl/kubectl_commands/#delete-all-pods-and-services-in-namespace", "title": "Delete all pods and services in namespace", "text": "<pre><code>kubectl -n {{ namespace_name }} delete po,svc --all\n</code></pre>"}, {"location": "devops/kubectl/kubectl_commands/#delete-all-evicted-pods", "title": "Delete all evicted pods", "text": "<pre><code>while read i; do kubectl delete pod \"$i\"; done &lt; &lt;(kubectl get pods | grep -i evicted | sed 's/ .*//g')\n</code></pre>"}, {"location": "devops/kubectl/kubectl_commands/#editing-resources", "title": "Editing resources", "text": ""}, {"location": "devops/kubectl/kubectl_commands/#edit-a-service", "title": "Edit a service", "text": "<pre><code>kubectl edit svc/{{ service_name }}\n</code></pre>"}, {"location": "devops/kubectl/kubectl_commands/#information-gathering", "title": "Information gathering", "text": ""}, {"location": "devops/kubectl/kubectl_commands/#get-credentials", "title": "Get credentials", "text": "<p>Get credentials <pre><code>kubectl config view --minify\n</code></pre></p>"}, {"location": "devops/kubectl/kubectl_commands/#deployments", "title": "Deployments", "text": ""}, {"location": "devops/kubectl/kubectl_commands/#restart-pods-without-taking-the-service-down", "title": "Restart pods without taking the service down", "text": "<pre><code>kubectl rollout deployment {{ deployment_name }}\n</code></pre>"}, {"location": "devops/kubectl/kubectl_commands/#view-status-of-deployments", "title": "View status of deployments", "text": "<pre><code>kubectl get deployments\n</code></pre>"}, {"location": "devops/kubectl/kubectl_commands/#describe-deployments", "title": "Describe Deployments", "text": "<pre><code>kubectl describe deployment {{ deployment_name }}\n</code></pre>"}, {"location": "devops/kubectl/kubectl_commands/#get-images-of-deployment", "title": "Get images of deployment", "text": "<pre><code>kubectl get pods --selector=app={{ deployment_name }} -o json |\\\njq '.items[] | .metadata.name + \": \" + .spec.containers[0].image'\n</code></pre>"}, {"location": "devops/kubectl/kubectl_commands/#nodes", "title": "Nodes", "text": ""}, {"location": "devops/kubectl/kubectl_commands/#list-all-nodes", "title": "List all nodes", "text": "<pre><code>kubectl get nodes\n</code></pre>"}, {"location": "devops/kubectl/kubectl_commands/#check-which-nodes-are-ready", "title": "Check which nodes are ready", "text": "<pre><code>JSONPATH='{range .items[*]}{@.metadata.name}:{range @.status.conditions[*]}{@.type}={@.status};{end}{end}' \\\n&amp;&amp; kubectl get nodes -o jsonpath=$JSONPATH | grep \"Ready=True\"\n</code></pre>"}, {"location": "devops/kubectl/kubectl_commands/#external-ips-of-all-nodes", "title": "External IPs of all nodes", "text": "<pre><code>kubectl get nodes -o jsonpath='{.items[*].status.addresses[?(@.type==\"ExternalIP\")].address}'\n</code></pre>"}, {"location": "devops/kubectl/kubectl_commands/#get-exposed-ports-of-node", "title": "Get exposed ports of node", "text": "<pre><code>export NODE_PORT=$(kubectl get services/kubernetes-bootcamp -o go-template='{{(index .spec.ports 0).nodePort}}')\n</code></pre>"}, {"location": "devops/kubectl/kubectl_commands/#pods", "title": "Pods", "text": ""}, {"location": "devops/kubectl/kubectl_commands/#list-all-pods-in-the-current-namespace", "title": "List all pods in the current namespace", "text": "<pre><code>kubectl get pods\n</code></pre>"}, {"location": "devops/kubectl/kubectl_commands/#list-all-pods-in-all-namespaces", "title": "List all pods in all namespaces", "text": "<pre><code>kubectl get pods --all-namespaces\n</code></pre>"}, {"location": "devops/kubectl/kubectl_commands/#list-all-pods-of-a-selected-namespace", "title": "List all pods of a selected namespace", "text": "<pre><code>kubectl get pods -n {{ namespace }}\n</code></pre>"}, {"location": "devops/kubectl/kubectl_commands/#list-with-more-detail", "title": "List with more detail", "text": "<pre><code>kubectl get pods -o wide\n</code></pre>"}, {"location": "devops/kubectl/kubectl_commands/#get-pods-of-a-selected-deployment", "title": "Get pods of a selected deployment", "text": "<pre><code>kubectl get pods --selector=\"name={{ name }}\"\n</code></pre>"}, {"location": "devops/kubectl/kubectl_commands/#get-pods-of-a-given-label", "title": "Get pods of a given label", "text": "<pre><code>kubectl get pods -l {{ label_name }}\n</code></pre>"}, {"location": "devops/kubectl/kubectl_commands/#get-pods-by-ip", "title": "Get pods by IP", "text": "<pre><code>kubectl get pods -o wide\nNAME                               READY     STATUS    RESTARTS   AGE       IP            NODE\nalpine-3835730047-ggn2v            1/1       Running   0          5d        10.22.19.69   ip-10-35-80-221.ec2.internal\n</code></pre>"}, {"location": "devops/kubectl/kubectl_commands/#sort-pods-by-restart-count", "title": "Sort pods by restart count", "text": "<pre><code>kubectl get pods --sort-by='.status.containerStatuses[0].restartCount'\n</code></pre>"}, {"location": "devops/kubectl/kubectl_commands/#describe-pods", "title": "Describe Pods", "text": "<pre><code>kubectl describe pods {{ pod_name }}\n</code></pre>"}, {"location": "devops/kubectl/kubectl_commands/#get-name-of-pod", "title": "Get name of pod", "text": "<pre><code>pod=$(kubectl get pod --selector={{ selector_label }}={{ selector_value }} -o jsonpath={.items..metadata.name})\n</code></pre>"}, {"location": "devops/kubectl/kubectl_commands/#list-pods-that-belong-to-a-particular-rc", "title": "List pods that belong to a particular RC", "text": "<pre><code>sel=${$(kubectl get rc my-rc --output=json | jq -j '.spec.selector | to_entries | .[] | \"\\(.key)=\\(.value),\"')%?}\necho $(kubectl get pods --selector=$sel --output=jsonpath={.items..metadata.name})\n</code></pre>"}, {"location": "devops/kubectl/kubectl_commands/#show-the-remaining-space-of-a-persistent-volume-claim", "title": "Show the remaining space of a persistent volume claim", "text": "<p>Either look it in Prometheus or run in the pod that has the PVC mounted:</p> <pre><code>kubectl -n &lt;namespace&gt; exec &lt;pod-name&gt; -- df -ah\n</code></pre> <p>You may need to use <code>kubectl get pod &lt;pod-name&gt; -o yaml</code> to know what volume is mounted where.</p>"}, {"location": "devops/kubectl/kubectl_commands/#services", "title": "Services", "text": ""}, {"location": "devops/kubectl/kubectl_commands/#list-services-in-namespace", "title": "List services in namespace", "text": "<pre><code>kubectl get services\n</code></pre> <p>List services sorted by name <pre><code>kubectl get services --sort-by=.metadata.name\n</code></pre></p>"}, {"location": "devops/kubectl/kubectl_commands/#describe-services", "title": "Describe Services", "text": "<pre><code>kubectl describe services {{ service_name }}\nkubectl describe svc {{ service_name }}\n</code></pre>"}, {"location": "devops/kubectl/kubectl_commands/#replication-controller", "title": "Replication controller", "text": ""}, {"location": "devops/kubectl/kubectl_commands/#list-all-replication-controller", "title": "List all replication controller", "text": "<pre><code>kubectl get rc\n</code></pre>"}, {"location": "devops/kubectl/kubectl_commands/#secrets", "title": "Secrets", "text": ""}, {"location": "devops/kubectl/kubectl_commands/#view-status-of-secrets", "title": "View status of secrets", "text": "<pre><code>kubectl get secrets\n</code></pre>"}, {"location": "devops/kubectl/kubectl_commands/#namespaces", "title": "Namespaces", "text": ""}, {"location": "devops/kubectl/kubectl_commands/#view-namespaces", "title": "View namespaces", "text": "<pre><code>kubectl get namespaces\n</code></pre>"}, {"location": "devops/kubectl/kubectl_commands/#limits", "title": "Limits", "text": "<pre><code>kubectl get limitrange\n</code></pre> <pre><code>kubectl describe limitrange limits\n</code></pre>"}, {"location": "devops/kubectl/kubectl_commands/#jobs-and-cronjobs", "title": "Jobs and cronjobs", "text": ""}, {"location": "devops/kubectl/kubectl_commands/#get-cronjobs-of-a-namespace", "title": "Get cronjobs of a namespace", "text": "<pre><code>kubectl get cronjobs -n {{ namespace }}\n</code></pre>"}, {"location": "devops/kubectl/kubectl_commands/#get-jobs-of-a-namespace", "title": "Get jobs of a namespace", "text": "<pre><code>kubectl get jobs -n {{ namespace }}\n</code></pre> <p>You can then describe a specific job to get the pod it created.</p> <pre><code>kubectl describe job -n {{ namespace }} {{ job_name }}\n</code></pre> <p>And now you can see the evolution of the job with:</p> <pre><code>kubectl logs -n {{ namespace }} {{ pod_name }}\n</code></pre>"}, {"location": "devops/kubectl/kubectl_commands/#interacting-with-nodes-and-cluster", "title": "Interacting with nodes and cluster", "text": ""}, {"location": "devops/kubectl/kubectl_commands/#mark-node-as-unschedulable", "title": "Mark node as unschedulable", "text": "<pre><code>kubectl cordon {{ node_name }}\n</code></pre>"}, {"location": "devops/kubectl/kubectl_commands/#mark-node-as-schedulable", "title": "Mark node as schedulable", "text": "<pre><code>kubectl uncordon {{ node_name }}\n</code></pre>"}, {"location": "devops/kubectl/kubectl_commands/#drain-node-in-preparation-for-maintenance", "title": "Drain node in preparation for maintenance", "text": "<pre><code>kubectl drain {{ node_name }}\n</code></pre>"}, {"location": "devops/kubectl/kubectl_commands/#show-metrics-of-all-node", "title": "Show metrics of all node", "text": "<pre><code>kubectl top node\n</code></pre>"}, {"location": "devops/kubectl/kubectl_commands/#show-metrics-of-a-node", "title": "Show metrics of a node", "text": "<pre><code>kubectl top node {{ node_name }}\n</code></pre>"}, {"location": "devops/kubectl/kubectl_commands/#display-addresses-of-the-master-and-servies", "title": "Display addresses of the master and servies", "text": "<pre><code>kubectl cluster-info\n</code></pre>"}, {"location": "devops/kubectl/kubectl_commands/#dump-current-cluster-state-to-stdout", "title": "Dump current cluster state to stdout", "text": "<pre><code>kubectl cluster-info dump\n</code></pre>"}, {"location": "devops/kubectl/kubectl_commands/#dump-current-cluster-state-to-directory", "title": "Dump current cluster state to directory", "text": "<pre><code>kubectl cluster-info dump --output-directory={{ path_to_directory }}\n</code></pre>"}, {"location": "devops/kubectl/kubectl_commands/#interacting-with-pods", "title": "Interacting with pods", "text": ""}, {"location": "devops/kubectl/kubectl_commands/#dump-logs-of-pod", "title": "Dump logs of pod", "text": "<pre><code>kubectl logs {{ pod_name }}\n</code></pre>"}, {"location": "devops/kubectl/kubectl_commands/#dump-logs-of-pod-and-specified-container", "title": "Dump logs of pod and specified container", "text": "<pre><code>kubectl logs {{ pod_name }} -c {{ container_name }}\n</code></pre>"}, {"location": "devops/kubectl/kubectl_commands/#stream-logs-of-pod", "title": "Stream logs of pod", "text": "<pre><code>kubectl logs -f {{ pod_name }}\nkubectl logs -f {{ pod_name }} -c {{ container_name }}\n</code></pre> <p>Another option is to use the kubetail program.</p>"}, {"location": "devops/kubectl/kubectl_commands/#attach-to-running-container", "title": "Attach to running container", "text": "<pre><code>kubectl attach {{ pod_name }} -i\n</code></pre>"}, {"location": "devops/kubectl/kubectl_commands/#get-a-shell-of-a-running-container", "title": "Get a shell of a running container", "text": "<pre><code>kubectl exec {{ pod_name }} -it bash\n</code></pre>"}, {"location": "devops/kubectl/kubectl_commands/#get-a-debian-container-inside-kubernetes", "title": "Get a debian container inside kubernetes", "text": "<pre><code>kubectl run --generator=run-pod/v1 -i --tty debian --image=debian -- bash\n</code></pre>"}, {"location": "devops/kubectl/kubectl_commands/#run-a-pod-in-a-defined-node", "title": "Run a pod in a defined node", "text": "<p>Get the node hostnames with <code>kubectl get nodes</code>, then override the node with:</p> <pre><code>kubectl run mypod --image ubuntu:18.04 --overrides='{\"apiVersion\": \"v1\", \"spec\": {\"nodeSelector\": { \"kubernetes.io/hostname\": \"my-node.internal\" }}}' --command -- sleep 100000000000000\n</code></pre>"}, {"location": "devops/kubectl/kubectl_commands/#get-a-root-shell-of-a-running-container", "title": "Get a root shell of a running container", "text": "<ol> <li> <p>Get the Node where the pod is and the docker ID <pre><code>kubectl describe pod {{ pod_name }}\n</code></pre></p> </li> <li> <p>SSH into the node <pre><code>ssh {{ node }}\n</code></pre></p> </li> <li> <p>Get into docker <pre><code>docker exec -it -u root {{ docker_id }} bash\n</code></pre></p> </li> </ol>"}, {"location": "devops/kubectl/kubectl_commands/#forward-port-of-pod-to-your-local-machine", "title": "Forward port of pod to your local machine", "text": "<pre><code>kubectl port-forward {{ pod_name }} {{ pod_port }}:{{ local_port }}\n</code></pre>"}, {"location": "devops/kubectl/kubectl_commands/#expose-port", "title": "Expose port", "text": "<pre><code>kubectl expose {{ deployment_name }} --type=\"{{ expose_type }}\" --port {{ port_number }}\n</code></pre> <p>Where expose_type is one of: ['NodePort', 'ClusterIP', 'LoadBalancer', 'externalName']</p>"}, {"location": "devops/kubectl/kubectl_commands/#run-command-on-existing-pod", "title": "Run command on existing pod", "text": "<pre><code>kubectl exec {{ pod_name }} -- ls /\nkubectl exec {{ pod_name }} -c {{ container_name }} -- ls /\n</code></pre>"}, {"location": "devops/kubectl/kubectl_commands/#show-metrics-for-a-given-pod-and-its-containers", "title": "Show metrics for a given pod and it's containers", "text": "<pre><code>kubectl top pod {{ pod_name }} --containers\n</code></pre>"}, {"location": "devops/kubectl/kubectl_commands/#extract-file-from-pod", "title": "Extract file from pod", "text": "<pre><code>kubectl cp {{ container_id }}:{{ path_to_file }} {{ path_to_local_file }}\n</code></pre>"}, {"location": "devops/kubectl/kubectl_commands/#scaling-resources", "title": "Scaling resources", "text": ""}, {"location": "devops/kubectl/kubectl_commands/#scale-a-deployment-with-a-specified-size", "title": "Scale a deployment with a specified size", "text": "<pre><code>kubectl scale deployment {{ deployment_name }} --replicas {{ replicas_number }}\n</code></pre>"}, {"location": "devops/kubectl/kubectl_commands/#scale-a-replicaset", "title": "Scale a replicaset", "text": "<pre><code>kubectl scale --replicas={{ replicas_number }} rs/{{ replicaset_name }}\n</code></pre>"}, {"location": "devops/kubectl/kubectl_commands/#scale-a-resource-specified-in-a-file", "title": "Scale a resource specified in a file", "text": "<pre><code>kubectl scale --replicas={{ replicas_number }} -f {{ path_to_yaml }}\n</code></pre>"}, {"location": "devops/kubectl/kubectl_commands/#updating-resources", "title": "Updating resources", "text": ""}, {"location": "devops/kubectl/kubectl_commands/#namespaces_1", "title": "Namespaces", "text": ""}, {"location": "devops/kubectl/kubectl_commands/#temporary-set-the-namespace-for-a-request", "title": "Temporary set the namespace for a request", "text": "<pre><code>kubectl -n {{ namespace_name }} {{ command_to_execute }}\nkubectl --namespace={{ namespace_name }} {{ command_to_execute }}\n</code></pre>"}, {"location": "devops/kubectl/kubectl_commands/#permanently-set-the-namespace-for-a-request", "title": "Permanently set the namespace for a request", "text": "<pre><code>kubectl config set-context $(kubectl config current-context) --namespace={{ namespace_name }}\n</code></pre>"}, {"location": "devops/kubectl/kubectl_commands/#deployment", "title": "Deployment", "text": ""}, {"location": "devops/kubectl/kubectl_commands/#modify-the-image-of-a-deployment", "title": "Modify the image of a deployment", "text": "<p><pre><code>kubectl set image {{ deployment_name }} {{ label }}:{{ label_value }}\n</code></pre> for example <pre><code>kubectl set image deployment/nginx-deployment nginx=nginx:1.9.1\n</code></pre></p> <p>Or edit it by hand <pre><code>kubectl edit {{ deployment_name }}\n</code></pre></p>"}, {"location": "devops/kubectl/kubectl_commands/#get-the-status-of-the-rolling-update", "title": "Get the status of the rolling update", "text": "<pre><code>kubectl rollout status {{ deployment_name }}\n</code></pre>"}, {"location": "devops/kubectl/kubectl_commands/#get-the-history-of-the-deployment", "title": "Get the history of the deployment", "text": "<pre><code>kubectl rollout history deployment {{ deployment_name }}\n</code></pre> <p>To get more details of a selected revision: <pre><code>kubectl rollout history deployment {{ deployment_name }} --revision={{ revision_number }}\n</code></pre></p>"}, {"location": "devops/kubectl/kubectl_commands/#get-back-to-a-specified-revision", "title": "Get back to a specified revision", "text": "<p>To get to the last version <pre><code>kubectl rollout undo deployment {{ deployment_name }}\n</code></pre></p> <p>To go to a specific version <pre><code>kubectl rollout undo {{ deployment_name }} --to-revision={{ revision_number }}\n</code></pre></p>"}, {"location": "devops/kubectl/kubectl_commands/#pods_1", "title": "Pods", "text": ""}, {"location": "devops/kubectl/kubectl_commands/#rolling-update-of-pods", "title": "Rolling update of pods", "text": "<p>Is prefered to use the deployment rollout</p> <pre><code>kubectl rolling-update {{ pod_name }} -f {{ new_pod_definition_yaml }}\n</code></pre>"}, {"location": "devops/kubectl/kubectl_commands/#change-the-name-of-the-resource-and-update-the-image", "title": "Change the name of the resource and update the image", "text": "<pre><code>kubectl rolling-update {{ old_pod_name }} {{ new_pod_name }} --image=image:{{ new_pod_version }}\n</code></pre>"}, {"location": "devops/kubectl/kubectl_commands/#abort-existing-rollout-in-progress", "title": "Abort existing rollout in progress", "text": "<pre><code>kubectl rolling-update {{ old_pod_name }} {{ new_pod_name }} --rollback\n</code></pre>"}, {"location": "devops/kubectl/kubectl_commands/#force-replace-delete-and-then-re-create-the-resource", "title": "Force replace, delete and then re-create the resource", "text": "<p>** Will cause a service outage **</p> <pre><code>kubectl replace --force -f {{ new_pod_yaml }}\n</code></pre>"}, {"location": "devops/kubectl/kubectl_commands/#add-a-label", "title": "Add a label", "text": "<pre><code>kubectl label pods {{ pod_name }} new-label={{ new_label }}\n</code></pre>"}, {"location": "devops/kubectl/kubectl_commands/#autoscale-a-deployment", "title": "Autoscale a deployment", "text": "<pre><code>kubectl autoscale deployment {{ deployment_name }} --min={{ min_instances }} --max={{ max_instances }} [--cpu-percent={{ cpu_percent }}]\n</code></pre>"}, {"location": "devops/kubectl/kubectl_commands/#copy-resources-between-namespaces", "title": "Copy resources between namespaces", "text": "<pre><code>kubectl get rs,secrets -o json --namespace old | jq '.items[].metadata.namespace = \"new\"' | kubectl create-f -\n</code></pre>"}, {"location": "devops/kubectl/kubectl_commands/#formatting-output", "title": "Formatting output", "text": ""}, {"location": "devops/kubectl/kubectl_commands/#print-a-table-using-a-comma-separated-list-of-custom-columns", "title": "Print a table using a comma separated list of custom columns", "text": "<pre><code>-o=custom-columns=&lt;spec&gt;\n</code></pre>"}, {"location": "devops/kubectl/kubectl_commands/#print-a-table-using-the-custom-columns-template-in-the-file", "title": "Print a table using the custom columns template in the  file <pre><code>-o=custom-columns-file=&lt;filename&gt;\n</code></pre>", "text": ""}, {"location": "devops/kubectl/kubectl_commands/#output-a-json-formatted-api-object", "title": "Output a JSON formatted API object <pre><code>-o=json\n</code></pre>", "text": ""}, {"location": "devops/kubectl/kubectl_commands/#print-the-fields-defined-in-a-jsonpath-expression", "title": "Print the fields defined in a jsonpath expression <pre><code>-o=jsonpath=&lt;template&gt;\n</code></pre>", "text": ""}, {"location": "devops/kubectl/kubectl_commands/#print-the-fields-defined-by-the-jsonpath-expression-in-the-file", "title": "Print the fields defined by the jsonpath expression in the  file <pre><code>-o=jsonpath-file=&lt;filename&gt;\n</code></pre>", "text": ""}, {"location": "devops/kubectl/kubectl_commands/#print-only-the-resource-name-and-nothing-else", "title": "Print only the resource name and nothing else <pre><code>-o=name\n</code></pre>", "text": ""}, {"location": "devops/kubectl/kubectl_commands/#output-in-the-plain-text-format-with-any-additional-information-and-for-pods-the-node-name-is-included", "title": "Output in the plain-text format with any additional information, and for pods, the node name is included <pre><code>-o=wide\n</code></pre>", "text": ""}, {"location": "devops/kubectl/kubectl_commands/#output-a-yaml-formatted-api-object", "title": "Output a YAML formatted API object <pre><code>-o=yaml\n</code></pre>", "text": ""}, {"location": "devops/kubectl/kubectl_installation/", "title": "Kubectl Installation", "text": "<p>Kubectl is available in the distribution package managers, </p> <pre><code>sudo apt-get install kubernetes-client\n</code></pre> <p>If you want the latest version you can install it manually.</p> <pre><code>curl -LO \"https://storage.googleapis.com/kubernetes-release/release/$(\\\ncurl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt\\\n)/bin/linux/amd64/kubectl\"\nchmod +x kubectl\nmv kubectl ~/.local/bin/kubectl\n</code></pre>"}, {"location": "devops/kubectl/kubectl_installation/#configure-kubectl", "title": "Configure kubectl", "text": ""}, {"location": "devops/kubectl/kubectl_installation/#set-editor", "title": "Set editor", "text": "<pre><code># File ~/.bashrc\nKUBE_EDITOR=\"vim\"\n</code></pre>"}, {"location": "devops/kubectl/kubectl_installation/#set-auto-completion", "title": "Set auto completion", "text": "<pre><code># File ~/.bashrc\nsource &lt;(kubectl completion bash)\n</code></pre>"}, {"location": "devops/kubectl/kubectl_installation/#configure-eks-cluster", "title": "Configure EKS cluster", "text": "<p>To configure the access to an existing cluster, we'll let aws-cli create the required files:</p> <pre><code>aws eks update-kubeconfig --name {{ cluster_name }}\n</code></pre>"}, {"location": "devops/kubernetes/kubernetes/", "title": "Introduction to Kubernetes", "text": "<p>Kubernetes (commonly stylized as k8s) is an open-source container-orchestration system for automating application deployment, scaling, and management. Developed by Google in Go under the Apache 2.0 license, it was first released on June 7, 2014 reaching 1.0 by July 21, 2015. It works with a range of container tools, including Docker. Many cloud services offer a Kubernetes-based platform or infrastructure as a service (PaaS or IaaS) on which Kubernetes can be deployed as a platform-providing service.  Many vendors also provide their own branded Kubernetes distributions.</p> <p> </p> <p>It has become the standard infrastructure to manage containers in production environments. Docker Swarm would be an alternative but it falls short in features compared with Kubernetes.</p> <p>These are some of the advantages of using Kubernetes:</p> <ul> <li>Widely used in production and actively developed.</li> <li>Ensure high availability of your services with autohealing and autoscaling.</li> <li>Easy, quickly and predictable deployment and promotion of applications.</li> <li>Seamless roll out of features.</li> <li>Optimize hardware use while guaranteeing resource isolation.</li> <li>Easiest way to build multi-cloud and baremetal environments.</li> </ul> <p>Several companies have used Kubernetes to release their own PaaS:</p> <ul> <li>OpenShift by Red Hat.</li> <li>Tectonic by CoreOS.</li> <li>Rancher labs by Rancher.</li> </ul>"}, {"location": "devops/kubernetes/kubernetes/#learn-roadmap", "title": "Learn roadmap", "text": "<p>K8s is huge, and growing at a pace that most mortals can't stay updated unless you work with it daily.</p> <p>This is how I learnt, but probably there are better resources now:</p> <ul> <li>Read containing container chaos kubernetes.</li> <li>Test the katacoda lab.</li> <li>Install Kubernetes in laptop with   minikube.</li> <li>Read K8s concepts.</li> <li>Then K8s tasks.</li> <li>I didn't like the book Getting started with kubernetes</li> <li>I'd personally avoid the book Getting started with   kubernetes,   I didn't like it <code>\u00af\\(\u00b0_o)/\u00af</code>.</li> </ul>"}, {"location": "devops/kubernetes/kubernetes/#tools-to-test", "title": "Tools to test", "text": "<ul> <li> <p>Velero: To backup and migrate Kubernetes resources and     persistent volumes.</p> </li> <li> <p>Popeye is a utility that scans live     Kubernetes cluster and reports potential issues with deployed resources and     configurations. It sanitizes your cluster based on what's deployed and not     what's sitting on disk. By scanning your cluster, it detects     misconfigurations and helps you to ensure that best practices are in place,     thus preventing future headaches. It aims at reducing the cognitive overload     one faces when operating a Kubernetes cluster in the wild. Furthermore, if     your cluster employs a metric-server, it reports potential resources     over/under allocations and attempts to warn you should your cluster run out     of capacity.</p> <p>Popeye is a readonly tool, it does not alter any of your Kubernetes resources in any way!</p> </li> <li> <p>Stern allows you to tail multiple pods on     Kubernetes and multiple containers within the pod. Each result is color     coded for quicker debugging.</p> <p>The query is a regular expression so the pod name can easily be filtered and you don't need to specify the exact id (for instance omitting the deployment id). If a pod is deleted it gets removed from tail and if a new pod is added it automatically gets tailed.</p> <p>When a pod contains multiple containers Stern can tail all of them too without having to do this manually for each one. Simply specify the container flag to limit what containers to show. By default all containers are listened to.</p> </li> <li> <p>Fairwinds' Polaris keeps your     clusters sailing smoothly. It runs a variety of checks to ensure that     Kubernetes pods and controllers are configured using best practices, helping     you avoid problems in the future.</p> </li> <li> <p>kube-hunter hunts for security     weaknesses in Kubernetes clusters. The tool was developed to increase     awareness and visibility for security issues in Kubernetes environments.</p> </li> </ul>"}, {"location": "devops/kubernetes/kubernetes/#references", "title": "References", "text": "<ul> <li>Docs</li> <li>Awesome K8s</li> <li>Katacoda playground</li> <li>Comic</li> </ul>"}, {"location": "devops/kubernetes/kubernetes/#diving-deeper", "title": "Diving deeper", "text": "<ul> <li>Architecture</li> <li>Resources</li> <li>Kubectl</li> <li>Additional Components</li> <li>Networking</li> <li>Helm</li> <li>Tools</li> <li>Debugging</li> </ul>"}, {"location": "devops/kubernetes/kubernetes/#reference", "title": "Reference", "text": "<ul> <li>References</li> <li>API conventions</li> </ul>"}, {"location": "devops/kubernetes/kubernetes_annotations/", "title": "Kubernetes annotations", "text": "<p>Annotations are non-identifying metadata key/value pairs attached to objects, such as pods. Annotations are intended to give meaningful and relevant information to libraries and tools.</p> <p>Annotations, like labels, are key/value maps:</p> <pre><code>\"annotations\": {\n\"key1\" : \"value1\",\n\"key2\" : \"value2\"\n}\n</code></pre> <p>Here are some examples of information that could be recorded in annotations:</p> <ul> <li>Fields managed by a declarative configuration layer. Attaching these fields   as annotations distinguishes them from default values set by clients or   servers, and from auto generated fields and fields set by auto sizing or   auto scaling systems.</li> <li>Build, release, or image information like timestamps, release IDs, git   branch, PR numbers, image hashes, and registry address.</li> <li>Pointers to logging, monitoring, analytics, or audit repositories.</li> <li>Client library or tool information that can be used for debugging purposes,   for example, name, version, and build information.</li> <li>User or tool/system provenance information, such as URLs of related objects   from other ecosystem components.</li> <li>Lightweight rollout tool metadata: for example, config or checkpoints.</li> </ul>"}, {"location": "devops/kubernetes/kubernetes_architecture/", "title": "Kubernetes architecture", "text": "<p>Kubernetes is a combination of components distributed between two kind of nodes, Masters and Workers.</p> <p></p>"}, {"location": "devops/kubernetes/kubernetes_architecture/#master-nodes", "title": "Master Nodes", "text": "<p>Master nodes or Kubernetes Control Plane are the controlling unit for the cluster. They manage scheduling of new containers, configure networking and provide health information.</p> <p>To do so it uses:</p> <ul> <li> <p>kube-api-server   exposes the Kubernetes control plane API validating and configuring data for   the different API objects. It's used by all the components to interact between   themselves.</p> </li> <li> <p>etcd is a \"Distributed   reliable key-value store for the most critical data of a distributed system\".   Kubernetes uses Etcd to store state about the cluster and service discovery   between nodes. This state includes what nodes exist in the cluster, which   nodes they are running on and what containers should be running.</p> </li> <li> <p>kube-scheduler   watches for newly created pods with no assigned node, and selects a node for   them to run on.</p> <p>Factors taken into account for scheduling decisions include individual and collective resource requirements, hardware/software/policy constraints, affinity and anti-affinity specifications, data locality, inter-workload interference and deadlines.</p> </li> <li> <p>kube-controller-manager   runs the following controllers:</p> <ul> <li>Node Controller: Responsible for noticing and responding when nodes go   down.</li> <li>Replication Controller: Responsible for maintaining the correct number   of pods for every replication controller object in the system.</li> <li>Endpoints Controller: Populates the Endpoints object (that is, joins   Services &amp; Pods).</li> <li>Service Account &amp; Token Controllers: Create default accounts and API   access tokens for new namespaces.</li> </ul> </li> <li> <p>cloud-controller-manager   runs controllers that interact with the underlying cloud providers.</p> <ul> <li>Node Controller: For checking the cloud provider to determine if a node   has been deleted in the cloud after it stops responding.</li> <li>Route Controller: For setting up routes in the underlying cloud   infrastructure.</li> <li>Service Controller: For creating, updating and deleting cloud provider   load balancers.</li> <li>Volume Controller: For creating, attaching, and mounting volumes, and   interacting with the cloud provider to orchestrate volumes.</li> </ul> </li> </ul>"}, {"location": "devops/kubernetes/kubernetes_architecture/#worker-nodes", "title": "Worker Nodes", "text": "<p>Worker nodes are the units that hold the application data of the cluster. There can be different types of nodes: CPU architecture, amount of resources (CPU, RAM, GPU) or cloud provider.</p> <p>Each node has the services necessary to run pods:</p> <ul> <li>Container   Runtime:   The software responsible for running containers (Docker, rkt, containerd,   CRI-O).</li> <li>kubelet: The primary \u201cnode   agent\u201d. It works in terms of a PodSpec (a YAML or JSON object that describes   it). <code>kubelet</code> takes a set of PodSpecs from the masters <code>kube-api-server</code> and   ensures that the containers described are running and healthy.</li> <li> <p>kube-proxy is   the network proxy that runs on each node. This reflects services as defined in   the Kubernetes API on each node and can do simple TCP and UDP stream   forwarding or round robin across a set of backends.</p> <p><code>kube-proxy</code> maintains network rules on nodes. These network rules allow network communication to your Pods from network sessions inside or outside of your cluster.</p> <p><code>kube-proxy</code> uses the operating system packet filtering layer if there is one and it's available. Otherwise, it will forward the traffic itself.</p> </li> </ul>"}, {"location": "devops/kubernetes/kubernetes_architecture/#kube-proxy-operation-modes", "title": "kube-proxy operation modes", "text": "<p><code>kube-proxy</code> currently supports three different operation modes:</p> <ul> <li>User space: This mode gets its name because the service routing takes place in   kube-proxy in the user process space instead of in the kernel network stack.   It is not commonly used as it is slow and outdated.</li> <li>iptables: This mode uses Linux kernel-level Netfilter rules to configure all   routing for Kubernetes Services. This mode is the default for kube-proxy on   most platforms. When load balancing for multiple backend pods, it uses   unweighted round-robin scheduling.</li> <li>IPVS (IP Virtual Server): Built on the Netfilter framework, IPVS implements   Layer-4 load balancing in the Linux kernel, supporting multiple load-balancing   algorithms, including least connections and shortest expected delay. This   kube-proxy mode became generally available in Kubernetes 1.11, but it requires   the Linux kernel to have the IPVS modules loaded. It is also not as widely   supported by various Kubernetes networking projects as the iptables mode.</li> </ul>"}, {"location": "devops/kubernetes/kubernetes_architecture/#kubectl", "title": "Kubectl", "text": "<p>The kubectl is the command line client used to communicate with the Masters.</p>"}, {"location": "devops/kubernetes/kubernetes_architecture/#number-of-clusters", "title": "Number of clusters", "text": "<p>You can run a given set of workloads either on few large clusters (with many workloads in each cluster) or on many clusters (with few workloads in each cluster).</p> <p>Here's a table that summarizes the pros and cons of various approaches:</p> <p> Figure: Possibilities of number of clusters from learnk8s.io article</p> <p>Reference to the original article for a full read (it's really good!). I'm going to analyze only the Large shared cluster and Cluster per environment options, as they are the closest to my use case.</p>"}, {"location": "devops/kubernetes/kubernetes_architecture/#one-large-shared-cluster", "title": "One Large shared cluster", "text": "<p>With this option, you run all your workloads in the same cluster. Kubernetes provides namespaces to logically separate portions of a cluster from each other, and in the above case, you could use a separate namespace for each application instance.</p> <p>Pros:</p> <ul> <li> <p>Efficient resource usage: You need to have only one copy of all the     resources that are needed to run and manage a Kubernetes cluster (master     nodes, load balancers, Ingress controllers, authentication, logging, and     monitoring).</p> </li> <li> <p>Cheap: As you avoid the duplication of resources, you require less hardware.</p> </li> <li> <p>Efficient administration: Administering a Kubernetes cluster requires:</p> <ul> <li>Upgrading the Kubernetes version</li> <li>Setting up a CI/CD pipeline</li> <li>Installing a CNI plugin</li> <li>Setting up the user authentication system</li> <li>Installing an admission controller</li> </ul> <p>If you have only a single cluster, you need to do all of this only once.</p> <p>If you have many clusters, then you need to apply everything multiple times, which probably requires you to develop some automated processes and tools for being able to do this consistently.</p> </li> </ul> <p>Cons:</p> <ul> <li> <p>Single point of failure: If you have only one cluster and if that cluster     breaks, then all your workloads are down.</p> <p>There are many ways that something can go wrong:</p> <ul> <li>A Kubernetes upgrade produces unexpected side effects.</li> <li>An cluster-wide component (such as a CNI plugin) doesn't work as expected.</li> <li>An erroneous configuration is made to one of the cluster components.</li> <li>An outage occurs in the underlying infrastructure.</li> </ul> <p>A single incident like this can produce major damage across all your workloads if you have only a single shared cluster.</p> </li> <li> <p>No hard security isolation: If multiple apps run in the same Kubernetes     cluster, this means that these apps share the hardware, network, and     operating system on the nodes of the cluster.</p> <p>Concretely, two containers of two different apps running on the same node are technically two processes running on the same hardware and operating system kernel.</p> <p>Linux containers provide some form of isolation, but this isolation is not as strong as the one provided by, for example, virtual machines (VMs). Under the hood, a process in a container is still just a process running on the host's operating system.</p> <p>This may be an issue from a security point of view \u2014 it theoretically allows unrelated apps to interact with each other in undesired ways (intentionally and unintentionally).</p> <p>Furthermore, all the workloads in a Kubernetes cluster share certain cluster-wide services, such as DNS \u2014 this allows apps to discover the Services of other apps in the cluster.</p> <p>It's important to keep in mind that Kubernetes is designed for sharing, and not for isolation and security.</p> </li> <li> <p>No hard multi-tenancy: Given the many shared resources in a Kubernetes     cluster, there are many ways that different apps can \"step on each other's     toes\".</p> <p>For example, an app may monopolize a certain shared resource, such as the CPU or memory, and thus starve other apps running on the same node.</p> <p>Kubernetes provides various ways to control this behaviour, however it's not trivial to tweak these tools in exactly the right way, and they cannot prevent every unwanted side effect either.</p> </li> <li> <p>Many users: If you have only a single cluster, then many people in your     organisation must have access to this cluster.</p> <p>The more people have access to a system, the higher the risk that they break something.</p> <p>Within the cluster, you can control who can do what with role-based access control (RBAC) \u2014 however, this still can't prevent that users break something within their area of authorisation.</p> </li> </ul>"}, {"location": "devops/kubernetes/kubernetes_architecture/#cluster-per-environment", "title": "Cluster per environment", "text": "<p>With this option you have a separate cluster for each environment.</p> <p>For example, you can have a <code>dev</code>, <code>test</code>, and <code>prod</code> cluster where you run all the application instances of a specific environment.</p> <ul> <li> <p>Isolation of the *prod environment*: In general, this approach isolates all     the environments from each other \u2014 but, in practice, this especially matters     for the prod environment.</p> <p>The production versions of your app are now not affected by whatever happens in any of the other clusters and application environments.</p> <p>So, if some misconfiguration creates havoc in your dev cluster, the prod versions of your apps keep running as if nothing had happened.</p> </li> <li> <p>Cluster can be customised for an environment:</p> <p>You can optimise each cluster for its environment \u2014 for example, you can:</p> <pre><code>* Install development and debugging tools in the dev cluster.\n* Install testing frameworks and tools in the test cluster.\n* Use more powerful hardware and network connections for the prod\ncluster.\n</code></pre> <p>This may improve the efficiency of both the development and operation of your apps. * Lock down access to prod cluster: Nobody really needs to do work on the prod cluster, so you can make access to it very restrictive.</p> </li> </ul> <p>Cons:</p> <ul> <li> <p>More administration and resources: In comparison with the single cluster.</p> </li> <li> <p>Lack of isolation between apps: The main disadvantage of this approach is     the missing hardware and resource isolation between apps.</p> <p>Unrelated apps share cluster resources, such as the operating system kernel, CPU, memory, and several other services.</p> <p>As already mentioned, this may be a security issue.</p> </li> <li> <p>App requirements are not localised: If an app has special requirements, then     these requirements must be satisfied in all clusters.</p> </li> </ul>"}, {"location": "devops/kubernetes/kubernetes_architecture/#conclusion", "title": "Conclusion", "text": "<p>If you don't need environment isolation and are not afraid of upgrading the Kubernetes cluster, go for the single cluster, otherwise use a cluster per environment.</p>"}, {"location": "devops/kubernetes/kubernetes_architecture/#references", "title": "References", "text": "<ul> <li>Kubernetes components overview</li> </ul>"}, {"location": "devops/kubernetes/kubernetes_cluster_autoscaler/", "title": "Kubernetes cluster autoscaler", "text": "<p>While Horizontal pod autoscaling allows a deployment to scale given the resources needed, they are limited to the kubernetes existing working nodes.</p> <p>To autoscale the number of working nodes we need the cluster autoscaler.</p> <p>For AWS, there are the Amazon guidelines to enable it. But I'd use the <code>cluster-autoscaler</code> helm chart.</p>"}, {"location": "devops/kubernetes/kubernetes_dashboard/", "title": "Kubernetes Dashboard", "text": "<p>Dashboard definition</p> <p>Dashboard is a web-based Kubernetes user interface. You can use Dashboard to deploy containerized applications to a Kubernetes cluster, troubleshoot your containerized application, and manage the cluster resources. You can use Dashboard to get an overview of applications running on your cluster, as well as for creating or modifying individual Kubernetes resources (such as Deployments, Jobs, DaemonSets, etc). For example, you can scale a Deployment, initiate a rolling update, restart a pod or deploy new applications using a deploy wizard.</p> <p>Dashboard also provides information on the state of Kubernetes resources in your cluster and on any errors that may have occurred.</p> <p></p>"}, {"location": "devops/kubernetes/kubernetes_dashboard/#deployment", "title": "Deployment", "text": "<p>The best way to install it is with the stable/kubernetes-dashboard chart with helmfile.</p>"}, {"location": "devops/kubernetes/kubernetes_dashboard/#links", "title": "Links", "text": "<ul> <li>Git</li> <li>Documentation</li> <li>Kubernetes introduction to the dashboard</li> <li>Hasham Haider guide</li> </ul>"}, {"location": "devops/kubernetes/kubernetes_deployments/", "title": "Kubernetes Deployments", "text": "<p>The different types of deployments configure a ReplicaSet and a PodSchema for your application.</p> <p>Depending on the type of application we'll use one of the following types.</p>"}, {"location": "devops/kubernetes/kubernetes_deployments/#deployments", "title": "Deployments", "text": "<p>Deployments are the controller for stateless applications, therefore it favors availability over consistency.</p> <p>It provides availability by creating multiple copies of the same pod. Those pods are disposable\u200a\u2014\u200aif they become unhealthy, Deployment will just create new replacements. What\u2019s more, you can update Deployment at a controlled rate, without a service outage. When an incident like the AWS outage happens, your workloads will automatically recover.</p> <p>Pods created by Deployment won't have the same identity after being killed and recreated, and they don't have unique persistent storage either.</p> <p>Concrete examples: Nginx, Tomcat</p> <p>A typical use case is:</p> <ul> <li>Create a Deployment to bring up a Replica Set and Pods.</li> <li>Check the status of a Deployment to see if it succeeds or not.</li> <li>Later, update that Deployment to recreate the Pods (for example, to use a new   image).</li> <li>Rollback to an earlier Deployment revision if the current Deployment isn't   stable.</li> <li>Pause and resume a Deployment.</li> </ul> <p>Deployment example</p> <pre><code>apiVersion: apps/v1beta1\nkind: Deployment\nmetadata:\nname: nginx-deployment\nspec:\nreplicas: 3\ntemplate:\nmetadata:\nlabels:\napp: nginx\nspec:\ncontainers:\n- name: nginx\nimage: nginx:1.7.9\nports:\n- containerPort: 80\n</code></pre>"}, {"location": "devops/kubernetes/kubernetes_deployments/#statefulsets", "title": "StatefulSets", "text": "<p>StatefulSets are the controller for stateful applications, therefore it favors consistency over availability.</p> <p>If your applications need to store data, like databases, cache, and message queues, each of your stateful pod will need a stronger notion of identity. Unlike Deployment, StatefulSets creates pods with stable, unique, and sticky identity and storage. You can deploy, scale, and delete pods in order. Which is safer, and makes it easier for you to reason about your stateful applications.</p> <p>Concrete examples: Zookeeper, MongoDB, MySQL</p> <p>The tricky part of the StatefulSets is that in multi node cluster on different regions in AWS, as the persistence is achieved with EBS volumes and they are fixed to a region. Your pod will always spawn in the same node. If there is a failure in that node, the pod will be marked as unschedulable and the service will be down.</p> <p>So as of 2020-03-02 is still not advised to host your databases inside kubernetes unless you have an operator.</p>"}, {"location": "devops/kubernetes/kubernetes_deployments/#daemonset", "title": "DaemonSet", "text": "<p>DaemonSets are the controller for applications that need to be run in each node. It's useful for recollecting log or monitoring purposes.</p> <p>DaemonSet makes sure that every node runs a copy of a pod. If you add or remove nodes, pods will be created or removed on them automatically. If you just want to run the daemons on some of the nodes, use node labels to control it.</p> <p>Concrete examples: fluentd, linkerd</p>"}, {"location": "devops/kubernetes/kubernetes_deployments/#job", "title": "Job", "text": "<p>Jobs are the controller to run batch processing workloads. It creates multiple pods running in parallel to process independent but related work items. This can be the emails to be sent or frames to be rendered.</p>"}, {"location": "devops/kubernetes/kubernetes_external_dns/", "title": "External DNS", "text": "<p>The external-dns resource allows the creation of DNS records from within kubernetes inside the definition of service and ingress resources.</p> <p>It currently supports the following providers:</p> Provider Status Google Cloud DNS Stable AWS Route 53 Stable AWS Cloud Map Beta AzureDNS Beta CloudFlare Beta RcodeZero Alpha DigitalOcean Alpha DNSimple Alpha Infoblox Alpha Dyn Alpha OpenStack Designate Alpha PowerDNS Alpha CoreDNS Alpha Exoscale Alpha Oracle Cloud Infrastructure DNS Alpha Linode DNS Alpha RFC2136 Alpha NS1 Alpha TransIP Alpha VinylDNS Alpha RancherDNS Alpha Akamai FastDNS Alpha <p>There are two reasons to enable it:</p> <ul> <li>If there is any change in the ingress or service load balancer endpoint, due   to a deployment, the dns records are automatically changed.</li> <li>It's easier for developers to connect their applications.</li> </ul>"}, {"location": "devops/kubernetes/kubernetes_external_dns/#deployment-in-aws", "title": "Deployment in AWS", "text": "<p>To install it inside EKS, create the <code>ExternalDNSEKSIAMPolicy</code>.</p> ExternalDNSEKSIAMPolicy <pre><code>{\n\"Version\": \"2012-10-17\",\n\"Statement\": [\n{\n\"Effect\": \"Allow\",\n\"Action\": [\n\"route53:ChangeResourceRecordSets\"\n],\n\"Resource\": [\n\"arn:aws:route53:::hostedzone/*\"\n]\n},\n{\n\"Effect\": \"Allow\",\n\"Action\": [\n\"route53:ListHostedZones\",\n\"route53:ListResourceRecordSets\"\n],\n\"Resource\": [\n\"*\"\n]\n}\n]\n}\n</code></pre> <p>and the associated <code>eks-external-dns</code> role that will be attached to the pod service account.</p> <p>When defining <code>iam_role</code> resources that are going to be used inside EKS, you need to use the EKS OIDC provider as trusted entity, so instead of using the default empty role policy:</p> <pre><code>{\n\"Version\": \"2012-10-17\",\n\"Statement\": [\n{\n\"Action\": \"sts:AssumeRole\",\n\"Effect\": \"Allow\",\n\"Principal\": {\n\"Service\": \"ec2.amazonaws.com\"\n}\n}\n]\n}\n</code></pre> <p>We'll use, as stated in the Creating an IAM Role and Policy for Service Account and Specifying an IAM Role for your Service Account documents:</p> <pre><code>{\n\"Version\": \"2012-10-17\",\n\"Statement\": [\n{\n\"Action\": \"sts:AssumeRoleWithWebIdentity\",\n\"Condition\": {\n\"StringEquals\": {\n\"oidc.eks.us-east-1.amazonaws.com/id/{{ cluster_fingerprint }}:sub\": \"system:serviceaccount:{{ service_account_namespace }}:{{ service_account_name }}\"\n}\n},\n\"Effect\": \"Allow\",\n\"Principal\": {\n\"Federated\": \"arn:aws:iam::{{ aws_account_id }}:oidc-provider/oidc.eks.{{ aws_zone }}.amazonaws.com/id/{{ cluster_fingerprint }}\"\n}\n}\n]\n}\n</code></pre> <p>Then particularize the external-dns helm chart.</p> <p>There are two ways of attaching the IAM role to <code>external-dns</code>, using the <code>asumeRoleArn</code> attribute on the <code>aws</code> values.yaml key or under the <code>rbac</code> <code>serviceAccountAnnotations</code>. I've tried the first but it gave an error because the cluster IAM role didn't have permissions to execute the assume role on that specific role. The second worked flawlessly.</p> <p>For more information visit the official external-dns aws documentation.</p>"}, {"location": "devops/kubernetes/kubernetes_hpa/", "title": "Kubernetes Horizontal pod autoscaling", "text": "<p>With Horizontal pod autoscaling, Kubernetes automatically scales the number of pods in a deployment or replication controller based on observed CPU utilization or on some other application provided metrics.</p> <p>The Horizontal Pod Autoscaler is implemented as a Kubernetes API resource and a controller. The resource determines the behavior of the controller. The controller periodically adjusts the number of replicas in a replication controller or deployment to match the observed average CPU utilization to the target specified by user.</p> <p>To make it work, the definition of pod resource consumption needs to be specified.</p>"}, {"location": "devops/kubernetes/kubernetes_ingress/", "title": "Kubernetes Ingress", "text": "<p>An Ingress is An API object that manages external access to the services in a cluster, typically HTTP.</p> <p>Ingress provide a centralized way to:</p> <ul> <li>Load balancing.</li> <li>SSL termination.</li> <li>Dynamic service discovery.</li> <li>Traffic routing.</li> <li>Authentication.</li> <li>Traffic distribution: canary deployments, A/B testing, mirroring/shadowing.</li> <li>Graphical user interface.</li> <li>JWT validation.</li> <li>WAF and DDOS protection.</li> <li>Requests tracing.</li> </ul> <p>An Ingress controller is responsible for fulfilling the Ingress, usually with a load balancer, though it may also configure your edge router or additional frontends to help handle the traffic.</p> <p>An Ingress does not expose arbitrary ports or protocols. Exposing services other than HTTP and HTTPS to the internet typically uses a service of type NodePort or LoadBalancer.</p>"}, {"location": "devops/kubernetes/kubernetes_ingress_controller/", "title": "Kubernetes Ingress Controller", "text": "<p>Ingress controllers monitor the cluster events for the creation or modification of Ingress resources, modifying accordingly the underlying load balancers. They are not part of the master kube-controller-manager, so you'll need to install them manually.</p> <p>There are different Ingress controllers, such as AWS ALB, Nginx, HAProxy or Traefik, using one or other depends on your needs.</p> <p>Almost all controllers are open sourced and support dynamic service discovery, SSL termination or WebSockets. But they differ in:</p> <ul> <li>Supported protocols: HTTP, HTTPS, gRPC, HTTP/2.0, TCP (with SNI) or UDP.</li> <li>Underlying software: NGINX, Traefik, HAProxy or Envoy.</li> <li>Traffic routing: host and path, regular expression support.</li> <li>Namespace limitations: supported or not.</li> <li>Upstream probes: active checks, passive checks, retries, circuit breakers,     custom health checks...</li> <li>Load balancing algorithms: round-robin, sticky sessions, rdp-cookie...</li> <li>Authentication: Basic, digest, Oauth, external auth, SSL certificate...</li> <li>Traffic distribution: canary deployments, A/B testing, mirroring/shadowing.</li> <li>Paid subscription: extended functionality or technical support.</li> <li>Graphical user interface:</li> <li>JWT validation:</li> <li>Customization of configuration:</li> <li>Basic DDOS protection mechanisms: rate limit, traffic filtering.</li> <li>WAF:</li> <li>Requests tracing: monitor, trace and debug requests via OpenTracing or other     options.</li> </ul> <p>Both ITNext and Flant provide good ingress controller comparisons, a synoptical resume of both articles follows.</p> <p></p>"}, {"location": "devops/kubernetes/kubernetes_ingress_controller/#kubernetes-ingress-controller", "title": "Kubernetes Ingress controller", "text": "<p>The \u201cofficial\u201d Kubernetes controller. Not to be confused with the one offered by the NGINX company. Developed by the community, it's based on the Nginx web server with a set of Lua plugins to implement extra features.</p> <p>Thanks to the popularity of NGINX and minimal modifications over it when using as a controller, it can be the simplest and most straightforward option for an average engineer dealing with K8s.</p>"}, {"location": "devops/kubernetes/kubernetes_ingress_controller/#traefik", "title": "Traefik", "text": "<p>Originally, this proxy was created for the routing of requests for microservices and their dynamic environment, hence many of its useful features:</p> <ul> <li>Continuous update of configuration (no restarts) .</li> <li>Support for multiple load balancing algorithms.</li> <li>Web UI.</li> <li>Metrics export.</li> <li>Support for various protocols.</li> <li>REST API.</li> <li>Canary releases.</li> <li>Let\u2019s Encrypt certificates support.</li> <li>TCP/SSL with SNI.</li> <li>Traffic mirroring/shadowing.</li> </ul> <p>The main disadvantage is that in order to organize the high availability of the controller you have to install and connect its own KV-storage.</p> <p>In 2019, the same developers have developed Maesh. Another service mesh solution built on top of Traefik.</p>"}, {"location": "devops/kubernetes/kubernetes_ingress_controller/#haproxy", "title": "HAProxy", "text": "<p>HAProxy is well known as a proxy server and load balancer. As part of the Kubernetes cluster, it offers: * \u201csoft\u201d configuration update (without traffic loss) * DNS-based service discovery * Dynamic configuration through API. * Full customization of a config-files template (via replacing a ConfigMap). * Using Spring Boot functions. * Great number of supported balancing algorithms.</p> <p>In general, developers put emphasis on high speed, optimization, and efficiency in consumed resources.</p> <p>It\u2019s worth mentioning a lot of new features have appeared in a recent (June\u201919) v2.0 release, and even more (including OpenTracing support) is expected with upcoming v2.1.</p>"}, {"location": "devops/kubernetes/kubernetes_ingress_controller/#istio-ingress", "title": "Istio Ingress", "text": "<p>Istio is a comprehensive service mesh solution. It can manage not just all incoming outside traffic (as an Ingress controller) but control all traffic inside the cluster as well. Under the hood, Istio uses Envoy as a sidecar-proxy for each service. In essence, it is a large processor that can do almost anything. Its central idea is maximum control, extensibility, security, and transparency.</p> <p>With Istio Ingress, you can fine tune traffic routing, access authorization between services, balancing, monitoring, canary releases and much more.</p> <p>\u201cBack to microservices with Istio\u201d is a great intro to learn about Istio.</p>"}, {"location": "devops/kubernetes/kubernetes_ingress_controller/#alb-ingress-controller", "title": "ALB Ingress controller", "text": "<p>The AWS ALB Ingress Controller satisfies Kubernetes ingress resources by provisioning Application Load Balancers.</p> <p>It's advantages are:</p> <ul> <li>AWS managed loadbalancer.</li> <li>Authentication with OIDC or Cognito.</li> <li>AWS WAF support.</li> <li>Natively redirect HTTP to HTTPS.</li> <li>Supports fixed response without forwarding to the application..</li> </ul> <p>It has also the potential advantage of using IP traffic mode. ALB support two types of traffic:</p> <ul> <li>instance mode: Ingress traffic starts from the ALB and reaches the NodePort   opened for your service. Traffic is then routed to the container Pods within   the cluster. The number of hops for the packet to reach its destination in   this mode is always two.</li> <li>IP mode: Ingress traffic starts from the ALB and reaches the container Pods   within cluster directly. In order to use this mode, the networking plugin for   the K8s cluster must use a secondary IP address on ENI as pod IP, aka AWS CNI   plugin for K8s. The number of hops for the packet to reach its destination is   always one.</li> </ul> <p>The IP mode gives the following advantages:</p> <ul> <li>The load balancer can be pod location-aware: reduce the chance to route   traffic to an irrelevant node and then rely on kube-proxy and network agent.</li> <li>The number of hops for the packet to reach its destination is always one</li> <li>No extra overlay network comparing to using Network plugins (Calico, Flannel)   directly int he cloud (AWS).</li> </ul> <p>It also has it's disadvantages:</p> <ul> <li>Even though AWS guides you on it's     deployment,     after two months of AWS Support cases, I wasn't able to deploy it using     terraform and helm.</li> <li> <p>You can't reuse existing ALBs instead of creating new ALB per   ingress.</p> <p>Therefore <code>ingress: false</code> needs to be specified in the service helm chart and manually edit the ALB ingress helm chart to add each new service.</p> </li> </ul>"}, {"location": "devops/kubernetes/kubernetes_ingress_controller/#alb-ingress-deployment", "title": "ALB ingress deployment", "text": "<p>This section is a defunct work in progress. I wasn't able to make it work, but it can be useful if you want to deploy it yourself. If you success, please make a PR.</p> <p>I've used the AWS Guide, in conjunction with the AWS general ALB controller documentation and the AWS general ALB documentation to define the properties of the incubator helm chart.</p> <p>Before that you need to an IAM policy ALBIngressControllerIAMPolicy to give the required permissions to manage the certificates, ALB creation and WAF integration. That IAM policy needs to be attached to the <code>eks-alb-ingress-controller</code> IAM role.</p> <p>You also had to create an IAM OIDC provider, which are entities in IAM that describe an external identity provider (IdP) service that supports the OpenID Connect (OIDC) standard, such as Google or Github. You use an IAM OIDC identity provider when you want to establish trust between an OIDC compatible IdP and your AWS account. This is useful when creating a mobile app or web application that requires access to AWS resources, but you don't want to create custom sign in code or manage your own user identities.</p> <p>The IAM OIDC provider is going to be used by the AWS EKS pod identity admission controller to attach IAM roles to specific service accounts. This will prevent the attachment of the IAM role to the whole node group. So instead of allowing every pod inside the worker groups to create the ALB, only the ones attached to the eks-alb-ingress-controller service account will be able to do so.</p>"}, {"location": "devops/kubernetes/kubernetes_ingress_controller/#links", "title": "Links", "text": "<ul> <li>ITNext ingress controller   comparison</li> <li>Flant ingress controller comparison</li> </ul>"}, {"location": "devops/kubernetes/kubernetes_jobs/", "title": "Kubernetes jobs", "text": "<p>Kubernetes jobs creates one or more Pods and ensures that a specified number of them successfully terminate. As pods successfully complete, the Job tracks the successful completions. When a specified number of successful completions is reached, the task (ie, Job) is complete. Deleting a Job will clean up the Pods it created.</p> <p>Cronjobs creates Jobs on a repeating schedule.</p> <p>This example CronJob manifest prints the current time and a hello message every minute:</p> <pre><code>apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\nname: hello\nspec:\nschedule: \"*/1 * * * *\"\njobTemplate:\nspec:\ntemplate:\nspec:\ncontainers:\n- name: hello\nimage: busybox\nargs:\n- /bin/sh\n- -c\n- date; echo Hello from the Kubernetes cluster\nrestartPolicy: OnFailure\n</code></pre> <p>To deploy cronjobs you can use the bambash helm chart.</p> <p>Check the kubectl commands to interact with jobs.</p>"}, {"location": "devops/kubernetes/kubernetes_jobs/#debugging-job-logs", "title": "Debugging job logs", "text": "<p>To obtain the logs of a completed or failed job, you need to:</p> <ul> <li>Locate the cronjob you want to debug: <code>kubectl get cronjobs -n cronjobs</code>.</li> <li>Locate the associated job: <code>kubectl get jobs -n cronjobs</code>.</li> <li>Locate the associated pod: <code>kubectl get pods -n cronjobs</code>.</li> </ul> <p>If the pod still exists, you can execute <code>kubectl logs -n cronjobs {{ pod_name }}</code>. If the pod doesn't exist anymore, you need to search the pod in your log centralizer solution.</p>"}, {"location": "devops/kubernetes/kubernetes_jobs/#rerunning-failed-jobs", "title": "Rerunning failed jobs", "text": "<p>If you have a job that has failed after the 6 default retries, it will show up in your monitorization forever, to fix it, you can manually trigger the job yourself with:</p> <pre><code>kubectl get job \"your-job\" -o json \\\n| jq 'del(.spec.selector)' \\\n| jq 'del(.spec.template.metadata.labels)' \\\n| kubectl replace --force -f -\n</code></pre>"}, {"location": "devops/kubernetes/kubernetes_jobs/#manually-creating-a-job-from-a-cronjob", "title": "Manually creating a job from a cronjob", "text": "<pre><code>kubectl create job {{ job_name }} -n {{ namespace }} \\\n--from=cronjobs/{{ cronjob_name}}\n</code></pre>"}, {"location": "devops/kubernetes/kubernetes_jobs/#monitorization-of-cronjobs", "title": "Monitorization of cronjobs", "text": "<p>Alerting of traditional Unix cronjobs meant sending an email if the job failed. Most job scheduling systems that have followed have provided the same experience, Kubernetes does not. One approach to alerting jobs is to use the Prometheus push gateway, allowing us to push richer metrics than the success/failure status. This approach has it\u2019s downsides; we have to update the code for our jobs, we also have to explicitly configure a push gateway location and update it if it changes (a burden alleviated by the pull based metrics for long lived workloads). You can use tools such as Sentry, but it will also require changes to the jobs.</p> <p>Jobs are powerful things allowing us to implement several different workflows, the combination of options can be overwhelming compared to a traditional Unix cron job. This variety makes it difficult to establish one simple rule for alerting failed jobs. Things get easier if we restrict ourselves to a subset of possible options. We will focus on non-concurrent jobs.</p> <p>The relationship between cronjobs and jobs makes the task ahead difficult. To make our life easier we will put one requirement on the jobs we create, they will have to include a label that associates them with the original cronjob.</p> <p>Below we present an example of our ideal cronjob (which matches what the helm chart deploys):</p> <pre><code>apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\nname: our-task\nspec:\nschedule: \"*/5 * * * *\"\nsuccessfulJobsHistoryLimit: 3\nconcurrencyPolicy: Forbid\njobTemplate:\nmetadata:\nlabels:\ncron: our-task # &lt;-- match created jobs with the cronjob\nspec:\nbackoffLimit: 3\ntemplate:\nmetadata:\nlabels:\ncronjob: our-task\nspec:\ncontainers:\n- name: our-task\ncommand:\n- /user/bin/false\nimage: alpine\nrestartPolicy: Never\n</code></pre>"}, {"location": "devops/kubernetes/kubernetes_jobs/#building-our-alert", "title": "Building our alert", "text": "<p>We are also going to need some metrics to get us started. K8s does not provide us any by default, but fortunately <code>kube-state-metrics</code> is installed with the Prometheus operator chart, so we have the following metrics:</p> <pre><code>kube_cronjob_labels{\n  cronjob=\"our-task\",\n  namespace=\"default\"} 1\nkube_job_created{\n  job=\"our-task-1520165700\",\n  namespace=\"default\"} 1.520165707e+09\nkube_job_failed{\n  condition=\"false\",\n  job=\"our-task-1520165700\",\n  namespace=\"default\"} 0\nkube_job_failed{\n  condition=\"true\",\n  job=\"our-task-1520165700\",\n  namespace=\"default\"} 1\nkube_job_labels{\n  job=\"our-task-1520165700\",\n  label_cron=\"our-task\",\n  namespace=\"default\"} 1\n</code></pre> <p>This shows the primary set of metrics we will be using to construct our alert. What is not shown above is the status of the cronjob. The big challenge with K8s cronjob alerting is that cronjobs themselves do not possess any status information, beyond the last time the cronjob created a job. The status information only exists on the job that the cronjob creates.</p> <p>In order to determine if our cronjob is failing, our first order of business is to find which jobs we should be looking at. A K8s cronjob creates new job objects and keeps a number of them around to help us debug the runs of our jobs. We have to be determine which job corresponds to the last run of our cronjob. If we have added the cron label to the jobs as above, we can find the last run time of the jobs for a given cronjob as follows:</p> <pre><code>max(\nkube_job_status_start_time\n* ON(job_name) GROUP_RIGHT()\nkube_job_labels{label_cron!=\"\"}\n) BY (job_name, label_cron)\n</code></pre> <p>This query demonstrates an important technique when working with <code>kube-state-metrics</code>. For each API object it exported data on, it exports a time series including all the labels for that object. These time series have a value of 1. As such we can join the set of labels for an object onto the metrics about that object by multiplication.</p> <p>Depending on how your Prometheus instance is configured, the value of the job label on your metrics will likely be <code>kube-state-metrics</code>. <code>kube-state-metrics</code> adds a job label itself with the name of the job object. Prometheus resolves this collision of label names by including the raw metric\u2019s label as an <code>job_name</code> label.</p> <p>Since we are querying the start time of jobs, and there should only ever be one job with a given name. You may wonder why we need the max aggregation. Manually plugging the query into Prometheus may convince you that it is unnecessary. Consider though that you may have multiple instances of <code>kube-state-metrics</code> running for redundancy. Using max ensures our query is valid even if we have multiple instances of kube-state-metrics running. Issues of duplicate metrics are common when constructing production recording rules and alerts.</p> <p>We can find the start time of the most recent job for a given cronjob by finding the maximum of all job start times as follows:</p> <pre><code>max(\nkube_job_status_start_time\n* ON(job_name) GROUP_RIGHT()\nkube_job_labels{label_cron!=\"\"}\n) BY (label_cron)\n</code></pre> <p>The only difference between this and the previous query is in the labels used for the aggregation. Now that we have the start time of each job, and the start time of the most recent job, we can do a simple equality match to find the start time of the most recent job for a given cronjob. We will create a metric for this:</p> <pre><code>- record: job_cronjob:kube_job_status_start_time:max\nexpr: |\nsum without (label_cron, job_name) (\nlabel_replace(\nlabel_replace(\nmax(\nkube_job_status_start_time\n* ON(job_name) GROUP_RIGHT()\nkube_job_labels{label_cron!=\"\"}\n) BY (job_name, label_cron)\n\n== ON(label_cron) GROUP_LEFT()\n\nmax(\nkube_job_status_start_time\n* ON(job_name) GROUP_RIGHT()\nkube_job_labels{label_cron!=\"\"}\n) BY (label_cron),\n\"job\", \"$1\", \"job_name\", \"(.+)\"\n),\n\"cronjob\", \"$1\", \"label_cron\", \"(.+)\"\n)\n)\n</code></pre> <p>We have also taken the opportunity to adjust the labels to be a little more aesthetically pleasing. By copying <code>job_name</code> to <code>job</code>, <code>label_cron</code> to <code>cronjob</code> and removing <code>job_name</code> and <code>label_cron</code>. Now that we have the most recently started job for a given cronjob, we can find which, if any, have failed attempts:</p> <pre><code>- record: job_cronjob:kube_job_status_failed:sum\nexpr: |\nsum without (label_cron, job_name) (\nclamp_max(\njob_cronjob:kube_job_status_start_time:max,\n1\n)\n\n* ON(job) GROUP_LEFT()\n\nlabel_replace(\nlabel_replace(\n(\nkube_job_status_failed != 0 and\nkube_job_status_succeeded == 0\n),\n\"job\", \"$1\", \"job_name\", \"(.+)\"\n),\n\"cronjob\", \"$1\", \"label_cron\", \"(.+)\"\n)\n)\n</code></pre> <p>The initial <code>clamp_max</code> clause is used to transform our start times metric into a set of time series to perform label matching to filter another set of metrics. Multiplication by 1 (or addition of 0), is a useful means of filter and merging time series and it is well worth taking the time to understand the technique.</p> <p>We get those cronjobs that have a failed job and no successful ones with the query:</p> <pre><code>(\nkube_job_status_failed != 0 and\nkube_job_status_succeeded == 0\n)\n</code></pre> <p>The <code>kube_job_status_succeeded == 0</code> it's important, otherwise once a job has a failed instance, it doesn't matter if there's a posterior one that succeeded, we're going to keep on receiving the alert that it failed.</p> <p>We adjust the labels on the previous query to match our start time metric so ensure the labels have the same meaning as those on our <code>job_cronjob:kube_job_status_start_time:max</code> metric. The label matching on the multiplication will then perform our filtering. We now have a metric containing the set of most recently failed jobs, labeled by their parent cronjob, so we can now construct the alert:</p> <pre><code>- alert: CronJobStatusFailed\nexpr: job_cronjob:kube_job_status_failed:sum &gt; 0\nfor: 1m\nannotations:\ndescription: '{{ $labels.cronjob }} last run has failed {{ $value }} times.'\n</code></pre> <p>We use the <code>kube_cronjob_labels</code> here to merge in labels from the original cronjob.</p>"}, {"location": "devops/kubernetes/kubernetes_labels/", "title": "Kubernetes Labels", "text": "<p>Labels are identifying metadata key/value pairs attached to objects, such as pods. Labels are intended to give meaningful and relevant information to users, but which do not directly imply semantics to the core system.</p> <pre><code>\"labels\": {\n\"key1\" : \"value1\",\n\"key2\" : \"value2\"\n}\n</code></pre>"}, {"location": "devops/kubernetes/kubernetes_metric_server/", "title": "Kubernetes Metric server", "text": "<p>The metrics server monitors the resource consumption inside the cluster. It populates the information in <code>kubectl top nodes</code> to get the node status and gives the information to automatically autoscale deployments with Horizontal pod autoscaling.</p> <p>To install it, you can use the <code>metrics-server</code> helm chart.</p> <p>To test that the horizontal pod autoscaling is working, follow the AWS EKS guide.</p>"}, {"location": "devops/kubernetes/kubernetes_namespaces/", "title": "Kubernetes Namespaces", "text": "<p>Namespaces are virtual clusters backed by the same physical cluster. It's the first level of isolation between applications.</p>"}, {"location": "devops/kubernetes/kubernetes_namespaces/#when-to-use-multiple-namespaces", "title": "When to Use Multiple Namespaces", "text": "<p>Namespaces are intended for use in environments with many users spread across multiple teams, or projects. For clusters with a few to tens of users, you should not need to create or think about namespaces at all. Start using namespaces when you need the features they provide.</p> <p>Namespaces provide a scope for names. Names of resources need to be unique within a namespace, but not across namespaces.</p> <p>Namespaces are a way to divide cluster resources between multiple uses (via resource quota).</p> <p>It is not necessary to use multiple namespaces just to separate slightly different resources, such as different versions of the same software: use labels to distinguish resources within the same namespace.</p>"}, {"location": "devops/kubernetes/kubernetes_networking/", "title": "Networking", "text": "<p>Container networking is the mechanism through which containers can optionally connect to other containers, the host, and outside networks like the internet.</p> <p>If you want to get a quickly grasp on how k8s networking works, I suggest you to read StackRox's Kubernetes networking demystified article.</p>"}, {"location": "devops/kubernetes/kubernetes_networking/#cni-comparison", "title": "CNI comparison", "text": "<p>Container networking is the mechanism through which containers can optionally connect to other containers, the host, and outside networks like the internet.</p> <p>There are different container networking plugins you can use for your cluster. To ensure that you choose the best one for your use case, I've made a summary based on the following resources:</p> <ul> <li>Rancher k8s CNI comparison.</li> <li>ITnext k8s CNI   comparison.</li> <li>Mark Ramm-Christensen AWS CNI   analysis.</li> </ul>"}, {"location": "devops/kubernetes/kubernetes_networking/#tldr", "title": "TL;DR", "text": "<p>When using EKS, if you have no networking experience and understand and accept their disadvantages I'd use the AWS VPC CNI as it's installed by default. Nevertheless, the pod limit per node and the vendor locking makes interesting to migrate in the future to Calico. To make the transition smoother, you can enable Calico Network Policies with the AWS VPC CNI and get used to them before fully migrating to Calico.</p> <p>Calico seems to be the best solution when you need a greater control of the networking inside k8s, as it supports security features (NetworkPolicies or encryption), that can be improved even more with the integration with Istio. It's known to be easy to debug and has commercial support.</p> <p>If you aren't using EKS, you could evaluate to first use Canal as it uses Flannel simple network overlay but with Calico's Network Policies. If you do this, you could first focus in getting used to Network Policies and then dive further into the network configuration with Calico. But a deeper analysis should be done to assess if the compared difficulty justifies the need of this step.</p> <p>I don't recommend to use Flannel alone even though it's simple as you'll probably need security features such as NetworkPolicies, although they say, it's the best insecure solution for low resource or several architecture nodes.</p> <p>I wouldn't use Weave either unless you need  encryption throughout all the internal network and multicast.</p>"}, {"location": "devops/kubernetes/kubernetes_networking/#flannel", "title": "Flannel", "text": "<p>Flannel, a project developed by the CoreOS, is perhaps the most straightforward and popular CNI plugin available. It is one of the most mature examples of networking fabric for container orchestration systems, intended to allow for better inter-container and inter-host networking. As the CNI concept took off, a CNI plugin for Flannel was an early entry.</p> <p>Flannel is relatively easy to install and configure. It is packaged as a single binary called flanneld and can be installed by default by many common Kubernetes cluster deployment tools and in many Kubernetes distributions. Flannel can use the Kubernetes cluster\u2019s existing etcd cluster to store its state information using the API to avoid having to provision a dedicated data store.</p> <p>Flannel configures a layer 3 IPv4 overlay network. A large internal network is created that spans across every node within the cluster. Within this overlay network, each node is given a subnet to allocate IP addresses internally. As pods are provisioned, the Docker bridge interface on each node allocates an address for each new container. Pods within the same host can communicate using the Docker bridge, while pods on different hosts will have their traffic encapsulated in UDP packets by flanneld for routing to the appropriate destination.</p> <p>Flannel has several different types of backends available for encapsulation and routing. The default and recommended approach is to use VXLAN, as it offers both good performance and is less manual intervention than other options.</p> <p>Overall, Flannel is a good choice for most users. From an administrative perspective, it offers a simple networking model that sets up an environment that\u2019s suitable for most use cases when you only need the basics. In general, it\u2019s a safe bet to start out with Flannel until you need something that it cannot provide or you need security features, such as NetworkPolicies or encryption.</p> <p>It's also recommended if you have low resource nodes in your cluster (only a few GB of RAM, a few cores). Moreover, it is compatible with a very large number of architectures (amd64, arm, arm64, etc.). It is the only one, along with Cilium, that is able to correctly auto-detect your MTU, so you don\u2019t have to configure anything to let it work.</p>"}, {"location": "devops/kubernetes/kubernetes_networking/#calico", "title": "Calico", "text": "<p>Calico, is another popular networking option in the Kubernetes ecosystem. While Flannel is positioned as the simple choice, Calico is best known for its performance, flexibility, and power. Calico takes a more holistic view of networking, concerning itself not only with providing network connectivity between hosts and pods, but also with network security and administration.</p> <p>On a freshly provisioned Kubernetes cluster that meets the system requirements, Calico can be deployed quickly by applying a single manifest file. If you are interested in Calico\u2019s optional network policy capabilities, you can enable them by applying an additional manifest to your cluster.</p> <p>Unlike Flannel, Calico does not use an overlay network. Instead, Calico configures a layer 3 network that uses the BGP routing protocol to route packets between hosts. This means that packets do not need to be wrapped in an extra layer of encapsulation when moving between hosts. The BGP routing mechanism can direct packets natively without an extra step of wrapping traffic in an additional layer of traffic.</p> <p>Besides the performance that this offers, one side effect of this is that it allows for more conventional troubleshooting when network problems arise. While encapsulated solutions using technologies like VXLAN work well, the process manipulates packets in a way that can make tracing difficult. With Calico, the standard debugging tools have access to the same information they would in simple environments, making it easier for a wider range of developers and administrators to understand behavior.</p> <p>In addition to networking connectivity, Calico is well known for its advanced network features. Network policy is one of its most sought after capabilities. In addition, Calico can also integrate with Istio, a service mesh, to interpret and enforce policy for workloads within the cluster both at the service mesh layer and the network infrastructure layer. This means that you can configure powerful rules describing how pods should be able to send and accept traffic, improving security and control over your networking environment.</p> <p>Project Calico is a good choice for environments that support its requirements and when performance and features like network policy are important. Additionally, Calico offers commercial support if you\u2019re seeking a support contract or want to keep that option open for the future. In general, it\u2019s a good choice for when you want to be able to control your network instead of just configuring it once and forgetting about it.</p> <p>If you are looking on how to install Calico, you can start with Calico guide for clusters with less than 50 nodes or if you use EKS with AWS guide.</p>"}, {"location": "devops/kubernetes/kubernetes_networking/#canal", "title": "Canal", "text": "<p>Canal is an interesting option for quite a few reasons.</p> <p>First of all, Canal was the name for a project that sought to integrate the networking layer provided by flannel with the networking policy capabilities of Calico. As the contributors worked through the details however, it became apparent that a full integration was not necessarily needed if work was done on both projects to ensure standardization and flexibility. As a result, the official project became somewhat defunct, but the intended ability to deploy the two technology together was achieved. For this reason, it\u2019s still sometimes easiest to refer to the combination as \u201cCanal\u201d even if the project no longer exists.</p> <p>Because Canal is a combination of Flannel and Calico, its benefits are also at the intersection of these two technologies. The networking layer is the simple overlay provided by Flannel that works across many different deployment environments without much additional configuration. The network policy capabilities layered on top supplement the base network with Calico\u2019s powerful networking rule evaluation to provide additional security and control.</p> <p>After ensuring that the cluster fulfills the necessary system requirements, Canal can be deployed by applying two manifests, making it no more difficult to configure than either of the projects on their own. Canal is a good way for teams to start to experiment and gain experience with network policy before they\u2019re ready to experiment with changing their actual networking.</p> <p>In general, Canal is a good choice if you like the networking model that Flannel provides but find some of Calico\u2019s features enticing. The ability define network policy rules is a huge advantage from a security perspective and is, in many ways, Calico\u2019s killer feature. Being able to apply that technology onto a familiar networking layer means that you can get a more capable environment without having to go through much of a transition.</p>"}, {"location": "devops/kubernetes/kubernetes_networking/#weave-net", "title": "Weave Net", "text": "<p>Weave Net by Weaveworks is a CNI-capable networking option for Kubernetes that offers a different paradigm than the others we\u2019ve discussed so far. Weave creates a mesh overlay network between each of the nodes in the cluster, allowing for flexible routing between participants. This, coupled with a few other unique features, allows Weave to intelligently route in situations that might otherwise cause problems.</p> <p>To create its network, Weave relies on a routing component installed on each host in the network. These routers then exchange topology information to maintain an up-to-date view of the available network landscape. When looking to send traffic to a pod located on a different node, the weave router makes an automatic decision whether to send it via \u201cfast datapath\u201d or to fall back on the \u201csleeve\u201d packet forwarding method.</p> <p>Fast datapath is an approach that relies on the kernel\u2019s native Open vSwitch datapath module to forward packets to the appropriate pod without moving in and out of userspace multiple times. The Weave router updates the Open vSwitch configuration to ensure that the kernel layer has accurate information about how to route incoming packets. In contrast, sleeve mode is available as a backup when the networking topology isn\u2019t suitable for fast datapath routing. It is a slower encapsulation mode that can route packets in instances where fast datapath does not have the necessary routing information or connectivity. As traffic flows through the routers, they learn which peers are associated with which MAC addresses, allowing them to route more intelligently with fewer hops for subsequent traffic. This same mechanism helps each node self-correct when a network change alters the available routes.</p> <p>Like Calico, Weave also provides network policy capabilities for your cluster. This is automatically installed and configured when you set up Weave, so no additional configuration is necessary beyond adding your network rules. One thing that Weave provides that the other options do not is easy encryption for the entire network. While it adds quite a bit of network overhead, Weave can be configured to automatically encrypt all routed traffic by using NaCl encryption for sleeve traffic and, since it needs to encrypt VXLAN traffic in the kernel, IPsec ESP for fast datapath traffic.</p> <p>Another advantage of Weave is that it's the only CNI that supports multicast. In case you need it.</p> <p>Weave is a great option for those looking for feature rich encrypted networking without adding a large amount of complexity or management at the expense of worse overall performance.  It is relatively easy to set up, offers many built in and automatically configured features, and can provide routing in scenarios where other solutions might fail. The mesh topography does put a limit on the size of the network that can be reasonably accommodated, but for most users, this won\u2019t be a problem. Additionally, Weave offers paid support for organizations that prefer to be able to have someone to contact for help and troubleshooting.</p>"}, {"location": "devops/kubernetes/kubernetes_networking/#aws-cni", "title": "AWS CNI", "text": "<p>AWS developed their own CNI that uses Elastic Network Interfaces for pod networking.</p> <p>It's the default CNI if you use EKS.</p>"}, {"location": "devops/kubernetes/kubernetes_networking/#advantages-of-the-aws-cni", "title": "Advantages of the AWS CNI", "text": "<p>Amazon native networking provides a number of significant advantages over some of the more traditional overlay network solutions for Kubernetes:</p> <ul> <li>Raw AWS network performance.</li> <li>Integration of tools familiar to AWS developers and admins, like AWS VPC flow   logs and Security Groups \u2014 allowing users with existing VPC networks and   networking best practices to carry those over directly to Kubernetes.</li> <li>The ability to enforce network policy decisions at the Kubernetes layer if you   install Calico.</li> </ul> <p>If your team has significant experience with AWS networking, and/or your application is sensitive to network performance all of this makes VPC CNI very attractive.</p>"}, {"location": "devops/kubernetes/kubernetes_networking/#disadvantages-of-the-aws-cni", "title": "Disadvantages of the AWS CNI", "text": "<p>On the other hand, there are a couple of limitations that may be significant to you. There are three primary reasons why you might instead choose an overlay network.</p> <ul> <li>Makes the multi-cloud k8s advantage more difficult.</li> <li>the CNI limits the number of pods that can be scheduled on each k8s node   according to the number of IP Addresses available to each EC2 instance type so   that each pod can be allocated an IP.</li> <li>Doesn't support encryption on the network.</li> <li>Multicast requirements.</li> <li>It eats up the number of IP Addresses available within your VPC unless you   give it an alternate subnet.</li> </ul>"}, {"location": "devops/kubernetes/kubernetes_networking/#vpc-cni-pod-density-limitations", "title": "VPC CNI Pod Density Limitations", "text": "<p>First, the VPC CNI plugin is designed to use/abuse ENI interfaces to get each pod in your cluster its own IP address from Amazon directly.</p> <p>This means that you will be network limited in the number of pods that you can run on any given worker node in the cluster.</p> <p>The primary IP for each ENI is used for cluster communication purposes. New pods are assigned to one of the secondary IPs for that ENI. VPC CNI has a custom DaemonSet that manages the assignment of IPs to pods. Because ENI and IP allocation requests can take time, this l-ipam daemon creates a warm pool of ENIs and IPs on each node and uses one of the available IPs for each new pod as it is assigned. This yields the following formula for maximum pod density for any given instance:</p> <pre><code>ENIs * (IPs_per_ENI - 1)\n</code></pre> <p>Each instance type in Amazon has unique limitations in the number of ENIs and IPs per ENI allowed. For example, an m5.large worker node allows 25 pod IP addresses per node at an approximate cost of $2.66/month per pod.</p> <p>Stepping up to an m5.xlarge allows for a theoretical maximum of 55 pods, for a monthly cost of $2.62, making the m5.large the more cost effective node choice by a small amount for clusters bound by IP address limitations.</p> <p>And if that set of calculations is not enough, there are a few other factors to consider. Kubernetes clusters generally run a set of services on each node. VPC CNI itself uses an l-ipam DaemonSet, and if you want Kubernetes network policies, calico requires another. Furthermore, production clusters generally also have DaemonSets for metrics collection, log aggregation, and other cluster-wide services. Each of these uses an IP address per node.</p> <p>So now the formula is:</p> <pre><code>(ENIs * (IPs_per_ENI - 1) - 1 * DaemonSets\n</code></pre> <p>This makes some of the cheaper instances on Amazon completely unusable because there are no IP addresses left for application pods.</p> <p>On the other hand, Kubernetes itself has a supported limit of 100 pods per node, making some of the larger instances with lots of available addresses less attractive. However, the pod per node limit IS configurable, and in my experience, this limit can be increased without much increase in Kubernetes overhead.</p> <p>Weave people made a quick Google sheet with each of the Amazon instance types, and the maximum pod densities for each based on the VPC CNI network plugin restrictions:</p> <ul> <li>A 100 pod/node limit setting in Kubernetes,</li> <li>A default of 4 DaemonSets (2 for AWS networking, 1 for log aggregation, and   1 for metric collection),</li> <li>A simple cost calculation for per-pod pricing.</li> </ul> <p>This sheet is not intended to provide a definitive answer on pod economics for AWS VPC based Kubernetes clusters. There are a number of important caveats to the use of this sheet:</p> <ul> <li>CPU and memory requirements will often dictate lower pod density than the   theoretical maximum here.</li> <li>Beyond DaemonSets, Kubernetes System pods, and other \u201csystem level\u201d   operational tools used in your cluster will consume Pod IP\u2019s and limit the   number of application pods that you can run.</li> <li>Each instance type also has network performance limitations which may impact   performance often far before theoretical pod limits are reached.</li> </ul>"}, {"location": "devops/kubernetes/kubernetes_networking/#cloud-portability", "title": "Cloud Portability", "text": "<p>Hybrid cloud, disaster recovery, and other requirements often push users away from custom cloud vendor solutions and towards open solutions.</p> <p>However, in this case the level of lock-in is quite low. The CNI layer provides a level of abstraction on top of the underlying network, and it is possible to deploy the same workloads using the same deployment configurations with different CNI backends. Which from my point of view, seems a bad and ugly idea.</p>"}, {"location": "devops/kubernetes/kubernetes_networking/#links", "title": "Links", "text": "<ul> <li>StackRox Kubernetes networking demystified article.</li> <li>Writing your own simple CNI plug   in.</li> </ul>"}, {"location": "devops/kubernetes/kubernetes_operators/", "title": "Kubernetes Operators", "text": "<p>Operators are Kubernetes specific applications (pods) that configure, manage and optimize other Kubernetes deployments automatically.</p> <p>A Kubernetes Operator might be able to:</p> <ul> <li>Install and provide sane initial configuration and sizing for your deployment,   according to the specs of your Kubernetes cluster.</li> <li>Perform live reloading of deployments and pods to accommodate for any   user requested parameter modification (hot config reloading).</li> <li>Safe coordination of application upgrades.</li> <li>Automatically scale up or down according to performance metrics.</li> <li>Service discovery via native Kubernetes APIs</li> <li>Application TLS certificate configuration</li> <li>Disaster recovery.</li> <li>Perform backups to offsite storage, integrity checks or any other maintenance task.</li> </ul>"}, {"location": "devops/kubernetes/kubernetes_operators/#how-do-they-work", "title": "How do they work?", "text": "<p>An Operator encodes this domain knowledge and extends the Kubernetes API through the third party resources mechanism, enabling users to create, configure, and manage applications. Like Kubernetes's built-in resources, an Operator doesn't manage just a single instance of the application, but multiple instances across the cluster.</p> <p>Operators build upon two central Kubernetes concepts: Resources and Controllers. As an example, the built-in ReplicaSet resource lets users set a desired number number of Pods to run, and controllers inside Kubernetes ensure the desired state set in the ReplicaSet resource remains true by creating or removing running Pods. There are many fundamental controllers and resources in Kubernetes that work in this manner, including Services, Deployments, and Daemon Sets.</p> <p>An Operator builds upon the basic Kubernetes resource and controller concepts and adds a set of knowledge or configuration that allows the Operator to execute common application tasks. For example, when scaling an etcd cluster manually, a user has to perform a number of steps: create a DNS name for the new etcd member, launch the new etcd instance, and then use the etcd administrative tools (etcdctl member add) to tell the existing cluster about this new member. Instead with the etcd Operator a user can simply increase the etcd cluster size field by 1.</p>"}, {"location": "devops/kubernetes/kubernetes_operators/#links", "title": "Links", "text": "<ul> <li>CoreOS introduction to Operators</li> <li>Sysdig Prometheus Operator guide part 3</li> </ul>"}, {"location": "devops/kubernetes/kubernetes_pods/", "title": "Kubernetes Pods", "text": "<p>Pods are the basic building block of Kubernetes, the smallest and simplest unit in the object model that you create or deploy. A Pod represents a running process on your cluster.</p> <p>A Pod represents a unit of deployment. It encapsulates:</p> <ul> <li>An application container (or, in some cases, multiple tightly coupled containers).</li> <li>Storage resources.</li> <li>A unique network IP.</li> <li>Options that govern how the container(s) should run.</li> </ul>"}, {"location": "devops/kubernetes/kubernetes_replicasets/", "title": "Kubernetes ReplicaSets", "text": "<p>ReplicaSet maintains a stable set of replica Pods running at any given time.  As such, it is often used to guarantee the availability of a specified number of identical Pods.</p> <p>You'll probably never manually use these resources, as they are defined inside the deployments. The older version of this resource are the Replication controllers.</p>"}, {"location": "devops/kubernetes/kubernetes_services/", "title": "Kubernetes Services", "text": "<p>A Service defines a policy to access a logical set of Pods using a reliable endpoint. Users and other programs can access pods running on your cluster seamlessly. Therefore allowing a loose coupling between dependent Pods.</p> <p>When a request arrives the endpoint, the kube-proxy pod of the node forwards the request to the Pods that match the service LabelSelector.</p> <p>Services can be exposed in different ways by specifying a type in the ServiceSpec:</p> <ul> <li> <p>ClusterIP (default): Exposes the Service on an internal IP in the cluster.   This type makes the Service only reachable from within the cluster.</p> </li> <li> <p>NodePort: Exposes the Service on the same port of each selected Node in the   cluster using NAT to the outside.</p> </li> <li> <p>LoadBalancer: Creates an external load balancer in the current cloud   and assigns a fixed, external IP to the Service.</p> <p>To create an internal ELB of AWs add to the annotations: <pre><code>annotations:\nservice.beta.kubernetes.io/aws-load-balancer-internal: 0.0.0.0/0\n</code></pre></p> </li> <li> <p>ExternalName: Exposes the Service using an arbitrary name by returning   a CNAME record with the name. No proxy is used.</p> </li> </ul> <p>If no RBAC or NetworkPolicies are applied, you can call a service of another namespace with the following nomenclature.</p> <pre><code>curl {{ service_name}}.{{ service_namespace }}.svc.cluster.local\n</code></pre>"}, {"location": "devops/kubernetes/kubernetes_storage_driver/", "title": "Kubernetes Storage Driver", "text": "<p>Storage drivers are pods that through the Container Storage Interface or CSI provide an interface to use external storage services from within Kubernetes.</p>"}, {"location": "devops/kubernetes/kubernetes_storage_driver/#amazon-ebs-csi-storage-driver", "title": "Amazon EBS CSI storage driver", "text": "<p>Allows Kubernetes clusters to manage the lifecycle of Amazon EBS volumes for persistent volumes with the <code>awsElasticBlockStore</code> volume type.</p> <p>To install it, you first need to attach the <code>Amazon_EBS_CSI_Driver</code> IAM policy to the worker nodes. Then you can use the <code>aws-ebs-csi-driver</code> helm chart.</p> <p>To test it worked follow the steps under To deploy a sample application and verify that the CSI driver is working.</p>"}, {"location": "devops/kubernetes/kubernetes_tools/", "title": "Kubernetes Tools", "text": "<p>There are several tools built to enhance the operation, installation and use of Kubernetes.</p>"}, {"location": "devops/kubernetes/kubernetes_tools/#tried", "title": "Tried", "text": "<ul> <li>K3s: Recommended small kubernetes, like hyperkube.</li> </ul>"}, {"location": "devops/kubernetes/kubernetes_tools/#to-try", "title": "To try", "text": "<ul> <li>crossplane: Crossplane is an   open source multicloud control plane. It introduces workload and resource   abstractions on-top of existing managed services that enables a high degree of   workload portability across cloud providers. A single crossplane enables the   provisioning and full-lifecycle management of services and infrastructure   across a wide range of providers, offerings, vendors, regions, and clusters.   Crossplane offers a universal API for cloud computing, a workload scheduler,   and a set of smart controllers that can automate work across clouds.</li> <li>razee: A multi-cluster continuous delivery tool for Kubernetes   Automate the rollout process of Kubernetes resources across multiple clusters,   environments, and cloud providers, and gain insight into what applications and   versions run in your cluster.</li> <li>kube-ops-view: it shows how are   the ops on the nodes.</li> <li>kubediff: a tool for Kubernetes to   show differences between running state and version controlled configuration.</li> <li>ksniff: A kubectl plugin that utilize   tcpdump and Wireshark to start a remote capture on any pod in your Kubernetes   cluster.</li> <li>kubeview: Visualize   dependencies kubernetes.</li> </ul>"}, {"location": "devops/kubernetes/kubernetes_vertical_pod_autoscaler/", "title": "Vertical Pod Autoscaler", "text": "<p>Kubernetes knows the amount of resources a pod needs to operate through some metadata specified in the deployment. Generally this values change and manually maintaining all the resources requested and limits is a nightmare.</p> <p>The Vertical pod autoscaler does data analysis on the pod metrics to automatically adjust these values.</p> <p>Nevertheless it's still not suggested to use it in conjunction with the horizontal pod autoscaler, so we'll need to watch out for future improvements.</p>"}, {"location": "devops/kubernetes/kubernetes_volumes/", "title": "Kubernetes Volumes", "text": "<p>On disk files in a Container are ephemeral by default, which presents the following issues:</p> <ul> <li>When a Container crashes, kubelet will restart it, but the files will be lost.</li> <li>When running Containers together in a Pod it is often necessary to share files   between those Containers.</li> </ul> <p>The Kubernetes Volume abstraction solves both of these problems with several types.</p>"}, {"location": "devops/kubernetes/kubernetes_volumes/#configmap", "title": "configMap", "text": "<p>The <code>configMap</code> resource provides a way to inject configuration data into Pods. The data stored in a ConfigMap object can be referenced in a volume of type <code>configMap</code> and then consumed by containerized applications running in a Pod.</p>"}, {"location": "devops/kubernetes/kubernetes_volumes/#emptydir", "title": "emptyDir", "text": "<p>An <code>emptyDir</code> volume is first created when a Pod is assigned to a Node, and exists as long as that Pod is running on that node. As the name says, it is initially empty. Containers in the Pod can all read and write the same files in the <code>emptyDir</code> volume. When a Pod is removed from a node for any reason, the data in the <code>emptyDir</code> is deleted forever.</p>"}, {"location": "devops/kubernetes/kubernetes_volumes/#hostpath", "title": "hostPath", "text": "<p>A <code>hostPath</code> volume mounts a file or directory from the host node\u2019s filesystem into your Pod. This is not something that most Pods will need, but it offers a powerful escape hatch for some applications.</p> <p>For example, some uses for a hostPath are:</p> <ul> <li>Running a Container that needs access to Docker internals; use a hostPath of   <code>/var/lib/docker</code>.</li> <li>Running cAdvisor in a Container; use a hostPath of <code>/sys</code>.</li> </ul>"}, {"location": "devops/kubernetes/kubernetes_volumes/#secret", "title": "secret", "text": "<p>A <code>secret</code> volume is used to pass sensitive information, such as passwords, to Pods. You can store secrets in the Kubernetes API and mount them as files for use by Pods without coupling to Kubernetes directly. <code>secret</code> volumes are backed by tmpfs (a RAM-backed filesystem) so they are never written to non-volatile storage.</p>"}, {"location": "devops/kubernetes/kubernetes_volumes/#awselasticblockstore", "title": "awsElasticBlockStore", "text": "<p>An <code>awsElasticBlockStore</code> volume mounts an Amazon Web Services (AWS) EBS Volume into your Pod. Unlike <code>emptyDir</code>, which is erased when a Pod is removed, the contents of an EBS volume are preserved and the volume is merely unmounted. This means that an EBS volume can be pre-populated with data, and that data can be \u201chanded off\u201d between Pods.</p> <p>There are some restrictions when using an awsElasticBlockStore volume:</p> <ul> <li>The nodes on which Pods are running must be AWS EC2 instances.</li> <li>Those instances need to be in the same region and availability-zone as the EBS   volume.</li> <li>EBS only supports a single EC2 instance mounting a volume.</li> </ul>"}, {"location": "devops/kubernetes/kubernetes_volumes/#nfs", "title": "nfs", "text": "<p>An <code>nfs</code> volume allows an existing NFS (Network File System) share to be mounted into your Pod. Unlike emptyDir, which is erased when a Pod is removed, the contents of an <code>nfs</code> volume are preserved and the volume is merely unmounted. This means that an NFS volume can be pre-populated with data, and that data can be \u201chanded off\u201d between Pods. NFS can be mounted by multiple writers simultaneously.</p>"}, {"location": "devops/kubernetes/kubernetes_volumes/#local", "title": "local", "text": "<p>A <code>local</code> volume represents a mounted local storage device such as a disk, partition or directory.</p> <p>Local volumes can only be used as a statically created PersistentVolume. Dynamic provisioning is not supported yet.</p> <p>Compared to <code>hostPath</code> volumes, local volumes can be used in a durable and portable manner without manually scheduling Pods to nodes, as the system is aware of the volume\u2019s node constraints by looking at the node affinity on the PersistentVolume.</p> <p>However, local volumes are still subject to the availability of the underlying node and are not suitable for all applications. If a node becomes unhealthy, then the local volume will also become inaccessible, and a Pod using it will not be able to run. Applications using local volumes must be able to tolerate this reduced availability, as well as potential data loss, depending on the durability characteristics of the underlying disk.</p>"}, {"location": "devops/kubernetes/kubernetes_volumes/#others", "title": "Others", "text": "<ul> <li>glusterfs</li> <li>cephfs</li> </ul>"}, {"location": "devops/prometheus/alertmanager/", "title": "AlertManager", "text": "<p>The Alertmanager handles alerts sent by client applications such as the Prometheus server. It takes care of deduplicating, grouping, and routing them to the correct receiver integrations such as email, PagerDuty, or OpsGenie. It also takes care of silencing and inhibition of alerts.</p>"}, {"location": "devops/prometheus/alertmanager/#configuration", "title": "Configuration", "text": "<p>It is configured through the <code>alertmanager.config</code> key of the <code>values.yaml</code> of the helm chart or the <code>alertmanager.yaml</code> file if you're using <code>docker-compose</code>.</p> <p>As stated in the configuration file, it has four main keys (as <code>templates</code> is handled in <code>alertmanager.config.templateFiles</code>):</p> <ul> <li><code>global</code>: SMTP and API main configuration, it will be inherited by the other     elements.</li> <li><code>route</code>: Route tree definition.</li> <li><code>receivers</code>: Notification integrations configuration.</li> <li><code>inhibit_rules</code>: Alert inhibition configuration.</li> </ul>"}, {"location": "devops/prometheus/alertmanager/#receivers", "title": "Receivers", "text": "<p>Notification receivers are the named configurations of one or more notification integrations.</p>"}, {"location": "devops/prometheus/alertmanager/#null-receiver", "title": "Null receiver", "text": "<p>Useful to ditch alerts that shouldn't be inhibited</p> <pre><code>receivers:\n- name: 'null'\n</code></pre>"}, {"location": "devops/prometheus/alertmanager/#email-notifications", "title": "Email notifications", "text": "<p>To configure email notifications, set up the following in your <code>config</code>:</p> <pre><code>  config:\nglobal:\nsmtp_from: {{ from_email_address }}\nsmtp_smarthost: {{ smtp_server_endpoint }}:{{ smtp_server_port }}\nsmtp_auth_username: {{ smpt_authentication_username }}\nsmtp_auth_password: {{ smpt_authentication_password }}\nreceivers:\n- name: 'email'\nemail_configs:\n- to: {{ receiver_email }}\nsend_resolved: true\n</code></pre> <p>If you need to set <code>smtp_auth_username</code> and <code>smtp_auth_password</code> you should value using helm secrets.</p> <p><code>send_resolved</code>, set to <code>False</code> by default, defines whether or not to notify about resolved alerts.</p>"}, {"location": "devops/prometheus/alertmanager/#rocketchat-notifications", "title": "Rocketchat Notifications", "text": "<p>Go to pavel-kazhavets AlertmanagerRocketChat repo for the updated rules.</p> <p>In RocketChat:</p> <ul> <li>Login as admin user and go to: Administration =&gt; Integrations =&gt; New Integration =&gt; Incoming WebHook.</li> <li>Set \"Enabled\" and \"Script Enabled\" to \"True\".</li> <li>Set all channel, icons, etc. as you need.</li> <li>Paste contents of the official AlertmanagerIntegrations.js or my version into Script field.</li> </ul> AlertmanagerIntegrations.js <pre><code>class Script {\nprocess_incoming_request({\nrequest\n}) {\nconsole.log(request.content);\n\nvar alertColor = \"warning\";\nif (request.content.status == \"resolved\") {\nalertColor = \"good\";\n} else if (request.content.status == \"firing\") {\nalertColor = \"danger\";\n}\n\nlet finFields = [];\nfor (i = 0; i &lt; request.content.alerts.length; i++) {\nvar endVal = request.content.alerts[i];\nvar elem = {\ntitle: \"alertname: \" + endVal.labels.alertname,\nvalue: \"*instance:* \" + endVal.labels.instance,\nshort: false\n};\n\nfinFields.push(elem);\n\nif (!!endVal.annotations.summary) {\nfinFields.push({\ntitle: \"summary\",\nvalue: endVal.annotations.summary\n});\n}\n\nif (!!endVal.annotations.severity) {\nfinFields.push({\ntitle: \"severity\",\nvalue: endVal.labels.severity\n});\n}\n\nif (!!endVal.annotations.grafana) {\nfinFields.push({\ntitle: \"grafana\",\nvalue: endVal.annotations.grafana\n});\n}\n\nif (!!endVal.annotations.prometheus) {\nfinFields.push({\ntitle: \"prometheus\",\nvalue: endVal.annotations.prometheus\n});\n}\n\nif (!!endVal.annotations.message) {\nfinFields.push({\ntitle: \"message\",\nvalue: endVal.annotations.message\n});\n}\n\nif (!!endVal.annotations.description) {\nfinFields.push({\ntitle: \"description\",\nvalue: endVal.annotations.description\n});\n}\n}\n\nreturn {\ncontent: {\nusername: \"Prometheus Alert\",\nattachments: [{\ncolor: alertColor,\ntitle_link: request.content.externalURL,\ntitle: \"Prometheus notification\",\nfields: finFields\n}]\n}\n};\n\nreturn {\nerror: {\nsuccess: false\n}\n};\n}\n}\n</code></pre> <ul> <li>Create Integration. The field <code>Webhook URL</code> will appear in the Integration configuration.</li> </ul> <p>In Alertmanager:</p> <ul> <li>Create new receiver or modify config of existing one. You'll need to add <code>webhooks_config</code> to it. Small example:</li> </ul> <pre><code>route:\nrepeat_interval: 30m\ngroup_interval: 30m\nreceiver: 'rocketchat'\n\nreceivers:\n- name: 'rocketchat'\nwebhook_configs:\n- send_resolved: false\nurl: '${WEBHOOK_URL}'\n</code></pre> <ul> <li>Reload/restart alertmanager.</li> </ul> <p>In order to test the webhook you can use the following curl (replace <code>{{ webhook-url }}</code>):</p> <pre><code>curl -X POST -H 'Content-Type: application/json' --data '\n{\n  \"text\": \"Example message\",\n  \"attachments\": [\n    {\n      \"title\": \"Rocket.Chat\",\n      \"title_link\": \"https://rocket.chat\",\n      \"text\": \"Rocket.Chat, the best open source chat\",\n      \"image_url\": \"https://rocket.cha t/images/mockup.png\",\n      \"color\": \"#764FA5\"\n    }\n  ],\n  \"status\": \"firing\",\n  \"alerts\": [\n    {\n      \"labels\": {\n        \"alertname\": \"high_load\",\n        \"severity\": \"major\",\n        \"instance\": \"node-exporter:9100\"\n      },\n      \"annotations\": {\n        \"message\": \"node-exporter:9100 of job xxxx is under high load.\",\n        \"summary\": \"node-exporter:9100 under high load.\"\n      }\n    }\n  ]\n}\n' {{ webhook-url }}\n</code></pre>"}, {"location": "devops/prometheus/alertmanager/#route", "title": "Route", "text": "<p>A route block defines a node in a routing tree and its children. Its optional configuration parameters are inherited from its parent node if not set.</p> <p>Every alert enters the routing tree at the configured top-level route, which must match all alerts (i.e. not have any configured matchers). It then traverses the child nodes. If continue is set to false, it stops after the first matching child. If continue is true on a matching node, the alert will continue matching against subsequent siblings. If an alert does not match any children of a node (no matching child nodes, or none exist), the alert is handled based on the configuration parameters of the current node.</p> <p>A basic configuration would be:</p> <pre><code>route:\ngroup_by: [job, alertname, severity]\ngroup_wait: 30s\ngroup_interval: 5m\nrepeat_interval: 12h\nreceiver: 'email'\nroutes:\n- match:\nalertname: Watchdog\nreceiver: 'null'\n</code></pre>"}, {"location": "devops/prometheus/alertmanager/#inhibit-rules", "title": "Inhibit rules", "text": "<p>Inhibit rules define which alerts triggered by Prometheus shouldn't be forwarded to the notification integrations. For example the <code>Watchdog</code> alert is meant to test that everything works as expected, but is not meant to be used by the users. Similarly, if you are using EKS, you'll probably have an <code>KubeVersionMismatch</code>, because Kubernetes allows a certain version skew between their components. So the alert is more strict than the Kubernetes policy.</p> <p>To disable both alerts, set a <code>match</code> rule in <code>config.inhibit_rules</code>:</p> <pre><code>  config:\ninhibit_rules:\n- target_match:\nalertname: Watchdog\n- target_match:\nalertname: KubeVersionMismatch\n</code></pre>"}, {"location": "devops/prometheus/alertmanager/#alert-rules", "title": "Alert rules", "text": "<p>Alert rules are a special kind of Prometheus Rules that trigger alerts based on PromQL expressions. People have gathered several examples under Awesome prometheus alert rules</p> <p>Alerts must be configured in the Prometheus configuration, either through the operator helm chart, under the <code>additionalPrometheusRulesMap</code> or in the <code>prometheus.yml</code> file. For example:</p> <pre><code>additionalPrometheusRulesMap:\n- groups:\n- name: alert-rules\nrules:\n- alert: BlackboxProbeFailed\nexpr: probe_success == 0\nfor: 5m\nlabels:\nseverity: error\nannotations:\nsummary: \"Blackbox probe failed (instance {{ $labels.target }})\"\ndescription: \"Probe failed\\n  VALUE = {{ $value }}\\n  LABELS: {{ $labels }}\"\n</code></pre> <p>Other examples of rules are:</p> <ul> <li>Blackbox Exporter rules</li> </ul> <p>If you are using <code>prometheus.yml</code> directly, you also need to configure the alerting:</p> <pre><code>alerting:\n  alertmanagers:\n    - scheme: http\n      static_configs:\n        - targets: [ 'alertmanager:9093' ]\n</code></pre>"}, {"location": "devops/prometheus/alertmanager/#silences", "title": "Silences", "text": "<p>To silence an alert with a regular expression use the matcher <code>alertname=~\".*Condition\"</code>.</p>"}, {"location": "devops/prometheus/alertmanager/#references", "title": "References", "text": "<ul> <li>Source</li> <li>[Docs](https://prometheus.io/docs/alerting/latest/alertmanager/</li> <li>Awesome prometheus alert rules</li> </ul>"}, {"location": "devops/prometheus/blackbox_exporter/", "title": "Blackbox Exporter", "text": "<p>The blackbox exporter allows blackbox probing of endpoints over HTTP, HTTPS, DNS, TCP and ICMP.</p> <p>It can be used to test:</p> <ul> <li>Website accessibility. Both for availability and       security purposes.</li> <li>Website loading time.</li> <li>DNS response times to diagnose network latency issues.</li> <li>SSL certificates expiration.</li> <li>ICMP requests to gather network health information.</li> <li>Security protections such as if and endpoint stops being     protected by VPN, WAF or SSL client certificate.</li> <li>Unauthorized read or write S3 buckets.</li> </ul> <p>When running, the Blackbox exporter is going to expose a HTTP endpoint that can be used in order to monitor targets over the network. By default, the Blackbox exporter exposes the <code>/probe</code> endpoint that is used to retrieve those metrics.</p> <p>The blackbox exporter is configured with a YAML configuration file made of modules.</p>"}, {"location": "devops/prometheus/blackbox_exporter/#installation", "title": "Installation", "text": "<p>To install the exporter we'll use helmfile to install the stable/prometheus-blackbox-exporter chart.</p> <p>Add the following lines to your <code>helmfile.yaml</code>.</p> <pre><code>- name: prometheus-blackbox-exporter\nnamespace: monitoring\nchart: stable/prometheus-blackbox-exporter\nvalues:\n- prometheus-blackbox-exporter/values.yaml\n</code></pre> <p>Edit the chart values. <pre><code>mkdir prometheus-blackbox-exporter\nhelm inspect values stable/prometheus-blackbox-exporter &gt; prometheus-blackbox-exporter/values.yaml\nvi prometheus-blackbox-exporter/values.yaml\n</code></pre></p> <p>Make sure to enable the <code>serviceMonitor</code> in the values and target at least one page:</p> <pre><code>serviceMonitor:\nenabled: true\n\n# Default values that will be used for all ServiceMonitors created by `targets`\ndefaults:\nlabels:\nrelease: prometheus-operator\ninterval: 30s\nscrapeTimeout: 30s\nmodule: http_2xx\n\ntargets:\n- name: lyz-code.github.io/blue-book\nurl: https://lyz-code.github.io/blue-book\n</code></pre> <p>The label <code>release: prometheus-operator</code> must be the one your prometheus instance is searching for.</p> <p>If you want to use the <code>icmp</code> probe, make sure to allow <code>allowIcmp: true</code>.</p> <p>If you want to probe endpoints protected behind client SSL certificates, until this chart issue is solved, you need to create them manually as the Prometheus blackbox exporter helm chart doesn't yet create the required secrets.</p> <pre><code>kubectl create secret generic monitor-certificates \\\n--from-file=monitor.crt.pem \\\n--from-file=monitor.key.pem \\\n-n monitoring\n</code></pre> <p>Where <code>monitor.crt.pem</code> and <code>monitor.key.pem</code> are the SSL certificate and key for the monitor account.</p> <p>I've found two grafana dashboards for the blackbox exporter. <code>7587</code> didn't work straight out of the box while <code>5345</code> did. Taking as reference the grafana helm chart values, add the following yaml under the <code>grafana</code> key in the <code>prometheus-operator</code> <code>values.yaml</code>.</p> <pre><code>grafana:\nenabled: true\ndefaultDashboardsEnabled: true\ndashboardProviders:\ndashboardproviders.yaml:\napiVersion: 1\nproviders:\n- name: 'default'\norgId: 1\nfolder: ''\ntype: file\ndisableDeletion: false\neditable: true\noptions:\npath: /var/lib/grafana/dashboards/default\ndashboards:\ndefault:\nblackbox-exporter:\n# Ref: https://grafana.com/dashboards/5345\ngnetId: 5345\nrevision: 3\ndatasource: Prometheus\n</code></pre> <p>And install.</p> <pre><code>helmfile diff\nhelmfile apply\n</code></pre>"}, {"location": "devops/prometheus/blackbox_exporter/#blackbox-exporter-probes", "title": "Blackbox exporter probes", "text": "<p>Modules define how blackbox exporter is going to query the endpoint, therefore one needs to be created for each request type under the <code>config.modules</code> section of the chart.</p> <p>The modules are then used in the <code>targets</code> section for the desired endpoints.</p> <pre><code>  targets:\n- name: lyz-code.github.io/blue-book\nurl: https://lyz-code.github.io/blue-book\nmodule: https_2xx\n</code></pre>"}, {"location": "devops/prometheus/blackbox_exporter/#http-endpoint-working-correctly", "title": "HTTP endpoint working correctly", "text": "<pre><code>http_2xx:\nprober: http\ntimeout: 5s\nhttp:\nvalid_http_versions: [\"HTTP/1.1\", \"HTTP/2.0\"]\nvalid_status_codes: [200]\nno_follow_redirects: false\npreferred_ip_protocol: \"ip4\"\n</code></pre>"}, {"location": "devops/prometheus/blackbox_exporter/#https-endpoint-working-correctly", "title": "HTTPS endpoint working correctly", "text": "<pre><code>https_2xx:\nprober: http\ntimeout: 5s\nhttp:\nmethod: GET\nfail_if_ssl: false\nfail_if_not_ssl: true\nvalid_http_versions: [\"HTTP/1.1\", \"HTTP/2.0\"]\nvalid_status_codes: [200]\nno_follow_redirects: false\npreferred_ip_protocol: \"ip4\"\n</code></pre>"}, {"location": "devops/prometheus/blackbox_exporter/#https-endpoint-behind-client-ssl-certificate", "title": "HTTPS endpoint behind client SSL certificate", "text": "<pre><code>https_client_2xx:\nprober: http\ntimeout: 5s\nhttp:\nmethod: GET\nfail_if_ssl: false\nfail_if_not_ssl: true\nvalid_http_versions: [\"HTTP/1.1\", \"HTTP/2.0\"]\nvalid_status_codes: [200]\nno_follow_redirects: false\npreferred_ip_protocol: \"ip4\"\ntls_config:\ncert_file: /etc/secrets/monitor.crt.pem\nkey_file: /etc/secrets/monitor.key.pem\n</code></pre> <p>Where the secrets have been created throughout the installation.</p>"}, {"location": "devops/prometheus/blackbox_exporter/#https-endpoint-with-an-specific-error", "title": "HTTPS endpoint with an specific error", "text": "<p>If you don't want to configure the authentication for example for an API, you can fetch the expected error.</p> <pre><code>https_client_api:\nprober: http\ntimeout: 5s\nhttp:\nmethod: GET\nfail_if_ssl: false\nfail_if_not_ssl: true\nvalid_http_versions: [\"HTTP/1.1\", \"HTTP/2.0\"]\nvalid_status_codes: [404]\nno_follow_redirects: false\npreferred_ip_protocol: \"ip4\"\nfail_if_body_not_matches_regexp:\n- '.*ERROR route not.*'\n</code></pre>"}, {"location": "devops/prometheus/blackbox_exporter/#http-endpoint-returning-an-error", "title": "HTTP endpoint returning an error", "text": "<pre><code>http_4xx:\nprober: http\ntimeout: 5s\nhttp:\nmethod: HEAD\nvalid_status_codes: [404, 403]\nvalid_http_versions: [\"HTTP/1.1\", \"HTTP/2.0\"]\nno_follow_redirects: false\n</code></pre>"}, {"location": "devops/prometheus/blackbox_exporter/#https-endpoint-through-an-http-proxy", "title": "HTTPS endpoint through an HTTP proxy", "text": "<pre><code>https_external_2xx:\nprober: http\ntimeout: 5s\nhttp:\nmethod: GET\nfail_if_ssl: false\nfail_if_not_ssl: true\nvalid_http_versions: [\"HTTP/1.0\", \"HTTP/1.1\", \"HTTP/2.0\"]\nvalid_status_codes: [200]\nno_follow_redirects: false\nproxy_url: \"http://{{ proxy_url }}:{{ proxy_port }}\"\npreferred_ip_protocol: \"ip4\"\n</code></pre>"}, {"location": "devops/prometheus/blackbox_exporter/#https-endpoint-with-basic-auth", "title": "HTTPS endpoint with basic auth", "text": "<pre><code>https_basic_auth_2xx:\nprober: http\ntimeout: 5s\nhttp:\nmethod: GET\nfail_if_ssl: false\nfail_if_not_ssl: true\nvalid_http_versions:\n- HTTP/1.1\n- HTTP/2.0\nvalid_status_codes:\n- 200\nno_follow_redirects: false\npreferred_ip_protocol: ip4\nbasic_auth:\nusername: {{ username }}\npassword: {{ password }}\n</code></pre>"}, {"location": "devops/prometheus/blackbox_exporter/#https-endpoint-with-api-key", "title": "HTTPs endpoint with API key", "text": "<pre><code>https_api_2xx:\nprober: http\ntimeout: 5s\nhttp:\nmethod: GET\nfail_if_ssl: false\nfail_if_not_ssl: true\nvalid_http_versions:\n- HTTP/1.1\n- HTTP/2.0\nvalid_status_codes:\n- 200\nno_follow_redirects: false\npreferred_ip_protocol: ip4\nheaders:\napikey: {{ api_key }}\n</code></pre>"}, {"location": "devops/prometheus/blackbox_exporter/#https-put-file", "title": "HTTPS Put file", "text": "<p>Test if the probe can upload a file.</p> <pre><code>    https_put_file_2xx:\nprober: http\ntimeout: 5s\nhttp:\nmethod: PUT\nbody: hi\nfail_if_ssl: false\nfail_if_not_ssl: true\nvalid_http_versions: [\"HTTP/1.1\", \"HTTP/2.0\"]\nvalid_status_codes: [200]\nno_follow_redirects: false\npreferred_ip_protocol: \"ip4\"\n</code></pre>"}, {"location": "devops/prometheus/blackbox_exporter/#check-open-port", "title": "Check open port", "text": "<pre><code>tcp_connect:\nprober: tcp\n</code></pre> <p>The port is specified when using the module.</p> <pre><code>- name: lyz-code.github.io\nurl: lyz-code.github.io:389\nmodule: tcp_connect\n</code></pre>"}, {"location": "devops/prometheus/blackbox_exporter/#check-tcp-with-tls", "title": "Check TCP with TLS", "text": "<p>If you want to test for example if an LDAP is serving the correct certificate on the port 636 you can use:</p> <pre><code>tcp_ssl_connect:\nprober: tcp\ntimeout: 10s\ntls: true\n</code></pre> <pre><code>- name: Ldap\nurl: my-ldap-server:636\nmodule: tcp_ssl_connect\n</code></pre>"}, {"location": "devops/prometheus/blackbox_exporter/#ping-to-the-resource", "title": "Ping to the resource", "text": "<p>Test if the target is alive. It's useful When you don't know what port to check or if it uses UDP.</p> <pre><code>ping:\nprober: icmp\ntimeout: 5s\nicmp:\npreferred_ip_protocol: \"ip4\"\n</code></pre>"}, {"location": "devops/prometheus/blackbox_exporter/#blackbox-exporter-alerts", "title": "Blackbox exporter alerts", "text": "<p>Now that we've got the metrics, we can define the alert rules. Most have been tweaked from the Awesome prometheus alert rules collection.</p> <p>To make security tests</p>"}, {"location": "devops/prometheus/blackbox_exporter/#availability-alerts", "title": "Availability alerts", "text": "<p>The most basic probes, test if the service is up and returning.</p>"}, {"location": "devops/prometheus/blackbox_exporter/#blackbox-probe-failed", "title": "Blackbox probe failed", "text": "<p>Blackbox probe failed.</p> <pre><code>  - alert: BlackboxProbeFailed\nexpr: probe_success == 0\nfor: 5m\nlabels:\nseverity: error\nannotations:\nsummary: \"Blackbox probe failed (instance {{ $labels.target }})\"\nmessage: \"Probe failed\\n  VALUE = {{ $value }}\"\ngrafana: \"{{ grafana_url }}&amp;var-targets={{ $labels.target }}\"\nprometheus: \"{{ prometheus_url }}/graph?g0.expr=probe_success+%3D%3D+0&amp;g0.tab=1\"\n</code></pre> <p>If you use the security alerts, use the following <code>expr:</code> instead</p> <pre><code>    expr: probe_success{target!~\".*-fail-.*$\"} == 0\n</code></pre>"}, {"location": "devops/prometheus/blackbox_exporter/#blackbox-probe-http-failure", "title": "Blackbox probe HTTP failure", "text": "<p>HTTP status code is not 200-399.</p> <pre><code>  - alert: BlackboxProbeHttpFailure\nexpr: probe_http_status_code &lt;= 199 OR probe_http_status_code &gt;= 400\nfor: 5m\nlabels:\nseverity: error\nannotations:\nsummary: \"Blackbox probe HTTP failure (instance {{ $labels.target }})\"\nmessage: \"HTTP status code is not 200-399\\n  VALUE = {{ $value }}\"\ngrafana: \"{{ grafana_url }}&amp;var-targets={{ $labels.target }}\"\nprometheus: \"{{ prometheus_url }}/graph?g0.expr=probe_ssl_earliest_cert_expiry+-+time%28%29+%3C+86400+%2A+30&amp;g0.tab=1\"\n</code></pre>"}, {"location": "devops/prometheus/blackbox_exporter/#performance-alerts", "title": "Performance alerts", "text": ""}, {"location": "devops/prometheus/blackbox_exporter/#blackbox-slow-probe", "title": "Blackbox slow probe", "text": "<p>Blackbox probe took more than 1s to complete.</p> <pre><code>  - alert: BlackboxSlowProbe\nexpr: avg_over_time(probe_duration_seconds[1m]) &gt; 1\nfor: 5m\nlabels:\nseverity: warning\nannotations:\nsummary: \"Blackbox slow probe (target {{ $labels.target }})\"\nmessage: \"Blackbox probe took more than 1s to complete\\n  VALUE = {{ $value }}\"\ngrafana: \"{{ grafana_url }}&amp;var-targets={{ $labels.target }}\"\nprometheus: \"{{ prometheus_url }}/graph?g0.expr=avg_over_time%28probe_duration_seconds%5B1m%5D%29+%3E+1&amp;g0.tab=1\"\n</code></pre> <p>If you use the security alerts, use the following <code>expr:</code> instead</p> <pre><code>    expr: avg_over_time(probe_duration_seconds{,target!~\".*-fail-.*\"}[1m]) &gt; 1\n</code></pre>"}, {"location": "devops/prometheus/blackbox_exporter/#blackbox-probe-slow-http", "title": "Blackbox probe slow HTTP", "text": "<p>HTTP request took more than 1s.</p> <pre><code>  - alert: BlackboxProbeSlowHttp\nexpr: avg_over_time(probe_http_duration_seconds[1m]) &gt; 1\nfor: 5m\nlabels:\nseverity: warning\nannotations:\nsummary: \"Blackbox probe slow HTTP (instance {{ $labels.target }})\"\nmessage: \"HTTP request took more than 1s\\n  VALUE = {{ $value }}\"\ngrafana: \"{{ grafana_url }}&amp;var-targets={{ $labels.target }}\"\nprometheus: \"{{ prometheus_url }}/graph?g0.expr=avg_over_time%28probe_http_duration_seconds%5B1m%5D%29+%3E+1&amp;g0.tab=1\"\n</code></pre> <p>If you use the security alerts, use the following <code>expr:</code> instead</p> <pre><code>    expr: avg_over_time(probe_http_duration_seconds{,target!~\".*-fail-.*\"}[1m]) &gt; 1\n</code></pre>"}, {"location": "devops/prometheus/blackbox_exporter/#blackbox-probe-slow-ping", "title": "Blackbox probe slow ping", "text": "<p>Blackbox ping took more than 1s.</p> <pre><code>  - alert: BlackboxProbeSlowPing\nexpr: avg_over_time(probe_icmp_duration_seconds[1m]) &gt; 1\nfor: 5m\nlabels:\nseverity: warning\nannotations:\nsummary: \"Blackbox probe slow ping (instance {{ $labels.target }})\"\nmessage: \"Blackbox ping took more than 1s\\n  VALUE = {{ $value }}\"\ngrafana: \"{{ grafana_url }}&amp;var-targets={{ $labels.target }}\"\nprometheus: \"{{ prometheus_url }}/graph?g0.expr=avg_over_time%28probe_icmp_duration_seconds%5B1m%5D%29+%3E+1&amp;g0.tab=1\"\n</code></pre>"}, {"location": "devops/prometheus/blackbox_exporter/#ssl-certificate-alerts", "title": "SSL certificate alerts", "text": ""}, {"location": "devops/prometheus/blackbox_exporter/#blackbox-ssl-certificate-will-expire-in-a-month", "title": "Blackbox SSL certificate will expire in a month", "text": "<p>SSL certificate expires in 30 days.</p> <pre><code>  - alert: BlackboxSslCertificateWillExpireSoon\nexpr: probe_ssl_earliest_cert_expiry - time() &lt; 86400 * 30\nfor: 5m\nlabels:\nseverity: warning\nannotations:\nsummary: \"Blackbox SSL certificate will expire soon (instance {{ $labels.target }})\"\nmessage: \"SSL certificate expires in 30 days\\n  VALUE = {{ $value }}\"\ngrafana: \"{{ grafana_url }}&amp;var-targets={{ $labels.target }}\"\nprometheus: \"{{ prometheus_url }}/graph?g0.expr=probe_ssl_earliest_cert_expiry+-+time%28%29+%3C+86400+%2A+30&amp;g0.tab=1\"\n</code></pre>"}, {"location": "devops/prometheus/blackbox_exporter/#blackbox-ssl-certificate-will-expire-in-a-few-days", "title": "Blackbox SSL certificate will expire in a few days", "text": "<p>SSL certificate expires in 3 days.</p> <pre><code>  - alert: BlackboxSslCertificateWillExpireSoon\nexpr: probe_ssl_earliest_cert_expiry - time() &lt; 86400 * 3\nfor: 5m\nlabels:\nseverity: error\nannotations:\nsummary: \"Blackbox SSL certificate will expire soon (instance {{ $labels.target }})\"\nmessage: \"SSL certificate expires in 3 days\\n  VALUE = {{ $value }}\"\ngrafana: \"{{ grafana_url }}&amp;var-targets={{ $labels.target }}\"\nprometheus: \"{{ prometheus_url }}/graph?g0.expr=probe_ssl_earliest_cert_expiry+-+time%28%29+%3C+86400+%2A+3&amp;g0.tab=1\"\n</code></pre>"}, {"location": "devops/prometheus/blackbox_exporter/#blackbox-ssl-certificate-expired", "title": "Blackbox SSL certificate expired", "text": "<p>SSL certificate has expired already.</p> <pre><code>  - alert: BlackboxSslCertificateExpired\nexpr: probe_ssl_earliest_cert_expiry - time() &lt;= 0\nfor: 5m\nlabels:\nseverity: error\nannotations:\nsummary: \"Blackbox SSL certificate expired (instance {{ $labels.target }})\"\nmessage: \"SSL certificate has expired already\\n  VALUE = {{ $value }}\"\ngrafana: \"{{ grafana_url }}&amp;var-targets={{ $labels.target }}\"\nprometheus: \"{{ prometheus_url }}/graph?g0.expr=probe_ssl_earliest_cert_expiry+-+time%28%29+%3C%3D+0&amp;g0.tab=1\"\n</code></pre>"}, {"location": "devops/prometheus/blackbox_exporter/#security-alerts", "title": "Security alerts", "text": "<p>To define the security alerts, I've found easier to create a probe with the action I want to prevent and make sure that the probe fails.</p> <p>This probes contain the <code>-fail-</code> key in the target name, followed by the test it's performing. This convention allows the concatenation of tests. For example, when testing if and endpoint is accessible without basic auth and without vpn we'd use:</p> <pre><code>- name: protected.endpoint.org-fail-without-ssl-and-without-credentials\nurl: protected.endpoint.org\nmodule: https_external_2xx\n</code></pre>"}, {"location": "devops/prometheus/blackbox_exporter/#test-endpoints-protected-with-network-policies", "title": "Test endpoints protected with network policies", "text": "<p>Assuming that the blackbox exporter is in the internal network and that there is an http proxy on the external network we want to test. Create a working probe with the <code>https_external_2xx</code> module containing the <code>-fail-without-vpn</code> key in the target name.</p> <pre><code>  - alert: BlackboxVPNProtectionRemoved\nexpr: probe_success{target=~\".*-fail-.*without-vpn.*\"} == 1\nfor: 5m\nlabels:\nseverity: error\nannotations:\nsummary: \"VPN protection was removed from (instance {{ $labels.target }})\"\nmessage: \"Successful probe to the endpoint from outside the internal network\"\ngrafana: \"{{ grafana_url }}&amp;var-targets={{ $labels.target }}\"\nprometheus: \"{{ prometheus_url }}/graph?g0.expr=probe_ssl_earliest_cert_expiry+-+time%28%29+%3C%3D+0&amp;g0.tab=1\"\n</code></pre>"}, {"location": "devops/prometheus/blackbox_exporter/#test-endpoints-protected-with-ssl-client-certificate", "title": "Test endpoints protected with SSL client certificate", "text": "<p>Create a working probe with a module without the SSL client certificate configured, such as <code>https_2xx</code> and set the <code>-fail-without-ssl</code> key in the target name.</p> <pre><code>  - alert: BlackboxClientSSLProtectionRemoved\nexpr: probe_success{target=~\".*-fail-.*without-ssl.*\"} == 1\nfor: 5m\nlabels:\nseverity: error\nannotations:\nsummary: \"SSL client certificate protection was removed from (instance {{ $labels.target }})\"\nmessage: \"Successful probe to the endpoint without SSL certificate\"\ngrafana: \"{{ grafana_url }}&amp;var-targets={{ $labels.target }}\"\nprometheus: \"{{ prometheus_url }}/graph?g0.expr=probe_ssl_earliest_cert_expiry+-+time%28%29+%3C%3D+0&amp;g0.tab=1\"\n</code></pre>"}, {"location": "devops/prometheus/blackbox_exporter/#test-endpoints-protected-with-credentials", "title": "Test endpoints protected with credentials.", "text": "<p>Create a working probe with a module without the basic auth credentials configured, such as <code>https_2xx</code> and set the <code>-fail-without-credentials</code> key in the target name.</p> <pre><code>  - alert: BlackboxCredentialsProtectionRemoved\nexpr: probe_success{target=~\".*-fail-.*without-credentials.*\"} == 1\nfor: 5m\nlabels:\nseverity: error\nannotations:\nsummary: \"Credentials protection was removed from (instance {{ $labels.target }})\"\nmessage: \"Successful probe to the endpoint without credentials\"\ngrafana: \"{{ grafana_url }}&amp;var-targets={{ $labels.target }}\"\nprometheus: \"{{ prometheus_url }}/graph?g0.expr=probe_ssl_earliest_cert_expiry+-+time%28%29+%3C%3D+0&amp;g0.tab=1\"\n</code></pre>"}, {"location": "devops/prometheus/blackbox_exporter/#test-endpoints-protected-with-waf", "title": "Test endpoints protected with WAF.", "text": "<p>Create a working probe with a module bypassing the WAF, for example directly attacking the service and set the <code>-fail-without-waf</code> key in the target name.</p> <pre><code>  - alert: BlackboxWAFProtectionRemoved\nexpr: probe_success{target=~\".*-fail-.*without-waf.*\"} == 1\nfor: 5m\nlabels:\nseverity: error\nannotations:\nsummary: \"WAF protection was removed from (instance {{ $labels.target }})\"\nmessage: \"Successful probe to the haproxy endpoint from the internal network (bypassed the WAF)\"\ngrafana: \"{{ grafana_url }}&amp;var-targets={{ $labels.target }}\"\nprometheus: \"{{ prometheus_url }}/graph?g0.expr=probe_ssl_earliest_cert_expiry+-+time%28%29+%3C%3D+0&amp;g0.tab=1\"\n</code></pre>"}, {"location": "devops/prometheus/blackbox_exporter/#unauthorized-read-of-s3-buckets", "title": "Unauthorized read of S3 buckets", "text": "<p>Create a working probe to an existent private object in an S3 bucket and set the <code>-fail-read-object</code> key in the target name.</p> <pre><code>  - alert: BlackboxS3BucketWrongReadPermissions\nexpr: probe_success{target=~\".*-fail-.*read-object.*\"} == 1\nfor: 5m\nlabels:\nseverity: error\nannotations:\nsummary: \"Wrong read permissions on S3 bucket (instance {{ $labels.target }})\"\nmessage: \"Successful read of a private object with an unauthenticated user\"\ngrafana: \"{{ grafana_url }}&amp;var-targets={{ $labels.target }}\"\nprometheus: \"{{ prometheus_url }}/graph?g0.expr=probe_ssl_earliest_cert_expiry+-+time%28%29+%3C%3D+0&amp;g0.tab=1\"\n</code></pre>"}, {"location": "devops/prometheus/blackbox_exporter/#unauthorized-write-of-s3-buckets", "title": "Unauthorized write of S3 buckets", "text": "<p>Create a working probe using the <code>https_put_file_2xx</code> module to try to create a file in an S3 bucket and set the <code>-fail-write-object</code> key in the target name.</p> <pre><code>  - alert: BlackboxS3BucketWrongWritePermissions\nexpr: probe_success{target=~\".*-fail-.*write-object.*\"} == 1\nfor: 5m\nlabels:\nseverity: error\nannotations:\nsummary: \"Wrong write permissions on S3 bucket (instance {{ $labels.target }})\"\nmessage: \"Successful write of a private object with an unauthenticated user\"\ngrafana: \"{{ grafana_url }}&amp;var-targets={{ $labels.target }}\"\nprometheus: \"{{ prometheus_url }}/graph?g0.expr=probe_ssl_earliest_cert_expiry+-+time%28%29+%3C%3D+0&amp;g0.tab=1\"\n</code></pre>"}, {"location": "devops/prometheus/blackbox_exporter/#monitoring-external-access-to-internal-services", "title": "Monitoring external access to internal services", "text": "<p>There are two possible solutions to simulate traffic from outside your infrastructure to the internal services. Both require the installation of an agent outside of your internal infrastructure, it can be:</p> <ul> <li>An HTTP proxy.</li> <li>A blackbox exporter instance.</li> </ul> <p>Using the proxy you have following advantages:</p> <ul> <li>It's really easy to set up a transparent http     proxy.</li> <li>All probe configuration goes in the same blackbox exporter instance     <code>values.yaml</code>.</li> </ul> <p>With the following disadvantages:</p> <ul> <li> <p>When using an external http proxy, the probe runs the DNS resolution locally. Therefore if the record doesn't exist in the local server the probe will fail, even if the proxy DNS resolver has the correct record.</p> <p>The ugly workaround I've implemented is to create a \"fake\" DNS record in my internal DNS server so the probe sees it exist. * There is no way to do <code>tcp</code> or <code>ping</code> probes to simulate external traffic. * The latency between the blackbox exporter and the proxy is added to all the external probes.</p> </li> </ul> <p>While using an external blackbox exporter gives the following advantages:</p> <ul> <li>Traffic is completely external to the infrastructure, so the proxy     disadvantages would be solved.</li> </ul> <p>And the following disadvantages:</p> <ul> <li> <p>Simulation of external traffic in AWS could be done by spawning the blackbox     exporter instance in another region, but as there is no way of using EKS     worker nodes in different regions, there is no way of managing the exporter     from within Kubernetes. This means:</p> <ul> <li>The loose of the advantages of the Prometheus   operator, so we have to write the configuration   manually.</li> <li>Configuration can't be managed with Helm, so two solutions     should be used to manage the monitorization (Ansible could be used).</li> <li>Even if it's possible to host the second external blackbox exporter within Kubernetes, two independent Helm charts are needed, with the consequent configuration management burden.</li> </ul> </li> </ul> <p>In conclusion, when using a Kubernetes cluster that allows the creation of worker nodes outside the main infrastructure, or if several non HTTP/HTTPS endpoints need to be probed with the <code>tcp</code> or <code>ping</code> modules, install an external blackbox exporter instance. Otherwise install an HTTP proxy and assume that you can only simulate external HTTP/HTTPS traffic.</p>"}, {"location": "devops/prometheus/blackbox_exporter/#troubleshooting", "title": "Troubleshooting", "text": "<p>To get more debugging information of the blackbox probes, add <code>&amp;debug=true</code> to the probe url, for example http://localhost:9115/probe?module=http_2xx&amp;target=https://www.prometheus.io/&amp;debug=true .</p>"}, {"location": "devops/prometheus/blackbox_exporter/#service-monitors-are-not-being-created", "title": "Service monitors are not being created", "text": "<p>When running <code>helmfile apply</code> several times to update the resources, some are not being correctly created. Until the bug is solved, a workaround is to remove the chart release <code>helm delete --purge prometeus-blackbox-exporter</code> and running <code>helmfile apply</code> again.</p>"}, {"location": "devops/prometheus/blackbox_exporter/#probe_success-0-when-using-an-http-proxy", "title": "probe_success == 0 when using an http proxy", "text": "<p>Even when using an external http proxy, the probe runs the DNS resolution locally. Therefore if the record doesn't exist in the local server the probe will fail, even if the proxy DNS resolver has the correct record.</p> <p>The ugly workaround I've implemented is to create a \"fake\" DNS record in my internal DNS server so the probe sees it exist.</p>"}, {"location": "devops/prometheus/blackbox_exporter/#links", "title": "Links", "text": "<ul> <li>Git.</li> <li>Blackbox exporter modules   configuration.</li> <li>Devconnected introduction to blackbox   exporter.</li> </ul>"}, {"location": "devops/prometheus/instance_sizing_analysis/", "title": "Instance sizing analysis", "text": "<p>Once we gather the instance metrics with the Node exporter, we can do statistical analysis on the evolution of time to detect the instances that are undersized or oversized.</p>"}, {"location": "devops/prometheus/instance_sizing_analysis/#ram-analysis", "title": "RAM analysis", "text": "<p>Instance RAM percent usage metric can be calculated with the following Prometheus rule:</p> <pre><code>  - record: instance_path:node_memory_MemAvailable_percent\nexpr: (1 - node_memory_MemAvailable_bytes/node_memory_MemTotal_bytes ) * 100\n</code></pre> <p>The average, standard deviation and the standard score of the last two weeks would be:</p> <pre><code>  - record: instance_path:node_memory_MemAvailable_percent:avg_over_time_2w\nexpr: avg_over_time(instance_path:node_memory_MemAvailable_percent[2w])\n- record: instance_path:node_memory_MemAvailable_percent:stddev_over_time_2w\nexpr: stddev_over_time(instance_path:node_memory_MemAvailable_percent[2w])\n- record: instance_path:node_memory_MemAvailable_percent:z_score\nexpr: &gt;\n(\ninstance_path:node_memory_MemAvailable_percent\n- instance_path:node_memory_MemAvailable_percent:avg_over_time_2w\n) / instance_path:node_memory_MemAvailable_percent:stddev_over_time_2w\n</code></pre> <p>With that data we can define that an instance is oversized if the average plus the standard deviation is less than 60% and undersized if its greater than 90%.</p> <p>With the average we take into account the nominal RAM consumption, and with the standard deviation we take into account the spikes.</p> <p>Tweak this rule to your use case</p> <p>The criteria of undersized and oversized is just a first approximation I'm going to use. You can use it as a base criteria, but don't go through with it blindly.</p> <p>See the disclaimer below for more information.</p> <pre><code>  # RAM\n- record: instance_path:wrong_resource_size\nexpr: &gt;\ninstance_path:node_memory_MemAvailable_percent:avg_plus_stddev_over_time_2w &lt; 60\nlabels:\ntype: EC2\nmetric: RAM\nproblem: oversized\n- record: instance_path:wrong_resource_size\nexpr: &gt;\ninstance_path:node_memory_MemAvailable_percent:avg_plus_stddev_over_time_2w &gt; 90\nlabels:\ntype: EC2\nmetric: RAM\nproblem: undersized\n</code></pre> <p>Where <code>avg_plus_stddev_over_time_2w</code> is:</p> <pre><code>  - record: instance_path:node_memory_MemAvailable_percent:avg_plus_stddev_over_time_2w\nexpr: &gt;\ninstance_path:node_memory_MemAvailable_percent:avg_over_time_2w\n+ instance_path:node_memory_MemAvailable_percent:stddev_over_time_2w\n</code></pre>"}, {"location": "devops/prometheus/instance_sizing_analysis/#cpu-analysis", "title": "CPU analysis", "text": "<p>Instance CPU percent usage metric can be calculated with the following Prometheus rule:</p> <pre><code>  - record: instance_path:node_cpu_percent:rate1m\nexpr: &gt;\n(1 - (avg by(instance) (rate(node_cpu_seconds_total{mode=\"idle\"}[1m])))) * 100\n</code></pre> <p>The <code>node_cpu_seconds_total</code> doesn't give us the percent of usage, that is why we need to do the average of the <code>rate</code> of the last minute.</p> <p>The average, standard deviation, the standard score and the undersize or oversize criteria is similar to the RAM case, so I'm adding it folded for reference only.</p> CPU usage rules <pre><code># ---------------------------------------\n# -- Resource consumption calculations --\n# ---------------------------------------\n\n# CPU\n- record: instance_path:node_cpu_percent:rate1m\nexpr: &gt;\n(1 - (avg by(instance) (rate(node_cpu_seconds_total{mode=\"idle\"}[1m])))) * 100\n- record: instance_path:node_cpu_percent:rate1m:avg_over_time_2w\nexpr: avg_over_time(instance_path:node_cpu_percent:rate1m[2w])\n- record: instance_path:node_cpu_percent:rate1m:stddev_over_time_2w\nexpr: stddev_over_time(instance_path:node_cpu_percent:rate1m[2w])\n- record: instance_path:node_cpu_percent:rate1m:avg_plus_stddev_over_time_2w\nexpr: &gt;\ninstance_path:node_cpu_percent:rate1m:avg_over_time_2w\n+ instance_path:node_cpu_percent:rate1m:stddev_over_time_2w\n- record: instance_path:node_cpu_percent:rate1m:z_score\nexpr: &gt;\n(\ninstance_path:node_cpu_percent:rate1m\n- instance_path:node_cpu_percent:rate1m:avg_over_time_2w\n) / instance_path:node_cpu_percent:rate1m:stddev_over_time_2w\n\n# ----------------------------------\n# -- Resource sizing calculations --\n# ----------------------------------\n\n# CPU\n- record: instance_path:wrong_resource_size\nexpr: instance_path:node_cpu_percent:rate1m:avg_plus_stddev_over_time_2w &lt; 60\nlabels:\ntype: EC2\nmetric: CPU\nproblem: oversized\n- record: instance_path:wrong_resource_size\nexpr: instance_path:node_cpu_percent:rate1m:avg_plus_stddev_over_time_2w &gt; 80\nlabels:\ntype: EC2\nmetric: CPU\nproblem: undersized\n</code></pre>"}, {"location": "devops/prometheus/instance_sizing_analysis/#network-analysis", "title": "Network analysis", "text": "<p>We can deduce the network usage from the <code>node_network_receive_bytes_total</code> and <code>node_network_transmit_bytes_total</code> metrics. For example for the transmit, the Gigabits per second transmitted can be calculated with the following Prometheus rule:</p> <pre><code>  - record: instance_path:node_network_transmit_gigabits_per_second:rate5m\nexpr: &gt;\nincrease(\nnode_network_transmit_bytes_total{device=~\"(eth0|ens.*)\"}[1m]\n) * 7.450580596923828 * 10^-9 / 60\n</code></pre> <p>Where we:</p> <ul> <li>Filter the traffic only to the external network interfaces     <code>node_network_transmit_bytes_total{device=~\"(eth0|ens.*)\"}</code>. Those are the     ones used by AWS, but you'll need to tweak that for your case.</li> <li>Convert the <code>increase</code> of Kilobytes per minute <code>[1m]</code> to Gigabits per second     by multiplying it by <code>7.450580596923828 * 10^-9 / 60</code>.</li> </ul> <p>The average, standard deviation, the standard score and the undersize or oversize criteria is similar to the RAM case, so I'm adding it folded for reference only.</p> Network usage rules <pre><code># ---------------------------------------\n# -- Resource consumption calculations --\n# ---------------------------------------\n\n# NetworkReceive\n- record: instance_path:node_network_receive_gigabits_per_second:rate1m\nexpr: &gt;\nincrease(\nnode_network_receive_bytes_total{device=~\"(eth0|ens.*)\"}[1m]\n) * 7.450580596923828 * 10^-9 / 60\n- record: instance_path:node_network_receive_gigabits_per_second:rate1m:avg_over_time_2w\nexpr: &gt;\navg_over_time(\ninstance_path:node_network_receive_gigabits_per_second:rate1m[2w]\n)\n- record: instance_path:node_network_receive_gigabits_per_second:rate1m:stddev_over_time_2w\nexpr: &gt;\nstddev_over_time(\ninstance_path:node_network_receive_gigabits_per_second:rate1m[2w]\n)\n- record: instance_path:node_network_receive_gigabits_per_second:rate1m:avg_plus_stddev_over_time_2w\nexpr: &gt;\ninstance_path:node_network_receive_gigabits_per_second:rate1m:avg_over_time_2w\n+ instance_path:node_network_receive_gigabits_per_second:rate1m:stddev_over_time_2w\n- record: instance_path:node_network_receive_gigabits_per_second:rate1m:z_score\nexpr: &gt;\n(\ninstance_path:node_network_receive_gigabits_per_second:rate1m\n- instance_path:node_network_receive_gigabits_per_second:rate1m:avg_over_time_2w\n) / instance_path:node_network_receive_gigabits_per_second:rate1m:stddev_over_time_2w\n\n# NetworkTransmit\n- record: instance_path:node_network_transmit_gigabits_per_second:rate1m\nexpr: &gt;\nincrease(\nnode_network_transmit_bytes_total{device=~\"(eth0|ens.*)\"}[1m]\n) * 7.450580596923828 * 10^-9 / 60\n- record: instance_path:node_network_transmit_gigabits_per_second:rate1m:avg_over_time_2w\nexpr: &gt;\navg_over_time(\ninstance_path:node_network_transmit_gigabits_per_second:rate1m[2w]\n)\n- record: instance_path:node_network_transmit_gigabits_per_second:rate1m:stddev_over_time_2w\nexpr: &gt;\nstddev_over_time(\ninstance_path:node_network_transmit_gigabits_per_second:rate1m[2w]\n)\n- record: instance_path:node_network_transmit_gigabits_per_second:rate1m:avg_plus_stddev_over_time_2w\nexpr: &gt;\ninstance_path:node_network_transmit_gigabits_per_second:rate1m:avg_over_time_2w\n+ instance_path:node_network_transmit_gigabits_per_second:rate1m:stddev_over_time_2w\n- record: instance_path:node_network_transmit_gigabits_per_second:rate1m:z_score\nexpr: &gt;\n(\ninstance_path:node_network_transmit_gigabits_per_second:rate1m\n- instance_path:node_network_transmit_gigabits_per_second:rate1m:avg_over_time_2w\n) / instance_path:node_network_transmit_gigabits_per_second:rate1m:stddev_over_time_2w\n\n# ----------------------------------\n# -- Resource sizing calculations --\n# ----------------------------------\n# NetworkReceive\n- record: instance_path:wrong_resource_size\nexpr: &gt;\ninstance_path:node_network_receive_gigabits_per_second:rate1m:avg_plus_stddev_over_time_2w &lt; 0.5\nlabels:\ntype: EC2\nmetric: NetworkReceive\nproblem: oversized\n- record: instance_path:wrong_resource_size\nexpr: &gt;\ninstance_path:node_network_receive_gigabits_per_second:rate1m:avg_plus_stddev_over_time_2w &gt; 3\nlabels:\ntype: EC2\nmetric: NetworkReceive\nproblem: undersized\n\n# NetworkTransmit\n- record: instance_path:wrong_resource_size\nexpr: &gt;\ninstance_path:node_network_transmit_gigabits_per_second:rate1m:avg_plus_stddev_over_time_2w &lt; 0.5\nlabels:\ntype: EC2\nmetric: NetworkTransmit\nproblem: oversized\n- record: instance_path:wrong_resource_size\nexpr: &gt;\ninstance_path:node_network_transmit_gigabits_per_second:rate1m:avg_plus_stddev_over_time_2w &gt; 3\nlabels:\ntype: EC2\nmetric: NetworkTransmit\nproblem: undersized\n</code></pre> <p>The difference with network is that we don't have a percent of the total instance bandwidth, In my case, my instances support from 0.5 to 5 Gbps which is more than I need, so most of my instances are marked as oversized with the <code>&lt; 0.5</code> rule. I will manually study the ones that go over 3 Gbps.</p> <p>The correct way to do it, is to tag the baseline, burst or/and maximum network performance by instance type. In the AWS case, the data can be extracted using the AWS docs or external benchmarks.</p> <p>Once you know the network performance per instance type, you can use relabeling in the Node exporter service monitor to add a label like <code>max_network_performance</code> and use it later in the rules.</p> <p>If you do follow this path, please contact me or do a pull request so I can test your solution.</p>"}, {"location": "devops/prometheus/instance_sizing_analysis/#overall-analysis", "title": "Overall analysis", "text": "<p>Now that we have all the analysis under the metric <code>instance_path:wrong_resource_size</code> with labels, we can aggregate them to see the number of rules each instance is breaking with the following rule:</p> <pre><code>  # Mark the number of oversize rules matched by each instance\n- record: instance_path:wrong_instance_size\nexpr: count by (instance) (sum by (metric, instance) (instance_path:wrong_resource_size))\n</code></pre> <p>By executing <code>sort_desc(instance_path:wrong_instance_size)</code> in the Prometheus web application, we'll be able to see such instances.</p> <pre><code>instance_path:wrong_instance_size{instance=\"frontend-production:192.168.1.2\"}   4\ninstance_path:wrong_instance_size{instance=\"backend-production-instance:172.30.0.195\"}  2\n...\n</code></pre> <p>To see the detail of what rules is our instance breaking we can use something like <code>instance_path:wrong_resource_size{instance =~'frontend.*'}</code></p> <pre><code>instance_path:wrong_resource_size{instance=\"fronted-production:192.168.1.2\",instance_type=\"c4.2xlarge\",job=\"node-exporter\",metric=\"RAM\",problem=\"oversized\",type=\"EC2\"} 5.126602454544287\ninstance_path:wrong_resource_size{instance=\"fronted-production:192.168.1.2\",metric=\"CPU\",problem=\"oversized\",type=\"EC2\"}    0.815639209497615\ninstance_path:wrong_resource_size{device=\"ens3\",instance=\"fronted-production:192.168.1.2\",instance_type=\"c4.2xlarge\",job=\"node-exporter\",metric=\"NetworkReceive\",problem=\"oversized\",type=\"EC2\"}    0.02973250128744766\ninstance_path:wrong_resource_size{device=\"ens3\",instance=\"fronted-production:192.168.1.2\",instance_type=\"c4.2xlarge\",job=\"node-exporter\",metric=\"NetworkTransmit\",problem=\"oversized\",type=\"EC2\"}   0.01586461503849804\n</code></pre> <p>Here we see that the <code>frontend-production</code> is a <code>c4.2xlarge</code> instance that is consuming an average plus standard deviation of CPU of 0.81%, RAM 5.12%, NetworkTransmit 0.015Gbps and NetworkReceive 0.029Gbps, which results in an <code>oversized</code> alert on all four metrics.</p> <p>If you want to see the evolution over the time, instead of <code>Console</code> click on <code>Graph</code> under the text box where you have entered the query.</p> <p>With this information, we can decide which is the correct instance for each application. Once all instances are migrated to their ideal size, we can add alerts on these metrics so we can have a continuous analysis of our instances. Once I've done it, I'll add the alerts here.</p>"}, {"location": "devops/prometheus/instance_sizing_analysis/#disclaimer", "title": "Disclaimer", "text": "<p>We haven't tested this rules yet in production to resize our infrastructure (will do soon), so use all the information in this document cautiously.</p> <p>What I can expect to fail is that the assumption of average plus a standard deviation criteria can not be enough, maybe I need to increase the resolution of the standard deviation so it can be more sensible to the spikes, or we need to use a safety factor of 2 or 3. We'll see :)</p> <p>Read throughly the Gitlab post on anomaly detection using Prometheus, it's awesome and it may give you insights on why this approach is not working with you, as well as other algorithms that for example take into account the seasonality of the metrics.</p> <p>In particular it's interesting to analyze your resources <code>z-score</code> evolution over time, if all values fall in the <code>+4</code> to <code>-4</code> range, you can statistically assert that your metric similarly follows the normal distribution, and can assume that any value of z_score above 3 is an anomaly. If your results return with a range of <code>+20</code> to <code>-20</code>, the tail is too long and your results will be skewed.</p> <p>To test it you can use the following queries to test the RAM behaviour, adapt them for the rest of the resources:</p> <pre><code># Minimum z_score value\n\nsort_desc(abs((min_over_time(instance_path:node_memory_MemAvailable_percent[1w]) - instance_path:node_memory_MemAvailable_percent:avg_over_time_2w) / instance_path:node_memory_MemAvailable_percent:stddev_over_time_2w))\n\n# Maximum z_score value\n\nsort_desc(abs((max_over_time(instance_path:node_memory_MemAvailable_percent[1w]) - instance_path:node_memory_MemAvailable_percent:avg_over_time_2w) / instance_path:node_memory_MemAvailable_percent:stddev_over_time_2w))\n</code></pre> <p>For a less exhaustive but more graphical analysis, execute <code>instance_path:node_memory_MemAvailable_percent:z_score</code> in <code>Graph</code> mode. In my case the RAM is in the +-5 interval, with some peaks of 20, but after reviewing <code>instance_path:node_memory_MemAvailable_percent:avg_plus_stddev_over_time_2w</code> in those periods, I feel it's still safe to use the assumption.</p> <p>Same criteria applies to <code>instance_path:node_cpu_percent:rate1m:z_score</code>, <code>instance_path:node_network_receive_gigabits_per_second:rate1m:z_score</code>, and <code>instance_path:node_network_transmit_gigabits_per_second:rate1m:z_score</code>, metrics.</p>"}, {"location": "devops/prometheus/instance_sizing_analysis/#references", "title": "References", "text": "<ul> <li>Gitlab post on anomaly detection using     Prometheus.</li> </ul>"}, {"location": "devops/prometheus/node_exporter/", "title": "Node Exporter", "text": "<p>Node Exporter is a Prometheus exporter for hardware and OS metrics exposed by *NIX kernels, written in Go with pluggable metric collectors.</p>"}, {"location": "devops/prometheus/node_exporter/#install", "title": "Install", "text": "<p>To install in kubernetes nodes, use this chart. Elsewhere use this ansible role.</p> <p>If you use node exporter agents outside kubernetes, you need to configure a prometheus service discovery to scrap the information from them.</p> <p>To auto discover EC2 instances use the ec2_sd_config configuration. It can be added in the helm chart values.yaml under the key <code>prometheus.prometheusSpec.additionalScrapeConfigs</code>.</p> <pre><code>      - job_name: node_exporter\nec2_sd_configs:\n- region: us-east-1\nport: 9100\nrefresh_interval: 1m\nrelabel_configs:\n- source_labels: ['__meta_ec2_tag_Name', '__meta_ec2_private_ip']\nseparator: ':'\ntarget_label: instance\n- source_labels:\n- __meta_ec2_instance_type\ntarget_label: instance_type\n</code></pre> <p>The <code>relabel_configs</code> part will substitute the <code>instance</code> label of each target from <code>{{ instance_ip }}:9100</code> to <code>{{ instance_name }}:{{ instance_ip }}</code>.</p> <p>If the worker nodes already have an IAM role with the <code>ec2:DescribeInstances</code> permission there is no need to specify the <code>role_arn</code> or <code>access_keys</code> and <code>secret_key</code>.</p> <p>If you have stopped instances, the node exporter will raise an alert because it won't be able to scrape the metrics from them. To only fetch data from running instances add a filter:</p> <pre><code>        ec2_sd_configs:\n- region: us-east-1\nfilters:\n- name: instance-state-name\nvalues:\n- running\n</code></pre> <p>To monitor only the instances of a list of VPCs use this filter:</p> <pre><code>        ec2_sd_configs:\n- region: us-east-1\nfilters:\n- name: vpc-id\nvalues:\n- vpc-xxxxxxxxxxxxxxxxx\n- vpc-yyyyyyyyyyyyyyyyy\n</code></pre> <p>By default, prometheus will try to scrape the private instance ip. To use the public one you need to relabel it with the following snippet:</p> <pre><code>        ec2_sd_configs:\n- region: us-east-1\nrelabel_configs:\n- source_labels: ['__meta_ec2_public_ip']\nregex: ^(.*)$\ntarget_label: __address__\nreplacement: ${1}:9100\n</code></pre> <p>I'm using the <code>11074</code> grafana dashboards for the blackbox exporter,  which worked straight out of the box. Taking as reference the grafana helm chart values, add the following yaml under the <code>grafana</code> key in the <code>prometheus-operator</code> <code>values.yaml</code>.</p> <pre><code>grafana:\nenabled: true\ndefaultDashboardsEnabled: true\ndashboardProviders:\ndashboardproviders.yaml:\napiVersion: 1\nproviders:\n- name: 'default'\norgId: 1\nfolder: ''\ntype: file\ndisableDeletion: false\neditable: true\noptions:\npath: /var/lib/grafana/dashboards/default\ndashboards:\ndefault:\nnode_exporter:\n# Ref: https://grafana.com/dashboards/11074\ngnetId: 11074\nrevision: 4\ndatasource: Prometheus\n</code></pre> <p>And install.</p> <pre><code>helmfile diff\nhelmfile apply\n</code></pre>"}, {"location": "devops/prometheus/node_exporter/#node-exporter-size-analysis", "title": "Node exporter size analysis", "text": "<p>Once the instance metrics are being ingested, we can do a periodic analysis to deduce which instances are undersized or oversized.</p>"}, {"location": "devops/prometheus/node_exporter/#node-exporter-alerts", "title": "Node exporter alerts", "text": "<p>Now that we've got the metrics, we can define the alert rules. Most have been tweaked from the Awesome prometheus alert rules collection.</p>"}, {"location": "devops/prometheus/node_exporter/#host-out-of-memory", "title": "Host out of memory", "text": "<p>Node memory is filling up (<code>&lt; 10%</code> left).</p> <pre><code>- alert: HostOutOfMemory\nexpr: node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes * 100 &lt; 10\nfor: 5m\nlabels:\nseverity: warning\nannotations:\nsummary: \"Host out of memory (instance {{ $labels.instance }})\"\nmessage: \"Node memory is filling up (&lt; 10% left)\\n  VALUE = {{ $value }}\"\ngrafana: \"{{ grafana_url}}?var-job=node_exporter&amp;var-hostname=All&amp;var-node={{ $labels.instance }}\"\n</code></pre>"}, {"location": "devops/prometheus/node_exporter/#host-memory-under-memory-pressure", "title": "Host memory under memory pressure", "text": "<p>The node is under heavy memory pressure. High rate of major page faults.</p> <pre><code>- alert: HostMemoryUnderMemoryPressure\nexpr: rate(node_vmstat_pgmajfault[1m]) &gt; 1000\nfor: 5m\nlabels:\nseverity: warning\nannotations:\nsummary: \"Host memory under memory pressure (instance {{ $labels.instance }})\"\nmessage: \"The node is under heavy memory pressure. High rate of major page faults.\"\ngrafana: \"{{ grafana_url}}?var-job=node_exporter&amp;var-hostname=All&amp;var-node={{ $labels.instance }}\"\n</code></pre>"}, {"location": "devops/prometheus/node_exporter/#host-unusual-network-throughput-in", "title": "Host unusual network throughput in", "text": "<p>Host network interfaces are probably receiving too much data (&gt; 100 MB/s)</p> <pre><code>- alert: HostUnusualNetworkThroughputIn\nexpr: sum by (instance) (irate(node_network_receive_bytes_total[2m])) / 1024 / 1024 &gt; 100\nfor: 5m\nlabels:\nseverity: warning\nannotations:\nsummary: \"Host unusual network throughput in (instance {{ $labels.instance }})\"\nmessage: \"Host network interfaces are probably receiving too much data (&gt; 100 MB/s)\\n  VALUE = {{ $value }}.\"\ngrafana: \"{{ grafana_url}}?var-job=node_exporter&amp;var-hostname=All&amp;var-node={{ $labels.instance }}\"\n</code></pre>"}, {"location": "devops/prometheus/node_exporter/#host-unusual-network-throughput-out", "title": "Host unusual network throughput out", "text": "<p>Host network interfaces are probably sending too much data (&gt; 100 MB/s)</p> <pre><code>- alert: HostUnusualNetworkThroughputOut\nexpr: sum by (instance) (irate(node_network_transmit_bytes_total[2m])) / 1024 / 1024 &gt; 100\nfor: 5m\nlabels:\nseverity: warning\nannotations:\nsummary: \"Host unusual network throughput out (instance {{ $labels.instance }})\"\nmessage: \"Host network interfaces are probably sending too much data (&gt; 100 MB/s)\\n  VALUE = {{ $value }}.\"\ngrafana: \"{{ grafana_url}}?var-job=node_exporter&amp;var-hostname=All&amp;var-node={{ $labels.instance }}\"\n</code></pre>"}, {"location": "devops/prometheus/node_exporter/#host-unusual-disk-read-rate", "title": "Host unusual disk read rate", "text": "<p>Disk is probably reading too much data (&gt; 50 MB/s)</p> <pre><code>- alert: HostUnusualDiskReadRate\nexpr: sum by (instance) (irate(node_disk_read_bytes_total[2m])) / 1024 / 1024 &gt; 50\nfor: 5m\nlabels:\nseverity: warning\nannotations:\nsummary: \"Host unusual disk read rate (instance {{ $labels.instance }})\"\nmessage: \"Disk is probably reading too much data (&gt; 50 MB/s)\\n  VALUE = {{ $value }}.\"\ngrafana: \"{{ grafana_url}}?var-job=node_exporter&amp;var-hostname=All&amp;var-node={{ $labels.instance }}\"\n</code></pre>"}, {"location": "devops/prometheus/node_exporter/#host-unusual-disk-write-rate", "title": "Host unusual disk write rate", "text": "<p>Disk is probably writing too much data (&gt; 50 MB/s)</p> <pre><code>- alert: HostUnusualDiskWriteRate\nexpr: sum by (instance) (irate(node_disk_written_bytes_total[2m])) / 1024 / 1024 &gt; 50\nfor: 5m\nlabels:\nseverity: warning\nannotations:\nsummary: \"Host unusual disk write rate (instance {{ $labels.instance }})\"\nmessage: \"Disk is probably writing too much data (&gt; 50 MB/s)\\n  VALUE = {{ $value }}.\"\ngrafana: \"{{ grafana_url}}?var-job=node_exporter&amp;var-hostname=All&amp;var-node={{ $labels.instance }}\"\n</code></pre>"}, {"location": "devops/prometheus/node_exporter/#host-out-of-disk-space", "title": "Host out of disk space", "text": "<p>Disk is worryingly almost full (<code>&lt; 10% left</code>).</p> <pre><code>- alert: HostOutOfDiskSpace\nexpr: (node_filesystem_avail_bytes{fstype!~\"tmpfs\"}  * 100) / node_filesystem_size_bytes{fstype!~\"tmpfs\"} &lt; 10\nfor: 5m\nlabels:\nseverity: critical\nannotations:\nsummary: \"Host out of disk space (instance {{ $labels.instance }})\"\nmessage: \"Host disk is almost full (&lt; 10% left)\\n  VALUE = {{ $value }}\"\ngrafana: \"{{ grafana_url}}?var-job=node_exporter&amp;var-hostname=All&amp;var-node={{ $labels.instance }}\"\n</code></pre> <p>Disk is almost full (<code>&lt; 20% left</code>)</p> <pre><code>- alert: HostReachingOutOfDiskSpace\nexpr: (node_filesystem_avail_bytes{fstype!~\"tmpfs\"}  * 100) / node_filesystem_size_bytes{fstype!~\"tmpfs\"} &lt; 20\nfor: 5m\nlabels:\nseverity: warning\nannotations:\nsummary: \"Host reaching out of disk space (instance {{ $labels.instance }})\"\nmessage: \"Host disk is almost full (&lt; 20% left)\\n  VALUE = {{ $value }}\"\ngrafana: \"{{ grafana_url}}?var-job=node_exporter&amp;var-hostname=All&amp;var-node={{ $labels.instance }}\"\n</code></pre>"}, {"location": "devops/prometheus/node_exporter/#host-disk-will-fill-in-4-hours", "title": "Host disk will fill in 4 hours", "text": "<p>Disk will fill in 4 hours at current write rate</p> <pre><code>- alert: HostDiskWillFillIn4Hours\nexpr: predict_linear(node_filesystem_free_bytes{fstype!~\"tmpfs\"}[1h], 4 * 3600) &lt; 0\nfor: 5m\nlabels:\nseverity: critical\nannotations:\nsummary: \"Host disk will fill in 4 hours (instance {{ $labels.instance }})\"\nmessage: \"Disk will fill in 4 hours at current write rate\\n  VALUE = {{ $value }}.\"\ngrafana: \"{{ grafana_url}}?var-job=node_exporter&amp;var-hostname=All&amp;var-node={{ $labels.instance }}\"\n</code></pre>"}, {"location": "devops/prometheus/node_exporter/#host-out-of-inodes", "title": "Host out of inodes", "text": "<p>Disk is almost running out of available inodes (<code>&lt; 10% left</code>).</p> <pre><code>- alert: HostOutOfInodes\nexpr: node_filesystem_files_free{fstype!~\"tmpfs\"} / node_filesystem_files{fstype!~\"tmpfs\"} * 100 &lt; 10\nfor: 5m\nlabels:\nseverity: warning\nannotations:\nsummary: \"Host out of inodes (instance {{ $labels.instance }})\"\nmessage: \"Disk is almost running out of available inodes (&lt; 10% left)\\n VALUE = {{ $value }}.\"\ngrafana: \"{{ grafana_url}}?var-job=node_exporter&amp;var-hostname=All&amp;var-node={{ $labels.instance }}\"\n</code></pre>"}, {"location": "devops/prometheus/node_exporter/#host-unusual-disk-read-latency", "title": "Host unusual disk read latency", "text": "<p>Disk latency is growing (read operations &gt; 100ms).</p> <pre><code>- alert: HostUnusualDiskReadLatency\nexpr: rate(node_disk_read_time_seconds_total[1m]) / rate(node_disk_reads_completed_total[1m]) &gt; 100\nfor: 5m\nlabels:\nseverity: warning\nannotations:\nsummary: \"Host unusual disk read latency (instance {{ $labels.instance }})\"\nmessage: \"Disk latency is growing (read operations &gt; 100ms)\\n  VALUE = {{ $value }}\"\ngrafana: \"{{ grafana_url}}?var-job=node_exporter&amp;var-hostname=All&amp;var-node={{ $labels.instance }}\"\n</code></pre>"}, {"location": "devops/prometheus/node_exporter/#host-unusual-disk-write-latency", "title": "Host unusual disk write latency", "text": "<p>Disk latency is growing (write operations &gt; 100ms)</p> <pre><code>- alert: HostUnusualDiskWriteLatency\nexpr: rate(node_disk_write_time_seconds_total[1m]) / rate(node_disk_writes_completed_total[1m]) &gt; 100\nfor: 5m\nlabels:\nseverity: warning\nannotations:\nsummary: \"Host unusual disk write latency (instance {{ $labels.instance }})\"\nmessage: \"Disk latency is growing (write operations &gt; 100ms)\\n  VALUE = {{ $value }}\"\ngrafana: \"{{ grafana_url}}?var-job=node_exporter&amp;var-hostname=All&amp;var-node={{ $labels.instance }}\"\n</code></pre>"}, {"location": "devops/prometheus/node_exporter/#host-high-cpu-load", "title": "Host high CPU load", "text": "<p>CPU load is &gt; 80%</p> <pre><code>- alert: HostHighCpuLoad\nexpr: 100 - (avg by(instance) (irate(node_cpu_seconds_total{mode=\"idle\"}[5m])) * 100) &gt; 80\nfor: 5m\nlabels:\nseverity: warning\nannotations:\nsummary: \"Host high CPU load (instance {{ $labels.instance }})\"\nmessage: \"CPU load is &gt; 80%\\n  VALUE = {{ $value }}.\"\ngrafana: \"{{ grafana_url}}?var-job=node_exporter&amp;var-hostname=All&amp;var-node={{ $labels.instance }}\"\n</code></pre>"}, {"location": "devops/prometheus/node_exporter/#host-context-switching", "title": "Host context switching", "text": "<p>Context switching is growing on node (&gt; 1000 / s)</p> <pre><code># 1000 context switches is an arbitrary number.\n# Alert threshold depends on nature of application.\n# Please read: https://github.com/samber/awesome-prometheus-alerts/issues/58\n- alert: HostContextSwitching\nexpr: (rate(node_context_switches_total[5m])) / (count without(cpu, mode) (node_cpu_seconds_total{mode=\"idle\"})) &gt; 1000\nfor: 5m\nlabels:\nseverity: warning\nannotations:\nsummary: \"Host context switching (instance {{ $labels.instance }})\"\nmessage: \"Context switching is growing on node (&gt; 1000 / s)\\n  VALUE = {{ $value }}.\"\ngrafana: \"{{ grafana_url}}?var-job=node_exporter&amp;var-hostname=All&amp;var-node={{ $labels.instance }}\"\n</code></pre>"}, {"location": "devops/prometheus/node_exporter/#host-swap-is-filling-up", "title": "Host swap is filling up", "text": "<p>Swap is filling up (&gt;80%)</p> <pre><code>- alert: HostSwapIsFillingUp\nexpr: (1 - (node_memory_SwapFree_bytes / node_memory_SwapTotal_bytes)) * 100 &gt; 80\nfor: 5m\nlabels:\nseverity: warning\nannotations:\nsummary: \"Host swap is filling up (instance {{ $labels.instance }})\"\nmessage: \"Swap is filling up (&gt;80%)\\n  VALUE = {{ $value }}.\"\ngrafana: \"{{ grafana_url}}?var-job=node_exporter&amp;var-hostname=All&amp;var-node={{ $labels.instance }}\"\n</code></pre>"}, {"location": "devops/prometheus/node_exporter/#host-systemd-service-crashed", "title": "Host SystemD service crashed", "text": "<p>SystemD service crashed</p> <pre><code>- alert: HostSystemdServiceCrashed\nexpr: node_systemd_unit_state{state=\"failed\"} == 1\nfor: 5m\nlabels:\nseverity: warning\nannotations:\nsummary: \"Host SystemD service crashed (instance {{ $labels.instance }})\"\nmessage: \"SystemD service crashed\\n  VALUE = {{ $value }}\"\ngrafana: \"{{ grafana_url}}?var-job=node_exporter&amp;var-hostname=All&amp;var-node={{ $labels.instance }}\"\n</code></pre>"}, {"location": "devops/prometheus/node_exporter/#host-physical-component-too-hot", "title": "Host physical component too hot", "text": "<p>Physical hardware component too hot</p> <pre><code>- alert: HostPhysicalComponentTooHot\nexpr: node_hwmon_temp_celsius &gt; 75\nfor: 5m\nlabels:\nseverity: warning\nannotations:\nsummary: \"Host physical component too hot (instance {{ $labels.instance }})\"\nmessage: \"Physical hardware component too hot\\n  VALUE = {{ $value }}.\"\ngrafana: \"{{ grafana_url}}?var-job=node_exporter&amp;var-hostname=All&amp;var-node={{ $labels.instance }}\"\n</code></pre>"}, {"location": "devops/prometheus/node_exporter/#host-node-overtemperature-alarm", "title": "Host node overtemperature alarm", "text": "<p>Physical node temperature alarm triggered</p> <pre><code>- alert: HostNodeOvertemperatureAlarm\nexpr: node_hwmon_temp_alarm == 1\nfor: 5m\nlabels:\nseverity: critical\nannotations:\nsummary: \"Host node overtemperature alarm (instance {{ $labels.instance }})\"\nmessage: \"Physical node temperature alarm triggered\\n  VALUE = {{ $value }}.\"\ngrafana: \"{{ grafana_url}}?var-job=node_exporter&amp;var-hostname=All&amp;var-node={{ $labels.instance }}\"\n</code></pre>"}, {"location": "devops/prometheus/node_exporter/#host-raid-array-got-inactive", "title": "Host RAID array got inactive", "text": "<p>RAID array <code>{{ $labels.device }}</code> is in degraded state due to one or more disks failures. Number of spare drives is insufficient to fix issue automatically.</p> <pre><code>- alert: HostRaidArrayGotInactive\nexpr: node_md_state{state=\"inactive\"} &gt; 0\nfor: 5m\nlabels:\nseverity: critical\nannotations:\nsummary: \"Host RAID array got inactive (instance {{ $labels.instance }})\"\nmessage: \"RAID array {{ $labels.device }} is in degraded state due to one or more disks failures. Number of spare drives is insufficient to fix issue automatically.\\n  VALUE = {{ $value }}\"\ngrafana: \"{{ grafana_url}}?var-job=node_exporter&amp;var-hostname=All&amp;var-node={{ $labels.instance }}\"\n</code></pre>"}, {"location": "devops/prometheus/node_exporter/#host-raid-disk-failure", "title": "Host RAID disk failure", "text": "<p>At least one device in RAID array on <code>{{ $labels.instance }}</code> failed. Array <code>{{ $labels.md_device }}</code> needs attention and possibly a disk swap.</p> <pre><code>- alert: HostRaidDiskFailure\nexpr: node_md_disks{state=\"fail\"} &gt; 0\nfor: 5m\nlabels:\nseverity: warning\nannotations:\nsummary: \"Host RAID disk failure (instance {{ $labels.instance }})\"\nmessage: \"At least one device in RAID array on {{ $labels.instance }} failed. Array {{ $labels.md_device }} needs attention and possibly a disk swap\\n  VALUE = {{ $value }}.\"\ngrafana: \"{{ grafana_url}}?var-job=node_exporter&amp;var-hostname=All&amp;var-node={{ $labels.instance }}\"\n</code></pre>"}, {"location": "devops/prometheus/node_exporter/#host-kernel-version-deviations", "title": "Host kernel version deviations", "text": "<p>Different kernel versions are running.</p> <pre><code>- alert: HostKernelVersionDeviations\nexpr: count(sum(label_replace(node_uname_info, \"kernel\", \"$1\", \"release\", \"([0-9]+.[0-9]+.[0-9]+).*\")) by (kernel)) &gt; 1\nfor: 5m\nlabels:\nseverity: warning\nannotations:\nsummary: \"Host kernel version deviations (instance {{ $labels.instance }})\"\nmessage: \"Different kernel versions are running\\n  VALUE = {{ $value }}\"\ngrafana: \"{{ grafana_url}}?var-job=node_exporter&amp;var-hostname=All&amp;var-node={{ $labels.instance }}\"\n</code></pre>"}, {"location": "devops/prometheus/node_exporter/#host-oom-kill-detected", "title": "Host OOM kill detected", "text": "<p>OOM kill detected</p> <pre><code>- alert: HostOomKillDetected\nexpr: increase(node_vmstat_oom_kill[5m]) &gt; 0\nfor: 5m\nlabels:\nseverity: warning\nannotations:\nsummary: \"Host OOM kill detected (instance {{ $labels.instance }})\"\nmessage: \"OOM kill detected\\n  VALUE = {{ $value }}\"\ngrafana: \"{{ grafana_url}}?var-job=node_exporter&amp;var-hostname=All&amp;var-node={{ $labels.instance }}\"\n</code></pre>"}, {"location": "devops/prometheus/node_exporter/#host-network-receive-errors", "title": "Host Network Receive Errors", "text": "<p><code>{{ $labels.instance }}</code> interface <code>{{ $labels.device }}</code> has encountered <code>{{ printf \"%.0f\" $value }}</code> receive errors in the last five minutes.</p> <pre><code>- alert: HostNetworkReceiveErrors\nexpr: increase(node_network_receive_errs_total[5m]) &gt; 0\nfor: 5m\nlabels:\nseverity: warning\nannotations:\nsummary: \"Host Network Receive Errors (instance {{ $labels.instance }})\"\nmessage: \"{{ $labels.instance }} interface {{ $labels.device }} has encountered {{ printf '%.0f' $value }} receive errors in the last five minutes.\\n  VALUE = {{ $value }}\"\ngrafana: \"{{ grafana_url}}?var-job=node_exporter&amp;var-hostname=All&amp;var-node={{ $labels.instance }}\"\n</code></pre>"}, {"location": "devops/prometheus/node_exporter/#host-network-transmit-errors", "title": "Host Network Transmit Errors", "text": "<p><code>{{ $labels.instance }}</code> interface <code>{{ $labels.device }}</code> has encountered <code>{{ printf \"%.0f\" $value }}</code> transmit errors in the last five minutes.</p> <pre><code>- alert: HostNetworkTransmitErrors\nexpr: increase(node_network_transmit_errs_total[5m]) &gt; 0\nfor: 5m\nlabels:\nseverity: warning\nannotations:\nsummary: \"Host Network Transmit Errors (instance {{ $labels.instance }})\"\nmessage: \"{{ $labels.instance }} interface {{ $labels.device }} has encountered {{ printf '%.0f' $value }} transmit errors in the last five minutes.\\n  VALUE = {{ $value }}\"\ngrafana: \"{{ grafana_url}}?var-job=node_exporter&amp;var-hostname=All&amp;var-node={{ $labels.instance }}\"\n</code></pre>"}, {"location": "devops/prometheus/node_exporter/#references", "title": "References", "text": "<ul> <li>Git</li> <li>Prometheus node exporter guide</li> <li>Node exporter alerts</li> </ul>"}, {"location": "devops/prometheus/prometheus/", "title": "Prometheus", "text": "<p>Prometheus is a free software application used for event monitoring and alerting. It records real-time metrics in a time series database (allowing for high dimensionality) built using a HTTP pull model, with flexible queries and real-time alerting. The project is written in Go and licensed under the Apache 2 License, with source code available on GitHub, and is a graduated project of the Cloud Native Computing Foundation, along with Kubernetes and Envoy.</p> <p> </p> <p>A quick overview of Prometheus would be, as stated in the coreos article:</p> <p>At the core of the Prometheus monitoring system is the main server, which ingests samples from monitoring targets. A target is any application that exposes metrics according to the open specification understood by Prometheus. Since Prometheus pulls data, rather than expecting targets to actively push stats into the monitoring system, it supports a variety of service discovery integrations, like that with Kubernetes, to immediately adapt to changes in the set of targets.</p> <p>The second core component is the Alertmanager, implementing the idea of time series based alerting. It intelligently removes duplicate alerts sent by Prometheus servers, groups the alerts into informative notifications, and dispatches them to a variety of integrations, like those with PagerDuty and Slack. It also handles silencing of selected alerts and advanced routing configurations for notifications.</p> <p>There are several additional Prometheus components, such as client libraries for different programming languages, and a growing number of exporters. Exporters are small programs that provide Prometheus compatible metrics from systems that are not natively instrumented.</p> <p>Go to the Prometheus architecture post for more details.</p> <p>We are living a shift to the DevOps culture, containers and Kubernetes. So nowadays:</p> <ul> <li>Developers need to integrate app and business related metrics as an organic   part of the infrastructure. So monitoring needs to be democratized, made more   accessible and cover additional layers of the stack.</li> <li>Container based infrastructures are changing how we monitor the resources.   Now we have a huge number of volatile software entities, services, virtual   network addresses, exposed metrics that suddenly appear or vanish. Traditional   monitoring tools are not designed to handle this.</li> </ul> <p>These reasons pushed Soundcloud to build a new monitoring system that had the following features</p> <ul> <li>Multi-dimensional data model: The model is based on key-value pairs, similar   to how Kubernetes itself organizes infrastructure metadata using labels. It   allows for flexible and accurate time series data, powering its Prometheus   query language.</li> <li>Accessible format and protocols: Exposing prometheus metrics is a pretty   straightforward task. Metrics are human readable, are in a self-explanatory   format, and are published using a standard HTTP transport. You can check that   the metrics are correctly exposed just using your web browser.</li> <li>Service discovery: The Prometheus server is in charge of periodically   scraping the targets, so that applications and services don\u2019t need to worry   about emitting data (metrics are pulled, not pushed). These Prometheus servers   have several methods to auto-discover scrape targets, some of them can be   configured to filter and match container metadata, making it an excellent fit   for ephemeral Kubernetes workloads.</li> <li>Modular and highly available components: Metric collection, alerting,   graphical visualization, etc, are performed by different composable services.   All these services are designed to support redundancy and sharding.</li> <li>Pull based metrics: Most monitoring systems are pushing metrics to     a centralized collection platform. Prometheus flips this model on it's head     with the following advantages:<ul> <li>No need to install custom software in the physical servers or containers.</li> <li>Doesn't require applications to use CPU cycles pushing metrics.</li> <li>Handles service failure/unavailability gracefully. If a target goes down,     Prometheus can record it was unable to retrieve data.</li> <li>You can use the Pushgateway if pulling metrics is not feasible.</li> </ul> </li> </ul>"}, {"location": "devops/prometheus/prometheus/#installation", "title": "Installation", "text": "<p>There are several ways to install prometheus, but I'd recommend using the Kubernetes or Docker Prometheus operator.</p>"}, {"location": "devops/prometheus/prometheus/#exposing-your-metrics", "title": "Exposing your metrics", "text": "<p>Prometheus defines a very nice text-based format for its metrics:</p> <pre><code># HELP prometheus_engine_query_duration_seconds Query timings\n# TYPE prometheus_engine_query_duration_seconds summary\nprometheus_engine_query_duration_seconds{slice=\"inner_eval\",quantile=\"0.5\"} 7.0442e-05\nprometheus_engine_query_duration_seconds{slice=\"inner_eval\",quantile=\"0.9\"} 0.0084092\nprometheus_engine_query_duration_seconds{slice=\"inner_eval\",quantile=\"0.99\"} 0.389403939\n</code></pre> <p>The data is relatively human readable and we even have TYPE and HELP decorators to increase the readability.</p> <p>To expose application metrics to the Prometheus server, use one of the client libraries and follow the suggested naming and units conventions for metrics.</p>"}, {"location": "devops/prometheus/prometheus/#metric-types", "title": "Metric types", "text": "<p>There are these metric types:</p> <ul> <li>Counter: A simple monotonically incrementing type; basically use this for   situations where you want to know \u201chow many times has x happened\u201d.</li> <li>Gauge: A representation of a metric that can go both up and down. Think of   a speedometer in a car, this type provides a snapshot of \u201cwhat is the current   value of x now\u201d.</li> <li>Histogram: It represents observed metrics sharded into distinct buckets.   Think of this as a mechanism to track \u201chow long something took\u201d or \u201chow big   something was\u201d.</li> <li>Summary: Similar to a histogram, except the bins are converted into an     aggregate immediately.</li> </ul>"}, {"location": "devops/prometheus/prometheus/#using-labels", "title": "Using labels", "text": "<p>Prometheus metrics support the concept of Labels to provide extra dimensions to your data. By using Labels efficiently we can essentially provide more insights into our data whilst having to manage less actual metrics.</p>"}, {"location": "devops/prometheus/prometheus/#prometheus-rules", "title": "Prometheus rules", "text": "<p>Prometheus supports two types of rules which may be configured and then evaluated at regular intervals: recording rules and alerting rules.</p> <p>Recording rules allow you to precompute frequently needed or computationally expensive expressions and save their result as a new set of time series. Querying the precomputed result will then often be much faster than executing the original expression every time it is needed.</p> <p>A simple example rules file would be:</p> <pre><code>groups:\n- name: example\nrules:\n- record: job:http_inprogress_requests:sum\nexpr: sum by (job) (http_inprogress_requests)\n</code></pre> <p>Regarding naming and aggregation conventions, Recording rules should be of the general form <code>level:metric:operations</code>. <code>level</code> represents the aggregation level and labels of the rule output. <code>metric</code> is the metric name and should be unchanged other than stripping <code>_total</code> off counters when using <code>rate()</code> or <code>irate()</code>. <code>operations</code> is a list of operations (splitted by <code>:</code>) that were applied to the metric, newest operation first.</p> <p>If you want to add extra labels to the calculated rule use the <code>labels</code> tag like the following example:</p> <pre><code>groups:\n- name: example\nrules:\n- record: instance_path:wrong_resource_size\nexpr: &gt;\ninstance_path:node_memory_MemAvailable_percent:avg_plus_stddev_over_time_2w &lt; 60\nlabels:\ntype: EC2\nmetric: RAM\nproblem: oversized\n</code></pre>"}, {"location": "devops/prometheus/prometheus/#finding-a-metric", "title": "Finding a metric", "text": "<p>You can use <code>{__name__=~\".*deploy.*\"}</code> to find the metrics that have <code>deploy</code> somewhere in the name.</p>"}, {"location": "devops/prometheus/prometheus/#accessing-prometheus-metrics-through-python", "title": "Accessing Prometheus metrics through python", "text": "<pre><code>import requests\n\nresponse = requests.get(\n    \"http://127.0.0.1:9090/api/v1/query\",\n    params={\"query\": \"container_cpu_user_seconds_total\"},\n)\n</code></pre>"}, {"location": "devops/prometheus/prometheus/#links", "title": "Links", "text": "<ul> <li>Homepage.</li> <li>Docs.</li> <li>Awesome Prometheus.</li> <li>Prometheus rules best     practices     and configuration.</li> </ul>"}, {"location": "devops/prometheus/prometheus/#diving-deeper", "title": "Diving deeper", "text": "<ul> <li>Architecture</li> <li>Prometheus Operator</li> <li>Prometheus Installation</li> <li>Blackbox Exporter</li> <li>Node Exporter</li> <li>Prometheus Troubleshooting</li> </ul>"}, {"location": "devops/prometheus/prometheus/#introduction-posts", "title": "Introduction posts", "text": "<ul> <li>Soundcloud   introduction.</li> <li>Sysdig guide.</li> <li>Prometheus monitoring solutions   comparison.</li> <li>ITNEXT overview</li> </ul>"}, {"location": "devops/prometheus/prometheus/#books", "title": "Books", "text": "<ul> <li>Prometheus Up &amp; Running.</li> <li>Monitoring With Prometheus.</li> </ul>"}, {"location": "devops/prometheus/prometheus_architecture/", "title": "Prometheus architecture", "text": ""}, {"location": "devops/prometheus/prometheus_architecture/#prometheus-server", "title": "Prometheus Server", "text": "<p>Prometheus servers have the following assignments:</p> <ul> <li>Periodically scrape and store metrics from instrumented jobs, either directly   or via an intermediary push gateway for short-lived jobs.</li> <li>Run rules over scraped data to either record new timeseries from existing data or   generate alerts.</li> <li>Discovers new targets from the Service discovery.</li> <li>Push alerts to the Alertmanager.</li> <li>Executes PromQL queries.</li> </ul>"}, {"location": "devops/prometheus/prometheus_architecture/#prometheus-targets", "title": "Prometheus Targets", "text": "<p>Prometheus Targets define how does prometheus extract the metrics from the different sources.</p> <p>If the services expose the metrics themselves such as Kubernetes, Prometheus fetch them directly. On the other cases, exporters are used.</p> <p>Exporters are modules that extract information and translate it into the Prometheus format, which the server can then ingest. There are several exporters available, for example:</p> <ul> <li>Hardware: Node/system</li> <li>HTTP: HAProxy,     NGINX,     Apache.</li> <li>APIs: Github, Docker     Hub.</li> <li>Other monitoring systems:     Cloudwatch.</li> <li>Databases: MySQL,     Elasticsearch.</li> <li>Messaging systems: RabbitMQ,     Kafka.</li> <li>Miscellaneous: Blackbox,     JMX.</li> </ul>"}, {"location": "devops/prometheus/prometheus_architecture/#pushgateway", "title": "Pushgateway", "text": "<p>In case the nodes are not exposing an endpoint from which the Prometheus server can collect the metrics, the Prometheus ecosystem has a push gateway.</p> <p>This gateway API is useful for one-off jobs that run, capture the data, transform that data into the Prometheus data format and then push that data into the Prometheus server.</p>"}, {"location": "devops/prometheus/prometheus_architecture/#service-discovery", "title": "Service discovery", "text": "<p>Prometheus is designed to require very little configuration when first setup, and was designed from the ground up to run in dynamic environments such Kubernetes. It therefore performs automatic discovery of services running to try and make a \u201cbest guess\u201d of what it should be monitoring.</p>"}, {"location": "devops/prometheus/prometheus_architecture/#alertmanager", "title": "Alertmanager", "text": "<p>The Alertmanager handles alerts sent by client applications such as the Prometheus server. It takes care of deduplicating, grouping, and routing them to the correct receiver integrations such as email, PagerDuty, or OpsGenie. It also takes care of silencing and inhibition of alerts.</p>"}, {"location": "devops/prometheus/prometheus_architecture/#data-visualization-and-export", "title": "Data visualization and export", "text": "<p>There are several ways to visualize or export data from Prometheus.</p>"}, {"location": "devops/prometheus/prometheus_architecture/#prometheus-web-ui", "title": "Prometheus web UI", "text": "<p>Prometheus comes with its own user interface that you can use to:</p> <ul> <li>Run PromQL queries.</li> <li>Check the Alertmanager rules.</li> <li>Check the configuration.</li> <li>Check the Targets.</li> <li>Check the service discovery.</li> </ul>"}, {"location": "devops/prometheus/prometheus_architecture/#grafana", "title": "Grafana", "text": "<p>Grafana is the best way to visually analyze the evolution of the metrics throughout time.</p>"}, {"location": "devops/prometheus/prometheus_architecture/#api-clients", "title": "API clients", "text": "<p>Prometheus also exposes an API, so in case you are interested in writing your own clients, you can do that too.</p>"}, {"location": "devops/prometheus/prometheus_architecture/#links", "title": "Links", "text": "<ul> <li>Prometheus Overview</li> <li>Open Source for U architecture overview</li> </ul>"}, {"location": "devops/prometheus/prometheus_installation/", "title": "Prometheus Installation", "text": ""}, {"location": "devops/prometheus/prometheus_installation/#kubernetes", "title": "Kubernetes", "text": "<p>Helm 2 is not supported anymore.</p> <p>Later versions of the chart return an Error: apiVersion 'v2' is not valid. The value must be \"v1\" when using helm 2.</p> <p>Diving deeper, it seems that from 11.1.7 support for helm 2 was dropped.</p> <p>To install the operator we'll use helmfile to install the stable/prometheus-operator chart.</p> <p>Add the following lines to your <code>helmfile.yaml</code>. <pre><code>- name: prometheus-operator\nnamespace: monitoring\nchart: stable/prometheus-operator\nvalues:\n- prometheus-operator/values.yaml\n</code></pre></p> <p>Edit the chart values. <pre><code>mkdir prometheus-operator\nhelm inspect values stable/prometheus-operator &gt; prometheus-operator/values.yaml\nvi prometheus-operator/values.yaml\n</code></pre></p> <p>I've implemented the following changes:</p> <ul> <li> <p>If you are using a managed solution like EKS, the provider will hide     <code>kube-scheduler</code> and <code>kube-controller-manager</code> so those metrics will fail.     Therefore you need to disable:</p> <ul> <li><code>defaultRules.rules.kubeScheduler: false</code>.</li> <li><code>kubeScheduler.enabled: false</code>.</li> <li><code>kubeControllerManager.enabled: false</code>.</li> <li>Enabled the ingress of <code>alertmanager</code>, <code>grafana</code> and <code>prometheus</code>.</li> <li>Set up the <code>storage</code> of <code>alertmanager</code> and <code>prometheus</code> with   <code>storageClassName: gp2</code> (for AWS).</li> <li>Change <code>additionalPrometheusRules</code> to <code>additionalPrometheusRulesMap</code> as the former is going to be deprecated in future releases.</li> <li> <p>For private clusters, disable the admission   webhook.</p> </li> <li> <p><code>prometheusOperator.admissionWebhooks.enabled=false</code></p> </li> <li><code>prometheusOperator.admissionWebhooks.patch.enabled=false</code></li> <li><code>prometheusOperator.tlsProxy.enabled=false</code></li> </ul> </li> </ul> <p>And install.</p> <pre><code>helmfile diff\nhelmfile apply\n</code></pre> <p>Once it's installed you can check everything is working by accessing the grafana dashboard.</p> <p>First of all get the pod name (we'll asume you've used the <code>monitoring</code> namespace). <pre><code>kubectl get pods -n monitoring | grep grafana\n</code></pre></p> <p>Then set up the proxies <pre><code>kubectl port-forward {{ grafana_pod }} -n monitoring 3000:3000\nkubectl port-forward -n monitoring \\\nprometheus-prometheus-operator-prometheus-0 9090:9090\n</code></pre></p> <p>To access grafana, go to http://localhost:3000 through your browser and at the top left, click on Home and select any dashboard. To access prometheus, go to http://localhost:9090.</p> <p>If you're using the EKS helm chart, you'll need to manually edit the kube-proxy-config configmap until this bug has been solved.</p> <p>Edit the <code>127.0.0.1</code> value to <code>0.0.0.0</code> for the key <code>metricsBindAddress</code> in <pre><code>kubectl -n kube-system edit cm kube-proxy-config\n</code></pre></p> <p>And restart the DaemonSet: <pre><code>kubectl rollout restart -n kube-system daemonset.apps/kube-proxy\n</code></pre></p>"}, {"location": "devops/prometheus/prometheus_installation/#upgrading-notes", "title": "Upgrading notes", "text": ""}, {"location": "devops/prometheus/prometheus_installation/#10x-1117", "title": "10.x -&gt; 11.1.7", "text": "<p>If you have a private cluster in EKS, you are not able to use the admission webhooks as the nodes are not able to reach the master.</p> <p>Between those versions, something changed and you need to disable tls too with:</p> <pre><code>prometheusOperator:\ntls:\nenabled: false\nadmissionWebhooks:\nenabled: false\n</code></pre> <p>If you run <code>helmfile apply</code> without that flag, the deployment gets tainted, and you may need to edit the deployment to remove the <code>tls-secret</code> volume.</p>"}, {"location": "devops/prometheus/prometheus_installation/#docker", "title": "Docker", "text": "<p>To install it outside Kubernetes, use the cloudalchemy ansible role for host installations or the prom/prometheus docker with the following command:</p> <pre><code>/usr/bin/docker run --rm \\\n--name prometheus \\\n-v /data/prometheus:/etc/prometheus \\\nprom/prometheus:latest \\\n--storage.tsdb.retention.time=30d \\\n--config.file=/etc/prometheus/prometheus.yml \\\n</code></pre> <p>With a basic prometheus configuration:</p> <p>File: /data/prometheus/prometheus.yml</p> <p>And some basic rules:</p> File: /data/prometheus/rules/node_exporter.yaml <pre><code>groups:\n- name: ansible managed alert rules\nrules:\n- alert: Watchdog\nannotations:\ndescription: |-\nThis is an alert meant to ensure that the entire alerting pipeline is functional.\nThis alert is always firing, therefore it should always be firing in Alertmanager\nand always fire against a receiver. There are integrations with various notification\nmechanisms that send a notification when this alert is not firing. For example the\n\"DeadMansSnitch\" integration in PagerDuty.\nsummary: Ensure entire alerting pipeline is functional\nexpr: vector(1)\nfor: 10m\nlabels:\nseverity: warning\n- alert: InstanceDown\nannotations:\ndescription: '{{ $labels.instance }} of job {{ $labels.job }} has been down for\nmore than 5 minutes.'\nsummary: Instance {{ $labels.instance }} down\nexpr: up == 0\nfor: 5m\nlabels:\nseverity: critical\n- alert: RebootRequired\nannotations:\ndescription: '{{ $labels.instance }} requires a reboot.'\nsummary: Instance {{ $labels.instance }} - reboot required\nexpr: node_reboot_required &gt; 0\nlabels:\nseverity: warning\n- alert: NodeFilesystemSpaceFillingUp\nannotations:\ndescription: Filesystem on {{ $labels.device }} at {{ $labels.instance }} has\nonly {{ printf \"%.2f\" $value }}% available space left and is filling up.\nsummary: Filesystem is predicted to run out of space within the next 24 hours.\nexpr: |-\n(\nnode_filesystem_avail_bytes{job=\"node\",fstype!=\"\"} / node_filesystem_size_bytes{job=\"node\",fstype!=\"\"} * 100 &lt; 40\nand\npredict_linear(node_filesystem_avail_bytes{job=\"node\",fstype!=\"\"}[6h], 24*60*60) &lt; 0\nand\nnode_filesystem_readonly{job=\"node\",fstype!=\"\"} == 0\n)\nfor: 1h\nlabels:\nseverity: warning\n- alert: NodeFilesystemSpaceFillingUp\nannotations:\ndescription: Filesystem on {{ $labels.device }} at {{ $labels.instance }} has\nonly {{ printf \"%.2f\" $value }}% available space left and is filling up fast.\nsummary: Filesystem is predicted to run out of space within the next 4 hours.\nexpr: |-\n(\nnode_filesystem_avail_bytes{job=\"node\",fstype!=\"\"} / node_filesystem_size_bytes{job=\"node\",fstype!=\"\"} * 100 &lt; 20\nand\npredict_linear(node_filesystem_avail_bytes{job=\"node\",fstype!=\"\"}[6h], 4*60*60) &lt; 0\nand\nnode_filesystem_readonly{job=\"node\",fstype!=\"\"} == 0\n)\nfor: 1h\nlabels:\nseverity: critical\n- alert: NodeFilesystemAlmostOutOfSpace\nannotations:\ndescription: Filesystem on {{ $labels.device }} at {{ $labels.instance }} has\nonly {{ printf \"%.2f\" $value }}% available space left.\nsummary: Filesystem has less than 5% space left.\nexpr: |-\n(\nnode_filesystem_avail_bytes{job=\"node\",fstype!=\"\"} / node_filesystem_size_bytes{job=\"node\",fstype!=\"\"} * 100 &lt; 5\nand\nnode_filesystem_readonly{job=\"node\",fstype!=\"\"} == 0\n)\nfor: 1h\nlabels:\nseverity: warning\n- alert: NodeFilesystemAlmostOutOfSpace\nannotations:\ndescription: Filesystem on {{ $labels.device }} at {{ $labels.instance }} has\nonly {{ printf \"%.2f\" $value }}% available space left.\nsummary: Filesystem has less than 3% space left.\nexpr: |-\n(\nnode_filesystem_avail_bytes{job=\"node\",fstype!=\"\"} / node_filesystem_size_bytes{job=\"node\",fstype!=\"\"} * 100 &lt; 3\nand\nnode_filesystem_readonly{job=\"node\",fstype!=\"\"} == 0\n)\nfor: 1h\nlabels:\nseverity: critical\n- alert: NodeFilesystemFilesFillingUp\nannotations:\ndescription: Filesystem on {{ $labels.device }} at {{ $labels.instance }} has\nonly {{ printf \"%.2f\" $value }}% available inodes left and is filling up.\nsummary: Filesystem is predicted to run out of inodes within the next 24 hours.\nexpr: |-\n(\nnode_filesystem_files_free{job=\"node\",fstype!=\"\"} / node_filesystem_files{job=\"node\",fstype!=\"\"} * 100 &lt; 40\nand\npredict_linear(node_filesystem_files_free{job=\"node\",fstype!=\"\"}[6h], 24*60*60) &lt; 0\nand\nnode_filesystem_readonly{job=\"node\",fstype!=\"\"} == 0\n)\nfor: 1h\nlabels:\nseverity: warning\n- alert: NodeFilesystemFilesFillingUp\nannotations:\ndescription: Filesystem on {{ $labels.device }} at {{ $labels.instance }} has\nonly {{ printf \"%.2f\" $value }}% available inodes left and is filling up fast.\nsummary: Filesystem is predicted to run out of inodes within the next 4 hours.\nexpr: |-\n(\nnode_filesystem_files_free{job=\"node\",fstype!=\"\"} / node_filesystem_files{job=\"node\",fstype!=\"\"} * 100 &lt; 20\nand\npredict_linear(node_filesystem_files_free{job=\"node\",fstype!=\"\"}[6h], 4*60*60) &lt; 0\nand\nnode_filesystem_readonly{job=\"node\",fstype!=\"\"} == 0\n)\nfor: 1h\nlabels:\nseverity: critical\n- alert: NodeFilesystemAlmostOutOfFiles\nannotations:\ndescription: Filesystem on {{ $labels.device }} at {{ $labels.instance }} has\nonly {{ printf \"%.2f\" $value }}% available inodes left.\nsummary: Filesystem has less than 5% inodes left.\nexpr: |-\n(\nnode_filesystem_files_free{job=\"node\",fstype!=\"\"} / node_filesystem_files{job=\"node\",fstype!=\"\"} * 100 &lt; 5\nand\nnode_filesystem_readonly{job=\"node\",fstype!=\"\"} == 0\n)\nfor: 1h\nlabels:\nseverity: warning\n- alert: NodeFilesystemAlmostOutOfFiles\nannotations:\ndescription: Filesystem on {{ $labels.device }} at {{ $labels.instance }} has\nonly {{ printf \"%.2f\" $value }}% available inodes left.\nsummary: Filesystem has less than 3% inodes left.\nexpr: |-\n(\nnode_filesystem_files_free{job=\"node\",fstype!=\"\"} / node_filesystem_files{job=\"node\",fstype!=\"\"} * 100 &lt; 3\nand\nnode_filesystem_readonly{job=\"node\",fstype!=\"\"} == 0\n)\nfor: 1h\nlabels:\nseverity: critical\n- alert: NodeNetworkReceiveErrs\nannotations:\ndescription: '{{ $labels.instance }} interface {{ $labels.device }} has encountered\n{{ printf \"%.0f\" $value }} receive errors in the last two minutes.'\nsummary: Network interface is reporting many receive errors.\nexpr: |-\nincrease(node_network_receive_errs_total[2m]) &gt; 10\nfor: 1h\nlabels:\nseverity: warning\n- alert: NodeNetworkTransmitErrs\nannotations:\ndescription: '{{ $labels.instance }} interface {{ $labels.device }} has encountered\n{{ printf \"%.0f\" $value }} transmit errors in the last two minutes.'\nsummary: Network interface is reporting many transmit errors.\nexpr: |-\nincrease(node_network_transmit_errs_total[2m]) &gt; 10\nfor: 1h\nlabels:\nseverity: warning\n- alert: NodeHighNumberConntrackEntriesUsed\nannotations:\ndescription: '{{ $value | humanizePercentage }} of conntrack entries are used'\nsummary: Number of conntrack are getting close to the limit\nexpr: |-\n(node_nf_conntrack_entries / node_nf_conntrack_entries_limit) &gt; 0.75\nlabels:\nseverity: warning\n- alert: NodeClockSkewDetected\nannotations:\nmessage: Clock on {{ $labels.instance }} is out of sync by more than 300s. Ensure\nNTP is configured correctly on this host.\nsummary: Clock skew detected.\nexpr: |-\n(\nnode_timex_offset_seconds &gt; 0.05\nand\nderiv(node_timex_offset_seconds[5m]) &gt;= 0\n)\nor\n(\nnode_timex_offset_seconds &lt; -0.05\nand\nderiv(node_timex_offset_seconds[5m]) &lt;= 0\n)\nfor: 10m\nlabels:\nseverity: warning\n- alert: NodeClockNotSynchronising\nannotations:\nmessage: Clock on {{ $labels.instance }} is not synchronising. Ensure NTP is configured\non this host.\nsummary: Clock not synchronising.\nexpr: |-\nmin_over_time(node_timex_sync_status[5m]) == 0\nfor: 10m\nlabels:\nseverity: warning\n</code></pre>"}, {"location": "devops/prometheus/prometheus_installation/#yaml", "title": "```yaml", "text": ""}, {"location": "devops/prometheus/prometheus_installation/#httpprometheusiodocsoperatingconfiguration", "title": "http://prometheus.io/docs/operating/configuration/", "text": "<p>global:   evaluation_interval: 1m   scrape_interval: 1m   scrape_timeout: 10s   external_labels:     environment: helm rule_files:   - /etc/prometheus/rules/*.yaml scrape_configs:   - job_name: prometheus     metrics_path: /metrics     static_configs:     - targets:       - prometheus:9090  ```</p>"}, {"location": "devops/prometheus/prometheus_installation/#next-steps", "title": "Next steps", "text": "<ul> <li>Configure the alertmanager alerts.</li> <li>Configure the Blackbox Exporter.</li> <li>Configure the grafana dashboards.</li> </ul>"}, {"location": "devops/prometheus/prometheus_installation/#issues", "title": "Issues", "text": "<ul> <li>Error: apiVersion 'v2' is not valid.  The value must be     \"v1\":     Update the warning above and update the clusters.</li> </ul>"}, {"location": "devops/prometheus/prometheus_operator/", "title": "Prometheus Operator", "text": "<p>Prometheus has it's own kubernetes operator, which makes it simple to install with helm, and enables users to configure and manage instances of Prometheus using simple declarative configuration that will, in response, create, configure, and manage Prometheus monitoring instances.</p> <p>Once installed the Prometheus Operator provides the following features:</p> <ul> <li> <p>Create/Destroy: Easily launch a Prometheus instance for your Kubernetes   namespace, a specific application or team easily using the Operator.</p> </li> <li> <p>Simple Configuration: Configure the fundamentals of Prometheus like   versions, persistence, retention policies, and replicas from a native   Kubernetes resource.</p> </li> <li> <p>Target Services via Labels: Automatically generate monitoring target   configurations based on familiar Kubernetes label queries; no need to learn   a Prometheus specific configuration language.</p> </li> </ul>"}, {"location": "devops/prometheus/prometheus_operator/#how-it-works", "title": "How it works", "text": "<p>The core idea of the Operator is to decouple deployment of Prometheus instances from the configuration of which entities they are monitoring.</p> <p>The Operator acts on the following custom resource definitions (CRDs):</p> <ul> <li>Prometheus: Defines the desired Prometheus deployment. The Operator ensures   at all times that a deployment matching the resource definition is running.   This entails aspects like the data retention time, persistent volume claims,   number of replicas, the Prometheus version, and Alertmanager instances to send   alerts to.</li> <li>ServiceMonitor: Specifies how metrics can be retrieved from a set of   services exposing them in a common way. The Operator configures the Prometheus   instance to monitor all services covered by included ServiceMonitors and keeps   this configuration synchronized with any changes happening in the cluster.</li> <li>PrometheusRule: Defines a desired Prometheus rule file, which can be loaded   by a Prometheus instance containing Prometheus alerting and recording rules.</li> <li>Alertmanager: Defines a desired Alertmanager deployment. The Operator   ensures at all times that a deployment matching the resource definition is   running.</li> </ul> <p></p>"}, {"location": "devops/prometheus/prometheus_operator/#links", "title": "Links", "text": "<ul> <li>Homepage</li> <li>CoreOS Prometheus operator presentation</li> <li>Sysdig Prometheus operator guide part 3</li> </ul>"}, {"location": "devops/prometheus/prometheus_troubleshooting/", "title": "Prometheus Troubleshooting", "text": "<p>Solutions for problems with Prometheus.</p>"}, {"location": "devops/prometheus/prometheus_troubleshooting/#service-monitor-not-being-recognized", "title": "Service monitor not being recognized", "text": "<p>Probably the service monitor labels aren't properly configured. Each prometheus monitors it's own targets, to see how you need to label your resources, describe the prometheus instance and search for <code>Service Monitor Selector</code>.</p> <pre><code>kubectl get prometheus -n monitoring\nkubectl describe prometheus prometheus-operator-prometheus -n monitoring\n</code></pre> <p>The last one will return something like:</p> <pre><code>  Service Monitor Selector:\nMatch Labels:\nRelease:  prometheus-operator\n</code></pre> <p>Which means you need to label your service monitors with <code>release: prometheus-operator</code>, be careful if you use <code>Release: prometheus-operator</code> it won't work.</p>"}, {"location": "devops/prometheus/prometheus_troubleshooting/#failed-calling-webhook-prometheusrulemutatemonitoringcoreoscom", "title": "Failed calling webhook prometheusrulemutate.monitoring.coreos.com", "text": "<pre><code>  Error: UPGRADE FAILED: failed to create resource: Internal error occurred: failed calling webhook \"prometheusrulemutate.monitoring.coreos.com\": Post https://prometheus-operator-operator.monitoring.svc:443/admission-prometheusrules/mutate?timeout=30s: no endpoints available for service \"prometheus-operator-operator\"\n</code></pre> <p>Since version 0.30 of the operator, there is an admission webhook to prevent malformed rules from being added to the cluster. Without this validation, creating an invalid resource will cause Prometheus not to load it. If the container then restarts, it will go into a crashloop.</p> <p>For the webhook to work, the control plane needs to be able to access the webhook service. That means the addition of a firewall rule in EKS and GKE deployments. People have succeeded with GKE, but people struggling with EKS have decided to disable the webhook.</p> <p>To disable it, the following options have to be set:</p> <ul> <li><code>prometheusOperator.admissionWebhooks.enabled=false</code></li> <li><code>prometheusOperator.admissionWebhooks.patch.enabled=false</code></li> <li><code>prometheusOperator.tlsProxy.enabled=false</code></li> </ul> <p>If you have deployed your release with the webhook enabled, you also need to remove all the resources that match the following:</p> <pre><code>kubectl get validatingwebhookconfigurations.admissionregistration.k8s.io\nkubectl get MutatingWebhookConfiguration\n</code></pre> <p>Before executing <code>helmfile apply</code> again.</p>"}, {"location": "drawing/drawing/", "title": "Drawing", "text": "<p>It's really difficult to choose where to start if you want to learn how to draw from scratch as there are too many resources. I'm starting with Drawabox, a free course based on series of practical exercises to teach the basics. I'd probably go to Ctrl+Paint once I'm done.</p>"}, {"location": "drawing/drawing/#the-basics", "title": "The basics", "text": ""}, {"location": "drawing/drawing/#changing-the-mindset", "title": "Changing the mindset", "text": "<p>Start your drawing path with the following guidelines to make your progression smoother:</p> <ul> <li>Focus on the experience of drawing instead of the result.</li> <li>Understand that doing something badly does not define who you are. So tackle     the I can't draw that feeling with I can't draw that well. If     you're afraid the thing you want to draw is out of your reach, draw it     anyway.</li> <li>At least half of the time spent     drawing must be devoted to drawing purely for its own sake. If you don't     have much time, alternate the purpose of your sessions.</li> <li>Don't over control your hand with your     brain to try to be absolutely     precise and accurate. Doing so will result in numerous course corrections     making your strokes wobbly, stiff and erratic. Furthermore, spending all     the focus resources in precision, will result in a lack to solve the other     problems involved. Once muscle memory is gained, the strokes will be     cleaner.</li> <li>Draw exactly what you see, while you see     it. Don't trust you memory, as     it will simplify things without you noticing it.</li> <li> <p>Draw from your shoulder.  We are     used to pivot on the wrist as it makes stiff and accurate linework, suitable     for writing. But falls apart when making smooth and consistent strokes.</p> <p>So use the wrist when drawing stiff but precise marks in areas of detail or texture. There are plenty of cases where the elbow will work fine, but using it will get you in the habit of taking the path of least resistance. So try to use the shoulder.</p> <p>This means driving the motion from the muscles that control that joint. As it has a considerable range of motion, you should be able to move your arm with minimal adjustment from your elbow.</p> <p>If you catch yourself having fallen back to drawing from the elbow, do the following exercise: Draw pivoting from your wrist while locking the rest of the joints, to get used to what that feels like. Then lock it and move to the elbow. Finally lock the elbow and go for the shoulder.</p> </li> </ul> <p>Drawing at it's simplest level is the act of putting marks on a page in order or communicate or convey something. Marks should:</p> <ul> <li> <p>Flow continuously: When making     a line between two points, do it with a single continuous stroke even if you     miss the end.</p> <p></p> </li> <li> <p>Flow smoothly: Draw with a confident     and persistent pace (enough to keep your brain from interfering and     attempting to course correct as you go). Again we favor flow over     accuracy, so expect to make your lines less     accurate.</p> <p></p> </li> <li> <p>Maintain a consistent     trayectory: Split lines     into derivable strokes. Otherwise, you'll make mindless zigzags.</p> <p></p> </li> </ul>"}, {"location": "drawing/drawing/#drawing-skills", "title": "Drawing skills", "text": "<p>The course focuses on these psychological skills and concepts:</p> <ul> <li>Confidence: The willingness to push forwards without hesitation once your     preparations are complete.</li> <li>Control: The ability to decide ahead of time what kind of mark you wish to     puto down on the page, and to execute as intended.</li> <li>Patience: The path is hard.</li> <li>Spatial Reasoning: To be able to understand the things we draw as being     three dimensional forms that exist in and relate to one another within     the three dimensional world.</li> <li>Construction: The ability to look at a complex object and break it down into     simple components that can be drawn individually and combined to reconstruct     our complex object on a page.</li> <li>Visual Communication: The ability to take a concept, idea, or amount of     information, and to convey it clearly and directly to an audience using     visual means.</li> </ul>"}, {"location": "drawing/drawing/#ghosting", "title": "Ghosting", "text": "<p>Ghosting lines is a technique to break the mark making process into a series of steps that allows us to draw with confidence while also improving the accuracy of our results. It also forces us to think and consider our intentions before each and every mark we put down.</p> <ul> <li> <p>Planning: Lay out the terms of     the line you want to draw, paint a dot for the start and another for the     end.</p> <p></p> </li> <li> <p>Rotating the page: Find the most     comfortable angle of approach for the line you've planned. Usually it's     roughly a 45 degree angle fom left to right (if you're right handed).</p> <p></p> </li> <li> <p>Ghosting: Go through the motion     of drawing your line, over and over, in one direction, without actually     touching the page, so as to build muscle memory.</p> <p></p> </li> <li> <p>Execution: Once you feel     comfortable with the motion, without missing a beat or breaking the rhythm     of repetition, lower your pen to the page and go through the motion one more     time.</p> </li> </ul>"}, {"location": "drawing/drawing/#drawabox-course-guidelines", "title": "Drawabox course guidelines", "text": "<p>When doing the course exercises, try to:</p> <ul> <li>Read only the instruction pages that are assigned to each exercise.</li> <li>Don't redo the exercises until you achieve perfection, even when you don't     feel satisfied with your results. Accept this now, and it will save you     a lot of grief and wasted time in the future.</li> <li>Your only focus should be on following the instructions to the best of your     current ability. Read and follow the instructions carefully to ensure you     understand them.</li> <li>Each time you finish an exercise incorporate into a pool from which take two     or three at the beginning of each session to do for 10 or 15 minutes.</li> </ul>"}, {"location": "drawing/drawing/#tools-to-use", "title": "Tools to use", "text": "<p>For the Drawabox course, you need a fineliner, also called felt tips or technical pens. The author recommends Staedtler Pigments Liners and Faber Castell PITT Artist Pens (their sizing is different, F is the equivalent to 0.5).</p> <p>When using it, make sure you're not applying too much pressure, as it will damage the tip and reduce the flow of ink.</p> <p>For the paper, use the regular printer one.</p>"}, {"location": "drawing/drawing/#basic-shapes", "title": "Basic Shapes", "text": ""}, {"location": "drawing/drawing/#ellipses", "title": "Ellipses", "text": "<p>Ellipses are the next basic shape we're going to study (after the lines). They are extremely important and notoriously annoying to draw. Important because we're going to be using ellipses in 2D space to represent circles that exist in 3D space.</p> <p>How our circle (in 3D space) is rotated relative to the viewer will determine how wide our ellipse is going to be drawn. The degree shift (or angle) is measured between the minor axis and the line joining the viewer to the center of the circle. It will therefore be 90 degrees when it's facing directly to us, and 0 when we only see a line.</p> <p></p> <p>Keep in mind that the degree shift will change if we move the circle around even if we don't rotate it.</p> <p></p> <p>The minor axis is the line that passes across the ellipse's narrowest span and through it's center. This axis will split the ellipse into two equal symmetrical halves. An interesting property of the minor axis is that it coincides with and aligns with the normal vector of the circle.</p> <p>This means that we've got two ways of establishing the orientation of our circle in 3D space, as we draw it on our 2D page.</p> <ul> <li>The degree (width) of the ellipse controlling the circle's rotation</li> <li>The minor axis of the ellipse controlling the orientation of the circle's     normal vector.</li> </ul> <p>If we need our circle to be oriented in a specific way in a more complicated scene, we will actually be able to start out with a normal vector line, then use it as the minor axis for our ellipse.</p> <p>Taking it further, the minor axis can be used to build cylindrical forms because any section of a cylinder shares the same normal vector.</p>"}, {"location": "drawing/drawing/#drawing-ellipses", "title": "Drawing ellipses", "text": "<p>To draw ellipses follow the next steps:</p> <ul> <li>Decide on the ellipse degree, position and boundaries.</li> <li>Ghost the ellipse</li> <li> <p>Draw around the ellipse two or three times before lifting your pen.</p> <p>When you try to hit your ellipse in a single round, it's usually going to come out uneven and wobbly (due to drawing too slowly and carefully) or extremely loose (due to simply not having built up the muscle memory to nail an ellipse). Drawing through your ellipses gives your arm the chance to familiarize itself with what's being asked of it in that first pass, and then firm it up in the second. It also helps you maintain the confidence needed to achieve a smooth, even shape, without totally losing control.</p> </li> </ul> <p>Check the ellipse exercises to practice your skills.</p>"}, {"location": "drawing/drawing/#links", "title": "Links", "text": "<ul> <li>Drawabox</li> <li>Ctrl+Paint</li> </ul>"}, {"location": "drawing/drawing/#dive-deeper", "title": "Dive deeper", "text": "<ul> <li>Pool of drawing exercises</li> </ul>"}, {"location": "drawing/exercise_pool/", "title": "Exercise pool", "text": "<p>Set of exercises to maintain the fundamental skills required for drawing. Before doing a drawing session, spend 10-20 minutes doing one or several of these exercises.</p>"}, {"location": "drawing/exercise_pool/#lines", "title": "Lines", "text": "<ul> <li> <p>Superimposed lines: for     different increasing lengths (4cm, 8cm, half the width and full width), draw     a line with a ruler and repeat the stroke freehand eight times. Also try     some arcing lines, and even some waves. Make sure you fray only at the end.</p> <p></p> <p>Example:</p> <p></p> </li> <li> <p>Ghosted lines: Fill up a page with     straight lines following the ghosting method.</p> <p>Special things to avoid:</p> <ul> <li>Wobbly lines</li> <li>Arcing lines</li> </ul> <p>There are several levels of success with this exercise:</p> <ul> <li>Level 1: Line is smooth and consistent without any visible wobbling, but     doesn't quite pass through A or B, due to not following the right     trajectory. It's a straight shot, but misses the mark a bit.</li> <li>Level 2: Like level 1 and maintains the correct trajectory. It does     however either fall short or overshot one or both points.</li> <li>Level 3: Like level 2 and also starts at right at one point and ends     exactly at the other.</li> </ul> <p>Example:</p> <p></p> </li> <li> <p>Ghosted planes: Fill up a page     with planes using the ghosting method. Start with     4 points, join them, then fill in the two diagonals, and then make a cross     through the center of the X.</p> <p>Special things to avoid:</p> <ul> <li>Wobbly lines</li> <li>Arcing lines</li> </ul> <p>Example:</p> <p></p> <p>As you repeat the exercise, you can start to envision these planes as being three dimensional rectilinear surfaces. The third and fourth steps, where we construct the diagonals and the cross can be treated as being a subdivision of the plane. The cross will require some estimation to find the center of each edge in space.</p> </li> </ul>"}, {"location": "drawing/exercise_pool/#ellipses", "title": "Ellipses", "text": ""}, {"location": "drawing/exercise_pool/#tables-of-ellipses", "title": "Tables of ellipses", "text": "<p>This exercise is meant to get you used to drawing ellipses, in a variety of sizes, orientations and degrees. It also sets out a clear space each ellipse is meant to occupy, giving us a means to assess whether or not an ellipse was successful, or if there were visible mistakes (where it went outside of its allotted space, or ended up falling short). Practicing against set criteria, with a way to judge success/failure is an important element of learning. There's nothing wrong with failure - it's an opportunity to learn. Having a clearly defined task allows us to analyze those failures and make the most of them.</p> <p>Start off by taking your piece of paper and dividing it into a table with two columns and a bunch of rows.</p> <p></p> <p>For this one, you draw a circle starting from the far left of the box. Then, draw another beside it. Keep repeating it until you fill in the whole box. Strive to make your circles touch the top and bottom of the box, as well as the line to the left of it.</p> <p></p> <p>Next, same idea, but with ellipses. Within the same section, you should aim to draw ellipses of the same degree. You can also play with the angle of the ellipse, and this should also be consistent within the same section.</p> <p></p> <p>This one's a little different. Draw a wave through the section, dividing it into irregular pockets of space. Then fill these spaces with circles or ellipses, trying to keep them touching the bounds of the section as well as the curve. Everything should fit in there snugly, and nothing should be floating around.</p>"}, {"location": "drawing/exercise_pool/#things-to-remember", "title": "Things to remember", "text": "<ul> <li>Draw confidently: use the ghosting method and \"draw through\" your     ellipses two full times before lifting your pen to achieve a smooth,     even shape</li> <li>Plan your ellipse: Set out a goal for the ellipse you're about to draw.</li> <li>Draw from the shoulder.</li> </ul> <p>As you get better, your ellipses will tighten up - the gaps between your successive passes will shrink and eventually your ellipses will appear much cleaner.</p>"}, {"location": "feminism/privileges/", "title": "Privilege notes", "text": "<p>This page has been gathered from my white cis male brain with myself and fellow privileged readers as target group.</p>"}, {"location": "feminism/privileges/#what-are-the-privileges", "title": "What are the privileges", "text": "<p>Privileges are a group of special structural benefits, social advantages, that a group holds over another. So they are elements that should be removed from our lives.</p> <p>Although our individualist culture pushes us to think that the privileges are the result of our own individual efforts, they mainly come from facts that are structural, so they are out of our reach. Such as our genre, skin colour, ethnic, sexuality or overall economic status.</p> <p>Most of the things we consider privileges are rights that we should universalize. This confusion makes us mix them with the ones that should be eliminated because they reinforce the hierarchies and give the privileged license to keep on feeding the inequality relationships.</p> <p>To differentiate between privileges and rights, we can answer the question: Is it desirable to remove that privilege to the privileged?</p> <ul> <li>If the answer is yes, it is a privilege and we should fight to remove it.</li> <li>If the answer is no, then we should fight for universalize the right.</li> </ul>"}, {"location": "feminism/privileges/#what-can-we-do-to-fight-the-privileges", "title": "What can we do to fight the privileges?", "text": "<p>As white cis males we can:</p> <ul> <li>Make the different access to the basic security and dignity rights visible.</li> <li>Raise awareness on yourself and others on this topic between our fellow   privileged environment. It's more effective to address and permeate   a privileged person from a privileged position.</li> </ul>"}, {"location": "feminism/privileges/#unstructured-ideas", "title": "Unstructured ideas", "text": "<ul> <li>It is really difficult to notice a privilege if you've always liven with it. Go to the Michael Kimmel book for more information.</li> <li>The limits and barriers that slow us down is what is usually seen, unlike the   facilities we have to run faster.</li> <li>The individual solutions usually fail to solve social issues.</li> </ul>"}, {"location": "feminism/privileges/#unfinished-list-of-male-privileges", "title": "Unfinished list of male privileges", "text": "<ul> <li>Male positions are more valued by social structures, even when we are wrong.</li> <li>There are several abilities that are taken for granted.</li> <li>We can neglect the care work.</li> <li>We are more hireables as stated by the Jennifer-John   effect.</li> <li>We escalate more in the organizational hierarchy due to informal fraternity   networks.</li> <li>We can use violence when we want.</li> </ul>"}, {"location": "feminism/privileges/#unfinished-list-of-rights-to-universalize", "title": "Unfinished list of rights to universalize", "text": "<ul> <li>Science in general is performed by white male cis for the white male cis   humans. Therefore there is a medical gender   gap   or facial recognition algorithms struggle to recognize black   faces.</li> </ul>"}, {"location": "feminism/privileges/#resources", "title": "Resources", "text": "<ul> <li>Racial equity tools resources on white     privilege</li> </ul>"}, {"location": "feminism/privileges/#articles", "title": "Articles", "text": "<ul> <li>\u00bfExisten los privilegios masculinos?</li> <li>How to talk to your family, friends about racism and white privilege</li> <li>Peggy McIntosh     articles</li> <li>How to talk to your white friends and family about     privilege</li> <li>How to reduce online racism by Mark     Holden,     a long essay with interesting tips and a lot of useful visualizations,     I haven't checked the sources but it looks legit. (Thanks for the     recommendation Laurie <code>:)</code>)</li> </ul>"}, {"location": "feminism/privileges/#books", "title": "Books", "text": "<ul> <li>Privilege through the looking glass by Patricia Leavy</li> <li>Privilege A reader by Michael Kimmel</li> </ul>"}, {"location": "feminism/privileges/#essays", "title": "Essays", "text": "<ul> <li>White Privilege: Unpacking the Invisible Knapsack by Peggy McIntosh</li> </ul>"}, {"location": "linux/brew/", "title": "Brew", "text": "<p>Complementary package manager to manage the programs that aren't in the Debian repositories.</p>"}, {"location": "linux/brew/#usage", "title": "Usage", "text": "<p>TBC</p>"}, {"location": "linux/brew/#references", "title": "References", "text": "<ul> <li>Homebrew formula for a Go app</li> </ul>"}, {"location": "linux/cookiecutter/", "title": "Cookiecutter", "text": "<p>Cookiecutter is a command-line utility that creates projects from cookiecutters (project templates).</p>"}, {"location": "linux/cookiecutter/#install", "title": "Install", "text": "<pre><code>pip install cookiecutter\n</code></pre>"}, {"location": "linux/cookiecutter/#use", "title": "Use", "text": "<p>DEPRECATION: use cruft instead</p> <p>You may want to use cruft to generate your templates instead, as it will help you maintain the project with the template updates. Something that it's not easy with cookiecutter alone</p> <pre><code>cookiecutter {{ path_or_url_to_cookiecutter_template }}\n</code></pre>"}, {"location": "linux/cookiecutter/#user-config", "title": "User config", "text": "<p>If you use Cookiecutter a lot, you\u2019ll find it useful to have a user config file. By default Cookiecutter tries to retrieve settings from a <code>.cookiecutterrc</code> file in your home directory.</p> <p>Example user config:</p> <pre><code>default_context:\nfull_name: \"Audrey Roy\"\nemail: \"audreyr@example.com\"\ngithub_username: \"audreyr\"\ncookiecutters_dir: \"/home/audreyr/my-custom-cookiecutters-dir/\"\nreplay_dir: \"/home/audreyr/my-custom-replay-dir/\"\nabbreviations:\npython: https://github.com/audreyr/cookiecutter-pypackage.git\ngh: https://github.com/{0}.git\nbb: https://bitbucket.org/{0}\n</code></pre> <p>Possible settings are:</p> <code>default_context</code> A list of key/value pairs that you want injected as context whenever you generate a project with Cookiecutter. These values are treated like the defaults in <code>cookiecutter.json</code>, upon generation of any project. <code>cookiecutters_dir</code> Directory where your cookiecutters are cloned to when you use Cookiecutter with a repo argument. <code>replay_dir</code> Directory where Cookiecutter dumps context data to, which you can fetch later on when using the replay feature. <code>abbreviations</code> A list of abbreviations for cookiecutters. Abbreviations can be simple aliases for a repo name, or can be used as a prefix, in the form <code>abbr:suffix</code>. Any suffix will be inserted into the expansion in place of the text <code>{0}</code>, using standard Python string formatting. With the above aliases, you could use the <code>cookiecutter-pypackage</code> template simply by saying cookiecutter <code>python</code>."}, {"location": "linux/cookiecutter/#write-your-own-cookietemplates", "title": "Write your own cookietemplates", "text": ""}, {"location": "linux/cookiecutter/#create-files-or-directories-with-conditions", "title": "Create files or directories with conditions", "text": "<p>For files use a filename like <code>'{{ \".vault_pass.sh\" if cookiecutter.vault_pass_entry != \"None\" else \"\" }}'</code>.</p> <p>For directories I haven't yet found a nice way to do it (as the above will fail), check the issue or the hooks documentation for more information.</p> <p>File: <code>post_gen_project.py</code></p> <pre><code>import os\nimport sys\n\nREMOVE_PATHS = [\n    '{% if cookiecutter.packaging != \"pip\" %} requirements.txt {% endif %}',\n    '{% if cookiecutter.packaging != \"poetry\" %} poetry.lock {% endif %}',\n]\n\nfor path in REMOVE_PATHS:\n    path = path.strip()\n    if path and os.path.exists(path):\n        if os.path.isdir(path):\n            os.rmdir(path)\n        else:\n            os.unlink(path)\n</code></pre>"}, {"location": "linux/cookiecutter/#add-some-text-to-a-file-if-a-condition-is-met", "title": "Add some text to a file if a condition is met", "text": "<p>Use jinja2 conditionals. Note the <code>-</code> at the end of the conditional opening, play with <code>{%- ... -%}</code> and <code>{% ... %}</code> for different results on line appending.</p> <pre><code>{% if cookiecutter.install_docker == 'yes' -%}\n- src: git+ssh://mywebpage.org/ansible-roles/docker.git\nversion: 1.0.3\n{%- else -%}\n- src: git+ssh://mywebpage.org/ansible-roles/other-role.git\nversion: 1.0.2\n{%- endif %}\n</code></pre>"}, {"location": "linux/cookiecutter/#initialize-git-repository-on-the-created-cookiecutter", "title": "Initialize git repository on the created cookiecutter", "text": "<p>Added the following to the post generation hooks.</p> <p>File: hooks/post_gen_project.py</p> <pre><code>import subprocess\n\nsubprocess.call(['git', 'init'])\nsubprocess.call(['git', 'add', '*'])\nsubprocess.call(['git', 'commit', '-m', 'Initial commit'])\n</code></pre>"}, {"location": "linux/cookiecutter/#prevent-cookiecutter-from-processing-some-files", "title": "Prevent cookiecutter from processing some files", "text": "<p>By default cookiecutter will try to process every file as a Jinja template. This behaviour produces wrong results if you have Jinja templates that are meant to be taken as literal. Starting with cookiecutter 1.1, you can tell cookiecutter to only copy some files without interpreting them as Jinja templates.</p> <p>Add a <code>_copy_without_render</code> key in the cookiecutter config file (<code>cookiecutter.json</code>). It takes a list of regular expressions. If a filename matches the regular expressions it will be copied and not processed as a Jinja template.</p> <pre><code>{\n\"project_slug\": \"sample\",\n\"_copy_without_render\": [\n\"*.js\",\n\"not_rendered_dir/*\",\n\"rendered_dir/not_rendered_file.ini\"\n]\n}\n</code></pre>"}, {"location": "linux/cookiecutter/#prevent-additional-whitespaces-when-jinja-condition-is-not-met", "title": "Prevent additional whitespaces when jinja condition is not met.", "text": "<p>Jinja2 has a whitespace control that can be used to manage the whitelines existent between the Jinja blocks. The problem comes when a condition is not met in an <code>if</code> block, in that case, Jinja adds a whitespace which will break most linters.</p> <p>This is the solution I've found out that works as expected.</p> <pre><code>### Multienvironment\n\nThis playbook has support for the following environments:\n\n{% if cookiecutter.production_environment == \"True\" -%}\n* Production\n{% endif %}\n{%- if cookiecutter.staging_environment == \"True\" -%}\n* Staging\n{% endif %}\n{%- if cookiecutter.development_environment == \"True\" -%}\n* Development\n{% endif %}\n### Tags\n</code></pre>"}, {"location": "linux/cookiecutter/#testing-your-own-cookiecutter-templates", "title": "Testing your own cookiecutter templates", "text": "<p>The pytest-cookies plugin comes with a <code>cookies</code> fixture which is a wrapper for the cookiecutter API for generating projects. It helps you verify that your template is working as expected and takes care of cleaning up after running the tests.</p>"}, {"location": "linux/cookiecutter/#install_1", "title": "Install", "text": "<pre><code>pip install pytest-cookies\n</code></pre>"}, {"location": "linux/cookiecutter/#usage", "title": "Usage", "text": "<p>@pytest.fixture def context():     return {         \"playbook_name\": \"My Test Playbook\",     }</p> <p>The <code>cookies.bake()</code> method generates a new project from your template based on the default values specified in cookiecutter.json:</p> <pre><code>def test_bake_project(cookies):\n    result = cookies.bake(extra_context={'repo_name': 'hello world'})\n\n    assert result.exit_code == 0\n    assert result.exception is None\n    assert result.project.basename == 'hello world'\n    assert result.project.isdir()\n</code></pre> <p>It accepts the <code>extra_context</code> keyword argument that is passed to cookiecutter. The given dictionary will override the default values of the template context, allowing you to test arbitrary user input data.</p> <p>The cookiecutter-django has a nice test file using this fixture.</p>"}, {"location": "linux/cookiecutter/#mocking-the-contents-of-the-cookiecutter-hooks", "title": "Mocking the contents of the cookiecutter hooks", "text": "<p>Sometimes it's interesting to add interactions with external services in the cookiecutter hooks, for example to activate a CI pipeline.</p> <p>If you want to test the cookiecutter template you need to mock those external interactions. But it's difficult to mock the contents of the hooks because their contents aren't run by the <code>cookies.bake()</code> code. Instead it delegates in cookiecutter to run them, which opens a <code>subprocess</code> to run them, so the mocks don't work.</p> <p>The alternative is setting an environmental variable in your tests to skip those steps:</p> <p>File: tests/conftest.py</p> <pre><code>import os\n\nos.environ[\"COOKIECUTTER_TESTING\"] = \"true\"\n</code></pre> <p>File: hooks/pre_gen_project.py</p> <pre><code>def main():\n    # ... pre_hook content ...\n\n\nif __name__ == \"__main__\":\n\n    if os.environ.get(\"COOKIECUTTER_TESTING\") != \"true\":\n        main()\n</code></pre> <p>If you want to test the content of <code>main</code>, you can now mock each of the external interactions. But you'll face the problem that these files are jinja2 templates of python files, so it's tricky to test them, due to syntax errors.</p>"}, {"location": "linux/cookiecutter/#debug-failing-template-generation", "title": "Debug failing template generation", "text": "<p>Sometimes the generation of the templates will fail in the tests, I've found that the easier way to debug why is to inspect the result object of the <code>result = cookies.bake()</code> statement with pdb.</p> <p>It has an <code>exception</code> method with <code>lineno</code> argument and <code>source</code>. With that information I've been able to locate the failing line. It also has a <code>filename</code> attribute but it doesn't seem to work for me.</p>"}, {"location": "linux/cookiecutter/#references", "title": "References", "text": "<ul> <li>Git</li> <li>Docs</li> </ul>"}, {"location": "linux/cruft/", "title": "Cruft", "text": "<p>cruft allows you to maintain all the necessary boilerplate for packaging and building projects separate from the code you intentionally write. Fully compatible with existing Cookiecutter templates.</p> <p>Many project template utilities exist that automate the copying and pasting of code to create new projects. This seems great! However, once created, most leave you with that copy-and-pasted code to manage through the life of your project.</p>"}, {"location": "linux/cruft/#key-features", "title": "Key Features", "text": "Cookiecutter Compatible cruft utilizes Cookiecutter as its template expansion engine. Meaning it retains full compatibility with all existing Cookiecutter templates. Template Validation cruft can quickly validate whether or not a project is using the latest version of a template using <code>cruft check</code>. This check can easily be added to CI pipelines to ensure your projects stay in-sync. Automatic Template Updates cruft automates the process of updating code to match the latest version of a template, making it easy to utilize template improvements across many projects."}, {"location": "linux/cruft/#installation", "title": "Installation", "text": "<pre><code>pip install cruft\n</code></pre>"}, {"location": "linux/cruft/#usage", "title": "Usage", "text": ""}, {"location": "linux/cruft/#creating-a-new-project", "title": "Creating a New Project", "text": "<p>To create a new project using cruft run <code>cruft create PROJECT_URL</code> from the command line.</p> <p>cruft will then ask you any necessary questions to create your new project. It will use your answers to expand the provided template, and then return the directory it placed the expanded project.</p> <p>Behind the scenes, cruft uses Cookiecutter to do the project expansion. The only difference in the resulting output is a <code>.cruft.json</code> file that contains the git hash of the template used as well as the parameters specified.</p>"}, {"location": "linux/cruft/#updating-a-project", "title": "Updating a Project", "text": "<p>To update an existing project, that was created using cruft, run <code>cruft update</code> in the root of the project.</p> <p>If there are any updates, cruft will have you review them before applying. If you accept the changes cruft will apply them to your project and update the <code>.cruft.json</code> file for you.</p> <p>Sometimes certain files just aren't good fits for updating. Such as test cases or <code>__init__</code> files. You can tell cruft to always skip updating these files on a project by project basis by added them to a skip section within your .cruft.json file:</p> <pre><code>{\n\"template\": \"https://github.com/timothycrosley/cookiecutter-python\",\n\"commit\": \"8a65a360d51250221193ed0ec5ed292e72b32b0b\",\n\"skip\": [\n\"cruft/__init__.py\",\n\"tests\"\n],\n...\n}\n</code></pre> <p>Or, if you have toml installed, you can add skip files directly to a <code>tool.cruft</code> section of your <code>pyproject.toml</code> file:</p> <pre><code>[tool.cruft]\nskip = [\"cruft/__init__.py\", \"tests\"]\n</code></pre>"}, {"location": "linux/cruft/#checking-a-project", "title": "Checking a Project", "text": "<p>Checking to see if a project is missing a template update is as easy as running <code>cruft check</code>. If the project is out-of-date an error and exit code 1 will be returned.</p> <p><code>cruft check</code> can be added to CI pipelines to ensure projects don't unintentionally drift.</p>"}, {"location": "linux/cruft/#linking-an-existing-project", "title": "Linking an Existing Project", "text": "<p>Have an existing project that you created from a template in the past using Cookiecutter directly? You can link it to the template that was used to create it using: <code>cruft link TEMPLATE_REPOSITORY</code>.</p> <p>You can then specify the last commit of the template the project has been updated to be consistent with, or accept the default of using the latest commit from the template.</p>"}, {"location": "linux/cruft/#compute-the-diff", "title": "Compute the diff", "text": "<p>With time, your boilerplate may end up being very different from the actual cookiecutter template. Cruft allows you to quickly see what changed in your local project compared to the template. It is as easy as running <code>cruft diff</code>. If any local file differs from the template, the diff will appear in your terminal in a similar fashion to <code>git diff</code>.</p> <p>The <code>cruft diff</code> command optionally accepts an <code>--exit-code</code> flag that will make cruft exit with a non-0 code should any diff is found. You can combine this flag with the <code>skip</code> section of your <code>.cruft.json</code> to make stricter CI checks that ensures any improvement to the template is always submitted upstream.</p>"}, {"location": "linux/cruft/#issues", "title": "Issues", "text": "<ul> <li>Save config in the     pyproject.toml: Update the     template once it's supported.</li> </ul>"}, {"location": "linux/cruft/#error-unable-to-interpret-changes-between-current-project-and-cookiecutter-template-as-unicode", "title": "Error: Unable to interpret changes between current project and cookiecutter template as unicode.", "text": "<p>Typically a result of hidden binary files in project folder. Maybe you have a hook that initializes the <code>.git</code> directory. Since <code>2.10.0</code> you can add a <code>skip</code> category inside the <code>.cruft.json</code>, so that it doesn't check that directory:</p> <pre><code>{\n\"template\": \"xxx\",\n\"commit\": \"xxx\",\n\"checkout\": null,\n\"context\": {\n\"cookiecutter\": {\n...\n}\n},\n\"directory\": null,\n\"skip\": [\n\".git\"\n]\n}\n</code></pre>"}, {"location": "linux/cruft/#references", "title": "References", "text": "<ul> <li>Docs</li> <li>Git</li> <li>Issues</li> </ul>"}, {"location": "linux/elasticsearch/", "title": "Commands for elasticsearch", "text": ""}, {"location": "linux/elasticsearch/#searching-documents", "title": "Searching documents", "text": "<p>We use HTTP requests to talk to ElasticSearch. A HTTP request is made up of several components such as the URL to make the request to, HTTP verbs (GET, POST etc) and headers. In order to succinctly and consistently describe HTTP requests the ElasticSearch documentation uses cURL command line syntax. This is also the standard practice to describe requests made to ElasticSearch within the user community.</p>"}, {"location": "linux/elasticsearch/#get-all-documents", "title": "Get all documents", "text": "<p>An example HTTP request using CURL syntax looks like this:</p> <pre><code>curl \\\n-H 'Content-Type: application/json' \\\n-XPOST \"https://localhost:9200/_search\" \\\n-d' { \"query\": { \"match_all\": {} }}'\n</code></pre>"}, {"location": "linux/elasticsearch/#get-documents-that-match-a-string", "title": "Get documents that match a string", "text": "<pre><code>curl \\\n-H 'Content-Type: application/json' \\\n-XPOST \"https://localhost:9200/_search\" \\\n-d' { \"query\": { \"query_string\": {\"query\": \"test company\"} }}'\n</code></pre>"}, {"location": "linux/elasticsearch/#backup", "title": "Backup", "text": "<p>It's better to use the <code>curator</code> tool</p>"}, {"location": "linux/elasticsearch/#create-snapshot", "title": "Create snapshot", "text": "<pre><code>curl {{ url }}/_snapshot/{{ backup_path }}/{{ snapshot_name }}?wait_for_completion=true\n</code></pre>"}, {"location": "linux/elasticsearch/#create-snapshot-of-selected-indices", "title": "Create snapshot of selected indices", "text": "<pre><code>curl {{ url }}/_snapshot/{{ backup_path }}/{{ snapshot_name }}?wait_for_completion=true\n\ncurl -XPUT 'localhost:9200/_snapshot/my_backup/snapshot_1?pretty' -H 'Content-Type: application/json' -d'\n{\n    \"indices\": \"index_1,index_2\",\n    \"ignore_unavailable\": true,\n    \"include_global_state\": false\n}\n'\n</code></pre>"}, {"location": "linux/elasticsearch/#list-all-backups", "title": "List all backups", "text": "<p>Check for my-snapshot-repo</p> <pre><code>curl {{ url }}/_snapshot/{{ backup_path }}/*?pretty\n</code></pre>"}, {"location": "linux/elasticsearch/#restore-backup", "title": "Restore backup", "text": "<p>First you need to close the selected indices</p> <pre><code>curl -X POST {{ url }}/{{ indice_name }}/_close\n</code></pre> <p>Then restore</p> <pre><code>curl {{ url }}/_snapshot/{{ backup_path }}/{{ snapshot_name }}/_restore?wait_for_completion=true\n</code></pre> <p>If you want to restore only one index, use:</p> <pre><code>curl -X POST \"{{ url }}/_snapshot/{{ backup_path }}/{{ snapshot_name }}/_restore?pretty\" -H 'Content-Type: application/json' -d'\n{\n    \"indices\": \"{{ index_to_restore }}\",\n}'\n</code></pre>"}, {"location": "linux/elasticsearch/#delete-snapshot", "title": "Delete snapshot", "text": "<pre><code>curl -XDELETE {{ url }}/_snapshot/{{ backup_path }}/{{ snapshot_name }}\n</code></pre>"}, {"location": "linux/elasticsearch/#delete-snapshot-repository", "title": "Delete snapshot repository", "text": "<pre><code>curl -XDELETE {{ url }}/_snapshot/{{ backup_path }}\n</code></pre>"}, {"location": "linux/elasticsearch/#delete-snapshots-older-than-x", "title": "Delete snapshots older than X", "text": "<p>!!! note \"File: curator.yml\" ```yaml client: hosts: - 'a data node' port: 9200 url_prefix: use_ssl: False certificate: client_cert: client_key: ssl_no_validate: False http_auth: timeout: 30 master_only: False</p> <pre><code>logging:\nloglevel: INFO\nlogfile: D:\\CuratorLogs\\logs.txt\nlogformat: default\nblacklist: ['elasticsearch', 'urllib3']\n```\n</code></pre> <p>File: delete_old_snapshots.yml</p> <p><code>yaml     actions:     1:     action: delete_snapshots     description: &gt;-     Delete snapshots from the selected repository older than 100 days     (based on creation_date), for everything but 'citydirectory-' prefixed snapshots.     options:     repository: 'dcs-elastic-snapshot'     disable_action: False     filters:     - filtertype: pattern     kind: prefix     value: citydirectory-     exclude: True     - filtertype: age     source: creation_date     direction: older     unit: days     unit_count: 100</code></p>"}, {"location": "linux/elasticsearch/#information-gathering", "title": "Information gathering", "text": ""}, {"location": "linux/elasticsearch/#get-status-of-cluster", "title": "Get status of cluster", "text": "<pre><code>curl {{ url }}/_cluster/health?pretty\ncurl {{ url }}/_cat/nodes?v\ncurl {{ url }}/_cat/indices?v\ncurl {{ url }}/_cat/shards\n</code></pre> <p>If you've got red status, use the following command to choose the first unassigned shard that it finds and explains why it cannot be allocated to a node.</p> <pre><code>curl {{ url }}/_cluster/allocation/explain?v\n</code></pre>"}, {"location": "linux/elasticsearch/#get-settings", "title": "Get settings", "text": "<pre><code>curl {{ url }}/_settings\n</code></pre>"}, {"location": "linux/elasticsearch/#get-space-left", "title": "Get space left", "text": "<pre><code>curl {{ url }}/_nodes/stats/fs?pretty\n</code></pre>"}, {"location": "linux/elasticsearch/#list-plugins", "title": "List plugins", "text": "<pre><code>curl {{ url }}/_nodes/plugins?pretty\n</code></pre>"}, {"location": "linux/elasticsearch/#upload", "title": "Upload", "text": ""}, {"location": "linux/elasticsearch/#single-data-upload", "title": "Single data upload", "text": "<pre><code>curl -XPOST '{{ url }}/{{ path_to_table }}' -d '{{ json_input }}'\n</code></pre> <p>where json_input can be <code>{ \"field\" : \"value\" }</code></p>"}, {"location": "linux/elasticsearch/#bulk-upload-of-data", "title": "Bulk upload of data", "text": "<pre><code>curl -H 'Content-Type: application/x-ndjson' -XPOST \\\n'{{ url }}/{{ path_to_table }}/_bulk?pretty' --data-binary @{{ json_file }}\n</code></pre>"}, {"location": "linux/elasticsearch/#delete", "title": "Delete", "text": ""}, {"location": "linux/elasticsearch/#delete-data", "title": "Delete data", "text": "<pre><code>curl -XDELETE {{ url }}/{{ path_to_ddbb }}\n</code></pre>"}, {"location": "linux/elasticsearch/#reindex-an-index", "title": "Reindex an index", "text": "<p>If you encountered errors while reindexing <code>source_index</code> to <code>destination_index</code> it can be because the cluster hit a timeout on the scroll locks. As a work around, you can increase the timeout period to a reasonable value and then reindex. The default AWS values are search context of 5 minutes, socket timeout of 30 seconds, and batch size of 1,000.</p> <p>First clear the cache of the index with:</p> <pre><code>curl -X POST https://elastic.url/destination_index/_cache/clear\n</code></pre> <p>If the index is big, they suggest to disable replicas in your destination index by setting number_of_replicas to 0 and re-enable them once the reindex process is complete.</p> <p>To get the current state use:</p> <pre><code>curl https://elastic.url/destination_index/_settings\n</code></pre> <p>Then disable the replicas with:</p> <pre><code>curl -X PUT \\\nhttps://elastic.url/destination_index \\\n-H 'Content-Type: application/json' \\\n-d '{\"settings\": {\"refresh_interval\": -1, \"number_of_replicas\": 0}}\n</code></pre> <p>Now you can reindex the index with:</p> <pre><code>curl -X POST \\\nhttps://elastic.url/_reindex?wait_for_completion=false\\&amp;timeout=10m\\&amp;scroll=10h\\&amp;pretty=true \\\n-H 'Content-Type: application/json' \\\n-d '{\"source\": { \"remote\": { \"host\": \"https://elastic.url:443\", \"socket_timeout\": \"60m\" }, \"index\": \"source_index\" }, \"dest\": {\"index\": \"destination_index\"}}'\n</code></pre> <p>And check the evolution of the task with:</p> <pre><code>curl 'https://elastic.url/_tasks?detailed=true&amp;actions=*reindex&amp;group_by=parents&amp;pretty=true'\n</code></pre> <p>The output is quite verbose, so I use <code>vimdiff</code> to see the differences between instant states.</p> <p>If you see there are no tasks running, check the indices status to see if the reindex ended well.</p> <pre><code>curl https://elastic.url/_cat/indices\n</code></pre> <p>After the reindex process is complete, you can reset your desired replica count and remove the refresh interval setting.</p>"}, {"location": "linux/elasticsearch/#knn", "title": "KNN", "text": ""}, {"location": "linux/elasticsearch/#knn-sizing", "title": "KNN sizing", "text": "<p>Typically, in an Elasticsearch cluster, a certain portion of RAM is set aside for the JVM heap. The k-NN plugin allocates graphs to a portion of the remaining RAM. This portion\u2019s size is determined by the circuit_breaker_limit cluster setting. By default, the circuit breaker limit is set at 50%.</p> <p>The memory required for graphs is estimated to be `1.1 * (4 * dimension</p> <ul> <li>8 * M)` bytes/vector.</li> </ul> <p>To get the <code>dimension</code> and <code>m</code> use the <code>/index</code> elasticsearch endpoint. To get the number of vectors, use <code>/index/_count</code>. The number of vectors is the same as the number of documents.</p> <p>As an example, assume that we have 1 Million vectors with a dimension of 256 and M of 16, and the memory required can be estimated as:</p> <pre><code>1.1 * (4 *256 + 8 * 16) * 1,000,000 ~= 1.26 GB\n</code></pre> <p>!!! note \"Remember that having a replica will double the total number of vectors.\"</p> <p>I've seen some queries work with indices that required 120% of the available memory for the KNN.</p> <p>A good way to see if it fits, is warming up the knn vectors. If the process returns a timeout, you probably don't have enough memory.</p>"}, {"location": "linux/elasticsearch/#knn-warmup", "title": "KNN warmup", "text": "<p>The Hierarchical Navigable Small World (HNSW) graphs that are used to perform an approximate k-Nearest Neighbor (k-NN) search are stored as .hnsw files with other Apache Lucene segment files. In order for you to perform a search on these graphs using the k-NN plugin, these files need to be loaded into native memory.</p> <p>If the plugin has not loaded the graphs into native memory, it loads them when it receives a search request. This loading time can cause high latency during initial queries. To avoid this situation, users often run random queries during a warmup period. After this warmup period, the graphs are loaded into native memory and their production workloads can begin. This loading process is indirect and requires extra effort.</p> <p>As an alternative, you can avoid this latency issue by running the k-NN plugin warmup API operation on whatever indices you\u2019re interested in searching. This operation loads all the graphs for all of the shards (primaries and replicas) of all the indices specified in the request into native memory.</p> <p>After the process finishes, you can start searching against the indices with no initial latency penalties. The warmup API operation is idempotent, so if a segment\u2019s graphs are already loaded into memory, this operation has no impact on those graphs. It only loads graphs that aren\u2019t currently in memory.</p> <p>This request performs a warmup on three indices:</p> <pre><code>GET /_opendistro/_knn/warmup/index1,index2,index3?pretty\n{\n  \"_shards\" : {\n    \"total\" : 6,\n    \"successful\" : 6,\n    \"failed\" : 0\n  }\n}\n</code></pre> <p><code>total</code> indicates how many shards the k-NN plugin attempted to warm up. The response also includes the number of shards the plugin succeeded and failed to warm up.</p> <p>The call does not return until the warmup operation is complete or the request times out. If the request times out, the operation still continues on the cluster. To monitor the warmup operation, use the Elasticsearch <code>_tasks</code> API:</p> <pre><code>GET /_tasks\n</code></pre>"}, {"location": "linux/elasticsearch/#troubleshooting", "title": "Troubleshooting", "text": ""}, {"location": "linux/elasticsearch/#deal-with-the-aws-service-timeout", "title": "Deal with the AWS service timeout", "text": "<p>AWS' Elasticsearch service is exposed behind a load balancer that returns a timeout after 300 seconds. If the query you're sending takes longer you won't be able to retrieve the information.</p> <p>You can consider using Asynchronous search which requires Elasticsearch 7.10 or later. Asynchronous search lets you run search requests that run in the background. You can monitor the progress of these searches and get back partial results as they become available. After the search finishes, you can save the results to examine at a later time.</p> <p>If the query you're running is a KNN one, you can try:</p> <ul> <li> <p>Using the knn warmup api before running initial queries.</p> </li> <li> <p>Scaling up the instances: Amazon ES uses half of an instance's RAM for the   Java heap (up to a heap size of 32 GiB). By default, KNN uses up to 50% of the   remaining half, so an instance type with 64 GiB of RAM can accommodate 16 GiB   of graphs (64 * 0.5 * 0.5). Performance can suffer if graph memory usage   exceeds this value.</p> </li> <li> <p>In a less recommended approach, you can make more percentage of memory   available for KNN operations.</p> </li> </ul> <p>Open Distro for Elasticsearch lets you modify all KNN settings using the   <code>_cluster/settings</code> API. On Amazon ES, you can change all settings except   <code>knn.memory.circuit_breaker.enabled</code> and <code>knn.circuit_breaker.triggered</code>.</p> <p>You can change the circuit breaker settings as:</p> <pre><code>PUT /_cluster/settings\n{\n  \"persistent\" : {\n    \"knn.memory.circuit_breaker.limit\" : \"&lt;value%&gt;\"\n  }\n}\n</code></pre> <p>You could also do performance tuning your KNN request.</p>"}, {"location": "linux/elasticsearch/#fix-circuit-breakers-triggers", "title": "Fix Circuit breakers triggers", "text": "<p>The <code>elasticsearch_exporter</code> has a <code>elasticsearch_breakers_tripped</code> metric, which counts then number of Circuit Breakers triggered of the different kinds. The Grafana dashboard paints a count of all the triggers with a big red number, which may scare you at first.</p> <p>Lets first understand what are Circuit Breakers. Elasticsearch is built with Java and as such depends on the JVM heap for many operations and caching purposes. By default in AWS, each data node is assigned half the RAM to be used for heap for ES. In Elasticsearch the default Garbage Collector is Concurrent-Mark and Sweep (CMS). When the JVM Memory Pressure reaches 75%, this collector pauses some threads and attempts to reclaim some heap space. High heap usage occurs when the garbage collection process cannot keep up. An indicator of high heap usage is when the garbage collection is incapable of reducing the heap usage to around 30%.</p> <p>When a request reaches the ES nodes, circuit breakers estimate the amount of memory needed to load the required data. The cluster then compares the estimated size with the configured heap size limit. If the estimated size of your data is greater than the available heap size, the query is terminated. As a result, a CircuitBreakerException is thrown to prevent overloading the node.</p> <p>In essence, these breakers are present to prevent a request overloading a data node and consuming more heap space than that node can provide at that time. If these breakers weren't present, then the request will use up all the heap that the node can provide and this node will then restart due to OOM.</p> <p>Lets assume a data node has 16GB heap configured, When the parent circuit breaker is tripped, then a similar error is thrown:</p> <pre><code>\"error\": {\n\"root_cause\": [\n{\n\"type\": \"circuit_breaking_exception\",\n\"reason\": \"[parent] Data too large, data for [&lt;HTTP_request&gt;] would be [16355096754/15.2gb], which is larger than the limit of [16213167308/15gb], real usage: [15283269136/14.2gb], new bytes reserved: [1071827618/1022.1mb]\",\n}\n]\n}\n</code></pre> <p>The parent circuit breaker (a circuit breaker type) is responsible for the overall memory usage of your Elasticsearch cluster. When a parent circuit breaker exception occurs, the total memory used across all circuit breakers has exceeded the set limit. A parent breaker throws an exception when the cluster exceeds 95% of 16 GB, which is 15.2 GB of heap (in above example).</p> <p>A circuit breaking exception is generally caused by high JVM. When the JVM Memory Pressure is high, it indicates that a large portion of one or more data nodes configured heap is currently being used heavily, and as such, the frequency of the circuit breakers being tripped increases as there is not enough heap available at the time to process concurrent smaller or larger requests.</p> <p>It is worth noting that that the error can also be thrown by a certain request that would just consume all the available heap on a certain data node at the time such as an intensive search query.</p> <p>If you see numerous spikes to the high 90%, with occasionally spikes to 100%, it's not uncommon for the parent circuit breaker to be tripped in response to requests.</p> <p>To troubleshoot circuit breakers, you'll then have to address the High JVM issues, which can be caused by:</p> <ul> <li>Increase in the number of requests to the cluster. Check the IndexRate and   SearchRate metrics in to determine your current load.</li> <li>Aggregation, wildcards, and using wide time ranges in your queries.</li> <li>Unbalanced shard allocation across nodes or too many shards in a cluster.</li> <li>Index mapping explosions.</li> <li>Using the fielddata data structure to query data. Fielddata can consume a   large amount of heap space, and remains in the heap for the lifetime of a   segment. As a result, JVM memory pressure remains high on the cluster when   fielddata is used.</li> </ul> <p>Here's what happens as JVM memory pressure increases in AWS:</p> <ul> <li>At 75%: Amazon ES triggers the Concurrent Mark Sweep (CMS) garbage collector.   The CMS collector runs alongside other processes to keep pauses and   disruptions to a minimum. The garbage collection is a CPU-intensive process.   If JVM memory pressure stays at this percentage for a few minutes, then you   could encounter ClusterBlockException, JVM OutOfMemoryError, or other cluster   performance issues.</li> <li>Above 75%: If the CMS collector fails to reclaim enough memory and usage   remains above 75%, Amazon ES triggers a different garbage collection   algorithm. This algorithm tries to free up memory and prevent a JVM   OutOfMemoryError (OOM) exception by slowing or stopping processes.</li> <li>Above 92% for 30 minutes: Amazon ES blocks all write operations.</li> <li>Around 95%: Amazon ES kills processes that try to allocate memory. If a   critical process is killed, one or more cluster nodes might fail.</li> <li>At 100%: Amazon ES JVM is configured to exit and eventually restarts on   OutOfMemory (OOM).</li> </ul> <p>To resolve high JVM memory pressure, try the following tips:</p> <ul> <li> <p>Reduce incoming traffic to your cluster, especially if you have a heavy   workload.</p> </li> <li> <p>Consider scaling the cluster to obtain more JVM memory to support your   workload. As mentioned above each data node gets half the RAM allocated to be   used as Heap. Consider scaling to a data node type with more RAM and hence   more Available Heap. Thereby increasing the parent circuit breaker limit.</p> </li> <li> <p>If cluster scaling isn't possible, try reducing the number of shards by   deleting old or unused indices. Because shard metadata is stored in memory,   reducing the number of shards can reduce overall memory usage.</p> </li> <li> <p>Enable slow logs to identify faulty requests. Note: Before enabling   configuration changes, verify that JVM memory pressure is below 85%. This way,   you can avoid additional overhead to existing resources.</p> </li> <li> <p>Optimize search and indexing requests, and choose the correct number of   shards.</p> </li> <li> <p>Disable and avoid using fielddata. By default, fielddata is set to \"false\" on   a text field unless it's explicitly defined as otherwise in index mappings.</p> </li> </ul> <p>Field data is a potentially a huge consumer of JVM Heap space. This build up   of field data occurs when aggregations are run on fields that are of type   <code>text</code>. More on how you can periodically clear field data below.</p> <ul> <li>Change your index mapping type to a <code>keyword</code>, using reindex API. You can use   the <code>keyword</code> type as an alternative for performing aggregations and sorting   on text fields.</li> </ul> <p>As mentioned in above point, by aggregating on <code>keyword</code> type instead of   <code>text</code>, no field data has to be built on demand and hence won't consume   precious heap space. Look into the commonly aggregated fields in index   mappings and ensure they are not of type <code>text</code>.</p> <p>If they are, you can consider changing them to <code>keyword</code>. You will have to   create a new index with the desired mapping and then use the Reindex API to   transfer over the documents from the source index to the new index. Once   Re-index has completed then you can delete the old index.</p> <ul> <li> <p>Avoid aggregating on text fields to prevent increases in field data. When you   use more field data, more heap space is consumed. Use the cluster stats API   operation to check your field data.</p> </li> <li> <p>Clear the fielddata cache with the following API call:</p> </li> </ul> <pre><code>POST /index_name/_cache/clear?fielddata=true (index-level cache)\nPOST */_cache/clear?fielddata=true (cluster-level cache)\n</code></pre> <p>Generally speaking, if you notice your workload (search rate and index rate) remaining consistent during these high spikes and non of the above optimizations can be applied or if they have already been applied and the JVM is still high during these workload times, then it is an indication that the cluster needs to be scaled in terms of JVM resources to cope with this workload.</p> <p>You can't reset the 'tripped' count. This is a Node level metric and thus will be reset to <code>0</code> when the Elasticsearch Service is restarted on that Node. Since in AWS it's a managed service, unfortunately you will not have access to the underlaying EC2 instance to restart the ES Process.</p> <p>However the ES Process can be restarted on your end (on all nodes) in the following ways:</p> <ul> <li>Initiate a Configuration Change that causes a blue/green deployment : When you   initiate a configuration change, a subsequent blue/green deployment process is   launched in which we launch a new fleet that matches the desired   configuration. The old fleet continues to run and serve requests.   Simultaneously, data in the form of shards are then migrated from the old   fleet to the new fleet. Once all this data has been migrated the old fleet is   terminated and the new one takes over.</li> </ul> <p>During this process ES is restarted on the Nodes.</p> <p>Ensure that CPU Utilization and JVM Memory Pressure are below the recommended   80% thresholds to prevent any issues with this process as it uses clusters   resources to initiate and complete.</p> <p>You can scale the EBS Volumes attached to the data nodes by an arbitrary   amount such as 1GB, wait for the blue/green to complete and then scale it   back.</p> <ul> <li>Wait for a new service software release and update the service software of the   Cluster.</li> </ul> <p>This will also cause a blue/green and hence ES process will be restarted on   the nodes.</p>"}, {"location": "linux/elasticsearch/#recover-from-yellow-state", "title": "Recover from yellow state", "text": "<p>A yellow cluster represents that some of the replica shards in the cluster are unassigned. I can see that around 14 replica shards are unassigned.</p> <p>You can confirm the state of the cluster with the following commands</p> <pre><code>curl &lt;domain-endpoint&gt;_cluster/health?pretty\ncurl -X GET &lt;domain-endpoint&gt;/_cat/shards | grep UNASSIGNED\ncurl -X GET &lt;domain-endpoint&gt;/_cat/indices | grep yellow\n</code></pre> <p>If you have metrics of the JVMMemoryPressure of the nodes, check if the memory of a node reached 100% around the time the cluster reached yellow state.</p> <p>One can generally confirm the reason for a cluster going yellow by looking at the output of the following API call:</p> <pre><code>curl -X GET &lt;domain-endpoint&gt;/_cluster/allocation/explain | jq\n</code></pre> <p>If it shows a <code>CircuitBreakerException</code>, it confirms that a spike in the JVM metric caused the node to go down. Check the Fix Circuit breaker triggers section above to see how to solve that case.</p>"}, {"location": "linux/elasticsearch/#reallocate-unassigned-shards", "title": "Reallocate unassigned shards", "text": "<p>Elasticsearch makes 5 attempts to assign the shard but if it fails to be assigned after 5 attempts, the shards will remain unassigned. There is a solution to this issue in order to bring the cluster to green state.</p> <p>You can disable the replicas on the failing index and then enable replicas back.</p> <ul> <li>Disable Replica</li> </ul> <pre><code>curl -X PUT \"&lt;ES_endpoint&gt;/&lt;index_name&gt;/_settings\" -H 'Content-Type: application/json' -d'\n{\n    \"index\" : {\n        \"number_of_replicas\" : 0\n    }\n}'\n</code></pre> <ul> <li>Enable the Replica back:</li> </ul> <pre><code>curl -X PUT \"&lt;ES_endpoint&gt;/&lt;index_name&gt;/_settings\" -H 'Content-Type: application/json' -d'\n{\n    \"index\" : {\n        \"number_of_replicas\" : 1\n    }\n}'\n</code></pre> <p>Please note that it will take some time for the shards to be completely assigned and hence you might see intermittent cluster status as YELLOW.</p>"}, {"location": "linux/fail2ban/", "title": "Fail2ban", "text": ""}, {"location": "linux/fail2ban/#usage", "title": "Usage", "text": ""}, {"location": "linux/fail2ban/#unban-ip", "title": "Unban IP", "text": "<pre><code>fail2ban-client set {{ jail }} unbanip {{ ip }}\n</code></pre> <p>Where <code>jail</code> can be <code>ssh</code>.</p>"}, {"location": "linux/google_chrome/", "title": "Install Google Chrome", "text": "<p>Although I hate it, there are web pages that don't work on Firefox or Chromium. In those cases I install <code>google-chrome</code> and uninstall as soon as I don't need to use that service.</p>"}, {"location": "linux/google_chrome/#installation", "title": "Installation", "text": ""}, {"location": "linux/google_chrome/#debian", "title": "Debian", "text": "<ul> <li> <p>Import the GPG key, and use the following command.   <pre><code>sudo wget -O- https://dl.google.com/linux/linux_signing_key.pub | gpg --dearmor &gt; /usr/share/keyrings/google-chrome.gpg\n</code></pre></p> </li> <li> <p>Once the GPG import is complete, you will need to import the Google Chrome repository.</p> </li> </ul> <pre><code>echo 'deb [arch=amd64 signed-by=/usr/share/keyrings/google-chrome.gpg] http://dl.google.com/linux/chrome/deb/ stable main' | sudo tee /etc/apt/sources.list.d/google-chrome.list\n</code></pre> <ul> <li>Install the program:   <pre><code>apt-get update\napt-get install google-chrome-stable\n</code></pre></li> </ul>"}, {"location": "linux/haproxy/", "title": "HAProxy", "text": "<p>HAProxy is free, open source software that provides a high availability load balancer and proxy server for TCP and HTTP-based applications that spreads requests across multiple servers. It is written in C and has a reputation for being fast and efficient (in terms of processor and memory usage).</p>"}, {"location": "linux/haproxy/#use-haproxy-as-a-reverse-proxy", "title": "Use HAProxy as a reverse proxy", "text": "<p>reverse proxy is a type of proxy server that retrieves resources on behalf of a client from one or more servers. These resources are then returned to the client, appearing as if they originated from the server itself. Unlike a forward proxy, which is an intermediary for its associated clients to contact any server, a reverse proxy is an intermediary for its associated servers to be contacted by any client. In other words, a proxy is associated with the client(s), while a reverse proxy is associated with the server(s); a reverse proxy is usually an internal-facing proxy used as a 'front-end' to control and protect access to a server on a private network.</p> <p>It can be done at Web server level (Nginx, Apache, ...) or at load balancer level.</p> <p>This HAProxy post shows how to translate Apache's proxy pass directives to the HAProxy configuration.</p> <pre><code>    frontend ft_global\n     acl host_dom.com    req.hdr(Host) dom.com\n     acl path_mirror_foo path -m beg   /mirror/foo/\n     use_backend bk_myapp if host_dom.com path_mirror_foo\n    backend bk_myapp\n    [...]\n    # external URL                  =&gt; internal URL\n    # http://dom.com/mirror/foo/bar =&gt; http://bk.dom.com/bar\n     # ProxyPass /mirror/foo/ http://bk.dom.com/bar\n     http-request set-header Host bk.dom.com\n     reqirep  ^([^ :]*)\\ /mirror/foo/(.*)     \\1\\ /\\2\n     # ProxyPassReverse /mirror/foo/ http://bk.dom.com/bar\n     # Note: we turn the urls into absolute in the mean time\n     acl hdr_location res.hdr(Location) -m found\n     rspirep ^Location:\\ (https?://bk.dom.com(:[0-9]+)?)?(/.*) Location:\\ /mirror/foo3 if hdr_location\n     # ProxyPassReverseCookieDomain bk.dom.com dom.com\n     acl hdr_set_cookie_dom res.hdr(Set-cookie) -m sub Domain= bk.dom.com\n     rspirep ^(Set-Cookie:.*)\\ Domain=bk.dom.com(.*) \\1\\ Domain=dom.com\\2 if hdr_set_cookie_dom\n     # ProxyPassReverseCookieDomain / /mirror/foo/\n     acl hdr_set_cookie_path res.hdr(Set-cookie) -m sub Path=\n     rspirep ^(Set-Cookie:.*)\\ Path=(.*) \\1\\ Path=/mirror/foo2 if hdr_set_cookie_path\n</code></pre> <p>Other useful examples can be retrieved from drmalex07  or ferdinandosimonetti gists.</p>"}, {"location": "linux/haproxy/#references", "title": "References", "text": "<ul> <li>Guidelines for HAProxy termination in AWS</li> </ul>"}, {"location": "linux/hypothesis/", "title": "Hypothesis", "text": "<p>Hypothesis is an open-source software project that aims to collect comments about statements made in any web-accessible content, and filter and rank those comments to assess each statement's credibility.</p> <p>It offers an online web application where registered users share highlights and annotations over any webpage.</p> <p>As of 2020-06-11, although the service can be self-hosted, it's not yet easy to do so.</p>"}, {"location": "linux/hypothesis/#install", "title": "Install", "text": ""}, {"location": "linux/hypothesis/#client", "title": "Client", "text": "<p>If you're using Chrome or any derivative there is an official extension.</p> <p>Unfortunately if you use Firefox the extension is still being developed #310 although an unofficial release works just fine. Alternatively you can use the Hypothesis bookmarklet.</p> <p>The only problem is that both the extensions and the bookmarklet only works for the official service. In theory you can tweak the extension build process to use your custom settings. Though there is yet no documentation on this topic.</p> <p>I've thought of opening them a bug regarding this issue, but their github issues are only for bug reports, they use a google group to track the feature requests, I don't have an easy way to post there, so if you follow this path, please contact me.</p>"}, {"location": "linux/hypothesis/#server", "title": "Server", "text": "<p>The infrastructure can be deployed with Docker-compose.</p> <pre><code>version: '3'\nservices:\npostgres:\nimage: postgres:11.5-alpine\nports:\n- 5432\n# - '5432:5432'\nelasticsearch:\nimage: hypothesis/elasticsearch:latest\nports:\n- 9200\n#- '9200:9200'\nenvironment:\n- discovery.type=single-node\nrabbit:\nimage: rabbitmq:3.6-management-alpine\nports:\n- 5672\n- 15672\n#- '5672:5672'\n#- '15672:15672'\nweb:\nimage: hypothesis/hypothesis:latest\nenvironment:\n- APP_URL=http://localhost:5000\n- AUTHORITY=localhost\n- BROKER_URL=amqp://guest:guest@rabbit:5672//\n- CLIENT_OAUTH_ID\n- CLIENT_URL=http://localhost:3001/hypothesis\n- DATABASE_URL=postgresql://postgres@postgres/postgres\n- ELASTICSEARCH_URL=http://elasticsearch:9200\n- NEW_RELIC_APP_NAME=h (dev)\n- NEW_RELIC_LICENSE_KEY\n- SECRET_KEY=notasecret\nports:\n- '5000:5000'\ndepends_on:\n- postgres\n- elasticsearch\n- rabbit\n</code></pre> <pre><code>docker-compose up\n</code></pre> <p>Initialize the database and create the admin user.</p> <pre><code>docker-compose exec web /bin/sh\nhypothesis init\n\nhypothesis user add\nhypothesis user admin &lt;username&gt;\n</code></pre> <p>The service is available at http://localhost:5000.</p> <p>To check the latest developments of the Docker compose deployment follow the issue #4899.</p> <p>They also provide the tools they use to deploy the production service into AWS.</p>"}, {"location": "linux/hypothesis/#references", "title": "References", "text": "<ul> <li>Homepage</li> <li>FAQ</li> <li>Bug tracker</li> <li>Feature request tracker</li> </ul>"}, {"location": "linux/hypothesis/#server-deployment-open-issues", "title": "Server deployment open issues", "text": "<ul> <li>Self-hosting Docker compose</li> <li>Create admin user when using Docker     compose</li> <li>Steps required to run both h and serve the client from internal server</li> <li>How to deploy h on VM</li> </ul>"}, {"location": "linux/mkdocs/", "title": "Mkdocs", "text": "<p>MkDocs is a fast, simple and downright gorgeous static site generator that's geared towards building project documentation. Documentation source files are written in Markdown, and configured with a single YAML configuration file.</p> <p>Note: I've automated the creation of the mkdocs site in this cookiecutter template.</p>"}, {"location": "linux/mkdocs/#installation", "title": "Installation", "text": "<ul> <li>Install the basic packages.</li> </ul> <pre><code>pip install \\\nmkdocs \\\nmkdocs-material \\\nmkdocs-autolink-plugin \\\nmkdocs-minify-plugin \\\npymdown-extensions \\\nmkdocs-git-revision-date-localized-plugin\n</code></pre> <ul> <li>Create the <code>docs</code> repository.</li> </ul> <pre><code>mkdocs new docs\n</code></pre> <ul> <li>Although there are   several themes, I   usually use the material one. I   won't dive into the different options, just show a working template of the   <code>mkdocs.yaml</code> file.</li> </ul> <pre><code>site_name: {{site_name: null}: null}\nsite_author: {{your_name: null}: null}\nsite_url: {{site_url: null}: null}\nnav:\n- Introduction: index.md\n- Basic Usage: basic_usage.md\n- Configuration: configuration.md\n- Update: update.md\n- Advanced Usage:\n- Projects: projects.md\n- Tags: tags.md\n\nplugins:\n- search\n- autolinks\n- git-revision-date-localized:\ntype: timeago\n- minify:\nminify_html: true\n\nmarkdown_extensions:\n- admonition\n- meta\n- toc:\npermalink: true\nbaselevel: 2\n- pymdownx.arithmatex\n- pymdownx.betterem:\nsmart_enable: all\n- pymdownx.caret\n- pymdownx.critic\n- pymdownx.details\n- pymdownx.emoji:\nemoji_generator: !%21python/name:pymdownx.emoji.to_svg\n- pymdownx.inlinehilite\n- pymdownx.magiclink\n- pymdownx.mark\n- pymdownx.smartsymbols\n- pymdownx.superfences\n- pymdownx.tasklist:\ncustom_checkbox: true\n- pymdownx.tilde\n\ntheme:\nname: material\ncustom_dir: theme\nlogo: images/logo.png\npalette:\nprimary: blue grey\naccent: light blue\n\nextra_css:\n- stylesheets/extra.css\n- stylesheets/links.css\n\nrepo_name: {{repository_name: null}: null} # for example: 'lyz-code/pydo'\nrepo_url: {{repository_url: null}: null} # for example: 'https://github.com/lyz-code/pydo'\n</code></pre> <ul> <li> <p>Configure your logo   by saving it into <code>docs/images/logo.png</code>.</p> </li> <li> <p>I like to show a small image above each link so you know where is it pointing   to. To do so add the content of   this directory to   <code>theme</code>. and   these   files under <code>docs/stylesheets</code>.</p> </li> <li> <p>Initialize the git repository and create the first commit.</p> </li> <li> <p>Start the server to see everything is alright.</p> </li> </ul> <pre><code>mkdocs serve\n</code></pre>"}, {"location": "linux/mkdocs/#material-theme-customizations", "title": "Material theme customizations", "text": ""}, {"location": "linux/mkdocs/#color-palette-toggle", "title": "Color palette toggle", "text": "<p>Since 7.1.0, you can have a light-dark mode on the site using a toggle in the upper bar.</p> <p>To enable it add to your <code>mkdocs.yml</code>:</p> <pre><code>theme:\npalette:\n\n# Light mode\n- media: '(prefers-color-scheme: light)'\nscheme: default\nprimary: blue grey\naccent: light blue\ntoggle:\nicon: material/toggle-switch-off-outline\nname: Switch to dark mode\n\n# Dark mode\n- media: '(prefers-color-scheme: dark)'\nscheme: slate\nprimary: blue grey\naccent: light blue\ntoggle:\nicon: material/toggle-switch\nname: Switch to light mode\n</code></pre> <p>Changing your desired colors for each mode</p>"}, {"location": "linux/mkdocs/#back-to-top-button", "title": "Back to top button", "text": "<p>Since 7.1.0, a back-to-top button can be shown when the user, after scrolling down, starts to scroll up again. It's rendered in the lower right corner of the viewport. Add the following lines to mkdocs.yml:</p> <pre><code>theme:\nfeatures:\n- navigation.top\n</code></pre>"}, {"location": "linux/mkdocs/#add-a-github-pages-hook", "title": "Add a github pages hook.", "text": "<ul> <li>Save your <code>requirements.txt</code>.</li> </ul> <pre><code>pip freeze &gt; requirements.txt\n</code></pre> <ul> <li>Create the <code>.github/workflows/gh-pages.yml</code> file with the following contents.</li> </ul> <pre><code>name: Github pages\n\non:\npush:\nbranches:\n- master\n\njobs:\ndeploy:\nruns-on: ubuntu-18.04\nsteps:\n- uses: actions/checkout@v2\nwith:\n# Number of commits to fetch. 0 indicates all history.\n# Default: 1\nfetch-depth: 0\n\n- name: Setup Python\nuses: actions/setup-python@v1\nwith:\npython-version: '3.7'\narchitecture: x64\n\n- name: Cache dependencies\nuses: actions/cache@v1\nwith:\npath: ~/.cache/pip\nkey: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}\nrestore-keys: |\n${{ runner.os }}-pip-\n\n- name: Install dependencies\nrun: |\npython3 -m pip install --upgrade pip\npython3 -m pip install -r ./requirements.txt\n\n- run: |\ncd docs\nmkdocs build\n\n- name: Deploy\nuses: peaceiris/actions-gh-pages@v3\nwith:\ndeploy_key: ${{ secrets.ACTIONS_DEPLOY_KEY }}\npublish_dir: ./docs/site\n</code></pre> <ul> <li> <p>Create an   SSH deploy key</p> </li> <li> <p>Activate <code>GitHub Pages</code> repository configuration with <code>gh-pages branch</code>.</p> </li> <li> <p>Make a new commit and push to check it's working.</p> </li> </ul>"}, {"location": "linux/mkdocs/#create-mermaidjs-diagrams", "title": "Create MermaidJS diagrams", "text": "<p>Even though the Material theme supports mermaid diagrams it's only giving it for the paid users. The funding needs to reach 5000$ so it's released to the general public.</p> <p>The alternative is to use the mkdocs-mermaid2-plugin plugin, which can't be used with <code>mkdocs-minify-plugin</code> and doesn't adapt to dark mode.</p> <p>To install it:</p> <ul> <li> <p>Download the package: <code>pip install mkdocs-mermaid2-plugin</code>.</p> </li> <li> <p>Enable the plugin in <code>mkdocs.yml</code>.</p> </li> </ul> <pre><code>plugins:\n# Not compatible with mermaid2\n# - minify:\n#    minify_html: true\n- mermaid2:\narguments:\nsecurityLevel: loose\nmarkdown_extensions:\n- pymdownx.superfences:\n# make exceptions to highlighting of code:\ncustom_fences:\n- name: mermaid\nclass: mermaid\nformat: !%21python/name:mermaid2.fence_mermaid\n</code></pre> <p>Check the MermaidJS article to see how to create the diagrams.</p>"}, {"location": "linux/mkdocs/#plugin-development", "title": "Plugin development", "text": "<p>Like MkDocs, plugins must be written in Python. It is expected that each plugin would be distributed as a separate Python module. At a minimum, a MkDocs Plugin must consist of a BasePlugin subclass and an entry point which points to it.</p> <p>The BasePlugin class is meant to have <code>on_&lt;event_name&gt;</code> methods that run actions on the MkDocs defined events.</p> <p>The same object is called at the different events, so you can save objects from one event to the other in the object attributes.</p> <p>Keep in mind that the order of execution of the plugins follows the ordering of the list of the <code>mkdocs.yml</code> file where they are defined.</p>"}, {"location": "linux/mkdocs/#interesting-objects", "title": "Interesting objects", "text": ""}, {"location": "linux/mkdocs/#files", "title": "Files", "text": "<p><code>mkdocs.structure.files.Files</code> contains a list of File objects under the <code>._files</code> attribute and allows you to <code>append</code> files to the collection. As well as extracting the different file types:</p> <ul> <li><code>documentation_pages</code>: Iterable of markdown page file objects.</li> <li><code>static_pages</code>: Iterable of static page file objects.</li> <li><code>media_files</code>: Iterable of all files that are not documentation or static   pages.</li> <li><code>javascript_files</code>: Iterable of javascript files.</li> <li><code>css_files</code>: Iterable of css files.</li> </ul> <p>It is initialized with a list of <code>File</code> objects.</p>"}, {"location": "linux/mkdocs/#file", "title": "File", "text": "<p><code>mkdocs.structure.files.File</code> objects points to the source and destination locations of a file. It has the following interesting attributes:</p> <ul> <li><code>name</code>: Name of the file without the extension.</li> <li><code>src_path</code> or <code>abs_src_path</code>: Relative or absolute path to the original path,   for example the markdown file.</li> <li><code>dest_path</code> or <code>abs_dest_path</code>: Relative or absolute path to the destination   path, for example the html file generated from the markdown one.</li> <li><code>url</code>: Url where the file is going to be exposed.</li> </ul> <p>It is initialized with the arguments:</p> <ul> <li><code>path</code>: Must be a path that exists relative to <code>src_dir</code>.</li> <li><code>src_dir</code>: Absolute path on the local file system to the directory where the   docs are.</li> <li><code>dest_dir</code>: Absolute path on the local file system to the directory where the   site is going to be built.</li> <li><code>use_directory_urls</code>: If <code>False</code>, a Markdown file is mapped to an HTML file of   the same name (the file extension is changed to <code>.html</code>). If True, a Markdown   file is mapped to an HTML index file (<code>index.html</code>) nested in a directory   using the \"name\" of the file in <code>path</code>. The <code>use_directory_urls</code> argument has   no effect on non-Markdown files. By default MkDocs uses <code>True</code>.</li> </ul>"}, {"location": "linux/mkdocs/#navigation", "title": "Navigation", "text": "<p><code>mkdocs.structure.nav.Navigation</code> objects hold the information to build the navigation of the site. It has the following interesting attributes:</p> <ul> <li><code>items</code>: Nested List with full navigation of Sections, SectionPages, Pages,   and Links.</li> <li><code>pages</code>: Flat List of subset of Pages in nav, in order.</li> </ul> <p>The <code>Navigation</code> object has no <code>__eq__</code> method, so when testing, instead of trying to build a similar <code>Navigation</code> object and compare them, you need to assert that the contents of the object are what you expect.</p>"}, {"location": "linux/mkdocs/#page", "title": "Page", "text": "<p><code>mkdocs.structure.pages.Page</code> models each page of the site.</p> <p>To initialize it you need the <code>title</code>, the <code>File</code> object of the page, and the MkDocs <code>config</code> object.</p>"}, {"location": "linux/mkdocs/#section", "title": "Section", "text": "<p><code>mkdocs.structure.nav.Section</code> object models a section of the navigation of a MkDocs site.</p> <p>To initialize it you need the <code>title</code> of the section and the <code>children</code> which are the elements that belong to the section. If you don't yet know the children, pass an empty list <code>[]</code>.</p>"}, {"location": "linux/mkdocs/#sectionpage", "title": "SectionPage", "text": "<p><code>mkdocs_section_index.SectionPage</code> , part of the mkdocs-section-index plugin, models Section objects that have an associated Page, allowing you to have nav sections that when clicked, load the Page and not only opens the menu for the children elements.</p> <p>To initialize it you need the <code>title</code> of the section, the <code>File</code> object of the page, , the MkDocs <code>config</code> object, and the <code>children</code> which are the elements that belong to the section. If you don't yet know the children, pass an empty list <code>[]</code>.</p>"}, {"location": "linux/mkdocs/#events", "title": "Events", "text": ""}, {"location": "linux/mkdocs/#on_config", "title": "on_config", "text": "<p>The config event is the first event called on build and is run immediately after the user configuration is loaded and validated. Any alterations to the config should be made here.</p> <p>Parameters:</p> <ul> <li><code>config</code>: global configuration object</li> </ul> <p>Returns:</p> <ul> <li>global configuration object</li> </ul>"}, {"location": "linux/mkdocs/#on_files", "title": "on_files", "text": "<p>The <code>files</code> event is called after the files collection is populated from the <code>docs_dir</code>. Use this event to add, remove, or alter files in the collection. Note that Page objects have not yet been associated with the file objects in the collection. Use Page Events to manipulate page specific data.</p> <p>Parameters:</p> <ul> <li><code>files</code>: global files collection</li> <li><code>config</code>: global configuration object</li> </ul> <p>Returns:</p> <ul> <li>global files collection</li> </ul>"}, {"location": "linux/mkdocs/#on_nav", "title": "on_nav", "text": "<p>The <code>nav</code> event is called after the site navigation is created and can be used to alter the site navigation.</p> <p>Warning: Read the following section if you want to add new files.</p> <p>Parameters:</p> <ul> <li><code>nav</code>: global navigation object.</li> <li><code>config</code>: global configuration object.</li> <li><code>files</code>: global files collection.</li> </ul> <p>Returns:</p> <ul> <li>global navigation object</li> </ul>"}, {"location": "linux/mkdocs/#adding-new-files", "title": "Adding new files", "text": "<p>Note: \"TL;DR: Add them in the <code>on_config</code> event.\"</p> <p>To add new files to the repository you will need two phases:</p> <ul> <li>Create the markdown article pages.</li> <li>Add them to the navigation.</li> </ul> <p>My first idea as a MkDocs user, and newborn plugin developer was to add the navigation items to the <code>nav</code> key in the <code>config</code> object, as it's more easy to add items to a dictionary I'm used to work with than to dive into the code and understand how MkDocs creates the navigation. As I understood from the docs, the files should be created in the <code>on_files</code> event. the problem with this approach is that the only event that allows you to change the <code>config</code> is the <code>on_config</code> event, which is before the <code>on_files</code> one, so you can't build the navigation this way after you've created the files.</p> <p>Next idea was to add the items in the <code>on_nav</code> event, that means creating yourself the <code>Section</code>, <code>Pages</code>, <code>SectionPages</code> or <code>Link</code> objects and append them to the <code>nav.items</code>. The problem is that MkDocs initializes and processes the <code>Navigation</code> object in the <code>get_navigation</code> function. If you want to add items with a plugin in the <code>on_nav</code> event, you need to manually run all the post processing functions such as building the <code>pages</code> attribute, by running the <code>_get_by_type</code>, <code>_add_previous_and_next_links</code> or <code>_add_parent_links</code> yourself. Additionally, when building the site you'll get the <code>The following pages exist in the docs directory, but are not included in the \"nav\" configuration</code> error, because that check is done before all plugins change the navigation in the <code>on_nav</code> object.</p> <p>The last approach is to build the files and tweak the navigation in the <code>on_config</code> event. This approach has the next advantages:</p> <ul> <li>You need less knowledge of how MkDocs works.</li> <li>You don't need to create the <code>File</code> or <code>Files</code> objects.</li> <li>You don't need to create the <code>Page</code>, <code>Section</code>, <code>SectionPage</code> objects.</li> <li>More robust as you rely on existent MkDocs functionality.</li> </ul>"}, {"location": "linux/mkdocs/#testing", "title": "Testing", "text": "<p>I haven't found any official documentation on how to test MkDocs plugins, in the issues they suggest you look at how they test it in the search plugin. I've looked at other plugins such as mkdocs_blog and used the next way to test mkdocs-newsletter.</p> <p>I see the plugin definition as an entrypoint to the functionality of our program, that's why I feel the definition should be in <code>src/mkdocs_newsletter/entrypoints/mkdocs_plugin.py</code>. As any entrypoint, the best way to test them are in end-to-end tests.</p> <p>You need to have a working test site in <code>tests/assets/test_data</code>, with it's <code>mkdocs.yml</code> file that loads your plugin and some fake articles.</p> <p>To prepare the test we can define the next fixture that prepares the building of the site:</p> <p>File: <code>tests/conftest.py</code>:</p> <pre><code>import os\nimport shutil\n\nfrom mkdocs import config\nfrom mkdocs.config.base import Config\n\n\n@pytest.fixture(name=\"config\")\ndef config_(tmp_path: Path) -&gt; Config:\n\"\"\"Load the mkdocs configuration.\"\"\"\n    repo_path = tmp_path / \"test_data\"\n    shutil.copytree(\"tests/assets/test_data\", repo_path)\n    mkdocs_config = config.load_config(os.path.join(repo_path, \"mkdocs.yml\"))\n    mkdocs_config[\"site_dir\"] = os.path.join(repo_path, \"site\")\n    return mkdocs_config\n</code></pre> <p>It does the next steps:</p> <ul> <li>Copy the fake MkDocs site to a temporal directory</li> <li>Prepare the MkDocs <code>Config</code> object to build the site.</li> </ul> <p>Now we can use it in the e2e tests:</p> <p>File: <code>tests/e2e/test_plugin.py</code>:</p> <pre><code>def test_plugin_builds_newsletters(full_repo: Repo, config: Config) -&gt; None:\n    build.build(config)  # act\n\n    newsletter_path = f\"{full_repo.working_dir}/site/newsletter/2021_02/index.html\"\n    with open(newsletter_path, \"r\") as newsletter_file:\n        newsletter = newsletter_file.read()\n    assert \"&lt;title&gt;February of 2021 - The Blue Book&lt;/title&gt;\" in newsletter\n</code></pre> <p>That test is meant to ensure that our plugin works with the MkDocs ecosystem, so the assertions should be done against the created html files.</p> <p>If your functionality can't be covered by the happy path of the end-to-end test, it's better to create unit tests to make sure that they work as you want.</p> <p>You can see a full example here.</p>"}, {"location": "linux/mkdocs/#issues", "title": "Issues", "text": "<p>Once they are closed:</p> <ul> <li>Mkdocs Deprecation warning,   once it's solved remove the warning filter on mkdocs-newsletter   <code>pyproject.toml</code>.</li> <li>Mkdocs-Material Deprecation warning,   once it's solved remove the warning filter on mkdocs-newsletter   <code>pyproject.toml</code>.</li> </ul>"}, {"location": "linux/mkdocs/#references", "title": "References", "text": "<ul> <li>Git</li> <li>Homepage.</li> <li>Material theme configuration guide</li> </ul>"}, {"location": "linux/mkdocs/#plugin-development_1", "title": "Plugin development", "text": "<ul> <li>User guide</li> <li>List of events</li> <li>Plugin testing example</li> </ul>"}, {"location": "linux/monica/", "title": "Monica", "text": "<p>Monica is an open-source web application to organize the interactions with your loved ones. They call it a PRM, or Personal Relationship Management. Think of it as a CRM (a popular tool used by sales teams in the corporate world) for your friends or family.</p> <p>Monica allows people to keep track of everything that's important about their friends and family. Like the activities done with them. When you last called someone. What you talked about. It will help you remember the name and the age of the kids. It can also remind you to call someone you haven't talked to in a while.</p> <p>They have pricing plans for their hosted service, but the self-hosted solution has all the features.</p> <p>It also has a nice API to interact with.</p>"}, {"location": "linux/monica/#install", "title": "Install", "text": "<p>They provide a very throughout documented Docker installation.</p> <p>If you just want to test it, use this docker compose</p> <p>File: docker-compose.yml</p> <pre><code>version: \"3.4\"\n\nservices:\n  app:\n    image: monicahq/monicahq\n    depends_on:\n      - db\n    ports:\n      - 8080:80\n    environment:\n      # generate with `pwgen -s 32 1` for instance:\n      - APP_KEY=DoKMvhGu795QcMBP1I5sw8uk85MMAPS9\n      - DB_HOST=db\n    volumes:\n      - data:/var/www/monica/storage\n    restart: always\n\n  db:\n    image: mysql:5.7\n    environment:\n      - MYSQL_RANDOM_ROOT_PASSWORD=true\n      - MYSQL_DATABASE=monica\n      - MYSQL_USER=homestead\n      - MYSQL_PASSWORD=secret\n    volumes:\n      - mysql:/var/lib/mysql\n    restart: always\n\nvolumes:\n  data:\n    name: data\n  mysql:\n    name: mysql\n</code></pre> <p>Once you install your own, you may want to:</p> <ul> <li>Change the <code>APP_KEY</code></li> <li>Change the database credentials. In the application docker are loaded as     <code>DB_USERNAME</code>, <code>DB_HOST</code> and <code>DB_PASSWORD</code>.</li> <li>Set up the environment and the application url with <code>APP_ENV=production</code> and     <code>APP_URL</code>.</li> <li> <p>Set up the email configuration</p> <pre><code>MAIL_MAILER: smtp\nMAIL_HOST: smtp.service.com # ex: smtp.sendgrid.net\nMAIL_PORT: 587 # is using tls, as you should\nMAIL_USERNAME: my_service_username # ex: apikey\nMAIL_PASSWORD: my_service_password # ex: SG.Psuoc6NZTrGHAF9fdsgsdgsbvjQ.JuxNWVYmJ8LE0\nMAIL_ENCRYPTION: tls\nMAIL_FROM_ADDRESS: no-reply@xxx.com # ex: email you want the email to be FROM\nMAIL_FROM_NAME: Monica # ex: name of the sender\n</code></pre> </li> </ul> <p>Here is an example of all the possible configurations.</p> <p>They also share other configuration examples where you can take ideas of alternate setups.</p> <p>If you don't want to use docker, check the other installation documentation.</p>"}, {"location": "linux/monica/#references", "title": "References", "text": "<ul> <li>Homepage</li> <li>Git</li> <li>Docs</li> <li>Blog</li> </ul>"}, {"location": "linux/nodejs/", "title": "Nodejs", "text": "<p>Node.js is a JavaScript runtime built on Chrome's V8 JavaScript engine.</p>"}, {"location": "linux/nodejs/#install", "title": "Install", "text": "<p>The debian base repositories are really outdated, so add the NodeSource repository</p> <pre><code>curl -fsSL https://deb.nodesource.com/setup_16.x | bash -\napt-get install -y nodejs npm\nnodejs --version\n</code></pre>"}, {"location": "linux/nodejs/#links", "title": "Links", "text": "<ul> <li>Home</li> </ul>"}, {"location": "linux/rm/", "title": "rm", "text": "<p>rm definition</p> <p>In computing, rm (short for remove) is a basic command on Unix and Unix-like operating systems used to remove objects such as computer files, directories and symbolic links from file systems and also special files such as device nodes, pipes and sockets</p>"}, {"location": "linux/rm/#debugging", "title": "Debugging", "text": ""}, {"location": "linux/rm/#cannot-remove-file-structure-needs-cleaning", "title": "Cannot remove file: \u201cStructure needs cleaning\u201d", "text": "<p>From Victoria Stuart and Depressed Daniel answer</p> <p>You first need to:</p> <ul> <li>Umount the partition.</li> <li>Do a sector level backup of your disk.</li> <li> <p>If your filesystem is ext4 run:   <pre><code>fsck.ext4 {{ device }}\n</code></pre>   Accept all suggested fixes.</p> </li> <li> <p>Mount again the partition.</p> </li> </ul>"}, {"location": "linux/syncthing/", "title": "Syncthing", "text": "<p>Syncthing is a continuous file synchronization program. It synchronizes files between two or more computers in real time, safely protected from prying eyes. Your data is your data alone and you deserve to choose where it is stored, whether it is shared with some third party, and how it's transmitted over the internet.</p>"}, {"location": "linux/syncthing/#installation", "title": "Installation", "text": ""}, {"location": "linux/syncthing/#debian-or-ubuntu", "title": "Debian or Ubuntu", "text": "<pre><code># Add the release PGP keys:\ncurl -s https://syncthing.net/release-key.txt | sudo apt-key add -\n\n# Add the \"stable\" channel to your APT sources:\necho \"deb https://apt.syncthing.net/ syncthing stable\" | sudo tee /etc/apt/sources.list.d/syncthing.list\n\n# Update and install syncthing:\nsudo apt-get update\nsudo apt-get install syncthing\n</code></pre>"}, {"location": "linux/syncthing/#docker", "title": "Docker", "text": "<p>Use Linuxserver Docker</p>"}, {"location": "linux/syncthing/#configuration", "title": "Configuration", "text": "<p>If you're only going to use syncthing in an internal network, or you're going to fix the IPs of the devices you can disable the Global Discovery and Relaying connections so that you don't leak the existence of your services to the syncthing servers.</p>"}, {"location": "linux/syncthing/#troubleshooting", "title": "Troubleshooting", "text": ""}, {"location": "linux/syncthing/#syncthing-over-tor", "title": "Syncthing over Tor", "text": "<p>There are many posts on this topic (1, 2) but I wasn't able to connect two clients through Tor. Here are the steps I took in case anyone is interested. If you make it work, please contact me.</p> <p>Suggest to use a relay, go to relays.syncthing.net to see the public ones. You need to add the required servers to the <code>Sync Protocol Listen Address</code> field, under <code>Actions</code> and <code>Settings</code>. The syntax is:</p> <pre><code>relay://&lt;host name|IP&gt;[:port]/?id=&lt;relay device ID&gt;\n</code></pre> <p>The only way I've found to get the <code>relay device ID</code> is setting a fake one, and getting the correct one from the logs of syncthing. It will say that <code>the fingerprint ( what you put ) doesn't match ( actual fingerprint )</code>.</p>"}, {"location": "linux/syncthing/#steps", "title": "Steps", "text": "<ul> <li> <p>Configure the client:</p> <pre><code>export all_proxy=socks5://127.0.0.1:9058\nexport ALL_PROXY_NO_FALLBACK=1\nsyncthing --home /tmp/syncthing_1\n</code></pre> </li> <li> <p>Allow the connection to the local server:</p> <pre><code>sudo iptables -I OUTPUT -o lo -p tcp --dport 8384 -j ACCEPT\n</code></pre> </li> <li> <p>If you're using Tails and Tor Browser, you'll need to set the <code>about:config</code>     setting <code>network.proxy.allow_hijacking_localhost</code> to <code>false</code>. Otherwise you     won't be able to access the user interface.</p> </li> </ul>"}, {"location": "linux/syncthing/#issues", "title": "Issues", "text": "<ul> <li>Wifi run condition needs location to be turned     on: update and     check that you no longer need it.</li> </ul>"}, {"location": "linux/syncthing/#links", "title": "Links", "text": "<ul> <li>Home</li> <li>Getting Started</li> </ul>"}, {"location": "linux/wireguard/", "title": "Wireguard", "text": "<p>Wireguard is an simple yet fast and modern VPN that utilizes state-of-the-art cryptography. It aims to be faster, simpler, leaner, and more useful than IPsec, while avoiding the massive headache. It intends to be considerably more performant than OpenVPN. WireGuard is a general purpose VPN for running on embedded interfaces and super computers alike. Initially released for the Linux kernel, it's now cross-platform (Windows, macOS, BSD, iOS, Android) and widely deployable. Although it's under heavy development, it already might be the most secure, easiest to use, and simplest VPN solution in the industry.</p> <p>Features:</p> <ul> <li> <p>Simple and easy to use: WireGuard aims to be as easy to configure and deploy     as SSH. A VPN connection is made by exchanging public keys \u2013 exactly like     exchanging SSH keys \u2013 and all the rest is transparently handled by     WireGuard. It's even capable of roaming between IP addresses, like     Mosh. There is no need to manage connections, worry about state,     manage daemons, or worry about what's under the hood. WireGuard presents a     basic yet powerful interface.</p> </li> <li> <p>Cryptographically Sound: WireGuard uses state-of-the-art cryptography, such as     the Noise protocol framework, Curve25519, ChaCha20, Poly1305, BLAKE2,     SipHash24, HKDF, and secure trusted constructions. It makes conservative and     reasonable choices and has been reviewed by cryptographers.</p> </li> <li> <p>Minimal Attack Surface: WireGuard is designed with ease-of-implementation and     simplicity in mind. It's meant to be implemented in very few lines of code,     and auditable for security vulnerabilities. Compared to behemoths like     *Swan/IPsec or OpenVPN/OpenSSL, in which auditing the gigantic codebases is     an overwhelming task even for large teams of security experts, WireGuard is     meant to be comprehensively reviewable by single individuals.</p> </li> <li> <p>High Performance: A combination of extremely high-speed cryptographic     primitives and the fact that WireGuard lives inside the Linux kernel means     that secure networking can be very high-speed. It is suitable for both small     embedded devices like smartphones and fully loaded backbone routers.</p> </li> <li> <p>Well Defined &amp; Thoroughly Considered: WireGuard is the result of a lengthy and     thoroughly considered academic process, resulting in the technical     whitepaper, an academic research paper which clearly defines the protocol     and the intense considerations that went into each decision.</p> </li> </ul> <p>Plus it's created by the same guy as <code>pass</code>, which uses Gentoo, I like this guy.</p>"}, {"location": "linux/wireguard/#conceptual-overview", "title": "Conceptual Overview", "text": "<p>WireGuard securely encapsulates IP packets over UDP. You add a WireGuard interface, configure it with your private key and your peers' public keys, and then you send packets across it. All issues of key distribution and pushed configurations are out of scope of WireGuard; these are issues much better left for other layers. It mimics the model of SSH and Mosh; both parties have each other's public keys, and then they're simply able to begin exchanging packets through the interface.</p>"}, {"location": "linux/wireguard/#simple-network-interface", "title": "Simple Network Interface", "text": "<p>WireGuard works by adding network interfaces, called wg0 (or wg1, wg2, wg3, etc). This network interface can then be configured normally using the ordinary networking utilities. The specific WireGuard aspects of the interface are configured using the <code>wg</code> tool. This interface acts as a tunnel interface.</p> <p>WireGuard associates tunnel IP addresses with public keys and remote endpoints. When the interface sends a packet to a peer, it does the following:</p> <ul> <li>This packet is meant for 192.168.30.8. Which peer is that? Let me look...     Okay, it's for peer ABCDEFGH. (Or if it's not for any configured peer, drop     the packet.)</li> <li>Encrypt entire IP packet using peer ABCDEFGH's public key.</li> <li>What is the remote endpoint of peer ABCDEFGH? Let me look... Okay, the     endpoint is UDP port 53133 on host 216.58.211.110.</li> <li>Send encrypted bytes from step 2 over the Internet to 216.58.211.110:53133     using UDP.</li> </ul> <p>When the interface receives a packet, this happens:</p> <ul> <li>I just got a packet from UDP port 7361 on host 98.139.183.24. Let's decrypt     it!</li> <li>It decrypted and authenticated properly for peer LMNOPQRS. Okay, let's     remember that peer LMNOPQRS's most recent Internet endpoint is     98.139.183.24:7361 using UDP.</li> <li>Once decrypted, the plain-text packet is from 192.168.43.89. Is peer LMNOPQRS     allowed to be sending us packets as 192.168.43.89?</li> <li>If so, accept the packet on the interface. If not, drop it.</li> </ul> <p>Behind the scenes there is much happening to provide proper privacy, authenticity, and perfect forward secrecy, using state-of-the-art cryptography.</p>"}, {"location": "linux/wireguard/#cryptokey-routing", "title": "Cryptokey Routing", "text": "<p>At the heart of WireGuard is a concept called Cryptokey Routing, which works by associating public keys with a list of tunnel IP addresses that are allowed inside the tunnel. Each network interface has a private key and a list of peers. Each peer has a public key.</p> <p>For example, a server computer might have this configuration:</p> <pre><code>[Interface]\nPrivateKey = yAnz5TF+lXXJte14tji3zlMNq+hd2rYUIgJBgB3fBmk=\nListenPort = 51820\n\n[Peer]\nPublicKey = xTIBA5rboUvnH4htodjb6e697QjLERt1NAB4mZqp8Dg=\nAllowedIPs = 10.192.122.3/32, 10.192.124.1/24\n\n[Peer]\nPublicKey = TrMvSoP4jYQlY6RIzBgbssQqY3vxI2Pi+y71lOWWXX0=\nAllowedIPs = 10.192.122.4/32, 192.168.0.0/16\n\n[Peer]\nPublicKey = gN65BkIKy1eCE9pP1wdc8ROUtkHLF2PfAqYdyYBz6EA=\nAllowedIPs = 10.10.10.230/32\n</code></pre> <p>And a client computer might have this simpler configuration:</p> <pre><code>[Interface]\nPrivateKey = gI6EdUSYvn8ugXOt8QQD6Yc+JyiZxIhp3GInSWRfWGE=\nListenPort = 21841\n\n[Peer]\nPublicKey = HIgo9xNzJMWLKASShiTqIybxZ0U3wGLiUeJ1PKf8ykw=\nEndpoint = 192.95.5.69:51820\nAllowedIPs = 0.0.0.0/0\n</code></pre> <p>In the server configuration, each peer (a client) will be able to send packets to the network interface with a source IP matching his corresponding list of allowed IPs. For example, when a packet is received by the server from peer gN65BkIK..., after being decrypted and authenticated, if its source IP is 10.10.10.230, then it's allowed onto the interface; otherwise it's dropped.</p> <p>In the server configuration, when the network interface wants to send a packet to a peer (a client), it looks at that packet's destination IP and compares it to each peer's list of allowed IPs to see which peer to send it to. For example, if the network interface is asked to send a packet with a destination IP of 10.10.10.230, it will encrypt it using the public key of peer gN65BkIK..., and then send it to that peer's most recent Internet endpoint.</p> <p>In the client configuration, its single peer (the server) will be able to send packets to the network interface with any source IP (since 0.0.0.0/0 is a wildcard). For example, when a packet is received from peer HIgo9xNz..., if it decrypts and authenticates correctly, with any source IP, then it's allowed onto the interface; otherwise it's dropped.</p> <p>In the client configuration, when the network interface wants to send a packet to its single peer (the server), it will encrypt packets for the single peer with any destination IP address (since 0.0.0.0/0 is a wildcard). For example, if the network interface is asked to send a packet with any destination IP, it will encrypt it using the public key of the single peer HIgo9xNz..., and then send it to the single peer's most recent Internet endpoint.</p> <p>In other words, when sending packets, the list of allowed IPs behaves as a sort of routing table, and when receiving packets, the list of allowed IPs behaves as a sort of access control list.</p> <p>This is what we call a Cryptokey Routing Table: the simple association of public keys and allowed IPs.</p> <p>Because all packets sent on the WireGuard interface are encrypted and authenticated, and because there is such a tight coupling between the identity of a peer and the allowed IP address of a peer, system administrators do not need complicated firewall extensions, such as in the case of IPsec, but rather they can simply match on \"is it from this IP? on this interface?\", and be assured that it is a secure and authentic packet. This greatly simplifies network management and access control, and provides a great deal more assurance that your iptables rules are actually doing what you intended for them to do.</p>"}, {"location": "linux/wireguard/#built-in-roaming", "title": "Built-in Roaming", "text": "<p>The client configuration contains an initial endpoint of its single peer (the server), so that it knows where to send encrypted data before it has received encrypted data. The server configuration doesn't have any initial endpoints of its peers (the clients). This is because the server discovers the endpoint of its peers by examining from where correctly authenticated data originates. If the server itself changes its own endpoint, and sends data to the clients, the clients will discover the new server endpoint and update the configuration just the same. Both client and server send encrypted data to the most recent IP endpoint for which they authentically decrypted data. Thus, there is full IP roaming on both ends.</p>"}, {"location": "linux/zfs/", "title": "ZFS", "text": "<p>OpenZFS is a file system with volume management capabilities designed specifically for storage servers.</p> <p>Some neat features of ZFS include:</p> <ul> <li>Aggregating multiple physical disks into a single filesystem.</li> <li>Automatically repairing data corruption.</li> <li>Creating point-in-time snapshots of data on disk.</li> <li>Optionally encrypting or compressing data on disk.</li> </ul>"}, {"location": "linux/zfs/#usage", "title": "Usage", "text": ""}, {"location": "linux/zfs/#mount-a-pool-as-readonly", "title": "Mount a pool as readonly", "text": "<pre><code>zpool import -o readonly=on {{ pool_name }}\n</code></pre>"}, {"location": "linux/zfs/#mount-a-zfs-snapshot-in-a-directory-as-readonly", "title": "Mount a ZFS snapshot in a directory as readonly", "text": "<pre><code>mount -t zfs {{ pool_name }}/{{ snapshot_name }} {{ mount_path }} -o ro\n</code></pre>"}, {"location": "linux/zfs/#list-pools", "title": "List pools", "text": "<pre><code>zpool list\n</code></pre>"}, {"location": "linux/zfs/#list-the-filesystems", "title": "List the filesystems", "text": "<pre><code>zfs list\n</code></pre>"}, {"location": "linux/zfs/#get-read-and-write-stats-from-pool", "title": "Get read and write stats from pool", "text": "<pre><code>zpool iostat -v {{ pool_name }} {{ refresh_time_in_seconds }}\n</code></pre>"}, {"location": "linux/zfs/#get-all-properties-of-a-pool", "title": "Get all properties of a pool", "text": "<pre><code>zpool get all {{ pool_name }}\n</code></pre>"}, {"location": "linux/zfs/#get-all-properties-of-a-filesystem", "title": "Get all properties of a filesystem", "text": "<pre><code>zfs get all {{ pool_name }}\n</code></pre>"}, {"location": "linux/zfs/#get-compress-ratio-of-a-filesystem", "title": "Get compress ratio of a filesystem", "text": "<pre><code>zfs get compressratio {{ filesystem }}\n</code></pre>"}, {"location": "linux/zfs/#rename-or-move-a-dataset", "title": "Rename or move a dataset", "text": "<p>NOTE: if you want to rename the topmost dataset look at rename the topmost dataset instead. File systems can be renamed by using the <code>zfs rename</code> command. You can perform the following operations:</p> <ul> <li>Change the name of a file system.</li> <li>Relocate the file system within the ZFS hierarchy.</li> <li>Change the name of a file system and relocate it within the ZFS hierarchy.</li> </ul> <p>The following example uses the <code>rename</code> subcommand to rename of a file system from <code>kustarz</code> to <code>kustarz_old</code>:</p> <pre><code>zfs rename tank/home/kustarz tank/home/kustarz_old\n</code></pre> <p>The following example shows how to use zfs <code>rename</code> to relocate a file system:</p> <pre><code>zfs rename tank/home/maybee tank/ws/maybee\n</code></pre> <p>In this example, the <code>maybee</code> file system is relocated from <code>tank/home</code> to <code>tank/ws</code>. When you relocate a file system through rename, the new location must be within the same pool and it must have enough disk space to hold this new file system. If the new location does not have enough disk space, possibly because it has reached its quota, rename operation fails.</p> <p>The rename operation attempts an unmount/remount sequence for the file system and any descendent file systems. The rename command fails if the operation is unable to unmount an active file system. If this problem occurs, you must forcibly unmount the file system.</p> <p>You'll loose the snapshots though, as explained below.</p>"}, {"location": "linux/zfs/#rename-the-topmost-dataset", "title": "Rename the topmost dataset", "text": "<p>If you want to rename the topmost dataset you need to rename the pool too as these two are tied. </p> <pre><code>$: zpool status -v\n\n  pool: tets\n state: ONLINE\n scrub: none requested\nconfig:\n\n        NAME        STATE     READ WRITE CKSUM\n        tets        ONLINE       0     0     0\nc0d1      ONLINE       0     0     0\nc1d0      ONLINE       0     0     0\nc1d1      ONLINE       0     0     0\n\nerrors: No known data errors\n</code></pre> <p>To fix this, first export the pool:</p> <pre><code>$ zpool export tets\n</code></pre> <p>And then imported it with the correct name:</p> <pre><code>$ zpool import tets test\n</code></pre> <p>After the import completed, the pool contains the correct name:</p> <pre><code>$ zpool status -v\n\n  pool: test\nstate: ONLINE\n scrub: none requested\nconfig:\n\n        NAME        STATE     READ WRITE CKSUM\n        test        ONLINE       0     0     0\nc0d1      ONLINE       0     0     0\nc1d0      ONLINE       0     0     0\nc1d1      ONLINE       0     0     0\n\nerrors: No known data errors\n</code></pre> <p>Now you may need to fix the ZFS mountpoints for each dataset</p> <pre><code>zfs set mountpoint=\"/opt/zones/[Newmountpoint]\" [ZFSPOOL/[ROOTor other filesystem]\n</code></pre>"}, {"location": "linux/zfs/#rename-or-move-snapshots", "title": "Rename or move snapshots", "text": "<p>If the dataset has snapshots you need to rename them too. They must be renamed within the same pool and dataset from which they were created though. For example:</p> <pre><code>zfs rename tank/home/cindys@083006 tank/home/cindys@today\n</code></pre> <p>In addition, the following shortcut syntax is equivalent to the preceding syntax:</p> <pre><code>zfs rename tank/home/cindys@083006 today\n</code></pre> <p>The following snapshot rename operation is not supported because the target pool and file system name are different from the pool and file system where the snapshot was created:</p> <pre><code>$: zfs rename tank/home/cindys@today pool/home/cindys@saturday\ncannot rename to 'pool/home/cindys@today': snapshots must be part of same \ndataset\n</code></pre> <p>You can recursively rename snapshots by using the <code>zfs rename -r</code> command. For example:</p> <pre><code>$: zfs list\nNAME                         USED  AVAIL  REFER  MOUNTPOINT\nusers                        270K  16.5G    22K  /users\nusers/home                    76K  16.5G    22K  /users/home\nusers/home@yesterday            0      -    22K  -\nusers/home/markm              18K  16.5G    18K  /users/home/markm\nusers/home/markm@yesterday      0      -    18K  -\nusers/home/marks              18K  16.5G    18K  /users/home/marks\nusers/home/marks@yesterday      0      -    18K  -\nusers/home/neil               18K  16.5G    18K  /users/home/neil\nusers/home/neil@yesterday       0      -    18K  -\n$: zfs rename -r users/home@yesterday @2daysago\n$: zfs list -r users/home\nNAME                        USED  AVAIL  REFER  MOUNTPOINT\nusers/home                   76K  16.5G    22K  /users/home\nusers/home@2daysago            0      -    22K  -\nusers/home/markm             18K  16.5G    18K  /users/home/markm\nusers/home/markm@2daysago      0      -    18K  -\nusers/home/marks             18K  16.5G    18K  /users/home/marks\nusers/home/marks@2daysago      0      -    18K  -\nusers/home/neil              18K  16.5G    18K  /users/home/neil\nusers/home/neil@2daysago       0      -    18K  -\n</code></pre>"}, {"location": "linux/zfs/#repair-a-degraded-pool", "title": "Repair a DEGRADED pool", "text": "<p>First let\u2019s offline the device we are going to replace: </p> <pre><code>zpool offline tank0 ata-WDC_WD2003FZEX-00SRLA0_WD-xxxxxxxxxxxx\n</code></pre> <p>Now let us have a look at the pool status.</p> <pre><code>zpool status\n\nNAME                                            STATE     READ WRITE CKSUM\ntank0                                           DEGRADED     0     0     0\nraidz2-1                                      DEGRADED     0     0     0\nata-TOSHIBA_HDWN180_xxxxxxxxxxxx            ONLINE       0     0     0\nata-TOSHIBA_HDWN180_xxxxxxxxxxxx            ONLINE       0     0     0\nata-TOSHIBA_HDWN180_xxxxxxxxxxxx            ONLINE       0     0     0\nata-WDC_WD80EFZX-68UW8N0_xxxxxxxx           ONLINE       0     0     0\nata-TOSHIBA_HDWG180_xxxxxxxxxxxx            ONLINE       0     0     0\nata-TOSHIBA_HDWG180_xxxxxxxxxxxx            ONLINE       0     0     0\nata-WDC_WD2003FZEX-00SRLA0_WD-xxxxxxxxxxxx  OFFLINE      0     0     0\nata-ST4000VX007-2DT166_xxxxxxxx             ONLINE       0     0     0\n</code></pre> <p>Sweet, the device is offline (last time it didn't show as offline for me, but the offline command returned a status code of 0). </p> <p>Time to shut the server down and physically replace the disk.</p> <pre><code>shutdown -h now\n</code></pre> <p>When you start again the server, it\u2019s time to instruct ZFS to replace the removed device with the disk we just installed.</p> <pre><code>zpool replace tank0 \\\nata-WDC_WD2003FZEX-00SRLA0_WD-xxxxxxxxxxxx \\\n/dev/disk/by-id/ata-TOSHIBA_HDWG180_xxxxxxxxxxxx\n</code></pre> <pre><code>zpool status tank0\n\npool: main\nstate: DEGRADED\nstatus: One or more devices is currently being resilvered.  The pool will\n        continue to function, possibly in a degraded state.\naction: Wait for the resilver to complete.\n  scan: resilver in progress since Fri Sep 22 12:40:28 2023\n4.00T scanned at 6.85G/s, 222G issued at 380M/s, 24.3T total\n        54.7G resilvered, 0.89% done, 18:28:03 to go\nNAME                                              STATE     READ WRITE CKSUM\ntank0                                             DEGRADED     0     0     0\nraidz2-1                                        DEGRADED     0     0     0\nata-TOSHIBA_HDWN180_xxxxxxxxxxxx              ONLINE       0     0     0\nata-TOSHIBA_HDWN180_xxxxxxxxxxxx              ONLINE       0     0     0\nata-TOSHIBA_HDWN180_xxxxxxxxxxxx              ONLINE       0     0     0\nata-WDC_WD80EFZX-68UW8N0_xxxxxxxx             ONLINE       0     0     0\nata-TOSHIBA_HDWG180_xxxxxxxxxxxx              ONLINE       0     0     0\nata-TOSHIBA_HDWG180_xxxxxxxxxxxx              ONLINE       0     0     0\nreplacing-6                                   DEGRADED     0     0     0\nata-WDC_WD2003FZEX-00SRLA0_WD-xxxxxxxxxxxx  OFFLINE      0     0     0\nata-TOSHIBA_HDWG180_xxxxxxxxxxxx            ONLINE       0     0     0  (resilvering)\nata-ST4000VX007-2DT166_xxxxxxxx               ONLINE       0     0     0\n</code></pre> <p>The disk is replaced and getting resilvered (which may take a long time to run (18 hours in a 8TB disk in my case).</p> <p>Once the resilvering is done; this is what the pool looks like.</p> <pre><code>zpool list\n\nNAME      SIZE  ALLOC   FREE  EXPANDSZ   FRAG    CAP  DEDUP  HEALTH  ALTROOT\ntank0    43.5T  33.0T  10.5T     14.5T     7%    75%  1.00x  ONLINE  -\n</code></pre> <p>If you want to read other blogs that have covered the same topic check out 1.</p>"}, {"location": "linux/zfs/#installation", "title": "Installation", "text": ""}, {"location": "linux/zfs/#install-the-required-programs", "title": "Install the required programs", "text": "<p>OpenZFS is not in the mainline kernel for license issues (fucking capitalism...) so it's not yet suggested to use it for the root of your filesystem. </p> <p>To install it in a Debian device:</p> <ul> <li>ZFS packages are included in the <code>contrib</code> repository, but the <code>backports</code> repository often provides newer releases of ZFS. You can use it as follows.</li> </ul> <p>Add the backports repository:</p> <pre><code>vi /etc/apt/sources.list.d/bullseye-backports.list\n</code></pre> <pre><code>deb http://deb.debian.org/debian bullseye-backports main contrib\ndeb-src http://deb.debian.org/debian bullseye-backports main contrib\n</code></pre> <pre><code>vi /etc/apt/preferences.d/90_zfs\n</code></pre> <pre><code>Package: src:zfs-linux\nPin: release n=bullseye-backports\nPin-Priority: 990\n</code></pre> <ul> <li>Install the packages:</li> </ul> <pre><code>apt update\napt install dpkg-dev linux-headers-generic linux-image-generic\napt install zfs-dkms zfsutils-linux\n</code></pre> <p>BE CAREFUL: if root doesn't have <code>sbin</code> in the <code>PATH</code> you will get an error of loading the zfs module as it's not signed. If this happens to you reinstall or try the debugging I did (which didn't work).</p>"}, {"location": "linux/zfs/#create-your-pool", "title": "Create your pool", "text": "<p>First read the ZFS storage planning article and then create your <code>main</code> pool with a command similar to:</p> <pre><code>zpool create \\\n-o ashift=12 \\ \n-o autoexpand=on \\ \nmain raidz /dev/sda /dev/sdb /dev/sdc /dev/sdd \\\nlog mirror \\\n/dev/disk/by-id/nvme-eui.e823gqkwadgp32uhtpobsodkjfl2k9d0-part4 \\\n/dev/disk/by-id/nvme-eui.a0sdgohosdfjlkgjwoqkegdkjfl2k9d0-part4 \\\ncache \\\n/dev/disk/by-id/nvme-eui.e823gqkwadgp32uhtpobsodkjfl2k9d0-part5 \\\n/dev/disk/by-id/nvme-eui.a0sdgohosdfjlkgjwoqkegdkjfl2k9d0-part5 \\\n</code></pre> <p>Where:</p> <ul> <li><code>-o ashift=12</code>: Adjusts the disk sector size to the disks in use.</li> <li><code>/dev/sda /dev/sdb /dev/sdc /dev/sdd</code> are the rotational data disks configured in RAIDZ1</li> <li>We set two partitions in mirror for the ZLOG</li> <li>We set two partitions in stripe for the L2ARC</li> </ul> <p>If you don't want the main pool to be mounted use <code>zfs set mountpoint=none main</code>.</p> <p>If you don't want replication use <code>zpool create main sde sdf sdg</code></p>"}, {"location": "linux/zfs/#create-your-filesystems", "title": "Create your filesystems", "text": "<p>Once we have the pool you can create the different filesystems. If you want to use encryption with a key follow the next steps:</p> <pre><code>mkdir /etc/zfs/keys\nchmod 700 /etc/zfs/keys\ndd if=/dev/random of=/etc/zfs/keys/home.key bs=1 count=32\n</code></pre> <p>Then create the filesystem:</p> <pre><code>zfs create \\ \n-o mountpoint=/home/lyz \\\n-o encryption=on \\\n-o keyformat=raw \\\n-o keylocation=file:///etc/zfs/keys/home.key \\\nmain/lyz\n</code></pre> <p>If you want to use a passphrase instead [you can use the <code>zfs create -o encryption=on -o keylocation=prompt -o keyformat=passphrase</code> command.</p> <p>I'm assuming that <code>compression</code> was set in the pool.</p> <p>You can check the created filesystems with <code>zfs list</code></p>"}, {"location": "linux/zfs/#enable-the-autoloading-of-datasets-on-boot", "title": "Enable the autoloading of datasets on boot", "text": "<p>It is possible to automatically unlock a pool dataset on boot time by using a systemd unit. For example create the following service to unlock any specific dataset:</p> <pre><code>/etc/systemd/system/zfs-load-key.service\n</code></pre> <pre><code>[Unit]\nDescription=Load encryption keys\nDefaultDependencies=no\nAfter=zfs-import.target\nBefore=zfs-mount.service\n\n[Service]\nType=oneshot\nRemainAfterExit=yes\nExecStart=/usr/bin/zfs load-key -a\nStandardInput=tty-force\n\n[Install]\nWantedBy=zfs-mount.service\n</code></pre> <pre><code>systemctl start zfs-load-key.service\nsystemctl enable zfs-load-key.service\nreboot\n</code></pre>"}, {"location": "linux/zfs/#configure-nfs", "title": "Configure NFS", "text": "<p>With ZFS you can share a specific dataset via NFS. If for whatever reason the dataset does not mount, then the export will not be available to the application, and the NFS client will be blocked.</p> <p>You still must install the necessary daemon software to make the share available. For example, if you wish to share a dataset via NFS, then you need to install the NFS server software, and it must be running. Then, all you need to do is flip the sharing NFS switch on the dataset, and it will be immediately available.</p>"}, {"location": "linux/zfs/#install-nfs", "title": "Install NFS", "text": "<p>To share a dataset via NFS, you first need to make sure the NFS daemon is running. On Debian and Ubuntu, this is the <code>nfs-kernel-server</code> package. </p> <pre><code>sudo apt-get install nfs-kernel-server\n</code></pre> <p>Further, with Debian and Ubuntu, the NFS daemon will not start unless there is an export in the <code>/etc/exports</code> file. So, you have two options: you can create a dummy export, only available to localhost, or you can edit the init script to start without checking for a current export. I prefer the former. Let's get that setup:</p> <pre><code>echo '/tmp localhost(ro)' &gt;&gt; /etc/exports\n$ sudo /etc/init.d/nfs-kernel-server start\n$ showmount -e hostname.example.com\nExport list for hostname.example.com:\n/mnt localhost\n</code></pre> <p>With our NFS daemon running, we can now start sharing ZFS datasets. The <code>sharenfs</code> property can be <code>on</code>, <code>off</code> or <code>opts</code>, where <code>opts</code> are valid NFS export options. So, if you want to share the <code>pool/srv</code> dataset, which is mounted to <code>/srv</code> to the <code>10.80.86.0/24</code> network you could run:</p> <pre><code># zfs set sharenfs=\"rw=@10.80.86.0/24\" pool/srv\n# zfs share pool/srv\n# showmount -e hostname.example.com\nExport list for hostname.example.com:\n/srv 10.80.86.0/24\n/mnt localhost\n</code></pre> <p>If you need to share to multiple subnets, you would do something like:</p> <pre><code>sudo zfs set sharenfs=\"rw=@192.168.0.0/24,rw=@10.0.0.0/24\" pool-name/dataset-name\n</code></pre> <p>If you need <code>root</code> to be able to write to the directory enable the <code>no_root_squash</code> NFS option</p> <p>root_squash \u2014 Prevents root users connected remotely from having root privileges and assigns them the user ID for the user nfsnobody. This effectively \"squashes\" the power of the remote root user to the lowest local user, preventing unauthorized alteration of files on the remote server. Alternatively, the no_root_squash option turns off root squashing. To squash every remote user, including root, use the all_squash option. To specify the user and group IDs to use with remote users from a particular host, use the anonuid and anongid options, respectively. In this case, a special user account can be created for remote NFS users to share and specify (anonuid=,anongid=), where is the user ID number and is the group ID number.</p> <p>You should now be able to mount the NFS export from an NFS client. Install the client with:</p> <pre><code>sudo apt-get install nfs-common\n</code></pre> <p>And then mount it with:</p> <pre><code>mount -t nfs hostname.example.com:/srv /mnt\n</code></pre> <p>To permanently mount it you need to add it to your <code>/etc/fstab</code>, check this section for more details.</p>"}, {"location": "linux/zfs/#backup", "title": "Backup", "text": "<p>Please remember that RAID is not a backup, it guards against one kind of hardware failure. There's lots of failure modes that it doesn't guard against though:</p> <ul> <li>File corruption</li> <li>Human error (deleting files by mistake)</li> <li>Catastrophic damage (someone dumps water onto the server)</li> <li>Viruses and other malware</li> <li>Software bugs that wipe out data</li> <li>Hardware problems that wipe out data or cause hardware damage (controller malfunctions, firmware bugs, voltage spikes, ...)</li> </ul> <p>That's why you still need to make backups.</p> <p>ZFS has the builtin feature to make snapshots of the pool. A snapshot is a first class read-only filesystem. It is a mirrored copy of the state of the filesystem at the time you took the snapshot. They are persistent across reboots, and they don't require any additional backing store; they use the same storage pool as the rest of your data. </p> <p>If you remember ZFS's awesome nature of copy-on-write filesystems, you will remember the discussion about Merkle trees. A ZFS snapshot is a copy of the Merkle tree in that state, except we make sure that the snapshot of that Merkle tree is never modified.</p> <p>Creating snapshots is near instantaneous, and they are cheap. However, once the data begins to change, the snapshot will begin storing data. If you have multiple snapshots, then multiple deltas will be tracked across all the snapshots. However, depending on your needs, snapshots can still be exceptionally cheap.</p>"}, {"location": "linux/zfs/#zfs-snapshot-lifecycle-management", "title": "ZFS snapshot lifecycle management", "text": "<p>ZFS doesn't though have a clean way to manage the lifecycle of those snapshots. There are many tools to fill the gap:</p> <ul> <li><code>sanoid</code>: Made in Perl, 2.4k stars, last commit April 2022, last release April 2021</li> <li>zfs-auto-snapshot: Made in Bash, 767 stars, last commit/release on September 2019</li> <li>pyznap: Made in Python, 176 stars, last commit/release on September 2020</li> <li>Custom scripts.</li> </ul> <p>It seems that the state of the art of ZFS backups is not changing too much in the last years, possibly because the functionality is covered so there is no need for further development. So I'm going to manage the backups with <code>sanoid</code> despite it being done in Perl because it's the most popular, it looks simple but flexible for complex cases, and it doesn't look I'd need to tweak the code.</p>"}, {"location": "linux/zfs/#restore-a-backup", "title": "Restore a backup", "text": "<p>You can list the available snapshots of a filesystem with <code>zfs list -t snapshot {{ pool_or_filesystem_name }}</code>, if you don't specify the <code>pool_or_filesystem_name</code> it will show all available snapshots.</p> <p>You have two ways to restore a backup:</p> <ul> <li>Mount the snapshot in a directory and manually copy the needed files:</li> </ul> <pre><code>mount -t zfs main/lyz@autosnap_2023-02-17_13:15:06_hourly /mnt\n</code></pre> <p>To umount the snapshot run <code>umount /mnt</code>.</p> <ul> <li>Rolling back the filesystem to the snapshot state: Rolling back to a previous snapshot will discard any data changes between that snapshot and the current time. Further, by default, you can only rollback to the most recent snapshot. In order to rollback to an earlier snapshot, you must destroy all snapshots between the current time and that snapshot you wish to rollback to. If that's not enough, the filesystem must be unmounted before the rollback can begin. This means downtime.</li> </ul> <p>To rollback the \"tank/test\" dataset to the \"tuesday\" snapshot, we would issue:</p> <pre><code>$: zfs rollback tank/test@tuesday\ncannot rollback to 'tank/test@tuesday': more recent snapshots exist\nuse '-r' to force deletion of the following snapshots:\ntank/test@wednesday\ntank/test@thursday\n</code></pre> <p>As expected, we must remove the <code>@wednesday</code> and <code>@thursday</code> snapshots before we can rollback to the <code>@tuesday</code> snapshot.</p>"}, {"location": "linux/zfs/#see-how-much-space-do-your-snapshots-consume", "title": "See how much space do your snapshots consume", "text": "<p>When a snapshot is created, its space is initially shared between the snapshot and the file system, and possibly with previous snapshots. As the file system changes, space that was previously shared becomes unique to the snapshot, and thus is counted in the snapshot\u2019s <code>used</code> property.</p> <p>Additionally, deleting snapshots can increase the amount of space that is unique for use by other snapshots.</p> <p>Note: The value for a snapshot\u2019s space referenced property is the same as that for the file system when the snapshot was created.</p> <p>You can display the amount of space or size that is consumed by snapshots and descendant file systems by using the <code>zfs list -o space</code> command.</p> <pre><code># zfs list -o space -r rpool\nNAME                             AVAIL   USED  USEDSNAP  USEDDS  USEDREFRESERV  USEDCHILD\nrpool                            10.2G  5.16G         0   4.52M              0      5.15G\nrpool/ROOT                       10.2G  3.06G         0     31K              0      3.06G\nrpool/ROOT/solaris               10.2G  3.06G     55.0M   2.78G              0       224M\nrpool/ROOT/solaris@install           -  55.0M         -       -              -          -\nrpool/ROOT/solaris/var           10.2G   224M     2.51M    221M              0          0\nrpool/ROOT/solaris/var@install       -  2.51M         -       -              -          -\n</code></pre> <p>From this output, you can see the amount of space that is:</p> <ul> <li>AVAIL: The amount of space available to the dataset and all its children, assuming that there is no other activity in the pool. </li> <li> <p>USED: The amount of space consumed by this dataset and all its descendants. This is the value that is checked against this dataset's quota and reservation. The space used does not include this dataset's reservation, but does take into account the reservations of any descendants datasets.</p> <p>The used space of a snapshot is the space referenced exclusively by this snapshot. If this snapshot is destroyed, the amount of <code>used</code> space will be freed. Space that is shared by multiple snapshots isn't accounted for in this metric.  * USEDSNAP: Space being consumed by snapshots of each data set * USEDDS: Space being used by the dataset itself * USEDREFRESERV: Space being used by a refreservation set on the dataset that would be freed if it was removed. * USEDCHILD: Space being used by the children of this dataset.</p> </li> </ul> <p>Other space properties are:</p> <ul> <li>LUSED: The amount of space that is \"logically\" consumed by this dataset and all its descendents. It ignores the effect of <code>compression</code> and <code>copies</code> properties, giving a quantity closer to the amount of data that aplication ssee. However it does include space consumed by metadata.</li> <li>REFER: The amount of data that is accessible by this dataset, which may or may not be shared with other dataserts in the pool. When a snapshot or clone is created, it initially references the same amount of space as the filesystem or snapshot it was created from, since its contents are identical.</li> </ul>"}, {"location": "linux/zfs/#see-the-differences-between-two-backups", "title": "See the differences between two backups", "text": "<p>To identify the differences between two snapshots, use syntax similar to the following:</p> <pre><code>$ zfs diff tank/home/tim@snap1 tank/home/tim@snap2\nM       /tank/home/tim/\n+       /tank/home/tim/fileB\n</code></pre> <p>The following table summarizes the file or directory changes that are identified by the <code>zfs diff</code> command.</p> File or Directory Change Identifier File or directory has been modified or file or directory link has changed M File or directory is present in the older snapshot but not in the more recent snapshot \u2014 File or directory is present in the more recent snapshot but not in the older snapshot + File or directory has been renamed R"}, {"location": "linux/zfs/#create-a-cold-backup-of-a-series-of-datasets", "title": "Create a cold backup of a series of datasets", "text": "<p>If you've used the <code>-o keyformat=raw -o keylocation=file:///etc/zfs/keys/home.key</code> arguments to encrypt your datasets you can't use a <code>keyformat=passphase</code> encryption on the cold storage device. You need to copy those keys on the disk. One way of doing it is to:</p> <ul> <li> <p>Create a 100M LUKS partition protected with a passphrase where you store the keys.</p> </li> <li> <p>The rest of the space is left for a partition for the zpool.</p> </li> </ul> <p>WARNING: substitute <code>/dev/sde</code> for the partition you need to work on in the next snippets</p> <p>To do it: - Create the partitions: </p> <pre><code>fdisk /dev/sde\nn\n+100M\nn\nw\n</code></pre> <ul> <li>Create the zpool</li> </ul> <pre><code>zpool create cold-backup-01 /dev/sde2\n</code></pre>"}, {"location": "linux/zfs/#troubleshooting", "title": "Troubleshooting", "text": ""}, {"location": "linux/zfs/#clear-a-permanent-zfs-error-in-a-healthy-pool", "title": "Clear a permanent ZFS error in a healthy pool", "text": "<p>Sometimes when you do a <code>zpool status</code> you may see that the pool is healthy but that there are \"Permanent errors\" that may point to files themselves or directly to memory locations.</p> <p>You can read this long discussion on what does these permanent errors mean, but what solved the issue for me was to run a new scrub</p> <p><code>zpool scrub my_pool</code></p> <p>It takes a long time to run, so be patient.</p> <p>If you want to stop a scrub run:</p> <pre><code>zpool scrub -s my_pool\n</code></pre>"}, {"location": "linux/zfs/#zfs-pool-is-in-suspended-mode", "title": "ZFS pool is in suspended mode", "text": "<p>Probably because you've unplugged a device without unmounting it.</p> <p>If you want to remount the device you can follow these steps to symlink the new devfs entries to where zfs thinks the vdev is. That way you can regain access to the pool without a reboot.</p> <p>So if zpool status says the vdev is /dev/disk2s1, but the reattached drive is at disk4, then do the following:</p> <pre><code>cd /dev\nsudo rm -f disk2s1\nsudo ln -s disk4s1 disk2s1\nsudo zpool clear -F WD_1TB\nsudo zpool export WD_1TB\nsudo rm disk2s1\nsudo zpool import WD_1TB\n</code></pre> <p>If you don't care about the zpool anymore, sadly your only solution is to reboot the server. Real ugly, so be careful when you umount zpools.</p>"}, {"location": "linux/zfs/#learning", "title": "Learning", "text": "<p>I've found that learning about ZFS was an interesting, intense and time consuming task. If you want a quick overview check this video. If you prefer to read, head to the awesome Aaron Toponce articles and read all of them sequentially, each is a jewel. The docs on the other hand are not that pleasant to read. For further information check JRS articles.</p>"}, {"location": "linux/zfs/#resources", "title": "Resources", "text": "<ul> <li>Aaron Toponce articles</li> <li>Docs</li> <li>JRS articles</li> <li>ZFS basic introduction video</li> </ul>"}, {"location": "linux/zip/", "title": "zip", "text": "<p><code>zip</code> is an UNIX command line tool to package and compress files.</p>"}, {"location": "linux/zip/#usage", "title": "Usage", "text": ""}, {"location": "linux/zip/#create-a-zip-file", "title": "Create a zip file", "text": "<pre><code>zip -r {{ zip_file }} {{ files_to_save }}\n</code></pre>"}, {"location": "linux/zip/#split-files-to-a-specific-size", "title": "Split files to a specific size", "text": "<pre><code>zip -s {{ size }} -r {{ destination_zip }} {{ files }}\n</code></pre> <p>Where <code>{{ size }}</code> can be <code>950m</code></p>"}, {"location": "linux/zip/#compress-with-password", "title": "Compress with password", "text": "<pre><code>zip -er {{ zip_file }} {{ files_to_save }}\n</code></pre>"}, {"location": "linux/zip/#read-files-to-compress-from-a-file", "title": "Read files to compress from a file", "text": "<pre><code>cat {{ files_to_compress.txt }} | zip -er {{ destination_zip }} -@\n</code></pre>"}, {"location": "linux/zip/#uncompress-a-zip-file", "title": "Uncompress a zip file", "text": "<pre><code>unzip {{ zip_file }}\n</code></pre>"}, {"location": "linux/luks/luks/", "title": "LUKS", "text": "<p>LUKS definition</p> <p>The Linux Unified Key Setup (LUKS) is a disk encryption specification created by Clemens Fruhwirth in 2004 and was originally intended for Linux.</p> <p>While most disk encryption software implements different, incompatible, and undocumented formats, LUKS implements a platform-independent standard on-disk format for use in various tools. This not only facilitates compatibility and interoperability among different programs, but also assures that they all implement password management in a secure and documented manner.</p> <p>The reference implementation for LUKS operates on Linux and is based on an enhanced version of cryptsetup, using dm-crypt as the disk encryption backend.</p> <p>LUKS is designed to conform to the TKS1 secure key setup scheme.</p>"}, {"location": "linux/luks/luks/#luks-commands", "title": "LUKS Commands", "text": "<p>We use the <code>cryptsetup</code> command to interact with LUKS partitions.</p>"}, {"location": "linux/luks/luks/#header-management", "title": "Header management", "text": ""}, {"location": "linux/luks/luks/#get-the-disk-header", "title": "Get the disk header", "text": "<pre><code>cryptsetup luksDump /dev/sda3\n</code></pre>"}, {"location": "linux/luks/luks/#backup-header", "title": "Backup header", "text": "<pre><code>cryptsetup luksHeaderBackup --header-backup-file {{ file }} {{ device }}\n</code></pre>"}, {"location": "linux/luks/luks/#key-management", "title": "Key management", "text": ""}, {"location": "linux/luks/luks/#add-a-key", "title": "Add a key", "text": "<pre><code>cryptsetup luksAddKey --key-slot 1 {{ luks_device }}\n</code></pre>"}, {"location": "linux/luks/luks/#change-a-key", "title": "Change a key", "text": "<pre><code>cryptsetup luksChangeKey {{ luks_device }} -s 0\n</code></pre>"}, {"location": "linux/luks/luks/#test-if-you-remember-the-key", "title": "Test if you remember the key", "text": "<p>Try to add a new key and cancel the process</p> <pre><code>cryptsetup luksAddKey --key-slot 3 {{ luks_device }}\n</code></pre>"}, {"location": "linux/luks/luks/#delete-some-keys", "title": "Delete some keys", "text": "<pre><code>cryptsetup luksDump {{ device }}\ncryptsetup luksKillSlot {{ device }} {{ slot_number }}\n</code></pre>"}, {"location": "linux/luks/luks/#delete-all-keys", "title": "Delete all keys", "text": "<pre><code>cryptsetup luksErase {{ device }}\n</code></pre>"}, {"location": "linux/luks/luks/#encrypt-hard-drive", "title": "Encrypt hard drive", "text": "<ul> <li>Configure LUKS partition</li> </ul> <pre><code>cryptsetup -y -v luksFormat /dev/sdg\n</code></pre> <ul> <li>Open the container</li> </ul> <pre><code>cryptsetup luksOpen /dev/sdg crypt\n</code></pre> <ul> <li>Fill it with zeros</li> </ul> <pre><code>pv -tpreb /dev/zero | dd of=/dev/mapper/crypt bs=128M\n</code></pre> <ul> <li>Make filesystem   <pre><code>mkfs.ext4 /dev/mapper/crypt\n</code></pre></li> </ul>"}, {"location": "linux/luks/luks/#break-a-luks-password", "title": "Break a luks password", "text": "<p>You can use <code>bruteforce-luks</code></p>"}, {"location": "linux/luks/luks/#luks-debugging", "title": "LUKS debugging", "text": ""}, {"location": "linux/luks/luks/#resource-busy", "title": "Resource busy", "text": "<ul> <li>Umount the lv first</li> </ul> <pre><code>lvscan\nlvchange -a n {{ partition_name }}\n</code></pre> <ul> <li>Then close the luks device</li> </ul> <pre><code>cryptsetup luksClose {{ device_name }}\n</code></pre>"}, {"location": "linux/vim/vim_plugins/", "title": "Vim plugins", "text": ""}, {"location": "linux/vim/vim_plugins/#black", "title": "Black", "text": "<p>To install Black you first need <code>python3-venv</code>.</p> <pre><code>sudo apt-get install python3-venv\n</code></pre> <p>Add the plugin and configure it so vim runs it each time you save.</p> <p>File <code>~/.vimrc</code></p> <pre><code>Plugin 'psf/black'\n\n\" Black\nautocmd BufWritePre *.py execute ':Black'\n</code></pre> <p>A configuration issue exists for neovim. If you encounter the error <code>AttributeError: module 'black' has no attribute 'find_pyproject_toml'</code>, do the following:</p> <pre><code>cd ~/.vim/bundle/black\ngit checkout 19.10b0\n</code></pre> <p>As the default line length is 88 (ugly number by the way), we need to change the indent, python-mode configuration as well</p> <pre><code>\"\" python indent\nautocmd BufNewFile,BufRead *.py setlocal foldmethod=indent tabstop=4 softtabstop=4 shiftwidth=4 textwidth=88 smarttab expandtab\n\n\" python-mode\nlet g:pymode_options_max_line_length = 88\nlet g:pymode_lint_options_pep8 = {'max_line_length': g:pymode_options_max_line_length}\n</code></pre>"}, {"location": "linux/vim/vim_plugins/#ale", "title": "ALE", "text": "<p>ALE (Asynchronous Lint Engine) is a plugin providing linting (syntax checking and semantic errors) in NeoVim 0.2.0+ and Vim 8 while you edit your text files, and acts as a Vim Language Server Protocol client.</p> <p>ALE makes use of NeoVim and Vim 8 job control functions and timers to run linters on the contents of text buffers and return errors as text changes in Vim. This allows for displaying warnings and errors in files before they are saved back to a filesystem.</p> <p>In other words, this plugin allows you to lint while you type.</p> <p>ALE offers support for fixing code with command line tools in a non-blocking manner with the <code>:ALEFix</code> feature, supporting tools in many languages, like prettier, eslint, autopep8, and more.</p>"}, {"location": "linux/vim/vim_plugins/#installation", "title": "Installation", "text": "<p>Install with Vundle:</p> <pre><code>Plugin 'dense-analysis/ale'\n</code></pre>"}, {"location": "linux/vim/vim_plugins/#configuration", "title": "Configuration", "text": "<pre><code>let g:ale_sign_error                  = '\u2718'\nlet g:ale_sign_warning                = '\u26a0'\nhighlight ALEErrorSign ctermbg        =NONE ctermfg=red\nhighlight ALEWarningSign ctermbg      =NONE ctermfg=yellow\nlet g:ale_linters_explicit            = 1\nlet g:ale_lint_on_text_changed        = 'normal'\n\" let g:ale_lint_on_text_changed        = 'never'\nlet g:ale_lint_on_enter               = 0\nlet g:ale_lint_on_save                = 1\nlet g:ale_fix_on_save                 = 1\n\nlet g:ale_linters = {\n\\  'markdown': ['markdownlint', 'writegood', 'alex', 'proselint'],\n\\  'json': ['jsonlint'],\n\\  'python': ['flake8', 'mypy', 'pylint', 'alex'],\n\\  'yaml': ['yamllint', 'alex'],\n\\   '*': ['alex', 'writegood'],\n\\}\n\nlet g:ale_fixers = {\n\\   '*': ['remove_trailing_lines', 'trim_whitespace'],\n\\   'json': ['jq'],\n\\   'python': ['isort'],\n\\   'terraform': ['terraform'],\n\\}\ninoremap &lt;leader&gt;e &lt;esc&gt;:ALENext&lt;cr&gt;\nnnoremap &lt;leader&gt;e :ALENext&lt;cr&gt;\ninoremap &lt;leader&gt;p &lt;esc&gt;:ALEPrevious&lt;cr&gt;\nnnoremap &lt;leader&gt;p :ALEPrevious&lt;cr&gt;\n</code></pre> <p>Where:</p> <ul> <li><code>let g:ale_linters_explicit</code>: Prevent ALE load only the selected linters.</li> <li>use <code>&lt;leader&gt;e</code> and <code>&lt;leader&gt;p</code> to navigate through the warnings.</li> </ul> <p>If you feel that it's too heavy, use <code>ale_lint_on_enter</code> or increase the <code>ale_lint_delay</code>.</p> <p>Use <code>:ALEInfo</code> to see the ALE configuration and any errors when running <code>:ALEFix</code> for the specific buffer.</p>"}, {"location": "linux/vim/vim_plugins/#flakehell", "title": "Flakehell", "text": "<p>Flakehell is not supported yet. Until that issue is closed we need the following configuration:</p> <pre><code>let g:ale_python_flake8_executable = flake8helled\nlet g:ale_python_flake8_use_global = 1\n</code></pre>"}, {"location": "linux/vim/vim_plugins/#toggle-fixers-on-save", "title": "Toggle fixers on save", "text": "<p>There are cases when you don't want to run the fixers in your code.</p> <p>Ale doesn't have an option to do it, but zArubaru showed how to do it. If you add to your configuration</p> <pre><code>command! ALEToggleFixer execute \"let g:ale_fix_on_save = get(g:, 'ale_fix_on_save', 0) ? 0 : 1\"\n</code></pre> <p>You can then use <code>:ALEToggleFixer</code> to activate an deactivate them.</p>"}, {"location": "linux/vim/vim_plugins/#vim-easymotion", "title": "vim-easymotion", "text": "<p>EasyMotion provides a much simpler way to use some motions in vim. It takes the <code>&lt;number&gt;</code> out of <code>&lt;number&gt;w</code> or <code>&lt;number&gt;f{char}</code> by highlighting all possible choices and allowing you to press one key to jump directly to the target.</p> <p>When one of the available motions is triggered, all visible text preceding or following the cursor is faded, and motion targets are highlighted.</p>"}, {"location": "linux/vim/vim_plugins/#installation_1", "title": "Installation", "text": "<p>Add to Vundle <code>Plugin 'easymotion/vim-easymotion'</code></p> <p>The configuration can be quite complex, but I'm starting with the basics:</p> <pre><code>\" Easymotion\nlet g:EasyMotion_do_mapping = 0 \" Disable default mappings\nlet g:EasyMotion_keys='asdfghjkl'\n\n\" Jump to anywhere you want with minimal keystrokes, with just one key binding.\n\" `s{char}{label}`\nnmap s &lt;Plug&gt;(easymotion-overwin-f)\n\n\" JK motions: Line motions\nmap &lt;Leader&gt;j &lt;Plug&gt;(easymotion-j)\nmap &lt;Leader&gt;k &lt;Plug&gt;(easymotion-k)\n</code></pre> <p>It's awesome to move between windows with <code>s</code>.</p>"}, {"location": "linux/vim/vim_plugins/#vim-fugitive", "title": "Vim Fugitive", "text": ""}, {"location": "linux/vim/vim_plugins/#add-portions-of-file-to-the-index", "title": "Add portions of file to the index", "text": "<p>To stage only part of the file to a commit, open it and launch <code>:Gdiff</code>. With <code>diffput</code> and <code>diffobtain</code> Vim's functionality you move to the index file (the one in the left) the changes you want to stage.</p>"}, {"location": "linux/vim/vim_plugins/#prepare-environment-to-write-the-commit-message", "title": "Prepare environment to write the commit message", "text": "<p>When I write the commit message I like to review what changes I'm commiting. To do it I find useful to close all windows and do a vertical split with the changes so I can write the commit message in one of the window while I scroll down in the other. As the changes are usually no the at the top of the file, I scroll the window of the right to the first change and then switch back to the left one in insert mode to start writing.</p> <p>I've also made some movement remappings:</p> <ul> <li><code>jj</code>, <code>kk</code>, <code>&lt;C-d&gt;</code> and <code>&lt;C-u&gt;</code> in insert mode will insert normal mode and go     to the window in the right to continue seeing the changes.</li> <li><code>i</code>, <code>a</code>, <code>o</code>, <code>O</code>: if you are in the changes window it will go to the commit message window     in insert mode.</li> </ul> <p>Once I've made the commit I want to only retain one buffer.</p> <p>Add the following snippet to do just that:</p> <pre><code>\" Open commit message buffer in fullscreen with a vertical split, and close it with\n\" leader q\nau BufNewFile,BufRead *COMMIT_EDITMSG call CommitMessage()\n\nfunction! RestoreBindings()\n  inoremap jj &lt;esc&gt;j\n  inoremap kk &lt;esc&gt;k\n  inoremap &lt;C-d&gt; &lt;C-d&gt;\n  inoremap &lt;C-u&gt; &lt;C-u&gt;\n  nnoremap i i\n  nnoremap a a\n  nnoremap o o\n  nnoremap O O\nendfunction\n\nfunction! CommitMessage()\n  \" Remap the saving mappings\n  \" Close buffer when saving\n  inoremap &lt;silent&gt; &lt;leader&gt;q &lt;esc&gt;:w&lt;cr&gt; \\| :only&lt;cr&gt; \\| :call RestoreBindings()&lt;cr&gt; \\|:Sayonara&lt;CR&gt;\n  nnoremap &lt;silent&gt; &lt;leader&gt;q &lt;esc&gt;:w&lt;cr&gt; \\| :only&lt;cr&gt; \\| :call RestoreBindings()&lt;cr&gt; \\|:Sayonara&lt;CR&gt;\n\n  inoremap jj &lt;esc&gt;:wincmd l&lt;cr&gt;j\n  inoremap kk &lt;esc&gt;:wincmd l&lt;cr&gt;k\n  inoremap &lt;C-d&gt; &lt;esc&gt;:wincmd l&lt;cr&gt;&lt;C-d&gt;\n  inoremap &lt;C-u&gt; &lt;esc&gt;:wincmd l&lt;cr&gt;&lt;C-u&gt;\n  nnoremap i :wincmd h&lt;cr&gt;i\n  nnoremap a :wincmd h&lt;cr&gt;a\n  nnoremap o :wincmd h&lt;cr&gt;o\n  nnoremap O :wincmd h&lt;cr&gt;O\n\n  \" Remove bad habits\n  inoremap jk &lt;nop&gt;\n  inoremap ZZ &lt;nop&gt;\n  nnoremap ZZ &lt;nop&gt;\n  \" Close all other windows\n  only\n  \" Create a vertical split\n  vsplit\n  \" Go to the right split\n  wincmd l\n  \" Go to the first change\n  execute \"normal! /^diff\\&lt;cr&gt;8j\"\n  \" Clear the search highlights\n  nohl\n  \" Go back to the left split\n  wincmd h\n  \" Enter insert mode\n  execute \"startinsert\"\nendfunction\n</code></pre> <p>I'm assuming that you save with <code>&lt;leader&gt;w</code> and that you're using Sayonara to close your buffers.</p>"}, {"location": "linux/vim/vim_plugins/#git-push-sets-the-upstream-by-default", "title": "Git push sets the upstream by default", "text": "<p>Add to your config:</p> <pre><code>nnoremap &lt;leader&gt;gp :Git -c push.default=current push&lt;CR&gt;\n</code></pre> <p>If you want to see the output of the push command, use <code>:copen</code> after the successful push.</p>"}, {"location": "linux/vim/vim_plugins/#vim-test", "title": "Vim-test", "text": "<p>A Vim wrapper for running tests on different granularities.</p> <p>Currently the following testing frameworks are supported:</p> Language Frameworks Identifiers C# .NET <code>dotnettest</code> Clojure Fireplace.vim <code>fireplacetest</code> Crystal Crystal <code>crystalspec</code> Elixir ESpec, ExUnit <code>espec</code>, <code>exunit</code> Erlang CommonTest <code>commontest</code> Go Ginkgo, Go <code>ginkgo</code>, <code>gotest</code> Java Maven <code>maventest</code> JavaScript Intern, Jasmine, Jest, Karma, Lab, Mocha, TAP, <code>intern</code>, <code>jasmine</code>, <code>jest</code>, <code>karma</code>, <code>lab</code>, <code>mocha</code>, <code>tap</code> Lua Busted <code>busted</code> PHP Behat, Codeception, Kahlan, Peridot, PHPUnit, PHPSpec <code>behat</code>, <code>codeception</code>, <code>kahlan</code>, <code>peridot</code>, <code>phpunit</code>, <code>phpspec</code> Perl Prove <code>prove</code> Python Django, Nose, Nose2, PyTest, PyUnit <code>djangotest</code>, <code>djangonose</code> <code>nose</code>, <code>nose2</code>, <code>pytest</code>, <code>pyunit</code> Racket RackUnit <code>rackunit</code> Ruby Cucumber, [M], [Minitest][minitest], Rails, RSpec <code>cucumber</code>, <code>m</code>, <code>minitest</code>, <code>rails</code>, <code>rspec</code> Rust Cargo <code>cargotest</code> Shell Bats <code>bats</code> VimScript Vader.vim, VSpec <code>vader</code>, <code>vspec</code>"}, {"location": "linux/vim/vim_plugins/#features", "title": "Features", "text": "<ul> <li>Zero dependencies</li> <li>Zero configuration required (it Does the Right Thing\u2122, see   Philosophy)</li> <li>Wide range of test runners which are automagically detected</li> <li>Polyfills for nearest tests (by constructing regexes)</li> <li>Wide range of execution environments (\"strategies\")</li> <li>Fully customized CLI options configuration</li> <li>Extendable with new runners and strategies</li> </ul> <p>Test.vim consists of a core which provides an abstraction over running any kind of tests from the command-line. Concrete test runners are then simply plugged in, so they all work in the same unified way.</p>"}, {"location": "linux/vim/vim_plugins/#diffview", "title": "DiffView", "text": "<p>Single tabpage interface for easily cycling through diffs for all modified files for any git rev.</p>"}, {"location": "linux/vim/vim_plugins/#installation_2", "title": "Installation", "text": "<p>If you use <code>Packer</code> in your <code>plugins.lua</code> file add:</p> <pre><code>  use {\n    'sindrets/diffview.nvim',\n    requires = {\n      'nvim-tree/nvim-web-devicons'\n    }\n  }\n</code></pre> <p>Then configure it with:</p> <pre><code>-- Configure diff viewer\nrequire(\"diffview\").setup({\n  keymaps = {\n    view = {\n      { { \"n\", \"v\" }, \"dc\", \":DiffviewClose&lt;CR&gt;\" },\n    }\n  }\n})\n\nvim.cmd[[\n  \" Enter the diff window\n  nmap dv :DiffviewOpen&lt;CR&gt;\n  \"\n]]\n</code></pre> <p>That way you can open the diff window with <code>do</code> and close it with <code>dc</code> (only if you are in one of the buffers)</p> <p>Some nice keymaps of the diff window:</p> <ul> <li><code>&lt;tab&gt;</code>: go to the next file</li> <li><code>-</code>: Stage/unstage the changes</li> <li><code>]x</code>: next conflict</li> <li><code>[x</code>: previous conflict</li> <li><code>X</code>: On the file panel to discard the changes</li> </ul>"}, {"location": "linux/vim/vim_plugins/#issues", "title": "Issues", "text": ""}, {"location": "linux/vim/vim_plugins/#vim-abolish", "title": "Vim-Abolish", "text": "<ul> <li>Error adding elipsis instead of three     dots: Pope said that it's     not possible :(.</li> </ul>"}, {"location": "linux/vim/vim_plugins/#references", "title": "References", "text": "<ul> <li>ALE supported tools</li> </ul>"}, {"location": "psychology/the_xy_problem/", "title": "The XY problem", "text": "<p>The XY problem is a communication or thinking problem encountered in situations where the real issue, X, of the human asking for help is obscured, because instead of asking directly about issue X, they ask how to solve a secondary issue, Y, which they believe will allow them to resolve issue X. However, resolving issue Y often does not resolve issue X, or is a poor way to resolve it, and the obscuring of the real issue and the introduction of the potentially strange secondary issue can lead to the person trying to help having unnecessary difficulties in communication and offering poor solutions.</p>"}, {"location": "psychology/the_xy_problem/#how-to-avoid-it", "title": "How to avoid it", "text": "<ul> <li>Always include information about a broader picture along with any attempted     solution.</li> <li>If someone asks for more information, do provide details.</li> <li>If there are other solutions you've already ruled out, share why you've ruled     them out. This gives more information about your requirements.</li> </ul>"}, {"location": "writing/build_your_own_wiki/", "title": "Build your own wiki", "text": "<p>This page is an early version of a WIP</p> <p>If you don't want to start from scratch, you can fork the blue book and start writing straight away.</p>"}, {"location": "writing/build_your_own_wiki/#enable-clickable-navigation-sections", "title": "Enable clickable navigation sections", "text": "<p>By default, mkdocs doesn't let you to have clickable sections that lead to an index page. It has been long discussed in #2060, #1042, #1139 and #1975. Thankfully, oprypin has solved it with the mkdocs-section-index plugin.</p> <p>Install the plugin with <code>pip install mkdocs-section-index</code> and configure your <code>mkdocs.yml</code> as:</p> <pre><code>nav:\n- Frob: index.md\n- Baz: baz.md\n- Borgs:\n- borgs/index.md\n- Bar: borgs/bar.md\n- Foo: borgs/foo.md\n\nplugins:\n- section-index\n</code></pre>"}, {"location": "writing/build_your_own_wiki/#unconnected-thoughts", "title": "Unconnected thoughts", "text": "<ul> <li>Set up ci with pre-commit and ALE.</li> <li>Create a custom     commitizen     changelog configuration to generate the rss entries periodically and publish     them as github releases.</li> <li>If some code needs to be in a file use:     <pre><code>!!! note \"File ~/script.py\"\n    ```python\n    print('hi')\n    ```\n</code></pre></li> <li>Define how to add links to newly created documents in the whole wiki.</li> <li>Deploy fast, deploy early</li> <li>Grab the ideas of todo</li> <li>Define a meaningful tag policy.</li> <li>https://www.gwern.net/About#confidence-tags</li> <li> <p>https://www.gwern.net/About#importance-tags</p> </li> <li> <p>Decide a meaningful nav policy.</p> </li> <li>How to decide when to create a new section,</li> <li>Add to the index nav once it's finished, not before</li> <li>Use the <code>tbd</code> tag to mark the articles that need attention.</li> <li>Avoid file hardcoding, use mkdocs autolinks     plugin, or     mkdocs-altlink-plugin if     #15     is not solved and you don't need to link images.</li> <li>Use underscores for the file names, so the autocompletion feature of your   editor works.</li> <li>Add a link to the github pages site both in the git repository description and   in the README.</li> <li>Use Github Actions to build   your blog.</li> <li>Make redirections of refactored articles.</li> <li>The newsletter could be split in   years with   one summary once the year has ended</li> <li>I want an rss support for my newsletters, mkdocs is not going to support   it, so the solution is to use   a static   template <code>rss.xml</code> that is manually generated each time you create a new newsletter   article. I could develop   a plugin so it creates it at   build time.</li> <li>Use of custom     domains</li> <li>Material guide to enable code blocks     highlight:     From what I've seen in some github issue, you should not use it with     codehilite, although you still have their syntax.</li> <li>User Guide: Writing your docs</li> <li>Example of markdown writing with Material theme and it's source</li> <li>Add revision date</li> <li>Add test for rotten     links, it seems    that htmltest was meant to be faster.</li> <li>Adds tooltips to preview the content of page links using     tooltipster.     I only managed to make it work with internal links and in an ugly way... So I'm     not using it.</li> <li>Plugin to generate Python Docstrings</li> <li>Add Mermaid graphs</li> <li>Add Plantuml graphs</li> <li>Analyze the text readability</li> </ul>"}, {"location": "writing/build_your_own_wiki/#about-page", "title": "About page", "text": "<ul> <li>Nikita</li> <li>Gwern</li> </ul>"}, {"location": "writing/build_your_own_wiki/#create-a-local-server-to-visualize-the-documentation", "title": "Create a local server to visualize the documentation", "text": "<pre><code>mkdocs serve\n</code></pre>"}, {"location": "writing/build_your_own_wiki/#links", "title": "Links", "text": "<ul> <li>Nikita's wiki workflow</li> </ul>"}, {"location": "writing/orthography/", "title": "Orthography Rules", "text": "<p>Bad grammar can thwart communication. It's especially important in today's world where we don't have the chance to have in person interactions or when your words might reach a wide audience. The quality of your text will impact how your message reaches people.</p> <p>This article is an effort to gather all my common pitfalls.</p>"}, {"location": "writing/orthography/#use-of-z-or-s-in-some-words", "title": "Use of z or s in some words", "text": "<p>It looks like American english uses <code>z</code> while British uses <code>s</code>, some examples:</p> <ul> <li>Organizations vs     organisation.</li> <li>Authorization vs     authorisation.</li> <li>Customized vs     customised.</li> </ul> <p>Both forms are correct, so choose the one that suits your liking.</p>"}, {"location": "writing/orthography/#stop-saying-i-know", "title": "Stop saying \"I know\"", "text": "<p>Using \"I know\" may not be the best way to show the other person that you've received the information. You can take the chance to use other words that additionally gives more context on how you stand with the information you've received, thus improving the communication and creating a bond. Each of the next words carries a different nuance:</p> <ul> <li> <p><code>Recognize</code>: You acknowledge the truth, existence or validity of the     information.</p> </li> <li> <p><code>From my perspective</code>: You're showing that given the information, you see the     person's point and state that you have a different one.</p> </li> <li> <p><code>Appreciate</code>: You recognize the implications and true value of the subject and     are being thankful for the information.</p> </li> <li> <p><code>Understand</code>: You show that you've perceived the underlying meaning of the     information. It implies a deeper level of information processing.</p> </li> <li> <p><code>I see</code>: You show that you understand and that you're paying all your     attention.</p> </li> </ul> <p>Besides showing where you stand or how you feel, you can use other phrases that make connection at the visual, audio and kinesthetic levels to improve the communication.</p> <ul> <li><code>I hear what you're saying</code>: Shows auditory connection.</li> <li><code>I get the picture</code> or <code>I see what you mean</code>: Shows visual connection.</li> <li><code>I catch your drift</code>: Shows kinesthetic connection.</li> </ul> <p>You can also add information when saying that you don't know. For example you can use:</p> <ul> <li><code>Misread</code>: You give the idea that you perceived the information wrong.</li> <li><code>Misunderstood</code>: You perceived the information well, but formed the wrong idea     in your head.</li> <li><code>Mixed up</code>: You had the correct information and idea, but you ended up saying     or doing the wrong answer.</li> <li><code>Confused</code>: You have the correct information but you can't form a clear idea.</li> </ul>"}, {"location": "writing/orthography/#use-collocations", "title": "Use collocations", "text": "<p>Collocation refers to a natural combination of words that are closely affiliated with each other. They make it easier to avoid overused or ambiguous words like \"very\", \"nice\", or \"beautiful\", by using a pair of words that fit the context better and that have a more precise meaning. Skilled users of the language can produce effects such as humor by varying the normal patterns of collocation.</p>"}, {"location": "writing/orthography/#stop-saying-very", "title": "Stop saying \"very\"", "text": "wrong collocation Very full stuffed Very risky perilous Very thin slender Very long extensive Very interesting intriguing Very happy jubilant, delighted, joyful, overjoyed Very worried anxious Very Thirsty parched Very dirty squalid Very clean spotless Very rude vulgar Very short brief Very boring dull Very good superb, marvelous, excellent, extraordinary, splendid, spectacular Very hot scalding / scorching Very cold freezing Very hungry ravenous Very slow sluggish Very fast rapid Very tired exhausted Very poor destitute Very rich wealthy Very hard challenging Very smart bright Very beautiful mesmerizing, stunning, astonishing, charming, magnificent Very sad depressing Very funny hilarious, absurd Very scared petrified / frightened / fearful Very sleepy drowsy Very full crowded Very ugly hideous Very wicked villainous Very quiet silent Very accurate exact Very large huge Very powerful compelling Very lazy indolent Very fat obese Very often frequently Very smooth sleek Very long term enduring Very strong unyelding Very tasty delicious Very valuable precious Very creative innovative Very light luminous Very wet soaked Very bright blinding Very strange abnormal. bizarre, outlandish Very small tiny Very big giant, immense, massive Very bad horrendous, atrocious, horrible Very important essential Very exciting engaging Very calm peaceful Very painful agonizing Very expensive priceless Very drunk intoxicated Very humble polite Very smart intelligent"}, {"location": "writing/orthography/#im-good-or-im-well", "title": "I'm good or I'm well", "text": "<p>TL;DR</p> <p>Use I'm well when referring to being ill, use I'm good for the rest.</p> <p><code>Good</code> is an adjective. <code>Well</code> is usually an adverb, but it can also act as an adjective.</p> <p>Adjectives modify nouns. When you say you're having a good day, the adjective <code>good</code> modifies the noun <code>day</code>.</p> <p>When people say <code>I'm good</code>, they're using <code>good</code> to modify <code>I</code>. Because <code>I</code> is a noun, this use of <code>good</code> is correct. The confusion comes when using the verb <code>am</code>, which makes people think we need an adverb. For example, you might say, <code>I play the piano poorly</code>. The adverb <code>poorly</code> is modifying the verb <code>play</code>, so that sentence is correct.</p> <p>But the sentence <code>I'm good</code>', <code>good</code> is modifying <code>I</code>, it's not modifying <code>am</code>, so <code>good</code> is correct, and <code>well</code> is not.</p> <p><code>Well</code> is an adverb, they are here to modify verbs, adjectives or other adverbs. You might say <code>I play the piano well</code>. Here <code>well</code> changes the verb <code>play</code>. However, <code>well</code> can also be an adjective, usually to describe someone who is in good health. So when some one says <code>I'm well</code>, they're using <code>well</code> as an adjective modifying <code>I</code>.</p>"}, {"location": "writing/orthography/#wont-vs-wont", "title": "Won't vs Wont", "text": "<ul> <li>Won't is the correct way to contract will not.</li> <li>Wont is a synonym of \"a habit\". For example, \"He went for a morning jog, as was     his wont\".</li> </ul>"}, {"location": "writing/orthography/#how-to-use-the-singular-they", "title": "How to use the singular they", "text": "<p>The singular \u201cthey\u201d is a generic third-person pronoun used in English.</p> <p>When readers see a gendered pronoun, such as he or she, they make assumptions about the gender of the person being described. It's better to use the singular \u201cthey\u201d because it is inclusive of all people and helps writers avoid making assumptions about gender.</p> <p>You should use it in these cases:</p> <ul> <li>When referring to a generic person whose gender is unknown or irrelevant to the context and</li> <li>When referring to a specific, known person who uses \u201cthey\u201d as their pronoun.</li> </ul> <p>When \u201cthey\u201d is the subject of a sentence, \u201cthey\u201d takes a plural verb regardless of whether \u201cthey\u201d is meant to be singular or plural.  For example, write \u201cthey are\u201d not \u201cthey is\u201d. The singular \u201cthey\u201d works similarly to the singular \u201cyou\u201d, even though \u201cyou\u201d may refer to one person or multiple people. However, if the noun in one sentence is a word like \u201cindividual\u201d or a person\u2019s name, use a singular verb.</p>"}, {"location": "writing/orthography/#where-to-add-your-pronouns", "title": "Where to add your pronouns", "text": "<p>The correct place to add your pronouns is after you present yourself, such as:</p> <p>Hi, I\u2019m Lyz (he/him), I'm writing to tell you\u2026</p>"}, {"location": "writing/orthography/#when-to-capitalize-after-a-question-mark", "title": "When to capitalize after a question mark", "text": "<p>TL;DR</p> <p>If the sentence ends after the question mark you should capitalize, if it doesn't end, you shouldn't have used the question mark, since it ends a sentence.</p> <p>The capitalization rule that we care about here is that the first word of a sentence starts with a capital letter, so the question is really about what ends a sentence. The answer to that is easy: terminal punctuation, i.e. a full stop, question mark or exclamation mark. There's a visual clue in that <code>?</code> and <code>!</code> are decorated full stops; you just have to remember that a colon (<code>:</code>) isn't really a decorated full stop, not that you'd ever know by looking at it. Colons, semicolons and commas aren't terminal punctuation, so they don't end a sentence and so don't force the next letter to be a capital. It may be a capital letter for some other reason such as being the start of a proper name, but not because it is starting a sentence.</p> <p>There are exceptions to this rule, occasions when <code>?</code> and <code>!</code> become non terminal punctuation. The most obvious is in quoted speech: if the speaker asks a question or makes an exclamation, the <code>?</code> or <code>!</code> doesn't have to be terminal if the sentence carries on after the quote.</p> <p>\"Should I write it like this?\" he asked. \"Or perhaps like this?\"</p> <p>The other class of exception is for what are probably really parenthetical comments. If you have a short phrase that you could have put aside in parentheses or dashes, then a question mark or exclamation mark can be used at the end of that phrase without ending the sentence. Be sparing with this. It looks wrong at a first read.</p> <p>Should I write it like this, or abracadabra! like this?</p> <p>When joining many questions you might may have the doubt of which of the following is correct:</p> <p>Should I write it like this? Or perhaps like this? Should I write it like this? or perhaps like this? Should I write it like this, or perhaps like that? \"Should I write it like this?\" he asked, \"or perhaps like that?\"</p> <p>The second with the lowercase <code>or</code> is just plain wrong.</p> <p>Crusty old grammarians who disapprove of starting sentences with conjunctions may frown at example 1 all they like, but it's a perfectly acceptable fragmentary sentence. Whether it's the right answer or not is another question entirely. Example 1 makes the point that the questions are distinct, though they are strongly linked otherwise the whole structure wouldn't work.</p> <p>Example 3 on the other hand emphasizes that the two questions are options in a common situation, as well as reflecting a different way of saying them. That is clear in this case because the two questions are tightly coupled alternatives. However, consider the following:</p> <p>Are the lights green? Or is the switch up? Are the lights green, or is the switch up?</p> <p>Both of these examples imply that the state of the lights and the state of the switch are related somehow. Version 2 couples them more tightly; I would usually assume (without more context) that either this is the same question being asked in two different ways (i.e. that the switch being up should cause the lights to be green), or that they are an exhaustive list of possibilities (either the switch is up or the lights are green, but not both or neither). This isn't an absolute rule, but it's quite strongly implied.</p> <p>Example 4 is also wrong, though it has a better disguise. If you unwrap the quotes, what you get is:</p> <p>Should I write it like this? or perhaps like this?</p> <p>Which is example 2 back again. What you actually want is one of:</p> <p>\"Should I write it like this?\" he asked. \"Or perhaps like this?\" (i.e. example 1) \"Should I write it like this,\" he asked, \"or perhaps like that? (i.e. example 3)</p> <p>Exclamation marks work like question marks for this purpose. Semicolons don't; they end a clause, not a sentence.</p>"}, {"location": "writing/orthography/#when-to-write-apostrophes-before-an-s", "title": "When to write Apostrophes before an s", "text": "<ul> <li>For most singular nouns, add <code>apostrophe + s</code>: The writer's desk.</li> <li>For most plural nouns, add <code>apostrophe</code>: The writers' desk (multiple writers).</li> <li>For plural nouns that do not end in s, add <code>apostrophe + s</code>: The geese's     migration route.</li> <li>For singular proper nouns both <code>apostrophe</code> and <code>apostrophe + s</code> is accepted,     but as the plural proper nouns ending in s, the correct form is <code>apostrophe</code>     I'd use that for both, so: Charles Dickens' novels and The     Smiths' vacation.</li> </ul> <p>The personal pronouns, do not have apostrophes to form possessives, such as your, yours, hers, its, ours, their, whose, and theirs. In fact, for some of these pronouns, adding an apostrophe forms a contraction instead of a possessive.</p>"}, {"location": "writing/orthography/#who-vs-whom", "title": "Who vs Whom", "text": "<p>If you can replace the word with she or he, use who. If you can replace it with her or him, use whom.</p> <ul> <li>Who: Should be used to refer to the subject of a sentence.</li> <li>Whom: Should be used to refer to the object of a verb or preposition.</li> </ul>"}, {"location": "writing/orthography/#a-vs-an", "title": "A vs An", "text": "<p>We were all taught that a precedes a word starting with a consonant and that an precedes a word starting with a vowel (a, e, i, o, u, and sometimes y). But what matters is the sound of the letter beginning the word, not just the letter itself. The way we say the word will determine whether or not we use a or an.</p> <p>If the word begins with a vowel sound, you must use an. If it begins with a consonant sound, you must use a.</p>"}, {"location": "writing/orthography/#comma-before-and", "title": "Comma before and", "text": "<p>There are two cases:</p> <ul> <li>It's required to put a comma before and when it\u2019s connecting two independent clauses.</li> <li>It\u2019s almost always     optional     the use of comma before and in lists. This case is also known as serial     commas or Oxford commas.</li> </ul> <p>Since in some cases is useful, I'm going to use them to reduce the mental load.</p>"}, {"location": "writing/orthography/#bear-with-me-or-bare-with-me", "title": "Bear with me or Bare with me", "text": "<p>\"Bear with me\" is the correct form.</p>"}, {"location": "writing/orthography/#references", "title": "References", "text": "<ul> <li> <p>Julia Olech article on     grammar,     even though is a bit sensational and I don't like the overall tone, it has     good insights on common grammar mistakes. Thanks for the link Dave :)</p> </li> <li> <p>JamesESL videos</p> </li> </ul>"}, {"location": "writing/writing/", "title": "Writing", "text": "<p>Writing is difficult, at least for me. Even more if you aren't using your native tongue. I'm focusing my efforts in improving my grammar and orthography and writing style.</p>"}, {"location": "writing/writing/#tests", "title": "Tests", "text": "<p>Using automatic tools that highlight the violations of the previous principles may help you to internalize all the measures, even more with the new ones.</p> <p>Configure your editor to:</p> <ul> <li>Run a spell checker that you can check as you write.</li> <li>Alert you on new orthography rules you want to adopt.</li> <li>Use linters to raise your awareness on the rest of issues.<ul> <li>alex to find gender favoring, polarizing, race related, religion     inconsiderate, or other unequal phrasing in text.</li> <li>markdownlint: style checker and lint tool for     Markdown/CommonMark files.</li> <li>proselint: Is another linter for prose.</li> <li>write-good is a naive linter for English     prose.</li> </ul> </li> <li>Use formatters to make your writing experience more     pleasant.<ul> <li>mdformat: I haven't tested it yet,     but looks promising.</li> </ul> </li> </ul> <p>There are some checks that I wasn't able to adopt:</p> <ul> <li>Try to use less than 30 words per sentence.</li> <li>Check that every sentence is ended with a dot.</li> <li>Be consistent across document structures, use References instead of Links, or     Installation instead of Install.</li> <li>gwern markdown-lint.sh script     file.</li> <li>Avoid the use of here, use descriptive link text.</li> <li>Rotten links: use linkchecker (I think   there was a mkdocs plugin to do this). Also read how to archive   urls.</li> <li>check for use of the word \"significant\"/\"significance\" and insert   \"[statistically]\" as appropriate (to disambiguate between effect sizes and   statistical significance; this common confusion is one reason for   \"statistical-significance considered harmful\")</li> </ul>"}, {"location": "writing/writing/#vim-enhancements", "title": "Vim enhancements", "text": "<ul> <li>vim-pencil looks promising but it's still not ready</li> <li>mdnav opens links to urls or files when     pressing <code>enter</code> in normal mode over a markdown link, similar to <code>gx</code> but     more powerful. I specially like the ability of following <code>[self referencing     link][]</code> links, that allows storing the links at the bottom.</li> </ul>"}, {"location": "writing/writing/#writing-workflow", "title": "Writing workflow", "text": "<ul> <li>Start with a template.</li> <li>Use synoptical reading to gather ideas in an unconnected thoughts section.</li> <li>Once you've got several refactor them in sections with markdown headers.</li> <li>Ideally you'll want to wrap your whole blog post into a story or timeline.</li> <li>Add an abstract so the reader may decide if she wants to read it.</li> </ul>"}, {"location": "writing/writing/#publication", "title": "Publication", "text": "<ul> <li>Think how to publicize:</li> <li>Hacker News</li> <li>Reddit</li> <li>LessWrong (and further sites as appropriate)</li> </ul>"}, {"location": "writing/writing/#references", "title": "References", "text": "<p>Awesome:</p> <ul> <li>Nikita's writing notes</li> <li>Gwern's writing checklist</li> </ul> <p>Good:</p> <ul> <li>Long Naomi pen post with some key   ideas</li> </ul>"}, {"location": "writing/writing/#doing", "title": "Doing", "text": "<ul> <li>https://github.com/mnielsen/notes-on-writing/blob/master/notes_on_writing.md#readme</li> </ul>"}, {"location": "writing/writing/#todo", "title": "Todo", "text": "<ul> <li>https://www.scottadamssays.com/2015/08/22/the-day-you-became-a-better-writer-2nd-look/</li> <li>https://blog.stephsmith.io/learning-to-write-with-confidence/</li> <li>https://styleguide.mailchimp.com/tldr/</li> <li>https://content-guide.18f.gov/inclusive-language/</li> <li>https://performancejs.com/post/31b361c/13-Tips-for-Writing-a-Technical-Book</li> <li>https://github.com/RacheltheEditor/ProductionGuide#readme</li> <li>https://mkaz.blog/misc/notes-on-technical-writing/</li> <li>https://www.swyx.io/writing/cfp-advice/</li> <li>https://sivers.org/d22</li> <li>https://homes.cs.washington.edu/~mernst/advice/write-technical-paper.html</li> <li>Investigate on readability tests:</li> <li>Definition</li> <li>Introduction on Readability</li> <li>List of readability tests and formulas</li> <li>An example of a formula</li> </ul>"}, {"location": "writing/writing/#vim-plugins", "title": "Vim plugins", "text": "<ul> <li>Vim-lexical</li> <li>Vim-textobj-quote</li> <li>Vim-textobj-sentence</li> <li>Vim-ditto</li> <li>Vim-exchange</li> </ul>"}, {"location": "writing/writing/#books", "title": "Books", "text": "<ul> <li>https://www.amazon.de/dp/0060891548/ref=as_li_tl?ie=UTF8&amp;linkCode=gs2&amp;linkId=39f2ab8ab47769b2a106e9667149df30&amp;creativeASIN=0060891548&amp;tag=gregdoesit03-21&amp;creative=9325&amp;camp=1789</li> <li>https://www.amazon.de/dp/0143127799/ref=as_li_tl?ie=UTF8&amp;linkCode=gs2&amp;linkId=ff9322c17ca288b1d9d6b5fb8d6df619&amp;creativeASIN=0143127799&amp;tag=gregdoesit03-21&amp;creative=9325&amp;camp=1789</li> </ul>"}]}