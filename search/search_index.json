{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"This is my personal digital garden where I share everything I learn about a huge variety of topics, from antifascism and other forms of activism , to life management , health , Python , DevOps , software architecture , writing , dancing , or data analysis . Unlike in common blogging where you write an article and forget about it, posts are treated as plants in various stages of growth and nurturing. Some might wither and die, and others will flourish and provide a source of continued knowledge for the gardener and folks in the community that visit. To follow the updates of this site, subscribe to any of the RSS feeds . Visiting the garden \u2691 If this is your first visit, welcome!, you may be overwhelmed by the amount of content and don't know where to start reading. Start with the first article that grabs your attention of the navigation tree on the left and be ready to follow the internal links to read the rest. Or you can use it as a reference, by using the top search field or by cloning the git repository and using grep like tools. Make your own digital garden \u2691 Don't be afraid to create one of your own and share what you know with the world. Or contribute to existing ones, I would love to see the blue-book maintained by other people too. If you don't want to start from scratch, you can fork the blue book and start writing straight away. You can view other similar gardens to get inspiration. History \u2691 I've tried writing blogs in the past, but it doesn't work for me. I can't stand the idea of having to save some time to sit and write a full article of something I've done. I need to document at the same time as I develop or learn. As I usually use incremental reading or work on several projects, I don't write one article, but improve several at the same time in a unordered way. That's why I embrace Gwern's Long Content principle . The only drawback of this format is that there is no friendly way for a reader to keep updated with the changes, that's why I created mkdocs-newsletter . In 2016 I started writing in text files summaries of different concepts, how to install or how to use tools. In the beginning it was plaintext, then came Markdown , then Asciidoc , I did several refactors to reorder the different articles based in different structured ways, but I always did it with myself as the only target audience. Three years, 7422 articles and almost 50 million lines later, I found Gwern's website and Nikita's wiki , which made me think that it was time to do another refactor to give my wiki a semantical structure, go beyond a simple reference making it readable and open it to the internet. And the blue book was born. Contributing \u2691 If you find a mistake or want to add new content, please make the changes. You can use the edit button on the top right of any article to add them in a pull request, if you don't know what that means, you can always open an issue or send me an email . Thank you \u2691 If you liked my book and want to show your support, please see if you know how can I fulfill any item of my wish list or contribute to my other projects .","title":"Introduction"},{"location":"#visiting-the-garden","text":"If this is your first visit, welcome!, you may be overwhelmed by the amount of content and don't know where to start reading. Start with the first article that grabs your attention of the navigation tree on the left and be ready to follow the internal links to read the rest. Or you can use it as a reference, by using the top search field or by cloning the git repository and using grep like tools.","title":"Visiting the garden"},{"location":"#make-your-own-digital-garden","text":"Don't be afraid to create one of your own and share what you know with the world. Or contribute to existing ones, I would love to see the blue-book maintained by other people too. If you don't want to start from scratch, you can fork the blue book and start writing straight away. You can view other similar gardens to get inspiration.","title":"Make your own digital garden"},{"location":"#history","text":"I've tried writing blogs in the past, but it doesn't work for me. I can't stand the idea of having to save some time to sit and write a full article of something I've done. I need to document at the same time as I develop or learn. As I usually use incremental reading or work on several projects, I don't write one article, but improve several at the same time in a unordered way. That's why I embrace Gwern's Long Content principle . The only drawback of this format is that there is no friendly way for a reader to keep updated with the changes, that's why I created mkdocs-newsletter . In 2016 I started writing in text files summaries of different concepts, how to install or how to use tools. In the beginning it was plaintext, then came Markdown , then Asciidoc , I did several refactors to reorder the different articles based in different structured ways, but I always did it with myself as the only target audience. Three years, 7422 articles and almost 50 million lines later, I found Gwern's website and Nikita's wiki , which made me think that it was time to do another refactor to give my wiki a semantical structure, go beyond a simple reference making it readable and open it to the internet. And the blue book was born.","title":"History"},{"location":"#contributing","text":"If you find a mistake or want to add new content, please make the changes. You can use the edit button on the top right of any article to add them in a pull request, if you don't know what that means, you can always open an issue or send me an email .","title":"Contributing"},{"location":"#thank-you","text":"If you liked my book and want to show your support, please see if you know how can I fulfill any item of my wish list or contribute to my other projects .","title":"Thank you"},{"location":"activitywatch/","text":"ActivityWatch is a bundle of software that tracks your computer activity. You are, by default, the sole owner of your data. ActivityWatch is: A set of watchers that record relevant information about what you do and what happens on your computer (such as if you are AFK or not, or which window is currently active). A way of storing data collected by the watchers. A dataformat accomodating most logging needs due to its flexibility. An ecosystem of tools to help users extend the software to fit their needs. Installation \u2691 Download the latest release Unpack it and move it for example to ~/.local/bin/activitywatch . Add the aw-qt executable to the autostart. It will start the web interface at http://localhost:5600 and will capture the data. Configuration \u2691 First go to the settings page of the Web UI, you can define there the rules for the categories. More advanced settings can be changed on the files, but I had no need to go there yet. The used directories are: Data: ~/.local/share/activitywatch . Config: ~/.config/activitywatch . Logs: ~/.cache/activitywatch/log . Cache: ~/.cache/activitywatch . Watchers \u2691 By default ActivityWatch comes with the next watchers: aw-watcher-afk : Watches for mouse & keyboard activity to detect if the user is active. aw-watcher-window : Watches the active window and its title. But you can add more, such as: aw-watcher-web : The official browser extension, supports Chrome and Firefox. Watches properties of the active tab like title, URL, and incognito state. It doesn't work if you Configure it to Never remember history , or if you use incognito mode It's known not to be very accurate . The overall time spent in the browser shown by the aw-watcher-window is greater than the one shown in aw-watcher-web-firefox . aw-watcher-vim : Watches the actively edited file and associated metadata like path, language, and project name (folder name of git root). It's impressive, plug and play: It still doesn't add the branch information , it could be useful to give hints of what task you're working on inside a project. They even show you how to create your own watcher . Syncing \u2691 There is currently no syncing support . You'll need to export the data (under Raw Data , Export all buckets as JSON ), and either tweak it so it can be imported, or analyze the data through other processes. Issues \u2691 Syncing support : See how to merge the data from the different devices. Firefox not logging data : Once it's solved, try it again. Making it work in incognito mode : Try it once it's solved. Add branch information in vim watcher : try it once it's out. Web tracking is not accurate : Test the solution once it's implemented. Physical Activity Monitor Integration (gadgetbridge) : Try it once there is a solution. References \u2691 Home Docs Git","title":"ActivityWatch"},{"location":"activitywatch/#installation","text":"Download the latest release Unpack it and move it for example to ~/.local/bin/activitywatch . Add the aw-qt executable to the autostart. It will start the web interface at http://localhost:5600 and will capture the data.","title":"Installation"},{"location":"activitywatch/#configuration","text":"First go to the settings page of the Web UI, you can define there the rules for the categories. More advanced settings can be changed on the files, but I had no need to go there yet. The used directories are: Data: ~/.local/share/activitywatch . Config: ~/.config/activitywatch . Logs: ~/.cache/activitywatch/log . Cache: ~/.cache/activitywatch .","title":"Configuration"},{"location":"activitywatch/#watchers","text":"By default ActivityWatch comes with the next watchers: aw-watcher-afk : Watches for mouse & keyboard activity to detect if the user is active. aw-watcher-window : Watches the active window and its title. But you can add more, such as: aw-watcher-web : The official browser extension, supports Chrome and Firefox. Watches properties of the active tab like title, URL, and incognito state. It doesn't work if you Configure it to Never remember history , or if you use incognito mode It's known not to be very accurate . The overall time spent in the browser shown by the aw-watcher-window is greater than the one shown in aw-watcher-web-firefox . aw-watcher-vim : Watches the actively edited file and associated metadata like path, language, and project name (folder name of git root). It's impressive, plug and play: It still doesn't add the branch information , it could be useful to give hints of what task you're working on inside a project. They even show you how to create your own watcher .","title":"Watchers"},{"location":"activitywatch/#syncing","text":"There is currently no syncing support . You'll need to export the data (under Raw Data , Export all buckets as JSON ), and either tweak it so it can be imported, or analyze the data through other processes.","title":"Syncing"},{"location":"activitywatch/#issues","text":"Syncing support : See how to merge the data from the different devices. Firefox not logging data : Once it's solved, try it again. Making it work in incognito mode : Try it once it's solved. Add branch information in vim watcher : try it once it's out. Web tracking is not accurate : Test the solution once it's implemented. Physical Activity Monitor Integration (gadgetbridge) : Try it once there is a solution.","title":"Issues"},{"location":"activitywatch/#references","text":"Home Docs Git","title":"References"},{"location":"adr/","text":"ADR are short text documents that captures an important architectural decision made along with its context and consequences. The whole document should be one or two pages long. Written as if it is a conversation with a future developer. This requires good writing style, with full sentences organized into paragraphs. Bullets are acceptable only for visual style, not as an excuse for writing sentence fragments. Pros: We have a clear log of the different decisions taken, which can help newcomers to understand past decisions. It can help in the discussion of such changes. Architecture decisions recorded in small modular readable documents. Cons: More time is required for each change, as we need to document and discuss it. How to use them \u2691 We will keep a collection of architecturally significant decisions, those that affect the structure, non-functional characteristics, dependencies, interfaces or construction techniques. There are different templates you can start with, being the most popular Michael Nygard's one . The documents are stored in the project repository under the doc/arch directory, with a name convention of NNN-title_with_underscores.md , where NNN is a monotonically increasing number. If a decision is reversed, we'll keep the old one around, but mark it as superseded, as it's still relevant to know that it was a decision, but is no longer. ADR template \u2691 Using Michael Nygard's template as a starting point, I'm going to use these sections: Title : A short noun phrase that describes the change. For example, \"ADR 1: Deployment on Ruby on Rails 3.0.10\". Date : Creation date of the document. Status : The ADRs go through the following phases: Draft : We are using the ADR to build the idea, so everything can change. Proposed : We have a solid proposal on the Decision to solve the Context. Accepted : We have agreed on the proposal, from now on the document can't be changed! Rejected : We have agreed not to solve the Context. Deprecated : The Context no longer applies, so the solution is no longer needed. Superseded : We have found another ADR that better solves the Context. Context : This section describes the situation we're trying to solve, including technological, political, social, and project local aspects. The language in this section is neutral, simply describing facts. Proposals : Analysis of the different solutions for the situation defined in the Context. Decision : Clear summary of the selected proposal. It is stated in full sentences, with active voice. Consequences : Description of the resulting context, after applying the decision. All consequences should be listed here, not just the positive ones. I'm using the following Ultisnip vim snippet: snippet adr \"ADR\" Date: `date + %Y - % m - % d ` # Status <!-- What is the status? Draft , Proposed , Accepted , Rejected , Deprecated or Superseded? --> $ 1 # Context <!-- What is the issue that we' re seeing that is motivating this decision or change? --> $ 0 # Proposals <!-- What are the possible solutions to the problem described in the context --> # Decision <!-- What is the change that we' re proposing and/or doing? --> # Consequences <!-- What becomes easier or more difficult to do because of this change? --> endsnippet Usage in a project \u2691 When starting a project, I'll do it by the ADRs, that way you evaluate the problem, structure the idea and leave a record of your initial train of thought. I found useful to: Define the general problem at high level in 001-high_level_problem_analysis.md . Describe the problem you want to solve in the Context. Reflect the key points to solve the problem at the start of the Proposals section. Go one by one analyzing possible outcomes trying not to dive deep into details and having at least two proposals for each key point (hard!). Build an initial proposal in the Decision section by reviewing that all the Context points have been addressed and summarizing each of the Proposal key points' outcomes. Review the positive and negative Consequences for each actor involved with the solution, such as: The final user that is going to consume the outcome. The middle user that is going to host and maintain the solution. Ourselves as developers. Use the problem definition of 001 and draft the phases of the solution at 002 . Create another ADR for each of the phases, getting a level closer to the final implementation. Use 00X for the early drafts. Once you give it a number try not to change the file name, or you'll need to manually update the references you make. As the project starts to grow, the relationships between the ADRs will get more complex, it's useful to create an ADR landing page, where the user can follow the logic between them. MermaidJS can be used to create a nice diagram that shows this information. In the mkdocs-newsletter I've used the next structure: graph TD 001[001: High level analysis] 002[002: Initial MkDocs plugin design] 003[003: Selected changes to record] 004[004: Article newsletter structure] 005[005: Article newsletter creation] 001 -- Extended --> 002 002 -- Extended --> 003 002 -- Extended --> 004 002 -- Extended --> 005 003 -- Extended --> 004 004 -- Extended --> 005 click 001 \"https://lyz-code.github.io/mkdocs-newsletter/adr/001-initial_approach\" _blank click 002 \"https://lyz-code.github.io/mkdocs-newsletter/adr/002-initial_plugin_design\" _blank click 003 \"https://lyz-code.github.io/mkdocs-newsletter/adr/003-select_the_changes_to_record\" _blank click 004 \"https://lyz-code.github.io/mkdocs-newsletter/adr/004-article_newsletter_structure\" _blank click 005 \"https://lyz-code.github.io/mkdocs-newsletter/adr/005-create_the_newsletter_articles\" _blank 001:::accepted 002:::accepted 003:::accepted 004:::accepted 005:::accepted classDef draft fill:#CDBFEA; classDef proposed fill:#B1CCE8; classDef accepted fill:#B1E8BA; classDef rejected fill:#E8B1B1; classDef deprecated fill:#E8B1B1; classDef superseeded fill:#E8E5B1; Where we define: The nodes with their title. The relationship between the ADRs. The link to the ADR article so it can be clicked. The state of the ADR. Tools \u2691 Although adr-tools exist, I feel it's an overkill to create new documents and search on an existing codebase. We are now used to using other tools for the similar purpose, like Vim or grep. References \u2691 Joel Parker guide on ADRs Michael Nygard post on ARDs","title":"Architecture Decision Record"},{"location":"adr/#how-to-use-them","text":"We will keep a collection of architecturally significant decisions, those that affect the structure, non-functional characteristics, dependencies, interfaces or construction techniques. There are different templates you can start with, being the most popular Michael Nygard's one . The documents are stored in the project repository under the doc/arch directory, with a name convention of NNN-title_with_underscores.md , where NNN is a monotonically increasing number. If a decision is reversed, we'll keep the old one around, but mark it as superseded, as it's still relevant to know that it was a decision, but is no longer.","title":"How to use them"},{"location":"adr/#adr-template","text":"Using Michael Nygard's template as a starting point, I'm going to use these sections: Title : A short noun phrase that describes the change. For example, \"ADR 1: Deployment on Ruby on Rails 3.0.10\". Date : Creation date of the document. Status : The ADRs go through the following phases: Draft : We are using the ADR to build the idea, so everything can change. Proposed : We have a solid proposal on the Decision to solve the Context. Accepted : We have agreed on the proposal, from now on the document can't be changed! Rejected : We have agreed not to solve the Context. Deprecated : The Context no longer applies, so the solution is no longer needed. Superseded : We have found another ADR that better solves the Context. Context : This section describes the situation we're trying to solve, including technological, political, social, and project local aspects. The language in this section is neutral, simply describing facts. Proposals : Analysis of the different solutions for the situation defined in the Context. Decision : Clear summary of the selected proposal. It is stated in full sentences, with active voice. Consequences : Description of the resulting context, after applying the decision. All consequences should be listed here, not just the positive ones. I'm using the following Ultisnip vim snippet: snippet adr \"ADR\" Date: `date + %Y - % m - % d ` # Status <!-- What is the status? Draft , Proposed , Accepted , Rejected , Deprecated or Superseded? --> $ 1 # Context <!-- What is the issue that we' re seeing that is motivating this decision or change? --> $ 0 # Proposals <!-- What are the possible solutions to the problem described in the context --> # Decision <!-- What is the change that we' re proposing and/or doing? --> # Consequences <!-- What becomes easier or more difficult to do because of this change? --> endsnippet","title":"ADR template"},{"location":"adr/#usage-in-a-project","text":"When starting a project, I'll do it by the ADRs, that way you evaluate the problem, structure the idea and leave a record of your initial train of thought. I found useful to: Define the general problem at high level in 001-high_level_problem_analysis.md . Describe the problem you want to solve in the Context. Reflect the key points to solve the problem at the start of the Proposals section. Go one by one analyzing possible outcomes trying not to dive deep into details and having at least two proposals for each key point (hard!). Build an initial proposal in the Decision section by reviewing that all the Context points have been addressed and summarizing each of the Proposal key points' outcomes. Review the positive and negative Consequences for each actor involved with the solution, such as: The final user that is going to consume the outcome. The middle user that is going to host and maintain the solution. Ourselves as developers. Use the problem definition of 001 and draft the phases of the solution at 002 . Create another ADR for each of the phases, getting a level closer to the final implementation. Use 00X for the early drafts. Once you give it a number try not to change the file name, or you'll need to manually update the references you make. As the project starts to grow, the relationships between the ADRs will get more complex, it's useful to create an ADR landing page, where the user can follow the logic between them. MermaidJS can be used to create a nice diagram that shows this information. In the mkdocs-newsletter I've used the next structure: graph TD 001[001: High level analysis] 002[002: Initial MkDocs plugin design] 003[003: Selected changes to record] 004[004: Article newsletter structure] 005[005: Article newsletter creation] 001 -- Extended --> 002 002 -- Extended --> 003 002 -- Extended --> 004 002 -- Extended --> 005 003 -- Extended --> 004 004 -- Extended --> 005 click 001 \"https://lyz-code.github.io/mkdocs-newsletter/adr/001-initial_approach\" _blank click 002 \"https://lyz-code.github.io/mkdocs-newsletter/adr/002-initial_plugin_design\" _blank click 003 \"https://lyz-code.github.io/mkdocs-newsletter/adr/003-select_the_changes_to_record\" _blank click 004 \"https://lyz-code.github.io/mkdocs-newsletter/adr/004-article_newsletter_structure\" _blank click 005 \"https://lyz-code.github.io/mkdocs-newsletter/adr/005-create_the_newsletter_articles\" _blank 001:::accepted 002:::accepted 003:::accepted 004:::accepted 005:::accepted classDef draft fill:#CDBFEA; classDef proposed fill:#B1CCE8; classDef accepted fill:#B1E8BA; classDef rejected fill:#E8B1B1; classDef deprecated fill:#E8B1B1; classDef superseeded fill:#E8E5B1; Where we define: The nodes with their title. The relationship between the ADRs. The link to the ADR article so it can be clicked. The state of the ADR.","title":"Usage in a project"},{"location":"adr/#tools","text":"Although adr-tools exist, I feel it's an overkill to create new documents and search on an existing codebase. We are now used to using other tools for the similar purpose, like Vim or grep.","title":"Tools"},{"location":"adr/#references","text":"Joel Parker guide on ADRs Michael Nygard post on ARDs","title":"References"},{"location":"afew/","text":"afew is an initial tagging script for notmuch mail . Its basic task is to provide automatic tagging each time new mail is registered with notmuch . In a classic setup, you might call it after notmuch new in an offlineimap post sync hook. It can do basic thing such as adding tags based on email headers or maildir folders, handling killed threads and spam. In move mode, afew will move mails between maildir folders according to configurable rules that can contain arbitrary notmuch queries to match against any searchable attributes. Installation \u2691 First install the requirements: sudo apt-get install notmuch python-notmuch python-dev python-setuptools Then configure notmuch . Finally install the program: pip3 install afew Usage \u2691 To tag new emails use: afew -v --tag --new References \u2691 Git Docs","title":"afew"},{"location":"afew/#installation","text":"First install the requirements: sudo apt-get install notmuch python-notmuch python-dev python-setuptools Then configure notmuch . Finally install the program: pip3 install afew","title":"Installation"},{"location":"afew/#usage","text":"To tag new emails use: afew -v --tag --new","title":"Usage"},{"location":"afew/#references","text":"Git Docs","title":"References"},{"location":"alot/","text":"alot is a terminal-based mail user agent based on the notmuch mail indexer . It is written in python using the urwid toolkit and features a modular and command prompt driven interface to provide a full MUA experience. Installation \u2691 sudo apt-get install alot Configuration \u2691 Alot reads the INI config file ~/.config/alot/config . That file is not created by default, if you don't want to start from scratch, you can use pazz's alot configuration , in particular the [accounts] section. UI interaction \u2691 Basic movement is done with: Move up and down: j / k , arrows and page up and page down. Cancel prompts: Escape Select highlighted element: Enter . Update buffer: @ . The interface shows one buffer at a time, basic buffer management is done with: Change buffer: Tab and Shift-Tab . Close the current buffer: d List all buffers: ; . The buffer type or mode (displayed at the bottom left) determines which prompt commands are available. Usage information on any command can be listed by typing help YOURCOMMAND to the prompt. The key bindings for the current mode are listed upon pressing ? . You can always run commands with : . Troubleshooting \u2691 Remove emails \u2691 Say you want to remove emails from the provider's server but keep them in the notmuch database. There is no straight way to do it, you need to tag them with a special tag like deleted and then remove them from the server with a post-hook. Theme not found \u2691 I don't know why but apt-get didn't install the default themes, you need to create the ~/.config/alot/themes and copy the contents of the themes directory . References \u2691 Git Docs Wiki FAQ","title":"alot"},{"location":"alot/#installation","text":"sudo apt-get install alot","title":"Installation"},{"location":"alot/#configuration","text":"Alot reads the INI config file ~/.config/alot/config . That file is not created by default, if you don't want to start from scratch, you can use pazz's alot configuration , in particular the [accounts] section.","title":"Configuration"},{"location":"alot/#ui-interaction","text":"Basic movement is done with: Move up and down: j / k , arrows and page up and page down. Cancel prompts: Escape Select highlighted element: Enter . Update buffer: @ . The interface shows one buffer at a time, basic buffer management is done with: Change buffer: Tab and Shift-Tab . Close the current buffer: d List all buffers: ; . The buffer type or mode (displayed at the bottom left) determines which prompt commands are available. Usage information on any command can be listed by typing help YOURCOMMAND to the prompt. The key bindings for the current mode are listed upon pressing ? . You can always run commands with : .","title":"UI interaction"},{"location":"alot/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"alot/#remove-emails","text":"Say you want to remove emails from the provider's server but keep them in the notmuch database. There is no straight way to do it, you need to tag them with a special tag like deleted and then remove them from the server with a post-hook.","title":"Remove emails"},{"location":"alot/#theme-not-found","text":"I don't know why but apt-get didn't install the default themes, you need to create the ~/.config/alot/themes and copy the contents of the themes directory .","title":"Theme not found"},{"location":"alot/#references","text":"Git Docs Wiki FAQ","title":"References"},{"location":"amazfit_band_5/","text":"Amazfit Band 5 it's the affordable fitness tracker I chose to buy because: It's supported by gadgetbridge . It has a SpO2 sensor which enhances the quality of the sleep metrics It has sleep metrics, not only time but also type of sleep (light, deep, REM). It has support with Alexa, not that I'd use that, but it would be cool if once I've got my personal voice assistant , I can use it through the band. Sleep detection quality \u2691 The sleep tracking using Gadgetbridge is not good at all . After two nights, the band has not been able to detect when I woke in the middle of the night, or when I really woke up, as I usually stay in the bed for a time before standing up. I'll try with the proprietary application soon and compare results. If it doesn't work either, I might think of getting a specific device like withings sleep analyzer which seems to have much more accuracy and useful insights. I've sent them an email to see if it's possible to extract the data before it reach their servers, and they confirmed that there is no way. Maybe you can route the requests to their servers to one of your own, bring up an http server and reverse engineer the communication. Karlicoss, the author of the awesome HPI uses the Emfit QS , so that could be another option. Firmware updates \u2691 Gadgetbridge people have a guide on how to upgrade the firmware , you need to get the firmware from the geek doing forum though, so it is interesting to create an account and watch the post.","title":"Amazfit Band 5"},{"location":"amazfit_band_5/#sleep-detection-quality","text":"The sleep tracking using Gadgetbridge is not good at all . After two nights, the band has not been able to detect when I woke in the middle of the night, or when I really woke up, as I usually stay in the bed for a time before standing up. I'll try with the proprietary application soon and compare results. If it doesn't work either, I might think of getting a specific device like withings sleep analyzer which seems to have much more accuracy and useful insights. I've sent them an email to see if it's possible to extract the data before it reach their servers, and they confirmed that there is no way. Maybe you can route the requests to their servers to one of your own, bring up an http server and reverse engineer the communication. Karlicoss, the author of the awesome HPI uses the Emfit QS , so that could be another option.","title":"Sleep detection quality"},{"location":"amazfit_band_5/#firmware-updates","text":"Gadgetbridge people have a guide on how to upgrade the firmware , you need to get the firmware from the geek doing forum though, so it is interesting to create an account and watch the post.","title":"Firmware updates"},{"location":"anonymous_feedback/","text":"Anonymous Feedback is a communication tool where people share feedback to teammates or other organizational members while protecting their identities. Why would you need anonymous feedback? \u2691 Ideally, everyone in your company should be able to give feedback publicly and not anonymously. They should share constructive criticism and not shy away from direct feedback if they believe and trust that their opinions will be heard and addressed. However, to achieve this ideal, people need to feel that they are in a safe space , a place or environment in which they feel confident that they will not be exposed to discrimination, criticism, harassment, or any other emotional or physical harm. The work place is usually not considered a safe space by the employees because they may: Fear of being judged : We want people to like us and not just in our personal lives, but in our professional lives as well. It also seems to bear a bigger importance that our supervisor likes us because he holds the power over our career and financial security. So we live in a constant state of anxiety of what might happen if our manager doesn't like us. Fear of losing their job : It\u2019s a form of self-preservation, abstaining from saying something that may be perceived as wrong to someone in a position of authority. Fear of being singled out : Giving direct feedback puts you in the spotlight. Being highlighted against the rest of the employees might be seen as a threat, especially by people belonging to a different race, gender, national origin, or other identities than most of their coworkers. Feel insecure : People may distrust their colleagues, because they just arrived at the organization or may have negative past experiences either with them or with similar people. They may not have a solid stance on an issue, be shy or have problems of self esteem. Distrust the open-door internal policies : Past experiences in other companies may lead the employee not to trust open-doors policies until they have seen them in practice. Not knowing the internal processes of the organization : As a Slack study shows , 55 percent of business owners described their organization as very transparent, but only 18 percent of their employees would agree. For all these reasons, some employees may remain silent when asked for direct feedback, to speak up against an internal issue or in need to report a colleague or manager. These factors are further amplified if: The person belongs to a minority group inside the organization. The greater the difference in position between the talking parties. It's more difficult to talk to the CEO than to the immediate manager. Until the safe space is built where direct feedback is viable, anonymous feedback gives these employees a mechanism to raise their concerns, practice their feedback-giving skills, test the waters, and understand how people perceive their constructive (and sometimes critical) opinions, thus building the needed trust. Pros and cons \u2691 Pros of Anonymous Feedback: Employees can express themselves freely and provide valuable insights : On topics that are considered sensitive, you\u2019ll often find employees who are afraid to share their opinions. But when employees have the option to use anonymous feedback, you will be offering a safe space for them to share their honest, constructive feedback about sensitive workplace issues, without fear of being judged, victimized, radicalized or labelled in any way. A formal, non-anonymous feedback form will only reveal some of the superficial, non-threatening issues that affect the workplace, without mentioning the most important, underlying problems. The real problems that no one talks about because they know they are so important that they could stir things up. In fact, these controversial, important issues are the ones that need to be brought to the table as soon as possible. They should be addressed by the entire team before they become a source of unhappiness, conflict and lack of productivity. An anonymous feedback instrument gives you real power over those issues because it doesn't matter who brought it up, but that it\u2019s resolved. For a manager, that insight is invaluable. It builds trust : Anonymous feedback allows the employee see how management reacts to feedback, understand how people perceive their constructive (and sometimes critical) opinions, how are the open-door policies being applied and build up self esteem. It offers a sense of security : Anonymity soothes the employee anxiety and creates a greater willingness to share our ideas and opinions. It allows every voice to be heard and respected : In workplaces, where they practice direct or attributed feedback, leaders may give preference to some voices over others. Due to our unconscious biases, people of higher authority, backgrounds, or eloquence tend to command respect and attention. In such situations, the issues they raise are likely to get immediate attention than those raised by the rest of the group. However, when feedback is collected anonymously, it eliminates biases and allows leaders to focus entirely on the feedback. It encourages new employees to share their opinions : Research has shown that new employees, who happen to be less senior or influential, see anonymous feedback as more appropriate for formal and informal evaluations than their older colleagues. Typically, the last thing a new employee wants is to start on the wrong foot, so they maintain a neutral stance. Using anonymous feedback can make new employees feel more comfortable sharing their real opinions on workplace issues. Cons of Anonymous Feedback: It can breed hostility : According to this Harvard Business Review article , anonymity often sets off a \u201cwitch hunt\u201d, where leaders seek to know the source of a negative comment. On the one hand, employees can hide behind anonymity to say personal and hurtful things about their colleagues or leaders. On the other hand, leaders may take constructive feedback as a personal attack and become suspicious and hostile to all their employees. It can be less impactful than attributed feedback : When using attributed feedback where responses carry the employees\u2019 names, information can be analyzed for relevance and impact. However, with anonymous feedback, it can be difficult to analyze information accurately. It is not uncommon for companies who choose to practice anonymous feedback, to find less specific responses since details may reveal respondents\u2019 identities. Vague feedback from employees would have less power to influence behaviors or drive change in the organization. It can be difficult to act on : Since anonymous feedback is often difficult to trace, it can be challenging for the organization to get context or follow up on important issues, especially when a problem is peculiar to an individual. How to request anonymous feedback \u2691 When requesting for anonymous feedback on an organizational level, it is necessary to: Set expectations for employees : Let your colleagues know how important their feedback is to the organization. Also, assure them that their responses will be non-identifiable (no identifiable names, titles, or other demographic details). According to a Harvard Business Review article , \u201crespondents are much more likely to participate if they are confident that personal anonymity is guaranteed.\u201d Set those expectations to increase the chances of response from them. Deploy a feedback platform : Use a trusted feedback platform to send feedback requests to the rest of the employees. How to Act on Anonymous Feedback \u2691 Once you have sent the anonymous feedback, be sure to: Gather and share the findings : A significant issue with employee feedback is that the data often ends up unused. After collecting the results, share the data\u2014the positives and negatives\u2014with everyone. Doing this shows transparency and makes your colleagues develop a positive attitude toward future requests for feedback. Get everyone involved : Engage employees, managers, and leaders in discussing and analyzing the feedback findings. Doing this helps to build trust and develop actionable ideas to move the organization forward. Identify the key issues : From the discussions and analysis, identify the key issues and understand how they would impact the organization, once addressed. Define and act on the next steps : The purpose of collecting feedback would be pointless if the next steps aren't defined. Real improvement comes from knowing and working on the next steps. References \u2691 Osasumwen Arigbe article on Diversity, Inclusion, and Anonymous Feedback Paula Clapon article Why anonymous employee feedback is the better alternative Julian Cook article . I haven't used it's text, but it's written for managers in their language, it may help someone there.","title":"Anonymous Feedback"},{"location":"anonymous_feedback/#why-would-you-need-anonymous-feedback","text":"Ideally, everyone in your company should be able to give feedback publicly and not anonymously. They should share constructive criticism and not shy away from direct feedback if they believe and trust that their opinions will be heard and addressed. However, to achieve this ideal, people need to feel that they are in a safe space , a place or environment in which they feel confident that they will not be exposed to discrimination, criticism, harassment, or any other emotional or physical harm. The work place is usually not considered a safe space by the employees because they may: Fear of being judged : We want people to like us and not just in our personal lives, but in our professional lives as well. It also seems to bear a bigger importance that our supervisor likes us because he holds the power over our career and financial security. So we live in a constant state of anxiety of what might happen if our manager doesn't like us. Fear of losing their job : It\u2019s a form of self-preservation, abstaining from saying something that may be perceived as wrong to someone in a position of authority. Fear of being singled out : Giving direct feedback puts you in the spotlight. Being highlighted against the rest of the employees might be seen as a threat, especially by people belonging to a different race, gender, national origin, or other identities than most of their coworkers. Feel insecure : People may distrust their colleagues, because they just arrived at the organization or may have negative past experiences either with them or with similar people. They may not have a solid stance on an issue, be shy or have problems of self esteem. Distrust the open-door internal policies : Past experiences in other companies may lead the employee not to trust open-doors policies until they have seen them in practice. Not knowing the internal processes of the organization : As a Slack study shows , 55 percent of business owners described their organization as very transparent, but only 18 percent of their employees would agree. For all these reasons, some employees may remain silent when asked for direct feedback, to speak up against an internal issue or in need to report a colleague or manager. These factors are further amplified if: The person belongs to a minority group inside the organization. The greater the difference in position between the talking parties. It's more difficult to talk to the CEO than to the immediate manager. Until the safe space is built where direct feedback is viable, anonymous feedback gives these employees a mechanism to raise their concerns, practice their feedback-giving skills, test the waters, and understand how people perceive their constructive (and sometimes critical) opinions, thus building the needed trust.","title":"Why would you need anonymous feedback?"},{"location":"anonymous_feedback/#pros-and-cons","text":"Pros of Anonymous Feedback: Employees can express themselves freely and provide valuable insights : On topics that are considered sensitive, you\u2019ll often find employees who are afraid to share their opinions. But when employees have the option to use anonymous feedback, you will be offering a safe space for them to share their honest, constructive feedback about sensitive workplace issues, without fear of being judged, victimized, radicalized or labelled in any way. A formal, non-anonymous feedback form will only reveal some of the superficial, non-threatening issues that affect the workplace, without mentioning the most important, underlying problems. The real problems that no one talks about because they know they are so important that they could stir things up. In fact, these controversial, important issues are the ones that need to be brought to the table as soon as possible. They should be addressed by the entire team before they become a source of unhappiness, conflict and lack of productivity. An anonymous feedback instrument gives you real power over those issues because it doesn't matter who brought it up, but that it\u2019s resolved. For a manager, that insight is invaluable. It builds trust : Anonymous feedback allows the employee see how management reacts to feedback, understand how people perceive their constructive (and sometimes critical) opinions, how are the open-door policies being applied and build up self esteem. It offers a sense of security : Anonymity soothes the employee anxiety and creates a greater willingness to share our ideas and opinions. It allows every voice to be heard and respected : In workplaces, where they practice direct or attributed feedback, leaders may give preference to some voices over others. Due to our unconscious biases, people of higher authority, backgrounds, or eloquence tend to command respect and attention. In such situations, the issues they raise are likely to get immediate attention than those raised by the rest of the group. However, when feedback is collected anonymously, it eliminates biases and allows leaders to focus entirely on the feedback. It encourages new employees to share their opinions : Research has shown that new employees, who happen to be less senior or influential, see anonymous feedback as more appropriate for formal and informal evaluations than their older colleagues. Typically, the last thing a new employee wants is to start on the wrong foot, so they maintain a neutral stance. Using anonymous feedback can make new employees feel more comfortable sharing their real opinions on workplace issues. Cons of Anonymous Feedback: It can breed hostility : According to this Harvard Business Review article , anonymity often sets off a \u201cwitch hunt\u201d, where leaders seek to know the source of a negative comment. On the one hand, employees can hide behind anonymity to say personal and hurtful things about their colleagues or leaders. On the other hand, leaders may take constructive feedback as a personal attack and become suspicious and hostile to all their employees. It can be less impactful than attributed feedback : When using attributed feedback where responses carry the employees\u2019 names, information can be analyzed for relevance and impact. However, with anonymous feedback, it can be difficult to analyze information accurately. It is not uncommon for companies who choose to practice anonymous feedback, to find less specific responses since details may reveal respondents\u2019 identities. Vague feedback from employees would have less power to influence behaviors or drive change in the organization. It can be difficult to act on : Since anonymous feedback is often difficult to trace, it can be challenging for the organization to get context or follow up on important issues, especially when a problem is peculiar to an individual.","title":"Pros and cons"},{"location":"anonymous_feedback/#how-to-request-anonymous-feedback","text":"When requesting for anonymous feedback on an organizational level, it is necessary to: Set expectations for employees : Let your colleagues know how important their feedback is to the organization. Also, assure them that their responses will be non-identifiable (no identifiable names, titles, or other demographic details). According to a Harvard Business Review article , \u201crespondents are much more likely to participate if they are confident that personal anonymity is guaranteed.\u201d Set those expectations to increase the chances of response from them. Deploy a feedback platform : Use a trusted feedback platform to send feedback requests to the rest of the employees.","title":"How to request anonymous feedback"},{"location":"anonymous_feedback/#how-to-act-on-anonymous-feedback","text":"Once you have sent the anonymous feedback, be sure to: Gather and share the findings : A significant issue with employee feedback is that the data often ends up unused. After collecting the results, share the data\u2014the positives and negatives\u2014with everyone. Doing this shows transparency and makes your colleagues develop a positive attitude toward future requests for feedback. Get everyone involved : Engage employees, managers, and leaders in discussing and analyzing the feedback findings. Doing this helps to build trust and develop actionable ideas to move the organization forward. Identify the key issues : From the discussions and analysis, identify the key issues and understand how they would impact the organization, once addressed. Define and act on the next steps : The purpose of collecting feedback would be pointless if the next steps aren't defined. Real improvement comes from knowing and working on the next steps.","title":"How to Act on Anonymous Feedback"},{"location":"anonymous_feedback/#references","text":"Osasumwen Arigbe article on Diversity, Inclusion, and Anonymous Feedback Paula Clapon article Why anonymous employee feedback is the better alternative Julian Cook article . I haven't used it's text, but it's written for managers in their language, it may help someone there.","title":"References"},{"location":"antifascism/","text":"Antifascism is a method of politics, a locus of individual and group self-indentification, it's a transnational movement that adapted preexisting socialist, anarchist, and communist currents to a sudden need to react to the far right menace ( Mark p. 11 ). It's based on the idea that any oppression form can't be allowed, and should be actively fought with whatever means are necessary. Usually sharing space and even blending with other politic stances that share the same principle, such as intersectional feminism . Read the references The articles under this section are the brushstrokes I use to learn how to become an efficient antifascist. It assumes that you identify yourself as an antifascist, so I'll go straight to the point, skipping much of the argumentation that is needed to sustain these ideas. I'll add links to Mark's and Pol's awesome books, which I strongly recommend you to buy, as they both are jewels that everyone should read. Despite the criminalization and stigmatization by the mainstream press and part of the society, antifascism is a rock solid organized movement with a lot of history, that has invested blood, tears and lives to prevent us from living in a yet more horrible world. The common stereotype is a small group of leftist young people that confront the nazis on the streets, preventing them from using the public space, and from further organizing through direct action and violence if needed. If you don't identify yourself with this stereotype, don't worry, they are only a small (but essential) part of antifascism, there are so many and diverse ways to be part of the antifascist movement that in fact, everyone can (and should) be an antifascist. What is fascism \u2691 Fascism in Paxton's words is: ... a form of political behavior marked by obsessive preoccupation with community decline, humiliation, or victimhood and by compensatory cults of unity, energy, and purity, in which a mass-based party of commited nationalist militians, working in uneasy but effective collaboration with traditional elites, abandons democratic liberties and pursues with redemptive violence and without ethical or legal restrains goals of internal cleansing and external expansion. They are nourished by the people's weariness with the corruption and inoperability of the traditional political parties, and the growing fear and anguish of an uncertain economic situation. They continuously adapt, redefine and reappropriate concepts under an irreverent, politically incorrect and critical spirit, to spread the old discourse of the privileged against the oppressed. They dress themselves as antisystems, pursuing the liberty behind the authority, and accepting the democratic system introducing totalitarianism nuances ( Pol p.20 ). How to identify fascism \u2691 We need to make sure that we use the term well, otherwise we run into the risk of the word loosing meaning. But equally important is not to fall in a wording discussion that paralyzes us. One way to make it measurable is to use Kimberl\u00e9 Williams Crenshaw intersectionality theory , which states that individuals experience oppression or privilege based on a belonging to a plurality of social categories, to measure how close an action or discourse follows fascism principles ( Pol p.26 ). Source Fascism has always been carried out by people with many privileges (the upper part of the diagram) against collectives under many oppressions (the lower part of the diagram). We can then state that the more the oppressions a discourse defends and perpetuates, the more probable it is to be fascist. If it also translates into physical or verbal aggressions, escalates into the will to transform that discourse into laws that backs up those aggressions, or tries to build a government under those ideas, then we clearly have a political roadmap towards fascism. The fact that they don't propose to abolish the democracy or try to send people to concentration camps doesn't mean they are not fascist. First, we don't need them to commit the exact same crimes that the fascists of last century made to put at risk some social collectives, and secondly, history tells us that classic fascism movements didn't show their true intentions in their early phases. Fascism shifts their form and particular characteristics based on place and time. Waiting to see it clear is risking being late to fight it. Therefore whenever we see a discourse that comes from a privileged person against a oppressed one, we should fight it immediately, once fought, you can analyze if it was fascist or not ( Pol p.28 ) References \u2691 Antifa: The anti-fascist handbook by Mark Bray Todo el mundo puede ser Antifa: Manual practico para destruir el fascismo de Pol Andi\u00f1ach","title":"Antifascism"},{"location":"antifascism/#what-is-fascism","text":"Fascism in Paxton's words is: ... a form of political behavior marked by obsessive preoccupation with community decline, humiliation, or victimhood and by compensatory cults of unity, energy, and purity, in which a mass-based party of commited nationalist militians, working in uneasy but effective collaboration with traditional elites, abandons democratic liberties and pursues with redemptive violence and without ethical or legal restrains goals of internal cleansing and external expansion. They are nourished by the people's weariness with the corruption and inoperability of the traditional political parties, and the growing fear and anguish of an uncertain economic situation. They continuously adapt, redefine and reappropriate concepts under an irreverent, politically incorrect and critical spirit, to spread the old discourse of the privileged against the oppressed. They dress themselves as antisystems, pursuing the liberty behind the authority, and accepting the democratic system introducing totalitarianism nuances ( Pol p.20 ).","title":"What is fascism"},{"location":"antifascism/#how-to-identify-fascism","text":"We need to make sure that we use the term well, otherwise we run into the risk of the word loosing meaning. But equally important is not to fall in a wording discussion that paralyzes us. One way to make it measurable is to use Kimberl\u00e9 Williams Crenshaw intersectionality theory , which states that individuals experience oppression or privilege based on a belonging to a plurality of social categories, to measure how close an action or discourse follows fascism principles ( Pol p.26 ). Source Fascism has always been carried out by people with many privileges (the upper part of the diagram) against collectives under many oppressions (the lower part of the diagram). We can then state that the more the oppressions a discourse defends and perpetuates, the more probable it is to be fascist. If it also translates into physical or verbal aggressions, escalates into the will to transform that discourse into laws that backs up those aggressions, or tries to build a government under those ideas, then we clearly have a political roadmap towards fascism. The fact that they don't propose to abolish the democracy or try to send people to concentration camps doesn't mean they are not fascist. First, we don't need them to commit the exact same crimes that the fascists of last century made to put at risk some social collectives, and secondly, history tells us that classic fascism movements didn't show their true intentions in their early phases. Fascism shifts their form and particular characteristics based on place and time. Waiting to see it clear is risking being late to fight it. Therefore whenever we see a discourse that comes from a privileged person against a oppressed one, we should fight it immediately, once fought, you can analyze if it was fascist or not ( Pol p.28 )","title":"How to identify fascism"},{"location":"antifascism/#references","text":"Antifa: The anti-fascist handbook by Mark Bray Todo el mundo puede ser Antifa: Manual practico para destruir el fascismo de Pol Andi\u00f1ach","title":"References"},{"location":"antifascist_actions/","text":"Collection of amazing and inspiring antifa actions. 2021 \u2691 A fake company and five million recycled flyers \u2691 A group of artists belonging to the Center for political beauty created a fake company Flyerservice Hahn and convinced more than 80 regional sections of the far right party AfD to hire them to deliver their electoral propaganda. They gathered five million flyers, with a total weight of 72 tons. They justify that they wouldn't be able to lie to the people, so they did nothing in the broader sense of the word. They declared that they are the \"world wide leader in the non-delivery of nazi propaganda\" . At the start of the electoral campaign, they went to the AfD stands, and they let their members to give them flyers the throw them to the closest bin. \"It's something that any citizen can freely do, we have only industrialized the process\". They've done a crowdfunding to fund the legal process that may result.","title":"Antifascist Actions"},{"location":"antifascist_actions/#2021","text":"","title":"2021"},{"location":"antifascist_actions/#a-fake-company-and-five-million-recycled-flyers","text":"A group of artists belonging to the Center for political beauty created a fake company Flyerservice Hahn and convinced more than 80 regional sections of the far right party AfD to hire them to deliver their electoral propaganda. They gathered five million flyers, with a total weight of 72 tons. They justify that they wouldn't be able to lie to the people, so they did nothing in the broader sense of the word. They declared that they are the \"world wide leader in the non-delivery of nazi propaganda\" . At the start of the electoral campaign, they went to the AfD stands, and they let their members to give them flyers the throw them to the closest bin. \"It's something that any citizen can freely do, we have only industrialized the process\". They've done a crowdfunding to fund the legal process that may result.","title":"A fake company and five million recycled flyers"},{"location":"antitransphobia/","text":"Anti-transphobia being reductionist is the opposition to the collection of ideas and phenomena that encompass a range of negative attitudes, feelings or actions towards transgender people or transness in general. Transphobia can include fear, aversion, hatred, violence, anger, or discomfort felt or expressed towards people who do not conform to social gender expectations. It is often expressed alongside homophobic views and hence is often considered an aspect of homophobia. It's yet another clear case of privileged people oppressing even further already oppressed collectives. We can clearly see it if we use the ever useful Kimberl\u00e9 Williams Crenshaw intersectionality theory diagram. Source TERF \u2691 TERF is an acronym for trans-exclusionary radical feminist . The term originally applied to the minority of feminists that expressed transphobic sentiments such as the rejection of the assertion that trans women are women, the exclusion of trans women from women's spaces, and opposition to transgender rights legislation. The meaning has since expanded to refer more broadly to people with trans-exclusionary views who may have no involvement with radical feminism. Arguments against theories that deny the reality of trans people \u2691 This section is a direct translation from Alana Portero's text called Definitions . Sex is a medical category and gender a social category \u2691 Sex is a medical category, it's not a biological one. According to body features like the chromosome structure and genitalia appearance, medicine assigns the sex (hence gender) to the bodies. In the case of intersexual people, they are usually mutilated and hormonated so that their bodies fit into one of the two options contemplated by the medicine. Gender is the term we use to refer to how a person feels about themselves as a boy/man, a girl/woman or non-binary. Since birth, we're told what's appropriate (and what isn't) for each gender. These are the gender roles. It's not the same gender than gender role: the gender determines how you interact with the other roles. For example, a woman can take traditionally understood male roles gender roles, that doesn't mean that she is or isn't a woman. The problem arises when these two oppressions are mixed up: cissexism (the believe that bodies have an immutable gender defined by the sex assigned at birth) and misogyny (the base of feminine oppression). When you mixing them up you get the idea that the trans movement erases the feminine structural oppression, when in reality, it broadens the scope and makes it more precise, as they suffer the same misogyny than the cis women. Women are killed for being women. They are socially assigned the responsibility for care, they are prevented from having individual will and they are deterred from accessing resources. This structural violence is suffered by all women regardless of the sex assigned at birth. Questioning the adjudication of gender to the bodies and questioning the roles assigned to the genders are complementary paths for the feminism liberation. Avoid the interested manipulation of the sexual or gender identity \u2691 The sexual or gender identity determines whether there is correspondence with the gender assigned at birth. When there isn't, it concerns a trans person. The sex and gender terms represent the same reality, being sex the medical term, and gender the academic one. Equally transexual and transgender represent the same reality, although these last have a pathologizing undertone, the term trans is preferred. Avoid the fears of letting trans people be \u2691 Some are afraid that the trans women negatively affect the statistics of unemployment, laboral inequality, feminization of the poverty and machist violence, and they contradict the problems of the cis women. Trans people usually have a greater unemployment rate (85% in Spain), so the glass ceiling is not yet even a concern, and they are also greatly affected by machist violence. The queer theory doesn't erase or blur women as a political subject. Thinking that it risks the rights and achievements earned through the feminist movement shows a complete misunderstanding of the theory. Women are not an entity \u2691 Women are not an entity, they are a group of people that are placed below men in the social scale, each with her own unique experience. The woman identity belongs to any person that identifies herself with it. The fight against discrimination and towards inclusion politics should be mandatory for all society, and shouldn't be used against the trans people. References \u2691 Wikipedia page on Transphobia","title":"Anti-Transphobia"},{"location":"antitransphobia/#terf","text":"TERF is an acronym for trans-exclusionary radical feminist . The term originally applied to the minority of feminists that expressed transphobic sentiments such as the rejection of the assertion that trans women are women, the exclusion of trans women from women's spaces, and opposition to transgender rights legislation. The meaning has since expanded to refer more broadly to people with trans-exclusionary views who may have no involvement with radical feminism.","title":"TERF"},{"location":"antitransphobia/#arguments-against-theories-that-deny-the-reality-of-trans-people","text":"This section is a direct translation from Alana Portero's text called Definitions .","title":"Arguments against theories that deny the reality of trans people"},{"location":"antitransphobia/#sex-is-a-medical-category-and-gender-a-social-category","text":"Sex is a medical category, it's not a biological one. According to body features like the chromosome structure and genitalia appearance, medicine assigns the sex (hence gender) to the bodies. In the case of intersexual people, they are usually mutilated and hormonated so that their bodies fit into one of the two options contemplated by the medicine. Gender is the term we use to refer to how a person feels about themselves as a boy/man, a girl/woman or non-binary. Since birth, we're told what's appropriate (and what isn't) for each gender. These are the gender roles. It's not the same gender than gender role: the gender determines how you interact with the other roles. For example, a woman can take traditionally understood male roles gender roles, that doesn't mean that she is or isn't a woman. The problem arises when these two oppressions are mixed up: cissexism (the believe that bodies have an immutable gender defined by the sex assigned at birth) and misogyny (the base of feminine oppression). When you mixing them up you get the idea that the trans movement erases the feminine structural oppression, when in reality, it broadens the scope and makes it more precise, as they suffer the same misogyny than the cis women. Women are killed for being women. They are socially assigned the responsibility for care, they are prevented from having individual will and they are deterred from accessing resources. This structural violence is suffered by all women regardless of the sex assigned at birth. Questioning the adjudication of gender to the bodies and questioning the roles assigned to the genders are complementary paths for the feminism liberation.","title":"Sex is a medical category and gender a social category"},{"location":"antitransphobia/#avoid-the-interested-manipulation-of-the-sexual-or-gender-identity","text":"The sexual or gender identity determines whether there is correspondence with the gender assigned at birth. When there isn't, it concerns a trans person. The sex and gender terms represent the same reality, being sex the medical term, and gender the academic one. Equally transexual and transgender represent the same reality, although these last have a pathologizing undertone, the term trans is preferred.","title":"Avoid the interested manipulation of the sexual or gender identity"},{"location":"antitransphobia/#avoid-the-fears-of-letting-trans-people-be","text":"Some are afraid that the trans women negatively affect the statistics of unemployment, laboral inequality, feminization of the poverty and machist violence, and they contradict the problems of the cis women. Trans people usually have a greater unemployment rate (85% in Spain), so the glass ceiling is not yet even a concern, and they are also greatly affected by machist violence. The queer theory doesn't erase or blur women as a political subject. Thinking that it risks the rights and achievements earned through the feminist movement shows a complete misunderstanding of the theory.","title":"Avoid the fears of letting trans people be"},{"location":"antitransphobia/#women-are-not-an-entity","text":"Women are not an entity, they are a group of people that are placed below men in the social scale, each with her own unique experience. The woman identity belongs to any person that identifies herself with it. The fight against discrimination and towards inclusion politics should be mandatory for all society, and shouldn't be used against the trans people.","title":"Women are not an entity"},{"location":"antitransphobia/#references","text":"Wikipedia page on Transphobia","title":"References"},{"location":"asyncio/","text":"asyncio is a library to write concurrent code using the async/await syntax. asyncio is used as a foundation for multiple Python asynchronous frameworks that provide high-performance network and web-servers, database connection libraries, distributed task queues, etc. asyncio is often a perfect fit for IO-bound and high-level structured network code. References \u2691 Docs Awesome Asyncio","title":"asyncio"},{"location":"asyncio/#references","text":"Docs Awesome Asyncio","title":"References"},{"location":"beancount/","text":"Beancount is a Python double entry accounting command line tool similar to ledger . Installation \u2691 pip3 install beancount Tools \u2691 beancount is the core component, it's a declarative language. It parses a text file, and produces reports from the resulting data structures. bean-check \u2691 bean-check is the program you use to verify that your input syntax and transactions work correctly. All it does is load your input file and run the various plugins you configured in it, plus some extra validation checks. bean-check /path/to/file.beancount If there are no errors, there should be no output, it should exit quietly. bean-report \u2691 This is the main tool used to extract specialized reports to the console in text or one of the various other formats. For a graphic exploration of your data, use the fava web application instead. bean-report /path/to/file.beancount {{ report_name }} There are many reports available, to get a full list run bean-report --help-reports Report names sometimes may accept arguments, if they do so use : bean-report /path/to/file.beancount balances:Vanguard To get the balances \u2691 bean-report {{ path/to/file.beancount }} balances | treeify To get the journal \u2691 bean-report {{ path/to/file.beancount }} journal To get the holdings \u2691 To get the aggregations for the total list of holdings bean-report {{ path/to/file.beancount }} holdings To get the accounts \u2691 bean-report {{ path/to/file.beancount }} accounts bean-query \u2691 bean-query is a command-line tool that acts like a client to that in-memory database in which you can type queries in a variant of SQL. It has it's own document bean-query /path/to/file.beancount bean-web \u2691 Deprecated use fava instead bean-web serves all the reports on a web server that runs on your computer bean-web /path/to/file.beancount It will serve on localhost:8080 bean-doctor \u2691 This is a debugging tool used to perform various diagnostics and run debugging commands, and to help provide information for reporting bugs. bean-format \u2691 Pure text processing tool will reformat Beancount input to right-align all the numbers at the same, minimal column. bean-example \u2691 Generates an example Beancount input file. bean-identify \u2691 Given a messy list of downloaded files automatically identify which of your configured importers is able to handle them and print them out. This is to be used for debugging and figuring out if your configuration is properly associating a suitable importer for each of the files you downloaded. bean-extract \u2691 Extracts transactions and statement date from each file, if at all possible. This produces some Beancount input text to be moved to your input file. bean-extract {{ path/to/config.config }} {{ path/to/source/files }} The tool calls methods on importer objects. You must provide a list of such importers; this list is the configuration for the importing process. For each file found, each of the importers is called to assert whether it can or cannot handle that file. If it deems that it can, methods can be called to produce a list of transactions extract a date, or produce a cleaned up filename for the downloaded file. The configuration should be a python3 module in which you instantiate the importers and assign the list to the module-level \"CONFIG\" variable #!/usr/bin/env python3 from myimporters.bank import acmebank from myimporters.bank import chase \u2026 CONFIG = [ acmebank . Importer (), chase . Importer (), \u2026 ] Writing an importer \u2691 Each of the importers must comply with a particular protocol and implement at least some of its methods. The full detail of the protocol is in the source of importer.py \"\"\"Importer protocol. All importers must comply with this interface and implement at least some of its methods. A configuration consists in a simple list of such importer instances. The importer processes run through the importers, calling some of its methods in order to identify, extract and file the downloaded files. Each of the methods accept a cache.FileMemo object which has a 'name' attribute with the filename to process, but which also provides a place to cache conversions. Use its convert() method whenever possible to avoid carrying out the same conversion multiple times. See beancount.ingest.cache for more details. Synopsis: name(): Return a unique identifier for the importer instance. identify(): Return true if the identifier is able to process the file. extract(): Extract directives from a file's contents and return of list of entries. file_account(): Return an account name associated with the given file for this importer. file_date(): Return a date associated with the downloaded file (e.g., the statement date). file_name(): Return a cleaned up filename for storage (optional). Just to be clear: Although this importer will not raise NotImplementedError exceptions (it returns default values for each method), you NEED to derive from it in order to do anything meaningful. Simply instantiating this importer will not match not provide any useful information. It just defines the protocol for all importers. \"\"\" __copyright__ = \"Copyright (C) 2016 Martin Blais\" __license__ = \"GNU GPLv2\" from beancount.core import flags class ImporterProtocol : \"Interface that all source importers need to comply with.\" # A flag to use on new transaction. Override this flag in derived classes if # you prefer to create your imported transactions with a different flag. FLAG = flags . FLAG_OKAY def name ( self ): \"\"\"Return a unique id/name for this importer. Returns: A string which uniquely identifies this importer. \"\"\" cls = self . __class__ return ' {} . {} ' . format ( cls . __module__ , cls . __name__ ) __str__ = name def identify ( self , file ): \"\"\"Return true if this importer matches the given file. Args: file: A cache.FileMemo instance. Returns: A boolean, true if this importer can handle this file. \"\"\" def extract ( self , file ): \"\"\"Extract transactions from a file. Args: file: A cache.FileMemo instance. Returns: A list of new, imported directives (usually mostly Transactions) extracted from the file. \"\"\" def file_account ( self , file ): \"\"\"Return an account associated with the given file. Note: If you don't implement this method you won't be able to move the files into its preservation hierarchy; the bean-file command won't work. Also, normally the returned account is not a function of the input file--just of the importer--but it is provided anyhow. Args: file: A cache.FileMemo instance. Returns: The name of the account that corresponds to this importer. \"\"\" def file_name ( self , file ): \"\"\"A filter that optionally renames a file before filing. This is used to make tidy filenames for filed/stored document files. The default implementation just returns the same filename. Note that a simple RELATIVE filename must be returned, not an absolute filename. Args: file: A cache.FileMemo instance. Returns: The tidied up, new filename to store it as. \"\"\" def file_date ( self , file ): \"\"\"Attempt to obtain a date that corresponds to the given file. Args: file: A cache.FileMemo instance. Returns: A date object, if successful, or None if a date could not be extracted. (If no date is returned, the file creation time is used. This is the default.) A summary of the methods you need to, or may want to implement: name() : Provides a unique id for each importer instance. It's convenient to be able to refer to your importers with a unique name; it gets printed out by the identification process. identify() : This method just returns true if this importer can handle the given file. You must implement this method , and all the tools invoke it ot figure out the list of (file, importer) pairs. extract() : This is called to attempt to extract some Beancount directives from the file contents. It must create the directives by instatiating the objects define in beancout.core.data and return them. from beancount.ingest import importer class Importer ( importer . ImporterProtocol ): def identify ( self , file ): \u2026 # Override other methods\u2026 Some importer examples: mterwill gist wzyboy importers bean-file \u2691 bean-file filing documents. It si able to identify which document belongs to which account, it can move the downloaded file to the documents archive automatically. Basic concepts \u2691 Beancount transaction \u2691 2014-05-23 * \"CAFE MOGADOR NEW YO\" \"Dinner with Caroline\" Liabilities:US:BofA:CreditCard -98.32 USD Expenses:Restaurant Currencies must be entirely in capital letters. Account names do not admit spaces. Description strings must be quoted. Dates are only parsed in YYYY-MM-DD format. Tags must begin with # and links with ^ . Beancount Operators \u2691 Open \u2691 All accounts need to be declared open in order to accept amounts posted to them. YYYY-MM-DD open {{ account_name }} [{{ ConstrainCurrency }}] Close \u2691 YYYY-MM-DD close {{ account_name }} It's useful to insert a balance of 0 units just before closing an account, just to make sure its contents are empty as you close it. Commodity \u2691 It can be used to declare currencies, financial instruments, commodities... It's optional YYYY-MM-DD commodity {{ currency_name }} Transactions \u2691 YYYY-MM-DD txn \"[{{ payee }}]\" \"{{ Comment }}\" {{ Account1 }} {{ value}} [{{ Accountn-1 }} {{ value }}] {{ Accountn }} Payee is a string that represents an external entity that is involved in the transaction. Payees are sometimes useful on transactions that post amounts to Expense accounts, whereby the account accumulates a category of expenses from multiple business As transactions is the most common, you can substitute txn for a flag, by default : * * : Completed transaction, known amounts, \"this looks correct\" * ! : Incomplete transaction, needs confirmation or revision, \"this looks incorrect\" You can also attach flags to the postings themselves, if you want to flag one of the transaction's legs in particular: 2014-05-05 * \"Transfer from Savings account\" Assets:MyBank:Checking -400.00 USD ! Assets:MyBank:Savings This is useful in the intermediate stage of de-duping transactions Tags vs Payee \u2691 You can tag your transactions with #{{tag_name}} , so you can later filter or generate reports based on that tag. Therefore the Payee could be used as whom or who pays and the tag for the context. For example, for a trip I could use the tag #34C3 To mark a series of transactions with tags use the following syntax pushtag #berlin-trip-2014 2014-04-23 * \"Flight to Berlin\" Expenses:Flights -1230.27 USD Liabilities:CreditCard ... poptag #berlin-trip-2014 Links \u2691 Transactions can also be linked together. You may think of the link as a special kind of tag that can be used to group together a set of financially related transactions over time. 2014-02-05 * \"Invoice for January\" ^invoice-acme-studios-jan14 Income:Clients:ACMEStudios -8450.00 USD Assets:AccountsReceivable ... 2014-02-20 * \"Check deposit - payment from ACME\" ^invoice-acme-studios-jan14 Assets:BofA:Checking 8450.00 USD Assets:AccountsReceivable Balance \u2691 A balance assertion is a way for you to input your statement balance into the flow of transactions. It tells Beancount to verify that the number of units of a particular commodity in some account should equal some expected value at some point in time. If no error is reported, you should have some confidence that the list of transactions that precedes it in this account is highly likely to be correct. This is useful in practice because in many cases some transactions can get imported separately from the accounts of each of their postings. As all other non-transaction directives, it applies at the beginning of it's date . Just imagine that the balance checks occurs right after midnight on that day. YYYY-MM-DD balance {{ account_name }} {{ amount }} Pad \u2691 A padding directive automatically inserts a transaction that will make the subsequent balance assertion succeed, if it is needed. It inserts the difference needed to fulfill that balance assertion. Being subsequent in date order, not in the order of the declarations in the file. YYYY-MM-DD pad {{ account_name }} {{ account_name_to_pad }} The first account is the account to credit the automatically calculated amount to. This is the account that should have a balance assertion following it. The second leg is the source where the funds will come from, and this is almost always some Equity account. 1990-05-17 open Assets:Cash EUR 1990-05-17 pad Assets:Cash Equity:Opening-Balances 2017-12-26 balance Assets:Cash 250 EUR You could also insert pad entries between balance assertions so as to fix un registered transactions Notes \u2691 A note directive is simply used to attach a dated comment to the journal of a particular account. this can be useful to record facts and claims associated with a financial event. YYYY-MM-DD note {{ account_name }} {{ comment }} Document \u2691 A Document directive can be used to attach an external file to the journal of an account. The filename gets rendered as a browser link in the journals of the web interface for the corresponding account and you should be able to click on it to view the contents of the file itself. YYYY-MM-DD {{ account_name }} {{ path/to/document }} Includes \u2691 This allows you to split up large input files into multiple files. include {{ path/to/file.beancount }} The path could be relative or absolute. Library usage \u2691 Beancount can also be used as a Python library. There are some articles in the documentation where you can start seeing how to use it: scripting plugins , external contributions and the api reference . Although I found it more pleasant to read the source code itself as it's really well documented (both by docstrings and type hints). References \u2691 Homepage Git Docs Awesome beancount Docs in google Vim plugin","title":"beancount"},{"location":"beancount/#installation","text":"pip3 install beancount","title":"Installation"},{"location":"beancount/#tools","text":"beancount is the core component, it's a declarative language. It parses a text file, and produces reports from the resulting data structures.","title":"Tools"},{"location":"beancount/#bean-check","text":"bean-check is the program you use to verify that your input syntax and transactions work correctly. All it does is load your input file and run the various plugins you configured in it, plus some extra validation checks. bean-check /path/to/file.beancount If there are no errors, there should be no output, it should exit quietly.","title":"bean-check"},{"location":"beancount/#bean-report","text":"This is the main tool used to extract specialized reports to the console in text or one of the various other formats. For a graphic exploration of your data, use the fava web application instead. bean-report /path/to/file.beancount {{ report_name }} There are many reports available, to get a full list run bean-report --help-reports Report names sometimes may accept arguments, if they do so use : bean-report /path/to/file.beancount balances:Vanguard","title":"bean-report"},{"location":"beancount/#to-get-the-balances","text":"bean-report {{ path/to/file.beancount }} balances | treeify","title":"To get the balances"},{"location":"beancount/#to-get-the-journal","text":"bean-report {{ path/to/file.beancount }} journal","title":"To get the journal"},{"location":"beancount/#to-get-the-holdings","text":"To get the aggregations for the total list of holdings bean-report {{ path/to/file.beancount }} holdings","title":"To get the holdings"},{"location":"beancount/#to-get-the-accounts","text":"bean-report {{ path/to/file.beancount }} accounts","title":"To get the accounts"},{"location":"beancount/#bean-query","text":"bean-query is a command-line tool that acts like a client to that in-memory database in which you can type queries in a variant of SQL. It has it's own document bean-query /path/to/file.beancount","title":"bean-query"},{"location":"beancount/#bean-web","text":"Deprecated use fava instead bean-web serves all the reports on a web server that runs on your computer bean-web /path/to/file.beancount It will serve on localhost:8080","title":"bean-web"},{"location":"beancount/#bean-doctor","text":"This is a debugging tool used to perform various diagnostics and run debugging commands, and to help provide information for reporting bugs.","title":"bean-doctor"},{"location":"beancount/#bean-format","text":"Pure text processing tool will reformat Beancount input to right-align all the numbers at the same, minimal column.","title":"bean-format"},{"location":"beancount/#bean-example","text":"Generates an example Beancount input file.","title":"bean-example"},{"location":"beancount/#bean-identify","text":"Given a messy list of downloaded files automatically identify which of your configured importers is able to handle them and print them out. This is to be used for debugging and figuring out if your configuration is properly associating a suitable importer for each of the files you downloaded.","title":"bean-identify"},{"location":"beancount/#bean-extract","text":"Extracts transactions and statement date from each file, if at all possible. This produces some Beancount input text to be moved to your input file. bean-extract {{ path/to/config.config }} {{ path/to/source/files }} The tool calls methods on importer objects. You must provide a list of such importers; this list is the configuration for the importing process. For each file found, each of the importers is called to assert whether it can or cannot handle that file. If it deems that it can, methods can be called to produce a list of transactions extract a date, or produce a cleaned up filename for the downloaded file. The configuration should be a python3 module in which you instantiate the importers and assign the list to the module-level \"CONFIG\" variable #!/usr/bin/env python3 from myimporters.bank import acmebank from myimporters.bank import chase \u2026 CONFIG = [ acmebank . Importer (), chase . Importer (), \u2026 ]","title":"bean-extract"},{"location":"beancount/#writing-an-importer","text":"Each of the importers must comply with a particular protocol and implement at least some of its methods. The full detail of the protocol is in the source of importer.py \"\"\"Importer protocol. All importers must comply with this interface and implement at least some of its methods. A configuration consists in a simple list of such importer instances. The importer processes run through the importers, calling some of its methods in order to identify, extract and file the downloaded files. Each of the methods accept a cache.FileMemo object which has a 'name' attribute with the filename to process, but which also provides a place to cache conversions. Use its convert() method whenever possible to avoid carrying out the same conversion multiple times. See beancount.ingest.cache for more details. Synopsis: name(): Return a unique identifier for the importer instance. identify(): Return true if the identifier is able to process the file. extract(): Extract directives from a file's contents and return of list of entries. file_account(): Return an account name associated with the given file for this importer. file_date(): Return a date associated with the downloaded file (e.g., the statement date). file_name(): Return a cleaned up filename for storage (optional). Just to be clear: Although this importer will not raise NotImplementedError exceptions (it returns default values for each method), you NEED to derive from it in order to do anything meaningful. Simply instantiating this importer will not match not provide any useful information. It just defines the protocol for all importers. \"\"\" __copyright__ = \"Copyright (C) 2016 Martin Blais\" __license__ = \"GNU GPLv2\" from beancount.core import flags class ImporterProtocol : \"Interface that all source importers need to comply with.\" # A flag to use on new transaction. Override this flag in derived classes if # you prefer to create your imported transactions with a different flag. FLAG = flags . FLAG_OKAY def name ( self ): \"\"\"Return a unique id/name for this importer. Returns: A string which uniquely identifies this importer. \"\"\" cls = self . __class__ return ' {} . {} ' . format ( cls . __module__ , cls . __name__ ) __str__ = name def identify ( self , file ): \"\"\"Return true if this importer matches the given file. Args: file: A cache.FileMemo instance. Returns: A boolean, true if this importer can handle this file. \"\"\" def extract ( self , file ): \"\"\"Extract transactions from a file. Args: file: A cache.FileMemo instance. Returns: A list of new, imported directives (usually mostly Transactions) extracted from the file. \"\"\" def file_account ( self , file ): \"\"\"Return an account associated with the given file. Note: If you don't implement this method you won't be able to move the files into its preservation hierarchy; the bean-file command won't work. Also, normally the returned account is not a function of the input file--just of the importer--but it is provided anyhow. Args: file: A cache.FileMemo instance. Returns: The name of the account that corresponds to this importer. \"\"\" def file_name ( self , file ): \"\"\"A filter that optionally renames a file before filing. This is used to make tidy filenames for filed/stored document files. The default implementation just returns the same filename. Note that a simple RELATIVE filename must be returned, not an absolute filename. Args: file: A cache.FileMemo instance. Returns: The tidied up, new filename to store it as. \"\"\" def file_date ( self , file ): \"\"\"Attempt to obtain a date that corresponds to the given file. Args: file: A cache.FileMemo instance. Returns: A date object, if successful, or None if a date could not be extracted. (If no date is returned, the file creation time is used. This is the default.) A summary of the methods you need to, or may want to implement: name() : Provides a unique id for each importer instance. It's convenient to be able to refer to your importers with a unique name; it gets printed out by the identification process. identify() : This method just returns true if this importer can handle the given file. You must implement this method , and all the tools invoke it ot figure out the list of (file, importer) pairs. extract() : This is called to attempt to extract some Beancount directives from the file contents. It must create the directives by instatiating the objects define in beancout.core.data and return them. from beancount.ingest import importer class Importer ( importer . ImporterProtocol ): def identify ( self , file ): \u2026 # Override other methods\u2026 Some importer examples: mterwill gist wzyboy importers","title":"Writing an importer"},{"location":"beancount/#bean-file","text":"bean-file filing documents. It si able to identify which document belongs to which account, it can move the downloaded file to the documents archive automatically.","title":"bean-file"},{"location":"beancount/#basic-concepts","text":"","title":"Basic concepts"},{"location":"beancount/#beancount-transaction","text":"2014-05-23 * \"CAFE MOGADOR NEW YO\" \"Dinner with Caroline\" Liabilities:US:BofA:CreditCard -98.32 USD Expenses:Restaurant Currencies must be entirely in capital letters. Account names do not admit spaces. Description strings must be quoted. Dates are only parsed in YYYY-MM-DD format. Tags must begin with # and links with ^ .","title":"Beancount transaction"},{"location":"beancount/#beancount-operators","text":"","title":"Beancount Operators"},{"location":"beancount/#open","text":"All accounts need to be declared open in order to accept amounts posted to them. YYYY-MM-DD open {{ account_name }} [{{ ConstrainCurrency }}]","title":"Open"},{"location":"beancount/#close","text":"YYYY-MM-DD close {{ account_name }} It's useful to insert a balance of 0 units just before closing an account, just to make sure its contents are empty as you close it.","title":"Close"},{"location":"beancount/#commodity","text":"It can be used to declare currencies, financial instruments, commodities... It's optional YYYY-MM-DD commodity {{ currency_name }}","title":"Commodity"},{"location":"beancount/#transactions","text":"YYYY-MM-DD txn \"[{{ payee }}]\" \"{{ Comment }}\" {{ Account1 }} {{ value}} [{{ Accountn-1 }} {{ value }}] {{ Accountn }} Payee is a string that represents an external entity that is involved in the transaction. Payees are sometimes useful on transactions that post amounts to Expense accounts, whereby the account accumulates a category of expenses from multiple business As transactions is the most common, you can substitute txn for a flag, by default : * * : Completed transaction, known amounts, \"this looks correct\" * ! : Incomplete transaction, needs confirmation or revision, \"this looks incorrect\" You can also attach flags to the postings themselves, if you want to flag one of the transaction's legs in particular: 2014-05-05 * \"Transfer from Savings account\" Assets:MyBank:Checking -400.00 USD ! Assets:MyBank:Savings This is useful in the intermediate stage of de-duping transactions","title":"Transactions"},{"location":"beancount/#tags-vs-payee","text":"You can tag your transactions with #{{tag_name}} , so you can later filter or generate reports based on that tag. Therefore the Payee could be used as whom or who pays and the tag for the context. For example, for a trip I could use the tag #34C3 To mark a series of transactions with tags use the following syntax pushtag #berlin-trip-2014 2014-04-23 * \"Flight to Berlin\" Expenses:Flights -1230.27 USD Liabilities:CreditCard ... poptag #berlin-trip-2014","title":"Tags vs Payee"},{"location":"beancount/#links","text":"Transactions can also be linked together. You may think of the link as a special kind of tag that can be used to group together a set of financially related transactions over time. 2014-02-05 * \"Invoice for January\" ^invoice-acme-studios-jan14 Income:Clients:ACMEStudios -8450.00 USD Assets:AccountsReceivable ... 2014-02-20 * \"Check deposit - payment from ACME\" ^invoice-acme-studios-jan14 Assets:BofA:Checking 8450.00 USD Assets:AccountsReceivable","title":"Links"},{"location":"beancount/#balance","text":"A balance assertion is a way for you to input your statement balance into the flow of transactions. It tells Beancount to verify that the number of units of a particular commodity in some account should equal some expected value at some point in time. If no error is reported, you should have some confidence that the list of transactions that precedes it in this account is highly likely to be correct. This is useful in practice because in many cases some transactions can get imported separately from the accounts of each of their postings. As all other non-transaction directives, it applies at the beginning of it's date . Just imagine that the balance checks occurs right after midnight on that day. YYYY-MM-DD balance {{ account_name }} {{ amount }}","title":"Balance"},{"location":"beancount/#pad","text":"A padding directive automatically inserts a transaction that will make the subsequent balance assertion succeed, if it is needed. It inserts the difference needed to fulfill that balance assertion. Being subsequent in date order, not in the order of the declarations in the file. YYYY-MM-DD pad {{ account_name }} {{ account_name_to_pad }} The first account is the account to credit the automatically calculated amount to. This is the account that should have a balance assertion following it. The second leg is the source where the funds will come from, and this is almost always some Equity account. 1990-05-17 open Assets:Cash EUR 1990-05-17 pad Assets:Cash Equity:Opening-Balances 2017-12-26 balance Assets:Cash 250 EUR You could also insert pad entries between balance assertions so as to fix un registered transactions","title":"Pad"},{"location":"beancount/#notes","text":"A note directive is simply used to attach a dated comment to the journal of a particular account. this can be useful to record facts and claims associated with a financial event. YYYY-MM-DD note {{ account_name }} {{ comment }}","title":"Notes"},{"location":"beancount/#document","text":"A Document directive can be used to attach an external file to the journal of an account. The filename gets rendered as a browser link in the journals of the web interface for the corresponding account and you should be able to click on it to view the contents of the file itself. YYYY-MM-DD {{ account_name }} {{ path/to/document }}","title":"Document"},{"location":"beancount/#includes","text":"This allows you to split up large input files into multiple files. include {{ path/to/file.beancount }} The path could be relative or absolute.","title":"Includes"},{"location":"beancount/#library-usage","text":"Beancount can also be used as a Python library. There are some articles in the documentation where you can start seeing how to use it: scripting plugins , external contributions and the api reference . Although I found it more pleasant to read the source code itself as it's really well documented (both by docstrings and type hints).","title":"Library usage"},{"location":"beancount/#references","text":"Homepage Git Docs Awesome beancount Docs in google Vim plugin","title":"References"},{"location":"beets/","text":"Beets is a music management library used to get your music collection right once and for all. It catalogs your collection, automatically improving its metadata as it goes using the MusicBrainz database. Then it provides a set of tools for manipulating and accessing your music. Through plugins it supports: Fetch or calculate all the metadata you could possibly need: album art, lyrics, genres, tempos, ReplayGain levels, or acoustic fingerprints. Get metadata from MusicBrainz, Discogs, or Beatport. Or guess metadata using songs\u2019 filenames or their acoustic fingerprints. Transcode audio to any format you like. Check your library for duplicate tracks and albums or for albums that are missing tracks. Browse your music library graphically through a Web browser and play it in any browser that supports HTML5 Audio. Still, if beets doesn't do what you want yet, writing your own plugin is easy if you know a little Python. Or you can use it as a library . Installation \u2691 pip install beets References \u2691 Git Docs Homepage","title":"Beets"},{"location":"beets/#installation","text":"pip install beets","title":"Installation"},{"location":"beets/#references","text":"Git Docs Homepage","title":"References"},{"location":"book_management/","text":"Book management is the set of systems and processes to get and categorize books so it's easy to browse and discover new content. It involves the next actions: Automatically index and download metadata of new books. Notify the user when a new book is added. Monitor the books of an author, and get them once they are released. Send books to the e-reader. A nice interface to browse the existent library, with the possibility of filtering by author, genre, years, tags or series. An interface to preview or read the items. An interface to rate and review library items. An interface to discover new content based on the ratings and item metadata. I haven't yet found a single piece of software that fulfills all these needs, so we need to split it into subsystems. Downloader and indexer. Gallery browser. Review system. Content discovery. Downloader and indexer \u2691 System that monitors the availability of books in a list of indexers, when they are available, they download it to a directory of the server. The best one that I've found is Readarr , it makes it easy to search for authors and books, supports a huge variety of indexers (such as Archive.org), and download clients (such as torrent clients). It can be used as a limited gallery browser, you can easily see the books of an author or series, but it doesn't yet support the automatic fetch of genres or tags. I haven't found an easy way of marking elements as read, prioritize the list of books to read, or add a user rating. Until these features are added (if they ever are), we need to use it in parallel with a better gallery browser. Gallery browser \u2691 System that shows the books in the library in a nice format, allowing the user to filter out the contents, prioritize them, mark them as read, rate them and optionally sync books with the ereader. Calibre-web is a beautiful solution, without trying it, it looks like it supports all of the required features, but it doesn't work well with Readarr. Readarr has support to interact with Calibre content server by defining a root folder to be managed by Calibre , but the books you want to have Readarr recognize on initial library import must already be in Calibre. Books within the folder and not in Calibre will be ignored. So you'll need to do the first import in Calibre, instead of Readarr (which is quite pleasant). Note also that you cannot add Calibre integration to a root folder after it's created. Calibre-web interacts directly with the Sqlite database of Calibre, so it doesn't expose the Calibre Content Server, therefore is not compatible with Readarr. To make it work, you'd need to have both the calibre server and the calibre-web running at the same time, which has led to database locks ( 1 , and 2 ) that the calibre-web developer has tried to avoid by controlling the database writes , and said that : If you start Calibre first and afterwards Calibre-Web, Calibre indeed locks the database and doesn't allow Calibre-Web to access the database (metadata.db) file. Starting Calibre-Web and afterwards Calibre should work. The problem comes when Readarr writes in the database through calibre to add books, and calibre-web tries to write too to add user ratings or other metadata. Another option would be to only run calibre-web and automatically import the books once they are downloaded by Readarr. calibre-web is not going to support a watch directory feature, the author recommends to use a cron script to do it. I haven't tried this path yet. Another option would be to assume that calibre-web is not going to do any insert in the database, so it would become a read-only web interface, therefore we wouldn't be able to edit the books or rate them, one of the features we'd like to have in the gallery browser. To make sure that we don't get locks, instead of using the same file, a cron job could do an rsync between the database managed by calibre and the one used by calibre-web . Calibre implements genres with tags, behind the scenes it uses the fetch-ebook-metadata command line tool, that returns all the metadata in human readable form $: fetch-ebook-metadata -i 9780061796883 -c cover.jpg Title : The Dispossessed Author ( s ) : Ursula K. le Guin Publisher : Harper Collins Tags : Fiction, Science Fiction, Space Exploration, Literary, Visionary & Metaphysical Languages : eng Published : 2009 -10-13T20:34:30.878865+00:00 Identifiers : google:tlhFtmTixvwC, isbn:9780061796883 Comments : \u201cOne of the greats\u2026.Not just a science fiction writer ; a literary icon.\u201d \u2013 Stephen KingFrom the brilliant and award-winning author Ursula K. Le Guin comes a classic tale of two planets torn apart by conflict and mistrust \u2014 and the man who risks everything to reunite them.A bleak moon settled by utopian anarchists, Anarres has long been isolated from other worlds, including its mother planet, Urras\u2014a civilization of warring nations, great poverty, and immense wealth. Now Shevek, a brilliant physicist, is determined to reunite the two planets, which have been divided by centuries of distrust. He will seek ans wers, question the unquestionable, and attempt to tear down the walls of hatred that have kept them apart.To visit Urras\u2014to learn, to teach, to share\u2014will require great sacrifice and risks, which Shevek willingly accepts. But the ambitious scientist ' s gift is soon seen as a threat, and in the profound conflict that ensues, he must reexamine his beliefs even as he ignites the fires of change. Cover : cover.jpg Or in xml if you use the -o flag. I've checked if these tags could be automatically applied to Readarr, but their tags are meant only to be attached to Authors to apply metadata profiles. I've opened an issue to see if they plan to implement tags for books. It's a pity we are not going to use calibre-web as it also had support to sync the reading stats from Kobo . In the past I used gcstar and then polar bookshelf , but decided not to use them anymore for different reasons. In conclusion, the tools reviewed don't work as I need them to, some ugly patches could be applied and maybe it would work, but it clearly shows that they are not ready yet unless you want to invest time in it, and even if you did, it will be unstable. Until a better system shows up, I'm going to use Readarr to browse the books that I want to read, and add them to an ordered markdown file with sections as genres, not ideal, but robust as hell xD. Review system \u2691 System to write reviews and rate books, if the gallery browser doesn't include it, we'll use an independent component. Until I find something better, I'm saving the title, author, genre, score, and review in a json file, so it's easy to import in the chosen component. Content discovery \u2691 Recommendation system to analyze the user taste and suggest books that might like. Right now I'm monitoring the authors with Readarr to get notifications when they release a new book. I also manually go through goodreads and similar websites looking for similar books to the ones I liked. Deprecated components \u2691 Polar bookself \u2691 It was a very promising piece of software that went wrong :(. It had a nice interface built for incremental reading and studying with anki, and a nice tag system. It was a desktop application you installed in your computer, but since Polar 2.0 they moved into a cloud hosted service, with no possibility of self-hosting it, so you give them your books and all your data, a nasty turn of events. GCStar \u2691 The first free open source application for managing collections I used, it has an old looking desktop interface and is no longer maintained.","title":"Book Management"},{"location":"book_management/#downloader-and-indexer","text":"System that monitors the availability of books in a list of indexers, when they are available, they download it to a directory of the server. The best one that I've found is Readarr , it makes it easy to search for authors and books, supports a huge variety of indexers (such as Archive.org), and download clients (such as torrent clients). It can be used as a limited gallery browser, you can easily see the books of an author or series, but it doesn't yet support the automatic fetch of genres or tags. I haven't found an easy way of marking elements as read, prioritize the list of books to read, or add a user rating. Until these features are added (if they ever are), we need to use it in parallel with a better gallery browser.","title":"Downloader and indexer"},{"location":"book_management/#gallery-browser","text":"System that shows the books in the library in a nice format, allowing the user to filter out the contents, prioritize them, mark them as read, rate them and optionally sync books with the ereader. Calibre-web is a beautiful solution, without trying it, it looks like it supports all of the required features, but it doesn't work well with Readarr. Readarr has support to interact with Calibre content server by defining a root folder to be managed by Calibre , but the books you want to have Readarr recognize on initial library import must already be in Calibre. Books within the folder and not in Calibre will be ignored. So you'll need to do the first import in Calibre, instead of Readarr (which is quite pleasant). Note also that you cannot add Calibre integration to a root folder after it's created. Calibre-web interacts directly with the Sqlite database of Calibre, so it doesn't expose the Calibre Content Server, therefore is not compatible with Readarr. To make it work, you'd need to have both the calibre server and the calibre-web running at the same time, which has led to database locks ( 1 , and 2 ) that the calibre-web developer has tried to avoid by controlling the database writes , and said that : If you start Calibre first and afterwards Calibre-Web, Calibre indeed locks the database and doesn't allow Calibre-Web to access the database (metadata.db) file. Starting Calibre-Web and afterwards Calibre should work. The problem comes when Readarr writes in the database through calibre to add books, and calibre-web tries to write too to add user ratings or other metadata. Another option would be to only run calibre-web and automatically import the books once they are downloaded by Readarr. calibre-web is not going to support a watch directory feature, the author recommends to use a cron script to do it. I haven't tried this path yet. Another option would be to assume that calibre-web is not going to do any insert in the database, so it would become a read-only web interface, therefore we wouldn't be able to edit the books or rate them, one of the features we'd like to have in the gallery browser. To make sure that we don't get locks, instead of using the same file, a cron job could do an rsync between the database managed by calibre and the one used by calibre-web . Calibre implements genres with tags, behind the scenes it uses the fetch-ebook-metadata command line tool, that returns all the metadata in human readable form $: fetch-ebook-metadata -i 9780061796883 -c cover.jpg Title : The Dispossessed Author ( s ) : Ursula K. le Guin Publisher : Harper Collins Tags : Fiction, Science Fiction, Space Exploration, Literary, Visionary & Metaphysical Languages : eng Published : 2009 -10-13T20:34:30.878865+00:00 Identifiers : google:tlhFtmTixvwC, isbn:9780061796883 Comments : \u201cOne of the greats\u2026.Not just a science fiction writer ; a literary icon.\u201d \u2013 Stephen KingFrom the brilliant and award-winning author Ursula K. Le Guin comes a classic tale of two planets torn apart by conflict and mistrust \u2014 and the man who risks everything to reunite them.A bleak moon settled by utopian anarchists, Anarres has long been isolated from other worlds, including its mother planet, Urras\u2014a civilization of warring nations, great poverty, and immense wealth. Now Shevek, a brilliant physicist, is determined to reunite the two planets, which have been divided by centuries of distrust. He will seek ans wers, question the unquestionable, and attempt to tear down the walls of hatred that have kept them apart.To visit Urras\u2014to learn, to teach, to share\u2014will require great sacrifice and risks, which Shevek willingly accepts. But the ambitious scientist ' s gift is soon seen as a threat, and in the profound conflict that ensues, he must reexamine his beliefs even as he ignites the fires of change. Cover : cover.jpg Or in xml if you use the -o flag. I've checked if these tags could be automatically applied to Readarr, but their tags are meant only to be attached to Authors to apply metadata profiles. I've opened an issue to see if they plan to implement tags for books. It's a pity we are not going to use calibre-web as it also had support to sync the reading stats from Kobo . In the past I used gcstar and then polar bookshelf , but decided not to use them anymore for different reasons. In conclusion, the tools reviewed don't work as I need them to, some ugly patches could be applied and maybe it would work, but it clearly shows that they are not ready yet unless you want to invest time in it, and even if you did, it will be unstable. Until a better system shows up, I'm going to use Readarr to browse the books that I want to read, and add them to an ordered markdown file with sections as genres, not ideal, but robust as hell xD.","title":"Gallery browser"},{"location":"book_management/#review-system","text":"System to write reviews and rate books, if the gallery browser doesn't include it, we'll use an independent component. Until I find something better, I'm saving the title, author, genre, score, and review in a json file, so it's easy to import in the chosen component.","title":"Review system"},{"location":"book_management/#content-discovery","text":"Recommendation system to analyze the user taste and suggest books that might like. Right now I'm monitoring the authors with Readarr to get notifications when they release a new book. I also manually go through goodreads and similar websites looking for similar books to the ones I liked.","title":"Content discovery"},{"location":"book_management/#deprecated-components","text":"","title":"Deprecated components"},{"location":"book_management/#polar-bookself","text":"It was a very promising piece of software that went wrong :(. It had a nice interface built for incremental reading and studying with anki, and a nice tag system. It was a desktop application you installed in your computer, but since Polar 2.0 they moved into a cloud hosted service, with no possibility of self-hosting it, so you give them your books and all your data, a nasty turn of events.","title":"Polar bookself"},{"location":"book_management/#gcstar","text":"The first free open source application for managing collections I used, it has an old looking desktop interface and is no longer maintained.","title":"GCStar"},{"location":"boto3/","text":"Boto3 is the AWS SDK for Python to create, configure, and manage AWS services, such as Amazon Elastic Compute Cloud (Amazon EC2) and Amazon Simple Storage Service (Amazon S3). The SDK provides an object-oriented API as well as low-level access to AWS services. Installation \u2691 pip install boto3 Usage \u2691 Run EC2 instance \u2691 Use the run_instances method of the ec2 client. Check their docs for the different configuration options. The required ones are MinCount and MaxCount . import boto3 ec2 = boto3 . client ( 'ec2' ) instance = ec2 . run_instances ( MinCount = 1 , MaxCount = 1 ) Type hints \u2691 AWS library doesn't have working type hints -.- , so you either use Any or dive into the myriad of packages that implement them. I've so far tried boto3_type_annotations , boto3-stubs , and mypy_boto3_builder without success. Any it is for now... Testing \u2691 Programs that interact with AWS through boto3 create, change or get information on real AWS resources. When developing these programs, you don't want the testing framework to actually do those changes, as it might break things and cost you money. You need to find a way to intercept the calls to AWS and substitute them with the data their API would return. I've found three ways to achieve this: Manually mocking the boto3 methods used by the program with unittest.mock . Using moto . Using Botocore's Stubber . TL;DR Try to use moto, using the stubber as fallback option. Using unittest.mock forces you to know what the API is going to return and hardcode it in your tests. If the response changes, you need to update your tests, which is not good. moto is a library that allows you to easily mock out tests based on AWS infrastructure. It works well because it mocks out all calls to AWS automatically without requiring any dependency injection. The downside is that it goes behind boto3 so some of the methods you need to test won't be still implemented, that leads us to the third option. Botocore's Stubber is a class that allows you to stub out requests so you don't have to hit an endpoint to write tests. Responses are returned first in, first out. If operations are called out of order, or are called with no remaining queued responses, an error will be raised. It's like the first option but cleaner. If you go down this path, check adamj's post on testing S3 . moto \u2691 moto's library lets you fictitiously create and change AWS resources as you normally do with the boto3 library. They mimic what the real methods do on fake objects. The Docs are awful though. Install \u2691 pip install moto Simple usage \u2691 To understand better how it works, I'm going to show you an understandable example, it's not the best way to use it though, go to the usage section for production ready usage. Imagine you have a function that you use to launch new ec2 instances: import boto3 def add_servers ( ami_id , count ): client = boto3 . client ( 'ec2' , region_name = 'us-west-1' ) client . run_instances ( ImageId = ami_id , MinCount = count , MaxCount = count ) To test it we'd use: from . import add_servers from moto import mock_ec2 @mock_ec2 def test_add_servers (): add_servers ( 'ami-1234abcd' , 2 ) client = boto3 . client ( 'ec2' , region_name = 'us-west-1' ) instances = client . describe_instances ()[ 'Reservations' ][ 0 ][ 'Instances' ] assert len ( instances ) == 2 instance1 = instances [ 0 ] assert instance1 [ 'ImageId' ] == 'ami-1234abcd' The decorator @mock_ec2 tells moto to capture all boto3 calls to AWS. When we run the add_servers function to test, it will create the fake objects on the memory (without contacting AWS servers), and the client.describe_instances boto3 method returns the data of that fake data. Isn't it awesome? Usage \u2691 You can use it with decorators , context managers , directly or with pytest fixtures . Being a pytest fan, the last option looks the cleaner to me. To make sure that you don't change the real infrastructure, ensure that your tests have dummy environmental variables. File: tests/conftest.py @pytest . fixture () def _aws_credentials () -> None : \"\"\"Mock the AWS Credentials for moto.\"\"\" os . environ [ \"AWS_ACCESS_KEY_ID\" ] = \"testing\" os . environ [ \"AWS_SECRET_ACCESS_KEY\" ] = \"testing\" os . environ [ \"AWS_SECURITY_TOKEN\" ] = \"testing\" os . environ [ \"AWS_SESSION_TOKEN\" ] = \"testing\" @pytest . fixture () def ec2 ( _aws_credentials : None ) -> Any : \"\"\"Configure the boto3 EC2 client.\"\"\" with mock_ec2 (): yield boto3 . client ( \"ec2\" , region_name = \"us-east-1\" ) The ec2 fixture can then be used in the tests to setup the environment or assert results. Testing EC2 \u2691 If you want to add security groups to the tests, you need to create the resource first. def test_ec2_with_security_groups ( ec2 : Any ) -> None : security_group_id = ec2 . create_security_group ( GroupName = \"TestSecurityGroup\" , Description = \"SG description\" )[ \"GroupId\" ] instance = ec2 . run_instances ( ImageId = \"ami-xxxx\" , MinCount = 1 , MaxCount = 1 , SecurityGroupIds = [ security_group_id ], )[ \"Instances\" ][ 0 ] # Test your code here To add tags, use: def test_ec2_with_security_groups ( ec2 : Any ) -> None : instance = ec2 . run_instances ( ImageId = \"ami-xxxx\" , MinCount = 1 , MaxCount = 1 , TagSpecifications = [ { \"ResourceType\" : \"instance\" , \"Tags\" : [ { \"Key\" : \"Name\" , \"Value\" : \"instance name\" , }, ], } ], )[ \"Instances\" ][ 0 ] # Test your code here Testing RDS \u2691 Use the rds fixture: from moto import mock_rds2 @pytest . fixture () def rds ( _aws_credentials : None ) -> Any : \"\"\"Configure the boto3 RDS client.\"\"\" with mock_rds2 (): yield boto3 . client ( \"rds\" , region_name = \"us-east-1\" ) To create an instance use: instance = rds . create_db_instance ( DBInstanceIdentifier = \"db-xxxx\" , DBInstanceClass = \"db.m3.2xlarge\" , Engine = \"postgres\" , )[ \"DBInstance\" ] It won't have VPC information, if you need it, create the subnet group first (you'll need the ec2 fixture too): subnets = [ subnet [ 'SubnetId' ] for subnet in ec2 . describe_subnets ()[ \"Subnets\" ]] rds . create_db_subnet_group ( DBSubnetGroupName = \"dbsg\" , SubnetIds = subnets , DBSubnetGroupDescription = \"Text\" ) instance = rds . create_db_instance ( DBInstanceIdentifier = \"db-xxxx\" , DBInstanceClass = \"db.m3.2xlarge\" , Engine = \"postgres\" , DBSubnetGroupName = \"dbsg\" , )[ \"DBInstance\" ] Testing S3 \u2691 Use the s3_mock fixture: from moto import mock_s3 @pytest . fixture () def s3_mock ( _aws_credentials : None ) -> Any : \"\"\"Configure the boto3 S3 client.\"\"\" with mock_s3 (): yield boto3 . client ( \"s3\" ) To create an instance use: s3_mock . create_bucket ( Bucket = \"mybucket\" ) instance = s3_mock . list_buckets ()[ \"Buckets\" ][ 0 ] Check the official docs to check the create_bucket arguments. Testing Route53 \u2691 Use the route53 fixture: from moto import mock_route53 @pytest . fixture ( name = 'route53' ) def route53_ ( _aws_credentials : None ) -> Any : \"\"\"Configure the boto3 Route53 client.\"\"\" with mock_route53 (): yield boto3 . client ( \"route53\" ) To create an instance use: hosted_zone = route53 . create_hosted_zone ( Name = \"example.com\" , CallerReference = \"Test\" )[ \"HostedZone\" ] hosted_zone_id = re . sub ( \".hostedzone.\" , \"\" , hosted_zone [ \"Id\" ]) route53 . change_resource_record_sets ( ChangeBatch = { \"Changes\" : [ { \"Action\" : \"CREATE\" , \"ResourceRecordSet\" : { \"Name\" : \"example.com\" , \"ResourceRecords\" : [ { \"Value\" : \"192.0.2.44\" , }, ], \"TTL\" : 60 , \"Type\" : \"A\" , }, }, ], \"Comment\" : \"Web server for example.com\" , }, HostedZoneId = hosted_zone_id , ) You need to first create a hosted zone. The change_resource_record_sets order to create the instance doesn't return any data, so if you need to work on it, use the list_resource_record_sets method of the route53 client (you'll need to set the HostedZoneId argument). If you have more than 300 records, the endpoint gives you a paginated response, so if the IsTruncated attribute is True , you need to call the method again setting the StartRecordName and StartRecordType to the NextRecordName and NextRecordType response arguments. Not nice at all. Pagination is not yet supported by moto , so you won't be able to test that part of your code. Check the official docs to check the method arguments: create_hosted_zone . change_resource_record_sets . Test VPC \u2691 Use the ec2 fixture defined in the usage section . To create an instance use: instance = ec2 . create_vpc ( CidrBlock = \"172.16.0.0/16\" , )[ \"Vpc\" ] Check the official docs to check the method arguments: create_vpc . create_subnet . Testing autoscaling groups \u2691 Use the autoscaling fixture: from moto import mock_autoscaling @pytest . fixture ( name = 'autoscaling' ) def autoscaling_ ( _aws_credentials : None ) -> Any : \"\"\"Configure the boto3 Autoscaling Group client.\"\"\" with mock_autoscaling (): yield boto3 . client ( \"autoscaling\" ) They don't yet support LaunchTemplates , so you'll have to use LaunchConfigurations. To create an instance use: autoscaling . create_launch_configuration ( LaunchConfigurationName = 'LaunchConfiguration' , ImageId = 'ami-xxxx' , InstanceType = 't2.medium' ) autoscaling . create_auto_scaling_group ( AutoScalingGroupName = 'ASG name' , MinSize = 1 , MaxSize = 3 , LaunchConfigurationName = 'LaunchConfiguration' , AvailabilityZones = [ 'us-east-1a' ]) instance = autoscaling . describe_auto_scaling_groups ()[ \"AutoScalingGroups\" ][ 0 ] Check the official docs to check the method arguments: create_auto_scaling_group . create_launch_configuration . describe_auto_scaling_groups . Test Security Groups \u2691 Use the ec2 fixture defined in the usage section . To create an instance use: instance_id = ec2 . create_security_group ( GroupName = \"TestSecurityGroup\" , Description = \"SG description\" )[ \"GroupId\" ] instance = ec2 . describe_security_groups ( GroupIds = [ instance_id ]) To add permissions to the security group you need to use the authorize_security_group_ingress and authorize_security_group_egress methods. ec2 . authorize_security_group_ingress ( GroupId = instance_id , IpPermissions = [ { \"IpProtocol\" : \"tcp\" , \"FromPort\" : 80 , \"ToPort\" : 80 , \"IpRanges\" : [{ \"CidrIp\" : \"0.0.0.0/0\" }], }, ], ) By default, the created security group comes with an egress rule to allow all traffic. To remove rules use the revoke_security_group_egress and revoke_security_group_ingress methods. ec2 . revoke_security_group_egress ( GroupId = instance_id , IpPermissions = [ { \"IpProtocol\" : \"-1\" , \"IpRanges\" : [{ \"CidrIp\" : \"0.0.0.0/0\" }]}, ], ) Check the official docs to check the method arguments: create_security_group . describe_security_group . authorize_security_group_ingress . authorize_security_group_egress . revoke_security_group_ingress . revoke_security_group_egress . Test IAM users \u2691 Use the iam fixture: from moto import mock_iam @pytest . fixture ( name = 'iam' ) def iam_ ( _aws_credentials : None ) -> Any : \"\"\"Configure the boto3 IAM client.\"\"\" with mock_iam (): yield boto3 . client ( \"iam\" ) To create an instance use: instance = iam . create_user ( UserName = \"User\" )[ \"User\" ] Check the official docs to check the method arguments: create_user list_users Test IAM Groups \u2691 Use the iam fixture defined in the test IAM users section : To create an instance use: user = iam . create_user ( UserName = \"User\" )[ \"User\" ] instance = iam . create_group ( GroupName = \"UserGroup\" )[ \"Group\" ] iam . add_user_to_group ( GroupName = instance [ \"GroupName\" ], UserName = user [ \"UserName\" ]) Check the official docs to check the method arguments: create_group add_user_to_group Issues \u2691 Support LaunchTemplates : Once they are, test clinv autoscaling group adapter support for launch templates. Support Route53 pagination : test clinv route53 update and update the test route53 section. cn-north-1 rds and autoscaling errors : increase the timeout of clinv, and test if the coverage has changed. References \u2691 Git Docs","title":"Boto3"},{"location":"boto3/#installation","text":"pip install boto3","title":"Installation"},{"location":"boto3/#usage","text":"","title":"Usage"},{"location":"boto3/#run-ec2-instance","text":"Use the run_instances method of the ec2 client. Check their docs for the different configuration options. The required ones are MinCount and MaxCount . import boto3 ec2 = boto3 . client ( 'ec2' ) instance = ec2 . run_instances ( MinCount = 1 , MaxCount = 1 )","title":"Run EC2 instance"},{"location":"boto3/#type-hints","text":"AWS library doesn't have working type hints -.- , so you either use Any or dive into the myriad of packages that implement them. I've so far tried boto3_type_annotations , boto3-stubs , and mypy_boto3_builder without success. Any it is for now...","title":"Type hints"},{"location":"boto3/#testing","text":"Programs that interact with AWS through boto3 create, change or get information on real AWS resources. When developing these programs, you don't want the testing framework to actually do those changes, as it might break things and cost you money. You need to find a way to intercept the calls to AWS and substitute them with the data their API would return. I've found three ways to achieve this: Manually mocking the boto3 methods used by the program with unittest.mock . Using moto . Using Botocore's Stubber . TL;DR Try to use moto, using the stubber as fallback option. Using unittest.mock forces you to know what the API is going to return and hardcode it in your tests. If the response changes, you need to update your tests, which is not good. moto is a library that allows you to easily mock out tests based on AWS infrastructure. It works well because it mocks out all calls to AWS automatically without requiring any dependency injection. The downside is that it goes behind boto3 so some of the methods you need to test won't be still implemented, that leads us to the third option. Botocore's Stubber is a class that allows you to stub out requests so you don't have to hit an endpoint to write tests. Responses are returned first in, first out. If operations are called out of order, or are called with no remaining queued responses, an error will be raised. It's like the first option but cleaner. If you go down this path, check adamj's post on testing S3 .","title":"Testing"},{"location":"boto3/#moto","text":"moto's library lets you fictitiously create and change AWS resources as you normally do with the boto3 library. They mimic what the real methods do on fake objects. The Docs are awful though.","title":"moto"},{"location":"boto3/#install","text":"pip install moto","title":"Install"},{"location":"boto3/#simple-usage","text":"To understand better how it works, I'm going to show you an understandable example, it's not the best way to use it though, go to the usage section for production ready usage. Imagine you have a function that you use to launch new ec2 instances: import boto3 def add_servers ( ami_id , count ): client = boto3 . client ( 'ec2' , region_name = 'us-west-1' ) client . run_instances ( ImageId = ami_id , MinCount = count , MaxCount = count ) To test it we'd use: from . import add_servers from moto import mock_ec2 @mock_ec2 def test_add_servers (): add_servers ( 'ami-1234abcd' , 2 ) client = boto3 . client ( 'ec2' , region_name = 'us-west-1' ) instances = client . describe_instances ()[ 'Reservations' ][ 0 ][ 'Instances' ] assert len ( instances ) == 2 instance1 = instances [ 0 ] assert instance1 [ 'ImageId' ] == 'ami-1234abcd' The decorator @mock_ec2 tells moto to capture all boto3 calls to AWS. When we run the add_servers function to test, it will create the fake objects on the memory (without contacting AWS servers), and the client.describe_instances boto3 method returns the data of that fake data. Isn't it awesome?","title":"Simple usage"},{"location":"boto3/#usage_1","text":"You can use it with decorators , context managers , directly or with pytest fixtures . Being a pytest fan, the last option looks the cleaner to me. To make sure that you don't change the real infrastructure, ensure that your tests have dummy environmental variables. File: tests/conftest.py @pytest . fixture () def _aws_credentials () -> None : \"\"\"Mock the AWS Credentials for moto.\"\"\" os . environ [ \"AWS_ACCESS_KEY_ID\" ] = \"testing\" os . environ [ \"AWS_SECRET_ACCESS_KEY\" ] = \"testing\" os . environ [ \"AWS_SECURITY_TOKEN\" ] = \"testing\" os . environ [ \"AWS_SESSION_TOKEN\" ] = \"testing\" @pytest . fixture () def ec2 ( _aws_credentials : None ) -> Any : \"\"\"Configure the boto3 EC2 client.\"\"\" with mock_ec2 (): yield boto3 . client ( \"ec2\" , region_name = \"us-east-1\" ) The ec2 fixture can then be used in the tests to setup the environment or assert results.","title":"Usage"},{"location":"boto3/#testing-ec2","text":"If you want to add security groups to the tests, you need to create the resource first. def test_ec2_with_security_groups ( ec2 : Any ) -> None : security_group_id = ec2 . create_security_group ( GroupName = \"TestSecurityGroup\" , Description = \"SG description\" )[ \"GroupId\" ] instance = ec2 . run_instances ( ImageId = \"ami-xxxx\" , MinCount = 1 , MaxCount = 1 , SecurityGroupIds = [ security_group_id ], )[ \"Instances\" ][ 0 ] # Test your code here To add tags, use: def test_ec2_with_security_groups ( ec2 : Any ) -> None : instance = ec2 . run_instances ( ImageId = \"ami-xxxx\" , MinCount = 1 , MaxCount = 1 , TagSpecifications = [ { \"ResourceType\" : \"instance\" , \"Tags\" : [ { \"Key\" : \"Name\" , \"Value\" : \"instance name\" , }, ], } ], )[ \"Instances\" ][ 0 ] # Test your code here","title":"Testing EC2"},{"location":"boto3/#testing-rds","text":"Use the rds fixture: from moto import mock_rds2 @pytest . fixture () def rds ( _aws_credentials : None ) -> Any : \"\"\"Configure the boto3 RDS client.\"\"\" with mock_rds2 (): yield boto3 . client ( \"rds\" , region_name = \"us-east-1\" ) To create an instance use: instance = rds . create_db_instance ( DBInstanceIdentifier = \"db-xxxx\" , DBInstanceClass = \"db.m3.2xlarge\" , Engine = \"postgres\" , )[ \"DBInstance\" ] It won't have VPC information, if you need it, create the subnet group first (you'll need the ec2 fixture too): subnets = [ subnet [ 'SubnetId' ] for subnet in ec2 . describe_subnets ()[ \"Subnets\" ]] rds . create_db_subnet_group ( DBSubnetGroupName = \"dbsg\" , SubnetIds = subnets , DBSubnetGroupDescription = \"Text\" ) instance = rds . create_db_instance ( DBInstanceIdentifier = \"db-xxxx\" , DBInstanceClass = \"db.m3.2xlarge\" , Engine = \"postgres\" , DBSubnetGroupName = \"dbsg\" , )[ \"DBInstance\" ]","title":"Testing RDS"},{"location":"boto3/#testing-s3","text":"Use the s3_mock fixture: from moto import mock_s3 @pytest . fixture () def s3_mock ( _aws_credentials : None ) -> Any : \"\"\"Configure the boto3 S3 client.\"\"\" with mock_s3 (): yield boto3 . client ( \"s3\" ) To create an instance use: s3_mock . create_bucket ( Bucket = \"mybucket\" ) instance = s3_mock . list_buckets ()[ \"Buckets\" ][ 0 ] Check the official docs to check the create_bucket arguments.","title":"Testing S3"},{"location":"boto3/#testing-route53","text":"Use the route53 fixture: from moto import mock_route53 @pytest . fixture ( name = 'route53' ) def route53_ ( _aws_credentials : None ) -> Any : \"\"\"Configure the boto3 Route53 client.\"\"\" with mock_route53 (): yield boto3 . client ( \"route53\" ) To create an instance use: hosted_zone = route53 . create_hosted_zone ( Name = \"example.com\" , CallerReference = \"Test\" )[ \"HostedZone\" ] hosted_zone_id = re . sub ( \".hostedzone.\" , \"\" , hosted_zone [ \"Id\" ]) route53 . change_resource_record_sets ( ChangeBatch = { \"Changes\" : [ { \"Action\" : \"CREATE\" , \"ResourceRecordSet\" : { \"Name\" : \"example.com\" , \"ResourceRecords\" : [ { \"Value\" : \"192.0.2.44\" , }, ], \"TTL\" : 60 , \"Type\" : \"A\" , }, }, ], \"Comment\" : \"Web server for example.com\" , }, HostedZoneId = hosted_zone_id , ) You need to first create a hosted zone. The change_resource_record_sets order to create the instance doesn't return any data, so if you need to work on it, use the list_resource_record_sets method of the route53 client (you'll need to set the HostedZoneId argument). If you have more than 300 records, the endpoint gives you a paginated response, so if the IsTruncated attribute is True , you need to call the method again setting the StartRecordName and StartRecordType to the NextRecordName and NextRecordType response arguments. Not nice at all. Pagination is not yet supported by moto , so you won't be able to test that part of your code. Check the official docs to check the method arguments: create_hosted_zone . change_resource_record_sets .","title":"Testing Route53"},{"location":"boto3/#test-vpc","text":"Use the ec2 fixture defined in the usage section . To create an instance use: instance = ec2 . create_vpc ( CidrBlock = \"172.16.0.0/16\" , )[ \"Vpc\" ] Check the official docs to check the method arguments: create_vpc . create_subnet .","title":"Test VPC"},{"location":"boto3/#testing-autoscaling-groups","text":"Use the autoscaling fixture: from moto import mock_autoscaling @pytest . fixture ( name = 'autoscaling' ) def autoscaling_ ( _aws_credentials : None ) -> Any : \"\"\"Configure the boto3 Autoscaling Group client.\"\"\" with mock_autoscaling (): yield boto3 . client ( \"autoscaling\" ) They don't yet support LaunchTemplates , so you'll have to use LaunchConfigurations. To create an instance use: autoscaling . create_launch_configuration ( LaunchConfigurationName = 'LaunchConfiguration' , ImageId = 'ami-xxxx' , InstanceType = 't2.medium' ) autoscaling . create_auto_scaling_group ( AutoScalingGroupName = 'ASG name' , MinSize = 1 , MaxSize = 3 , LaunchConfigurationName = 'LaunchConfiguration' , AvailabilityZones = [ 'us-east-1a' ]) instance = autoscaling . describe_auto_scaling_groups ()[ \"AutoScalingGroups\" ][ 0 ] Check the official docs to check the method arguments: create_auto_scaling_group . create_launch_configuration . describe_auto_scaling_groups .","title":"Testing autoscaling groups"},{"location":"boto3/#test-security-groups","text":"Use the ec2 fixture defined in the usage section . To create an instance use: instance_id = ec2 . create_security_group ( GroupName = \"TestSecurityGroup\" , Description = \"SG description\" )[ \"GroupId\" ] instance = ec2 . describe_security_groups ( GroupIds = [ instance_id ]) To add permissions to the security group you need to use the authorize_security_group_ingress and authorize_security_group_egress methods. ec2 . authorize_security_group_ingress ( GroupId = instance_id , IpPermissions = [ { \"IpProtocol\" : \"tcp\" , \"FromPort\" : 80 , \"ToPort\" : 80 , \"IpRanges\" : [{ \"CidrIp\" : \"0.0.0.0/0\" }], }, ], ) By default, the created security group comes with an egress rule to allow all traffic. To remove rules use the revoke_security_group_egress and revoke_security_group_ingress methods. ec2 . revoke_security_group_egress ( GroupId = instance_id , IpPermissions = [ { \"IpProtocol\" : \"-1\" , \"IpRanges\" : [{ \"CidrIp\" : \"0.0.0.0/0\" }]}, ], ) Check the official docs to check the method arguments: create_security_group . describe_security_group . authorize_security_group_ingress . authorize_security_group_egress . revoke_security_group_ingress . revoke_security_group_egress .","title":"Test Security Groups"},{"location":"boto3/#test-iam-users","text":"Use the iam fixture: from moto import mock_iam @pytest . fixture ( name = 'iam' ) def iam_ ( _aws_credentials : None ) -> Any : \"\"\"Configure the boto3 IAM client.\"\"\" with mock_iam (): yield boto3 . client ( \"iam\" ) To create an instance use: instance = iam . create_user ( UserName = \"User\" )[ \"User\" ] Check the official docs to check the method arguments: create_user list_users","title":"Test IAM users"},{"location":"boto3/#test-iam-groups","text":"Use the iam fixture defined in the test IAM users section : To create an instance use: user = iam . create_user ( UserName = \"User\" )[ \"User\" ] instance = iam . create_group ( GroupName = \"UserGroup\" )[ \"Group\" ] iam . add_user_to_group ( GroupName = instance [ \"GroupName\" ], UserName = user [ \"UserName\" ]) Check the official docs to check the method arguments: create_group add_user_to_group","title":"Test IAM Groups"},{"location":"boto3/#issues","text":"Support LaunchTemplates : Once they are, test clinv autoscaling group adapter support for launch templates. Support Route53 pagination : test clinv route53 update and update the test route53 section. cn-north-1 rds and autoscaling errors : increase the timeout of clinv, and test if the coverage has changed.","title":"Issues"},{"location":"boto3/#references","text":"Git Docs","title":"References"},{"location":"cone/","text":"Cone is a mobile ledger application compatible with beancount . I use it as part of my accounting automation workflow . Installation \u2691 Download the application from F-droid . It assumes that you have a txt file to store the information . As it doesn't yet support the edition or deletion of transactions , I suggest you create the ledger.txt file with your favorite mobile editor such as Markor . Open the application and load the ledger.txt file. Usage \u2691 To be compliant with my beancount ledger: I've initialized the ledger.txt file with the open statements of the beancount accounts, so the transaction UI autocompletes them. Cone doesn't still support the beancount format by default, so in the description of the transaction I also introduce the payee. For example: * \"payee1\" \"Bought X instead of just Bought X . If I need to edit or delete a transaction, I change it with the Markor editor. To send the ledger file to the computer, I use either Share via HTTP or Termux through ssh. References \u2691 Git Docs","title":"cone"},{"location":"cone/#installation","text":"Download the application from F-droid . It assumes that you have a txt file to store the information . As it doesn't yet support the edition or deletion of transactions , I suggest you create the ledger.txt file with your favorite mobile editor such as Markor . Open the application and load the ledger.txt file.","title":"Installation"},{"location":"cone/#usage","text":"To be compliant with my beancount ledger: I've initialized the ledger.txt file with the open statements of the beancount accounts, so the transaction UI autocompletes them. Cone doesn't still support the beancount format by default, so in the description of the transaction I also introduce the payee. For example: * \"payee1\" \"Bought X instead of just Bought X . If I need to edit or delete a transaction, I change it with the Markor editor. To send the ledger file to the computer, I use either Share via HTTP or Termux through ssh.","title":"Usage"},{"location":"cone/#references","text":"Git Docs","title":"References"},{"location":"contact/","text":"I'm available through: Email: lyz at riseup.net PGP Key: 6ADA882386CDF9BD1884534C6C7D7C1612CDE02F -----BEGIN PGP PUBLIC KEY BLOCK----- mQINBFhs5wUBEAC289UxruAPfjvJ723AKhUhRI0/fw+cG0IeSUJfOSvWW+HJ7Elo QoPkKYv6E1k4SzIt6AgbEWpL35PQP79aQ5BFog2SbfVvfnq1/gIasFlyeFX1BUTh zxTKrYKwbUdsTeMYw32v5p2Q+D8CZK6/0RCM/GSb5oMPVancOeoZs8IebKpJH2x7 HCniyQbq7xiFU5sUyB6tmgCiXg8INib+oTZqGKW/sVaxmTdH+fF9a2nnH0TN8h2W 5V5XQ9/VQZk/GHQVq/Y0Z73BibOJM5Bv+3r2EIJfozlpWdUblat45lSATBo/sktf YKlxwAztWPtcTavJ58F1ufGcUPjwGW4E92zRaozC+tpzd5QtHeYM7m6fGlXxckua UesZcZLl9pY4Bc8Mw40WvI1ibhA2mP2R5AO8hJ0vJyFfi35lqM/DJVV1900yp+em uY+u6bNJ1gLLb7QnhbV1VYLTSCoWzPQvWHgMHAKpAjO15rKAItXD17BM2eQgJMuX LcoWeOcz/MrMQiGKSkqpmapwgtDZ5t81D2qWv+wsaZgcO/erknugHFmR3kAP8YHp JsIpaYY7kj+yVJb92uzZKQAEaUpq3uRsBDtkoC2MPzKN4fgWa8f4jBpIzxuBTd+6 75sVq5VB5eaq3w4J0Z4kbk1DVyNffv3LeZCv9oC2mb1aXyVD/gWHlPD+6wARAQAB tBZseXouLiA8bHl6QHJpc2V1cC5uZXQ+iQJUBBMBCAA+AhsDBQsJCAcCBhUICQoL AgQWAgMBAh4BAheAFiEEatqII4bN+b0YhFNMbH18FhLN4C8FAl/oSy0FCQlcl6gA CgkQbH18FhLN4C/7Xg/+IyKUbaRwTYX+BqJ0kbi8auM0m+8oUFtFBK7FIzR860m/ n+PDmKxLKHEi1yaa6gyylLHub3OSNI8DgiFD3eQiT3eKjotOaGwFK2lziynysO5k Jdq3wAlzteoHXblUvtXt4RD7unBlE2LIlmq2KR2jNvtHbawhRNsjvcuft9rvkMot 2qgZfZBixXKWYU0u9e+nucfR0dwlnc3kXnS/VFF3pfQBNLy515v6pA9TXUNMZ9+K ZQeRRfIqVpr7Tu/4f0M4wPbW80cR1tcRz8vOOTHXi1+KsnpkjkwAYdlNmL7uziGQ xuhBlCrEM94OauuBvMkACysAOfor8SgpNYovuAOxYrojLwr7EBE+MI60cNpWP5tv QTC6kcATf3QFAtnm3INEfkh9s/7HSEsImWOwMTLlnWLTcto3Q2LLY5O6OX8ak0sv iYRMAjHdoCp3bgWJl+wrrVV0G7VQMJFC895Z21JrUBBroGSyuefHOrkN13qsrO1K wJovpvlWkf5PzyJnIAuDASh7SvJaocOOD3zOCW/RxGY0W51+T2fCpGbNA54JJ2gj kplJo00ekXWGxv/fxguNgrtkV3F9GbubQ00SyXZ4tbRfgpZLOdjxblPjvPptIXIt tZxmQ7YWa/MyUBXLiwvDfxDYiGxEamqzrYZO4xiyM+PvQ+ZDlzEcjpFztNxmjnW5 Ag0EWGznBQEQALNL9sNc4SytS3fOcS4gHvZpH3TLJ6o0K/Lxg4RfkLMebDJwWvSW mjQLv3GqRfOhGj2Osi2YukFIJb4vxPJFO7wQhCi5LLSVEb5d/z9ZOJUdGdI9JvGW dFDuLEXwDnJaP5Jmjm3DwbvHK+goI7Fn3TKc27iqOVAKVIjWNPaqFZxwIE9o/+1c 3bTk3A8WOBmcv1IaxsUNkRDOFJlQYLM/bFIuDD+cW/CcYro8ouC9aekmvTDoRaU5 xv++fXtesn6Cy+xBgvBGIIXGo5xzd6Y66Yf8uNpuJXo9Dc6rApH1QEQNwZX1cxvG UpQx+9JNF0eptDLvTgmxcCglllrylcw8ZsVEt6BTgrCd2JXMGxUcAnhXpRWRmXNL n97FOBb6OBd6k7DC6QCiVKr7sytq1Ywl8GTtWrTP7sK+/+KDLPJ/oY7+bwV94+N8 Gthr94njNqb5G6t9fqQ/+cJv7oF8DoBvylYGqm2hvYpOH53hMq1y3OTPoFKP6AIx twIWHkdmMALm6a6bxAetGQxiaPZTOduJDehwiF9EUkiNhpESMl3I2+vH86jV2IiT 4BuUqGBU5wrAN/FixIRlmaSUX7e0OkUkDexVlpw5poJbPEbvhOtuj/V9BOxQKWB4 bjXMHEHR5YcJ1lhPjFFM3pqOz6ZaN8Hs70KOBE+/3/c1hS5debWPBMdlABEBAAGJ AjwEGAEIACYCGwwWIQRq2ogjhs35vRiEU0xsfXwWEs3gLwUCX+hLMwUJCVyXrgAK CRBsfXwWEs3gL2JMEACZyzLv3IubRVR6cP5HyKbdAA4+fgzONwxw1TloC3T0okjN wHuhZJuv22GEaicklqHceIIYYssh7jER+plaDBA8t8xsvJ/7T3xMAt5RMsU2INWc KFkUkOmj3Q0aLnyVGKlrynMGnetMA+OliMnOrWWqDIomrfcJjdc3/OpCigR/lgCL jItH4nH/NwjyexcQuKfpDgGcxUQhhPyfkyQrOxRBRXgjhB1Q3ra6MqM2g36EPLn3 hSoQFbmUIbOYye6Vn7fENC5fmRS/4RcGGaq+wlAK0Mc1U8Bzl0AVBU3q+bKgDihO dRWEz9GV2UuDN7MhuUMSX3GFWIS/Gd0fc6EqDDHP0IWdwd268S5jTqvaz9IddQJ3 vGPR++Vjex8VepCHsPBC2i7RmlBgbEvjWIHCEcBtyxd8TY7/3VFkzrQqY4bD0pyK l67QIP/ybgPqYgD0zfVyYa0oaZlk9OIH48SE4AwOVE7lTsEOWJ+EBodUtW094TFv nTZ2Uusuxg+rS6SbtDqcxvVBoSPYqtRSNS9FidWamuXudb6Ia/hLfBvDQxnzfaVR EGGpxmgzqLwKaGx7Cf5dnrfz2NVD9Mxb78n/Lk3qnQvD6CpzcdB+u4S+aWMzvuiN ziOWBy/hZuRWttlZW2dN290w0csREWldUw7jkAWUCxLiMePCOVUj9cNDlPbPeQ== =QNxk -----END PGP PUBLIC KEY BLOCK----- XMPP at lyz at disroot.org Through Github by opening an issue .","title":"Contact"},{"location":"cooking/","text":"Cooking as defined in Wikipedia, is the art, science, and craft of using heat to prepare food for consumption. It sounds like an enlightening experience that brings you joy. Reality then slaps you in the face yet once again. It's very different to cook because you want to, than cooking because you need to. I love to eat, but have always hated to cook, mainly because I've always seen cooking with that second view. Being something disgusting that I had to do, pushed me to batch cook once a week as quickly as possible, and to buy prepared food from the local squat center's tavern. Now I aim to shift my point of view to enjoy the time invested preparing food. Two are the main reasons: I'm going to spend a great amount of my life in front of the stove, so I'd better enjoy it, and probably the end result would be better for my well being. One way that can help me with the switch, is to understand the science behind it and be precise with the process. Thus this section was born, I'll start with the very basics and build from there on.","title":"Cooking"},{"location":"cooking_basics/","text":"All great recipes are based on the same basic principles and processes, these are the cooking snippets that I've needed. Boiling an egg \u2691 Cooking an egg well is a matter of time. Put enough water in the pot so that the eggs are completely covered. Add a pinch of salt and a dash of vinegar. Let the water boil. Use a kettle to heat it if you have one. Add the eggs. Depending on the type of egg you want, you need to wait more or less time: 5-6 minutes: You'll get soft boiled eggs, whose yolk is liquid and the white is semi-liquid. 7 minutes: mollet egs, with semi-liquid yolk and curdled white. 10-12 minutes: boiled eggs, compact white and curdled yolk. Pour off the hot water, shake it gently to crack the eggs, and add cold water, with even a few ice cubes. Wait 5 minutes if you want to serve them warm, or 15 otherwise, and then peel them under the same water. Here are some tips to improve your chances to get the perfect egg: Use fresh eggs, when they've been in the fridge for a while, they get dehydrated and the air that's inside gets expanded. That's why, when you put an egg into a glass of water, if it doesn't stay at the bottom you'd better not use it. Take them out of the fridge an hour before cooking them. (Yeah, like you're going to remember to do it :P). Cooking legumes \u2691 Legumes are wonderful, to squeeze their value remember to: Don't use old ones: If you're legumes are older than a year, they can be old . They loose water with the time up to a point that they can be impossible to cook. Soak them: Some legumes like the lentils don't need to be soaked, but for most of them it's better to be hydrated before putting them in the pot. For chickpeas and beans, the best is to soak them for 10 to 12 hours. They'll drink the water, so add it until you double the volume of the legumes. Once done, discard that water, rinse them and use new one for the pot. That way you'll prevent the acids and oligosaccharides that promotes a heavy digestion. * Don't scare them: Don't cut the cooking with cold water, they won't help you avoid farting and for some great chefs it's one of the worst errors you can do, at least with the chickpeas. Remember always to have enough water from the start to avoid this situation. * Know when to add them in the pot: chickpeas need to be added when the water is already boiling When you're using boiled legumes in your recipes, be careful, after the hydration, they weight the double! Boil chickpeas when you've forgotten to soak them \u2691 Soaked chickpeas take one or two hours to cook in a normal pot, 20 to 30 in a fast pot, which saves a ton of energy. If you forgot to soak them, add a level teaspoon of baking soda to the pot and cook them as usual. When not using a fast pot, you'll need to periodically remove the foam that will be created. The problem with this method is that you don't discard the first water round, and they can be more indigestible. Another option is to cook them for an hour, change the water and then cook them again.","title":"Cooking Basics"},{"location":"cooking_basics/#boiling-an-egg","text":"Cooking an egg well is a matter of time. Put enough water in the pot so that the eggs are completely covered. Add a pinch of salt and a dash of vinegar. Let the water boil. Use a kettle to heat it if you have one. Add the eggs. Depending on the type of egg you want, you need to wait more or less time: 5-6 minutes: You'll get soft boiled eggs, whose yolk is liquid and the white is semi-liquid. 7 minutes: mollet egs, with semi-liquid yolk and curdled white. 10-12 minutes: boiled eggs, compact white and curdled yolk. Pour off the hot water, shake it gently to crack the eggs, and add cold water, with even a few ice cubes. Wait 5 minutes if you want to serve them warm, or 15 otherwise, and then peel them under the same water. Here are some tips to improve your chances to get the perfect egg: Use fresh eggs, when they've been in the fridge for a while, they get dehydrated and the air that's inside gets expanded. That's why, when you put an egg into a glass of water, if it doesn't stay at the bottom you'd better not use it. Take them out of the fridge an hour before cooking them. (Yeah, like you're going to remember to do it :P).","title":"Boiling an egg"},{"location":"cooking_basics/#cooking-legumes","text":"Legumes are wonderful, to squeeze their value remember to: Don't use old ones: If you're legumes are older than a year, they can be old . They loose water with the time up to a point that they can be impossible to cook. Soak them: Some legumes like the lentils don't need to be soaked, but for most of them it's better to be hydrated before putting them in the pot. For chickpeas and beans, the best is to soak them for 10 to 12 hours. They'll drink the water, so add it until you double the volume of the legumes. Once done, discard that water, rinse them and use new one for the pot. That way you'll prevent the acids and oligosaccharides that promotes a heavy digestion. * Don't scare them: Don't cut the cooking with cold water, they won't help you avoid farting and for some great chefs it's one of the worst errors you can do, at least with the chickpeas. Remember always to have enough water from the start to avoid this situation. * Know when to add them in the pot: chickpeas need to be added when the water is already boiling When you're using boiled legumes in your recipes, be careful, after the hydration, they weight the double!","title":"Cooking legumes"},{"location":"cooking_basics/#boil-chickpeas-when-youve-forgotten-to-soak-them","text":"Soaked chickpeas take one or two hours to cook in a normal pot, 20 to 30 in a fast pot, which saves a ton of energy. If you forgot to soak them, add a level teaspoon of baking soda to the pot and cook them as usual. When not using a fast pot, you'll need to periodically remove the foam that will be created. The problem with this method is that you don't discard the first water round, and they can be more indigestible. Another option is to cook them for an hour, change the water and then cook them again.","title":"Boil chickpeas when you've forgotten to soak them"},{"location":"digital_garden/","text":"Digital Garden is a method of storing and maintaining knowledge in an maintainable, scalable and searchable way. They are also known as second brains. Unlike in common blogging where you write an article and forget about it, posts are treated as plants in various stages of growth and nurturing. Some might wither and die, and others will flourish and provide a source of continued knowledge for the gardener and folks in the community that visit. The content is diverse, you can find ideas, articles, investigations, snippets, resources, thoughts, collections, and other bits and pieces that I find interesting and useful. It's my personal Stock , the content that\u2019s as interesting in two months (or two years) as it is today. It\u2019s what people discover via search. It\u2019s what spreads slowly but surely, building fans over time. They are a metaphor for thinking about writing and creating that focuses less on the resulting \"showpiece\" and more on the process, care, and craft it takes to get there. Existing digital gardens \u2691 If you look for inspiration check my favourite digital gardens: Nikita's Everything I know : It's awesome both in content quality and length, as the way he presents it. Gwern's site : The way he presents content is unique and gorgeous. I've found myself not very hooked to the content, but when you find something you like it's awesome, such as the about page , his article on spaced repetition or the essay of Death Note: L, Anonymity & Eluding Entropy . Or browse the following lists: Maggie Appleton's compilation Nikita's compilation Richard Litt's compilation KasperZutterman's compilation Or the digital garden 's reddit. References \u2691 Joel Hooks article on Digital Gardens Tom Critchlow article on Digital Gardens","title":"Digital Gardens"},{"location":"digital_garden/#existing-digital-gardens","text":"If you look for inspiration check my favourite digital gardens: Nikita's Everything I know : It's awesome both in content quality and length, as the way he presents it. Gwern's site : The way he presents content is unique and gorgeous. I've found myself not very hooked to the content, but when you find something you like it's awesome, such as the about page , his article on spaced repetition or the essay of Death Note: L, Anonymity & Eluding Entropy . Or browse the following lists: Maggie Appleton's compilation Nikita's compilation Richard Litt's compilation KasperZutterman's compilation Or the digital garden 's reddit.","title":"Existing digital gardens"},{"location":"digital_garden/#references","text":"Joel Hooks article on Digital Gardens Tom Critchlow article on Digital Gardens","title":"References"},{"location":"diversity/","text":"Diversity, equity, and inclusion (DEI) can be defined as : Diversity is the representation and acknowledgement of the multitudes of identities, experiences, and ways of moving through the world. This includes\u2014but is not limited to\u2014ability, age, citizenship status, criminal record and/or incarceration, educational attainment, ethnicity, gender, geographical location, language, nationality, political affiliation, religion, race, sexuality, socioeconomic status, and veteran status. Further, we recognize that each individual's experience is informed by intersections across multiple identities. Equity seeks to ensure respect and equal opportunity for all, using all resources and tools to elevate the voices of under-represented and/or disadvantaged groups. Inclusion is fostering an environment in which people of all identities are welcome, valued, and supported. An inclusive organization solicits, listens to, learns from, and acts on the contributions of all its stakeholders. References \u2691 Pulitzer center DEI page Journalist's Toolbox DEI links","title":"Diversity, Equity and Inclusion"},{"location":"diversity/#references","text":"Pulitzer center DEI page Journalist's Toolbox DEI links","title":"References"},{"location":"docker/","text":"Docker is a set of platform as a service (PaaS) products that use OS-level virtualization to deliver software in packages called containers. Containers are isolated from one another and bundle their own software, libraries and configuration files; they can communicate with each other through well-defined channels. Because all of the containers share the services of a single operating system kernel, they use fewer resources than virtual machines. How to keep containers updated \u2691 With watchtower you can update the running version of your containerized app simply by pushing a new image to the Docker Hub or your own image registry. Watchtower will pull down your new image, gracefully shut down your existing container and restart it with the same options that were used when it was deployed initially. Run the watchtower container with the next command: docker run -d \\ --name watchtower \\ -v /var/run/docker.sock:/var/run/docker.sock \\ -v /etc/localtime:/etc/localtime:ro \\ -e WATCHTOWER_NOTIFICATIONS = email \\ -e WATCHTOWER_NOTIFICATION_EMAIL_FROM ={{ email.from }} \\ -e WATCHTOWER_NOTIFICATION_EMAIL_TO ={{ email.to }} \\\\ -e WATCHTOWER_NOTIFICATION_EMAIL_SERVER = mail.riseup.net \\ -e WATCHTOWER_NOTIFICATION_EMAIL_SERVER_PORT = 587 \\ -e WATCHTOWER_NOTIFICATION_EMAIL_SERVER_USER ={{ email.user }} \\ -e WATCHTOWER_NOTIFICATION_EMAIL_SERVER_PASSWORD ={{ email.password }} \\ -e WATCHTOWER_NOTIFICATION_EMAIL_DELAY = 2 \\ containrrr/watchtower:latest --no-restart --no-startup-message Use the --no-restart flag if you use systemd to manage the dockers, and --no-startup-message if you don't want watchtower to send you an email each time it starts the update process. Keep in mind that if the containers don't have good migration scripts, upgrading may break the service. To enable this feature, make sure you have frequent backups and a tested rollback process. If you're not sure one of the containers is going to behave well, you can only monitor it or disable it by using docker labels. The first check will be done by default in the next 24 hours, to check that everything works use the --run-once flag. Another alternative is Diun , which is a CLI application written in Go and delivered as a single executable (and a Docker image) to receive notifications when a Docker image is updated on a Docker registry. They don't yet support Prometheus metrics but it surely looks promising. [Logging in \u2691 automatically]( https://docs.docker.com/engine/reference/commandline/login/#provide-a-password-using-stdin ) To log in automatically without entering the password, you need to have the password stored in your personal password store (not in root's!), imagine it's in the dockerhub entry. Then you can use: pass show dockerhub | docker login --username foo --password-stdin Troubleshooting \u2691 If you are using a VPN and docker, you're going to have a hard time. The docker systemd service logs systemctl status docker.service usually doesn't give much information. Try to start the daemon directly with sudo /usr/bin/dockerd . Don't store credentials in plaintext \u2691 It doesn't work, don't go this painful road and assume that docker is broken. The official steps are horrible, and once you've spent two hours debugging it, you won't be able to push or pull images with your user . When you use docker login and introduce the user and password you get the next warning: WARNING! Your password will be stored unencrypted in /root/.docker/config.json. Configure a credential helper to remove this warning. See https://docs.docker.com/engine/reference/commandline/login/#credentials-store I got a nice surprise when I saw that pass was suggested in the link of the warning, to be used as a backend to store the password. But that feeling soon faded. To make docker understand that you want to use pass you need to use the docker-credential-pass script. A Go script \"maintained\" by docker, whose last commit was two years ago , has the CI broken and many old unanswered issues. Setting it up it's not easy either and it's ill documented . Furthermore, the script doesn't do what I expected, which is to store the password of your registry user in a pass entry. Instead, you need to create an empty pass entry in docker-credential-helpers/docker-pass-initialized-check , and when you use docker login , manually introducing your data, it creates another entry, as you can see in the next pass output: Password Store \u2514\u2500\u2500 docker-credential-helpers \u251c\u2500\u2500 aHR0cHM6Ly9pbmRleC5kb2NrZXIuaW8vdjEv \u2502 \u2514\u2500\u2500 lyz \u2514\u2500\u2500 docker-pass-initialized-check That entry is removed when you use docker logout so the next time you log in you need to introduce the user and password (\u256f\u00b0\u25a1\u00b0)\u256f \u253b\u2501\u253b . Installing docker-credential-pass \u2691 You first need to install the script: # Check for later releases at https://github.com/docker/docker-credential-helpers/releases version = \"v0.6.3\" archive = \"docker-credential-pass- $version -amd64.tar.gz\" url = \"https://github.com/docker/docker-credential-helpers/releases/download/ $version / $archive \" # Download cred helper, unpack, make executable, and move it where Docker will find it. wget $url \\ && tar -xf $archive \\ && chmod +x docker-credential-pass \\ && mv -f docker-credential-pass /usr/local/bin/ Another tricky issue is that even if you use a non-root user who's part of the docker group, the script is not aware of that, so it will look in the password store of root instead of the user's. This means that additionally to your own, you need to create a new password store for root. Follow the next steps with the root user: Create the password with gpg --full-gen , and copy the key id. Use a non empty password, otherwise you are getting the same security as with the password in cleartext. Initialize the password store pass init gpg_id , changing gpg_id for the one of the last step. Create the empty docker-credential-helpers/docker-pass-initialized-check entry: pass insert docker-credential-helpers/docker-pass-initialized-check And press enter twice. Finally we need to specify in the root's docker configuration that we want to use the pass credential storage. File: /root/.docker/config.json { \"credsStore\" : \"pass\" } Testing it works \u2691 To test that docker is able to use pass as backend to store the credentials, run docker login and introduce the user and password. You should see the Login Succeeded message without any warning. Login with your Docker ID to push and pull images from Docker Hub. If you don't have a Docker ID, head over to https://hub.docker.com to create one. Username: lyz Password: Login Succeeded Awful experience, wasn't it? Don't worry it gets worse. Now that you're logged in, whenever you try to push an image you're probably going to get an denied: requested access to the resource is denied error. That's because docker is not able to use the password it has stored in the root's password store. If you're using root to push the image (bad idea anyway), you will need to export GPG_TTY=$(tty) so that docker can ask you for your password to unlock root's pass entry. If you're like me that uses a non-root user belonging to the docker group, not even that works, so you've spent all this time reading and trying to fix everything for nothing... Thank you Docker -.- . Start request repeated too quickly \u2691 Shutdown the VPN and it will work.","title":"Docker"},{"location":"docker/#how-to-keep-containers-updated","text":"With watchtower you can update the running version of your containerized app simply by pushing a new image to the Docker Hub or your own image registry. Watchtower will pull down your new image, gracefully shut down your existing container and restart it with the same options that were used when it was deployed initially. Run the watchtower container with the next command: docker run -d \\ --name watchtower \\ -v /var/run/docker.sock:/var/run/docker.sock \\ -v /etc/localtime:/etc/localtime:ro \\ -e WATCHTOWER_NOTIFICATIONS = email \\ -e WATCHTOWER_NOTIFICATION_EMAIL_FROM ={{ email.from }} \\ -e WATCHTOWER_NOTIFICATION_EMAIL_TO ={{ email.to }} \\\\ -e WATCHTOWER_NOTIFICATION_EMAIL_SERVER = mail.riseup.net \\ -e WATCHTOWER_NOTIFICATION_EMAIL_SERVER_PORT = 587 \\ -e WATCHTOWER_NOTIFICATION_EMAIL_SERVER_USER ={{ email.user }} \\ -e WATCHTOWER_NOTIFICATION_EMAIL_SERVER_PASSWORD ={{ email.password }} \\ -e WATCHTOWER_NOTIFICATION_EMAIL_DELAY = 2 \\ containrrr/watchtower:latest --no-restart --no-startup-message Use the --no-restart flag if you use systemd to manage the dockers, and --no-startup-message if you don't want watchtower to send you an email each time it starts the update process. Keep in mind that if the containers don't have good migration scripts, upgrading may break the service. To enable this feature, make sure you have frequent backups and a tested rollback process. If you're not sure one of the containers is going to behave well, you can only monitor it or disable it by using docker labels. The first check will be done by default in the next 24 hours, to check that everything works use the --run-once flag. Another alternative is Diun , which is a CLI application written in Go and delivered as a single executable (and a Docker image) to receive notifications when a Docker image is updated on a Docker registry. They don't yet support Prometheus metrics but it surely looks promising.","title":"How to keep containers updated"},{"location":"docker/#logging-in","text":"automatically]( https://docs.docker.com/engine/reference/commandline/login/#provide-a-password-using-stdin ) To log in automatically without entering the password, you need to have the password stored in your personal password store (not in root's!), imagine it's in the dockerhub entry. Then you can use: pass show dockerhub | docker login --username foo --password-stdin","title":"[Logging in"},{"location":"docker/#troubleshooting","text":"If you are using a VPN and docker, you're going to have a hard time. The docker systemd service logs systemctl status docker.service usually doesn't give much information. Try to start the daemon directly with sudo /usr/bin/dockerd .","title":"Troubleshooting"},{"location":"docker/#dont-store-credentials-in-plaintext","text":"It doesn't work, don't go this painful road and assume that docker is broken. The official steps are horrible, and once you've spent two hours debugging it, you won't be able to push or pull images with your user . When you use docker login and introduce the user and password you get the next warning: WARNING! Your password will be stored unencrypted in /root/.docker/config.json. Configure a credential helper to remove this warning. See https://docs.docker.com/engine/reference/commandline/login/#credentials-store I got a nice surprise when I saw that pass was suggested in the link of the warning, to be used as a backend to store the password. But that feeling soon faded. To make docker understand that you want to use pass you need to use the docker-credential-pass script. A Go script \"maintained\" by docker, whose last commit was two years ago , has the CI broken and many old unanswered issues. Setting it up it's not easy either and it's ill documented . Furthermore, the script doesn't do what I expected, which is to store the password of your registry user in a pass entry. Instead, you need to create an empty pass entry in docker-credential-helpers/docker-pass-initialized-check , and when you use docker login , manually introducing your data, it creates another entry, as you can see in the next pass output: Password Store \u2514\u2500\u2500 docker-credential-helpers \u251c\u2500\u2500 aHR0cHM6Ly9pbmRleC5kb2NrZXIuaW8vdjEv \u2502 \u2514\u2500\u2500 lyz \u2514\u2500\u2500 docker-pass-initialized-check That entry is removed when you use docker logout so the next time you log in you need to introduce the user and password (\u256f\u00b0\u25a1\u00b0)\u256f \u253b\u2501\u253b .","title":"Don't store credentials in plaintext"},{"location":"docker/#installing-docker-credential-pass","text":"You first need to install the script: # Check for later releases at https://github.com/docker/docker-credential-helpers/releases version = \"v0.6.3\" archive = \"docker-credential-pass- $version -amd64.tar.gz\" url = \"https://github.com/docker/docker-credential-helpers/releases/download/ $version / $archive \" # Download cred helper, unpack, make executable, and move it where Docker will find it. wget $url \\ && tar -xf $archive \\ && chmod +x docker-credential-pass \\ && mv -f docker-credential-pass /usr/local/bin/ Another tricky issue is that even if you use a non-root user who's part of the docker group, the script is not aware of that, so it will look in the password store of root instead of the user's. This means that additionally to your own, you need to create a new password store for root. Follow the next steps with the root user: Create the password with gpg --full-gen , and copy the key id. Use a non empty password, otherwise you are getting the same security as with the password in cleartext. Initialize the password store pass init gpg_id , changing gpg_id for the one of the last step. Create the empty docker-credential-helpers/docker-pass-initialized-check entry: pass insert docker-credential-helpers/docker-pass-initialized-check And press enter twice. Finally we need to specify in the root's docker configuration that we want to use the pass credential storage. File: /root/.docker/config.json { \"credsStore\" : \"pass\" }","title":"Installing docker-credential-pass"},{"location":"docker/#testing-it-works","text":"To test that docker is able to use pass as backend to store the credentials, run docker login and introduce the user and password. You should see the Login Succeeded message without any warning. Login with your Docker ID to push and pull images from Docker Hub. If you don't have a Docker ID, head over to https://hub.docker.com to create one. Username: lyz Password: Login Succeeded Awful experience, wasn't it? Don't worry it gets worse. Now that you're logged in, whenever you try to push an image you're probably going to get an denied: requested access to the resource is denied error. That's because docker is not able to use the password it has stored in the root's password store. If you're using root to push the image (bad idea anyway), you will need to export GPG_TTY=$(tty) so that docker can ask you for your password to unlock root's pass entry. If you're like me that uses a non-root user belonging to the docker group, not even that works, so you've spent all this time reading and trying to fix everything for nothing... Thank you Docker -.- .","title":"Testing it works"},{"location":"docker/#start-request-repeated-too-quickly","text":"Shutdown the VPN and it will work.","title":"Start request repeated too quickly"},{"location":"documentation/","text":"It doesn't matter how good your program is, because if its documentation is not good enough, people will not use it. Even if they have to use it because they have no choice, without good documentation, they won\u2019t use it effectively or the way you\u2019d like them to. People working with software need different kinds of documentation at different times, in different circumstances, so good software documentation needs them all. They first need to get started and see how to solve specific problems. Then they need a way to search the software possibilities in a reference. Finally when they hit a road block, they need to understand how everything works so they can solve it. Each of these sections must be clearly differenced and the writing style must be adapted. The five types of documentation are: Introduction : A short description with optional pictures or screen casts, that catches the user's attention and makes them want to use it. Like the advertisement of your program. Get started : Lessons that allows the newcomer learn how to start using the software. Like teaching a small child how to cook. How-to guides : Series of steps that show how to solve a specific problem. Like a recipe in a cookery book. Technical reference : Searchable and organized dry description of the software's machinery. Like a reference encyclopedia article. Background information : Discursive explanations that makes the user understand how the software works and how has it evolved. Like an article on culinary social history. This division makes it obvious to both author and reader what material, and what kind of material, goes where. It tells the author how to write, what to write, and where to write it. Introduction \u2691 The introduction is the first gateway for the users to your program, as such, it needs to be eye-catching, otherwise they will walk pass it to one of the other thousand programs or libraries out there. It needs to start with a short phrase that defines the whole project in a way that catches the user's attention. If the short phrase doesn't give enough context, you can add a small paragraph with further information. But don't make it too long, human's attention is weak. It's also a good idea to add a screenshot or screencast showing the usage of the program. Optionally, you can also add a list of features that differentiate your solution from the rest. Get started \u2691 Made of tutorials that take the reader by the hand through a series of steps to complete a meaningful project achievable for a complete beginner. They are what your project needs in order to show a beginner that they can achieve something with it. Tutorials are what will turn your learners into users. A bad or missing tutorial will prevent your project from acquiring new users. They need to be useful for the beginner, easy to follow, meaningful, extremely robust, and kept up-to-date. You might well find that writing and maintaining your tutorials can occupy as much time and energy as the other four parts put together. How to write good tutorials \u2691 Allow the user to learn by doing \u2691 Your learner needs to do things. The different things that they do while following your tutorial need to cover a wide range of tools and operations, building up from the simplest ones at the start to more complex ones. Get the user started \u2691 It\u2019s perfectly acceptable if your beginner\u2019s first steps are hand-held baby steps. It\u2019s also good if what you get the beginner to do is not the way an experienced person would, or even if it\u2019s not the \u2018correct\u2019 way. The point of a tutorial is to get your learner started on their journey, not to get them to a final destination. Make sure that your tutorial works \u2691 One of your jobs as a tutor is to inspire the beginner\u2019s confidence: in the software, in the tutorial, in the tutor, and in their own ability to achieve what\u2019s being asked of them. There are many things that contribute to this. A friendly tone helps, as does consistent use of language, and a logical progression through the material. But the single most important thing is that what you ask the beginner to do must work. If the learner\u2019s actions produce an error or unexpected results, your tutorial has failed. When your students are there with you, you can rescue them; if they\u2019re reading your documentation on their own you can\u2019t. So you have to prevent that from happening in advance. One way of achieving this is by adding the snippets in your documentation to the test suite. Ensure the user sees results immediately \u2691 Everything the learner does should accomplish something comprehensible, however small. If your student has to do strange and incomprehensible things for two pages before they even see a result, that\u2019s much too long. The effect of every action should be visible and evident as soon as possible, and the connection to the action should be clear. The conclusion of each section of a tutorial, or the tutorial as a whole, must be a meaningful accomplishment. Focus on concrete steps, not abstract concepts \u2691 Tutorials need to be concrete, built around specific, particular actions and outcomes. The temptation to introduce abstraction is huge; it is after all how most computing derives its power. But all learning proceeds from the particular and concrete to the general and abstract, and asking the learner to appreciate levels of abstraction before they have even had a chance to grasp the concrete is poor teaching. Provide the minimum necessary explanation \u2691 Don\u2019t explain anything the learner doesn\u2019t need to know in order to complete the tutorial. Extended discussion is important, just not in a tutorial. In a tutorial, it is an obstruction and a distraction. Only the bare minimum is appropriate. Instead, link to explanations elsewhere in the documentation. Focus only on the steps the user needs to take \u2691 Your tutorial needs to be focused on the task in hand. Maybe the command you\u2019re introducing has many other options, or maybe there are different ways to access a certain API. It doesn\u2019t matter: right now, your learner does not need to know about those in order to make progress. How-to guides \u2691 Technical reference \u2691 Background information \u2691 References \u2691 divio's documentation wiki Vue's guidelines FastAPI awesome docs","title":"Writing good documentation"},{"location":"documentation/#introduction","text":"The introduction is the first gateway for the users to your program, as such, it needs to be eye-catching, otherwise they will walk pass it to one of the other thousand programs or libraries out there. It needs to start with a short phrase that defines the whole project in a way that catches the user's attention. If the short phrase doesn't give enough context, you can add a small paragraph with further information. But don't make it too long, human's attention is weak. It's also a good idea to add a screenshot or screencast showing the usage of the program. Optionally, you can also add a list of features that differentiate your solution from the rest.","title":"Introduction"},{"location":"documentation/#get-started","text":"Made of tutorials that take the reader by the hand through a series of steps to complete a meaningful project achievable for a complete beginner. They are what your project needs in order to show a beginner that they can achieve something with it. Tutorials are what will turn your learners into users. A bad or missing tutorial will prevent your project from acquiring new users. They need to be useful for the beginner, easy to follow, meaningful, extremely robust, and kept up-to-date. You might well find that writing and maintaining your tutorials can occupy as much time and energy as the other four parts put together.","title":"Get started"},{"location":"documentation/#how-to-write-good-tutorials","text":"","title":"How to write good tutorials"},{"location":"documentation/#allow-the-user-to-learn-by-doing","text":"Your learner needs to do things. The different things that they do while following your tutorial need to cover a wide range of tools and operations, building up from the simplest ones at the start to more complex ones.","title":"Allow the user to learn by doing"},{"location":"documentation/#get-the-user-started","text":"It\u2019s perfectly acceptable if your beginner\u2019s first steps are hand-held baby steps. It\u2019s also good if what you get the beginner to do is not the way an experienced person would, or even if it\u2019s not the \u2018correct\u2019 way. The point of a tutorial is to get your learner started on their journey, not to get them to a final destination.","title":"Get the user started"},{"location":"documentation/#make-sure-that-your-tutorial-works","text":"One of your jobs as a tutor is to inspire the beginner\u2019s confidence: in the software, in the tutorial, in the tutor, and in their own ability to achieve what\u2019s being asked of them. There are many things that contribute to this. A friendly tone helps, as does consistent use of language, and a logical progression through the material. But the single most important thing is that what you ask the beginner to do must work. If the learner\u2019s actions produce an error or unexpected results, your tutorial has failed. When your students are there with you, you can rescue them; if they\u2019re reading your documentation on their own you can\u2019t. So you have to prevent that from happening in advance. One way of achieving this is by adding the snippets in your documentation to the test suite.","title":"Make sure that your tutorial works"},{"location":"documentation/#ensure-the-user-sees-results-immediately","text":"Everything the learner does should accomplish something comprehensible, however small. If your student has to do strange and incomprehensible things for two pages before they even see a result, that\u2019s much too long. The effect of every action should be visible and evident as soon as possible, and the connection to the action should be clear. The conclusion of each section of a tutorial, or the tutorial as a whole, must be a meaningful accomplishment.","title":"Ensure the user sees results immediately"},{"location":"documentation/#focus-on-concrete-steps-not-abstract-concepts","text":"Tutorials need to be concrete, built around specific, particular actions and outcomes. The temptation to introduce abstraction is huge; it is after all how most computing derives its power. But all learning proceeds from the particular and concrete to the general and abstract, and asking the learner to appreciate levels of abstraction before they have even had a chance to grasp the concrete is poor teaching.","title":"Focus on concrete steps, not abstract concepts"},{"location":"documentation/#provide-the-minimum-necessary-explanation","text":"Don\u2019t explain anything the learner doesn\u2019t need to know in order to complete the tutorial. Extended discussion is important, just not in a tutorial. In a tutorial, it is an obstruction and a distraction. Only the bare minimum is appropriate. Instead, link to explanations elsewhere in the documentation.","title":"Provide the minimum necessary explanation"},{"location":"documentation/#focus-only-on-the-steps-the-user-needs-to-take","text":"Your tutorial needs to be focused on the task in hand. Maybe the command you\u2019re introducing has many other options, or maybe there are different ways to access a certain API. It doesn\u2019t matter: right now, your learner does not need to know about those in order to make progress.","title":"Focus only on the steps the user needs to take"},{"location":"documentation/#how-to-guides","text":"","title":"How-to guides"},{"location":"documentation/#technical-reference","text":"","title":"Technical reference"},{"location":"documentation/#background-information","text":"","title":"Background information"},{"location":"documentation/#references","text":"divio's documentation wiki Vue's guidelines FastAPI awesome docs","title":"References"},{"location":"dunst/","text":"Dunst is a lightweight replacement for the notification daemons provided by most desktop environments. It\u2019s very customizable, isn\u2019t dependent on any toolkits, and therefore fits into those window manager centric setups we all love to customize to perfection. Installation \u2691 sudo apt-get install dunst Test it's working with: notify-send \"Notification Title\" \"Notification Messages\" If your distro version is too old that doesn't have dunstctl or dunstify , you can install it manually : git clone https://github.com/dunst-project/dunst.git cd dunst # Install dependencies sudo apt-get install libgdk-pixbuf2.0-0 libnotify-dev librust-pangocairo-dev # Build the program and install make WAYLAND = 0 SYSTEMD = 1 sudo make WAYLAND = 0 SYSTEMD = 1 install Read and tweak the ~/.dunst/dunstrc file to your liking. References \u2691 Git Home Archwiki page on dunst","title":"dunst"},{"location":"dunst/#installation","text":"sudo apt-get install dunst Test it's working with: notify-send \"Notification Title\" \"Notification Messages\" If your distro version is too old that doesn't have dunstctl or dunstify , you can install it manually : git clone https://github.com/dunst-project/dunst.git cd dunst # Install dependencies sudo apt-get install libgdk-pixbuf2.0-0 libnotify-dev librust-pangocairo-dev # Build the program and install make WAYLAND = 0 SYSTEMD = 1 sudo make WAYLAND = 0 SYSTEMD = 1 install Read and tweak the ~/.dunst/dunstrc file to your liking.","title":"Installation"},{"location":"dunst/#references","text":"Git Home Archwiki page on dunst","title":"References"},{"location":"elasticsearch_exporter/","text":"The elasticsearch exporter allows monitoring Elasticsearch clusters with Prometheus . Installation \u2691 To install the exporter we'll use helmfile to install the prometheus-elasticsearch-exporter chart . Add the following lines to your helmfile.yaml . - name : prometheus-elasticsearch-exporter namespace : monitoring chart : prometheus-community/prometheus-elasticsearch-exporter values : - prometheus-elasticsearch-exporter/values.yaml Edit the chart values. mkdir prometheus-elasticsearch-exporter helm inspect values prometheus-community/prometheus-elasticsearch-exporter > prometheus-elasticsearch-exporter/values.yaml vi prometheus-elasticsearch-exporter/values.yaml Comment out all the values you don't edit, so that the chart doesn't break when you upgrade it. Make sure that the serviceMonitor labels match your Prometheus serviceMonitorSelector otherwise they won't be added to the configuration. es : ## Address (host and port) of the Elasticsearch node we should connect to. ## This could be a local node (localhost:9200, for instance), or the address ## of a remote Elasticsearch server. When basic auth is needed, ## specify as: <proto>://<user>:<password>@<host>:<port>. e.g., http://admin:pass@localhost:9200. ## uri : http://localhost:9200 serviceMonitor : ## If true, a ServiceMonitor CRD is created for a prometheus operator ## https://github.com/coreos/prometheus-operator ## enabled : true # namespace: monitoring labels : release : prometheus-operator interval : 30s # scrapeTimeout: 10s # scheme: http # relabelings: [] # targetLabels: [] metricRelabelings : - sourceLabels : [ cluster ] targetLabel : cluster_name regex : '.*:(.*)' # sampleLimit: 0 You can build the cluster label following this instructions , I didn't find the required meta tags, so I've built the cluster_name label for alerting purposes. The grafana dashboard I chose is 2322 . Taking as reference the grafana helm chart values, add the next yaml under the grafana key in the prometheus-operator values.yaml . grafana : enabled : true defaultDashboardsEnabled : true dashboardProviders : dashboardproviders.yaml : apiVersion : 1 providers : - name : 'default' orgId : 1 folder : '' type : file disableDeletion : false editable : true options : path : /var/lib/grafana/dashboards/default dashboards : default : elasticsearch : # Ref: https://grafana.com/dashboards/2322 gnetId : 2322 revision : 4 datasource : Prometheus And install. helmfile diff helmfile apply Elasticsearch exporter alerts \u2691 Now that we've got the metrics, we can define the alert rules . Most have been tweaked from the Awesome prometheus alert rules collection. Availability alerts \u2691 The most basic probes, test if the service is healthy - alert : ElasticsearchClusterRed expr : elasticsearch_cluster_health_status{color=\"red\"} == 1 for : 0m labels : severity : critical annotations : summary : > Elasticsearch Cluster Red (cluster {{ $labels.cluster_name }}) description : | Elastic Cluster Red status VALUE = {{ $value }} LABELS = {{ $labels }} - alert : ElasticsearchClusterYellow expr : elasticsearch_cluster_health_status{color=\"yellow\"} == 1 for : 0m labels : severity : warning annotations : summary : > Elasticsearch Cluster Yellow (cluster {{ $labels.cluster_name }}) description : | Elastic Cluster Yellow status VALUE = {{ $value }} LABELS = {{ $labels }} - alert : ElasticsearchHealthyNodes expr : elasticsearch_cluster_health_number_of_nodes < 3 for : 0m labels : severity : critical annotations : summary : > Elasticsearch Healthy Nodes (cluster {{ $labels.cluster_name }}) description : | Missing node in Elasticsearch cluster VALUE = {{ $value }} LABELS = {{ $labels }} - alert : ElasticsearchHealthyMasterNodes expr : > elasticsearch_cluster_health_number_of_nodes - elasticsearch_cluster_health_number_of_data_nodes > 0 < 3 for : 0m labels : severity : critical annotations : summary : > Elasticsearch Healthy Master Nodes < 3 (cluster {{ $labels.cluster_name }}) description : | Missing master node in Elasticsearch cluster VALUE = {{ $value }} LABELS = {{ $labels }} - alert : ElasticsearchHealthyDataNodes expr : elasticsearch_cluster_health_number_of_data_nodes < 3 for : 0m labels : severity : critical annotations : summary : > Elasticsearch Healthy Data Nodes (cluster {{ $labels.cluster_name }}) description : | Missing data node in Elasticsearch cluster VALUE = {{ $value }} LABELS = {{ $labels }} Performance alerts \u2691 - alert : ElasticsearchCPUUsageTooHigh expr : elasticsearch_os_cpu_percent > 90 for : 2m labels : severity : critical annotations : summary : > Elasticsearch Node CPU Usage Too High (cluster {{ $labels.cluster_name }} node {{ $labels.name }}) description : | The CPU usage of node {{ $labels.name }} is over 90% VALUE = {{ $value }} LABELS = {{ $labels }} - alert : ElasticsearchCPUUsageWarning expr : elasticsearch_os_cpu_percent > 80 for : 2m labels : severity : warning annotations : summary : > Elasticsearch Node CPU Usage Too High (cluster {{ $labels.cluster_name }} node {{ $labels.name }}) description : | The CPU usage of node {{ $labels.name }} is over 90% VALUE = {{ $value }} LABELS = {{ $labels }} - alert : ElasticsearchHeapUsageTooHigh expr : > ( elasticsearch_jvm_memory_used_bytes{area=\"heap\"} / elasticsearch_jvm_memory_max_bytes{area=\"heap\"} ) * 100 > 90 for : 2m labels : severity : critical annotations : summary : > Elasticsearch Node Heap Usage Critical (cluster {{ $labels.cluster_name }} node {{ $labels.name }}) description : | The heap usage of node {{ $labels.name }} is over 90% VALUE = {{ $value }} LABELS = {{ $labels }} - alert : ElasticsearchHeapUsageWarning expr : > ( elasticsearch_jvm_memory_used_bytes{area=\"heap\"} / elasticsearch_jvm_memory_max_bytes{area=\"heap\"} ) * 100 > 80 for : 2m labels : severity : warning annotations : summary : > Elasticsearch Node Heap Usage Warning (cluster {{ $labels.cluster_name }} node {{ $labels.name }}) description : | The heap usage of node {{ $labels.name }} is over 80% VALUE = {{ $value }} LABELS = {{ $labels }} - alert : ElasticsearchDiskOutOfSpace expr : > elasticsearch_filesystem_data_available_bytes / elasticsearch_filesystem_data_size_bytes * 100 < 10 for : 0m labels : severity : critical annotations : summary : > Elasticsearch disk out of space (cluster {{ $labels.cluster_name }}) description : | The disk usage is over 90% VALUE = {{ $value }} LABELS = {{ $labels }} - alert : ElasticsearchDiskSpaceLow expr : > elasticsearch_filesystem_data_available_bytes / elasticsearch_filesystem_data_size_bytes * 100 < 20 for : 2m labels : severity : warning annotations : summary : > Elasticsearch disk space low (cluster {{ $labels.cluster_name }}) description : | The disk usage is over 80% VALUE = {{ $value }} LABELS = {{ $labels }} - alert : ElasticsearchRelocatingShardsTooLong expr : elasticsearch_cluster_health_relocating_shards > 0 for : 15m labels : severity : warning annotations : summary : > Elasticsearch relocating shards too long (cluster {{ $labels.cluster_name }}) description : | Elasticsearch has been relocating shards for 15min VALUE = {{ $value }} LABELS = {{ $labels }} - alert : ElasticsearchInitializingShardsTooLong expr : elasticsearch_cluster_health_initializing_shards > 0 for : 15m labels : severity : warning annotations : summary : > Elasticsearch initializing shards too long (cluster_name {{ $labels.cluster }}) description : | Elasticsearch has been initializing shards for 15 min VALUE = {{ $value }} LABELS = {{ $labels }} - alert : ElasticsearchUnassignedShards expr : elasticsearch_cluster_health_unassigned_shards > 0 for : 0m labels : severity : critical annotations : summary : > Elasticsearch unassigned shards (cluster {{ $labels.cluster_name }}) description : | Elasticsearch has unassigned shards VALUE = {{ $value }} LABELS = {{ $labels }} - alert : ElasticsearchPendingTasks expr : elasticsearch_cluster_health_number_of_pending_tasks > 0 for : 15m labels : severity : warning annotations : summary : > Elasticsearch pending tasks (cluster {{ $labels.cluster_name }}) description : | Elasticsearch has pending tasks. Cluster works slowly. VALUE = {{ $value }} LABELS = {{ $labels }} - alert : ElasticsearchCountOfJVMGarbageCollectorRuns expr : rate(elasticsearch_jvm_gc_collection_seconds_count{}[5m]) > 5 for : 1m labels : severity : warning annotations : summary : > Elasticsearch JVM Garbage Collector runs > 5 (cluster {{ $labels.cluster_name }}) description : | Elastic Cluster JVM Garbage Collector runs > 5 VALUE = {{ $value }} LABELS = {{ $labels }} - alert : ElasticsearchCountOfJVMGarbageCollectorTime expr : rate(elasticsearch_jvm_gc_collection_seconds_sum[5m]) > 0.3 for : 1m labels : severity : warning annotations : summary : > Elasticsearch JVM Garbage Collector time > 0.3 (cluster {{ $labels.cluster_name }}) description : | Elastic Cluster JVM Garbage Collector runs > 0.3 VALUE = {{ $value }} LABELS = {{ $labels }} - alert : ElasticsearchJSONParseErrors expr : elasticsearch_cluster_health_json_parse_failures > 0 for : 1m labels : severity : warning annotations : summary : > Elasticsearch json parse error (cluster {{ $labels.cluster_name }}) description : | Elasticsearch json parse error VALUE = {{ $value }} LABELS = {{ $labels }} - alert : ElasticsearchCircuitBreakerTripped expr : rate(elasticsearch_breakers_tripped{}[5m])>0 for : 1m labels : severity : warning annotations : summary : > Elasticsearch breaker {{ $labels.breaker }} tripped (cluster {{ $labels.cluster_name }}, node {{ $labels.name }}) description : | Elasticsearch breaker {{ $labels.breaker }} tripped (cluster {{ $labels.cluster_name }}, node {{ $labels.name }}) VALUE = {{ $value }} LABELS = {{ $labels }} Snapshot alerts \u2691 - alert : ElasticsearchMonthlySnapshot expr : > time() - elasticsearch_snapshot_stats_snapshot_end_time_timestamp{state=\"SUCCESS\"} > (3600 * 24 * 32) for : 15m labels : severity : warning annotations : summary : > Elasticsearch monthly snapshot failed (cluster {{ $labels.cluster_name }}, snapshot {{ $labels.repository }}) description : | Last successful elasticsearch snapshot of repository {{ $labels.repository}} is older than 32 days. VALUE = {{ $value }} LABELS = {{ $labels }} - record : elasticsearch_indices_search_latency:rate1m expr : | increase(elasticsearch_indices_search_query_time_seconds[1m])/ increase(elasticsearch_indices_search_query_total[1m]) - record : elasticsearch_indices_search_rate:rate1m expr : increase(elasticsearch_indices_search_query_total[1m])/60 - alert : ElasticsearchSlowSearchLatency expr : elasticsearch_indices_search_latency:rate1m > 1 for : 2m labels : severity : warning annotations : summary : > Elasticsearch search latency is greater than 1 s (cluster {{ $labels.cluster_name }}, node {{ $labels.name }}) description : | Elasticsearch search latency is greater than 1 s (cluster {{ $labels.cluster_name }}, node {{ $labels.name }}) VALUE = {{ $value }} LABELS = {{ $labels }} Links \u2691 Git","title":"Elasticsearch Exporter"},{"location":"elasticsearch_exporter/#installation","text":"To install the exporter we'll use helmfile to install the prometheus-elasticsearch-exporter chart . Add the following lines to your helmfile.yaml . - name : prometheus-elasticsearch-exporter namespace : monitoring chart : prometheus-community/prometheus-elasticsearch-exporter values : - prometheus-elasticsearch-exporter/values.yaml Edit the chart values. mkdir prometheus-elasticsearch-exporter helm inspect values prometheus-community/prometheus-elasticsearch-exporter > prometheus-elasticsearch-exporter/values.yaml vi prometheus-elasticsearch-exporter/values.yaml Comment out all the values you don't edit, so that the chart doesn't break when you upgrade it. Make sure that the serviceMonitor labels match your Prometheus serviceMonitorSelector otherwise they won't be added to the configuration. es : ## Address (host and port) of the Elasticsearch node we should connect to. ## This could be a local node (localhost:9200, for instance), or the address ## of a remote Elasticsearch server. When basic auth is needed, ## specify as: <proto>://<user>:<password>@<host>:<port>. e.g., http://admin:pass@localhost:9200. ## uri : http://localhost:9200 serviceMonitor : ## If true, a ServiceMonitor CRD is created for a prometheus operator ## https://github.com/coreos/prometheus-operator ## enabled : true # namespace: monitoring labels : release : prometheus-operator interval : 30s # scrapeTimeout: 10s # scheme: http # relabelings: [] # targetLabels: [] metricRelabelings : - sourceLabels : [ cluster ] targetLabel : cluster_name regex : '.*:(.*)' # sampleLimit: 0 You can build the cluster label following this instructions , I didn't find the required meta tags, so I've built the cluster_name label for alerting purposes. The grafana dashboard I chose is 2322 . Taking as reference the grafana helm chart values, add the next yaml under the grafana key in the prometheus-operator values.yaml . grafana : enabled : true defaultDashboardsEnabled : true dashboardProviders : dashboardproviders.yaml : apiVersion : 1 providers : - name : 'default' orgId : 1 folder : '' type : file disableDeletion : false editable : true options : path : /var/lib/grafana/dashboards/default dashboards : default : elasticsearch : # Ref: https://grafana.com/dashboards/2322 gnetId : 2322 revision : 4 datasource : Prometheus And install. helmfile diff helmfile apply","title":"Installation"},{"location":"elasticsearch_exporter/#elasticsearch-exporter-alerts","text":"Now that we've got the metrics, we can define the alert rules . Most have been tweaked from the Awesome prometheus alert rules collection.","title":"Elasticsearch exporter alerts"},{"location":"elasticsearch_exporter/#availability-alerts","text":"The most basic probes, test if the service is healthy - alert : ElasticsearchClusterRed expr : elasticsearch_cluster_health_status{color=\"red\"} == 1 for : 0m labels : severity : critical annotations : summary : > Elasticsearch Cluster Red (cluster {{ $labels.cluster_name }}) description : | Elastic Cluster Red status VALUE = {{ $value }} LABELS = {{ $labels }} - alert : ElasticsearchClusterYellow expr : elasticsearch_cluster_health_status{color=\"yellow\"} == 1 for : 0m labels : severity : warning annotations : summary : > Elasticsearch Cluster Yellow (cluster {{ $labels.cluster_name }}) description : | Elastic Cluster Yellow status VALUE = {{ $value }} LABELS = {{ $labels }} - alert : ElasticsearchHealthyNodes expr : elasticsearch_cluster_health_number_of_nodes < 3 for : 0m labels : severity : critical annotations : summary : > Elasticsearch Healthy Nodes (cluster {{ $labels.cluster_name }}) description : | Missing node in Elasticsearch cluster VALUE = {{ $value }} LABELS = {{ $labels }} - alert : ElasticsearchHealthyMasterNodes expr : > elasticsearch_cluster_health_number_of_nodes - elasticsearch_cluster_health_number_of_data_nodes > 0 < 3 for : 0m labels : severity : critical annotations : summary : > Elasticsearch Healthy Master Nodes < 3 (cluster {{ $labels.cluster_name }}) description : | Missing master node in Elasticsearch cluster VALUE = {{ $value }} LABELS = {{ $labels }} - alert : ElasticsearchHealthyDataNodes expr : elasticsearch_cluster_health_number_of_data_nodes < 3 for : 0m labels : severity : critical annotations : summary : > Elasticsearch Healthy Data Nodes (cluster {{ $labels.cluster_name }}) description : | Missing data node in Elasticsearch cluster VALUE = {{ $value }} LABELS = {{ $labels }}","title":"Availability alerts"},{"location":"elasticsearch_exporter/#performance-alerts","text":"- alert : ElasticsearchCPUUsageTooHigh expr : elasticsearch_os_cpu_percent > 90 for : 2m labels : severity : critical annotations : summary : > Elasticsearch Node CPU Usage Too High (cluster {{ $labels.cluster_name }} node {{ $labels.name }}) description : | The CPU usage of node {{ $labels.name }} is over 90% VALUE = {{ $value }} LABELS = {{ $labels }} - alert : ElasticsearchCPUUsageWarning expr : elasticsearch_os_cpu_percent > 80 for : 2m labels : severity : warning annotations : summary : > Elasticsearch Node CPU Usage Too High (cluster {{ $labels.cluster_name }} node {{ $labels.name }}) description : | The CPU usage of node {{ $labels.name }} is over 90% VALUE = {{ $value }} LABELS = {{ $labels }} - alert : ElasticsearchHeapUsageTooHigh expr : > ( elasticsearch_jvm_memory_used_bytes{area=\"heap\"} / elasticsearch_jvm_memory_max_bytes{area=\"heap\"} ) * 100 > 90 for : 2m labels : severity : critical annotations : summary : > Elasticsearch Node Heap Usage Critical (cluster {{ $labels.cluster_name }} node {{ $labels.name }}) description : | The heap usage of node {{ $labels.name }} is over 90% VALUE = {{ $value }} LABELS = {{ $labels }} - alert : ElasticsearchHeapUsageWarning expr : > ( elasticsearch_jvm_memory_used_bytes{area=\"heap\"} / elasticsearch_jvm_memory_max_bytes{area=\"heap\"} ) * 100 > 80 for : 2m labels : severity : warning annotations : summary : > Elasticsearch Node Heap Usage Warning (cluster {{ $labels.cluster_name }} node {{ $labels.name }}) description : | The heap usage of node {{ $labels.name }} is over 80% VALUE = {{ $value }} LABELS = {{ $labels }} - alert : ElasticsearchDiskOutOfSpace expr : > elasticsearch_filesystem_data_available_bytes / elasticsearch_filesystem_data_size_bytes * 100 < 10 for : 0m labels : severity : critical annotations : summary : > Elasticsearch disk out of space (cluster {{ $labels.cluster_name }}) description : | The disk usage is over 90% VALUE = {{ $value }} LABELS = {{ $labels }} - alert : ElasticsearchDiskSpaceLow expr : > elasticsearch_filesystem_data_available_bytes / elasticsearch_filesystem_data_size_bytes * 100 < 20 for : 2m labels : severity : warning annotations : summary : > Elasticsearch disk space low (cluster {{ $labels.cluster_name }}) description : | The disk usage is over 80% VALUE = {{ $value }} LABELS = {{ $labels }} - alert : ElasticsearchRelocatingShardsTooLong expr : elasticsearch_cluster_health_relocating_shards > 0 for : 15m labels : severity : warning annotations : summary : > Elasticsearch relocating shards too long (cluster {{ $labels.cluster_name }}) description : | Elasticsearch has been relocating shards for 15min VALUE = {{ $value }} LABELS = {{ $labels }} - alert : ElasticsearchInitializingShardsTooLong expr : elasticsearch_cluster_health_initializing_shards > 0 for : 15m labels : severity : warning annotations : summary : > Elasticsearch initializing shards too long (cluster_name {{ $labels.cluster }}) description : | Elasticsearch has been initializing shards for 15 min VALUE = {{ $value }} LABELS = {{ $labels }} - alert : ElasticsearchUnassignedShards expr : elasticsearch_cluster_health_unassigned_shards > 0 for : 0m labels : severity : critical annotations : summary : > Elasticsearch unassigned shards (cluster {{ $labels.cluster_name }}) description : | Elasticsearch has unassigned shards VALUE = {{ $value }} LABELS = {{ $labels }} - alert : ElasticsearchPendingTasks expr : elasticsearch_cluster_health_number_of_pending_tasks > 0 for : 15m labels : severity : warning annotations : summary : > Elasticsearch pending tasks (cluster {{ $labels.cluster_name }}) description : | Elasticsearch has pending tasks. Cluster works slowly. VALUE = {{ $value }} LABELS = {{ $labels }} - alert : ElasticsearchCountOfJVMGarbageCollectorRuns expr : rate(elasticsearch_jvm_gc_collection_seconds_count{}[5m]) > 5 for : 1m labels : severity : warning annotations : summary : > Elasticsearch JVM Garbage Collector runs > 5 (cluster {{ $labels.cluster_name }}) description : | Elastic Cluster JVM Garbage Collector runs > 5 VALUE = {{ $value }} LABELS = {{ $labels }} - alert : ElasticsearchCountOfJVMGarbageCollectorTime expr : rate(elasticsearch_jvm_gc_collection_seconds_sum[5m]) > 0.3 for : 1m labels : severity : warning annotations : summary : > Elasticsearch JVM Garbage Collector time > 0.3 (cluster {{ $labels.cluster_name }}) description : | Elastic Cluster JVM Garbage Collector runs > 0.3 VALUE = {{ $value }} LABELS = {{ $labels }} - alert : ElasticsearchJSONParseErrors expr : elasticsearch_cluster_health_json_parse_failures > 0 for : 1m labels : severity : warning annotations : summary : > Elasticsearch json parse error (cluster {{ $labels.cluster_name }}) description : | Elasticsearch json parse error VALUE = {{ $value }} LABELS = {{ $labels }} - alert : ElasticsearchCircuitBreakerTripped expr : rate(elasticsearch_breakers_tripped{}[5m])>0 for : 1m labels : severity : warning annotations : summary : > Elasticsearch breaker {{ $labels.breaker }} tripped (cluster {{ $labels.cluster_name }}, node {{ $labels.name }}) description : | Elasticsearch breaker {{ $labels.breaker }} tripped (cluster {{ $labels.cluster_name }}, node {{ $labels.name }}) VALUE = {{ $value }} LABELS = {{ $labels }}","title":"Performance alerts"},{"location":"elasticsearch_exporter/#snapshot-alerts","text":"- alert : ElasticsearchMonthlySnapshot expr : > time() - elasticsearch_snapshot_stats_snapshot_end_time_timestamp{state=\"SUCCESS\"} > (3600 * 24 * 32) for : 15m labels : severity : warning annotations : summary : > Elasticsearch monthly snapshot failed (cluster {{ $labels.cluster_name }}, snapshot {{ $labels.repository }}) description : | Last successful elasticsearch snapshot of repository {{ $labels.repository}} is older than 32 days. VALUE = {{ $value }} LABELS = {{ $labels }} - record : elasticsearch_indices_search_latency:rate1m expr : | increase(elasticsearch_indices_search_query_time_seconds[1m])/ increase(elasticsearch_indices_search_query_total[1m]) - record : elasticsearch_indices_search_rate:rate1m expr : increase(elasticsearch_indices_search_query_total[1m])/60 - alert : ElasticsearchSlowSearchLatency expr : elasticsearch_indices_search_latency:rate1m > 1 for : 2m labels : severity : warning annotations : summary : > Elasticsearch search latency is greater than 1 s (cluster {{ $labels.cluster_name }}, node {{ $labels.name }}) description : | Elasticsearch search latency is greater than 1 s (cluster {{ $labels.cluster_name }}, node {{ $labels.name }}) VALUE = {{ $value }} LABELS = {{ $labels }}","title":"Snapshot alerts"},{"location":"elasticsearch_exporter/#links","text":"Git","title":"Links"},{"location":"email_automation/","text":"Most of the received emails require repetitive actions that can be automated, and you may also want to access your emails through a command line interface and be able to search through them. One of the ways to achieve that goals is to use a combination of tools to synchronize the mailboxes, tag them, and run scripts automatically based on the tags. Installation \u2691 First you need a program that syncs your mailboxes, following pazz's advice , I'll use mbsync . Follow the steps under installation to configure your accounts, taking as an example an account called lyz you should be able to sync all your emails with: mbsync -V lyz Now we need to install notmuch a tool to index, search, read, and tag large collections of email messages. Follow the steps under installation under you have created the database that indexes your emails. Once we have that, we need a tool to tag the emails following our desired rules. afew is one way to go. Follow the steps under installation . The remaining step to keep the inboxes synced and tagged is to run all the steps above in a cron. Particularize pazz's script for your usecase: #!/bin/bash # # Download and index new mail. # # Copyright (c) 2017 Patrick Totzke # Dependencies: flock, nm-online, mbsync, notmuch, afew # Example crontab entry: # # */2 * * * * /usr/bin/flock -n /home/pazz/.pullmail.lock /home/pazz/bin/pullmail.sh > /home/pazz/.pullmail.log # PATH = /home/pazz/.local/bin:/usr/local/bin/: $PATH ACCOUNTDIR = /home/pazz/.pullmail/ # this makes the keyring daemon accessible function keyring-control () { local -a vars =( \\ DBUS_SESSION_BUS_ADDRESS \\ GNOME_KEYRING_CONTROL \\ GNOME_KEYRING_PID \\ XDG_SESSION_COOKIE \\ GPG_AGENT_INFO \\ SSH_AUTH_SOCK \\ ) local pid = $( ps -C i3 -o pid --no-heading ) eval \"unset ${ vars [@] } ; $( printf \"export %s;\" $( sed 's/\\x00/\\n/g' /proc/ ${ pid //[^0-9]/ } /environ | grep $( printf -- \"-e ^%s= \" \" ${ vars [@] } \" )) ) \" } function log () { notify-send -t 2000 'mail sync:' \" $@ \" } function die () { notify-send -t 2000 -u critical 'mail sync:' \" $@ \" exit 1 } # Let's Do stuff keyring-control # abort as soon as something fails set -e # abort if not online nm-online -x -t 0 echo --------------------------------------------------------- date for accfile in ` ls $ACCOUNTDIR ` ; do ACC = $( basename $accfile ) echo ------------------------ $ACC ------------------------ mbsync -V $ACC || log \" $ACC failed\" done # index and tag new mails echo ------------------------ NOTMUCH ------------------------ notmuch new 2 >/dev/null || die \"NOTMUCH new failed\" echo ------------------------ AFEW ------------------------ afew -v --tag --new || die \"AFEW died\" echo --------------------------------------------------------- echo \"all done, goodbye.\" Where flock is a tool to manage locks from shell scripts. And add the entry in your crontab -e . If you want to process your emails with this system through a command line interface, you can configure alot .","title":"Email Automation"},{"location":"email_automation/#installation","text":"First you need a program that syncs your mailboxes, following pazz's advice , I'll use mbsync . Follow the steps under installation to configure your accounts, taking as an example an account called lyz you should be able to sync all your emails with: mbsync -V lyz Now we need to install notmuch a tool to index, search, read, and tag large collections of email messages. Follow the steps under installation under you have created the database that indexes your emails. Once we have that, we need a tool to tag the emails following our desired rules. afew is one way to go. Follow the steps under installation . The remaining step to keep the inboxes synced and tagged is to run all the steps above in a cron. Particularize pazz's script for your usecase: #!/bin/bash # # Download and index new mail. # # Copyright (c) 2017 Patrick Totzke # Dependencies: flock, nm-online, mbsync, notmuch, afew # Example crontab entry: # # */2 * * * * /usr/bin/flock -n /home/pazz/.pullmail.lock /home/pazz/bin/pullmail.sh > /home/pazz/.pullmail.log # PATH = /home/pazz/.local/bin:/usr/local/bin/: $PATH ACCOUNTDIR = /home/pazz/.pullmail/ # this makes the keyring daemon accessible function keyring-control () { local -a vars =( \\ DBUS_SESSION_BUS_ADDRESS \\ GNOME_KEYRING_CONTROL \\ GNOME_KEYRING_PID \\ XDG_SESSION_COOKIE \\ GPG_AGENT_INFO \\ SSH_AUTH_SOCK \\ ) local pid = $( ps -C i3 -o pid --no-heading ) eval \"unset ${ vars [@] } ; $( printf \"export %s;\" $( sed 's/\\x00/\\n/g' /proc/ ${ pid //[^0-9]/ } /environ | grep $( printf -- \"-e ^%s= \" \" ${ vars [@] } \" )) ) \" } function log () { notify-send -t 2000 'mail sync:' \" $@ \" } function die () { notify-send -t 2000 -u critical 'mail sync:' \" $@ \" exit 1 } # Let's Do stuff keyring-control # abort as soon as something fails set -e # abort if not online nm-online -x -t 0 echo --------------------------------------------------------- date for accfile in ` ls $ACCOUNTDIR ` ; do ACC = $( basename $accfile ) echo ------------------------ $ACC ------------------------ mbsync -V $ACC || log \" $ACC failed\" done # index and tag new mails echo ------------------------ NOTMUCH ------------------------ notmuch new 2 >/dev/null || die \"NOTMUCH new failed\" echo ------------------------ AFEW ------------------------ afew -v --tag --new || die \"AFEW died\" echo --------------------------------------------------------- echo \"all done, goodbye.\" Where flock is a tool to manage locks from shell scripts. And add the entry in your crontab -e . If you want to process your emails with this system through a command line interface, you can configure alot .","title":"Installation"},{"location":"email_management/","text":"Email can be one of the main aggregators of interruptions as it's supported by almost everything. I use it as the notification backend of services that don't need to be acted upon immediately or when more powerful mechanisms are not available. If not used wisely, it can be a sink of productivity. Analyze how often you need to check it \u2691 Follow the interruption analysis to discover how often you need to check it and if you need the notifications. Once you've decided the frequency, try to respect it!. If you want an example, check my work or personal analysis. Workflow \u2691 Each time I decide to go through my emails I follow the inbox processing guidelines . I understand the email inbox are items that need to be taken care of. If an email doesn't fall in that category I either archive or delete it. That way the inbox has the smallest number of items, and if everything went well, it is empty. Having an empty inbox helps you a lot to reduce the mental load for many reasons: When you look at it and don't see any mail, you get the small satisfaction that you have done everything. When there is something new, it stands out, without the distraction of other email subjects that can drift your attention. Accounts shared by many people \u2691 On email accounts managed by many people, I delete/archive emails that I know that need no interaction by any of them. If there is nothing for me to do, I mark them as read and wait for them to archive/delete them. If an email is left unread for 3 or 4 days I ask by other channels what should we do with that event. Use email to transport information, not to store it \u2691 Email was envisioned as a protocol for person A to send information to person B. The fact that the \"free email providers\" such as Google allow users to have almost no limit on their inbox has driven people to store all their emails and use it as a knowledge repository. This approach has many problems: As most people don't use end to end encryption (GPG), the data of their emails is available for the email provider to read. This is a privacy violation that leads to scary behaviours, such as targeted adds or google suggestions based on the content of recent emails. You could improve the situation by using POP3 instead of IMAP, but that'll force you to only use one device to check your email, something that's becoming uncommon. The decent email providers that respect you, such as RiseUp , Autistici or Disroot , are maintained by communities and can only offer a limited storage, so you're forced to empty your emails periodically to be able to receive new ones. If you don't spend time and effort classifying your emails, searching between them is a nightmare. It is even if you classify them. There are more efficient knowledge repositories to store your information. On my personal emails, I forward the information to my archive, task manager or knowledge manager, deleting the email afterwards. At work, they use an indecent provider, encrypts most of emails with GPG and trust the provider to hold the rest of the data. I try to leak the least amount of personal information and I archive every email because you don't know when you're going to need them. Use key bindings \u2691 Using the mouse to interact with the email client graphical interface is not efficient, try to learn the key bindings and use them as much as possible. Environment setup \u2691 Account management \u2691 It's common to have more than one account to check. For example, at work, I have my own account and another for each team I'm part of, the last ones are managed by all the team members. On the personal level, I've got many accounts for the different OpSec profiles or identities. For efficiency reasons, you need to be able to check all of them on one place. You can use an email manager such as Thunderbird . Once you choose one, try to master it. Isolate your work and personal environments \u2691 Make sure that you set your environment so that you can't check your personal email when you're working and the other way around. For example, you could set two Thunderbird profiles, or you could avoid configuring the work email in your personal phone. Automatic filtering and processing \u2691 Inbox management is time consuming, so you want to reduce the number of emails to process. From the interruption analysis you'll know which ones don't give you any value, our goal is to make them disappear before we open our inbox. You can get rid of them by: Preventing the sender to send them: Unsubscribe from the newsletters you no longer read or fix the configuration of the services that send you notifications that don't want. Tweak your spam filter: If you have no control on the source, tweak your spam filter so that it filters them out for you. Use your email client filtering and processing features: If you want to receive the emails for archival purposes, configure your email client to match them by regular expressions on the sender or subject, mark them as read and move them to the desired directory. Use email automation software: If you want to run automatic processes triggered by emails, use email automation solutions . Use your preferred editor to write the emails \u2691 You'll probably be less efficient with the email client's editor in comparison with your own. If you use vim or emacs, there's a good chance that the email client has a plugin that allows you to use it. Or you can always migrate to a command line client. I'll probably do that once I set up the email automation system .","title":"Email Management"},{"location":"email_management/#analyze-how-often-you-need-to-check-it","text":"Follow the interruption analysis to discover how often you need to check it and if you need the notifications. Once you've decided the frequency, try to respect it!. If you want an example, check my work or personal analysis.","title":"Analyze how often you need to check it"},{"location":"email_management/#workflow","text":"Each time I decide to go through my emails I follow the inbox processing guidelines . I understand the email inbox are items that need to be taken care of. If an email doesn't fall in that category I either archive or delete it. That way the inbox has the smallest number of items, and if everything went well, it is empty. Having an empty inbox helps you a lot to reduce the mental load for many reasons: When you look at it and don't see any mail, you get the small satisfaction that you have done everything. When there is something new, it stands out, without the distraction of other email subjects that can drift your attention.","title":"Workflow"},{"location":"email_management/#accounts-shared-by-many-people","text":"On email accounts managed by many people, I delete/archive emails that I know that need no interaction by any of them. If there is nothing for me to do, I mark them as read and wait for them to archive/delete them. If an email is left unread for 3 or 4 days I ask by other channels what should we do with that event.","title":"Accounts shared by many people"},{"location":"email_management/#use-email-to-transport-information-not-to-store-it","text":"Email was envisioned as a protocol for person A to send information to person B. The fact that the \"free email providers\" such as Google allow users to have almost no limit on their inbox has driven people to store all their emails and use it as a knowledge repository. This approach has many problems: As most people don't use end to end encryption (GPG), the data of their emails is available for the email provider to read. This is a privacy violation that leads to scary behaviours, such as targeted adds or google suggestions based on the content of recent emails. You could improve the situation by using POP3 instead of IMAP, but that'll force you to only use one device to check your email, something that's becoming uncommon. The decent email providers that respect you, such as RiseUp , Autistici or Disroot , are maintained by communities and can only offer a limited storage, so you're forced to empty your emails periodically to be able to receive new ones. If you don't spend time and effort classifying your emails, searching between them is a nightmare. It is even if you classify them. There are more efficient knowledge repositories to store your information. On my personal emails, I forward the information to my archive, task manager or knowledge manager, deleting the email afterwards. At work, they use an indecent provider, encrypts most of emails with GPG and trust the provider to hold the rest of the data. I try to leak the least amount of personal information and I archive every email because you don't know when you're going to need them.","title":"Use email to transport information, not to store it"},{"location":"email_management/#use-key-bindings","text":"Using the mouse to interact with the email client graphical interface is not efficient, try to learn the key bindings and use them as much as possible.","title":"Use key bindings"},{"location":"email_management/#environment-setup","text":"","title":"Environment setup"},{"location":"email_management/#account-management","text":"It's common to have more than one account to check. For example, at work, I have my own account and another for each team I'm part of, the last ones are managed by all the team members. On the personal level, I've got many accounts for the different OpSec profiles or identities. For efficiency reasons, you need to be able to check all of them on one place. You can use an email manager such as Thunderbird . Once you choose one, try to master it.","title":"Account management"},{"location":"email_management/#isolate-your-work-and-personal-environments","text":"Make sure that you set your environment so that you can't check your personal email when you're working and the other way around. For example, you could set two Thunderbird profiles, or you could avoid configuring the work email in your personal phone.","title":"Isolate your work and personal environments"},{"location":"email_management/#automatic-filtering-and-processing","text":"Inbox management is time consuming, so you want to reduce the number of emails to process. From the interruption analysis you'll know which ones don't give you any value, our goal is to make them disappear before we open our inbox. You can get rid of them by: Preventing the sender to send them: Unsubscribe from the newsletters you no longer read or fix the configuration of the services that send you notifications that don't want. Tweak your spam filter: If you have no control on the source, tweak your spam filter so that it filters them out for you. Use your email client filtering and processing features: If you want to receive the emails for archival purposes, configure your email client to match them by regular expressions on the sender or subject, mark them as read and move them to the desired directory. Use email automation software: If you want to run automatic processes triggered by emails, use email automation solutions .","title":"Automatic filtering and processing"},{"location":"email_management/#use-your-preferred-editor-to-write-the-emails","text":"You'll probably be less efficient with the email client's editor in comparison with your own. If you use vim or emacs, there's a good chance that the email client has a plugin that allows you to use it. Or you can always migrate to a command line client. I'll probably do that once I set up the email automation system .","title":"Use your preferred editor to write the emails"},{"location":"emojis/","text":"Curated list of emojis to copy paste. Angry \u2691 (\u0482\u2323\u0300_\u2323\u0301) ( >\u0434<) \u0295\u2022\u0300o\u2022\u0301\u0294 \u30fd(\u2267\u0414\u2266)\u30ce \u1559(\u21c0\u2038\u21bc\u2036)\u1557 \u0669(\u256c\u0298\u76ca\u0298\u256c)\u06f6 Annoyed \u2691 (>_<) Awesome \u2691 ( \u00b7_\u00b7) ( \u00b7_\u00b7) --\u25a0-\u25a0 ( \u00b7_\u00b7)--\u25a0-\u25a0 (-\u25a0_\u25a0) YEAAAAAAAAAAAAAAAAAAAAAHHHHHHHHHHHH Conforting \u2691 (\uff4f\u30fb_\u30fb)\u30ce\u201d(\u1d17_ \u1d17\u3002) Congratulations \u2691 ( \u141b )\u0648 \uff3c\\ \u0669( \u141b )\u0648 /\uff0f Crying \u2691 (\u2565\ufe4f\u2565) Excited \u2691 (((o(*\uff9f\u25bd\uff9f*)o))) o(\u2267\u2207\u2266o) Dance \u2691 (~\u203e\u25bf\u203e)~ ~(\u203e\u25bf\u203e)~ ~(\u203e\u25bf\u203e~) \u250c(\u30fb\u3002\u30fb)\u2518 \u266a \u2514(\u30fb\u3002\u30fb)\u2510 \u266a \u250c(\u30fb\u3002\u30fb)\u2518 \u01aa(\u02d8\u2323\u02d8)\u2510 \u01aa(\u02d8\u2323\u02d8)\u0283 \u250c(\u02d8\u2323\u02d8)\u0283 (>'-')> <('-'<) ^('-')^ v('-')v (>'-')> (^-^) Happy \u2691 \u1555( \u141b )\u1557 \u0295\u2022\u1d25\u2022\u0294 (\u2022\u203f\u2022) (\u25e1\u203f\u25e1\u273f) (\u273f\u25e0\u203f\u25e0) \u266a(\u0e51\u1d16\u25e1\u1d16\u0e51)\u266a Kisses \u2691 (\u3065\uffe3 \u00b3\uffe3)\u3065 ( \u02d8 \u00b3\u02d8)\u2665 Love \u2691 \u2764 Pride \u2691 <(\uffe3\uff3e\uffe3)> Relax \u2691 _\u3078__(\u203e\u25e1\u25dd )> Sad \u2691 \uff61\uff9f(*\u00b4\u25a1`)\uff9f\uff61 (\u25de\u2038\u25df\uff1b) Scared \u2691 \u30fd(\uff9f\u0414\uff9f)\uff89 \u30fd\u3014\uff9f\u0414\uff9f\u3015\u4e3f Sleepy \u2691 (\u1d17\u02f3\u1d17) Smug \u2691 \uff08\uffe3\uff5e\uffe3\uff09 Whyyyy? \u2691 (/\uff9f\u0414\uff9f)/ Surprised \u2691 (\\_/) (O.o) (> <) (\u2299_\u2609) (\u00ac\u00ba-\u00b0)\u00ac (\u2609_\u2609) (\u2022 \u0325\u0306\u2006\u2022) \u00af\\(\u00b0_o)/\u00af (\u30fb0\u30fb\u3002(\u30fb-\u30fb\u3002(\u30fb0\u30fb\u3002(\u30fb-\u30fb\u3002) (*\uff9f\u25ef\uff9f*) Who cares \u2691 \u00af\\_(\u30c4)_/\u00af WTF \u2691 (\u256f\u00b0\u25a1\u00b0)\u256f \u253b\u2501\u253b \u30d8\uff08\u3002\u25a1\u00b0\uff09\u30d8 Links \u2691 Japanese Emoticons","title":"Emojis"},{"location":"emojis/#angry","text":"(\u0482\u2323\u0300_\u2323\u0301) ( >\u0434<) \u0295\u2022\u0300o\u2022\u0301\u0294 \u30fd(\u2267\u0414\u2266)\u30ce \u1559(\u21c0\u2038\u21bc\u2036)\u1557 \u0669(\u256c\u0298\u76ca\u0298\u256c)\u06f6","title":"Angry"},{"location":"emojis/#annoyed","text":"(>_<)","title":"Annoyed"},{"location":"emojis/#awesome","text":"( \u00b7_\u00b7) ( \u00b7_\u00b7) --\u25a0-\u25a0 ( \u00b7_\u00b7)--\u25a0-\u25a0 (-\u25a0_\u25a0) YEAAAAAAAAAAAAAAAAAAAAAHHHHHHHHHHHH","title":"Awesome"},{"location":"emojis/#conforting","text":"(\uff4f\u30fb_\u30fb)\u30ce\u201d(\u1d17_ \u1d17\u3002)","title":"Conforting"},{"location":"emojis/#congratulations","text":"( \u141b )\u0648 \uff3c\\ \u0669( \u141b )\u0648 /\uff0f","title":"Congratulations"},{"location":"emojis/#crying","text":"(\u2565\ufe4f\u2565)","title":"Crying"},{"location":"emojis/#excited","text":"(((o(*\uff9f\u25bd\uff9f*)o))) o(\u2267\u2207\u2266o)","title":"Excited"},{"location":"emojis/#dance","text":"(~\u203e\u25bf\u203e)~ ~(\u203e\u25bf\u203e)~ ~(\u203e\u25bf\u203e~) \u250c(\u30fb\u3002\u30fb)\u2518 \u266a \u2514(\u30fb\u3002\u30fb)\u2510 \u266a \u250c(\u30fb\u3002\u30fb)\u2518 \u01aa(\u02d8\u2323\u02d8)\u2510 \u01aa(\u02d8\u2323\u02d8)\u0283 \u250c(\u02d8\u2323\u02d8)\u0283 (>'-')> <('-'<) ^('-')^ v('-')v (>'-')> (^-^)","title":"Dance"},{"location":"emojis/#happy","text":"\u1555( \u141b )\u1557 \u0295\u2022\u1d25\u2022\u0294 (\u2022\u203f\u2022) (\u25e1\u203f\u25e1\u273f) (\u273f\u25e0\u203f\u25e0) \u266a(\u0e51\u1d16\u25e1\u1d16\u0e51)\u266a","title":"Happy"},{"location":"emojis/#kisses","text":"(\u3065\uffe3 \u00b3\uffe3)\u3065 ( \u02d8 \u00b3\u02d8)\u2665","title":"Kisses"},{"location":"emojis/#love","text":"\u2764","title":"Love"},{"location":"emojis/#pride","text":"<(\uffe3\uff3e\uffe3)>","title":"Pride"},{"location":"emojis/#relax","text":"_\u3078__(\u203e\u25e1\u25dd )>","title":"Relax"},{"location":"emojis/#sad","text":"\uff61\uff9f(*\u00b4\u25a1`)\uff9f\uff61 (\u25de\u2038\u25df\uff1b)","title":"Sad"},{"location":"emojis/#scared","text":"\u30fd(\uff9f\u0414\uff9f)\uff89 \u30fd\u3014\uff9f\u0414\uff9f\u3015\u4e3f","title":"Scared"},{"location":"emojis/#sleepy","text":"(\u1d17\u02f3\u1d17)","title":"Sleepy"},{"location":"emojis/#smug","text":"\uff08\uffe3\uff5e\uffe3\uff09","title":"Smug"},{"location":"emojis/#whyyyy","text":"(/\uff9f\u0414\uff9f)/","title":"Whyyyy?"},{"location":"emojis/#surprised","text":"(\\_/) (O.o) (> <) (\u2299_\u2609) (\u00ac\u00ba-\u00b0)\u00ac (\u2609_\u2609) (\u2022 \u0325\u0306\u2006\u2022) \u00af\\(\u00b0_o)/\u00af (\u30fb0\u30fb\u3002(\u30fb-\u30fb\u3002(\u30fb0\u30fb\u3002(\u30fb-\u30fb\u3002) (*\uff9f\u25ef\uff9f*)","title":"Surprised"},{"location":"emojis/#who-cares","text":"\u00af\\_(\u30c4)_/\u00af","title":"Who cares"},{"location":"emojis/#wtf","text":"(\u256f\u00b0\u25a1\u00b0)\u256f \u253b\u2501\u253b \u30d8\uff08\u3002\u25a1\u00b0\uff09\u30d8","title":"WTF"},{"location":"emojis/#links","text":"Japanese Emoticons","title":"Links"},{"location":"fastapi/","text":"FastAPI is a modern, fast (high-performance), web framework for building APIs with Python 3.6+ based on standard Python type hints. The key features are: Fast: Very high performance, on par with NodeJS and Go (thanks to Starlette and Pydantic). One of the fastest Python frameworks available. Fast to code: Increase the speed to develop features by about 200% to 300%. Fewer bugs: Reduce about 40% of human (developer) induced errors. Intuitive: Great editor support. Completion everywhere. Less time debugging. Easy: Designed to be easy to use and learn. Less time reading docs. Short: Minimize code duplication. Multiple features from each parameter declaration. Fewer bugs. Robust: Get production-ready code. With automatic interactive documentation. Standards-based: Based on (and fully compatible with) the open standards for APIs: OpenAPI (previously known as Swagger) and JSON Schema. Authentication with JWT : with a super nice tutorial on how to set it up. Installation \u2691 pip install fastapi You will also need an ASGI server, for production such as Uvicorn or Hypercorn. pip install uvicorn [ standard ] Simple example \u2691 Create a file main.py with: from typing import Optional from fastapi import FastAPI app = FastAPI () @app . get ( \"/\" ) def read_root (): return { \"Hello\" : \"World\" } @app . get ( \"/items/ {item_id} \" ) def read_item ( item_id : int , q : Optional [ str ] = None ): return { \"item_id\" : item_id , \"q\" : q } Run the server: uvicorn main:app --reload Open your browser at http://127.0.0.1:8000/items/5?q=somequery . You will see the JSON response as: { \"item_id\" : 5 , \"q\" : \"somequery\" } You already created an API that: Receives HTTP requests in the paths / and /items/{item_id} . Both paths take GET operations (also known as HTTP methods). The path /items/{item_id} has a path parameter item_id that should be an int . The path /items/{item_id} has an optional str query parameter q . Has interactive API docs made for you: Swagger: http://127.0.0.1:8000/docs . Redoc: http://127.0.0.1:8000/redoc . You will see the automatic interactive API documentation (provided by Swagger UI): Sending data to the server \u2691 When you need to send data from a client (let's say, a browser) to your API, you have three basic options: As path parameters in the URL ( /items/2 ). As query parameters in the URL ( /items/2?skip=true ). In the body of a POST request. To send simple data use the first two, to send complex or sensitive data, use the last. It also supports sending data through cookies and headers . Path Parameters \u2691 You can declare path \"parameters\" or \"variables\" with the same syntax used by Python format strings: @app . get ( \"/items/ {item_id} \" ) def read_item ( item_id : int ): return { \"item_id\" : item_id } If you define the type hints of the function arguments, FastAPI will use pydantic data validation. If you need to use a Linux path as an argument, check this workaround , but be aware that it's not supported by OpenAPI. Order matters \u2691 Because path operations are evaluated in order, you need to make sure that the path for the fixed endpoint /users/me is declared before the variable one /users/{user_id} : @app . get ( \"/users/me\" ) async def read_user_me (): return { \"user_id\" : \"the current user\" } @app . get ( \"/users/ {user_id} \" ) async def read_user ( user_id : str ): return { \"user_id\" : user_id } Otherwise, the path for /users/{user_id} would match also for /users/me , \"thinking\" that it's receiving a parameter user_id with a value of \"me\". Predefined values \u2691 If you want the possible valid path parameter values to be predefined, you can use a standard Python Enum . from enum import Enum class ModelName ( str , Enum ): alexnet = \"alexnet\" resnet = \"resnet\" lenet = \"lenet\" @app . get ( \"/models/ {model_name} \" ) def get_model ( model_name : ModelName ): if model_name == ModelName . alexnet : return { \"model_name\" : model_name , \"message\" : \"Deep Learning FTW!\" } if model_name . value == \"lenet\" : return { \"model_name\" : model_name , \"message\" : \"LeCNN all the images\" } return { \"model_name\" : model_name , \"message\" : \"Have some residuals\" } These are the basics, FastAPI supports more complex path parameters and string validations . Query Parameters \u2691 When you declare other function parameters that are not part of the path parameters, they are automatically interpreted as \"query\" parameters. fake_items_db = [{ \"item_name\" : \"Foo\" }, { \"item_name\" : \"Bar\" }, { \"item_name\" : \"Baz\" }] @app . get ( \"/items/\" ) async def read_item ( skip : int = 0 , limit : int = 10 ): return fake_items_db [ skip : skip + limit ] The query is the set of key-value pairs that go after the ? in a URL, separated by & characters. For example, in the URL: http://127.0.0.1:8000/items/?skip=0&limit=10 These are the basics, FastAPI supports more complex query parameters and string validations . Request Body \u2691 To declare a request body, you use Pydantic models with all their power and benefits. from typing import Optional from pydantic import BaseModel class Item ( BaseModel ): name : str description : Optional [ str ] = None price : float tax : Optional [ float ] = None @app . post ( \"/items/\" ) async def create_item ( item : Item ): return item With just that Python type declaration, FastAPI will: Read the body of the request as JSON. Convert the corresponding types (if needed). Validate the data: If the data is invalid, it will return a nice and clear error, indicating exactly where and what was the incorrect data. Give you the received data in the parameter item . Generate JSON Schema definitions for your model. Those schemas will be part of the generated OpenAPI schema, and used by the automatic documentation UIs. These are the basics, FastAPI supports more complex patterns such as: Using multiple models in the same query . Additional validations of the pydantic models . Nested models . Sending data to the client \u2691 When you create a FastAPI path operation you can normally return any data from it: a dict , a list , a Pydantic model, a database model, etc. By default, FastAPI would automatically convert that return value to JSON using the jsonable_encoder . To return custom responses such as a direct string, xml or html use Response : from fastapi import FastAPI , Response app = FastAPI () @app . get ( \"/legacy/\" ) def get_legacy_data (): data = \"\"\"<?xml version=\"1.0\"?> <shampoo> <Header> Apply shampoo here. </Header> <Body> You'll have to use soap here. </Body> </shampoo> \"\"\" return Response ( content = data , media_type = \"application/xml\" ) Handling errors \u2691 There are many situations in where you need to notify an error to a client that is using your API. In these cases, you would normally return an HTTP status code in the range of 400 (from 400 to 499). This is similar to the 200 HTTP status codes (from 200 to 299). Those \"200\" status codes mean that somehow there was a \"success\" in the request. To return HTTP responses with errors to the client you use HTTPException . from fastapi import HTTPException items = { \"foo\" : \"The Foo Wrestlers\" } @app . get ( \"/items/ {item_id} \" ) async def read_item ( item_id : str ): if item_id not in items : raise HTTPException ( status_code = 404 , detail = \"Item not found\" ) return { \"item\" : items [ item_id ]} Updating data \u2691 Update replacing with PUT \u2691 To update an item you can use the HTTP PUT operation. You can use the jsonable_encoder to convert the input data to data that can be stored as JSON (e.g. with a NoSQL database). For example, converting datetime to str. from typing import List , Optional from fastapi.encoders import jsonable_encoder from pydantic import BaseModel class Item ( BaseModel ): name : Optional [ str ] = None description : Optional [ str ] = None price : Optional [ float ] = None tax : float = 10.5 tags : List [ str ] = [] items = { \"foo\" : { \"name\" : \"Foo\" , \"price\" : 50.2 }, \"bar\" : { \"name\" : \"Bar\" , \"description\" : \"The bartenders\" , \"price\" : 62 , \"tax\" : 20.2 }, \"baz\" : { \"name\" : \"Baz\" , \"description\" : None , \"price\" : 50.2 , \"tax\" : 10.5 , \"tags\" : []}, } @app . get ( \"/items/ {item_id} \" , response_model = Item ) async def read_item ( item_id : str ): return items [ item_id ] @app . put ( \"/items/ {item_id} \" , response_model = Item ) async def update_item ( item_id : str , item : Item ): update_item_encoded = jsonable_encoder ( item ) items [ item_id ] = update_item_encoded return update_item_encoded Partial updates with PATCH \u2691 You can also use the HTTP PATCH operation to partially update data. This means that you can send only the data that you want to update, leaving the rest intact. Configuration \u2691 Application configuration \u2691 In many cases your application could need some external settings or configurations, for example secret keys, database credentials, credentials for email services, etc. You can load these configurations through environmental variables , or you can use the awesome Pydantic settings management , whose advantages are: Do Pydantic's type validation on the fields. Automatically reads the missing values from environmental variables . Supports reading variables from Dotenv files . Supports secrets . First you define the Settings class with all the fields: File: config.py from pydantic import BaseSettings class Settings ( BaseSettings ): verbose : bool = True database_url : str = \"tinydb://~/.local/share/pyscrobbler/database.tinydb\" Then in the api definition, set the dependency . File: api.py from functools import lru_cache from fastapi import Depends , FastAPI app = FastAPI () @lru_cache () def get_settings () -> Settings : \"\"\"Configure the program settings.\"\"\" return Settings () @app . get ( \"/verbose\" ) def verbose ( settings : Settings = Depends ( get_settings )) -> bool : return settings . verbose Where: get_settings is the dependency function that configures the Settings object. The endpoint verbose is dependant of get_settings . The @lru_cache decorator changes the function it decorates to return the same value that was returned the first time, instead of computing it again, executing the code of the function every time. So, the function will be executed once for each combination of arguments. And then the values returned by each of those combinations of arguments will be used again and again whenever the function is called with exactly the same combination of arguments. Creating the Settings object is a costly operation as it needs to check the environment variables or read a file, so we want to do it just once, not on each request. This setup makes it easy to inject testing configuration so as not to break production code. OpenAPI configuration \u2691 Define title, description and version \u2691 from fastapi import FastAPI app = FastAPI ( title = \"My Super Project\" , description = \"This is a very fancy project, with auto docs for the API and everything\" , version = \"2.5.0\" , ) Define path tags \u2691 You can add tags to your path operation, pass the parameter tags with a list of str (commonly just one str ): from typing import Optional , Set from pydantic import BaseModel class Item ( BaseModel ): name : str description : Optional [ str ] = None price : float tax : Optional [ float ] = None tags : Set [ str ] = [] @app . post ( \"/items/\" , response_model = Item , tags = [ \"items\" ]) async def create_item ( item : Item ): return item @app . get ( \"/items/\" , tags = [ \"items\" ]) async def read_items (): return [{ \"name\" : \"Foo\" , \"price\" : 42 }] @app . get ( \"/users/\" , tags = [ \"users\" ]) async def read_users (): return [{ \"username\" : \"johndoe\" }] They will be added to the OpenAPI schema and used by the automatic documentation interfaces. Add metadata to the tags \u2691 tags_metadata = [ { \"name\" : \"users\" , \"description\" : \"Operations with users. The **login** logic is also here.\" , }, { \"name\" : \"items\" , \"description\" : \"Manage items. So _fancy_ they have their own docs.\" , \"externalDocs\" : { \"description\" : \"Items external docs\" , \"url\" : \"https://fastapi.tiangolo.com/\" , }, }, ] app = FastAPI(openapi_tags=tags_metadata) Add a summary and description \u2691 @app . post ( \"/items/\" , response_model = Item , summary = \"Create an item\" ) async def create_item ( item : Item ): \"\"\" Create an item with all the information: - **name**: each item must have a name - **description**: a long description - **price**: required - **tax**: if the item doesn't have tax, you can omit this - **tags**: a set of unique tag strings for this item \"\"\" return item Response description \u2691 @app . post ( \"/items/\" , response_description = \"The created item\" , ) async def create_item ( item : Item ): return item Deprecate a path operation \u2691 When you need to mark a path operation as deprecated, but without removing it @app . get ( \"/elements/\" , tags = [ \"items\" ], deprecated = True ) async def read_elements (): return [{ \"item_id\" : \"Foo\" }] Deploy with Docker . \u2691 FastAPI has it's own optimized docker , which makes the deployment of your applications really easy. In your project directory create the Dockerfile file: FROM tiangolo/uvicorn-gunicorn-fastapi:python3.7 COPY ./app /app Go to the project directory (in where your Dockerfile is, containing your app directory). Build your FastAPI image: docker build -t myimage . Run a container based on your image: docker run -d --name mycontainer -p 80 :80 myimage Now you have an optimized FastAPI server in a Docker container. Auto-tuned for your current server (and number of CPU cores). Installing dependencies \u2691 If your program needs other dependencies, use the next dockerfile: FROM tiangolo/uvicorn-gunicorn-fastapi:python3.7 COPY ./requirements.txt /app RUN pip install -r requirements.txt COPY ./app /app Other project structures \u2691 The previous examples assume that you have followed the FastAPI project structure. If instead you've used mine your application will be defined in the app variable in the src/program_name/entrypoints/api.py file. To make things simpler make the app variable available on the root of your package, so you can do from program_name import app instead of from program_name.entrypoints.api import app . To do that we need to add app to the __all__ internal python variable of the __init__.py file of our package. File: src/program_name/ init .py from .entrypoints.api import app __all__ : List [ str ] = [ 'app' ] The image is configured through environmental variables So we will need to use: FROM tiangolo/uvicorn-gunicorn-fastapi:python3.7 ENV MODULE_NAME = \"program_name\" COPY ./src/program_name /app/program_name Testing \u2691 FastAPI gives a TestClient object borrowed from Starlette to do the integration tests on your application. from fastapi import FastAPI from fastapi.testclient import TestClient app = FastAPI () @app . get ( \"/\" ) async def read_main (): return { \"msg\" : \"Hello World\" } @pytest . fixture ( name = \"client\" ) def client_ () -> TestClient : \"\"\"Configure FastAPI TestClient.\"\"\" return TestClient ( app ) def test_read_main ( client : TestClient ): response = client . get ( \"/\" ) assert response . status_code == 200 assert response . json () == { \"msg\" : \"Hello World\" } Test a POST request \u2691 result = client . post ( \"/items/\" , headers = { \"X-Token\" : \"coneofsilence\" }, json = { \"id\" : \"foobar\" , \"title\" : \"Foo Bar\" , \"description\" : \"The Foo Barters\" }, ) Inject testing configuration \u2691 If your application follows the application configuration section , injecting testing configuration is easy with dependency injection . Imagine you have a db_tinydb fixture that sets up the testing database: @pytest . fixture ( name = \"db_tinydb\" ) def db_tinydb_ ( tmpdir : LocalPath ) -> str : \"\"\"Create an TinyDB database engine. Returns: database_url: Url used to connect to the database. \"\"\" tinydb_file_path = str ( tmpdir . join ( \"tinydb.db\" )) return f \"tinydb:/// { tinydb_file_path } \" You can override the default database_url with: @pytest . fixture ( name = \"client\" ) def client_ ( db_tinydb : str ) -> TestClient : \"\"\"Configure FastAPI TestClient.\"\"\" def override_settings () -> Settings : \"\"\"Inject the testing database in the application settings.\"\"\" return Settings ( database_url = db_tinydb ) app . dependency_overrides [ get_settings ] = override_settings return TestClient ( app ) Tips and tricks \u2691 Create redirections \u2691 Returns an HTTP redirect. Uses a 307 status code (Temporary Redirect) by default. from fastapi import FastAPI from fastapi.responses import RedirectResponse app = FastAPI () @app . get ( \"/typer\" ) async def read_typer (): return RedirectResponse ( \"https://typer.tiangolo.com\" ) Test that your application works locally \u2691 Once you have your application built and tested , everything should work right? well, sometimes it don't. If you need to use pdb to debug what's going on, you can't use the docker as you won't be able to interact with the debugger. Instead, launch an uvicorn application directly with: uvicorn program_name:app --reload The command is assuming that your app is available at the root of your package, look at the deploy section if you feel lost. Logging \u2691 By default the application log messages are not shown in the uvicorn log , you need to add the next lines to the file where your app is defined: File: src/program_name/entrypoints/api.py from fastapi import FastAPI from fastapi.logger import logger import logging log = logging . getLogger ( \"gunicorn.error\" ) logger . handlers = log . handlers if __name__ != \"main\" : logger . setLevel ( log . level ) else : logger . setLevel ( logging . DEBUG ) app = FastAPI () # rest of the application... Logging to Sentry \u2691 FastAPI can integrate with Sentry or similar application loggers through the ASGI middleware . Run a FastAPI server in the background for testing purposes \u2691 Sometimes you want to launch a web server with a simple API to test a program that can't use the testing client . First define the API to launch with: File: tests/api_server.py from fastapi import FastAPI , HTTPException app = FastAPI () @app . get ( \"/existent\" ) async def existent (): return { \"msg\" : \"exists!\" } @app . get ( \"/inexistent\" ) async def inexistent (): raise HTTPException ( status_code = 404 , detail = \"It doesn't exist\" ) Then create the fixture: File: tests/conftest.py from multiprocessing import Process from typing import Generator import pytest import uvicorn from .api_server import app def run_server () -> None : \"\"\"Command to run the fake api server.\"\"\" uvicorn . run ( app ) @pytest . fixture () def _server () -> Generator [ None , None , None ]: \"\"\"Start the fake api server.\"\"\" proc = Process ( target = run_server , args = (), daemon = True ) proc . start () yield proc . kill () # Cleanup after test Now you can use the server: None fixture in your tests and run your queries against http://localhost:8000 . Interesting features to explore \u2691 Structure big applications . Dependency injection . Running background tasks after the request is finished . Return a different response model . Upload files . Set authentication . Host behind a proxy . Static files . Issues \u2691 FastAPI does not log messages : update pyscrobbler and any other maintained applications and remove the snippet defined in the logging section . References \u2691 Docs Git Awesome FastAPI Testdriven.io course : suggested by the developer.","title":"FastAPI"},{"location":"fastapi/#installation","text":"pip install fastapi You will also need an ASGI server, for production such as Uvicorn or Hypercorn. pip install uvicorn [ standard ]","title":"Installation"},{"location":"fastapi/#simple-example","text":"Create a file main.py with: from typing import Optional from fastapi import FastAPI app = FastAPI () @app . get ( \"/\" ) def read_root (): return { \"Hello\" : \"World\" } @app . get ( \"/items/ {item_id} \" ) def read_item ( item_id : int , q : Optional [ str ] = None ): return { \"item_id\" : item_id , \"q\" : q } Run the server: uvicorn main:app --reload Open your browser at http://127.0.0.1:8000/items/5?q=somequery . You will see the JSON response as: { \"item_id\" : 5 , \"q\" : \"somequery\" } You already created an API that: Receives HTTP requests in the paths / and /items/{item_id} . Both paths take GET operations (also known as HTTP methods). The path /items/{item_id} has a path parameter item_id that should be an int . The path /items/{item_id} has an optional str query parameter q . Has interactive API docs made for you: Swagger: http://127.0.0.1:8000/docs . Redoc: http://127.0.0.1:8000/redoc . You will see the automatic interactive API documentation (provided by Swagger UI):","title":"Simple example"},{"location":"fastapi/#sending-data-to-the-server","text":"When you need to send data from a client (let's say, a browser) to your API, you have three basic options: As path parameters in the URL ( /items/2 ). As query parameters in the URL ( /items/2?skip=true ). In the body of a POST request. To send simple data use the first two, to send complex or sensitive data, use the last. It also supports sending data through cookies and headers .","title":"Sending data to the server"},{"location":"fastapi/#path-parameters","text":"You can declare path \"parameters\" or \"variables\" with the same syntax used by Python format strings: @app . get ( \"/items/ {item_id} \" ) def read_item ( item_id : int ): return { \"item_id\" : item_id } If you define the type hints of the function arguments, FastAPI will use pydantic data validation. If you need to use a Linux path as an argument, check this workaround , but be aware that it's not supported by OpenAPI.","title":"Path Parameters"},{"location":"fastapi/#order-matters","text":"Because path operations are evaluated in order, you need to make sure that the path for the fixed endpoint /users/me is declared before the variable one /users/{user_id} : @app . get ( \"/users/me\" ) async def read_user_me (): return { \"user_id\" : \"the current user\" } @app . get ( \"/users/ {user_id} \" ) async def read_user ( user_id : str ): return { \"user_id\" : user_id } Otherwise, the path for /users/{user_id} would match also for /users/me , \"thinking\" that it's receiving a parameter user_id with a value of \"me\".","title":"Order matters"},{"location":"fastapi/#predefined-values","text":"If you want the possible valid path parameter values to be predefined, you can use a standard Python Enum . from enum import Enum class ModelName ( str , Enum ): alexnet = \"alexnet\" resnet = \"resnet\" lenet = \"lenet\" @app . get ( \"/models/ {model_name} \" ) def get_model ( model_name : ModelName ): if model_name == ModelName . alexnet : return { \"model_name\" : model_name , \"message\" : \"Deep Learning FTW!\" } if model_name . value == \"lenet\" : return { \"model_name\" : model_name , \"message\" : \"LeCNN all the images\" } return { \"model_name\" : model_name , \"message\" : \"Have some residuals\" } These are the basics, FastAPI supports more complex path parameters and string validations .","title":"Predefined values"},{"location":"fastapi/#query-parameters","text":"When you declare other function parameters that are not part of the path parameters, they are automatically interpreted as \"query\" parameters. fake_items_db = [{ \"item_name\" : \"Foo\" }, { \"item_name\" : \"Bar\" }, { \"item_name\" : \"Baz\" }] @app . get ( \"/items/\" ) async def read_item ( skip : int = 0 , limit : int = 10 ): return fake_items_db [ skip : skip + limit ] The query is the set of key-value pairs that go after the ? in a URL, separated by & characters. For example, in the URL: http://127.0.0.1:8000/items/?skip=0&limit=10 These are the basics, FastAPI supports more complex query parameters and string validations .","title":"Query Parameters"},{"location":"fastapi/#request-body","text":"To declare a request body, you use Pydantic models with all their power and benefits. from typing import Optional from pydantic import BaseModel class Item ( BaseModel ): name : str description : Optional [ str ] = None price : float tax : Optional [ float ] = None @app . post ( \"/items/\" ) async def create_item ( item : Item ): return item With just that Python type declaration, FastAPI will: Read the body of the request as JSON. Convert the corresponding types (if needed). Validate the data: If the data is invalid, it will return a nice and clear error, indicating exactly where and what was the incorrect data. Give you the received data in the parameter item . Generate JSON Schema definitions for your model. Those schemas will be part of the generated OpenAPI schema, and used by the automatic documentation UIs. These are the basics, FastAPI supports more complex patterns such as: Using multiple models in the same query . Additional validations of the pydantic models . Nested models .","title":"Request Body"},{"location":"fastapi/#sending-data-to-the-client","text":"When you create a FastAPI path operation you can normally return any data from it: a dict , a list , a Pydantic model, a database model, etc. By default, FastAPI would automatically convert that return value to JSON using the jsonable_encoder . To return custom responses such as a direct string, xml or html use Response : from fastapi import FastAPI , Response app = FastAPI () @app . get ( \"/legacy/\" ) def get_legacy_data (): data = \"\"\"<?xml version=\"1.0\"?> <shampoo> <Header> Apply shampoo here. </Header> <Body> You'll have to use soap here. </Body> </shampoo> \"\"\" return Response ( content = data , media_type = \"application/xml\" )","title":"Sending data to the client"},{"location":"fastapi/#handling-errors","text":"There are many situations in where you need to notify an error to a client that is using your API. In these cases, you would normally return an HTTP status code in the range of 400 (from 400 to 499). This is similar to the 200 HTTP status codes (from 200 to 299). Those \"200\" status codes mean that somehow there was a \"success\" in the request. To return HTTP responses with errors to the client you use HTTPException . from fastapi import HTTPException items = { \"foo\" : \"The Foo Wrestlers\" } @app . get ( \"/items/ {item_id} \" ) async def read_item ( item_id : str ): if item_id not in items : raise HTTPException ( status_code = 404 , detail = \"Item not found\" ) return { \"item\" : items [ item_id ]}","title":"Handling errors"},{"location":"fastapi/#updating-data","text":"","title":"Updating data"},{"location":"fastapi/#update-replacing-with-put","text":"To update an item you can use the HTTP PUT operation. You can use the jsonable_encoder to convert the input data to data that can be stored as JSON (e.g. with a NoSQL database). For example, converting datetime to str. from typing import List , Optional from fastapi.encoders import jsonable_encoder from pydantic import BaseModel class Item ( BaseModel ): name : Optional [ str ] = None description : Optional [ str ] = None price : Optional [ float ] = None tax : float = 10.5 tags : List [ str ] = [] items = { \"foo\" : { \"name\" : \"Foo\" , \"price\" : 50.2 }, \"bar\" : { \"name\" : \"Bar\" , \"description\" : \"The bartenders\" , \"price\" : 62 , \"tax\" : 20.2 }, \"baz\" : { \"name\" : \"Baz\" , \"description\" : None , \"price\" : 50.2 , \"tax\" : 10.5 , \"tags\" : []}, } @app . get ( \"/items/ {item_id} \" , response_model = Item ) async def read_item ( item_id : str ): return items [ item_id ] @app . put ( \"/items/ {item_id} \" , response_model = Item ) async def update_item ( item_id : str , item : Item ): update_item_encoded = jsonable_encoder ( item ) items [ item_id ] = update_item_encoded return update_item_encoded","title":"Update replacing with PUT"},{"location":"fastapi/#partial-updates-with-patch","text":"You can also use the HTTP PATCH operation to partially update data. This means that you can send only the data that you want to update, leaving the rest intact.","title":"Partial updates with PATCH"},{"location":"fastapi/#configuration","text":"","title":"Configuration"},{"location":"fastapi/#application-configuration","text":"In many cases your application could need some external settings or configurations, for example secret keys, database credentials, credentials for email services, etc. You can load these configurations through environmental variables , or you can use the awesome Pydantic settings management , whose advantages are: Do Pydantic's type validation on the fields. Automatically reads the missing values from environmental variables . Supports reading variables from Dotenv files . Supports secrets . First you define the Settings class with all the fields: File: config.py from pydantic import BaseSettings class Settings ( BaseSettings ): verbose : bool = True database_url : str = \"tinydb://~/.local/share/pyscrobbler/database.tinydb\" Then in the api definition, set the dependency . File: api.py from functools import lru_cache from fastapi import Depends , FastAPI app = FastAPI () @lru_cache () def get_settings () -> Settings : \"\"\"Configure the program settings.\"\"\" return Settings () @app . get ( \"/verbose\" ) def verbose ( settings : Settings = Depends ( get_settings )) -> bool : return settings . verbose Where: get_settings is the dependency function that configures the Settings object. The endpoint verbose is dependant of get_settings . The @lru_cache decorator changes the function it decorates to return the same value that was returned the first time, instead of computing it again, executing the code of the function every time. So, the function will be executed once for each combination of arguments. And then the values returned by each of those combinations of arguments will be used again and again whenever the function is called with exactly the same combination of arguments. Creating the Settings object is a costly operation as it needs to check the environment variables or read a file, so we want to do it just once, not on each request. This setup makes it easy to inject testing configuration so as not to break production code.","title":"Application configuration"},{"location":"fastapi/#openapi-configuration","text":"","title":"OpenAPI configuration"},{"location":"fastapi/#define-title-description-and-version","text":"from fastapi import FastAPI app = FastAPI ( title = \"My Super Project\" , description = \"This is a very fancy project, with auto docs for the API and everything\" , version = \"2.5.0\" , )","title":"Define title, description and version"},{"location":"fastapi/#define-path-tags","text":"You can add tags to your path operation, pass the parameter tags with a list of str (commonly just one str ): from typing import Optional , Set from pydantic import BaseModel class Item ( BaseModel ): name : str description : Optional [ str ] = None price : float tax : Optional [ float ] = None tags : Set [ str ] = [] @app . post ( \"/items/\" , response_model = Item , tags = [ \"items\" ]) async def create_item ( item : Item ): return item @app . get ( \"/items/\" , tags = [ \"items\" ]) async def read_items (): return [{ \"name\" : \"Foo\" , \"price\" : 42 }] @app . get ( \"/users/\" , tags = [ \"users\" ]) async def read_users (): return [{ \"username\" : \"johndoe\" }] They will be added to the OpenAPI schema and used by the automatic documentation interfaces.","title":"Define path tags"},{"location":"fastapi/#add-metadata-to-the-tags","text":"tags_metadata = [ { \"name\" : \"users\" , \"description\" : \"Operations with users. The **login** logic is also here.\" , }, { \"name\" : \"items\" , \"description\" : \"Manage items. So _fancy_ they have their own docs.\" , \"externalDocs\" : { \"description\" : \"Items external docs\" , \"url\" : \"https://fastapi.tiangolo.com/\" , }, }, ] app = FastAPI(openapi_tags=tags_metadata)","title":"Add metadata to the tags"},{"location":"fastapi/#add-a-summary-and-description","text":"@app . post ( \"/items/\" , response_model = Item , summary = \"Create an item\" ) async def create_item ( item : Item ): \"\"\" Create an item with all the information: - **name**: each item must have a name - **description**: a long description - **price**: required - **tax**: if the item doesn't have tax, you can omit this - **tags**: a set of unique tag strings for this item \"\"\" return item","title":"Add a summary and description"},{"location":"fastapi/#response-description","text":"@app . post ( \"/items/\" , response_description = \"The created item\" , ) async def create_item ( item : Item ): return item","title":"Response description"},{"location":"fastapi/#deprecate-a-path-operation","text":"When you need to mark a path operation as deprecated, but without removing it @app . get ( \"/elements/\" , tags = [ \"items\" ], deprecated = True ) async def read_elements (): return [{ \"item_id\" : \"Foo\" }]","title":"Deprecate a path operation"},{"location":"fastapi/#deploy-with-docker","text":"FastAPI has it's own optimized docker , which makes the deployment of your applications really easy. In your project directory create the Dockerfile file: FROM tiangolo/uvicorn-gunicorn-fastapi:python3.7 COPY ./app /app Go to the project directory (in where your Dockerfile is, containing your app directory). Build your FastAPI image: docker build -t myimage . Run a container based on your image: docker run -d --name mycontainer -p 80 :80 myimage Now you have an optimized FastAPI server in a Docker container. Auto-tuned for your current server (and number of CPU cores).","title":"Deploy with Docker."},{"location":"fastapi/#installing-dependencies","text":"If your program needs other dependencies, use the next dockerfile: FROM tiangolo/uvicorn-gunicorn-fastapi:python3.7 COPY ./requirements.txt /app RUN pip install -r requirements.txt COPY ./app /app","title":"Installing dependencies"},{"location":"fastapi/#other-project-structures","text":"The previous examples assume that you have followed the FastAPI project structure. If instead you've used mine your application will be defined in the app variable in the src/program_name/entrypoints/api.py file. To make things simpler make the app variable available on the root of your package, so you can do from program_name import app instead of from program_name.entrypoints.api import app . To do that we need to add app to the __all__ internal python variable of the __init__.py file of our package. File: src/program_name/ init .py from .entrypoints.api import app __all__ : List [ str ] = [ 'app' ] The image is configured through environmental variables So we will need to use: FROM tiangolo/uvicorn-gunicorn-fastapi:python3.7 ENV MODULE_NAME = \"program_name\" COPY ./src/program_name /app/program_name","title":"Other project structures"},{"location":"fastapi/#testing","text":"FastAPI gives a TestClient object borrowed from Starlette to do the integration tests on your application. from fastapi import FastAPI from fastapi.testclient import TestClient app = FastAPI () @app . get ( \"/\" ) async def read_main (): return { \"msg\" : \"Hello World\" } @pytest . fixture ( name = \"client\" ) def client_ () -> TestClient : \"\"\"Configure FastAPI TestClient.\"\"\" return TestClient ( app ) def test_read_main ( client : TestClient ): response = client . get ( \"/\" ) assert response . status_code == 200 assert response . json () == { \"msg\" : \"Hello World\" }","title":"Testing"},{"location":"fastapi/#test-a-post-request","text":"result = client . post ( \"/items/\" , headers = { \"X-Token\" : \"coneofsilence\" }, json = { \"id\" : \"foobar\" , \"title\" : \"Foo Bar\" , \"description\" : \"The Foo Barters\" }, )","title":"Test a POST request"},{"location":"fastapi/#inject-testing-configuration","text":"If your application follows the application configuration section , injecting testing configuration is easy with dependency injection . Imagine you have a db_tinydb fixture that sets up the testing database: @pytest . fixture ( name = \"db_tinydb\" ) def db_tinydb_ ( tmpdir : LocalPath ) -> str : \"\"\"Create an TinyDB database engine. Returns: database_url: Url used to connect to the database. \"\"\" tinydb_file_path = str ( tmpdir . join ( \"tinydb.db\" )) return f \"tinydb:/// { tinydb_file_path } \" You can override the default database_url with: @pytest . fixture ( name = \"client\" ) def client_ ( db_tinydb : str ) -> TestClient : \"\"\"Configure FastAPI TestClient.\"\"\" def override_settings () -> Settings : \"\"\"Inject the testing database in the application settings.\"\"\" return Settings ( database_url = db_tinydb ) app . dependency_overrides [ get_settings ] = override_settings return TestClient ( app )","title":"Inject testing configuration"},{"location":"fastapi/#tips-and-tricks","text":"","title":"Tips and tricks"},{"location":"fastapi/#create-redirections","text":"Returns an HTTP redirect. Uses a 307 status code (Temporary Redirect) by default. from fastapi import FastAPI from fastapi.responses import RedirectResponse app = FastAPI () @app . get ( \"/typer\" ) async def read_typer (): return RedirectResponse ( \"https://typer.tiangolo.com\" )","title":"Create redirections"},{"location":"fastapi/#test-that-your-application-works-locally","text":"Once you have your application built and tested , everything should work right? well, sometimes it don't. If you need to use pdb to debug what's going on, you can't use the docker as you won't be able to interact with the debugger. Instead, launch an uvicorn application directly with: uvicorn program_name:app --reload The command is assuming that your app is available at the root of your package, look at the deploy section if you feel lost.","title":"Test that your application works locally"},{"location":"fastapi/#logging","text":"By default the application log messages are not shown in the uvicorn log , you need to add the next lines to the file where your app is defined: File: src/program_name/entrypoints/api.py from fastapi import FastAPI from fastapi.logger import logger import logging log = logging . getLogger ( \"gunicorn.error\" ) logger . handlers = log . handlers if __name__ != \"main\" : logger . setLevel ( log . level ) else : logger . setLevel ( logging . DEBUG ) app = FastAPI () # rest of the application...","title":"Logging"},{"location":"fastapi/#logging-to-sentry","text":"FastAPI can integrate with Sentry or similar application loggers through the ASGI middleware .","title":"Logging to Sentry"},{"location":"fastapi/#run-a-fastapi-server-in-the-background-for-testing-purposes","text":"Sometimes you want to launch a web server with a simple API to test a program that can't use the testing client . First define the API to launch with: File: tests/api_server.py from fastapi import FastAPI , HTTPException app = FastAPI () @app . get ( \"/existent\" ) async def existent (): return { \"msg\" : \"exists!\" } @app . get ( \"/inexistent\" ) async def inexistent (): raise HTTPException ( status_code = 404 , detail = \"It doesn't exist\" ) Then create the fixture: File: tests/conftest.py from multiprocessing import Process from typing import Generator import pytest import uvicorn from .api_server import app def run_server () -> None : \"\"\"Command to run the fake api server.\"\"\" uvicorn . run ( app ) @pytest . fixture () def _server () -> Generator [ None , None , None ]: \"\"\"Start the fake api server.\"\"\" proc = Process ( target = run_server , args = (), daemon = True ) proc . start () yield proc . kill () # Cleanup after test Now you can use the server: None fixture in your tests and run your queries against http://localhost:8000 .","title":"Run a FastAPI server in the background for testing purposes"},{"location":"fastapi/#interesting-features-to-explore","text":"Structure big applications . Dependency injection . Running background tasks after the request is finished . Return a different response model . Upload files . Set authentication . Host behind a proxy . Static files .","title":"Interesting features to explore"},{"location":"fastapi/#issues","text":"FastAPI does not log messages : update pyscrobbler and any other maintained applications and remove the snippet defined in the logging section .","title":"Issues"},{"location":"fastapi/#references","text":"Docs Git Awesome FastAPI Testdriven.io course : suggested by the developer.","title":"References"},{"location":"ffmpeg/","text":"ffmpeg is a complete, cross-platform solution to record, convert and stream audio and video You can run ffmpeg -formats to get a list of every format that is supported. Cut \u2691 Cut video file into a shorter clip \u2691 You can use the time offset parameter -ss to specify the start time stamp in HH:MM:SS.ms format while the -t parameter is for specifying the actual duration of the clip in seconds. ffmpeg -i input.mp4 -ss 00 :00:50.0 -codec copy -t 20 output.mp4 Split a video into multiple parts \u2691 The next command will split the source video into 2 parts. One ending at 50s from the start and the other beginning at 50s and ending at the end of the input video. ffmpeg -i video.mp4 -t 00 :00:50 -c copy small-1.mp4 -ss 00 :00:50 -codec copy small-2.mp4 Crop an audio file \u2691 To create a 30 second audio file starting at 90 seconds from the original audio file without transcoding use: ffmpeg -ss 00 :01:30 -t 30 -acodec copy -i inputfile.mp3 outputfile.mp3 Join \u2691 Join (concatenate) video files \u2691 If you have multiple audio or video files encoded with the same codecs, you can join them into a single file. Create a input file with a list of all source files that you wish to concatenate and then run this command. Create first the file list with a Bash for loop: for f in ./*.wav ; do echo \"file ' $f '\" >> mylist.txt ; done Then convert ffmpeg -f concat -safe 0 -i mylist.txt -c copy output.mp4 Merge an audio and video file \u2691 You can also specify the -shortest switch to finish the encoding when the shortest clip ends. ffmpeg -i video.mp4 -i audio.mp3 -c:v copy -c:a aac -strict experimental output.mp4 ffmpeg -i video.mp4 -i audio.mp3 -c:v copy -c:a aac -strict experimental -shortest output.mp4 Mute \u2691 Use the -an parameter to disable the audio portion of a video stream. ffmpeg -i video.mp4 -an mute-video.mp4 Convert \u2691 Convert video from one format to another \u2691 You can use the -vcodec parameter to specify the encoding format to be used for the output video. Encoding a video takes time but you can speed up the process by forcing a preset though it would degrade the quality of the output video. ffmpeg -i youtube.flv -c:v libx264 filename.mp4 ffmpeg -i video.wmv -c:v libx264 -preset ultrafast video.mp4 Convert a x265 file into x264 \u2691 for i in *.mkv ; do ffmpeg -i \" $i \" -bsf:v h264_mp4toannexb -vcodec libx264 \" $i .x264.mkv\" done ffmpeg -i \"$i\" : Executes the program ffmpeg and calls for files to be processed. -bsf:v : Activates the video bit stream filter to be used. h264_mp4toannexb : Is the bit stream filter that is activated. Convert an H.264 bitstream from length prefixed mode to start code prefixed mode (as defined in the Annex B of the ITU-T H.264 specification). This is required by some streaming formats, typically the MPEG-2 transport stream format (mpegts) processing MKV h.264 (currently)requires this, if is not included you will get an error in the terminal window instructing you to use it. * -vcodec libx264 This tells ffmpeg to encode the output to H.264. * \"$i.ts\" Saves the output to .ts format, this is useful so as not to overwrite your source files. Convert a video into animated GIF \u2691 ffmpeg -ss 30 -t 3 -i input.mp4 -vf \"fps=10,scale=480:-1:flags=lanczos,split[s0][s1];[s0]palettegen[p];[s1][p]paletteuse\" -loop 0 output.gif This example will skip the first 30 seconds (-ss 30) of the input and create a 3 second output (-t 3). fps filter sets the frame rate. A rate of 10 frames per second is used in the example. Scale filter will resize the output to 320 pixels wide and automatically determine the height while preserving the aspect ratio. The lanczos scaling algorithm is used in this example. Palettegen and paletteuse filters will generate and use a custom palette generated from your input. These filters have many options, so refer to the links for a list of all available options and values. Also see the Advanced options section below. Split filter will allow everything to be done in one command and avoids having to create a temporary PNG file of the palette. Control looping with -loop output option but the values are confusing. A value of 0 is infinite looping, -1 is no looping, and 1 will loop once meaning it will play twice. So a value of 10 will cause the GIF to play 11 times. Convert video into images \u2691 You can use FFmpeg to automatically extract image frames from a video every n seconds and the images are saved in a sequence. This command saves image frame after every 4 seconds. ffmpeg -i movie.mp4 -r 0 .25 frames_%04d.png Convert a single image into a video \u2691 Use the -t parameter to specify the duration of the video. ffmpeg -loop 1 -i image.png -c:v libx264 -t 30 -pix_fmt yuv420p video.mp4 Convert opus or wav to mp3 \u2691 ffmpeg -i input.wav -vn -ar 44100 -ac 2 -b:a 320k output.mp3 -i : input file. -vn : Disable video, to make sure no video (including album cover image) is included if the source would be a video file. -ar : Set the audio sampling frequency. For output streams it is set by default to the frequency of the corresponding input stream. For input streams this option only makes sense for audio grabbing devices and raw demuxers and is mapped to the corresponding demuxer options. -ac : Set the number of audio channels. For output streams it is set by default to the number of input audio channels. For input streams this option only makes sense for audio grabbing devices and raw demuxers and is mapped to the corresponding demuxer options. So used here to make sure it is stereo (2 channels). -b:a : Converts the audio bitrate to be exact 320kbit per second. Extract \u2691 Extract the audio from video \u2691 The -vn switch extracts the audio portion from a video and we are using the -ab switch to save the audio as a 256kbps MP3 audio file. ffmpeg -i video.mp4 -vn -ab 256 audio.mp3 Extract image frames from a video \u2691 This command will extract the video frame at the 15s mark and saves it as a 800px wide JPEG image. You can also use the -s switch (like -s 400\u00d7300) to specify the exact dimensions of the image file though it will probably create a stretched image if the image size doesn\u2019t follow the aspect ratio of the original video file. ffmpeg -ss 00 :00:15 -i video.mp4 -vf scale = 800 :-1 -vframes 1 image.jpg Extract metadata of video \u2691 ffprobe {{ file }} Resize \u2691 Resize a video \u2691 Change the Constat Rate Factor \u2691 Setting the Constant Rate Factor, which lowers the average bit rate, but retains better quality. Vary the CRF between around 18 and 24 \u2014 the lower, the higher the bitrate. ffmpeg -i input.mp4 -vcodec libx265 -crf 24 output.mp4 Change the codec as needed - libx264 may be available if libx265 is not, at the cost of a slightly larger resultant file size. Change video resolution \u2691 Use the size -s switch with ffmpeg to resize a video while maintaining the aspect ratio. ffmpeg -i input.mp4 -s 480x320 -c:a copy output.mp4 Presentation \u2691 Create video slideshow from images \u2691 This command creates a video slideshow using a series of images that are named as img001.png , img002.png , etc. Each image will have a duration of 5 seconds (-r \u2155). ffmpeg -r 1 /5 -i img%03d.png -c:v libx264 -r 30 -pix_fmt yuv420p slideshow.mp4 Add a poster image to audio \u2691 You can add a cover image to an audio file and the length of the output video will be the same as that of the input audio stream. This may come handy for uploading MP3s to YouTube. ffmpeg -loop 1 -i image.jpg -i audio.mp3 -c:v libx264 -c:a aac -strict experimental -b:a 192k -shortest output.mp4 Add subtitles to a movie \u2691 This will take the subtitles from the .srt file. FFmpeg can decode most common subtitle formats. ffmpeg -i movie.mp4 -i subtitles.srt -map 0 -map 1 -c copy -c:v libx264 -crf 23 -preset veryfast output.mkv Change the audio volume \u2691 You can use the volume filter to alter the volume of a media file using FFmpeg. This command will half the volume of the audio file. ffmpeg -i input.wav -af 'volume=0.5' output.wav Rotate a video \u2691 This command will rotate a video clip 90\u00b0 clockwise. You can set transpose to 2 to rotate the video 90\u00b0 anti-clockwise. ffmpeg -i input.mp4 -filter:v 'transpose=1' rotated-video.mp4 This will rotate the video 180\u00b0 counter-clockwise. ffmpeg -i input.mp4 -filter:v 'transpose=2,transpose=2' rotated-video.mp4 Speed up or Slow down the video \u2691 You can change the speed of your video using the setpts (set presentation time stamp) filter of FFmpeg. This command will make the video 8x (\u215b) faster or use setpts=4*PTS to make the video 4x slower. ffmpeg -i input.mp4 -filter:v \"setpts=0.125*PTS\" output.mp4 Speed up or Slow down the audio \u2691 For changing the speed of audio, use the atempo audio filter. This command will double the speed of audio. You can use any value between 0.5 and 2.0 for audio. bash ffmpeg -i input.mkv -filter:a \"atempo=2.0\" -vn output.mkv Stack Exchange has a good overview to get you started with FFmpeg. You should also check out the official documentation at ffmpeg.org or the wiki at trac.ffmpeg.org to know about all the possible things you can do with FFmpeg. References \u2691 Home","title":"ffmpeg"},{"location":"ffmpeg/#cut","text":"","title":"Cut"},{"location":"ffmpeg/#cut-video-file-into-a-shorter-clip","text":"You can use the time offset parameter -ss to specify the start time stamp in HH:MM:SS.ms format while the -t parameter is for specifying the actual duration of the clip in seconds. ffmpeg -i input.mp4 -ss 00 :00:50.0 -codec copy -t 20 output.mp4","title":"Cut video file into a shorter clip"},{"location":"ffmpeg/#split-a-video-into-multiple-parts","text":"The next command will split the source video into 2 parts. One ending at 50s from the start and the other beginning at 50s and ending at the end of the input video. ffmpeg -i video.mp4 -t 00 :00:50 -c copy small-1.mp4 -ss 00 :00:50 -codec copy small-2.mp4","title":"Split a video into multiple parts"},{"location":"ffmpeg/#crop-an-audio-file","text":"To create a 30 second audio file starting at 90 seconds from the original audio file without transcoding use: ffmpeg -ss 00 :01:30 -t 30 -acodec copy -i inputfile.mp3 outputfile.mp3","title":"Crop an audio file"},{"location":"ffmpeg/#join","text":"","title":"Join"},{"location":"ffmpeg/#join-concatenate-video-files","text":"If you have multiple audio or video files encoded with the same codecs, you can join them into a single file. Create a input file with a list of all source files that you wish to concatenate and then run this command. Create first the file list with a Bash for loop: for f in ./*.wav ; do echo \"file ' $f '\" >> mylist.txt ; done Then convert ffmpeg -f concat -safe 0 -i mylist.txt -c copy output.mp4","title":"Join (concatenate) video files"},{"location":"ffmpeg/#merge-an-audio-and-video-file","text":"You can also specify the -shortest switch to finish the encoding when the shortest clip ends. ffmpeg -i video.mp4 -i audio.mp3 -c:v copy -c:a aac -strict experimental output.mp4 ffmpeg -i video.mp4 -i audio.mp3 -c:v copy -c:a aac -strict experimental -shortest output.mp4","title":"Merge an audio and video file"},{"location":"ffmpeg/#mute","text":"Use the -an parameter to disable the audio portion of a video stream. ffmpeg -i video.mp4 -an mute-video.mp4","title":"Mute"},{"location":"ffmpeg/#convert","text":"","title":"Convert"},{"location":"ffmpeg/#convert-video-from-one-format-to-another","text":"You can use the -vcodec parameter to specify the encoding format to be used for the output video. Encoding a video takes time but you can speed up the process by forcing a preset though it would degrade the quality of the output video. ffmpeg -i youtube.flv -c:v libx264 filename.mp4 ffmpeg -i video.wmv -c:v libx264 -preset ultrafast video.mp4","title":"Convert video from one format to another"},{"location":"ffmpeg/#convert-a-x265-file-into-x264","text":"for i in *.mkv ; do ffmpeg -i \" $i \" -bsf:v h264_mp4toannexb -vcodec libx264 \" $i .x264.mkv\" done ffmpeg -i \"$i\" : Executes the program ffmpeg and calls for files to be processed. -bsf:v : Activates the video bit stream filter to be used. h264_mp4toannexb : Is the bit stream filter that is activated. Convert an H.264 bitstream from length prefixed mode to start code prefixed mode (as defined in the Annex B of the ITU-T H.264 specification). This is required by some streaming formats, typically the MPEG-2 transport stream format (mpegts) processing MKV h.264 (currently)requires this, if is not included you will get an error in the terminal window instructing you to use it. * -vcodec libx264 This tells ffmpeg to encode the output to H.264. * \"$i.ts\" Saves the output to .ts format, this is useful so as not to overwrite your source files.","title":"Convert a x265 file into x264"},{"location":"ffmpeg/#convert-a-video-into-animated-gif","text":"ffmpeg -ss 30 -t 3 -i input.mp4 -vf \"fps=10,scale=480:-1:flags=lanczos,split[s0][s1];[s0]palettegen[p];[s1][p]paletteuse\" -loop 0 output.gif This example will skip the first 30 seconds (-ss 30) of the input and create a 3 second output (-t 3). fps filter sets the frame rate. A rate of 10 frames per second is used in the example. Scale filter will resize the output to 320 pixels wide and automatically determine the height while preserving the aspect ratio. The lanczos scaling algorithm is used in this example. Palettegen and paletteuse filters will generate and use a custom palette generated from your input. These filters have many options, so refer to the links for a list of all available options and values. Also see the Advanced options section below. Split filter will allow everything to be done in one command and avoids having to create a temporary PNG file of the palette. Control looping with -loop output option but the values are confusing. A value of 0 is infinite looping, -1 is no looping, and 1 will loop once meaning it will play twice. So a value of 10 will cause the GIF to play 11 times.","title":"Convert a video into animated GIF"},{"location":"ffmpeg/#convert-video-into-images","text":"You can use FFmpeg to automatically extract image frames from a video every n seconds and the images are saved in a sequence. This command saves image frame after every 4 seconds. ffmpeg -i movie.mp4 -r 0 .25 frames_%04d.png","title":"Convert video into images"},{"location":"ffmpeg/#convert-a-single-image-into-a-video","text":"Use the -t parameter to specify the duration of the video. ffmpeg -loop 1 -i image.png -c:v libx264 -t 30 -pix_fmt yuv420p video.mp4","title":"Convert a single image into a video"},{"location":"ffmpeg/#convert-opus-or-wav-to-mp3","text":"ffmpeg -i input.wav -vn -ar 44100 -ac 2 -b:a 320k output.mp3 -i : input file. -vn : Disable video, to make sure no video (including album cover image) is included if the source would be a video file. -ar : Set the audio sampling frequency. For output streams it is set by default to the frequency of the corresponding input stream. For input streams this option only makes sense for audio grabbing devices and raw demuxers and is mapped to the corresponding demuxer options. -ac : Set the number of audio channels. For output streams it is set by default to the number of input audio channels. For input streams this option only makes sense for audio grabbing devices and raw demuxers and is mapped to the corresponding demuxer options. So used here to make sure it is stereo (2 channels). -b:a : Converts the audio bitrate to be exact 320kbit per second.","title":"Convert opus or wav to mp3"},{"location":"ffmpeg/#extract","text":"","title":"Extract"},{"location":"ffmpeg/#extract-the-audio-from-video","text":"The -vn switch extracts the audio portion from a video and we are using the -ab switch to save the audio as a 256kbps MP3 audio file. ffmpeg -i video.mp4 -vn -ab 256 audio.mp3","title":"Extract the audio from video"},{"location":"ffmpeg/#extract-image-frames-from-a-video","text":"This command will extract the video frame at the 15s mark and saves it as a 800px wide JPEG image. You can also use the -s switch (like -s 400\u00d7300) to specify the exact dimensions of the image file though it will probably create a stretched image if the image size doesn\u2019t follow the aspect ratio of the original video file. ffmpeg -ss 00 :00:15 -i video.mp4 -vf scale = 800 :-1 -vframes 1 image.jpg","title":"Extract image frames from a video"},{"location":"ffmpeg/#extract-metadata-of-video","text":"ffprobe {{ file }}","title":"Extract metadata of video"},{"location":"ffmpeg/#resize","text":"","title":"Resize"},{"location":"ffmpeg/#resize-a-video","text":"","title":"Resize a video"},{"location":"ffmpeg/#change-the-constat-rate-factor","text":"Setting the Constant Rate Factor, which lowers the average bit rate, but retains better quality. Vary the CRF between around 18 and 24 \u2014 the lower, the higher the bitrate. ffmpeg -i input.mp4 -vcodec libx265 -crf 24 output.mp4 Change the codec as needed - libx264 may be available if libx265 is not, at the cost of a slightly larger resultant file size.","title":"Change the Constat Rate Factor"},{"location":"ffmpeg/#change-video-resolution","text":"Use the size -s switch with ffmpeg to resize a video while maintaining the aspect ratio. ffmpeg -i input.mp4 -s 480x320 -c:a copy output.mp4","title":"Change video resolution"},{"location":"ffmpeg/#presentation","text":"","title":"Presentation"},{"location":"ffmpeg/#create-video-slideshow-from-images","text":"This command creates a video slideshow using a series of images that are named as img001.png , img002.png , etc. Each image will have a duration of 5 seconds (-r \u2155). ffmpeg -r 1 /5 -i img%03d.png -c:v libx264 -r 30 -pix_fmt yuv420p slideshow.mp4","title":"Create video slideshow from images"},{"location":"ffmpeg/#add-a-poster-image-to-audio","text":"You can add a cover image to an audio file and the length of the output video will be the same as that of the input audio stream. This may come handy for uploading MP3s to YouTube. ffmpeg -loop 1 -i image.jpg -i audio.mp3 -c:v libx264 -c:a aac -strict experimental -b:a 192k -shortest output.mp4","title":"Add a poster image to audio"},{"location":"ffmpeg/#add-subtitles-to-a-movie","text":"This will take the subtitles from the .srt file. FFmpeg can decode most common subtitle formats. ffmpeg -i movie.mp4 -i subtitles.srt -map 0 -map 1 -c copy -c:v libx264 -crf 23 -preset veryfast output.mkv","title":"Add subtitles to a movie"},{"location":"ffmpeg/#change-the-audio-volume","text":"You can use the volume filter to alter the volume of a media file using FFmpeg. This command will half the volume of the audio file. ffmpeg -i input.wav -af 'volume=0.5' output.wav","title":"Change the audio volume"},{"location":"ffmpeg/#rotate-a-video","text":"This command will rotate a video clip 90\u00b0 clockwise. You can set transpose to 2 to rotate the video 90\u00b0 anti-clockwise. ffmpeg -i input.mp4 -filter:v 'transpose=1' rotated-video.mp4 This will rotate the video 180\u00b0 counter-clockwise. ffmpeg -i input.mp4 -filter:v 'transpose=2,transpose=2' rotated-video.mp4","title":"Rotate a video"},{"location":"ffmpeg/#speed-up-or-slow-down-the-video","text":"You can change the speed of your video using the setpts (set presentation time stamp) filter of FFmpeg. This command will make the video 8x (\u215b) faster or use setpts=4*PTS to make the video 4x slower. ffmpeg -i input.mp4 -filter:v \"setpts=0.125*PTS\" output.mp4","title":"Speed up or Slow down the video"},{"location":"ffmpeg/#speed-up-or-slow-down-the-audio","text":"For changing the speed of audio, use the atempo audio filter. This command will double the speed of audio. You can use any value between 0.5 and 2.0 for audio. bash ffmpeg -i input.mkv -filter:a \"atempo=2.0\" -vn output.mkv Stack Exchange has a good overview to get you started with FFmpeg. You should also check out the official documentation at ffmpeg.org or the wiki at trac.ffmpeg.org to know about all the possible things you can do with FFmpeg.","title":"Speed up or Slow down the audio"},{"location":"ffmpeg/#references","text":"Home","title":"References"},{"location":"fitness_band/","text":"Fitness tracker or activity trackers are devices or applications for monitoring and tracking fitness-related metrics such as distance walked or run, calorie consumption, and in some cases heartbeat. It is a type of wearable computer. As with anything that can be bought, I usually first try a cheap model to see if I need the advanced features that the expensive ones offer. After a quick model review, I went for the Amazfit band 5 . I've now discovered wasp-os an open source firmware for smart watches that are based on the nRF52 family of microcontrollers. Fully supported by gadgetbridge , Wasp-os features full heart rate monitoring and step counting support together with multiple clock faces, a stopwatch, an alarm clock, a countdown timer, a calculator and lots of other games and utilities. All of this, and still with access to the MicroPython REPL for interactive tweaking, development and testing. Currently it support the following devices: Colmi P8 Senbono K9 Pine64 PineTime Pinetime seems to be a work in progress, Colmi P8 looks awesome, and the Senbono K9 looks good too but wasp-os is lacking touch screen support. So if I had to choose now, I'd try the Colmi P8, the only thing that I'd miss is the possible voice assistant support. They say that you can take one for 18$ in aliexpress.","title":"Fitness Tracker"},{"location":"forking_this_wiki/","text":"Follow the next steps. Download the repository \u2691 On your terminal, clone this repository with: git clone https://github.com/lyz-code/blue-book.git Don't fork the repository via Github If you do that, you'll have all the history of my repository, which will make your repository more heavy than it should (as I have a lot of images), and it will make it hard for me to make pull requests to your digital garden. Adaptations \u2691 Project name and repository URL \u2691 There are several files that contain references to this repository's name and URL, which is different to the new forked repository URL, since the user name and the repository name might have changed. As of now, the files where you should replace the references are: README.md mkdocs.yml theme/main.html Blue book is the name of my personal digital garden, try to find a different name for your project that is meaningful to you. Documents and structure \u2691 You can either use the documents of this wiki and extend them, or change the structure by editing the nav section of the mkdocs.yml file. If you want to start from scratch, remove everything on the docs directory. Remove the newsletter feature \u2691 The newsletter feature allows your readers to keep updated on the changes of your garden. If you don't want them: Remove the plugin mkdocs-newsletter from the requirements.in and mkdocs.yaml files. Remove the references both to header and footer. To do that, undo the steps described here . Remove the cron configuration of the .github/workflows/gh-pages.yml pipeline: schedule : - cron : 11 06 * * * Dependencies \u2691 In order to be able to build your site, some Python dependencies are needed. You can install them by running make update Checking how it looks \u2691 First, clean the old generated site with make clean Then, you can preview the site on your local machine by running make and then opening the link in your web browser. Set up the Github repository \u2691 On GitHub create a new repository by clicking on the + symbol on the top right and then New Repository . Removing the old commits \u2691 The mkdocs-newsletter plugin uses the commit history to generate the newsletter articles, so if you want to start the newsletter from scratch, a way of doing so is removing the commit history. A way of doing so is removing the .git folder and re-initializing the repository. Within the repository directory do rm -rf .git git init git remote add origin git@github.com:your_username/your_project_name.git git add . git commit -m \"Initial commit\" git push --set-upstream origin master Remember to change your_username and your_project_name to your real values. Setting up GitHub Pages \u2691 To enable the Github Pages website associated with your repo, follow these steps: Create SSH Deploy Key . Activate the GitHub Pages repository configuration with the gh-pages branch. Now, the site will be built whenever you push new commits and periodically, according to the cron configuration from .github/workflows/gh-pages.yml .","title":"Forking this garden"},{"location":"forking_this_wiki/#download-the-repository","text":"On your terminal, clone this repository with: git clone https://github.com/lyz-code/blue-book.git Don't fork the repository via Github If you do that, you'll have all the history of my repository, which will make your repository more heavy than it should (as I have a lot of images), and it will make it hard for me to make pull requests to your digital garden.","title":"Download the repository"},{"location":"forking_this_wiki/#adaptations","text":"","title":"Adaptations"},{"location":"forking_this_wiki/#project-name-and-repository-url","text":"There are several files that contain references to this repository's name and URL, which is different to the new forked repository URL, since the user name and the repository name might have changed. As of now, the files where you should replace the references are: README.md mkdocs.yml theme/main.html Blue book is the name of my personal digital garden, try to find a different name for your project that is meaningful to you.","title":"Project name and repository URL"},{"location":"forking_this_wiki/#documents-and-structure","text":"You can either use the documents of this wiki and extend them, or change the structure by editing the nav section of the mkdocs.yml file. If you want to start from scratch, remove everything on the docs directory.","title":"Documents and structure"},{"location":"forking_this_wiki/#remove-the-newsletter-feature","text":"The newsletter feature allows your readers to keep updated on the changes of your garden. If you don't want them: Remove the plugin mkdocs-newsletter from the requirements.in and mkdocs.yaml files. Remove the references both to header and footer. To do that, undo the steps described here . Remove the cron configuration of the .github/workflows/gh-pages.yml pipeline: schedule : - cron : 11 06 * * *","title":"Remove the newsletter feature"},{"location":"forking_this_wiki/#dependencies","text":"In order to be able to build your site, some Python dependencies are needed. You can install them by running make update","title":"Dependencies"},{"location":"forking_this_wiki/#checking-how-it-looks","text":"First, clean the old generated site with make clean Then, you can preview the site on your local machine by running make and then opening the link in your web browser.","title":"Checking how it looks"},{"location":"forking_this_wiki/#set-up-the-github-repository","text":"On GitHub create a new repository by clicking on the + symbol on the top right and then New Repository .","title":"Set up the Github repository"},{"location":"forking_this_wiki/#removing-the-old-commits","text":"The mkdocs-newsletter plugin uses the commit history to generate the newsletter articles, so if you want to start the newsletter from scratch, a way of doing so is removing the commit history. A way of doing so is removing the .git folder and re-initializing the repository. Within the repository directory do rm -rf .git git init git remote add origin git@github.com:your_username/your_project_name.git git add . git commit -m \"Initial commit\" git push --set-upstream origin master Remember to change your_username and your_project_name to your real values.","title":"Removing the old commits"},{"location":"forking_this_wiki/#setting-up-github-pages","text":"To enable the Github Pages website associated with your repo, follow these steps: Create SSH Deploy Key . Activate the GitHub Pages repository configuration with the gh-pages branch. Now, the site will be built whenever you push new commits and periodically, according to the cron configuration from .github/workflows/gh-pages.yml .","title":"Setting up GitHub Pages"},{"location":"free_knowledge/","text":"One of the early principles of the internet has been to make knowledge free to everyone. Alexandra Elbakyan of Sci-Hub , bookwarrior of Library Genesis , Aaron Swartz , and countless unnamed others have fought to free science from the grips of for-profit publishers. Today, they do it working in hiding, alone, without acknowledgment, in fear of imprisonment, and even now wiretapped by the FBI. They sacrifice everything for one vision: Open Science. Some days ago, a post appeared on reddit to rescue Sci-Hub by increasing the seeders of the 850 scihub torrents . The plan is to follow the steps done last year to move Libgen to IPFS to make it more difficult for the states to bring down this marvelous collection. A good way to start is to look at the most ill torrents and fix their state. If you follow this path, take care of IP leaking, they're surely monitoring who's sharing. Another way to contribute is by following the guidelines of freeread.org and contribute to the IPFS free library . Beware though, the guidelines don't explain how to install IPFS behind a VPN or Tor. This could be contributed to the site. Something that is needed is a command line tool that reads the list of ill torrents , and downloads the torrents that have a low number of seeders and DHT peers. The number of torrents to download could be limited by the amount the user wants to share. A second version could have an interaction with the torrent client so that when a torrent is no longer ill, it's automatically replaced with one that is. References \u2691 FreeRead.org Libgen reddit Sci-Hub reddit DataHoarder reddit","title":"Free Knowledge"},{"location":"free_knowledge/#references","text":"FreeRead.org Libgen reddit Sci-Hub reddit DataHoarder reddit","title":"References"},{"location":"gadgetbridge/","text":"Gadgetbridge is an Android (4.4+) application which will allow you to use your Pebble, Mi Band, Amazfit Bip and HPlus device (and more) without the vendor's closed source application and without the need to create an account and transmit any of your data to the vendor's servers. It wont be ever be as good as the proprietary application, but it supports a good range of features , and supports a huge range of bands . Data extraction \u2691 You can use the Data export or Data Auto export to get copy of your data. Here is an example of a Python program that post processes the data. Also is this post explaining how to reverse engineer the miband2 with this or this scripts. If you start the path of reverse engineering the Bluetooth protocol look at gadgetbridge guidelines . If you start to think on how to avoid the connection with an android phone and directly extract or interact from a linux device through python, I'd go with pybluez for the bluetooth interface, understand the band code of Gadgetbridge porting the logic to the python module, and reverse engineering the call you want to process. There isn't much in the internet following this approach, I've found an implementation for the Mi Band 4 though , which can be a good start. Heartrate measurement \u2691 Follow the official instructions . Sleep \u2691 It looks that they don't yet support smart alarms . Weather \u2691 Follow the official instructions Events \u2691 I haven't figured out yet how to let the events show in the \"events\" tab. Mobile calendar events show up as notifications, but you can't see the list of the next ones. For the Amazfit band 5, there is a bug that prevents events from showing in the reminders tab . Notifications work well though. Setup \u2691 In the case of the Amazfit band 5 , we need to use the Huami server pairing : Install the Zepp application Create an account through the application. Pair your band and wait for the firmware update Use the python script to extract the credentials git clone https://github.com/argrento/huami-token.git pip install -r requirements.txt Run script with your credentials: python huami_token.py --method amazfit --email youemail@example.com --password your_password --bt_keys Do not unpair the band/watch from MiFit/Amazfit/Zepp app Kill or uninstall the MiFit/Amazfit/Zepp app Ensure GPS/location services are enabled The official instructions tell you to unpair the band/watch from your phone's bluetooth but I didn't have to do it. Add the band in gadgetbridge. Under Auth key add your key. References \u2691 Git Home Issue tracker Blog , although the RSS is not working .","title":"GadgetBridge"},{"location":"gadgetbridge/#data-extraction","text":"You can use the Data export or Data Auto export to get copy of your data. Here is an example of a Python program that post processes the data. Also is this post explaining how to reverse engineer the miband2 with this or this scripts. If you start the path of reverse engineering the Bluetooth protocol look at gadgetbridge guidelines . If you start to think on how to avoid the connection with an android phone and directly extract or interact from a linux device through python, I'd go with pybluez for the bluetooth interface, understand the band code of Gadgetbridge porting the logic to the python module, and reverse engineering the call you want to process. There isn't much in the internet following this approach, I've found an implementation for the Mi Band 4 though , which can be a good start.","title":"Data extraction"},{"location":"gadgetbridge/#heartrate-measurement","text":"Follow the official instructions .","title":"Heartrate measurement"},{"location":"gadgetbridge/#sleep","text":"It looks that they don't yet support smart alarms .","title":"Sleep"},{"location":"gadgetbridge/#weather","text":"Follow the official instructions","title":"Weather"},{"location":"gadgetbridge/#events","text":"I haven't figured out yet how to let the events show in the \"events\" tab. Mobile calendar events show up as notifications, but you can't see the list of the next ones. For the Amazfit band 5, there is a bug that prevents events from showing in the reminders tab . Notifications work well though.","title":"Events"},{"location":"gadgetbridge/#setup","text":"In the case of the Amazfit band 5 , we need to use the Huami server pairing : Install the Zepp application Create an account through the application. Pair your band and wait for the firmware update Use the python script to extract the credentials git clone https://github.com/argrento/huami-token.git pip install -r requirements.txt Run script with your credentials: python huami_token.py --method amazfit --email youemail@example.com --password your_password --bt_keys Do not unpair the band/watch from MiFit/Amazfit/Zepp app Kill or uninstall the MiFit/Amazfit/Zepp app Ensure GPS/location services are enabled The official instructions tell you to unpair the band/watch from your phone's bluetooth but I didn't have to do it. Add the band in gadgetbridge. Under Auth key add your key.","title":"Setup"},{"location":"gadgetbridge/#references","text":"Git Home Issue tracker Blog , although the RSS is not working .","title":"References"},{"location":"gajim/","text":"Gajim is the best Linux XMPP client in terms of end-to-end encryption support as it's able to speak OMEMO. Installation \u2691 sudo apt-get install gajim gajim-omemo Once you open it, you need to enable the plugin in the main program dropdown. The only problem I've encountered so far is that OMEMO is not enabled by default , they made a PR but closed it because it encountered some errors that was not able to solve . It's a crucial feature, so if you have some spare time and know a bit of Python please try to fix it! Developing \u2691 I've found the Developing section in the wiki to get started. Issues \u2691 Enable encryption by default : Nothing to do. References \u2691 Homepage","title":"Gajim"},{"location":"gajim/#installation","text":"sudo apt-get install gajim gajim-omemo Once you open it, you need to enable the plugin in the main program dropdown. The only problem I've encountered so far is that OMEMO is not enabled by default , they made a PR but closed it because it encountered some errors that was not able to solve . It's a crucial feature, so if you have some spare time and know a bit of Python please try to fix it!","title":"Installation"},{"location":"gajim/#developing","text":"I've found the Developing section in the wiki to get started.","title":"Developing"},{"location":"gajim/#issues","text":"Enable encryption by default : Nothing to do.","title":"Issues"},{"location":"gajim/#references","text":"Homepage","title":"References"},{"location":"helm_git/","text":"helm-git is a helm downloader plugin that provides GIT protocol support. This fits the following use cases: Need to keep charts private. Doesn't want to package charts before installing. Charts in a sub-path, or with another ref than master. Pull values files directly from (private) Git repository. Installation \u2691 helm plugin install https://github.com/aslafy-z/helm-git --version 0 .8.0 Until this issue is solved, you need to stick to the 0.8.0 version. Usage \u2691 helm-git will package any chart that is not so you can directly reference paths to original charts. Here's the Git urls format, followed by examples: git+https://[provider.com]/[user]/[repo]@[path/to/charts][?[ref=git-ref][&sparse=0][&depupdate=0]] git+ssh://git@[provider.com]/[user]/[repo]@[path/to/charts][?[ref=git-ref][&sparse=0][&depupdate=0]] git+file://[path/to/repo]@[path/to/charts][?[ref=git-ref][&sparse=0][&depupdate=0]] git+https://github.com/jetstack/cert-manager@deploy/charts?ref=v0.6.2&sparse=0 git+ssh://git@github.com/jetstack/cert-manager@deploy/charts?ref=v0.6.2&sparse=1 git+ssh://git@github.com/jetstack/cert-manager@deploy/charts?ref=v0.6.2 git+https://github.com/istio/istio@install/kubernetes/helm?ref=1.5.4&sparse=0&depupdate=0 Add your repository: helm repo add cert-manager git+https://github.com/jetstack/cert-manager@deploy/charts?ref = v0.6.2 You can use it as any other Helm chart repository. Try: $ helm search cert-manager NAME CHART VERSION APP VERSION DESCRIPTION cert-manager/cert-manager v0.6.6 v0.6.2 A Helm chart for cert-manager $ helm install cert-manager/cert-manager --version \"0.6.6\" Fetching also works: helm fetch cert-manager/cert-manager --version \"0.6.6\" helm fetch git+https://github.com/jetstack/cert-manager@deploy/charts/cert-manager-v0.6.2.tgz?ref = v0.6.2 Issues \u2691 bug: regression: unable to fetch with empty path : update the installation docs, update the plugin version and test if it works. References \u2691 Git","title":"Helm Git"},{"location":"helm_git/#installation","text":"helm plugin install https://github.com/aslafy-z/helm-git --version 0 .8.0 Until this issue is solved, you need to stick to the 0.8.0 version.","title":"Installation"},{"location":"helm_git/#usage","text":"helm-git will package any chart that is not so you can directly reference paths to original charts. Here's the Git urls format, followed by examples: git+https://[provider.com]/[user]/[repo]@[path/to/charts][?[ref=git-ref][&sparse=0][&depupdate=0]] git+ssh://git@[provider.com]/[user]/[repo]@[path/to/charts][?[ref=git-ref][&sparse=0][&depupdate=0]] git+file://[path/to/repo]@[path/to/charts][?[ref=git-ref][&sparse=0][&depupdate=0]] git+https://github.com/jetstack/cert-manager@deploy/charts?ref=v0.6.2&sparse=0 git+ssh://git@github.com/jetstack/cert-manager@deploy/charts?ref=v0.6.2&sparse=1 git+ssh://git@github.com/jetstack/cert-manager@deploy/charts?ref=v0.6.2 git+https://github.com/istio/istio@install/kubernetes/helm?ref=1.5.4&sparse=0&depupdate=0 Add your repository: helm repo add cert-manager git+https://github.com/jetstack/cert-manager@deploy/charts?ref = v0.6.2 You can use it as any other Helm chart repository. Try: $ helm search cert-manager NAME CHART VERSION APP VERSION DESCRIPTION cert-manager/cert-manager v0.6.6 v0.6.2 A Helm chart for cert-manager $ helm install cert-manager/cert-manager --version \"0.6.6\" Fetching also works: helm fetch cert-manager/cert-manager --version \"0.6.6\" helm fetch git+https://github.com/jetstack/cert-manager@deploy/charts/cert-manager-v0.6.2.tgz?ref = v0.6.2","title":"Usage"},{"location":"helm_git/#issues","text":"bug: regression: unable to fetch with empty path : update the installation docs, update the plugin version and test if it works.","title":"Issues"},{"location":"helm_git/#references","text":"Git","title":"References"},{"location":"husboard/","text":"Hushboard is an utility that mutes your microphone while you\u2019re typing. Installation \u2691 They recommend using the Snap Store package but you can also install it manually as follows: sudo apt install libgirepository1.0-dev libcairo2-dev mkvirtualenv hushboard git clone https://github.com/stuartlangridge/hushboard cd hushboard pip install pycairo PyGObject six xlib pip install . deactivate Running the application \u2691 You can run it manually as follows workon hushboard python -m hushboard deactivate Or if you use i3wm, create the following script. #!/usr/bin/env bash source { WORKON_PATH } /hushboard/bin/activate python -m hushboard deactivate You should replace {WORKON_PATH} with your virtual environments path. Then add this line to your i3wm configuration file to start it automatically. exec --no-startup-id ~/scripts/hushboard.sh Reference \u2691 M0wer Husboard article","title":"Hushboard"},{"location":"husboard/#installation","text":"They recommend using the Snap Store package but you can also install it manually as follows: sudo apt install libgirepository1.0-dev libcairo2-dev mkvirtualenv hushboard git clone https://github.com/stuartlangridge/hushboard cd hushboard pip install pycairo PyGObject six xlib pip install . deactivate","title":"Installation"},{"location":"husboard/#running-the-application","text":"You can run it manually as follows workon hushboard python -m hushboard deactivate Or if you use i3wm, create the following script. #!/usr/bin/env bash source { WORKON_PATH } /hushboard/bin/activate python -m hushboard deactivate You should replace {WORKON_PATH} with your virtual environments path. Then add this line to your i3wm configuration file to start it automatically. exec --no-startup-id ~/scripts/hushboard.sh","title":"Running the application"},{"location":"husboard/#reference","text":"M0wer Husboard article","title":"Reference"},{"location":"instant_messages_management/","text":"Instant messaging in all it's forms is becoming the main communication channel. As any other input system, if not used wisely, it can be a sink of productivity. Analyze how often you need to check it \u2691 Follow the interruption analysis to discover how often you need to check it and if you need the notifications or fine grain them to the sources that have higher priority. Once you've decided the frequency, try to respect it!. If you want an example, check my work or personal analysis. Workflow \u2691 I interact with messaging applications in two ways: To read the new items and answer questions. To start a conversation. The passively reading for new items works perfectly with the interruption management processes. Each time you decide to check for new messages, follow the inbox processing guidelines to extract the information to the appropriate system (task manager, calendar or knowledge manager). If you answer someone or if you start a new conversation, assume that any work done in the next 5 to 10 minutes will probably be interrupted, so choose small or mindless tasks. If the person doesn't answer in that time, start a new pomodoro and go back when the next interruption event comes. Use calls for non short conversations \u2691 Chats are good for short conversations that don't require long or quick responses. Even though people may have forgotten it, they are an asynchronous communication channel. They're not suited for long conversations though as: Typing on a keyboard (or a mobile \u1559(\u21c0\u2038\u21bc\u2036)\u1557 ) is slower than talking directly. It's difficult to transmit the conversation tone by message, and each reader can interpret it differently, leading to misunderstandings. If the conversation topic is complex, graphical aids such as screen sharing or doodling can make the conversation more efficient. Unless everyone involved is fully focused on the conversation, the delays between messages can be high, and all that time, the attendees need to manage the interruptions. If you fully focus on the conversation, you're loosing your time while you wait for the other to answer. For all these reasons, whenever a conversation looks not to be short or trivial, arrange a quick call or video call. At work or collectives, use group rooms over direct messages \u2691 Asking for help through direct messages should be avoided whenever possible, instead of interrupting one person, it's better to ask in the group rooms because: More people are reading, so you'll probably get answered sooner. Knowledge is spread throughout the group instead of isolated on specific people. Even if I don't answer a question, I read what others have said thus learning in the process. The responsibility of answering is shared between the group members, making it easier to define the interruptions role . Use threads or replies if the client allows it \u2691 Threads are a feature that allows people to have parallel conversations in the same room in a way that the messages aren't mixed. This makes it easier to maintain the focus and follow past messages. It also allows users that are not interested, to silence the thread, so they won't get application or/and desktop notifications on that particular topic. Replies can be used when the conversation is not lengthy enough to open a thread. They give the benefit of giving context to the user you're replying to. Use chats to transport information, not to store it \u2691 Chat applications were envisioned as a protocol for person A to send information to person B. The fact that the message providers allow users to have almost no limit on their message history has driven people to use them as a knowledge repository. This approach has many problems: As most people don't use end to end encryption (OMEMO/OTR/Signal), the data of their messages is available for the service provider to read. This is a privacy violation that should be avoided. Most providers don't allow you to set a message limit, so you'd have to delete them manually. Searching information in the chats is a nightmare. There are more efficient knowledge repositories to store your information. Use key bindings \u2691 Using the mouse to interact with the chat client graphical interfaces is not efficient, try to learn the key bindings and use them as much as possible. Environment setup \u2691 Account management \u2691 It's common to have more than one account or application to check. There are many instant messaging solutions, such as XMPP, Signal, IRC, Telegram, Slack, Whatssap or Facebook. It would be ideal to have a client that could act as a bridge to all the solutions, but at least I don't know it, so you're forced to install the different applications to interact with them. The obvious suggestion would be to reduce the number of platforms in use, but we all know that it's asking too much as it will probably isolate you from specific people. Once you have the minimum clients chosen, put them all on the same workspace, for example an i3 window manager workspace, and only check them following the workflow rules. Isolate your work and personal environments \u2691 Make sure that you set your environment so that you can't check your personal chats when you're working and the other way around. For example, you could configure different instances of the chat clients and only open the ones that you need to. Or you could avoid configuring the work clients in your personal phone. For example, at work, I have my own account and another for each team I'm part of, the last ones are managed by all the team members. On the personal level, I've got many accounts for the different OpSec profiles or identities. For efficiency reasons, you need to be able to check all of them on one place. You can use an email manager such as Thunderbird . Once you choose one, try to master it. Fine grain configure the notifications \u2691 Modern client applications allow you to define the notifications at room or people level. I usually: Use notifications on all messages on high priority channels. For example the infrastructure monitorization one. Agree with your team to write as less as possible. Use notifications when mentioned on group rooms: Don't get notified on any message unless they add your name on it. Use notifications on direct messages: Decide which people are important enough to activate the notifications. Sometimes the client applications don't give enough granularity, or you would like to show notifications based on more complex conditions, that's why I created the seed project to improve the notification management in Linux .","title":"Instant Messages Management"},{"location":"instant_messages_management/#analyze-how-often-you-need-to-check-it","text":"Follow the interruption analysis to discover how often you need to check it and if you need the notifications or fine grain them to the sources that have higher priority. Once you've decided the frequency, try to respect it!. If you want an example, check my work or personal analysis.","title":"Analyze how often you need to check it"},{"location":"instant_messages_management/#workflow","text":"I interact with messaging applications in two ways: To read the new items and answer questions. To start a conversation. The passively reading for new items works perfectly with the interruption management processes. Each time you decide to check for new messages, follow the inbox processing guidelines to extract the information to the appropriate system (task manager, calendar or knowledge manager). If you answer someone or if you start a new conversation, assume that any work done in the next 5 to 10 minutes will probably be interrupted, so choose small or mindless tasks. If the person doesn't answer in that time, start a new pomodoro and go back when the next interruption event comes.","title":"Workflow"},{"location":"instant_messages_management/#use-calls-for-non-short-conversations","text":"Chats are good for short conversations that don't require long or quick responses. Even though people may have forgotten it, they are an asynchronous communication channel. They're not suited for long conversations though as: Typing on a keyboard (or a mobile \u1559(\u21c0\u2038\u21bc\u2036)\u1557 ) is slower than talking directly. It's difficult to transmit the conversation tone by message, and each reader can interpret it differently, leading to misunderstandings. If the conversation topic is complex, graphical aids such as screen sharing or doodling can make the conversation more efficient. Unless everyone involved is fully focused on the conversation, the delays between messages can be high, and all that time, the attendees need to manage the interruptions. If you fully focus on the conversation, you're loosing your time while you wait for the other to answer. For all these reasons, whenever a conversation looks not to be short or trivial, arrange a quick call or video call.","title":"Use calls for non short conversations"},{"location":"instant_messages_management/#at-work-or-collectives-use-group-rooms-over-direct-messages","text":"Asking for help through direct messages should be avoided whenever possible, instead of interrupting one person, it's better to ask in the group rooms because: More people are reading, so you'll probably get answered sooner. Knowledge is spread throughout the group instead of isolated on specific people. Even if I don't answer a question, I read what others have said thus learning in the process. The responsibility of answering is shared between the group members, making it easier to define the interruptions role .","title":"At work or collectives, use group rooms over direct messages"},{"location":"instant_messages_management/#use-threads-or-replies-if-the-client-allows-it","text":"Threads are a feature that allows people to have parallel conversations in the same room in a way that the messages aren't mixed. This makes it easier to maintain the focus and follow past messages. It also allows users that are not interested, to silence the thread, so they won't get application or/and desktop notifications on that particular topic. Replies can be used when the conversation is not lengthy enough to open a thread. They give the benefit of giving context to the user you're replying to.","title":"Use threads or replies if the client allows it"},{"location":"instant_messages_management/#use-chats-to-transport-information-not-to-store-it","text":"Chat applications were envisioned as a protocol for person A to send information to person B. The fact that the message providers allow users to have almost no limit on their message history has driven people to use them as a knowledge repository. This approach has many problems: As most people don't use end to end encryption (OMEMO/OTR/Signal), the data of their messages is available for the service provider to read. This is a privacy violation that should be avoided. Most providers don't allow you to set a message limit, so you'd have to delete them manually. Searching information in the chats is a nightmare. There are more efficient knowledge repositories to store your information.","title":"Use chats to transport information, not to store it"},{"location":"instant_messages_management/#use-key-bindings","text":"Using the mouse to interact with the chat client graphical interfaces is not efficient, try to learn the key bindings and use them as much as possible.","title":"Use key bindings"},{"location":"instant_messages_management/#environment-setup","text":"","title":"Environment setup"},{"location":"instant_messages_management/#account-management","text":"It's common to have more than one account or application to check. There are many instant messaging solutions, such as XMPP, Signal, IRC, Telegram, Slack, Whatssap or Facebook. It would be ideal to have a client that could act as a bridge to all the solutions, but at least I don't know it, so you're forced to install the different applications to interact with them. The obvious suggestion would be to reduce the number of platforms in use, but we all know that it's asking too much as it will probably isolate you from specific people. Once you have the minimum clients chosen, put them all on the same workspace, for example an i3 window manager workspace, and only check them following the workflow rules.","title":"Account management"},{"location":"instant_messages_management/#isolate-your-work-and-personal-environments","text":"Make sure that you set your environment so that you can't check your personal chats when you're working and the other way around. For example, you could configure different instances of the chat clients and only open the ones that you need to. Or you could avoid configuring the work clients in your personal phone. For example, at work, I have my own account and another for each team I'm part of, the last ones are managed by all the team members. On the personal level, I've got many accounts for the different OpSec profiles or identities. For efficiency reasons, you need to be able to check all of them on one place. You can use an email manager such as Thunderbird . Once you choose one, try to master it.","title":"Isolate your work and personal environments"},{"location":"instant_messages_management/#fine-grain-configure-the-notifications","text":"Modern client applications allow you to define the notifications at room or people level. I usually: Use notifications on all messages on high priority channels. For example the infrastructure monitorization one. Agree with your team to write as less as possible. Use notifications when mentioned on group rooms: Don't get notified on any message unless they add your name on it. Use notifications on direct messages: Decide which people are important enough to activate the notifications. Sometimes the client applications don't give enough granularity, or you would like to show notifications based on more complex conditions, that's why I created the seed project to improve the notification management in Linux .","title":"Fine grain configure the notifications"},{"location":"interruption_management/","text":"Interruption management is the life management area that gathers the processes to minimize the time and willpower toll consumed by interruptions. We've come to accept that we need to be available 24/7 and answer immediately, that makes us slaves of the interruptions, it drives our work and personal relations. I feel that out of respect of ourselves and the other's time, we need to change that perspective. Most of the times interruptions can wait 20 or 60 minutes, and many of them can be avoided with better task and time planning. Interruptions are one of the main productivity killers. Not only they unexpectedly break your workflow, they also add undesired mental load as you are waiting for them to happen, and need to check them often. As we've seen previously, to be productive you need to be able to work on a task for 20 minutes without checking the interruption channels. Interruption analysis \u2691 The interruption analysis is the main input to do interruption management. With it you consider what are the sources of the interruptions, and for each of them you classify the different source events in categories evaluating: How many interruption events does the source or category create. How many of the events require an action, and if it can be automated. How many hold information that don't need any action, and what do you want to do with that information. How many could be automatically filtered out. What priority do the events have, and if it's the same for all events. How long can the associated action be delayed. Once you have that list, think if you can reduce it. Can you merge or directly remove one of the sources? The less channels to check, the better. Then think which of them you have no control over and think of ways to regain it. If you decide when to address the interruptions, your mind will have less load and will perform better when you're actually working. The ultimate goal of the analysis is to safely define the maximum amount of time you can spend without looking at the channels. Checking them continuously makes no sense, you're breaking your workflow for no good reason, as most times there is nothing new, and if there is, you feel the urge to act upon them, even though they could wait. In some teams, the situation doesn't allow you not to check them frequently. In those cases you can define the interruption manager role. A figure that is rotated by the team's members so that only one human needs to be monitoring the interruption channels, while the rest of them are able to work continuously on their tasks. If you want to see the analysis in action, check my work analysis or my personal one . Workflow \u2691 Once you have all the interruption sources identified, classified, and defined the checking periodicity, you need to decide how to handle them. Define your interruption events \u2691 To minimize the times you interrupt your workflow, aggregate the different sources and schedule when are you want to check them. For example, if the analysis gave the next sources: Source A: check each 4 hours. Source B: check each 5 hours. Source C: check each 20 minutes. You can schedule the next interruption events: Check sources A, B and C: when you start working, before lunch and before the end of the day. Check C: after each Pomodoro iteration . Process the interruption event information \u2691 When an interruption event arrives, process sequentially each source following the inbox emptying guidelines .","title":"Interruption Management"},{"location":"interruption_management/#interruption-analysis","text":"The interruption analysis is the main input to do interruption management. With it you consider what are the sources of the interruptions, and for each of them you classify the different source events in categories evaluating: How many interruption events does the source or category create. How many of the events require an action, and if it can be automated. How many hold information that don't need any action, and what do you want to do with that information. How many could be automatically filtered out. What priority do the events have, and if it's the same for all events. How long can the associated action be delayed. Once you have that list, think if you can reduce it. Can you merge or directly remove one of the sources? The less channels to check, the better. Then think which of them you have no control over and think of ways to regain it. If you decide when to address the interruptions, your mind will have less load and will perform better when you're actually working. The ultimate goal of the analysis is to safely define the maximum amount of time you can spend without looking at the channels. Checking them continuously makes no sense, you're breaking your workflow for no good reason, as most times there is nothing new, and if there is, you feel the urge to act upon them, even though they could wait. In some teams, the situation doesn't allow you not to check them frequently. In those cases you can define the interruption manager role. A figure that is rotated by the team's members so that only one human needs to be monitoring the interruption channels, while the rest of them are able to work continuously on their tasks. If you want to see the analysis in action, check my work analysis or my personal one .","title":"Interruption analysis"},{"location":"interruption_management/#workflow","text":"Once you have all the interruption sources identified, classified, and defined the checking periodicity, you need to decide how to handle them.","title":"Workflow"},{"location":"interruption_management/#define-your-interruption-events","text":"To minimize the times you interrupt your workflow, aggregate the different sources and schedule when are you want to check them. For example, if the analysis gave the next sources: Source A: check each 4 hours. Source B: check each 5 hours. Source C: check each 20 minutes. You can schedule the next interruption events: Check sources A, B and C: when you start working, before lunch and before the end of the day. Check C: after each Pomodoro iteration .","title":"Define your interruption events"},{"location":"interruption_management/#process-the-interruption-event-information","text":"When an interruption event arrives, process sequentially each source following the inbox emptying guidelines .","title":"Process the interruption event information"},{"location":"issues/","text":"I haven't found a tool to monitor the context it made me track certain software issues, so I get lost when updates come. Until a tool shows up, I'll use the good old markdown to keep track. Pydantic errors \u2691 No name 'BaseModel' in module 'pydantic' (no-name-in-module) , you can find a patch in the pydantic article , the pydantic developers took that as a solution as it lays in pylint's roof , once that last issue is solved try to find a better way to improve the patch solution. Vim workflow improvements \u2691 Manually formatting paragraphs is an unproductive pain in the ass, Vim-pencil looks promising but there are still some usability issues that need to be fixed first: Wrong list management: #93 linked to #31 and #95 . Disable wrap of document headers (less important). Gitea improvements \u2691 Replying discussion comments redirects to mail pull request page : Notify the people that it's fixed. Gitea Kanban board improvements \u2691 Remove the Default issue template: #14383 . When it's solved apply it in the work's issue tracker. Docker monitorization \u2691 Integrate diun in the CI pipelines when they support prometheus metrics . Update the docker article too. Gadgetbridge improvements \u2691 Smart alarm support : Use it whenever it's available. GET Sp02 real time data, or at least export it : See how to use this data once it's available. export heart rate for activities without a GPX track : See if I can export the heart rate for post processing. Maybe it's covered here . Add UI and logic for more complex database import, export and merging : Monitor to see if there are new ways or improvements of exporting data. Blog's RSS is not working : Add it to the feed reader once it does, and remove the warning from the gadgetbridge article Integrate with home assistant : Check if the integration with kalliope is easy. Issues with zoom, swipe, interact with graphs : enable back disable swipe between tabs in the chart settings. PAI implementation : Check it once it's ready. Calendar synchronization issue , could be related with notifications work after restart : try it when it's solved Change snooze time span : Change the timespan from 10 to 5 minutes. Ombi improvements \u2691 Ebook requests : Configure it in the service, notify the people and start using it. Add working links to the details pages : nothing to do, just start using it. Allow search by genre : Notify the people and start using it.","title":"Issues"},{"location":"issues/#pydantic-errors","text":"No name 'BaseModel' in module 'pydantic' (no-name-in-module) , you can find a patch in the pydantic article , the pydantic developers took that as a solution as it lays in pylint's roof , once that last issue is solved try to find a better way to improve the patch solution.","title":"Pydantic errors"},{"location":"issues/#vim-workflow-improvements","text":"Manually formatting paragraphs is an unproductive pain in the ass, Vim-pencil looks promising but there are still some usability issues that need to be fixed first: Wrong list management: #93 linked to #31 and #95 . Disable wrap of document headers (less important).","title":"Vim workflow improvements"},{"location":"issues/#gitea-improvements","text":"Replying discussion comments redirects to mail pull request page : Notify the people that it's fixed.","title":"Gitea improvements"},{"location":"issues/#gitea-kanban-board-improvements","text":"Remove the Default issue template: #14383 . When it's solved apply it in the work's issue tracker.","title":"Gitea Kanban board improvements"},{"location":"issues/#docker-monitorization","text":"Integrate diun in the CI pipelines when they support prometheus metrics . Update the docker article too.","title":"Docker monitorization"},{"location":"issues/#gadgetbridge-improvements","text":"Smart alarm support : Use it whenever it's available. GET Sp02 real time data, or at least export it : See how to use this data once it's available. export heart rate for activities without a GPX track : See if I can export the heart rate for post processing. Maybe it's covered here . Add UI and logic for more complex database import, export and merging : Monitor to see if there are new ways or improvements of exporting data. Blog's RSS is not working : Add it to the feed reader once it does, and remove the warning from the gadgetbridge article Integrate with home assistant : Check if the integration with kalliope is easy. Issues with zoom, swipe, interact with graphs : enable back disable swipe between tabs in the chart settings. PAI implementation : Check it once it's ready. Calendar synchronization issue , could be related with notifications work after restart : try it when it's solved Change snooze time span : Change the timespan from 10 to 5 minutes.","title":"Gadgetbridge improvements"},{"location":"issues/#ombi-improvements","text":"Ebook requests : Configure it in the service, notify the people and start using it. Add working links to the details pages : nothing to do, just start using it. Allow search by genre : Notify the people and start using it.","title":"Ombi improvements"},{"location":"jellyfin/","text":"Jellyfin is a Free Software Media System that puts you in control of managing and streaming your media. It is an alternative to the proprietary Emby and Plex, to provide media from a dedicated server to end-user devices via multiple apps. Jellyfin is descended from Emby's 3.5.2 release and ported to the .NET Core framework to enable full cross-platform support. There are no strings attached, no premium licenses or features, and no hidden agendas: just a team who want to build something better and work together to achieve it. Troubleshooting \u2691 Wrong image covers \u2691 Remove all the jpg files of the directory and then fetch again the data from your favourite media management software. Green bars in the reproduction \u2691 It's related to some hardware transcoding issue related to some video codecs, the solution is to either get a file with other codec, or convert it yourself without the hardware transcoding with: ffmpeg -i input.avi -c:v libx264 out.mp4 Stuck at login page \u2691 Sometimes Jellyfin gets stuck at the login screen when trying to log in with an endlessly spinning loading wheel. It looks like it's already fixed, so first try to update to the latest version. If the error remains, follow the next steps: To fix it run the next snippet: systemctl stop jellyfin.service mv /var/lib/jellyfin/data/jellyfin.db { ,.bak } systemctl start jellyfin.service # Go to JF URL, get asked to log in even though # there are no Users in the JF DB now systemctl stop jellyfin.service mv /var/lib/jellyfin/data/jellyfin.db { .bak, } systemctl start jellyfin.service If you use jfa-go for the invites, you may need to regenerate all the user profiles , so that the problem is not introduced again. Issues \u2691 Trailers not working : No solution until it's fixed Intel Hardware transcoding broken : Until fixed run each time you restart: docker exec -it jellyfin /bin/bash wget https://repo.jellyfin.org/releases/server/ubuntu/versions/jellyfin-ffmpeg/4.3.2-1/jellyfin-ffmpeg_4.3.2-1-focal_amd64.deb dpkg -i jellyfin-ffmpeg_4.3.2-1-focal_amd64.deb Once fixed tweak the init container script in /config/custom-cont-init.d . * Unnecessary transcoding : nothing to do * Local social features : test it and see how to share rating between users. * Skip intro/outro/credits : try it. * Music star rating : try it and plan to migrate everything to Jellyfin. * Remove pagination/use lazy loading : try it. * Support 2FA : try it. * Mysql server backend : implement it to add robustness. * Watched history : try it. * A richer ePub reader : migrate from Polar and add jellyfin to the awesome selfhosted list. * Prometheus exporter : monitor it. * Easy Import/Export Jellyfin settings : add to the backup process. * Temporary direct file sharing links : try it. * Remember subtitle and audio track choice between episodes : try it. * IMBD Rating and Rotten Tomatoes Audiance Rating and Fresh rating on Movies and TV Shows : try the new ratings. * Trailers Plugin : Once it's merged to the core, remove the plugin. * Jellyfin for apple tv : tell the people that use the shitty device. References \u2691 Home Git Blog ( RSS )","title":"Jellyfin"},{"location":"jellyfin/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"jellyfin/#wrong-image-covers","text":"Remove all the jpg files of the directory and then fetch again the data from your favourite media management software.","title":"Wrong image covers"},{"location":"jellyfin/#green-bars-in-the-reproduction","text":"It's related to some hardware transcoding issue related to some video codecs, the solution is to either get a file with other codec, or convert it yourself without the hardware transcoding with: ffmpeg -i input.avi -c:v libx264 out.mp4","title":"Green bars in the reproduction"},{"location":"jellyfin/#stuck-at-login-page","text":"Sometimes Jellyfin gets stuck at the login screen when trying to log in with an endlessly spinning loading wheel. It looks like it's already fixed, so first try to update to the latest version. If the error remains, follow the next steps: To fix it run the next snippet: systemctl stop jellyfin.service mv /var/lib/jellyfin/data/jellyfin.db { ,.bak } systemctl start jellyfin.service # Go to JF URL, get asked to log in even though # there are no Users in the JF DB now systemctl stop jellyfin.service mv /var/lib/jellyfin/data/jellyfin.db { .bak, } systemctl start jellyfin.service If you use jfa-go for the invites, you may need to regenerate all the user profiles , so that the problem is not introduced again.","title":"Stuck at login page"},{"location":"jellyfin/#issues","text":"Trailers not working : No solution until it's fixed Intel Hardware transcoding broken : Until fixed run each time you restart: docker exec -it jellyfin /bin/bash wget https://repo.jellyfin.org/releases/server/ubuntu/versions/jellyfin-ffmpeg/4.3.2-1/jellyfin-ffmpeg_4.3.2-1-focal_amd64.deb dpkg -i jellyfin-ffmpeg_4.3.2-1-focal_amd64.deb Once fixed tweak the init container script in /config/custom-cont-init.d . * Unnecessary transcoding : nothing to do * Local social features : test it and see how to share rating between users. * Skip intro/outro/credits : try it. * Music star rating : try it and plan to migrate everything to Jellyfin. * Remove pagination/use lazy loading : try it. * Support 2FA : try it. * Mysql server backend : implement it to add robustness. * Watched history : try it. * A richer ePub reader : migrate from Polar and add jellyfin to the awesome selfhosted list. * Prometheus exporter : monitor it. * Easy Import/Export Jellyfin settings : add to the backup process. * Temporary direct file sharing links : try it. * Remember subtitle and audio track choice between episodes : try it. * IMBD Rating and Rotten Tomatoes Audiance Rating and Fresh rating on Movies and TV Shows : try the new ratings. * Trailers Plugin : Once it's merged to the core, remove the plugin. * Jellyfin for apple tv : tell the people that use the shitty device.","title":"Issues"},{"location":"jellyfin/#references","text":"Home Git Blog ( RSS )","title":"References"},{"location":"kitty/","text":"kitty is a fast, feature-rich, GPU based terminal emulator written in C and Python with nice features for the keyboard driven humans like me. Installation \u2691 Although it's in the official repos, the version of Debian is quite old, instead you can install it for the current user with: curl -L https://sw.kovidgoyal.net/kitty/installer.sh | sh /dev/stdin You'll need to add the next alias too to your .zshrc or .bashrc alias kitty = \"~/.local/kitty.app/bin/kitty\" Configuration \u2691 It's configuration is a simple, human editable, single file for easy reproducibility stored at ~/.config/kitty/kitty.conf Print images in the terminal \u2691 Create an alias in your .zshrc : alias icat = \"kitty +kitten icat\" Colors \u2691 The themes kitten allows you to easily change color themes, from a collection of almost two hundred pre-built themes available at kitty-themes. To use it run: kitty +kitten themes The kitten allows you to pick a theme, with live previews of the colors. You can choose between light and dark themes and search by theme name by just typing a few characters from the name. If you want to tweak some colors once you select a theme, you can use terminal sexy . Make the background transparent \u2691 File: ~/.config/kitty/kitty.conf background_opacity 0.85 A number between 0 and 1, where 1 is opaque and 0 is fully transparent. This will only work if supported by the OS (for instance, when using a compositor under X11). If you're using i3wm you need to configure compton Install it with sudo apt-get install compton , and configure i3 to start it in the background adding exec --no-startup-id compton to your i3 config. Terminal bell \u2691 I hate the auditive terminal bell, disable it with: enable_audio_bell no Movement \u2691 By default the movement is not vim friendly because if you use the same keystrokes, they will be captured by kitty and not forwarded to the application. The closest I got is: # Movement map ctrl+shift+k scroll_line_up map ctrl+shift+j scroll_line_down map ctrl+shift+u scroll_page_up map ctrl+shift+d scroll_page_down If you need more fine grained movement, use the scrollback buffer . The scrollback buffer \u2691 kitty supports scrolling back to view history, just like most terminals. You can use either keyboard shortcuts or the mouse scroll wheel to do so. However, kitty has an extra, neat feature. Sometimes you need to explore the scrollback buffer in more detail, maybe search for some text or refer to it side-by-side while typing in a follow-up command. kitty allows you to do this by pressing the ctrl+shift+h key-combination, which will open the scrollback buffer in your favorite pager program (which is less by default). Colors and text formatting are preserved. You can explore the scrollback buffer comfortably within the pager. To use nvim as the pager follow this discussion , the latest working snippet was: # Scrollback buffer # https://sw.kovidgoyal.net/kitty/overview/#the-scrollback-buffer # `bash -c '...'` Run everything in a shell taking the scrollback content on stdin # `-u NORC` Load plugins but not initialization files # `-c \"map q :qa!<CR>\"` Close with `q` key # `-c \"autocmd TermOpen * normal G\"` On opening of the embedded terminal go to last line # `-c \"terminal cat /proc/$$/fd/0 -\"` Open the embedded terminal and read stdin of the shell # `-c \"set clipboard+=unnamedplus\"` Always use clipboard to yank/put instead of having to specify + scrollback_pager bash -c 'nvim </dev/null -u NORC -c \"map q :qa!<CR>\" -c \"autocmd TermOpen * normal G\" -c \"terminal cat /proc/$$/fd/0 -\" -c \"set clipboard+=unnamedplus\" -c \"call cursor(CURSOR_LINE, CURSOR_COLUMN)\"' To make the history scrollback infinite add the next lines: scrollback_lines -1 scrollback_pager_history_size 0 Clipboard management \u2691 # Clipboard map ctrl+v paste_from_clipboard Troubleshooting \u2691 Scrollback when ssh into a machine doesn't work \u2691 This happens because the kitty terminfo files are not available on the server. You can ssh in using the following command which will automatically copy the terminfo files to the server: kitty +kitten ssh myserver This ssh kitten takes all the same command line arguments as ssh, you can alias it to ssh in your shell\u2019s rc files to avoid having to type it each time: alias ssh = \"kitty +kitten ssh\" Reasons to migrate from urxvt to kitty \u2691 It doesn't fuck up your terminal colors. You can use peek to record your screen. Easier to extend. References \u2691 Homepage","title":"Kitty"},{"location":"kitty/#installation","text":"Although it's in the official repos, the version of Debian is quite old, instead you can install it for the current user with: curl -L https://sw.kovidgoyal.net/kitty/installer.sh | sh /dev/stdin You'll need to add the next alias too to your .zshrc or .bashrc alias kitty = \"~/.local/kitty.app/bin/kitty\"","title":"Installation"},{"location":"kitty/#configuration","text":"It's configuration is a simple, human editable, single file for easy reproducibility stored at ~/.config/kitty/kitty.conf","title":"Configuration"},{"location":"kitty/#print-images-in-the-terminal","text":"Create an alias in your .zshrc : alias icat = \"kitty +kitten icat\"","title":"Print images in the terminal"},{"location":"kitty/#colors","text":"The themes kitten allows you to easily change color themes, from a collection of almost two hundred pre-built themes available at kitty-themes. To use it run: kitty +kitten themes The kitten allows you to pick a theme, with live previews of the colors. You can choose between light and dark themes and search by theme name by just typing a few characters from the name. If you want to tweak some colors once you select a theme, you can use terminal sexy .","title":"Colors"},{"location":"kitty/#make-the-background-transparent","text":"File: ~/.config/kitty/kitty.conf background_opacity 0.85 A number between 0 and 1, where 1 is opaque and 0 is fully transparent. This will only work if supported by the OS (for instance, when using a compositor under X11). If you're using i3wm you need to configure compton Install it with sudo apt-get install compton , and configure i3 to start it in the background adding exec --no-startup-id compton to your i3 config.","title":"Make the background transparent"},{"location":"kitty/#terminal-bell","text":"I hate the auditive terminal bell, disable it with: enable_audio_bell no","title":"Terminal bell"},{"location":"kitty/#movement","text":"By default the movement is not vim friendly because if you use the same keystrokes, they will be captured by kitty and not forwarded to the application. The closest I got is: # Movement map ctrl+shift+k scroll_line_up map ctrl+shift+j scroll_line_down map ctrl+shift+u scroll_page_up map ctrl+shift+d scroll_page_down If you need more fine grained movement, use the scrollback buffer .","title":"Movement"},{"location":"kitty/#the-scrollback-buffer","text":"kitty supports scrolling back to view history, just like most terminals. You can use either keyboard shortcuts or the mouse scroll wheel to do so. However, kitty has an extra, neat feature. Sometimes you need to explore the scrollback buffer in more detail, maybe search for some text or refer to it side-by-side while typing in a follow-up command. kitty allows you to do this by pressing the ctrl+shift+h key-combination, which will open the scrollback buffer in your favorite pager program (which is less by default). Colors and text formatting are preserved. You can explore the scrollback buffer comfortably within the pager. To use nvim as the pager follow this discussion , the latest working snippet was: # Scrollback buffer # https://sw.kovidgoyal.net/kitty/overview/#the-scrollback-buffer # `bash -c '...'` Run everything in a shell taking the scrollback content on stdin # `-u NORC` Load plugins but not initialization files # `-c \"map q :qa!<CR>\"` Close with `q` key # `-c \"autocmd TermOpen * normal G\"` On opening of the embedded terminal go to last line # `-c \"terminal cat /proc/$$/fd/0 -\"` Open the embedded terminal and read stdin of the shell # `-c \"set clipboard+=unnamedplus\"` Always use clipboard to yank/put instead of having to specify + scrollback_pager bash -c 'nvim </dev/null -u NORC -c \"map q :qa!<CR>\" -c \"autocmd TermOpen * normal G\" -c \"terminal cat /proc/$$/fd/0 -\" -c \"set clipboard+=unnamedplus\" -c \"call cursor(CURSOR_LINE, CURSOR_COLUMN)\"' To make the history scrollback infinite add the next lines: scrollback_lines -1 scrollback_pager_history_size 0","title":"The scrollback buffer"},{"location":"kitty/#clipboard-management","text":"# Clipboard map ctrl+v paste_from_clipboard","title":"Clipboard management"},{"location":"kitty/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"kitty/#scrollback-when-ssh-into-a-machine-doesnt-work","text":"This happens because the kitty terminfo files are not available on the server. You can ssh in using the following command which will automatically copy the terminfo files to the server: kitty +kitten ssh myserver This ssh kitten takes all the same command line arguments as ssh, you can alias it to ssh in your shell\u2019s rc files to avoid having to type it each time: alias ssh = \"kitty +kitten ssh\"","title":"Scrollback when ssh into a machine doesn't work"},{"location":"kitty/#reasons-to-migrate-from-urxvt-to-kitty","text":"It doesn't fuck up your terminal colors. You can use peek to record your screen. Easier to extend.","title":"Reasons to migrate from urxvt to kitty"},{"location":"kitty/#references","text":"Homepage","title":"References"},{"location":"lazy_loading/","text":"Lazy loading is an programming implementation paradigm which delays the evaluation of an expression until its value is needed and which also avoids repeated evaluations. Lazy evaluation is the preferred implementation when the operation is expensive, requiring either extensive processing time or memory. For example, in Python, one of the best-known techniques involving lazy evaluation is generators. Instead of creating whole sequences for the iteration, which can consume lots of memory, generators lazily evaluate the current need and yield one element at a time when requested. Other example are attributes that take long to compute: class Person : def __init__ ( self , name , occupation ): self . name = name self . occupation = occupation self . relatives = self . _get_all_relatives () def _get_all_relatives (): ... # This is an expensive operation This approach may cause initialization to take unnecessarily long, especially when you don't always need to access Person.relatives . A better strategy would be to get relatives when it's needed. class Person : def __init__ ( self , name , occupation ): self . name = name self . occupation = occupation self . _relatives = None @property def relatives ( self ): if self . _relatives is None : self . _relatives = ... # Get all relatives return self . _relatives In this case, the list of relatives is computed the first time Person.relatives is accessed. After that, it's stored in Person._relatives to prevent repeated evaluations. A perhaps more Pythonic approach would be to use a decorator that makes a property lazy-evaluated. def lazy_property ( fn ): '''Decorator that makes a property lazy-evaluated. ''' attr_name = '_lazy_' + fn . __name__ @property def _lazy_property ( self ): if not hasattr ( self , attr_name ): setattr ( self , attr_name , fn ( self )) return getattr ( self , attr_name ) return _lazy_property class Person : def __init__ ( self , name , occupation ): self . name = name self . occupation = occupation @lazy_property def relatives ( self ): # Get all relatives relatives = ... return relatives This removes a lot of boilerplate, especially when an object has many lazily-evaluated properties. Another approach is to use the getattr special method . References \u2691 Steven Loria article on Lazy Properties Yong Cui article on Lazy attributes","title":"Lazy loading"},{"location":"lazy_loading/#references","text":"Steven Loria article on Lazy Properties Yong Cui article on Lazy attributes","title":"References"},{"location":"life_management/","text":"I understand life management as the act of analyzing yourself and your interactions with the world to define processes and automations that shape the way to efficiently achieve your goals.","title":"Life Management"},{"location":"linux_snippets/","text":"Bypass client SSL certificate with cli tool \u2691 Websites that require clients to authorize with an TLS certificate are difficult to interact with through command line tools that don't support this feature. To solve it, we can use a transparent proxy that does the exchange for us. Export your certificate: If you have a p12 certificate, you first need to extract the key, crt and the ca from the certificate into the site.pem . openssl pkcs12 -in certificate.p12 -out site.key.pem -nocerts -nodes # It asks for the p12 password openssl pkcs12 -in certificate.p12 -out site.crt.pem -clcerts -nokeys openssl pkcs12 -cacerts -nokeys -in certificate.p12 -out site-ca-cert.ca cat site.key.pem site.crt.pem site-ca-cert.ca > site.pem Build the proxy ca: Then we merge the site and the client ca's into the site-ca-file.cert file: openssl s_client -connect www.site.org:443 2 >/dev/null | openssl x509 -text > site-ca-file.cert cat site-ca-cert.ca >> web-ca-file.cert * Change your hosts file to redirect all requests to the proxy. # vim /etc/ hosts [...] 0 . 0 . 0 . 0 www.site.org Run the proxy docker run --rm \\ -v $( pwd ) :/certs/ \\ -p 3001 :3001 \\ -it ghostunnel/ghostunnel \\ client \\ --listen 0 .0.0.0:3001 \\ --target www.site.org:443 \\ --keystore /certs/site.pem \\ --cacert /certs/site-ca-file.cert \\ --unsafe-listen Run the command line tool using the http protocol on the port 3001: wpscan --url http://www.site.org:3001/ --disable-tls-checks Remember to clean up your env afterwards. Allocate space for a virtual filesystem \u2691 fallocate -l 20G /path/to/file Identify what a string or file contains \u2691 Identify anything. pyWhat easily lets you identify emails, IP addresses, and more. Feed it a .pcap file or some text and it'll tell you what it is. Split a file into many with equal number of lines \u2691 You could do something like this: split -l 200000 filename Which will create files each with 200000 lines named xaa , xab , xac , ... Check if an rsync command has gone well \u2691 Sometimes after you do an rsync between two directories of different devices (an usb and your hard drive for example), the sizes of the directories don't match. I've seen a difference of a 30% less on the destination. du , ncdu and and have a long story of reporting wrong sizes with advanced filesystems (ZFS, VxFS or compressing filesystems), these do a lot of things to reduce the disk usage (deduplication, compression, extents, files with holes...) which may lead to the difference in space. To check if everything went alright run diff -r --brief source/ dest/ , and check that there is no output. List all process swap usage \u2691 for file in /proc/*/status ; do awk '/VmSwap|Name/{printf $2 \" \" $3}END{ print \"\"}' $file ; done | sort -k 2 -n -r | less","title":"Linux Snippets"},{"location":"linux_snippets/#bypass-client-ssl-certificate-with-cli-tool","text":"Websites that require clients to authorize with an TLS certificate are difficult to interact with through command line tools that don't support this feature. To solve it, we can use a transparent proxy that does the exchange for us. Export your certificate: If you have a p12 certificate, you first need to extract the key, crt and the ca from the certificate into the site.pem . openssl pkcs12 -in certificate.p12 -out site.key.pem -nocerts -nodes # It asks for the p12 password openssl pkcs12 -in certificate.p12 -out site.crt.pem -clcerts -nokeys openssl pkcs12 -cacerts -nokeys -in certificate.p12 -out site-ca-cert.ca cat site.key.pem site.crt.pem site-ca-cert.ca > site.pem Build the proxy ca: Then we merge the site and the client ca's into the site-ca-file.cert file: openssl s_client -connect www.site.org:443 2 >/dev/null | openssl x509 -text > site-ca-file.cert cat site-ca-cert.ca >> web-ca-file.cert * Change your hosts file to redirect all requests to the proxy. # vim /etc/ hosts [...] 0 . 0 . 0 . 0 www.site.org Run the proxy docker run --rm \\ -v $( pwd ) :/certs/ \\ -p 3001 :3001 \\ -it ghostunnel/ghostunnel \\ client \\ --listen 0 .0.0.0:3001 \\ --target www.site.org:443 \\ --keystore /certs/site.pem \\ --cacert /certs/site-ca-file.cert \\ --unsafe-listen Run the command line tool using the http protocol on the port 3001: wpscan --url http://www.site.org:3001/ --disable-tls-checks Remember to clean up your env afterwards.","title":"Bypass client SSL certificate with cli tool"},{"location":"linux_snippets/#allocate-space-for-a-virtual-filesystem","text":"fallocate -l 20G /path/to/file","title":"Allocate space for a virtual filesystem"},{"location":"linux_snippets/#identify-what-a-string-or-file-contains","text":"Identify anything. pyWhat easily lets you identify emails, IP addresses, and more. Feed it a .pcap file or some text and it'll tell you what it is.","title":"Identify what a string or file contains"},{"location":"linux_snippets/#split-a-file-into-many-with-equal-number-of-lines","text":"You could do something like this: split -l 200000 filename Which will create files each with 200000 lines named xaa , xab , xac , ...","title":"Split a file into many with equal number of lines"},{"location":"linux_snippets/#check-if-an-rsync-command-has-gone-well","text":"Sometimes after you do an rsync between two directories of different devices (an usb and your hard drive for example), the sizes of the directories don't match. I've seen a difference of a 30% less on the destination. du , ncdu and and have a long story of reporting wrong sizes with advanced filesystems (ZFS, VxFS or compressing filesystems), these do a lot of things to reduce the disk usage (deduplication, compression, extents, files with holes...) which may lead to the difference in space. To check if everything went alright run diff -r --brief source/ dest/ , and check that there is no output.","title":"Check if an rsync command has gone well"},{"location":"linux_snippets/#list-all-process-swap-usage","text":"for file in /proc/*/status ; do awk '/VmSwap|Name/{printf $2 \" \" $3}END{ print \"\"}' $file ; done | sort -k 2 -n -r | less","title":"List all process swap usage"},{"location":"mbsync/","text":"mbsync is a command line application which synchronizes mailboxes; currently Maildir and IMAP4 mailboxes are supported. New messages, message deletions and flag changes can be propagated both ways; the operation set can be selected in a fine-grained manner. Installation \u2691 apt-get install isync Configuration \u2691 Assuming that you want to sync the mails of example@examplehost.com and that you have your password stored in pass under mail/example . File: ~/.mbsyncrc IMAPAccount example Host examplehost.com User \"example@examplehost.com\" PassCmd \"/usr/bin/pass mail/example\" IMAPStore example-remote Account example UseNamespace no MaildirStore example-local Path ~/mail/example/ Inbox ~/mail/example/Inbox Channel example Master :example-remote: Slave :example-local: Create Both Patterns * SyncState * CopyArrivalDate yes Sync Pull You need to manually create the directories where you store the emails. mkdir -p ~/mail/example References \u2691 Homepage","title":"mbsync"},{"location":"mbsync/#installation","text":"apt-get install isync","title":"Installation"},{"location":"mbsync/#configuration","text":"Assuming that you want to sync the mails of example@examplehost.com and that you have your password stored in pass under mail/example . File: ~/.mbsyncrc IMAPAccount example Host examplehost.com User \"example@examplehost.com\" PassCmd \"/usr/bin/pass mail/example\" IMAPStore example-remote Account example UseNamespace no MaildirStore example-local Path ~/mail/example/ Inbox ~/mail/example/Inbox Channel example Master :example-remote: Slave :example-local: Create Both Patterns * SyncState * CopyArrivalDate yes Sync Pull You need to manually create the directories where you store the emails. mkdir -p ~/mail/example","title":"Configuration"},{"location":"mbsync/#references","text":"Homepage","title":"References"},{"location":"meditation/","text":"Meditation is a practice where an individual uses a technique,such as mindfulness, or focusing the mind on a particular object, thought, or activity, to train attention and awareness, and achieve a mentally clear and emotionally calm and stable state. Meditation may reduce stress, anxiety, depression, and pain, and enhance peace, perception, self-concept, and well-being. Types of meditation \u2691 Although there isn't a right or wrong way to meditate, it\u2019s important to find a practice that meets your needs and complements your personality. There are nine popular types of meditation practice: Mindfulness meditation : You pay attention to your thoughts as they pass through your mind. You don't judge the thoughts or become involved with them. You simply observe and take note of any patterns. This practice combines concentration with awareness. You may find it helpful to focus on an object or your breath while you observe any bodily sensations, thoughts, or feelings. This type of meditation is good for people who don\u2019t have a teacher to guide them, as it can be easily practiced alone. Focused meditation : Involves concentration using any of the five senses. For example, you can focus on something internal, like your breath, or you can bring in external influences to help focus your attention. Try counting mala beads, listening to a gong, or staring at a candle flame. This practice may be simple in theory, but it can be difficult for beginners to hold their focus for longer than a few minutes at first. If your mind does wander, it\u2019s important to come back to the practice and refocus. As the name suggests, this practice is ideal for anyone who requires additional focus in their life. Movement meditation : It\u2019s an active form of meditation where the movement guides you. It can be achieved through yoga, martial arts or by walking through the woods, gardening, qigong, and other gentle forms of motion. Movement meditation is good for people who find peace in action and prefer to let their minds wander. Mantra meditation : Uses a repetitive sound to clear the mind. It can be a word, phrase, or sound, such as the popular \u201cOm.\u201d It doesn't matter if your mantra is spoken loudly or quietly. After chanting the mantra for some time, you\u2019ll be more alert and in tune with your environment. This allows you to experience deeper levels of awareness. Some people enjoy mantra meditation because they find it easier to focus on a word than on their breath. This is also a good practice for people who don't like silence and enjoy repetition. Transcendental Meditation : It is more customizable than mantra meditation, using a mantra or series of words that are specific to each practitioner. This practice is for those who like structure and are serious about maintaining a meditation practice. Progressive relaxation : Also known as body scan meditation, it's a practice aimed at reducing tension in the body and promoting relaxation. Oftentimes, this form of meditation involves slowly tightening and relaxing one muscle group at a time throughout the body. In some cases, it may also encourage you to imagine a gentle wave flowing through your body to help release any tension. This form of meditation is often used to relieve stress and unwind before bedtime. Loving-kindness meditation : is used to strengthen feelings of compassion, kindness, and acceptance toward oneself and others. It typically involves opening the mind to receive love from others and then sending a series of well wishes to loved ones, friends, acquaintances, and all living beings. Because this type of meditation is intended to promote compassion and kindness, it may be ideal for those holding feelings of anger or resentment. * Visualization meditation : Is a technique focused on enhancing feelings of relaxation, peace, and calmness by visualizing positive scenes or images. With this practice, it\u2019s important to imagine the scene vividly and use all five senses to add as much detail as possible. Another form of visualization meditation involves imagining yourself succeeding at specific goals, which is intended to increase focus and motivation. Many people use visualization meditation to boost their mood, reduce stress levels, and promote inner peace. Spiritual meditation : Spiritual meditation is used in Eastern religions, such as Hinduism and Daoism, and in Christian faith.. It\u2019s similar to prayer in that you reflect on the silence around you and seek a deeper connection with your God or Universe. How to get started \u2691 The easiest way to begin is to sit quietly and focus on your breath for 20 minutes every day. If it's too much for you, start in small moments of time, even 5 or 10 minutes, and grow from there. References \u2691 healthline article on types of meditation To review \u2691 https://wiki.nikitavoloboev.xyz/mindfulness/meditation https://www.healthline.com/health/4-7-8-breathing#Other-techniques-to-help-you-sleep https://threader.app/thread/1261481222359801856 https://quietkit.com/box-breathing/ https://www.healthline.com/health/mental-health/best-mindfulness-blogs#8 https://www.mindful.org/how-to-meditate/","title":"Meditation"},{"location":"meditation/#types-of-meditation","text":"Although there isn't a right or wrong way to meditate, it\u2019s important to find a practice that meets your needs and complements your personality. There are nine popular types of meditation practice: Mindfulness meditation : You pay attention to your thoughts as they pass through your mind. You don't judge the thoughts or become involved with them. You simply observe and take note of any patterns. This practice combines concentration with awareness. You may find it helpful to focus on an object or your breath while you observe any bodily sensations, thoughts, or feelings. This type of meditation is good for people who don\u2019t have a teacher to guide them, as it can be easily practiced alone. Focused meditation : Involves concentration using any of the five senses. For example, you can focus on something internal, like your breath, or you can bring in external influences to help focus your attention. Try counting mala beads, listening to a gong, or staring at a candle flame. This practice may be simple in theory, but it can be difficult for beginners to hold their focus for longer than a few minutes at first. If your mind does wander, it\u2019s important to come back to the practice and refocus. As the name suggests, this practice is ideal for anyone who requires additional focus in their life. Movement meditation : It\u2019s an active form of meditation where the movement guides you. It can be achieved through yoga, martial arts or by walking through the woods, gardening, qigong, and other gentle forms of motion. Movement meditation is good for people who find peace in action and prefer to let their minds wander. Mantra meditation : Uses a repetitive sound to clear the mind. It can be a word, phrase, or sound, such as the popular \u201cOm.\u201d It doesn't matter if your mantra is spoken loudly or quietly. After chanting the mantra for some time, you\u2019ll be more alert and in tune with your environment. This allows you to experience deeper levels of awareness. Some people enjoy mantra meditation because they find it easier to focus on a word than on their breath. This is also a good practice for people who don't like silence and enjoy repetition. Transcendental Meditation : It is more customizable than mantra meditation, using a mantra or series of words that are specific to each practitioner. This practice is for those who like structure and are serious about maintaining a meditation practice. Progressive relaxation : Also known as body scan meditation, it's a practice aimed at reducing tension in the body and promoting relaxation. Oftentimes, this form of meditation involves slowly tightening and relaxing one muscle group at a time throughout the body. In some cases, it may also encourage you to imagine a gentle wave flowing through your body to help release any tension. This form of meditation is often used to relieve stress and unwind before bedtime. Loving-kindness meditation : is used to strengthen feelings of compassion, kindness, and acceptance toward oneself and others. It typically involves opening the mind to receive love from others and then sending a series of well wishes to loved ones, friends, acquaintances, and all living beings. Because this type of meditation is intended to promote compassion and kindness, it may be ideal for those holding feelings of anger or resentment. * Visualization meditation : Is a technique focused on enhancing feelings of relaxation, peace, and calmness by visualizing positive scenes or images. With this practice, it\u2019s important to imagine the scene vividly and use all five senses to add as much detail as possible. Another form of visualization meditation involves imagining yourself succeeding at specific goals, which is intended to increase focus and motivation. Many people use visualization meditation to boost their mood, reduce stress levels, and promote inner peace. Spiritual meditation : Spiritual meditation is used in Eastern religions, such as Hinduism and Daoism, and in Christian faith.. It\u2019s similar to prayer in that you reflect on the silence around you and seek a deeper connection with your God or Universe.","title":"Types of meditation"},{"location":"meditation/#how-to-get-started","text":"The easiest way to begin is to sit quietly and focus on your breath for 20 minutes every day. If it's too much for you, start in small moments of time, even 5 or 10 minutes, and grow from there.","title":"How to get started"},{"location":"meditation/#references","text":"healthline article on types of meditation","title":"References"},{"location":"meditation/#to-review","text":"https://wiki.nikitavoloboev.xyz/mindfulness/meditation https://www.healthline.com/health/4-7-8-breathing#Other-techniques-to-help-you-sleep https://threader.app/thread/1261481222359801856 https://quietkit.com/box-breathing/ https://www.healthline.com/health/mental-health/best-mindfulness-blogs#8 https://www.mindful.org/how-to-meditate/","title":"To review"},{"location":"mermaidjs/","text":"MermaidJS is a Javascript library that lets you create diagrams using text and code. It can render the next diagram types : Flowchart Sequence. Gantt Class Git graph Entity Relationship User journey Installation \u2691 Installing it requires node, I've only used it in mkdocs , which is easier to install and use. Usage \u2691 Flowchart \u2691 It can have two orientations top to bottom ( TB ) or left to right ( LR ). graph TD Start --> Stop By default the text shown is the same as the id, if you need a big text it's recommended to use the id1[This is the text in the box] syntax so it's easy to reference the node in the relationships. To link nodes, use --> or --- . If you cant to add text to the link use A-- text -->B Adding links \u2691 You can add click events to the diagrams: graph LR; A-->B; B-->C; C-->D; click A callback \"Tooltip for a callback\" click B \"http://www.github.com\" \"This is a tooltip for a link\" click A call callback() \"Tooltip for a callback\" click B href \"http://www.github.com\" \"This is a tooltip for a link\" By default the links are opened in the same browser tab/window. It is possible to change this by adding a link target to the click definition ( _self , _blank , _parent , or _top ). graph LR; A-->B; B-->C; C-->D; D-->E; click A \"http://www.github.com\" _blank Node styling \u2691 You can define the style for each node with: graph LR id1(Start)-->id2(Stop) style id1 fill:#f9f,stroke:#333,stroke-width:4px style id2 fill:#bbf,stroke:#f66,stroke-width:2px,color:#fff,stroke-dasharray: 5 5 Or if you're going to use the same style for multiple nodes, you can define classes: graph LR A:::someclass --> B classDef someclass fill:#f96; References \u2691 Docs","title":"MermaidJS"},{"location":"mermaidjs/#installation","text":"Installing it requires node, I've only used it in mkdocs , which is easier to install and use.","title":"Installation"},{"location":"mermaidjs/#usage","text":"","title":"Usage"},{"location":"mermaidjs/#flowchart","text":"It can have two orientations top to bottom ( TB ) or left to right ( LR ). graph TD Start --> Stop By default the text shown is the same as the id, if you need a big text it's recommended to use the id1[This is the text in the box] syntax so it's easy to reference the node in the relationships. To link nodes, use --> or --- . If you cant to add text to the link use A-- text -->B","title":"Flowchart"},{"location":"mermaidjs/#adding-links","text":"You can add click events to the diagrams: graph LR; A-->B; B-->C; C-->D; click A callback \"Tooltip for a callback\" click B \"http://www.github.com\" \"This is a tooltip for a link\" click A call callback() \"Tooltip for a callback\" click B href \"http://www.github.com\" \"This is a tooltip for a link\" By default the links are opened in the same browser tab/window. It is possible to change this by adding a link target to the click definition ( _self , _blank , _parent , or _top ). graph LR; A-->B; B-->C; C-->D; D-->E; click A \"http://www.github.com\" _blank","title":"Adding links"},{"location":"mermaidjs/#node-styling","text":"You can define the style for each node with: graph LR id1(Start)-->id2(Stop) style id1 fill:#f9f,stroke:#333,stroke-width:4px style id2 fill:#bbf,stroke:#f66,stroke-width:2px,color:#fff,stroke-dasharray: 5 5 Or if you're going to use the same style for multiple nodes, you can define classes: graph LR A:::someclass --> B classDef someclass fill:#f96;","title":"Node styling"},{"location":"mermaidjs/#references","text":"Docs","title":"References"},{"location":"money_management/","text":"Money management is the act of analyzing where you spend your money on with the least amount of mental load. Some years ago I started using the double entry counting method with beancount . System inputs \u2691 I have two types of financial transactions to track: The credit/debit card movements: Easy to track as usually the banks support exporting them as CSV, and beancount have specific bank importers . The cash movements: Harder to track as you need to keep them manually. This has been my biggest source of errors, once I understood how to correctly use beancount . In the latest iteration, I'm using the cone Android app to keep track of these expenses. Workflow \u2691 Beancount ledger organization \u2691 My beancount project directory tree is: . \u251c\u2500\u2500 .git \u2502 \u2514\u2500\u2500 ... \u251c\u2500\u2500 2011 \u2502 \u251c\u2500\u2500 09.book \u2502 \u251c\u2500\u2500 10.book \u2502 \u251c\u2500\u2500 11.book \u2502 \u251c\u2500\u2500 12.book \u2502 \u2514\u2500\u2500 year.book \u251c\u2500\u2500 ... \u251c\u2500\u2500 2020 \u2502 \u251c\u2500\u2500 01.book \u2502 \u251c\u2500\u2500 02.book \u2502 \u251c\u2500\u2500 ... \u2502 \u251c\u2500\u2500 11.book \u2502 \u251c\u2500\u2500 12.book \u2502 \u2514\u2500\u2500 year.book \u251c\u2500\u2500 ledger.book \u251c\u2500\u2500 .closed.accounts.book \u251c\u2500\u2500 roadmap.md \u2514\u2500\u2500 to.process \u251c\u2500\u2500 cone.book \u251c\u2500\u2500 bank1.csv \u2514\u2500\u2500 bank2.csv Where: .git : I keep everything in a git repository to have a version controlled ledger. Each year has it's own directory with: A book file per month, check below it's contents. A year.book file with just include statements: include \"01.book\" include \"02.book\" include \"03.book\" include \"04.book\" include \"05.book\" include \"06.book\" include \"07.book\" include \"08.book\" include \"09.book\" # include \"10.book\" # include \"11.book\" # include \"12.book\" * ledger.book : The beancount entry point where the accounts are defined. * .closed.accounts.book : To store the account closing statements. * roadmap.md : To store the financial plan for the semester/year/life. * to.process : To store the raw data from external sources. The main ledger \u2691 The ledger.book file contains the beancount configuration, with the opening of accounts and inclusion of the monthly books. I like to split it in sections. TL;DR: The full ledger.book # Options option \"title\" \"Lyz Lair Ledge\" option \"operating_currency\" \"EUR\" # Events 2016-12-19 event \"employer\" \"XXX\" # Eternal accounts # Assets 2010-05-17 open Assets:Cash EUR 2021-01-25 open Assets:Cash:Coins EUR 2021-01-25 open Assets:Cash:Paper EUR 2018-09-10 open Assets:Cashbox EUR 2019-01-11 open Assets:Cashbox:Coins EUR 2019-01-11 open Assets:Cashbox:Paper EUR 2018-04-01 open Assets:Savings EUR 2019-08-01 open Assets:Savings:CashFlowRefiller EUR 2019-08-01 open Assets:Savings:UnexpectedExpenses EUR 2019-08-01 open Assets:Savings:Home EUR 2018-04-01 open Assets:CashFlowCard EUR 2018-04-01 open Assets:CashDeposit EUR # Debts 2016-01-01 open Assets:Debt:Person1 EUR 2016-01-01 open Assets:Debt:Person2 EUR # Income 2016-12-01 open Income:Employer1 EUR 2019-05-21 open Income:Employer2 EUR 2010-05-17 open Income:State EUR 2019-01-01 open Income:Gifts EUR # Equity 2010-05-17 open Equity:Opening-Balances 2010-05-17 open Equity:Errors 2010-05-17 open Equity:Forgiven # Expenses 2010-01-01 open Expenses:Bills EUR 2013-01-01 open Expenses:Bills:Gas EUR 2010-01-01 open Expenses:Bills:Phone EUR 2019-01-01 open Expenses:Bills:Light EUR 2010-01-01 open Expenses:Bills:Rent EUR 2010-01-01 open Expenses:Bills:PublicTransport EUR 2017-01-01 open Expenses:Bills:Subscriptions EUR 2016-01-01 open Expenses:Bills:Union EUR 2010-01-01 open Expenses:Books EUR 2010-12-01 open Expenses:Car EUR 2010-12-01 open Expenses:Car:Fuel EUR 2010-12-01 open Expenses:Car:Insurance EUR 2010-12-01 open Expenses:Car:Repair EUR 2010-12-01 open Expenses:Car:Taxes EUR 2010-12-01 open Expenses:Car:Tickets EUR 2010-01-01 open Expenses:Clothes EUR 2018-11-01 open Expenses:Donations EUR 2010-05-17 open Expenses:Financial EUR 2010-01-01 open Expenses:Games EUR 2010-01-01 open Expenses:Games:Steam EUR 2010-01-01 open Expenses:Games:HumbleBundle EUR 2019-06-01 open Expenses:Games:GOG EUR 2020-06-01 open Expenses:Games:Itchio EUR 2010-01-01 open Expenses:Gifts EUR 2010-01-01 open Expenses:Gifts:Person1 EUR 2010-01-01 open Expenses:Gifts:Person2 EUR 2010-01-01 open Expenses:Gifts:Mine EUR 2010-01-01 open Expenses:Groceries EUR 2018-11-01 open Expenses:Groceries:Extras EUR 2020-01-01 open Expenses:Groceries:Supermarket EUR 2020-01-01 open Expenses:Groceries:Prepared EUR 2020-01-01 open Expenses:Groceries:GreenGrocery EUR 2010-01-01 open Expenses:Hardware EUR 2010-01-01 open Expenses:Home EUR 2010-01-01 open Expenses:Home:WashingMachine EUR 2010-01-01 open Expenses:Home:DishWasher EUR 2010-01-01 open Expenses:Home:Fridge EUR 2020-06-01 open Expenses:Legal EUR 2010-01-01 open Expenses:Medicines EUR 2010-01-01 open Expenses:Social EUR 2010-01-01 open Expenses:Social:Eat EUR 2010-01-01 open Expenses:Social:Drink EUR 2019-06-01 open Expenses:Taxes:Tax1 EUR 2016-01-01 open Expenses:Taxes:Tax2 EUR 2010-05-17 open Expenses:Trips EUR 2010-05-17 open Expenses:Trips:Accommodation EUR 2010-05-17 open Expenses:Trips:Drink EUR 2010-05-17 open Expenses:Trips:Food EUR 2010-05-17 open Expenses:Trips:Tickets EUR 2010-05-17 open Expenses:Trips:Transport EUR 2019-05-20 open Expenses:Work EUR 2019-05-20 open Expenses:Work:Phone EUR 2019-05-20 open Expenses:Work:Hardware EUR 2019-05-20 open Expenses:Work:Trips EUR 2019-05-20 open Expenses:Work:Trips:Accommodation EUR 2019-05-20 open Expenses:Work:Trips:Drink EUR 2019-05-20 open Expenses:Work:Trips:Food EUR 2019-05-20 open Expenses:Work:Trips:Tickets EUR 2019-05-20 open Expenses:Work:Trips:Transport EUR ## Initialization 2010-05-17 pad Assets:Cash Equity:Opening-Balances 2016-01-01 pad Assets:Debt:Person1 Equity:Opening-Balances # Transfers include \".closed.accounts.book\" include \"2011/year.book\" include \"2012/year.book\" include \"2013/year.book\" include \"2014/year.book\" include \"2015/year.book\" include \"2016/year.book\" include \"2017/year.book\" include \"2018/year.book\" include \"2019/year.book\" include \"2020/year.book\" Assets \u2691 Asset accounts represent something you have. # Assets 2010-05-17 open Assets:Cash EUR 2021-01-25 open Assets:Cash:Coins EUR 2021-01-25 open Assets:Cash:Paper EUR 2018-09-10 open Assets:Cashbox EUR 2019-01-11 open Assets:Cashbox:Coins EUR 2019-01-11 open Assets:Cashbox:Paper EUR 2018-04-01 open Assets:Savings EUR 2019-08-01 open Assets:Savings:CashFlowRefiller EUR 2019-08-01 open Assets:Savings:UnexpectedExpenses EUR 2019-08-01 open Assets:Savings:Home EUR 2018-04-01 open Assets:CashFlowCard EUR 2018-04-01 open Assets:CashDeposit EUR Being a privacy minded person, I try to pay everything by cash. To track it, I've created the following asset accounts: Assets:Cash:Paper : Paper money in my wallet. I like to have a 5, 10 and 20 euro bills as it gives the best flexibility without carrying too much money. Assets:Cash:Coins : Coins in my wallet. Assets:Cashbox:Paper : Paper money stored at home. I fill it up monthly to my average monthly expenses, so I reduce the trips to the ATM to the minimum. Once this account is below 100 EUR, I add the mental task to refill it. Assets:Cashbox:Coins : Coins stored at home. I keep it at 10 EUR in coins of 2 EUR, so it's quick to count at the same time as it's able to cover most of the things you need to buy with coins. Assets:Cashbox:SmallCoins : If my coins wallet is starting to get heavy, I extract the coins smaller than 50 cents into a container with copper coins. Assets:CashDeposit : You never know when the bank system is going to fuck you, so it's always good to have some cash under the mattress. Having this level of granularity and doing weekly balances of each of those accounts has helped me understand the flaws in my processes that lead to the cash accounting errors. As most humans living in the first world, I'm forced to have at least one bank account. For security reasons I have two: Assets:CashFlowCard : The bank account with an associated debit card. Here is from where I make my expenses, such as home rental, supplies payment, ATM money withdrawal. As it is exposed to all the payment platforms, I assume that it will come a time when a vulnerability is found in one of them, so I keep the least amount of money I can. As with the Cashbox I monthly refill it with the expected expenses amount plus a safety amount. Assets:Savings : The bank account where I store my savings. I have it subdivided in three sections: Assets:Savings:CashFlowRefiller : Here I store the average monthly expenses for the following two months. Assets:Savings:UnexpectedExpenses : Deposit for unexpected expenses such as car or domestic appliances repairs. Assets:Savings:Home : Deposit for the initial payment or a house. Debts \u2691 Debts can be tracked either as an asset or as a liability. If you expect them to owe you more often (you lend a friend some money), model it as an asset, if you're going to owe them (you borrow from the bank to buy a house), model it as a liability. # Debts 2016-01-01 open Assets:Debt:Person1 EUR 2016-01-01 open Assets:Debt:Person2 EUR Income \u2691 Income accounts represent where you get the money from. # Income 2016-12-01 open Income:Employer1 EUR 2019-05-21 open Income:Employer2 EUR 2010-05-17 open Income:State EUR 2019-01-01 open Income:Gifts EUR Equity \u2691 I use equity accounts to make adjustments. # Equity 2010-05-17 open Equity:Opening-Balances 2010-05-17 open Equity:Errors 2010-05-17 open Equity:Forgiven Equity:Opening-Balances : Used to set the initial balance of an account. Equity:Errors : Used with the pad statements to track the errors in the accounting. Equity:Forgiven : Used in the transactions to forgive someone's debts. Expenses \u2691 Expense accounts model where you expend the money on. # Expenses 2010-01-01 open Expenses:Bills EUR 2013-01-01 open Expenses:Bills:Gas EUR 2010-01-01 open Expenses:Bills:Phone EUR 2019-01-01 open Expenses:Bills:Light EUR 2010-01-01 open Expenses:Bills:Rent EUR 2010-01-01 open Expenses:Bills:PublicTransport EUR 2017-01-01 open Expenses:Bills:Subscriptions EUR 2016-01-01 open Expenses:Bills:Union EUR 2010-01-01 open Expenses:Books EUR 2010-12-01 open Expenses:Car EUR 2010-12-01 open Expenses:Car:Fuel EUR 2010-12-01 open Expenses:Car:Insurance EUR 2010-12-01 open Expenses:Car:Repair EUR 2010-12-01 open Expenses:Car:Taxes EUR 2010-12-01 open Expenses:Car:Tickets EUR 2010-01-01 open Expenses:Clothes EUR 2018-11-01 open Expenses:Donations EUR 2010-05-17 open Expenses:Financial EUR 2010-01-01 open Expenses:Games EUR 2010-01-01 open Expenses:Games:Steam EUR 2010-01-01 open Expenses:Games:HumbleBundle EUR 2019-06-01 open Expenses:Games:GOG EUR 2020-06-01 open Expenses:Games:Itchio EUR 2010-01-01 open Expenses:Gifts EUR 2010-01-01 open Expenses:Gifts:Person1 EUR 2010-01-01 open Expenses:Gifts:Person2 EUR 2010-01-01 open Expenses:Gifts:Mine EUR 2010-01-01 open Expenses:Groceries EUR 2018-11-01 open Expenses:Groceries:Extras EUR 2020-01-01 open Expenses:Groceries:Supermarket EUR 2020-01-01 open Expenses:Groceries:Prepared EUR 2020-01-01 open Expenses:Groceries:GreenGrocery EUR 2010-01-01 open Expenses:Hardware EUR 2010-01-01 open Expenses:Home EUR 2010-01-01 open Expenses:Home:WashingMachine EUR 2010-01-01 open Expenses:Home:DishWasher EUR 2010-01-01 open Expenses:Home:Fridge EUR 2020-06-01 open Expenses:Legal EUR 2010-01-01 open Expenses:Medicines EUR 2010-01-01 open Expenses:Social EUR 2010-01-01 open Expenses:Social:Eat EUR 2010-01-01 open Expenses:Social:Drink EUR 2019-06-01 open Expenses:Taxes:Tax1 EUR 2016-01-01 open Expenses:Taxes:Tax2 EUR 2010-05-17 open Expenses:Trips EUR 2010-05-17 open Expenses:Trips:Accommodation EUR 2010-05-17 open Expenses:Trips:Drink EUR 2010-05-17 open Expenses:Trips:Food EUR 2010-05-17 open Expenses:Trips:Tickets EUR 2010-05-17 open Expenses:Trips:Transport EUR 2019-05-20 open Expenses:Work EUR 2019-05-20 open Expenses:Work:Phone EUR 2019-05-20 open Expenses:Work:Hardware EUR 2019-05-20 open Expenses:Work:Trips EUR 2019-05-20 open Expenses:Work:Trips:Accommodation EUR 2019-05-20 open Expenses:Work:Trips:Drink EUR 2019-05-20 open Expenses:Work:Trips:Food EUR 2019-05-20 open Expenses:Work:Trips:Tickets EUR 2019-05-20 open Expenses:Work:Trips:Transport EUR I decided to split my expenses in: Expenses:Bills : All the periodic bills I pay Expenses:Bills:Gas : Expenses:Bills:Phone : Expenses:Bills:Light : Expenses:Bills:Rent : Expenses:Bills:PublicTransport : Expenses:Bills:Subscriptions : Newspaper, magazine, web service subscriptions. Expenses:Bills:Union : Expenses:Books : Expenses:Car : Expenses:Car:Fuel : Expenses:Car:Insurance : Expenses:Car:Repair : Expenses:Car:Taxes : Expenses:Car:Tickets : Expenses:Clothes : Expenses:Donations : Expenses:Financial : Expenses related to financial operations or account maintenance. Expenses:Games : Expenses:Games:Steam : Expenses:Games:HumbleBundle : Expenses:Games:GOG : Expenses:Games:Itchio : Expenses:Gifts : Expenses:Gifts:Person1 : Expenses:Gifts:Person2 : Expenses:Gifts:Mine : Expenses:Groceries : Expenses:Groceries:Extras : Expenses:Groceries:Supermarket : Expenses:Groceries:Prepared : Expenses:Groceries:GreenGrocery : Expenses:Hardware : Expenses:Home : Expenses:Home:WashingMachine : Expenses:Home:DishWasher : Expenses:Home:Fridge : Expenses:Legal : Expenses:Medicines : Expenses:Social : Expenses:Social:Eat : Expenses:Social:Drink : Expenses:Taxes : Expenses:Taxes:Tax1 : Expenses:Taxes:Tax2 : Expenses:Trips : Expenses:Trips:Accommodation : Expenses:Trips:Drink : Expenses:Trips:Food : Expenses:Trips:Tickets : Expenses:Trips:Transport : Expenses:Work : Expenses:Work:Phone : Expenses:Work:Hardware : Expenses:Work:Trips : Expenses:Work:Trips:Accommodation : Expenses:Work:Trips:Drink : Expenses:Work:Trips:Food : Expenses:Work:Trips:Tickets : Expenses:Work:Trips:Transport : Initialization of accounts \u2691 # Initialization 2010-05-17 pad Assets:Cash Equity:Opening-Balances 2016-01-01 pad Assets:Debt:Person1 Equity:Opening-Balances Transfer includes \u2691 I reference each year's year.book and the .closed.accounts.book . # Transfers include \".closed.accounts.book\" include \"2011/year.book\" ... include \"2020/year.book\" The monthly book \u2691 Each month has a file with this structure: # Cash transfers # CashFlowCard # Savings # Balances taken at 2020-12-06T19:32 ## Active accounts 2020-12-06 balance Assets:Cashbox:Paper EUR 2020-12-06 balance Assets:Cashbox:Coins EUR 2020-12-06 balance Assets:Cash:Paper EUR 2020-12-06 balance Assets:Cash:Coins EUR 2020-12-06 balance Assets:CashFlowCard EUR 2020-12-06 balance Assets:Savings EUR ## Deposits 2020-12-06 balance Assets:Savings:CashFlowRefiller XXX EUR 2020-12-06 balance Assets:Savings:UnexpectedExpenses XXX EUR 2020-12-06 balance Assets:CashDeposit XXX EUR ## Debts 2020-12-06 balance Assets:Debt:Person1 XXX EUR ## Equity # 2020-12-05 pad Assets:Cash Equity:Errors # 2020-12-05 pad Assets:Cashbox Equity:Errors # Weekly balances ## Measure done on 2020-09-04T17:10 2020-09-04 balance Assets:Cash:Coins XXX EUR 2020-09-04 balance Assets:Cash:Paper XXX EUR 2020-09-04 balance Assets:Cashbox:Coins XXX EUR 2020-09-04 balance Assets:Cashbox:Paper XXX EUR Where each section stores: Cash transfers : The transactions done by cash, extracted from the Android cone application. CashFlowCard : Bank account extracts transformed from the csv to postings with bean-extract . Savings : Bank account extracts transformed from the csv to postings with bean-extract . Monthly balances : I try to review the accounts once each month. This section is subdivided in: Active accounts : The accounts whose value changes monthly. Deposits : The accounts that don't change much each month. Debts : The balance of debt accounts. Equity : The pad statements to track the errors in the monthly account. Weekly balances : As doing the monthly review is long, but it doesn't give me the enough information to not mess up the cash transactions, I do a weekly balance of those accounts.","title":"Money Management"},{"location":"money_management/#system-inputs","text":"I have two types of financial transactions to track: The credit/debit card movements: Easy to track as usually the banks support exporting them as CSV, and beancount have specific bank importers . The cash movements: Harder to track as you need to keep them manually. This has been my biggest source of errors, once I understood how to correctly use beancount . In the latest iteration, I'm using the cone Android app to keep track of these expenses.","title":"System inputs"},{"location":"money_management/#workflow","text":"","title":"Workflow"},{"location":"money_management/#beancount-ledger-organization","text":"My beancount project directory tree is: . \u251c\u2500\u2500 .git \u2502 \u2514\u2500\u2500 ... \u251c\u2500\u2500 2011 \u2502 \u251c\u2500\u2500 09.book \u2502 \u251c\u2500\u2500 10.book \u2502 \u251c\u2500\u2500 11.book \u2502 \u251c\u2500\u2500 12.book \u2502 \u2514\u2500\u2500 year.book \u251c\u2500\u2500 ... \u251c\u2500\u2500 2020 \u2502 \u251c\u2500\u2500 01.book \u2502 \u251c\u2500\u2500 02.book \u2502 \u251c\u2500\u2500 ... \u2502 \u251c\u2500\u2500 11.book \u2502 \u251c\u2500\u2500 12.book \u2502 \u2514\u2500\u2500 year.book \u251c\u2500\u2500 ledger.book \u251c\u2500\u2500 .closed.accounts.book \u251c\u2500\u2500 roadmap.md \u2514\u2500\u2500 to.process \u251c\u2500\u2500 cone.book \u251c\u2500\u2500 bank1.csv \u2514\u2500\u2500 bank2.csv Where: .git : I keep everything in a git repository to have a version controlled ledger. Each year has it's own directory with: A book file per month, check below it's contents. A year.book file with just include statements: include \"01.book\" include \"02.book\" include \"03.book\" include \"04.book\" include \"05.book\" include \"06.book\" include \"07.book\" include \"08.book\" include \"09.book\" # include \"10.book\" # include \"11.book\" # include \"12.book\" * ledger.book : The beancount entry point where the accounts are defined. * .closed.accounts.book : To store the account closing statements. * roadmap.md : To store the financial plan for the semester/year/life. * to.process : To store the raw data from external sources.","title":"Beancount ledger organization"},{"location":"money_management/#the-main-ledger","text":"The ledger.book file contains the beancount configuration, with the opening of accounts and inclusion of the monthly books. I like to split it in sections. TL;DR: The full ledger.book # Options option \"title\" \"Lyz Lair Ledge\" option \"operating_currency\" \"EUR\" # Events 2016-12-19 event \"employer\" \"XXX\" # Eternal accounts # Assets 2010-05-17 open Assets:Cash EUR 2021-01-25 open Assets:Cash:Coins EUR 2021-01-25 open Assets:Cash:Paper EUR 2018-09-10 open Assets:Cashbox EUR 2019-01-11 open Assets:Cashbox:Coins EUR 2019-01-11 open Assets:Cashbox:Paper EUR 2018-04-01 open Assets:Savings EUR 2019-08-01 open Assets:Savings:CashFlowRefiller EUR 2019-08-01 open Assets:Savings:UnexpectedExpenses EUR 2019-08-01 open Assets:Savings:Home EUR 2018-04-01 open Assets:CashFlowCard EUR 2018-04-01 open Assets:CashDeposit EUR # Debts 2016-01-01 open Assets:Debt:Person1 EUR 2016-01-01 open Assets:Debt:Person2 EUR # Income 2016-12-01 open Income:Employer1 EUR 2019-05-21 open Income:Employer2 EUR 2010-05-17 open Income:State EUR 2019-01-01 open Income:Gifts EUR # Equity 2010-05-17 open Equity:Opening-Balances 2010-05-17 open Equity:Errors 2010-05-17 open Equity:Forgiven # Expenses 2010-01-01 open Expenses:Bills EUR 2013-01-01 open Expenses:Bills:Gas EUR 2010-01-01 open Expenses:Bills:Phone EUR 2019-01-01 open Expenses:Bills:Light EUR 2010-01-01 open Expenses:Bills:Rent EUR 2010-01-01 open Expenses:Bills:PublicTransport EUR 2017-01-01 open Expenses:Bills:Subscriptions EUR 2016-01-01 open Expenses:Bills:Union EUR 2010-01-01 open Expenses:Books EUR 2010-12-01 open Expenses:Car EUR 2010-12-01 open Expenses:Car:Fuel EUR 2010-12-01 open Expenses:Car:Insurance EUR 2010-12-01 open Expenses:Car:Repair EUR 2010-12-01 open Expenses:Car:Taxes EUR 2010-12-01 open Expenses:Car:Tickets EUR 2010-01-01 open Expenses:Clothes EUR 2018-11-01 open Expenses:Donations EUR 2010-05-17 open Expenses:Financial EUR 2010-01-01 open Expenses:Games EUR 2010-01-01 open Expenses:Games:Steam EUR 2010-01-01 open Expenses:Games:HumbleBundle EUR 2019-06-01 open Expenses:Games:GOG EUR 2020-06-01 open Expenses:Games:Itchio EUR 2010-01-01 open Expenses:Gifts EUR 2010-01-01 open Expenses:Gifts:Person1 EUR 2010-01-01 open Expenses:Gifts:Person2 EUR 2010-01-01 open Expenses:Gifts:Mine EUR 2010-01-01 open Expenses:Groceries EUR 2018-11-01 open Expenses:Groceries:Extras EUR 2020-01-01 open Expenses:Groceries:Supermarket EUR 2020-01-01 open Expenses:Groceries:Prepared EUR 2020-01-01 open Expenses:Groceries:GreenGrocery EUR 2010-01-01 open Expenses:Hardware EUR 2010-01-01 open Expenses:Home EUR 2010-01-01 open Expenses:Home:WashingMachine EUR 2010-01-01 open Expenses:Home:DishWasher EUR 2010-01-01 open Expenses:Home:Fridge EUR 2020-06-01 open Expenses:Legal EUR 2010-01-01 open Expenses:Medicines EUR 2010-01-01 open Expenses:Social EUR 2010-01-01 open Expenses:Social:Eat EUR 2010-01-01 open Expenses:Social:Drink EUR 2019-06-01 open Expenses:Taxes:Tax1 EUR 2016-01-01 open Expenses:Taxes:Tax2 EUR 2010-05-17 open Expenses:Trips EUR 2010-05-17 open Expenses:Trips:Accommodation EUR 2010-05-17 open Expenses:Trips:Drink EUR 2010-05-17 open Expenses:Trips:Food EUR 2010-05-17 open Expenses:Trips:Tickets EUR 2010-05-17 open Expenses:Trips:Transport EUR 2019-05-20 open Expenses:Work EUR 2019-05-20 open Expenses:Work:Phone EUR 2019-05-20 open Expenses:Work:Hardware EUR 2019-05-20 open Expenses:Work:Trips EUR 2019-05-20 open Expenses:Work:Trips:Accommodation EUR 2019-05-20 open Expenses:Work:Trips:Drink EUR 2019-05-20 open Expenses:Work:Trips:Food EUR 2019-05-20 open Expenses:Work:Trips:Tickets EUR 2019-05-20 open Expenses:Work:Trips:Transport EUR ## Initialization 2010-05-17 pad Assets:Cash Equity:Opening-Balances 2016-01-01 pad Assets:Debt:Person1 Equity:Opening-Balances # Transfers include \".closed.accounts.book\" include \"2011/year.book\" include \"2012/year.book\" include \"2013/year.book\" include \"2014/year.book\" include \"2015/year.book\" include \"2016/year.book\" include \"2017/year.book\" include \"2018/year.book\" include \"2019/year.book\" include \"2020/year.book\"","title":"The main ledger"},{"location":"money_management/#assets","text":"Asset accounts represent something you have. # Assets 2010-05-17 open Assets:Cash EUR 2021-01-25 open Assets:Cash:Coins EUR 2021-01-25 open Assets:Cash:Paper EUR 2018-09-10 open Assets:Cashbox EUR 2019-01-11 open Assets:Cashbox:Coins EUR 2019-01-11 open Assets:Cashbox:Paper EUR 2018-04-01 open Assets:Savings EUR 2019-08-01 open Assets:Savings:CashFlowRefiller EUR 2019-08-01 open Assets:Savings:UnexpectedExpenses EUR 2019-08-01 open Assets:Savings:Home EUR 2018-04-01 open Assets:CashFlowCard EUR 2018-04-01 open Assets:CashDeposit EUR Being a privacy minded person, I try to pay everything by cash. To track it, I've created the following asset accounts: Assets:Cash:Paper : Paper money in my wallet. I like to have a 5, 10 and 20 euro bills as it gives the best flexibility without carrying too much money. Assets:Cash:Coins : Coins in my wallet. Assets:Cashbox:Paper : Paper money stored at home. I fill it up monthly to my average monthly expenses, so I reduce the trips to the ATM to the minimum. Once this account is below 100 EUR, I add the mental task to refill it. Assets:Cashbox:Coins : Coins stored at home. I keep it at 10 EUR in coins of 2 EUR, so it's quick to count at the same time as it's able to cover most of the things you need to buy with coins. Assets:Cashbox:SmallCoins : If my coins wallet is starting to get heavy, I extract the coins smaller than 50 cents into a container with copper coins. Assets:CashDeposit : You never know when the bank system is going to fuck you, so it's always good to have some cash under the mattress. Having this level of granularity and doing weekly balances of each of those accounts has helped me understand the flaws in my processes that lead to the cash accounting errors. As most humans living in the first world, I'm forced to have at least one bank account. For security reasons I have two: Assets:CashFlowCard : The bank account with an associated debit card. Here is from where I make my expenses, such as home rental, supplies payment, ATM money withdrawal. As it is exposed to all the payment platforms, I assume that it will come a time when a vulnerability is found in one of them, so I keep the least amount of money I can. As with the Cashbox I monthly refill it with the expected expenses amount plus a safety amount. Assets:Savings : The bank account where I store my savings. I have it subdivided in three sections: Assets:Savings:CashFlowRefiller : Here I store the average monthly expenses for the following two months. Assets:Savings:UnexpectedExpenses : Deposit for unexpected expenses such as car or domestic appliances repairs. Assets:Savings:Home : Deposit for the initial payment or a house.","title":"Assets"},{"location":"money_management/#debts","text":"Debts can be tracked either as an asset or as a liability. If you expect them to owe you more often (you lend a friend some money), model it as an asset, if you're going to owe them (you borrow from the bank to buy a house), model it as a liability. # Debts 2016-01-01 open Assets:Debt:Person1 EUR 2016-01-01 open Assets:Debt:Person2 EUR","title":"Debts"},{"location":"money_management/#income","text":"Income accounts represent where you get the money from. # Income 2016-12-01 open Income:Employer1 EUR 2019-05-21 open Income:Employer2 EUR 2010-05-17 open Income:State EUR 2019-01-01 open Income:Gifts EUR","title":"Income"},{"location":"money_management/#equity","text":"I use equity accounts to make adjustments. # Equity 2010-05-17 open Equity:Opening-Balances 2010-05-17 open Equity:Errors 2010-05-17 open Equity:Forgiven Equity:Opening-Balances : Used to set the initial balance of an account. Equity:Errors : Used with the pad statements to track the errors in the accounting. Equity:Forgiven : Used in the transactions to forgive someone's debts.","title":"Equity"},{"location":"money_management/#expenses","text":"Expense accounts model where you expend the money on. # Expenses 2010-01-01 open Expenses:Bills EUR 2013-01-01 open Expenses:Bills:Gas EUR 2010-01-01 open Expenses:Bills:Phone EUR 2019-01-01 open Expenses:Bills:Light EUR 2010-01-01 open Expenses:Bills:Rent EUR 2010-01-01 open Expenses:Bills:PublicTransport EUR 2017-01-01 open Expenses:Bills:Subscriptions EUR 2016-01-01 open Expenses:Bills:Union EUR 2010-01-01 open Expenses:Books EUR 2010-12-01 open Expenses:Car EUR 2010-12-01 open Expenses:Car:Fuel EUR 2010-12-01 open Expenses:Car:Insurance EUR 2010-12-01 open Expenses:Car:Repair EUR 2010-12-01 open Expenses:Car:Taxes EUR 2010-12-01 open Expenses:Car:Tickets EUR 2010-01-01 open Expenses:Clothes EUR 2018-11-01 open Expenses:Donations EUR 2010-05-17 open Expenses:Financial EUR 2010-01-01 open Expenses:Games EUR 2010-01-01 open Expenses:Games:Steam EUR 2010-01-01 open Expenses:Games:HumbleBundle EUR 2019-06-01 open Expenses:Games:GOG EUR 2020-06-01 open Expenses:Games:Itchio EUR 2010-01-01 open Expenses:Gifts EUR 2010-01-01 open Expenses:Gifts:Person1 EUR 2010-01-01 open Expenses:Gifts:Person2 EUR 2010-01-01 open Expenses:Gifts:Mine EUR 2010-01-01 open Expenses:Groceries EUR 2018-11-01 open Expenses:Groceries:Extras EUR 2020-01-01 open Expenses:Groceries:Supermarket EUR 2020-01-01 open Expenses:Groceries:Prepared EUR 2020-01-01 open Expenses:Groceries:GreenGrocery EUR 2010-01-01 open Expenses:Hardware EUR 2010-01-01 open Expenses:Home EUR 2010-01-01 open Expenses:Home:WashingMachine EUR 2010-01-01 open Expenses:Home:DishWasher EUR 2010-01-01 open Expenses:Home:Fridge EUR 2020-06-01 open Expenses:Legal EUR 2010-01-01 open Expenses:Medicines EUR 2010-01-01 open Expenses:Social EUR 2010-01-01 open Expenses:Social:Eat EUR 2010-01-01 open Expenses:Social:Drink EUR 2019-06-01 open Expenses:Taxes:Tax1 EUR 2016-01-01 open Expenses:Taxes:Tax2 EUR 2010-05-17 open Expenses:Trips EUR 2010-05-17 open Expenses:Trips:Accommodation EUR 2010-05-17 open Expenses:Trips:Drink EUR 2010-05-17 open Expenses:Trips:Food EUR 2010-05-17 open Expenses:Trips:Tickets EUR 2010-05-17 open Expenses:Trips:Transport EUR 2019-05-20 open Expenses:Work EUR 2019-05-20 open Expenses:Work:Phone EUR 2019-05-20 open Expenses:Work:Hardware EUR 2019-05-20 open Expenses:Work:Trips EUR 2019-05-20 open Expenses:Work:Trips:Accommodation EUR 2019-05-20 open Expenses:Work:Trips:Drink EUR 2019-05-20 open Expenses:Work:Trips:Food EUR 2019-05-20 open Expenses:Work:Trips:Tickets EUR 2019-05-20 open Expenses:Work:Trips:Transport EUR I decided to split my expenses in: Expenses:Bills : All the periodic bills I pay Expenses:Bills:Gas : Expenses:Bills:Phone : Expenses:Bills:Light : Expenses:Bills:Rent : Expenses:Bills:PublicTransport : Expenses:Bills:Subscriptions : Newspaper, magazine, web service subscriptions. Expenses:Bills:Union : Expenses:Books : Expenses:Car : Expenses:Car:Fuel : Expenses:Car:Insurance : Expenses:Car:Repair : Expenses:Car:Taxes : Expenses:Car:Tickets : Expenses:Clothes : Expenses:Donations : Expenses:Financial : Expenses related to financial operations or account maintenance. Expenses:Games : Expenses:Games:Steam : Expenses:Games:HumbleBundle : Expenses:Games:GOG : Expenses:Games:Itchio : Expenses:Gifts : Expenses:Gifts:Person1 : Expenses:Gifts:Person2 : Expenses:Gifts:Mine : Expenses:Groceries : Expenses:Groceries:Extras : Expenses:Groceries:Supermarket : Expenses:Groceries:Prepared : Expenses:Groceries:GreenGrocery : Expenses:Hardware : Expenses:Home : Expenses:Home:WashingMachine : Expenses:Home:DishWasher : Expenses:Home:Fridge : Expenses:Legal : Expenses:Medicines : Expenses:Social : Expenses:Social:Eat : Expenses:Social:Drink : Expenses:Taxes : Expenses:Taxes:Tax1 : Expenses:Taxes:Tax2 : Expenses:Trips : Expenses:Trips:Accommodation : Expenses:Trips:Drink : Expenses:Trips:Food : Expenses:Trips:Tickets : Expenses:Trips:Transport : Expenses:Work : Expenses:Work:Phone : Expenses:Work:Hardware : Expenses:Work:Trips : Expenses:Work:Trips:Accommodation : Expenses:Work:Trips:Drink : Expenses:Work:Trips:Food : Expenses:Work:Trips:Tickets : Expenses:Work:Trips:Transport :","title":"Expenses"},{"location":"money_management/#initialization-of-accounts","text":"# Initialization 2010-05-17 pad Assets:Cash Equity:Opening-Balances 2016-01-01 pad Assets:Debt:Person1 Equity:Opening-Balances","title":"Initialization of accounts"},{"location":"money_management/#transfer-includes","text":"I reference each year's year.book and the .closed.accounts.book . # Transfers include \".closed.accounts.book\" include \"2011/year.book\" ... include \"2020/year.book\"","title":"Transfer includes"},{"location":"money_management/#the-monthly-book","text":"Each month has a file with this structure: # Cash transfers # CashFlowCard # Savings # Balances taken at 2020-12-06T19:32 ## Active accounts 2020-12-06 balance Assets:Cashbox:Paper EUR 2020-12-06 balance Assets:Cashbox:Coins EUR 2020-12-06 balance Assets:Cash:Paper EUR 2020-12-06 balance Assets:Cash:Coins EUR 2020-12-06 balance Assets:CashFlowCard EUR 2020-12-06 balance Assets:Savings EUR ## Deposits 2020-12-06 balance Assets:Savings:CashFlowRefiller XXX EUR 2020-12-06 balance Assets:Savings:UnexpectedExpenses XXX EUR 2020-12-06 balance Assets:CashDeposit XXX EUR ## Debts 2020-12-06 balance Assets:Debt:Person1 XXX EUR ## Equity # 2020-12-05 pad Assets:Cash Equity:Errors # 2020-12-05 pad Assets:Cashbox Equity:Errors # Weekly balances ## Measure done on 2020-09-04T17:10 2020-09-04 balance Assets:Cash:Coins XXX EUR 2020-09-04 balance Assets:Cash:Paper XXX EUR 2020-09-04 balance Assets:Cashbox:Coins XXX EUR 2020-09-04 balance Assets:Cashbox:Paper XXX EUR Where each section stores: Cash transfers : The transactions done by cash, extracted from the Android cone application. CashFlowCard : Bank account extracts transformed from the csv to postings with bean-extract . Savings : Bank account extracts transformed from the csv to postings with bean-extract . Monthly balances : I try to review the accounts once each month. This section is subdivided in: Active accounts : The accounts whose value changes monthly. Deposits : The accounts that don't change much each month. Debts : The balance of debt accounts. Equity : The pad statements to track the errors in the monthly account. Weekly balances : As doing the monthly review is long, but it doesn't give me the enough information to not mess up the cash transactions, I do a weekly balance of those accounts.","title":"The monthly book"},{"location":"monitoring_comparison/","text":"As with any technology, when you want to adopt it, you first need to analyze your options. In this article we're going to compare the two most popular solutions at the moment, Nagios and Prometheus. Zabbix is similar in architecture and features to Nagios, so for the first iteration we're going to skip it. TL;DR: Prometheus is better, but it needs more effort. Nagios is suitable for basic monitoring of small and/or static systems where blackbox probing is sufficient. If you want to do whitebox monitoring, or have a dynamic or cloud based environment, then Prometheus is a good choice. Nagios \u2691 Nagios is an industry leader in IT infrastructure monitoring. It has four different products to choose from: Nagios XI: Is an enterprise-ready server and network monitoring system that supplies data to track app or network infrastructure health, performance, availability, of the components, protocols, and services. It has a user-friendly interface that allows UI configuration, customized visualizations, and alert preferences. Nagios Log Server: It's used for log management and analysis of user scenarios. It has the ability to correlate logged events across different services and servers in real time, which helps with the investigation of incidents and the performance of root cause analysis. Because Nagios Log Server\u2019s design is specifically for network security and audits, it lets users generate alerts for suspicious operations and commands. Log Server retains historical data from all events, supplying organizations with everything they need to pass a security audit. Nagios Network Analyzer: It's a tool for collecting and displaying either metrics or extra information about an application network. It identifies which IPs are communicating with the application servers and what requests they\u2019re sending. The Network Analyzer maintains a record of all server traffic, including who connected a specific server, to a specific port and the specific request. This helps plan out server and network capacity, plus understand various kinds of security breaches likes unauthorized access, data leaks, DDoS, and viruses or malwares on servers. Nagios Fusion: is a compilation of the three tools Nagios offers. It provides a complete solution that assists businesses in satisfying any and all of their monitoring requirements. Its design is for scalability and for visibility of the application and all of its dependencies. Prometheus \u2691 Prometheus is a free software application used for event monitoring and alerting. It records real-time metrics in a time series database (allowing for high dimensionality) built using a HTTP pull model, with flexible queries and real-time alerting. The project is written in Go and licensed under the Apache 2 License, with source code available on GitHub, and is a graduated project of the Cloud Native Computing Foundation, along with Kubernetes and Envoy. At the core of the Prometheus monitoring system is the main server, which ingests samples from monitoring targets. A target is any application that exposes metrics according to the open specification understood by Prometheus. Since Prometheus pulls data, rather than expecting targets to actively push stats into the monitoring system, it supports a variety of service discovery integrations, like that with Kubernetes, to immediately adapt to changes in the set of targets. The second core component is the Alertmanager, implementing the idea of time series based alerting. It intelligently removes duplicate alerts sent by Prometheus servers, groups the alerts into informative notifications, and dispatches them to a variety of integrations, like those with PagerDuty and Slack. It also handles silencing of selected alerts and advanced routing configurations for notifications. There are several additional Prometheus components, such as client libraries for different programming languages, and a growing number of exporters. Exporters are small programs that provide Prometheus compatible metrics from systems that are not natively instrumented. Comparison \u2691 For each dimension we'll check how each solution meets the criteria. An aggregation of all the results can be found in the summary . Open source \u2691 Only the Nagios Core is open sourced , it provides basic monitoring but it's enhanced by community contributions . It's also the base of the rest solutions, which are proprietary. Prometheus is completely open source under the Apache 2.0 license. Community \u2691 In Nagios, only the Nagios Core is an open-source tool. The rest are proprietary, so there is no community behind them. Community contributions to Nagios are gathered in the Nagios Exchange , it's hard to get other activity statistics than the overall number of contributions, but there are more than 850 addons, 4.5k plugins and 300 documentation contributions. Overall metrics (2021-02-22): Metric Nagios Core Prometheus Stars 932 35.4k Forks 341 5.7k Watch 121 1.2k Commits 3.4k 8.5k Open Issues 195 290 Closed Issues 455 3.5k Open PR 9 116 Closed PR 155 4.5k Last month metrics (2021-02-22): Metric Nagios Core Prometheus Active PR 1 80 Active Issues 3 64 Commits 0 74 Authors 0 35 We can see that Prometheus in comparison with Nagios Core is: More popular in terms of community contributions. More maintained. Growing more. Development is more distributed. Manages the issues collaboratively. This comparison is biased though, because Nagios comes from a time where GitHub and Git (and Youtube!) did not exist, and the communities formed around different sites. Also, given that Nagios has almost 20 years of existence, and that it forked from a previous monitoring project (NetSaint), the low number contributions indicate a stable and mature product, whereas the high numbers for Prometheus are indicators of a young, still in development product. Keep in mind that this comparison only analyzes the core, it doesn't take into account the metrics of the community contributions, as it is not easy to aggregate their statistics. Which makes Prometheus one of the biggest open-source projects in existence. It actually has hundreds of contributors maintaining it. The tool continues to be up-to-date to contemporary and popular apps, extending its list of exporters and responding to requests. On 16 January 2014 , Nagios Enterprises redirected the nagios-plugins.org domain to a web server controlled by Nagios Enterprises without explicitly notifying the Nagios Plugins community team the consequences of their actions. Nagios Enterprises replaced the nagios-plugins team with a group of new, different members. The community team members who were replaced continued their work under the name Monitoring Plugins along with a new website with the new domain of monitoring-plugins.org. Which is a nasty move against the community. Configuration and usage \u2691 Neither solution is easy to configure, you need to invest time in them. Nagios is easier to use for non technical users though. Visualizations \u2691 The graphs and dashboards Prometheus provides don't meet today's needs. As a result, users resort to other visualization tools to display metrics collected by Prometheus, often Grafana. Nagios comes with a set of dashboards that fit the requirements of monitoring networks and infrastructure components. Yet, it still lacks graphs for more applicative-related issues. Personally I find Grafana dashboards more beautiful and easier to change. It also has a massive community behind providing customizable dashboards for free. Installation \u2691 Nagios comes as a downloadable bundle with dedicated packages for every product with Windows or Linux distributions. After downloading and installing the tool, a set of first-time configurations is required. Once you\u2019ve installed the Nagios agents, data should start streaming into Nagios and its generic dashboards. Prometheus deployment is done through Docker containers that can spin up on every machine type, or through pre-compiled or self-compiled binaries. There are community maintained ansible roles for both solutions, doing a quick search I've found a Prometheus one that it's more maintained. For Kubernetes installation, I've only found helm charts for Prometheus. Kubernetes integration \u2691 Prometheus, as Kubernetes are leading projects of the Cloud Native Computing Foundation , which is a Linux Foundation project that was founded in 2015 to help advance container technology and align the tech industry around its evolution. Prometheus has native support to be run in and to monitor Kubernetes clusters. Although Nagios can monitor Kubernetes, it's not meant to be run inside it. Documentation \u2691 I haven't used much the Nagios documentation , but I can tell you that even though it's improving Prometheus ' is not very complete, and you find yourself often looking at issues and stackoverflow. Integrations \u2691 Official Prometheus\u2019 integrations are practically boundless . The long list of existing exporters combined with the user\u2019s ability to write new exporters allows integration with any tool, and PromQL allows users to query Prometheus data from any visualization tool that supports it. Nagios has a very limited list of official integrations . Most of them are operating systems which use the agents to monitor other network components. Others include MongoDB, Oracle, Selenium, and VMware. Once again, the community comes to rescue us with their contributions , keep in mind that you'll need to dive into the exchange for special monitoring needs. Alerts \u2691 Prometheus offers Alertmanager, a simple service that allows users to set thresholds and push alerts when breaches occur. Nagios uses a variety of media channels for alerts, including email, SMS, and audio alerts. Because its integration with the operating system is swift, Nagios even knows to generate a WinPopup message with the alert details. On a side note, there is an alert Nagios plugin that alerts for Prometheus query results. As Nagios doesn't support labels for the metrics, so there is no grouping, routing or deduplication of alerts as Prometheus do. Also the silence of alerts is done individually on each alert, while in Prometheus it's done using labels, which is more powerful. Advanced monitorization \u2691 Nagios alerting is based on the return codes of scripts, Prometheus on the other hand alerts based on metrics, this fact together with the easy and powerful query language PromQL allows the user to make much more rich alerts that better represent the state of the system to monitor. In Nagios there is no concept of making queries to the gathered data. Data storage \u2691 Nagios has no storage per-se, beyond the current check state. There are plugins which can store data such as for visualisation . Prometheus has a defined amount of data that's available (for example 30 days), to be able to store more you need to use Thanos, the prometheus long term storage solution. High availability \u2691 Nagios servers are standalone, they are not meant to collaborate with other instances, so to achieve high availability you need to do it the old way, with multiple independent instances with a loadbalancer upfront. Prometheus can have different servers running collaboratively, monitoring between themselves. So you get high availability for free without any special configuration. Dynamic infrastructure \u2691 In the past, infrastructure had a low rate of change, it was strange that you needed to add something to the monitorization system. Nowadays, with cloud infrastructures and kubernetes, instances are spawned and killed continuously. In Nagios, you need to manually configure each new service following the push architecture. In prometheus, thanks to the pull architecture and service discovery, new services are added and dead one removed automatically. Custom script execution \u2691 Nagios alerting is based on the return codes of scripts, therefore it's straightforward to create an alert based on a custom script. If you need to monitor something in Prometheus, and nobody has done it before, the development costs of an ad-hoc solutions are incredibly high, compared to Nagios. You'd need either to: Use the script_exporter with your script. I've seen their repo, and the last commit is from March, and they don't have a helm chart to install it . I've searched other alternative exporters, but this one seems to be the best for this approach. The advantages of this approach is that you don't need to create and maintain a new prometheus exporter. The disadvantages though are that you'd have to: Manually install the required exporter resources in the cluster until a helm chart exists. Create the helm charts yourself if they don't develop it. Integrate your tool inside the script_exporter docker through one of these ways: Changing the exporter Docker image to add it. Which would mean a Docker image to maintain. Mounting the binary through a volume inside kubernetes. Which would mean defining a way on how to upload it and assume the high availability penalty that a stateful kubernetes service entail with the cluster configuration right now. If it's not already in your stack, it would mean adding a new exporter to maintain and a new development team to depend on. Alternatively you can use the script exporter binary in a baremetal or virtualized server instead of using a docker, that way you wouldn't need to maintain the different dockers for the different solutions, but you'd need a \"dedicated\" server for this purpose. Create your own exporter. You'd need to create a docker that exposes the command line functionality through a metrics endpoint. You wouldn't depend on a third party development team and would be able to use your script. On the other side it has the following disadvantages: We would need to create and maintain a new prometheus exporter. That would mean creating and maintaining the Docker with the command line tool and a simple http server that exposes the /metrics endpoint, that will run the command whenever the Prometheus server accesses this endpoint. We add a new exporter to maintain but we develop it ourselves, so we don't depend on third party developers. Use other exporters to do the check. For example, if you can deduce the critical API call that will decide if the script fails or succeeds, you could use the blackbox exporter to monitor it instead. The advantages of this solution are: We don't add new infrastructure to develop or maintain. We don't depend on third party development teams. And the disadvantage is that if the logic changes, we would need to update how we do the check. Network monitorization \u2691 Both can use the Simple Network Management Protocol (SNMP) to communicate with network switches or other components by using SNMP protocol to query their status. Not being an expert on the topic, knowing it's been one of the core focus of Nagios in the past years and as I've not been able to find good comparison between both, I'm going to suppose that even though both support network monitoring, Nagios does a better job. Summary \u2691 Metric Nagios Prometheus Open Source \u2713* \u2713\u2713 Community \u2713 \u2713\u2713 Configuration and usage \u2713 x Visualizations \u2713 \u2713\u2713 Ansible Role \u2713 \u2713\u2713 Helm chart x \u2713 Kubernetes x \u2713 Documentation \u2713 x Integrations \u2713 \u2713\u2713 Alerts \u2713 \u2713\u2713 Advanced monitoring x \u2713 Custom script execution \u2713\u2713 \u2713 Data storage x \u2713 Dynamic infrastructure x \u2713 High availability \u2713 \u2713\u2713 Network Monitoring \u2713 \u2713 * Only Nagios Core and the community contributions are open sourced. Where each symbol means: x: Doesn't meet the criteria. \u2713: Meets the criteria. \u2713\u2713: Meets the criteria and it's better than the other solution. ?: I'm not sure. Nagios is the reference of the old-school monitoring solutions, suitable for basic monitoring of small, static and/or old-school systems where blackbox probing is sufficient. Prometheus is the reference of the new-wave monitoring solutions, suitable for more advanced monitoring of dynamic, new-wave systems (web applications, cloud, containers or Kubernetes) where whitebox monitoring is desired. References \u2691 Logz io post on Prometheus vs Nagios","title":"Monitoring Comparison"},{"location":"monitoring_comparison/#nagios","text":"Nagios is an industry leader in IT infrastructure monitoring. It has four different products to choose from: Nagios XI: Is an enterprise-ready server and network monitoring system that supplies data to track app or network infrastructure health, performance, availability, of the components, protocols, and services. It has a user-friendly interface that allows UI configuration, customized visualizations, and alert preferences. Nagios Log Server: It's used for log management and analysis of user scenarios. It has the ability to correlate logged events across different services and servers in real time, which helps with the investigation of incidents and the performance of root cause analysis. Because Nagios Log Server\u2019s design is specifically for network security and audits, it lets users generate alerts for suspicious operations and commands. Log Server retains historical data from all events, supplying organizations with everything they need to pass a security audit. Nagios Network Analyzer: It's a tool for collecting and displaying either metrics or extra information about an application network. It identifies which IPs are communicating with the application servers and what requests they\u2019re sending. The Network Analyzer maintains a record of all server traffic, including who connected a specific server, to a specific port and the specific request. This helps plan out server and network capacity, plus understand various kinds of security breaches likes unauthorized access, data leaks, DDoS, and viruses or malwares on servers. Nagios Fusion: is a compilation of the three tools Nagios offers. It provides a complete solution that assists businesses in satisfying any and all of their monitoring requirements. Its design is for scalability and for visibility of the application and all of its dependencies.","title":"Nagios"},{"location":"monitoring_comparison/#prometheus","text":"Prometheus is a free software application used for event monitoring and alerting. It records real-time metrics in a time series database (allowing for high dimensionality) built using a HTTP pull model, with flexible queries and real-time alerting. The project is written in Go and licensed under the Apache 2 License, with source code available on GitHub, and is a graduated project of the Cloud Native Computing Foundation, along with Kubernetes and Envoy. At the core of the Prometheus monitoring system is the main server, which ingests samples from monitoring targets. A target is any application that exposes metrics according to the open specification understood by Prometheus. Since Prometheus pulls data, rather than expecting targets to actively push stats into the monitoring system, it supports a variety of service discovery integrations, like that with Kubernetes, to immediately adapt to changes in the set of targets. The second core component is the Alertmanager, implementing the idea of time series based alerting. It intelligently removes duplicate alerts sent by Prometheus servers, groups the alerts into informative notifications, and dispatches them to a variety of integrations, like those with PagerDuty and Slack. It also handles silencing of selected alerts and advanced routing configurations for notifications. There are several additional Prometheus components, such as client libraries for different programming languages, and a growing number of exporters. Exporters are small programs that provide Prometheus compatible metrics from systems that are not natively instrumented.","title":"Prometheus"},{"location":"monitoring_comparison/#comparison","text":"For each dimension we'll check how each solution meets the criteria. An aggregation of all the results can be found in the summary .","title":"Comparison"},{"location":"monitoring_comparison/#open-source","text":"Only the Nagios Core is open sourced , it provides basic monitoring but it's enhanced by community contributions . It's also the base of the rest solutions, which are proprietary. Prometheus is completely open source under the Apache 2.0 license.","title":"Open source"},{"location":"monitoring_comparison/#community","text":"In Nagios, only the Nagios Core is an open-source tool. The rest are proprietary, so there is no community behind them. Community contributions to Nagios are gathered in the Nagios Exchange , it's hard to get other activity statistics than the overall number of contributions, but there are more than 850 addons, 4.5k plugins and 300 documentation contributions. Overall metrics (2021-02-22): Metric Nagios Core Prometheus Stars 932 35.4k Forks 341 5.7k Watch 121 1.2k Commits 3.4k 8.5k Open Issues 195 290 Closed Issues 455 3.5k Open PR 9 116 Closed PR 155 4.5k Last month metrics (2021-02-22): Metric Nagios Core Prometheus Active PR 1 80 Active Issues 3 64 Commits 0 74 Authors 0 35 We can see that Prometheus in comparison with Nagios Core is: More popular in terms of community contributions. More maintained. Growing more. Development is more distributed. Manages the issues collaboratively. This comparison is biased though, because Nagios comes from a time where GitHub and Git (and Youtube!) did not exist, and the communities formed around different sites. Also, given that Nagios has almost 20 years of existence, and that it forked from a previous monitoring project (NetSaint), the low number contributions indicate a stable and mature product, whereas the high numbers for Prometheus are indicators of a young, still in development product. Keep in mind that this comparison only analyzes the core, it doesn't take into account the metrics of the community contributions, as it is not easy to aggregate their statistics. Which makes Prometheus one of the biggest open-source projects in existence. It actually has hundreds of contributors maintaining it. The tool continues to be up-to-date to contemporary and popular apps, extending its list of exporters and responding to requests. On 16 January 2014 , Nagios Enterprises redirected the nagios-plugins.org domain to a web server controlled by Nagios Enterprises without explicitly notifying the Nagios Plugins community team the consequences of their actions. Nagios Enterprises replaced the nagios-plugins team with a group of new, different members. The community team members who were replaced continued their work under the name Monitoring Plugins along with a new website with the new domain of monitoring-plugins.org. Which is a nasty move against the community.","title":"Community"},{"location":"monitoring_comparison/#configuration-and-usage","text":"Neither solution is easy to configure, you need to invest time in them. Nagios is easier to use for non technical users though.","title":"Configuration and usage"},{"location":"monitoring_comparison/#visualizations","text":"The graphs and dashboards Prometheus provides don't meet today's needs. As a result, users resort to other visualization tools to display metrics collected by Prometheus, often Grafana. Nagios comes with a set of dashboards that fit the requirements of monitoring networks and infrastructure components. Yet, it still lacks graphs for more applicative-related issues. Personally I find Grafana dashboards more beautiful and easier to change. It also has a massive community behind providing customizable dashboards for free.","title":"Visualizations"},{"location":"monitoring_comparison/#installation","text":"Nagios comes as a downloadable bundle with dedicated packages for every product with Windows or Linux distributions. After downloading and installing the tool, a set of first-time configurations is required. Once you\u2019ve installed the Nagios agents, data should start streaming into Nagios and its generic dashboards. Prometheus deployment is done through Docker containers that can spin up on every machine type, or through pre-compiled or self-compiled binaries. There are community maintained ansible roles for both solutions, doing a quick search I've found a Prometheus one that it's more maintained. For Kubernetes installation, I've only found helm charts for Prometheus.","title":"Installation"},{"location":"monitoring_comparison/#kubernetes-integration","text":"Prometheus, as Kubernetes are leading projects of the Cloud Native Computing Foundation , which is a Linux Foundation project that was founded in 2015 to help advance container technology and align the tech industry around its evolution. Prometheus has native support to be run in and to monitor Kubernetes clusters. Although Nagios can monitor Kubernetes, it's not meant to be run inside it.","title":"Kubernetes integration"},{"location":"monitoring_comparison/#documentation","text":"I haven't used much the Nagios documentation , but I can tell you that even though it's improving Prometheus ' is not very complete, and you find yourself often looking at issues and stackoverflow.","title":"Documentation"},{"location":"monitoring_comparison/#integrations","text":"Official Prometheus\u2019 integrations are practically boundless . The long list of existing exporters combined with the user\u2019s ability to write new exporters allows integration with any tool, and PromQL allows users to query Prometheus data from any visualization tool that supports it. Nagios has a very limited list of official integrations . Most of them are operating systems which use the agents to monitor other network components. Others include MongoDB, Oracle, Selenium, and VMware. Once again, the community comes to rescue us with their contributions , keep in mind that you'll need to dive into the exchange for special monitoring needs.","title":"Integrations"},{"location":"monitoring_comparison/#alerts","text":"Prometheus offers Alertmanager, a simple service that allows users to set thresholds and push alerts when breaches occur. Nagios uses a variety of media channels for alerts, including email, SMS, and audio alerts. Because its integration with the operating system is swift, Nagios even knows to generate a WinPopup message with the alert details. On a side note, there is an alert Nagios plugin that alerts for Prometheus query results. As Nagios doesn't support labels for the metrics, so there is no grouping, routing or deduplication of alerts as Prometheus do. Also the silence of alerts is done individually on each alert, while in Prometheus it's done using labels, which is more powerful.","title":"Alerts"},{"location":"monitoring_comparison/#advanced-monitorization","text":"Nagios alerting is based on the return codes of scripts, Prometheus on the other hand alerts based on metrics, this fact together with the easy and powerful query language PromQL allows the user to make much more rich alerts that better represent the state of the system to monitor. In Nagios there is no concept of making queries to the gathered data.","title":"Advanced monitorization"},{"location":"monitoring_comparison/#data-storage","text":"Nagios has no storage per-se, beyond the current check state. There are plugins which can store data such as for visualisation . Prometheus has a defined amount of data that's available (for example 30 days), to be able to store more you need to use Thanos, the prometheus long term storage solution.","title":"Data storage"},{"location":"monitoring_comparison/#high-availability","text":"Nagios servers are standalone, they are not meant to collaborate with other instances, so to achieve high availability you need to do it the old way, with multiple independent instances with a loadbalancer upfront. Prometheus can have different servers running collaboratively, monitoring between themselves. So you get high availability for free without any special configuration.","title":"High availability"},{"location":"monitoring_comparison/#dynamic-infrastructure","text":"In the past, infrastructure had a low rate of change, it was strange that you needed to add something to the monitorization system. Nowadays, with cloud infrastructures and kubernetes, instances are spawned and killed continuously. In Nagios, you need to manually configure each new service following the push architecture. In prometheus, thanks to the pull architecture and service discovery, new services are added and dead one removed automatically.","title":"Dynamic infrastructure"},{"location":"monitoring_comparison/#custom-script-execution","text":"Nagios alerting is based on the return codes of scripts, therefore it's straightforward to create an alert based on a custom script. If you need to monitor something in Prometheus, and nobody has done it before, the development costs of an ad-hoc solutions are incredibly high, compared to Nagios. You'd need either to: Use the script_exporter with your script. I've seen their repo, and the last commit is from March, and they don't have a helm chart to install it . I've searched other alternative exporters, but this one seems to be the best for this approach. The advantages of this approach is that you don't need to create and maintain a new prometheus exporter. The disadvantages though are that you'd have to: Manually install the required exporter resources in the cluster until a helm chart exists. Create the helm charts yourself if they don't develop it. Integrate your tool inside the script_exporter docker through one of these ways: Changing the exporter Docker image to add it. Which would mean a Docker image to maintain. Mounting the binary through a volume inside kubernetes. Which would mean defining a way on how to upload it and assume the high availability penalty that a stateful kubernetes service entail with the cluster configuration right now. If it's not already in your stack, it would mean adding a new exporter to maintain and a new development team to depend on. Alternatively you can use the script exporter binary in a baremetal or virtualized server instead of using a docker, that way you wouldn't need to maintain the different dockers for the different solutions, but you'd need a \"dedicated\" server for this purpose. Create your own exporter. You'd need to create a docker that exposes the command line functionality through a metrics endpoint. You wouldn't depend on a third party development team and would be able to use your script. On the other side it has the following disadvantages: We would need to create and maintain a new prometheus exporter. That would mean creating and maintaining the Docker with the command line tool and a simple http server that exposes the /metrics endpoint, that will run the command whenever the Prometheus server accesses this endpoint. We add a new exporter to maintain but we develop it ourselves, so we don't depend on third party developers. Use other exporters to do the check. For example, if you can deduce the critical API call that will decide if the script fails or succeeds, you could use the blackbox exporter to monitor it instead. The advantages of this solution are: We don't add new infrastructure to develop or maintain. We don't depend on third party development teams. And the disadvantage is that if the logic changes, we would need to update how we do the check.","title":"Custom script execution"},{"location":"monitoring_comparison/#network-monitorization","text":"Both can use the Simple Network Management Protocol (SNMP) to communicate with network switches or other components by using SNMP protocol to query their status. Not being an expert on the topic, knowing it's been one of the core focus of Nagios in the past years and as I've not been able to find good comparison between both, I'm going to suppose that even though both support network monitoring, Nagios does a better job.","title":"Network monitorization"},{"location":"monitoring_comparison/#summary","text":"Metric Nagios Prometheus Open Source \u2713* \u2713\u2713 Community \u2713 \u2713\u2713 Configuration and usage \u2713 x Visualizations \u2713 \u2713\u2713 Ansible Role \u2713 \u2713\u2713 Helm chart x \u2713 Kubernetes x \u2713 Documentation \u2713 x Integrations \u2713 \u2713\u2713 Alerts \u2713 \u2713\u2713 Advanced monitoring x \u2713 Custom script execution \u2713\u2713 \u2713 Data storage x \u2713 Dynamic infrastructure x \u2713 High availability \u2713 \u2713\u2713 Network Monitoring \u2713 \u2713 * Only Nagios Core and the community contributions are open sourced. Where each symbol means: x: Doesn't meet the criteria. \u2713: Meets the criteria. \u2713\u2713: Meets the criteria and it's better than the other solution. ?: I'm not sure. Nagios is the reference of the old-school monitoring solutions, suitable for basic monitoring of small, static and/or old-school systems where blackbox probing is sufficient. Prometheus is the reference of the new-wave monitoring solutions, suitable for more advanced monitoring of dynamic, new-wave systems (web applications, cloud, containers or Kubernetes) where whitebox monitoring is desired.","title":"Summary"},{"location":"monitoring_comparison/#references","text":"Logz io post on Prometheus vs Nagios","title":"References"},{"location":"mopidy/","text":"Mopidy is an extensible music server written in Python. The key features are: Plays music from many sources: local disk, Spotify, SoundCloud, Google Play Music, and more. Can be used as a server: Out of the box, Mopidy is an HTTP server. If you install the Mopidy-MPD extension, it becomes an MPD server too. Given that MPD is a popular, old, and robust solution, you can benefit of the many solutions that exist out there for MPD. Edit the playlist from any phone, tablet, or computer using a variety of MPD and web clients. It supports Beets as a library source. Is hackable: The awesome documentation, being Python based, the extension system, JSON-RPC, and JavaScript APIs make Mopidy a perfect base for your projects. References \u2691 Docs Git Home Developer info \u2691 API reference Write your own extension","title":"Mopidy"},{"location":"mopidy/#references","text":"Docs Git Home","title":"References"},{"location":"mopidy/#developer-info","text":"API reference Write your own extension","title":"Developer info"},{"location":"networkx/","text":"NetworkX is a Python package for the creation, manipulation, and study of the structure, dynamics, and functions of complex networks. References \u2691 Docs Git Home","title":"NetworkX"},{"location":"networkx/#references","text":"Docs Git Home","title":"References"},{"location":"notmuch/","text":"notmuch is a command-line based program for indexing, searching, reading, and tagging large collections of email messages. Installation \u2691 In order to use Notmuch, you will need to have your email messages stored in your local filesystem, one message per file. You can use mbsync to do that. sudo apt-get install notmuch Configuration \u2691 To configure Notmuch, just run notmuch This will interactively guide you through the setup process, and save the configuration to ~/.notmuch-config . If you'd like to change the configuration in the future, you can either edit that file directly, or run notmuch setup . If you plan to use afew set the tags to new . To test everything works as expected, and create a database that indexes all of your mail run: notmuch new References \u2691 Docs","title":"notmuch"},{"location":"notmuch/#installation","text":"In order to use Notmuch, you will need to have your email messages stored in your local filesystem, one message per file. You can use mbsync to do that. sudo apt-get install notmuch","title":"Installation"},{"location":"notmuch/#configuration","text":"To configure Notmuch, just run notmuch This will interactively guide you through the setup process, and save the configuration to ~/.notmuch-config . If you'd like to change the configuration in the future, you can either edit that file directly, or run notmuch setup . If you plan to use afew set the tags to new . To test everything works as expected, and create a database that indexes all of your mail run: notmuch new","title":"Configuration"},{"location":"notmuch/#references","text":"Docs","title":"References"},{"location":"oracle_database/","text":"Oracle Database is an awful proprietary database, run away from it! Install \u2691 Download or clone the files of their docker repository . Create an account in their page to be able to download the required binary files . Fake person generator might come handy for this step. Download the files . After downloading the file we need to copy it to the folder referring to the oracle version in the cloned folder. In this case, 19.3.0: mv ~/Download/LINUX.X64_193000_db_home.zip ./docker-images/OracleDatabase/SingleInstance/dockerfiles/19.3.0/ The next step is to build the image. You need at least 20G free in /var/lib/docker . ./docker-images/OracleDatabase/SingleInstance/dockerfiles/19.3.0/buildDockerImage.sh -v 19 .3.0 -e Confirm that the image was created docker images REPOSITORY TAG IMAGE ID CREATED SIZE oracle/database 19 .3.0-ee d8be8934332d 53 minutes ago 6 .54GB Run the database docker. docker run --name myOracle1930 \\ -p 127 .0.0.1:1521:1521 \\ -p 127 .0.0.1:5500:5500 \\ -e ORACLE_SID = ORCLCDB \\ -e ORACLE_PDB = ORCLPDB1 \\ -e ORACLE_PWD = root \\ -e INIT_SGA_SIZE = 1024 \\ -e INIT_PGA_SIZE = 1024 \\ -e ORACLE_CHARACTERSET = AL32UTF8 \\ oracle/database:19.3.0-ee","title":"Oracle Database"},{"location":"oracle_database/#install","text":"Download or clone the files of their docker repository . Create an account in their page to be able to download the required binary files . Fake person generator might come handy for this step. Download the files . After downloading the file we need to copy it to the folder referring to the oracle version in the cloned folder. In this case, 19.3.0: mv ~/Download/LINUX.X64_193000_db_home.zip ./docker-images/OracleDatabase/SingleInstance/dockerfiles/19.3.0/ The next step is to build the image. You need at least 20G free in /var/lib/docker . ./docker-images/OracleDatabase/SingleInstance/dockerfiles/19.3.0/buildDockerImage.sh -v 19 .3.0 -e Confirm that the image was created docker images REPOSITORY TAG IMAGE ID CREATED SIZE oracle/database 19 .3.0-ee d8be8934332d 53 minutes ago 6 .54GB Run the database docker. docker run --name myOracle1930 \\ -p 127 .0.0.1:1521:1521 \\ -p 127 .0.0.1:5500:5500 \\ -e ORACLE_SID = ORCLCDB \\ -e ORACLE_PDB = ORCLPDB1 \\ -e ORACLE_PWD = root \\ -e INIT_SGA_SIZE = 1024 \\ -e INIT_PGA_SIZE = 1024 \\ -e ORACLE_CHARACTERSET = AL32UTF8 \\ oracle/database:19.3.0-ee","title":"Install"},{"location":"origami/","text":"Origami , is the art of paper folding, it comes from ori meaning \"folding\", and kami meaning \"paper\" (kami changes to gami due to rendaku)). In modern usage, the word \"origami\" is used as an inclusive term for all folding practices, regardless of their culture of origin. The goal is to transform a flat square sheet of paper into a finished sculpture through folding and sculpting techniques. Modern origami practitioners generally discourage the use of cuts, glue, or markings on the paper. References \u2691 Mark1626 Digital garden origami section , for example the Clover and Hydrangea Tesslation .","title":"Origami"},{"location":"origami/#references","text":"Mark1626 Digital garden origami section , for example the Clover and Hydrangea Tesslation .","title":"References"},{"location":"osmand/","text":"OsmAnd is a mobile application for global map viewing and navigating based on OpenStreetMaps . Perfect if you're looking for a privacy focused, community maintained open source alternative to google maps. Issues \u2691 Live map update not working : test that it works. Add amenity to favourites : Once it's done, remove the \"Restaurant\", or any other amenity words from the name of the favourite. Search within favourites description : Nothing to do. Favourites inherit the POI data : Nothing to do. References \u2691 Home Git Reddit","title":"OsmAnd"},{"location":"osmand/#issues","text":"Live map update not working : test that it works. Add amenity to favourites : Once it's done, remove the \"Restaurant\", or any other amenity words from the name of the favourite. Search within favourites description : Nothing to do. Favourites inherit the POI data : Nothing to do.","title":"Issues"},{"location":"osmand/#references","text":"Home Git Reddit","title":"References"},{"location":"outrun/","text":"Outrun lets you execute a local command using the processing power of another Linux machine. Installation \u2691 pip install outrun References \u2691 Git","title":"Outrun"},{"location":"outrun/#installation","text":"pip install outrun","title":"Installation"},{"location":"outrun/#references","text":"Git","title":"References"},{"location":"peek/","text":"Peek is a simple animated GIF screen recorder with an easy to use interface. Installation \u2691 sudo apt-get install peek If you try to use it with i3, you're going to have a bad time, you'd need to install Compton , and then the elements may not even be clickable . With kitty it works though :) References \u2691 Git","title":"Peek"},{"location":"peek/#installation","text":"sudo apt-get install peek If you try to use it with i3, you're going to have a bad time, you'd need to install Compton , and then the elements may not even be clickable . With kitty it works though :)","title":"Installation"},{"location":"peek/#references","text":"Git","title":"References"},{"location":"personal_interruption_analysis/","text":"This is the interruption analysis report applied to my personal life. I've identified the next interruption sources: Physical interruptions . Emails . Calls . Instant message applications . Calendar events . Other desktop notifications . Physical interruptions \u2691 The analysis is similar to the work physical interruptions . Emails \u2691 Email can be used as one of the main aggregators of interruptions as it's supported by almost everything. I use it as the notification of things that don't need to be acted upon immediately or when more powerful mechanisms are not available. In my case emails can be categorized as: Bill or bank receipt emails: I receive at least one per provider per month, the associated action is to download the attached pdf and remove the email. I've got it automated using a Python program that I need to manually run. In the future I expect it to be done automatically without my interaction . There is no urgency to act on them. General information: In the past I was subscribed to newsletters, now I prefer to use RSS. They don't usually require any direct action, so they can wait more than two days. Videogame deals: I was subscribed to Humblebundle, GOG and Steam notifications to be notified on the deals, but then I migrated to IsThereAnyDeal because it only sends the notifications of the deals that match a defined criteria (reducing the amount of emails), and monitors all sites in one place. I can act on them with one or two days of delay. Source code manager notifications: The web where I host my source code sends me emails when there are new pull requests or when there are comments on existent ones. I try to act on them daily. The CI sends notifications when some job fails. Unless it's a new pipeline or I'm actively working on it, a failed work job can wait many days broken before I need to interact with ithe. Infrastructure notifications: For example LetsEncrypt renewals or cloud provider notification or support cases. The related actions can wait a day or more. Monitorization notifications: I've configured Prometheus's alertmanager to send the notifications to the email as a fallback channel, but it's to be checked only if the main channel is down. Stranger emails: People whom I don't know that contacts me asking questions. These can be dealt with daily. In conclusion, I can check the personal emails twice a day, one after breakfast and another in the middle of the afternoon. So its safe to disable the notifications. I'm eager to start the email automation project so I can spend even less time and willpower managing the email. Calls \u2691 People are not used to call anymore, most of them prefer to chat. Even though it is much less efficient. I prefer to have less frequent calls where you have full focused interaction rather than many chat sessions. I categorize the calls in two groups: Social interactions: managed similar as the physical social interactions , with the people that I speak regularly, we arrange meetings that suit us both, the others I tell which are good time spans to call me. If the conversation allows it, I try to use headphones and simultaneously do mindless tasks such as folding the clothes or cleaning the kitchen. To prioritize and adjust the time between calls for each people I use relationship management processes . Spam callers: Hateful events where you can't dump all the frustration that they produce on the person that calls you as it's not their fault and they surely are not enjoying either the situation. They have the lowest priority and can be safely ignored and blocked. You can manually do it in the phone, although it's not very effective as they change numbers. A better approach is to add your number to do not call registries which legally allow you to scare them off. As calls are very rare and of high priority, I have my phone configured to ring on incoming calls. Instant messages \u2691 It's the main communication channel for most people, so it has a great volume of events but most have low priority. They can be categorized as: Asking for help through direct messages: We don't have many as we've agreed to use groups as much as possible . So they have high priority and I have the notifications enabled. Social interaction through direct messages: I don't have many as I try to arrange one on one calls instead , so they have a low priority. Team group or support rooms: We've defined the interruption role so I check them whenever an chosen interruption event comes. Information rooms: They have no priority and can be checked daily. In conclusion, I can check the personal chat applications three times per day, for example, after each meal. As I usually need them when I'm off the computer, I only have them configured at my mobile phone, with no sound notifications. That way I only check them when I want to. Desktop notifications \u2691 I have none but I've seen people have a notification each time the music player changes of song. It makes no sense at all.","title":"Personal Interruption Analysis"},{"location":"personal_interruption_analysis/#physical-interruptions","text":"The analysis is similar to the work physical interruptions .","title":"Physical interruptions"},{"location":"personal_interruption_analysis/#emails","text":"Email can be used as one of the main aggregators of interruptions as it's supported by almost everything. I use it as the notification of things that don't need to be acted upon immediately or when more powerful mechanisms are not available. In my case emails can be categorized as: Bill or bank receipt emails: I receive at least one per provider per month, the associated action is to download the attached pdf and remove the email. I've got it automated using a Python program that I need to manually run. In the future I expect it to be done automatically without my interaction . There is no urgency to act on them. General information: In the past I was subscribed to newsletters, now I prefer to use RSS. They don't usually require any direct action, so they can wait more than two days. Videogame deals: I was subscribed to Humblebundle, GOG and Steam notifications to be notified on the deals, but then I migrated to IsThereAnyDeal because it only sends the notifications of the deals that match a defined criteria (reducing the amount of emails), and monitors all sites in one place. I can act on them with one or two days of delay. Source code manager notifications: The web where I host my source code sends me emails when there are new pull requests or when there are comments on existent ones. I try to act on them daily. The CI sends notifications when some job fails. Unless it's a new pipeline or I'm actively working on it, a failed work job can wait many days broken before I need to interact with ithe. Infrastructure notifications: For example LetsEncrypt renewals or cloud provider notification or support cases. The related actions can wait a day or more. Monitorization notifications: I've configured Prometheus's alertmanager to send the notifications to the email as a fallback channel, but it's to be checked only if the main channel is down. Stranger emails: People whom I don't know that contacts me asking questions. These can be dealt with daily. In conclusion, I can check the personal emails twice a day, one after breakfast and another in the middle of the afternoon. So its safe to disable the notifications. I'm eager to start the email automation project so I can spend even less time and willpower managing the email.","title":"Emails"},{"location":"personal_interruption_analysis/#calls","text":"People are not used to call anymore, most of them prefer to chat. Even though it is much less efficient. I prefer to have less frequent calls where you have full focused interaction rather than many chat sessions. I categorize the calls in two groups: Social interactions: managed similar as the physical social interactions , with the people that I speak regularly, we arrange meetings that suit us both, the others I tell which are good time spans to call me. If the conversation allows it, I try to use headphones and simultaneously do mindless tasks such as folding the clothes or cleaning the kitchen. To prioritize and adjust the time between calls for each people I use relationship management processes . Spam callers: Hateful events where you can't dump all the frustration that they produce on the person that calls you as it's not their fault and they surely are not enjoying either the situation. They have the lowest priority and can be safely ignored and blocked. You can manually do it in the phone, although it's not very effective as they change numbers. A better approach is to add your number to do not call registries which legally allow you to scare them off. As calls are very rare and of high priority, I have my phone configured to ring on incoming calls.","title":"Calls"},{"location":"personal_interruption_analysis/#instant-messages","text":"It's the main communication channel for most people, so it has a great volume of events but most have low priority. They can be categorized as: Asking for help through direct messages: We don't have many as we've agreed to use groups as much as possible . So they have high priority and I have the notifications enabled. Social interaction through direct messages: I don't have many as I try to arrange one on one calls instead , so they have a low priority. Team group or support rooms: We've defined the interruption role so I check them whenever an chosen interruption event comes. Information rooms: They have no priority and can be checked daily. In conclusion, I can check the personal chat applications three times per day, for example, after each meal. As I usually need them when I'm off the computer, I only have them configured at my mobile phone, with no sound notifications. That way I only check them when I want to.","title":"Instant messages"},{"location":"personal_interruption_analysis/#desktop-notifications","text":"I have none but I've seen people have a notification each time the music player changes of song. It makes no sense at all.","title":"Desktop notifications"},{"location":"pexpect/","text":"pexpect is a pure Python module for spawning child applications; controlling them; and responding to expected patterns in their output. Pexpect works like Don Libes\u2019 Expect. Pexpect allows your script to spawn a child application and control it as if a human were typing commands. Installation \u2691 pip install pexpect Usage \u2691 import pexpect child = pexpect . spawn ( 'ftp ftp.openbsd.org' ) child . expect ( 'Name .*: ' ) child . sendline ( 'anonymous' ) If you're using it to spawn a program that asks something and then ends, you can catch the end with .expect_exact(pexpect.EOF) . tui = pexpect . spawn ( \"python source.py\" , timeout = 5 ) tui . expect ( \"Give me .*\" ) tui . sendline ( \"HI\" ) tui . expect_exact ( pexpect . EOF ) The timeout=5 is useful if the pexpect interaction is not well defined, so that the script is not hung forever. Send key presses \u2691 To simulate key presses, you can use prompt_toolkit keys with REVERSE_ANSI_SEQUENCES . from prompt_toolkit.input.ansi_escape_sequences import REVERSE_ANSI_SEQUENCES from prompt_toolkit.keys import Keys tui = pexpect . spawn ( \"python source.py\" , timeout = 5 ) tui . send ( REVERSE_ANSI_SEQUENCES [ Keys . ControlC ]) To make your code cleaner you can use a helper class : from prompt_toolkit.input.ansi_escape_sequences import REVERSE_ANSI_SEQUENCES from prompt_toolkit.keys import Keys class Keyboard ( str , Enum ): ControlH = REVERSE_ANSI_SEQUENCES [ Keys . ControlH ] Enter = \" \\r \" Esc = REVERSE_ANSI_SEQUENCES [ Keys . Escape ] # Equivalent keystrokes in terminals; see python-prompt-toolkit for # further explanations Alt = Esc Backspace = ControlH Read output of command \u2691 import sys import pexpect child = pexpect . spawn ( 'ls' ) child . logfile = sys . stdout child . expect ( pexpect . EOF ) For the tests, you can use the capsys fixture to do assertions on the content: out , err = capsys . readouterr () assert \"WARNING! you took 1 seconds to process the last element\" in out References \u2691 Docs","title":"pexpect"},{"location":"pexpect/#installation","text":"pip install pexpect","title":"Installation"},{"location":"pexpect/#usage","text":"import pexpect child = pexpect . spawn ( 'ftp ftp.openbsd.org' ) child . expect ( 'Name .*: ' ) child . sendline ( 'anonymous' ) If you're using it to spawn a program that asks something and then ends, you can catch the end with .expect_exact(pexpect.EOF) . tui = pexpect . spawn ( \"python source.py\" , timeout = 5 ) tui . expect ( \"Give me .*\" ) tui . sendline ( \"HI\" ) tui . expect_exact ( pexpect . EOF ) The timeout=5 is useful if the pexpect interaction is not well defined, so that the script is not hung forever.","title":"Usage"},{"location":"pexpect/#send-key-presses","text":"To simulate key presses, you can use prompt_toolkit keys with REVERSE_ANSI_SEQUENCES . from prompt_toolkit.input.ansi_escape_sequences import REVERSE_ANSI_SEQUENCES from prompt_toolkit.keys import Keys tui = pexpect . spawn ( \"python source.py\" , timeout = 5 ) tui . send ( REVERSE_ANSI_SEQUENCES [ Keys . ControlC ]) To make your code cleaner you can use a helper class : from prompt_toolkit.input.ansi_escape_sequences import REVERSE_ANSI_SEQUENCES from prompt_toolkit.keys import Keys class Keyboard ( str , Enum ): ControlH = REVERSE_ANSI_SEQUENCES [ Keys . ControlH ] Enter = \" \\r \" Esc = REVERSE_ANSI_SEQUENCES [ Keys . Escape ] # Equivalent keystrokes in terminals; see python-prompt-toolkit for # further explanations Alt = Esc Backspace = ControlH","title":"Send key presses"},{"location":"pexpect/#read-output-of-command","text":"import sys import pexpect child = pexpect . spawn ( 'ls' ) child . logfile = sys . stdout child . expect ( pexpect . EOF ) For the tests, you can use the capsys fixture to do assertions on the content: out , err = capsys . readouterr () assert \"WARNING! you took 1 seconds to process the last element\" in out","title":"Read output of command"},{"location":"pexpect/#references","text":"Docs","title":"References"},{"location":"pilates/","text":"Pilates is a physical fitness system based on controlled movements putting emphasis on alignment, breathing, developing a strong core, and improving coordination and balance. The core (or powerhouse), consisting of the muscles of the abdomen, low back, and hips, is thought to be the key to a person's stability. Pilates' system allows for different exercises to be modified in range of difficulty from beginner to advanced or to any other level, and also in terms of the instructor and practitioner's specific goals and/or limitations. Intensity can be increased over time as the body adapts itself to the exercises. You can think of yoga, but without the spiritual aspects. Principles \u2691 Breathing \u2691 The breathing in Pilates is meant the to be deeper, with full inhalations and complete exhalations. In order to keep the lower abdominals close to the spine, the breathing needs to be directed to the back and sides of the lower rib cage. When exhaling, you need to squeeze out the lungs as they would wring a wet towel dry. To do that you need to contract the deep abdominal and pelvic floor muscles, feeling your bellybutton going to your back and a little bit up. When inhaling you need to maintain this engagement to keep the core in control. The difficult part comes when you try to properly coordinate this breathing practice with the exercise movement, breathes out with the effort and in on the return. This technique is important as it increases the intake of oxygen and the circulation of this oxygenated blood to every part of the body, cleaning and invigorating it. Concentration \u2691 It demands intense focus, as you need to be aware of the position of each part of your body, and how they move to precisely do the exercise. Control \u2691 You don't see many quick movements, most of the exercises are anaerobic. The difficult relies on controlling your muscles to do what you want them to while they fight against gravity, springs and other torture tools. Flow \u2691 Pilates aims for elegant economy of movement, creating flow through the use of appropriate transitions. Once precision has been achieved, the exercises are intended to flow within and into each other in order to build strength and stamina. A smoothly doing a roll down (from seated position with your legs straight, slowly lay down, vertebrae by vertebrae) is a difficult challenge, as you need every muscle to coordinate to share the load of the weight. The muscles that we use more are stronger, and some of them are barely used, Pilates positions and slow transitions force you to use those weak, forgotten muscles, and when the load is transferred from the strong to the weak, your body starts shaking or breaks the movement rate thus breaking the flow. Even though it looks silly, it's tough. Postural alignment \u2691 Being more aware of your body by bringing to it's limits with each exercise, seeing where it fails, strengthening the weak muscles and practicing flow and control results in a better postural alignment. Precision \u2691 The focus is on doing one precise and perfect movement, rather than many halfhearted ones. The goal is for this precision to eventually become second nature and carry over into everyday life as grace and economy of movement. Relaxation \u2691 Correct muscle firing patterns and improved mental concentration are enhanced with relaxation. Stamina \u2691 Stamina is increased through the gradual strengthening of your body, and with the increasing precision of the motion, making them more efficient so there is less stress to perform the exercises. Positions \u2691 Feet in flex: your toes go away from your shins, so your foot follows your shin line. Exercises \u2691 I'm going to annotate the exercises I like most, probably the name is incorrect and the explanation not perfect. If you do them, please be careful, and in case of doubt ask a Pilates teacher. Swing from table \u2691 Lvl 0: Starting position (Inhale): Start at step 1 of the table . Step 1 (Exhale): Instead of going to the mat, when exhaling move your ass between your arms without touching the mat until it's behind them. Round your spine in the process. You'll feel a nice spine movement similar to the cat - cow movement. Return (Inhale): Slowly go back to starting position. Lvl 1: Starting position (Inhale): Start at step 1 of the table with one leg straight in the air, in the same line as your shoulders, hips and knee. Step 1 (Exhale): Similar to Lvl 0 but make sure that the heel of the foot doesn't touch the mat, feet in flex . Return (Inhale): Slowly go back to starting position, do X repetitions and then switch to the other foot. Lvl 2: Starting position (Inhale): Start at step 1 of the inverted plank. Step 1 (Exhale): Similar to Lvl 0. Return (Inhale): Slowly go back to starting position. Lvl 3: Similar to Lvl 2 with the leg up like Lvl 1. I've found that Lvl 2 and Lvl 3 give a less pleasant spine rub. Table \u2691 Starting position (Exhale): Sit in your mat with your legs parallel, knees bent and your feet at two or three fists from your ass, hands on the mat behind you, fingers pointing to your ass. If you have shoulder aches, you can point the fingers at 45 degrees or away from your ass. * Step 1 (Inhale): Slowly move your ass up until your shoulders, knees and hips are on the same line. To avoid neck pain, keep your chin down so you're looking at your knees. Your knees should be over your ankles and your arms should be extended. * Return (Exhale): Slowly come back to the starting position. References \u2691 Books \u2691 Pilates anatomy by Rael Isacowitz and Karen Clippinger : With gorgeous illustrations.","title":"Pilates"},{"location":"pilates/#principles","text":"","title":"Principles"},{"location":"pilates/#breathing","text":"The breathing in Pilates is meant the to be deeper, with full inhalations and complete exhalations. In order to keep the lower abdominals close to the spine, the breathing needs to be directed to the back and sides of the lower rib cage. When exhaling, you need to squeeze out the lungs as they would wring a wet towel dry. To do that you need to contract the deep abdominal and pelvic floor muscles, feeling your bellybutton going to your back and a little bit up. When inhaling you need to maintain this engagement to keep the core in control. The difficult part comes when you try to properly coordinate this breathing practice with the exercise movement, breathes out with the effort and in on the return. This technique is important as it increases the intake of oxygen and the circulation of this oxygenated blood to every part of the body, cleaning and invigorating it.","title":"Breathing"},{"location":"pilates/#concentration","text":"It demands intense focus, as you need to be aware of the position of each part of your body, and how they move to precisely do the exercise.","title":"Concentration"},{"location":"pilates/#control","text":"You don't see many quick movements, most of the exercises are anaerobic. The difficult relies on controlling your muscles to do what you want them to while they fight against gravity, springs and other torture tools.","title":"Control"},{"location":"pilates/#flow","text":"Pilates aims for elegant economy of movement, creating flow through the use of appropriate transitions. Once precision has been achieved, the exercises are intended to flow within and into each other in order to build strength and stamina. A smoothly doing a roll down (from seated position with your legs straight, slowly lay down, vertebrae by vertebrae) is a difficult challenge, as you need every muscle to coordinate to share the load of the weight. The muscles that we use more are stronger, and some of them are barely used, Pilates positions and slow transitions force you to use those weak, forgotten muscles, and when the load is transferred from the strong to the weak, your body starts shaking or breaks the movement rate thus breaking the flow. Even though it looks silly, it's tough.","title":"Flow"},{"location":"pilates/#postural-alignment","text":"Being more aware of your body by bringing to it's limits with each exercise, seeing where it fails, strengthening the weak muscles and practicing flow and control results in a better postural alignment.","title":"Postural alignment"},{"location":"pilates/#precision","text":"The focus is on doing one precise and perfect movement, rather than many halfhearted ones. The goal is for this precision to eventually become second nature and carry over into everyday life as grace and economy of movement.","title":"Precision"},{"location":"pilates/#relaxation","text":"Correct muscle firing patterns and improved mental concentration are enhanced with relaxation.","title":"Relaxation"},{"location":"pilates/#stamina","text":"Stamina is increased through the gradual strengthening of your body, and with the increasing precision of the motion, making them more efficient so there is less stress to perform the exercises.","title":"Stamina"},{"location":"pilates/#positions","text":"Feet in flex: your toes go away from your shins, so your foot follows your shin line.","title":"Positions"},{"location":"pilates/#exercises","text":"I'm going to annotate the exercises I like most, probably the name is incorrect and the explanation not perfect. If you do them, please be careful, and in case of doubt ask a Pilates teacher.","title":"Exercises"},{"location":"pilates/#swing-from-table","text":"Lvl 0: Starting position (Inhale): Start at step 1 of the table . Step 1 (Exhale): Instead of going to the mat, when exhaling move your ass between your arms without touching the mat until it's behind them. Round your spine in the process. You'll feel a nice spine movement similar to the cat - cow movement. Return (Inhale): Slowly go back to starting position. Lvl 1: Starting position (Inhale): Start at step 1 of the table with one leg straight in the air, in the same line as your shoulders, hips and knee. Step 1 (Exhale): Similar to Lvl 0 but make sure that the heel of the foot doesn't touch the mat, feet in flex . Return (Inhale): Slowly go back to starting position, do X repetitions and then switch to the other foot. Lvl 2: Starting position (Inhale): Start at step 1 of the inverted plank. Step 1 (Exhale): Similar to Lvl 0. Return (Inhale): Slowly go back to starting position. Lvl 3: Similar to Lvl 2 with the leg up like Lvl 1. I've found that Lvl 2 and Lvl 3 give a less pleasant spine rub.","title":"Swing from table"},{"location":"pilates/#table","text":"Starting position (Exhale): Sit in your mat with your legs parallel, knees bent and your feet at two or three fists from your ass, hands on the mat behind you, fingers pointing to your ass. If you have shoulder aches, you can point the fingers at 45 degrees or away from your ass. * Step 1 (Inhale): Slowly move your ass up until your shoulders, knees and hips are on the same line. To avoid neck pain, keep your chin down so you're looking at your knees. Your knees should be over your ankles and your arms should be extended. * Return (Exhale): Slowly come back to the starting position.","title":"Table"},{"location":"pilates/#references","text":"","title":"References"},{"location":"pilates/#books","text":"Pilates anatomy by Rael Isacowitz and Karen Clippinger : With gorgeous illustrations.","title":"Books"},{"location":"process_automation/","text":"I understand process automation as the act of analyzing yourself and your interactions with the world to find the way to reduce the time or willpower spent on your life processes. Once you've covered some minimum life requirements (health, money or happiness), time is your most valued asset. It's sad to waste it doing stuff that we need but don't increase our happiness. I've also faced the problem of having so much stuff in my mind. Having background processes increase your brain load and are a constant sink of willpower. As a result, when you really need that CPU time, your brain is tired and doesn't work to it's full performance. Automating processes, as well as life logging and task management, allows you to delegate those worries. Process automation can lead to habit building , which reduces even more the willpower consumption of processes, at the same time it reduces the error rate. The marvelous xkcd comic has gathered the essence and pitfalls of process automation many times: Automating home chores \u2691 Using Grocy to maintain the house stock, shopping lists and meal plans.","title":"Process Automation"},{"location":"process_automation/#automating-home-chores","text":"Using Grocy to maintain the house stock, shopping lists and meal plans.","title":"Automating home chores"},{"location":"prompt_toolkit_fullscreen_applications/","text":"Prompt toolkit can be used to build full screen interfaces . This section focuses in how to do it. If you want to build REPL applications instead go to this other article . Typically, an application consists of a layout (to describe the graphical part) and a set of key bindings. Every prompt_toolkit application is an instance of an Application object. from prompt_toolkit import Application app = Application ( full_screen = True ) app . run () When run() is called, the event loop will run until the application is done. An application will quit when exit() is called. The event loop is basically a while-true loop that waits for user input, and when it receives something (like a key press), it will send that to the appropriate handler, like for instance, a key binding. An application consists of several components. The most important are: I/O objects: the input and output device. The layout: this defines the graphical structure of the application. For instance, a text box on the left side, and a button on the right side. You can also think of the layout as a collection of \u2018widgets\u2019. A style: this defines what colors and underline/bold/italic styles are used everywhere. A set of key bindings. [The \u2691 layout]( https://python-prompt-toolkit.readthedocs.io/en/master/pages/full_screen_apps.html#the-layout ) With the Layout object you define the graphical structure of the application, it accepts as argument a nested structure of Container objects, these arrange the layout by splitting the screen in many regions, while controls (children of UIControl , such as BufferControl or FormattedTextControl ) are responsible for generating the actual content. Some of the Container s you can use are: HSplit , Vsplit , FloatContainer , Window or ScrollablePane . The Window class itself is particular: it is a Container that can contain a UIControl . Thus, it\u2019s the adapter between the two. The Window class also takes care of scrolling the content and wrapping the lines if needed. Tables \u2691 Currently they are not supported :(, although there is an old PR . Controls \u2691 Focusing windows \u2691 Focusing something can be done by calling the focus() method. This method is very flexible and accepts a Window , a Buffer , a UIControl and more. In the following example, we use get_app() for getting the active application. from prompt_toolkit.application import get_app # This window was created earlier. w = Window () # ... # Now focus it. get_app () . layout . focus ( w ) To focus the next window in the layout you can use app.layout.focus_next() . [Key \u2691 bindings]( https://python-prompt-toolkit.readthedocs.io/en/master/pages/full_screen_apps.html#key-bindings ) In order to react to user actions, we need to create a KeyBindings object and pass that to our Application . There are two kinds of key bindings: Global key bindings , which are always active. Key bindings that belong to a certain UIControl and are only active when this control is focused. Both BufferControl and FormattedTextControl take a key_bindings argument. For complex keys you can always look at the Keys class . [Global key \u2691 bindings]( https://python-prompt-toolkit.readthedocs.io/en/master/pages/full_screen_apps.html#global-key-bindings ) Key bindings can be passed to the application as follows: from prompt_toolkit import Application from prompt_toolkit.key_binding.key_processor import KeyPressEvent kb = KeyBindings () app = Application ( key_bindings = kb ) app . run () To register a new keyboard shortcut, we can use the add() method as a decorator of the key handler: from prompt_toolkit import Application from prompt_toolkit.key_binding import KeyBindings from prompt_toolkit.key_binding.key_processor import KeyPressEvent kb = KeyBindings () @kb . add ( 'c-q' ) def exit_ ( event : KeyPressEvent ) -> None : \"\"\" Pressing Ctrl-Q will exit the user interface. Setting a return value means: quit the event loop that drives the user interface and return this value from the `Application.run()` call. \"\"\" event . app . exit () app = Application ( key_bindings = kb , full_screen = True ) app . run () Here you can read for more complex patterns with key bindings. A more programmatically way to add bindings is: from prompt_toolkit.key_binding import KeyBindings from prompt_toolkit.key_binding.bindings.focus import focus_next kb = KeyBindings () kb . add ( \"tab\" )( focus_next ) Pass more than one key \u2691 To map an action to two key presses use kb.add('g', 'g') . Styles \u2691 Many user interface controls, like Window accept a style argument which can be used to pass the formatting as a string. For instance, we can select a foreground color: fg:ansired : ANSI color palette fg:ansiblue : ANSI color palette fg:#ffaa33 : hexadecimal notation fg:darkred : named color Or a background color: bg:ansired : ANSI color palette bg:#ffaa33 : hexadecimal notation Like we do for web design, it is not a good habit to specify all styling inline. Instead, we can attach class names to UI controls and have a style sheet that refers to these class names. The Style can be passed as an argument to the Application . from prompt_toolkit.layout import VSplit , Window from prompt_toolkit.styles import Style layout = VSplit ([ Window ( BufferControl ( ... ), style = 'class:left' ), HSplit ([ Window ( BufferControl ( ... ), style = 'class:top' ), Window ( BufferControl ( ... ), style = 'class:bottom' ), ], style = 'class:right' ) ]) style = Style ([ ( 'left' , 'bg:ansired' ), ( 'top' , 'fg:#00aaaa' ), ( 'bottom' , 'underline bold' ), ]) You may need to define the 24bit color depths to see the colors you expect: from prompt_toolkit.output.color_depth import ColorDepth app = Application ( color_depth = ColorDepth . DEPTH_24_BIT , # ... ) If you want to see if a style is being applied in a component, set the style to bg:#dc322f and it will be highlighted in red. Dynamically changing the style \u2691 You'll need to create a widget, you can take as inspiration the package widgets . To create a row that changes color when it's focused use: from prompt_toolkit.layout.controls import FormattedTextControl from prompt_toolkit.layout.containers import Window from prompt_toolkit.application import get_app class Row : \"\"\"Define row. Args: text: text to print \"\"\" def __init__ ( self , text : str , ) -> None : \"\"\"Initialize the widget.\"\"\" self . text = text self . control = FormattedTextControl ( self . text , focusable = True , ) def get_style () -> str : if get_app () . layout . has_focus ( self ): return \"class:row.focused\" else : return \"class:row\" self . window = Window ( self . control , height = 1 , style = get_style , always_hide_cursor = True ) def __pt_container__ ( self ) -> Window : \"\"\"Return the window object. Mandatory to be considered a widget. \"\"\" return self . window An example of use would be: layout = HSplit ( [ Row ( \"Test1\" ), Row ( \"Test2\" ), Row ( \"Test3\" ), ] ) # Key bindings kb = KeyBindings () kb . add ( \"j\" )( focus_next ) kb . add ( \"k\" )( focus_previous ) @kb . add ( \"c-c\" , eager = True ) @kb . add ( \"q\" , eager = True ) def exit_ ( event : KeyPressEvent ) -> None : \"\"\"Exit the user interface.\"\"\" event . app . exit () # Styles style = Style ( [ ( \"row\" , \"bg:#073642 #657b83\" ), ( \"row.focused\" , \"bg:#002b36 #657b83\" ), ] ) # Application app = Application ( layout = Layout ( layout ), full_screen = True , key_bindings = kb , style = style , color_depth = ColorDepth . DEPTH_24_BIT , ) app . run () Examples \u2691 The best way to understand how it works is by running the examples in the repository, some interesting ones in increasing order of difficult are: Managing autocompletion Managing focus Managing floats , and floats with transparency Add margins , such as line number or a scroll bar. Managing styles . Wrapping lines in buffercontrol, and line prefixes A working REPL example . Interesting to see the SearchToolbar in use (press Ctrl+r ), and how to interact with other windows with handlers. A working editor : Complex application that shows how to build a working menu. pyvim : A rewrite of vim in python, it shows how to test, handle commands and a lot more, very interesting. pymux : Another full screen application example. Testing \u2691 Prompt toolkit application testing can be done at different levels: Component level: Useful to test how a component manages it's data by itself. Application level: Useful to test how a user interacts with the component. If you don't know how to test something, I suggest you check how prompt toolkit tests itself. You can also check how do third party packages do their tests too, such as prompt-toolkit-table or pyvim . Keep in mind that you don't usually want to check the result of the stdout or stderr directly, but the state of your component or the application itself. Component level \u2691 If your component accepts some input and does some magic on that input without the need to load the application, import the object directly and run tests changing the input directly and asserting the results of the output. Application level \u2691 If you want to test the interaction with your component at application level, for example what happens when a user presses a key, you need to instantiate a dummy application and play with it. Imagine we have a TableControl component we want to test that accepts some input in the form of data . We'll use the set_dummy_app function to configure an application that outputs to DummyOutput , and a helper function get_app_and_processor to return the active app and a processor to send key presses. def set_dummy_app ( data : Any ) -> Any : \"\"\"Return a context manager that starts the dummy application. This is important, because we need an `Application` with `is_done=False` flag, otherwise no keys will be processed. \"\"\" app : Application [ Any ] = Application ( layout = Layout ( Window ( TableControl ( data ))), output = DummyOutput (), input = create_pipe_input (), ) return set_app ( app ) def get_app_and_processor () -> Tuple [ Application [ Any ], KeyProcessor ]: \"\"\"Return the active application and it's key processor.\"\"\" app = get_app () key_bindings = app . layout . container . get_key_bindings () if key_bindings is None : key_bindings = KeyBindings () processor = KeyProcessor ( key_bindings ) return app , processor We've loaded the processor with the key bindings defined in the container. If you want other bindings change them there. For example prompt-toolkit uses a fixture to set them. Remember that you have the merge_key_bindings to join two key binding objects with: key_bindings = merge_key_bindings ([ key_bindings , control_bindings ]) Once the functions are set, you can make your test. Imagine that we want to check that if the user presses j , the variable _focused_row is incremented by 1. This variable will be used by the component internally to change the style of the rows so that the next element is highlighted. def test_j_moves_to_the_next_row ( self , pydantic_data : PydanticData ) -> None : \"\"\" Given: A well configured table When: j is press Then: the focus is moved to the next line \"\"\" with set_dummy_app ( pydantic_data ): app , processor = get_app_and_processor () processor . feed ( KeyPress ( \"j\" , \"j\" )) # act processor . process_keys () assert app . layout . container . content . _focused_row == 1 References \u2691 Docs Git Projects using prompt_toolkit","title":"Full screen applications"},{"location":"prompt_toolkit_fullscreen_applications/#the","text":"layout]( https://python-prompt-toolkit.readthedocs.io/en/master/pages/full_screen_apps.html#the-layout ) With the Layout object you define the graphical structure of the application, it accepts as argument a nested structure of Container objects, these arrange the layout by splitting the screen in many regions, while controls (children of UIControl , such as BufferControl or FormattedTextControl ) are responsible for generating the actual content. Some of the Container s you can use are: HSplit , Vsplit , FloatContainer , Window or ScrollablePane . The Window class itself is particular: it is a Container that can contain a UIControl . Thus, it\u2019s the adapter between the two. The Window class also takes care of scrolling the content and wrapping the lines if needed.","title":"[The"},{"location":"prompt_toolkit_fullscreen_applications/#tables","text":"Currently they are not supported :(, although there is an old PR .","title":"Tables"},{"location":"prompt_toolkit_fullscreen_applications/#controls","text":"","title":"Controls"},{"location":"prompt_toolkit_fullscreen_applications/#focusing-windows","text":"Focusing something can be done by calling the focus() method. This method is very flexible and accepts a Window , a Buffer , a UIControl and more. In the following example, we use get_app() for getting the active application. from prompt_toolkit.application import get_app # This window was created earlier. w = Window () # ... # Now focus it. get_app () . layout . focus ( w ) To focus the next window in the layout you can use app.layout.focus_next() .","title":"Focusing windows"},{"location":"prompt_toolkit_fullscreen_applications/#key","text":"bindings]( https://python-prompt-toolkit.readthedocs.io/en/master/pages/full_screen_apps.html#key-bindings ) In order to react to user actions, we need to create a KeyBindings object and pass that to our Application . There are two kinds of key bindings: Global key bindings , which are always active. Key bindings that belong to a certain UIControl and are only active when this control is focused. Both BufferControl and FormattedTextControl take a key_bindings argument. For complex keys you can always look at the Keys class .","title":"[Key"},{"location":"prompt_toolkit_fullscreen_applications/#global-key","text":"bindings]( https://python-prompt-toolkit.readthedocs.io/en/master/pages/full_screen_apps.html#global-key-bindings ) Key bindings can be passed to the application as follows: from prompt_toolkit import Application from prompt_toolkit.key_binding.key_processor import KeyPressEvent kb = KeyBindings () app = Application ( key_bindings = kb ) app . run () To register a new keyboard shortcut, we can use the add() method as a decorator of the key handler: from prompt_toolkit import Application from prompt_toolkit.key_binding import KeyBindings from prompt_toolkit.key_binding.key_processor import KeyPressEvent kb = KeyBindings () @kb . add ( 'c-q' ) def exit_ ( event : KeyPressEvent ) -> None : \"\"\" Pressing Ctrl-Q will exit the user interface. Setting a return value means: quit the event loop that drives the user interface and return this value from the `Application.run()` call. \"\"\" event . app . exit () app = Application ( key_bindings = kb , full_screen = True ) app . run () Here you can read for more complex patterns with key bindings. A more programmatically way to add bindings is: from prompt_toolkit.key_binding import KeyBindings from prompt_toolkit.key_binding.bindings.focus import focus_next kb = KeyBindings () kb . add ( \"tab\" )( focus_next )","title":"[Global key"},{"location":"prompt_toolkit_fullscreen_applications/#pass-more-than-one-key","text":"To map an action to two key presses use kb.add('g', 'g') .","title":"Pass more than one key"},{"location":"prompt_toolkit_fullscreen_applications/#styles","text":"Many user interface controls, like Window accept a style argument which can be used to pass the formatting as a string. For instance, we can select a foreground color: fg:ansired : ANSI color palette fg:ansiblue : ANSI color palette fg:#ffaa33 : hexadecimal notation fg:darkred : named color Or a background color: bg:ansired : ANSI color palette bg:#ffaa33 : hexadecimal notation Like we do for web design, it is not a good habit to specify all styling inline. Instead, we can attach class names to UI controls and have a style sheet that refers to these class names. The Style can be passed as an argument to the Application . from prompt_toolkit.layout import VSplit , Window from prompt_toolkit.styles import Style layout = VSplit ([ Window ( BufferControl ( ... ), style = 'class:left' ), HSplit ([ Window ( BufferControl ( ... ), style = 'class:top' ), Window ( BufferControl ( ... ), style = 'class:bottom' ), ], style = 'class:right' ) ]) style = Style ([ ( 'left' , 'bg:ansired' ), ( 'top' , 'fg:#00aaaa' ), ( 'bottom' , 'underline bold' ), ]) You may need to define the 24bit color depths to see the colors you expect: from prompt_toolkit.output.color_depth import ColorDepth app = Application ( color_depth = ColorDepth . DEPTH_24_BIT , # ... ) If you want to see if a style is being applied in a component, set the style to bg:#dc322f and it will be highlighted in red.","title":"Styles"},{"location":"prompt_toolkit_fullscreen_applications/#dynamically-changing-the-style","text":"You'll need to create a widget, you can take as inspiration the package widgets . To create a row that changes color when it's focused use: from prompt_toolkit.layout.controls import FormattedTextControl from prompt_toolkit.layout.containers import Window from prompt_toolkit.application import get_app class Row : \"\"\"Define row. Args: text: text to print \"\"\" def __init__ ( self , text : str , ) -> None : \"\"\"Initialize the widget.\"\"\" self . text = text self . control = FormattedTextControl ( self . text , focusable = True , ) def get_style () -> str : if get_app () . layout . has_focus ( self ): return \"class:row.focused\" else : return \"class:row\" self . window = Window ( self . control , height = 1 , style = get_style , always_hide_cursor = True ) def __pt_container__ ( self ) -> Window : \"\"\"Return the window object. Mandatory to be considered a widget. \"\"\" return self . window An example of use would be: layout = HSplit ( [ Row ( \"Test1\" ), Row ( \"Test2\" ), Row ( \"Test3\" ), ] ) # Key bindings kb = KeyBindings () kb . add ( \"j\" )( focus_next ) kb . add ( \"k\" )( focus_previous ) @kb . add ( \"c-c\" , eager = True ) @kb . add ( \"q\" , eager = True ) def exit_ ( event : KeyPressEvent ) -> None : \"\"\"Exit the user interface.\"\"\" event . app . exit () # Styles style = Style ( [ ( \"row\" , \"bg:#073642 #657b83\" ), ( \"row.focused\" , \"bg:#002b36 #657b83\" ), ] ) # Application app = Application ( layout = Layout ( layout ), full_screen = True , key_bindings = kb , style = style , color_depth = ColorDepth . DEPTH_24_BIT , ) app . run ()","title":"Dynamically changing the style"},{"location":"prompt_toolkit_fullscreen_applications/#examples","text":"The best way to understand how it works is by running the examples in the repository, some interesting ones in increasing order of difficult are: Managing autocompletion Managing focus Managing floats , and floats with transparency Add margins , such as line number or a scroll bar. Managing styles . Wrapping lines in buffercontrol, and line prefixes A working REPL example . Interesting to see the SearchToolbar in use (press Ctrl+r ), and how to interact with other windows with handlers. A working editor : Complex application that shows how to build a working menu. pyvim : A rewrite of vim in python, it shows how to test, handle commands and a lot more, very interesting. pymux : Another full screen application example.","title":"Examples"},{"location":"prompt_toolkit_fullscreen_applications/#testing","text":"Prompt toolkit application testing can be done at different levels: Component level: Useful to test how a component manages it's data by itself. Application level: Useful to test how a user interacts with the component. If you don't know how to test something, I suggest you check how prompt toolkit tests itself. You can also check how do third party packages do their tests too, such as prompt-toolkit-table or pyvim . Keep in mind that you don't usually want to check the result of the stdout or stderr directly, but the state of your component or the application itself.","title":"Testing"},{"location":"prompt_toolkit_fullscreen_applications/#component-level","text":"If your component accepts some input and does some magic on that input without the need to load the application, import the object directly and run tests changing the input directly and asserting the results of the output.","title":"Component level"},{"location":"prompt_toolkit_fullscreen_applications/#application-level","text":"If you want to test the interaction with your component at application level, for example what happens when a user presses a key, you need to instantiate a dummy application and play with it. Imagine we have a TableControl component we want to test that accepts some input in the form of data . We'll use the set_dummy_app function to configure an application that outputs to DummyOutput , and a helper function get_app_and_processor to return the active app and a processor to send key presses. def set_dummy_app ( data : Any ) -> Any : \"\"\"Return a context manager that starts the dummy application. This is important, because we need an `Application` with `is_done=False` flag, otherwise no keys will be processed. \"\"\" app : Application [ Any ] = Application ( layout = Layout ( Window ( TableControl ( data ))), output = DummyOutput (), input = create_pipe_input (), ) return set_app ( app ) def get_app_and_processor () -> Tuple [ Application [ Any ], KeyProcessor ]: \"\"\"Return the active application and it's key processor.\"\"\" app = get_app () key_bindings = app . layout . container . get_key_bindings () if key_bindings is None : key_bindings = KeyBindings () processor = KeyProcessor ( key_bindings ) return app , processor We've loaded the processor with the key bindings defined in the container. If you want other bindings change them there. For example prompt-toolkit uses a fixture to set them. Remember that you have the merge_key_bindings to join two key binding objects with: key_bindings = merge_key_bindings ([ key_bindings , control_bindings ]) Once the functions are set, you can make your test. Imagine that we want to check that if the user presses j , the variable _focused_row is incremented by 1. This variable will be used by the component internally to change the style of the rows so that the next element is highlighted. def test_j_moves_to_the_next_row ( self , pydantic_data : PydanticData ) -> None : \"\"\" Given: A well configured table When: j is press Then: the focus is moved to the next line \"\"\" with set_dummy_app ( pydantic_data ): app , processor = get_app_and_processor () processor . feed ( KeyPress ( \"j\" , \"j\" )) # act processor . process_keys () assert app . layout . container . content . _focused_row == 1","title":"Application level"},{"location":"prompt_toolkit_fullscreen_applications/#references","text":"Docs Git Projects using prompt_toolkit","title":"References"},{"location":"prompt_toolkit_repl/","text":"Prompt toolkit can be used to build REPL interfaces. This section focuses in how to do it. If you want to build full screen applications instead go to this other article . Testing \u2691 Testing prompt_toolkit or any text-based user interface (TUI) with python is not well documented. Some of the main developers suggest mocking it while others use pexpect . With the first approach you can test python functions and methods internally but it can lead you to the over mocking problem. The second will limit you to test functionality exposed through your program's command line, as it will spawn a process and interact it externally. Given that the TUIs are entrypoints to your program, it makes sense to test them in end-to-end tests, so I'm going to follow the second option. On the other hand, you may want to test a small part of your TUI in a unit test, if you want to follow this other path, I'd start with monkeypatch , although I didn't have good results with it. def test_method ( monkeypatch ): monkeypatch . setattr ( 'sys.stdin' , io . StringIO ( 'my input' )) Using pexpect \u2691 This method is useful to test end to end functions as you need to all the program as a command line. You can't use it to tests python functions internally. File: source.py from prompt_toolkit import prompt text = prompt ( \"Give me some input: \" ) with open ( \"/tmp/tui.txt\" , \"w\" ) as f : f . write ( text ) File: test_source.py ```python import pexpect def test_tui() -> None: tui = pexpect.spawn(\"python source.py\", timeout=5) tui.expect(\"Give me .*\") tui.sendline(\"HI\") tui.expect_exact(pexpect.EOF) with open(\"/tmp/tui.txt\", \"r\") as f: assert f.read() == \"HI\" ``` Where: The tui.expect_exact(pexpect.EOF) line is required so that the tests aren't run before the process has ended, otherwise the file might not exist yet. The timeout=5 is required in case that the pexpect interaction is not well defined, so that the test is not hung forever. Thank you Jairo Llopis for this solution. I've deduced the solution from his #260 PR into copier , and the comments of #1243","title":"REPL"},{"location":"prompt_toolkit_repl/#testing","text":"Testing prompt_toolkit or any text-based user interface (TUI) with python is not well documented. Some of the main developers suggest mocking it while others use pexpect . With the first approach you can test python functions and methods internally but it can lead you to the over mocking problem. The second will limit you to test functionality exposed through your program's command line, as it will spawn a process and interact it externally. Given that the TUIs are entrypoints to your program, it makes sense to test them in end-to-end tests, so I'm going to follow the second option. On the other hand, you may want to test a small part of your TUI in a unit test, if you want to follow this other path, I'd start with monkeypatch , although I didn't have good results with it. def test_method ( monkeypatch ): monkeypatch . setattr ( 'sys.stdin' , io . StringIO ( 'my input' ))","title":"Testing"},{"location":"prompt_toolkit_repl/#using-pexpect","text":"This method is useful to test end to end functions as you need to all the program as a command line. You can't use it to tests python functions internally. File: source.py from prompt_toolkit import prompt text = prompt ( \"Give me some input: \" ) with open ( \"/tmp/tui.txt\" , \"w\" ) as f : f . write ( text ) File: test_source.py ```python import pexpect def test_tui() -> None: tui = pexpect.spawn(\"python source.py\", timeout=5) tui.expect(\"Give me .*\") tui.sendline(\"HI\") tui.expect_exact(pexpect.EOF) with open(\"/tmp/tui.txt\", \"r\") as f: assert f.read() == \"HI\" ``` Where: The tui.expect_exact(pexpect.EOF) line is required so that the tests aren't run before the process has ended, otherwise the file might not exist yet. The timeout=5 is required in case that the pexpect interaction is not well defined, so that the test is not hung forever. Thank you Jairo Llopis for this solution. I've deduced the solution from his #260 PR into copier , and the comments of #1243","title":"Using pexpect"},{"location":"python/","text":"Python is an interpreted, high-level and general-purpose programming language. Python's design philosophy emphasizes code readability with its notable use of significant indentation. Its language constructs and object-oriented approach aim to help programmers write clear, logical code for small and large-scale projects. Interesting libraries to explore \u2691 tryceratops : A linter of exceptions. schedule : Python job scheduling for humans. Run Python functions (or any other callable) periodically using a friendly syntax. textual : Textual is a TUI (Text User Interface) framework for Python using Rich as a renderer. parso : Parses Python code. kivi : Create android/Linux/iOS/Windows applications with python. Use it with kivimd to make it beautiful, check the examples and the docs . For beginner tutorials check the real python's and towards data science (and part 2 ). * apprise : Allows you to send a notification to almost all of the most popular notification services available to us today such as: Linux, Telegram, Discord, Slack, Amazon SNS, Gotify, etc. Look at all the supported notifications (\u00ac\u00ba-\u00b0)\u00ac . * aiomultiprocess : Presents a simple interface, while running a full AsyncIO event loop on each child process, enabling levels of concurrency never before seen in a Python application. Each child process can execute multiple coroutines at once, limited only by the workload and number of cores available. * twint : An advanced Twitter scraping & OSINT tool written in Python that doesn't use Twitter's API, allowing you to scrape a user's followers, following, Tweets and more while evading most API limitations. * tweepy : Twitter for Python.","title":"Python"},{"location":"python/#interesting-libraries-to-explore","text":"tryceratops : A linter of exceptions. schedule : Python job scheduling for humans. Run Python functions (or any other callable) periodically using a friendly syntax. textual : Textual is a TUI (Text User Interface) framework for Python using Rich as a renderer. parso : Parses Python code. kivi : Create android/Linux/iOS/Windows applications with python. Use it with kivimd to make it beautiful, check the examples and the docs . For beginner tutorials check the real python's and towards data science (and part 2 ). * apprise : Allows you to send a notification to almost all of the most popular notification services available to us today such as: Linux, Telegram, Discord, Slack, Amazon SNS, Gotify, etc. Look at all the supported notifications (\u00ac\u00ba-\u00b0)\u00ac . * aiomultiprocess : Presents a simple interface, while running a full AsyncIO event loop on each child process, enabling levels of concurrency never before seen in a Python application. Each child process can execute multiple coroutines at once, limited only by the workload and number of cores available. * twint : An advanced Twitter scraping & OSINT tool written in Python that doesn't use Twitter's API, allowing you to scrape a user's followers, following, Tweets and more while evading most API limitations. * tweepy : Twitter for Python.","title":"Interesting libraries to explore"},{"location":"python_jinja2/","text":"Jinja2 is a modern and designer-friendly templating language for Python, modelled after Django\u2019s templates. It is fast, widely used and secure with the optional sandboxed template execution environment: <title>{% block title %}{% endblock %}</title> <ul> {% for user in users %} <li><a href=\"{{ user.url }}\">{{ user.username }}</a></li> {% endfor %} </ul> Features: Sandboxed execution. Powerful automatic HTML escaping system for XSS prevention. Template inheritance. Compiles down to the optimal python code just in time. Optional ahead-of-time template compilation. Easy to debug. Line numbers of exceptions directly point to the correct line in the template. Configurable syntax. Installation \u2691 pip install Jinja2 Usage \u2691 The most basic way to create a template and render it is through Template . This however is not the recommended way to work with it if your templates are not loaded from strings but the file system or another data source: >>> from jinja2 import Template >>> template = Template ( 'Hello {{ name }}!' ) >>> template . render ( name = 'John Doe' ) u 'Hello John Doe!' Jinja uses a central object called the template Environment . Instances of this class are used to store the configuration and global objects, and are used to load templates from the file system or other locations. The simplest way to configure Jinja to load templates for your application looks roughly like this: from jinja2 import Environment , PackageLoader , select_autoescape env = Environment ( loader = PackageLoader ( 'yourapplication' , 'templates' ), autoescape = select_autoescape ([ 'html' , 'xml' ]) ) This will create a template environment with the default settings and a loader that looks up the templates in the templates folder inside the yourapplication python package. Different loaders are available and you can also write your own if you want to load templates from a database or other resources. This also enables autoescaping for HTML and XML files. To load a template from this environment you just have to call the get_template() method which then returns the loaded Template: template = env . get_template ( 'mytemplate.html' ) To render it with some variables, just call the render() method: print ( template . render ( the = 'variables' , go = 'here' )) Template guidelines \u2691 Variables \u2691 Reference variables using {{ braces }} notation. Iteration/Loops \u2691 One in each line: {% for host in groups['tag_Function_logdb'] %} elasticsearch_discovery_zen_ping_unicast_hosts = {{ host }}:9300 {% endfor %} Inline: ALLOWED_HOSTS = [{% for domain in domains %}\" {{ domain }}\",{% endfor %}] Get the counter of the iteration \u2691 >>> from jinja2 import Template >>> s = \"{ % f or element in elements %}{{loop.index}} { % e ndfor %}\" >>> Template ( s ) . render ( elements = [ \"a\" , \"b\" , \"c\" , \"d\" ]) 1 2 3 4 Lookup \u2691 Get environmental variable \u2691 lookup('env','HOME') Join \u2691 lineinfile: dest=/etc/hosts line=\"{{ item.ip }} {{ item.aliases|join(' ') }}\" Map \u2691 Get elements of a dictionary set_fact : asg_instances=\"{{ instances.results | map(attribute='instances') | map('first') | map(attribute='public_ip_address') | list}}\" Default \u2691 Set default value of variable name : 'lol' new_name : \"{{ name | default('trol') }}\" Rejectattr \u2691 Exclude elements from a list. Filters a sequence of objects by applying a test to the specified attribute of each object, and rejecting the objects with the test succeeding. If no test is specified, the attribute's value will be evaluated as a boolean. {{ users|rejectattr(\"is_active\") }} {{ users|rejectattr(\"email\", \"none\") }} Regex \u2691 {{ 'ansible' | regex_replace('^a.*i(.*)$', 'a\\\\1') }} {{ 'foobar' | regex_replace('^f.*o(.*)$', '\\\\1') }} {{ 'localhost:80' | regex_replace('^(?P<host>.+):(?P<port>\\\\d+)$', '\\\\g<host>, \\\\g<port>') }} Slice string \u2691 {{ variable_name[:-8] }} Conditional \u2691 Conditional variable definition \u2691 {{ 'Update' if files else 'Continue' }} Check if variable is defined \u2691 { % if variable is defined % } Variable : {{ variable }} defined { % endif % } With two statements: \u2691 { % if (backend_environment == 'backend' and environment == 'Dev') : % } { % elif ... % } { % else % } { % endif % } Extract extension from file \u2691 s3_object : code/frontal/vodafone-v8.18.81.zip remote_clone_dir : \"{{deploy_dir}}/{{ s3_object | basename | splitext | first}}\" Comments \u2691 {# comment here #} Inheritance \u2691 For simple inclusions use include for more complex extend . Include \u2691 To include a snippet from another file you can use {% include '_post.html' %} Extend \u2691 To inherit from another document you can use the block control statement. Blocks are given a unique name, which derived templates can reference when they provide their content .base.html < html > < head > {% if title %} < title > {{ title }} - Microblog </ title > {% else %} < title > Welcome to Microblog </ title > {% endif %} </ head > < body > < div > Microblog: < a href = \"/index\" > Home </ a > </ div > < hr > {% block content %}{% endblock %} </ body > </ html > .index.html {% extends \"base.html\" %} {% block content %} < h1 > Hi </ h1 > {% endblock %} Execute a function and return the value to a variable \u2691 {% with messages = get_flashed_messages() %} {% if messages %} <ul> {% for message in messages %} <li>{{ message }}</li> {% endfor %} </ul> {% endif %} {% endwith %} Macros \u2691 Macros are comparable with functions in regular programming languages. They are useful to put often used idioms into reusable functions to not repeat yourself (\u201cDRY\u201d). {% macro input(name, value='', type='text', size=20) -%} <input type=\"{{ type }}\" name=\"{{ name }}\" value=\"{{ value|e }}\" size=\"{{ size }}\"> {%- endmacro %} The macro can then be called like a function in the namespace: <p>{{ input('username') }}</p> <p>{{ input('password', type='password') }}</p> Wrap long lines and indent \u2691 You can prepend the given string with a newline character, then use the wordwrap filter to wrap the text into multiple lines first, and use the replace filter to replace newline characters with newline plus ' ': {{ ('\\n' ~ item.comment) | wordwrap(76) | replace('\\n', '\\n ') }} The above assumes you want each line to be no more than 80 characters. Change 76 to your desired line width minus 4 to leave room for the indentation. Test if variable is None \u2691 Use the none test (not to be confused with Python's None object!): {% if p is not none %} {{ p.User['first_name'] }} {% else %} NONE {% endif %} References \u2691 Docs","title":"Jinja2"},{"location":"python_jinja2/#installation","text":"pip install Jinja2","title":"Installation"},{"location":"python_jinja2/#usage","text":"The most basic way to create a template and render it is through Template . This however is not the recommended way to work with it if your templates are not loaded from strings but the file system or another data source: >>> from jinja2 import Template >>> template = Template ( 'Hello {{ name }}!' ) >>> template . render ( name = 'John Doe' ) u 'Hello John Doe!' Jinja uses a central object called the template Environment . Instances of this class are used to store the configuration and global objects, and are used to load templates from the file system or other locations. The simplest way to configure Jinja to load templates for your application looks roughly like this: from jinja2 import Environment , PackageLoader , select_autoescape env = Environment ( loader = PackageLoader ( 'yourapplication' , 'templates' ), autoescape = select_autoescape ([ 'html' , 'xml' ]) ) This will create a template environment with the default settings and a loader that looks up the templates in the templates folder inside the yourapplication python package. Different loaders are available and you can also write your own if you want to load templates from a database or other resources. This also enables autoescaping for HTML and XML files. To load a template from this environment you just have to call the get_template() method which then returns the loaded Template: template = env . get_template ( 'mytemplate.html' ) To render it with some variables, just call the render() method: print ( template . render ( the = 'variables' , go = 'here' ))","title":"Usage"},{"location":"python_jinja2/#template-guidelines","text":"","title":"Template guidelines"},{"location":"python_jinja2/#variables","text":"Reference variables using {{ braces }} notation.","title":"Variables"},{"location":"python_jinja2/#iterationloops","text":"One in each line: {% for host in groups['tag_Function_logdb'] %} elasticsearch_discovery_zen_ping_unicast_hosts = {{ host }}:9300 {% endfor %} Inline: ALLOWED_HOSTS = [{% for domain in domains %}\" {{ domain }}\",{% endfor %}]","title":"Iteration/Loops"},{"location":"python_jinja2/#get-the-counter-of-the-iteration","text":">>> from jinja2 import Template >>> s = \"{ % f or element in elements %}{{loop.index}} { % e ndfor %}\" >>> Template ( s ) . render ( elements = [ \"a\" , \"b\" , \"c\" , \"d\" ]) 1 2 3 4","title":"Get the counter of the iteration"},{"location":"python_jinja2/#lookup","text":"","title":"Lookup"},{"location":"python_jinja2/#get-environmental-variable","text":"lookup('env','HOME')","title":"Get environmental variable"},{"location":"python_jinja2/#join","text":"lineinfile: dest=/etc/hosts line=\"{{ item.ip }} {{ item.aliases|join(' ') }}\"","title":"Join"},{"location":"python_jinja2/#map","text":"Get elements of a dictionary set_fact : asg_instances=\"{{ instances.results | map(attribute='instances') | map('first') | map(attribute='public_ip_address') | list}}\"","title":"Map"},{"location":"python_jinja2/#default","text":"Set default value of variable name : 'lol' new_name : \"{{ name | default('trol') }}\"","title":"Default"},{"location":"python_jinja2/#rejectattr","text":"Exclude elements from a list. Filters a sequence of objects by applying a test to the specified attribute of each object, and rejecting the objects with the test succeeding. If no test is specified, the attribute's value will be evaluated as a boolean. {{ users|rejectattr(\"is_active\") }} {{ users|rejectattr(\"email\", \"none\") }}","title":"Rejectattr"},{"location":"python_jinja2/#regex","text":"{{ 'ansible' | regex_replace('^a.*i(.*)$', 'a\\\\1') }} {{ 'foobar' | regex_replace('^f.*o(.*)$', '\\\\1') }} {{ 'localhost:80' | regex_replace('^(?P<host>.+):(?P<port>\\\\d+)$', '\\\\g<host>, \\\\g<port>') }}","title":"Regex"},{"location":"python_jinja2/#slice-string","text":"{{ variable_name[:-8] }}","title":"Slice string"},{"location":"python_jinja2/#conditional","text":"","title":"Conditional"},{"location":"python_jinja2/#conditional-variable-definition","text":"{{ 'Update' if files else 'Continue' }}","title":"Conditional variable definition"},{"location":"python_jinja2/#check-if-variable-is-defined","text":"{ % if variable is defined % } Variable : {{ variable }} defined { % endif % }","title":"Check if variable is defined"},{"location":"python_jinja2/#with-two-statements","text":"{ % if (backend_environment == 'backend' and environment == 'Dev') : % } { % elif ... % } { % else % } { % endif % }","title":"With two statements:"},{"location":"python_jinja2/#extract-extension-from-file","text":"s3_object : code/frontal/vodafone-v8.18.81.zip remote_clone_dir : \"{{deploy_dir}}/{{ s3_object | basename | splitext | first}}\"","title":"Extract extension from file"},{"location":"python_jinja2/#comments","text":"{# comment here #}","title":"Comments"},{"location":"python_jinja2/#inheritance","text":"For simple inclusions use include for more complex extend .","title":"Inheritance"},{"location":"python_jinja2/#include","text":"To include a snippet from another file you can use {% include '_post.html' %}","title":"Include"},{"location":"python_jinja2/#extend","text":"To inherit from another document you can use the block control statement. Blocks are given a unique name, which derived templates can reference when they provide their content .base.html < html > < head > {% if title %} < title > {{ title }} - Microblog </ title > {% else %} < title > Welcome to Microblog </ title > {% endif %} </ head > < body > < div > Microblog: < a href = \"/index\" > Home </ a > </ div > < hr > {% block content %}{% endblock %} </ body > </ html > .index.html {% extends \"base.html\" %} {% block content %} < h1 > Hi </ h1 > {% endblock %}","title":"Extend"},{"location":"python_jinja2/#execute-a-function-and-return-the-value-to-a-variable","text":"{% with messages = get_flashed_messages() %} {% if messages %} <ul> {% for message in messages %} <li>{{ message }}</li> {% endfor %} </ul> {% endif %} {% endwith %}","title":"Execute a function and return the value to a variable"},{"location":"python_jinja2/#macros","text":"Macros are comparable with functions in regular programming languages. They are useful to put often used idioms into reusable functions to not repeat yourself (\u201cDRY\u201d). {% macro input(name, value='', type='text', size=20) -%} <input type=\"{{ type }}\" name=\"{{ name }}\" value=\"{{ value|e }}\" size=\"{{ size }}\"> {%- endmacro %} The macro can then be called like a function in the namespace: <p>{{ input('username') }}</p> <p>{{ input('password', type='password') }}</p>","title":"Macros"},{"location":"python_jinja2/#wrap-long-lines-and-indent","text":"You can prepend the given string with a newline character, then use the wordwrap filter to wrap the text into multiple lines first, and use the replace filter to replace newline characters with newline plus ' ': {{ ('\\n' ~ item.comment) | wordwrap(76) | replace('\\n', '\\n ') }} The above assumes you want each line to be no more than 80 characters. Change 76 to your desired line width minus 4 to leave room for the indentation.","title":"Wrap long lines and indent"},{"location":"python_jinja2/#test-if-variable-is-none","text":"Use the none test (not to be confused with Python's None object!): {% if p is not none %} {{ p.User['first_name'] }} {% else %} NONE {% endif %}","title":"Test if variable is None"},{"location":"python_jinja2/#references","text":"Docs","title":"References"},{"location":"python_logging/","text":"Logging information in your Python programs makes it possible to debug problems when running. For command line application that the user is going to run directly, the logging module might be enough. For command line tools or APIs that are going to be run by a server, it might fall short. logging will write exceptions and breadcrumbs to a file, and unless you look at it directly most errors will pass unnoticed. To actively monitor and react to code exceptions use an application monitoring platform like sentry . They gather de data on your application and aggregate the errors in a user friendly way, such as: Showing the context of the error in a web interface. Gather both front and backend issues in one place. Show the trail of events that led to the exceptions with breadcrumbs. Show the probable commit that introduced the bug. Link problems with issue tracker issues. See the impact of each bug with the number of occurrences and users that are experiencing it. Visualize all the data in dashboards. Get notifications on the issues raised. Check the demo to see its features. You can self-host sentry , but it uses a docker-compose that depends on 12 services, including postgres, redis and kafka with a minimum requirements of 4 cores and 8 GB of RAM. So I've looked for a simple solution, and arrived to GlitchTip , a similar solution that even uses the sentry SDK, but has a smaller system footprint, and it's open sourced, while sentry is not anymore . Check it's documentation and source code .","title":"Logging"},{"location":"python_mysql/","text":"Installation \u2691 pip install mysql-connector-python Usage \u2691 import mysql.connector # Connect to server cnx = mysql . connector . connect ( host = \"127.0.0.1\" , port = 3306 , user = \"mike\" , password = \"s3cre3t!\" ) # Get a cursor cur = cnx . cursor () # Execute a query cur . execute ( \"SELECT CURDATE()\" ) # Fetch one result row = cur . fetchone () print ( \"Current date is: {0} \" . format ( row [ 0 ])) # Close connection cnx . close () Iterate over the results of the cursor execution \u2691 cursor . execute ( show_db_query ) for db in cursor : print ( db ) References \u2691 Git Docs RealPython tutorial","title":"Python Mysql"},{"location":"python_mysql/#installation","text":"pip install mysql-connector-python","title":"Installation"},{"location":"python_mysql/#usage","text":"import mysql.connector # Connect to server cnx = mysql . connector . connect ( host = \"127.0.0.1\" , port = 3306 , user = \"mike\" , password = \"s3cre3t!\" ) # Get a cursor cur = cnx . cursor () # Execute a query cur . execute ( \"SELECT CURDATE()\" ) # Fetch one result row = cur . fetchone () print ( \"Current date is: {0} \" . format ( row [ 0 ])) # Close connection cnx . close ()","title":"Usage"},{"location":"python_mysql/#iterate-over-the-results-of-the-cursor-execution","text":"cursor . execute ( show_db_query ) for db in cursor : print ( db )","title":"Iterate over the results of the cursor execution"},{"location":"python_mysql/#references","text":"Git Docs RealPython tutorial","title":"References"},{"location":"python_optimization/","text":"Optimization can be done through different metrics, such as, CPU performance (execution time) or memory footprint. Optimizing your code makes sense when you are sure that the business logic in the code is correct and not going to change soon. \"First make it work. Then make it right. Then make it fast.\" ~ Kent Beck Unless you're developing a performance-intensive product or a code dependency that is going to be used by other projects which might be performance-intensive, optimizing every aspect of the code can be overkill. For most of the scenarios, the 80-20 principle (80 percent of performance benefits may come from optimizing 20 percent of your code) will be more appropriate. Most of the time we make intuitive guesses on what the bottlenecks are, but more often than not, our guesses are either wrong or just approximately correct. So, it's always advisable to use profiling tools to identify how often a resource is used and who is using the resource. For instance, a profiler designed for profiling execution time will measure how often and for how various long parts of the code are executed. Using a profiling mechanism becomes a necessity when the codebase grows large, and you still want to maintain efficiency. Tips \u2691 Minimize the relative import statements on command line tools \u2691 When developing a library, it's common to expose the main objects into the package __init__.py under the variable __all__ . The problem with command line programs is that each time you run the command it will load those objects, which can mean an increase of 0.5s or even a second for each command, which is unacceptable. Following this string, if you manage to minimize the relative imports, you'll make your code faster. Python's wiki discusses different places to locate your import statements. If you put them on the top, the imports that you don't need for that command in particular will worsen your load time, if you add them inside the functions, if you run the function more than once, the performance drops too, and it's a common etiquete to have all your imports on the top. One step that you can do is to mark the imports required for type checking under a conditional: from typing import TYPE_CHECKING if TYPE_CHECKING : from model import Object This change can be negligible, and it will force you to use 'Object' , instead of Object in the typing information, which is not nice, so it may not be worth it. If you are still unable to make the loading time drop below an acceptable time, you can migrate to a server-client architecture, where all the logic is loaded by the backend (once as it's always running), and have a \"silly\" client that only does requests to the backend. Beware though, as you will add the network latency. Don't dynamically install the package \u2691 If you install the package with pip install -e . you will see an increase on the load time of ~0.2s. It is useful to develop the package, but when you use it, do so from a virtualenv that installs it directly without the -e flag. References \u2691 Satwik Kansal article on Scout APM","title":"Optimization"},{"location":"python_optimization/#tips","text":"","title":"Tips"},{"location":"python_optimization/#minimize-the-relative-import-statements-on-command-line-tools","text":"When developing a library, it's common to expose the main objects into the package __init__.py under the variable __all__ . The problem with command line programs is that each time you run the command it will load those objects, which can mean an increase of 0.5s or even a second for each command, which is unacceptable. Following this string, if you manage to minimize the relative imports, you'll make your code faster. Python's wiki discusses different places to locate your import statements. If you put them on the top, the imports that you don't need for that command in particular will worsen your load time, if you add them inside the functions, if you run the function more than once, the performance drops too, and it's a common etiquete to have all your imports on the top. One step that you can do is to mark the imports required for type checking under a conditional: from typing import TYPE_CHECKING if TYPE_CHECKING : from model import Object This change can be negligible, and it will force you to use 'Object' , instead of Object in the typing information, which is not nice, so it may not be worth it. If you are still unable to make the loading time drop below an acceptable time, you can migrate to a server-client architecture, where all the logic is loaded by the backend (once as it's always running), and have a \"silly\" client that only does requests to the backend. Beware though, as you will add the network latency.","title":"Minimize the relative import statements on command line tools"},{"location":"python_optimization/#dont-dynamically-install-the-package","text":"If you install the package with pip install -e . you will see an increase on the load time of ~0.2s. It is useful to develop the package, but when you use it, do so from a virtualenv that installs it directly without the -e flag.","title":"Don't dynamically install the package"},{"location":"python_optimization/#references","text":"Satwik Kansal article on Scout APM","title":"References"},{"location":"python_plugin_system/","text":"When building Python applications, it's good to develop the core of your program, and allow extension via plugins. I still don't know how to do it, but I'm going to gather interesting references until I tackle it. Beets plugin system looks awesome.","title":"Plugin System"},{"location":"python_profiling/","text":"Profiling is to find out where your code spends its time. Profilers can collect several types of information: timing, function calls, interruptions or cache faults. It can be useful to identify bottlenecks, which should be the first step when trying to optimize some code, or to study the evolution of the performance of your code. Profiling types \u2691 There are two types of profiling: Deterministic Profiling All events are monitored. It provides accurate information but has a big impact on performance (overhead). It means the code runs slower under profiling. Its use in production systems is often impractical. This type of profiling is suitable for small functions. Statistical profiling Sampling the execution state at regular intervals to compute indicators. This method is less accurate, but it also reduces the overhead. Profiling tools \u2691 The profiling tools you should use vary with the code you are working on. If you are writing a single algorithm or a small program, you should use a simple profiler like cProfile or even a fine-grained tool like line_profiler . In contrast, when you are optimizing a whole program, you may want to use a statistical profiler to avoid overhead, such as pyinstrument , or if you're debugging a running process, using py-spy . Deterministic Profiling \u2691 cProfile \u2691 Python comes with two built-in modules for deterministic profiling: cProfile and profile. Both are different implementations of the same interface. The former is a C extension with relatively small overhead, and the latter is a pure Python module. As the official documentation says, the module profile would be suitable when we want to extend the profiler in some way. Otherwise, cProfile is preferred for long-running programs. Unfortunately, there is no built-in module for statistical profiling, but we will see some external packages for it. $: python3 -m cProfile script.py 58 function calls in 9 .419 seconds Ordered by: standard namen calls tottime percall cumtime percall filename:lineno ( function ) 1 0 .000 0 .000 9 .419 9 .419 part1.py:1 ( <module> ) 51 9 .419 0 .185 9 .419 0 .185 part1.py:1 ( computation ) 1 0 .000 0 .000 9 .419 9 .419 part1.py:10 ( function1 ) 1 0 .000 0 .000 9 .243 9 .243 part1.py:15 ( function2 ) 1 0 .000 0 .000 0 .176 0 .176 part1.py:20 ( function3 ) 1 0 .000 0 .000 9 .419 9 .419 part1.py:24 ( main ) Where: ncalls Is the number of calls. We should try to optimize functions that have a lot of calls or consume too much time per call. tottime The total time spent in the function itself, excluding sub calls. This is where we should look closely at. We can see that the function computation is called 51 times, and each time consumes 0.185s. cumtime Cumulative time. It includes sub calls. percall We have two \u201cper call\u201d metrics. The first one: total time per call, and the second one: cumulative time per call. Again, we should focus on the total time metric. We can also sort the functions by some criteria, for example python3 -m cProfile -s tottime script.py . Statistical profiling \u2691 Py-spy \u2691 Py-Spy is a statistical (sampling) profiler that lets you visualize the time each function consumes during the execution. An important feature is that you can attach the profiler without restarting the program or modifying the code, and has a low overhead. This makes the tool highly suitable for production code. To install it, just type: pip install py-spy To test the performance of a file use: py-spy top python3 script.py To assess the performance of a runnin process, specify it's PID: py-spy top --pid $PID They will show a top like interface showing the following data: GIL: 100.00%, Active: 100.00%, Threads: 1 %Own %Total OwnTime TotalTime Function (filename:line) 61.00% 61.00% 10.50s 10.50s computation (script.py:7) 39.00% 39.00% 7.50s 7.50s computation (script.py:6) 0.00% 100.00% 0.000s 18.00s <module> (script.py:30) 0.00% 100.00% 0.000s 18.00s function2 (script.py:18) 0.00% 100.00% 0.000s 18.00s main (script.py:26) 0.00% 100.00% 0.000s 18.00s function1 (script.py:12) pyinstrument \u2691 It is similar to cProfile in the sense that we can\u2019t attach the profiler to a running program, but that is where the similarities end, as pyinstrument doesn't track every function call that your program makes. Instead, it's recording the call stack every 1ms. Install it with: pip install pyinstrument Use: The advantages are that: The output is far more attractive. It has less overhead, so it distorts less the results. Doesn't show the internal calls that make cProfiling result reading difficult. It uses wall-clock time instead of CPU time. So it takes into account the IO time. $: pyinstrument script.py _ ._ __/__ _ _ _ _ _/_ Recorded: 15 :45:20 Samples: 51 /_//_/// /_ \\ / //_// / //_ ' / // Duration: 4 .517 CPU time: 4 .516 / _/ v3.3.0 Program: script.py 4 .516 <module> script.py:2 \u2514\u2500 4 .516 main script.py:25 \u2514\u2500 4 .516 function1 script.py:11 \u251c\u2500 4 .425 function2 script.py:16 \u2502 \u2514\u2500 4 .425 computation script.py:2 \u2514\u2500 0 .092 function3 script.py:21 \u2514\u2500 0 .092 computation script.py:2 With the possibility to generate an HTML report. The disadvantages are that it's only easy to profile python script files, not full packages. You can also profile a chunk of code , which can be useful when developing or for writing performance tests. from pyinstrument import Profiler profiler = Profiler () profiler . start () # code you want to profile profiler . stop () print ( profiler . output_text ( unicode = True , color = True )) To explore the profile in a web browser, use profiler.open_in_browser() . To save this HTML for later, use profiler.output_html() . Introduce profiling in your test workflow \u2691 I run out of time, so here are the starting points: Niklas Meinzer post Pypi page of pytest-benchmark , Docs , Git Docs of pytest-profiling uwpce guide on using pstats The idea is to develop the following ideas: How to integrate profiling with pytest. How to compare benchmark results between CI runs. Some guidelines on writing performance tests References \u2691 Antonio Molner article on Python Profiling","title":"Profiling"},{"location":"python_profiling/#profiling-types","text":"There are two types of profiling: Deterministic Profiling All events are monitored. It provides accurate information but has a big impact on performance (overhead). It means the code runs slower under profiling. Its use in production systems is often impractical. This type of profiling is suitable for small functions. Statistical profiling Sampling the execution state at regular intervals to compute indicators. This method is less accurate, but it also reduces the overhead.","title":"Profiling types"},{"location":"python_profiling/#profiling-tools","text":"The profiling tools you should use vary with the code you are working on. If you are writing a single algorithm or a small program, you should use a simple profiler like cProfile or even a fine-grained tool like line_profiler . In contrast, when you are optimizing a whole program, you may want to use a statistical profiler to avoid overhead, such as pyinstrument , or if you're debugging a running process, using py-spy .","title":"Profiling tools"},{"location":"python_profiling/#deterministic-profiling","text":"","title":"Deterministic Profiling"},{"location":"python_profiling/#cprofile","text":"Python comes with two built-in modules for deterministic profiling: cProfile and profile. Both are different implementations of the same interface. The former is a C extension with relatively small overhead, and the latter is a pure Python module. As the official documentation says, the module profile would be suitable when we want to extend the profiler in some way. Otherwise, cProfile is preferred for long-running programs. Unfortunately, there is no built-in module for statistical profiling, but we will see some external packages for it. $: python3 -m cProfile script.py 58 function calls in 9 .419 seconds Ordered by: standard namen calls tottime percall cumtime percall filename:lineno ( function ) 1 0 .000 0 .000 9 .419 9 .419 part1.py:1 ( <module> ) 51 9 .419 0 .185 9 .419 0 .185 part1.py:1 ( computation ) 1 0 .000 0 .000 9 .419 9 .419 part1.py:10 ( function1 ) 1 0 .000 0 .000 9 .243 9 .243 part1.py:15 ( function2 ) 1 0 .000 0 .000 0 .176 0 .176 part1.py:20 ( function3 ) 1 0 .000 0 .000 9 .419 9 .419 part1.py:24 ( main ) Where: ncalls Is the number of calls. We should try to optimize functions that have a lot of calls or consume too much time per call. tottime The total time spent in the function itself, excluding sub calls. This is where we should look closely at. We can see that the function computation is called 51 times, and each time consumes 0.185s. cumtime Cumulative time. It includes sub calls. percall We have two \u201cper call\u201d metrics. The first one: total time per call, and the second one: cumulative time per call. Again, we should focus on the total time metric. We can also sort the functions by some criteria, for example python3 -m cProfile -s tottime script.py .","title":"cProfile"},{"location":"python_profiling/#statistical-profiling","text":"","title":"Statistical profiling"},{"location":"python_profiling/#py-spy","text":"Py-Spy is a statistical (sampling) profiler that lets you visualize the time each function consumes during the execution. An important feature is that you can attach the profiler without restarting the program or modifying the code, and has a low overhead. This makes the tool highly suitable for production code. To install it, just type: pip install py-spy To test the performance of a file use: py-spy top python3 script.py To assess the performance of a runnin process, specify it's PID: py-spy top --pid $PID They will show a top like interface showing the following data: GIL: 100.00%, Active: 100.00%, Threads: 1 %Own %Total OwnTime TotalTime Function (filename:line) 61.00% 61.00% 10.50s 10.50s computation (script.py:7) 39.00% 39.00% 7.50s 7.50s computation (script.py:6) 0.00% 100.00% 0.000s 18.00s <module> (script.py:30) 0.00% 100.00% 0.000s 18.00s function2 (script.py:18) 0.00% 100.00% 0.000s 18.00s main (script.py:26) 0.00% 100.00% 0.000s 18.00s function1 (script.py:12)","title":"Py-spy"},{"location":"python_profiling/#pyinstrument","text":"It is similar to cProfile in the sense that we can\u2019t attach the profiler to a running program, but that is where the similarities end, as pyinstrument doesn't track every function call that your program makes. Instead, it's recording the call stack every 1ms. Install it with: pip install pyinstrument Use: The advantages are that: The output is far more attractive. It has less overhead, so it distorts less the results. Doesn't show the internal calls that make cProfiling result reading difficult. It uses wall-clock time instead of CPU time. So it takes into account the IO time. $: pyinstrument script.py _ ._ __/__ _ _ _ _ _/_ Recorded: 15 :45:20 Samples: 51 /_//_/// /_ \\ / //_// / //_ ' / // Duration: 4 .517 CPU time: 4 .516 / _/ v3.3.0 Program: script.py 4 .516 <module> script.py:2 \u2514\u2500 4 .516 main script.py:25 \u2514\u2500 4 .516 function1 script.py:11 \u251c\u2500 4 .425 function2 script.py:16 \u2502 \u2514\u2500 4 .425 computation script.py:2 \u2514\u2500 0 .092 function3 script.py:21 \u2514\u2500 0 .092 computation script.py:2 With the possibility to generate an HTML report. The disadvantages are that it's only easy to profile python script files, not full packages. You can also profile a chunk of code , which can be useful when developing or for writing performance tests. from pyinstrument import Profiler profiler = Profiler () profiler . start () # code you want to profile profiler . stop () print ( profiler . output_text ( unicode = True , color = True )) To explore the profile in a web browser, use profiler.open_in_browser() . To save this HTML for later, use profiler.output_html() .","title":"pyinstrument"},{"location":"python_profiling/#introduce-profiling-in-your-test-workflow","text":"I run out of time, so here are the starting points: Niklas Meinzer post Pypi page of pytest-benchmark , Docs , Git Docs of pytest-profiling uwpce guide on using pstats The idea is to develop the following ideas: How to integrate profiling with pytest. How to compare benchmark results between CI runs. Some guidelines on writing performance tests","title":"Introduce profiling in your test workflow"},{"location":"python_profiling/#references","text":"Antonio Molner article on Python Profiling","title":"References"},{"location":"questionary/","text":"questionary is a Python library based on Prompt Toolkit to effortlessly building pretty command line interfaces. It makes it very easy to query your user for input. Installation \u2691 pip install questionary Usage \u2691 Asking a single question \u2691 Questionary ships with a lot of different Question Types to provide the right prompt for the right question. All of them work in the same way though. import questionary answer = questionary . text ( \"What's your first name\" ) . ask () Since our question is a text prompt, answer will contain the text the user typed after they submitted it. Asking Multiple Questions \u2691 You can use the form() function to ask a collection of Questions . The questions will be asked in the order they are passed to questionary.form . import questionary answers = questionary . form ( first = questionary . confirm ( \"Would you like the next question?\" , default = True ), second = questionary . select ( \"Select item\" , choices = [ \"item1\" , \"item2\" , \"item3\" ]) ) . ask () The output will have the following format: { ' f irs t ' : True , 'seco n d' : 'i te m 2 ' } The prompt() function also allows you to ask a collection of questions, however instead of taking Question instances, it takes a dictionary: import questionary questions = [ { \"type\" : \"confirm\" , \"name\" : \"first\" , \"message\" : \"Would you like the next question?\" , \"default\" : True , }, { \"type\" : \"select\" , \"name\" : \"second\" , \"message\" : \"Select item\" , \"choices\" : [ \"item1\" , \"item2\" , \"item3\" ], }, ] questionary . prompt ( questions ) Question types \u2691 The different question types are meant to cover different use cases. The parameters and configuration options are explained in detail for each type. But before we get into to many details, here is a cheatsheet with the available question types: Use Text to ask for free text input. Use Password to ask for free text where the text is hidden. Use File Path to ask for a file or directory path with autocompletion. Use Confirmation to ask a yes or no question. Use Select to ask the user to select one item from a beautiful list. Use Raw Select to ask the user to select one item from a list. Use Checkbox to ask the user to select any number of items from a list. Use Autocomplete to ask for free text with autocomplete help. Check the examples to see them in action and how to use them. Testing \u2691 To test questionary code, follow the guidelines of testing prompt_toolkit . References \u2691 Docs Git","title":"questionary"},{"location":"questionary/#installation","text":"pip install questionary","title":"Installation"},{"location":"questionary/#usage","text":"","title":"Usage"},{"location":"questionary/#asking-a-single-question","text":"Questionary ships with a lot of different Question Types to provide the right prompt for the right question. All of them work in the same way though. import questionary answer = questionary . text ( \"What's your first name\" ) . ask () Since our question is a text prompt, answer will contain the text the user typed after they submitted it.","title":"Asking a single question"},{"location":"questionary/#asking-multiple-questions","text":"You can use the form() function to ask a collection of Questions . The questions will be asked in the order they are passed to questionary.form . import questionary answers = questionary . form ( first = questionary . confirm ( \"Would you like the next question?\" , default = True ), second = questionary . select ( \"Select item\" , choices = [ \"item1\" , \"item2\" , \"item3\" ]) ) . ask () The output will have the following format: { ' f irs t ' : True , 'seco n d' : 'i te m 2 ' } The prompt() function also allows you to ask a collection of questions, however instead of taking Question instances, it takes a dictionary: import questionary questions = [ { \"type\" : \"confirm\" , \"name\" : \"first\" , \"message\" : \"Would you like the next question?\" , \"default\" : True , }, { \"type\" : \"select\" , \"name\" : \"second\" , \"message\" : \"Select item\" , \"choices\" : [ \"item1\" , \"item2\" , \"item3\" ], }, ] questionary . prompt ( questions )","title":"Asking Multiple Questions"},{"location":"questionary/#question-types","text":"The different question types are meant to cover different use cases. The parameters and configuration options are explained in detail for each type. But before we get into to many details, here is a cheatsheet with the available question types: Use Text to ask for free text input. Use Password to ask for free text where the text is hidden. Use File Path to ask for a file or directory path with autocompletion. Use Confirmation to ask a yes or no question. Use Select to ask the user to select one item from a beautiful list. Use Raw Select to ask the user to select one item from a list. Use Checkbox to ask the user to select any number of items from a list. Use Autocomplete to ask for free text with autocomplete help. Check the examples to see them in action and how to use them.","title":"Question types"},{"location":"questionary/#testing","text":"To test questionary code, follow the guidelines of testing prompt_toolkit .","title":"Testing"},{"location":"questionary/#references","text":"Docs Git","title":"References"},{"location":"refinement_template/","text":"Refinement \u2691 Doubts \u2691 Expected current sprint undone tasks \u2691 Review the proposed Kanban board \u2691 Sprint Goals \u2691 With this proposed plan we'll: *","title":"Refinement Template"},{"location":"refinement_template/#refinement","text":"","title":"Refinement"},{"location":"refinement_template/#doubts","text":"","title":"Doubts"},{"location":"refinement_template/#expected-current-sprint-undone-tasks","text":"","title":"Expected current sprint undone tasks"},{"location":"refinement_template/#review-the-proposed-kanban-board","text":"","title":"Review the proposed Kanban board"},{"location":"refinement_template/#sprint-goals","text":"With this proposed plan we'll: *","title":"Sprint Goals"},{"location":"relationship_management/","text":"I try to keep my mind as empty as possible of non relevant processes, that's why I use a task manager to handle my tasks and meetings. This system has a side effect, if there isn't something reminding you that you have to do something, you fail to do it. That principle applied to human relationships means that if you don't stumble that person in your daily life, it doesn't exist for you and you will probably not take enough care that the person deserves. To solve that problem I started creating periodic tasks to call these people or hang out. I've also used those tasks to keep a diary of the interactions. Recently I've found Monica a popular open source personal CRM that helps in the same direction. So I'm going to migrate all my information to the system and see how it goes.","title":"Relationship Management"},{"location":"remote_work/","text":"Remote working is a work arrangement in which employees do not commute or travel (e.g. by bus, bicycle or car, etc.) to a central place of work, such as an office building, warehouse, or store. As a side effect, we're spending a lot of time in front of our computers, so we should be careful that our working environment helps us to stay healthy. For example we could: Use an external monitor: Your laptop's screen is usually not big enough and will force you to look down instead of look straight which can lead to neck pain. Some prefer super big monitors (48 inches) while others feel that 24 inches is more than enough so you don't have to turn your head to reach each side of the screen. For me the sweet spot is having two terminals with 100 characters of width one beside the other. If you use a tiling window manager like i3wm , that should be enough. Some people valued that the screen was not fixed, so it could be tilted or it's height could be changed. Adjust the screen to your eye level: The center of the monitor should be at eye level, if the monitor height adjustment is not enough, you can use some old books or buy a screen support. Use an external keyboard: Sometimes the keys of the laptop keyboards have a cheap feedback or a weird key disposition, which leads to finger and wrist aches. The use of an external keyboard (better if it's a mechanical one) can help with this issue. The chair should support your back and don't be too hard to hurt your butt, nor too soft. Your legs should not be hanging in the air, that will add unnecessary pressure on your thighs which can lead to tingling. If you're in this situation, a leg support comes handy. The legs shouldn't be crossed either in front or below you, they should be straight with a 90 degree angle between your thighs and your calves, with a waist level separation between the feet. The table height should be enough to have a 90 degree angle between your forearms and your biceps , and your shoulders are in a relaxed stance. Small people may need a table with no drawers between your elbows and your legs, or you wont be able to fulfill the arm's requirement. The table height should be low enough to fulfill the leg's requirement above. Sometimes they are too high to be able to have a 90 degree angle between the thighs and calves even with feet support, in that case, change the desk or cut it's legs. Think about using a standing desk. Desk's with variable height are quite expensive, but there is always the option to buy a laptop support that let's you stand. Your hands should be at the same level as your forearms, you could use a wrist support for that and also to soften the contact of your forearms with the desk. If you're a heavy mouse user, think of using a vertical mouse instead of the traditional to prevent the metacarpal syndrome. And try not to use it! learn how to use a tiling window manager and Vim shortcuts for everything, such as using tridactyl for Firefox. Keep your working place between 19 and 21 degrees centigrades, otherwise you may unawarely contract your body. Use blue filter either in your glasses or in your screen. Have enough light so you don't need to strain your eyes. Having your monitor screen as your only source of light is harmful. Try to promote initiatives that increase the social interaction between your fellow workers. Stand up and walk around at least once each two hours. Meetings are a good moment to do an outside walk. Other tips non related with your work environment but with the consequences of your work experience can be: Don't remain static, doing exercise daily is a must. As you don't need to go out, it's quite easy to fall into the routine of waking up, sit in your computer, eat and go back to sleep. Both anaerobic (pilates, yoga or stretching) and aerobic (running, biking or dancing) give different benefits. Drink enough water, around 8 to 10 glasses per day. Use the extra time that remote working gives you to strengthen your outside work social relationships. Try not to be exposed to any screen light for an hour before you go to sleep. If you use an e-book, don't rely on their builtin light, use an external source instead. If you have a remote work contract, make sure that your employer pays for any upgrades, it's their responsibility.","title":"Remote Working"},{"location":"requests/","text":"Requests is an elegant and simple HTTP library for Python, built for human beings. Installation \u2691 pip install requests Usage \u2691 Download file \u2691 url = \"http://beispiel.dort/ichbineinbild.jpg\" filename = url . split ( \"/\" )[ - 1 ] r = requests . get ( url , timeout = 0.5 ) if r . status_code == 200 : with open ( filename , 'wb' ) as f : f . write ( r . content ) Encode url \u2691 requests . utils . quote ( '/test' , safe = '' ) Get \u2691 requests . get ( '{{ url }}' ) Put url \u2691 requests . put ({{ url }}) Put json data url \u2691 data = { \"key\" : \"value\" } requests . put ({{ url }} json = data ) Use cookies between requests \u2691 You can use Session objects to persists cookies or default data across all requests. s = requests . Session () s . get ( 'https://httpbin.org/cookies/set/sessioncookie/123456789' ) r = s . get ( 'https://httpbin.org/cookies' ) print ( r . text ) # '{\"cookies\": {\"sessioncookie\": \"123456789\"}}' s . auth = ( 'user' , 'pass' ) s . headers . update ({ 'x-test' : 'true' }) # both 'x-test' and 'x-test2' are sent s . get ( 'https://httpbin.org/headers' , headers = { 'x-test2' : 'true' }) References \u2691 Docs","title":"Requests"},{"location":"requests/#installation","text":"pip install requests","title":"Installation"},{"location":"requests/#usage","text":"","title":"Usage"},{"location":"requests/#download-file","text":"url = \"http://beispiel.dort/ichbineinbild.jpg\" filename = url . split ( \"/\" )[ - 1 ] r = requests . get ( url , timeout = 0.5 ) if r . status_code == 200 : with open ( filename , 'wb' ) as f : f . write ( r . content )","title":"Download file"},{"location":"requests/#encode-url","text":"requests . utils . quote ( '/test' , safe = '' )","title":"Encode url"},{"location":"requests/#get","text":"requests . get ( '{{ url }}' )","title":"Get"},{"location":"requests/#put-url","text":"requests . put ({{ url }})","title":"Put url"},{"location":"requests/#put-json-data-url","text":"data = { \"key\" : \"value\" } requests . put ({{ url }} json = data )","title":"Put json data url"},{"location":"requests/#use-cookies-between-requests","text":"You can use Session objects to persists cookies or default data across all requests. s = requests . Session () s . get ( 'https://httpbin.org/cookies/set/sessioncookie/123456789' ) r = s . get ( 'https://httpbin.org/cookies' ) print ( r . text ) # '{\"cookies\": {\"sessioncookie\": \"123456789\"}}' s . auth = ( 'user' , 'pass' ) s . headers . update ({ 'x-test' : 'true' }) # both 'x-test' and 'x-test2' are sent s . get ( 'https://httpbin.org/headers' , headers = { 'x-test2' : 'true' })","title":"Use cookies between requests"},{"location":"requests/#references","text":"Docs","title":"References"},{"location":"rich/","text":"Rich is a Python library for rich text and beautiful formatting in the terminal. Installation \u2691 pip install rich Usage \u2691 Progress display \u2691 Rich can display continuously updated information regarding the progress of long running tasks / file copies etc. The information displayed is configurable, the default will display a description of the \u2018task\u2019, a progress bar, percentage complete, and estimated time remaining. Rich progress display supports multiple tasks, each with a bar and progress information. You can use this to track concurrent tasks where the work is happening in threads or processes. It's beautiful, check it out with python -m rich.progress . Basic Usage \u2691 For basic usage call the track() function, which accepts a sequence (such as a list or range object) and an optional description of the job you are working on. The track method will yield values from the sequence and update the progress information on each iteration. Here\u2019s an example: from rich.progress import track for n in track ( range ( n ), description = \"Processing...\" ): do_work ( n ) Tables \u2691 from rich.console import Console from rich.table import Table table = Table ( title = \"Star Wars Movies\" ) table . add_column ( \"Released\" , justify = \"right\" , style = \"cyan\" , no_wrap = True ) table . add_column ( \"Title\" , style = \"magenta\" ) table . add_column ( \"Box Office\" , justify = \"right\" , style = \"green\" ) table . add_row ( \"Dec 20, 2019\" , \"Star Wars: The Rise of Skywalker\" , \"$952,110,690\" ) table . add_row ( \"May 25, 2018\" , \"Solo: A Star Wars Story\" , \"$393,151,347\" ) table . add_row ( \"Dec 15, 2017\" , \"Star Wars Ep. V111: The Last Jedi\" , \"$1,332,539,889\" ) table . add_row ( \"Dec 16, 2016\" , \"Rogue One: A Star Wars Story\" , \"$1,332,439,889\" ) console = Console () console . print ( table ) Rich text \u2691 from rich.console import Console from rich.text import Text console = Console () text = Text . assemble (( \"Hello\" , \"bold magenta\" ), \" World!\" ) console . print ( text ) References \u2691 Git Docs","title":"rich"},{"location":"rich/#installation","text":"pip install rich","title":"Installation"},{"location":"rich/#usage","text":"","title":"Usage"},{"location":"rich/#progress-display","text":"Rich can display continuously updated information regarding the progress of long running tasks / file copies etc. The information displayed is configurable, the default will display a description of the \u2018task\u2019, a progress bar, percentage complete, and estimated time remaining. Rich progress display supports multiple tasks, each with a bar and progress information. You can use this to track concurrent tasks where the work is happening in threads or processes. It's beautiful, check it out with python -m rich.progress .","title":"Progress display"},{"location":"rich/#basic-usage","text":"For basic usage call the track() function, which accepts a sequence (such as a list or range object) and an optional description of the job you are working on. The track method will yield values from the sequence and update the progress information on each iteration. Here\u2019s an example: from rich.progress import track for n in track ( range ( n ), description = \"Processing...\" ): do_work ( n )","title":"Basic Usage"},{"location":"rich/#tables","text":"from rich.console import Console from rich.table import Table table = Table ( title = \"Star Wars Movies\" ) table . add_column ( \"Released\" , justify = \"right\" , style = \"cyan\" , no_wrap = True ) table . add_column ( \"Title\" , style = \"magenta\" ) table . add_column ( \"Box Office\" , justify = \"right\" , style = \"green\" ) table . add_row ( \"Dec 20, 2019\" , \"Star Wars: The Rise of Skywalker\" , \"$952,110,690\" ) table . add_row ( \"May 25, 2018\" , \"Solo: A Star Wars Story\" , \"$393,151,347\" ) table . add_row ( \"Dec 15, 2017\" , \"Star Wars Ep. V111: The Last Jedi\" , \"$1,332,539,889\" ) table . add_row ( \"Dec 16, 2016\" , \"Rogue One: A Star Wars Story\" , \"$1,332,439,889\" ) console = Console () console . print ( table )","title":"Tables"},{"location":"rich/#rich-text","text":"from rich.console import Console from rich.text import Text console = Console () text = Text . assemble (( \"Hello\" , \"bold magenta\" ), \" World!\" ) console . print ( text )","title":"Rich text"},{"location":"rich/#references","text":"Git Docs","title":"References"},{"location":"scrum/","text":"Scrum is an agile framework for developing, delivering, and sustaining complex products, with an initial emphasis on software development, although it has been used in other fields such as personal task management. It is designed for teams of ten or fewer members, who break their work into goals that can be completed within time-boxed iterations, called sprints, no longer than one month and most commonly two weeks. The Scrum Team track progress in 15-minute time-boxed daily meetings, called daily scrums. At the end of the sprint, the team holds sprint review, to demonstrate the work done, a sprint retrospective to improve continuously, and a sprint planning to prepare next sprint's tasks. For my personal scrum workflow and in the DevOps and DevSecOps teams I've found that Sprint goals are not operative, as multiple unrelated tasks need to be done, so it doesn't make sense to define just one goal. The meetings \u2691 Scrum tries to minimize the time spent in meetings while keeping a clearly defined direction and a healthy environment between all the people involved in the project. To achieve that is uses four types of meetings: Daily . Refinement . Retros . Reviews . Plannings . Daily meetings \u2691 Dailies or weeklies are the meetings where the development team exposes at high level of detail the current work. Similar to the dailies in the scrum terms, in the meeting each development team member exposes: The advances in the assigned tasks, with special interest in the encountered problems and deviations from the steps defined in the refinement. An estimation of the tasks that are going to be left unfinished by the end of the sprint. The goals of the meeting are: Get a general knowledge of what everyone else is doing. Learn from the experience gained by the others while doing their tasks. Get a clear idea of where we stand in terms of completing the sprint tasks. As opposed to what it may seem, this meeting is not meant to keep track of the productivity of each of us, we work based on trust, and know that each of us is working our best. Refinement meetings \u2691 Refinement are the meetings where the development team reviews the issues in the backlog and prepares the tasks that will probably be done in the following sprint. The goals of the meeting are: Next sprint tasks are ready to be worked upon in the next sprint. That means each task: Meets the Definition of Ready. All disambiguation in task description, validation criteria and steps is solved. Make the Planning meeting more dynamic. The meeting is composed of the following phases: Scrum master preparation. Development team refinement. Product owner refinement. Refinement preparation \u2691 To prepare the refinement, the scrum master has to: Make a copy of the Refinement document template . Open the OKRs document if you have one and for category in OKR categories: Select the category label in the issue tracker and select the milestone of the semester. Review which of those issues might enter the next sprint, and set the sprint project on them. Remove the milestone from the issue filter to see if there are interesting issues without the milestone set. Go to the next sprint Kanban board: Order the issues by priority. Make sure there are tasks with the Good first issue label. Make sure that there are more tasks than we can probably do so we can remove some instead of need to review the backlog and add more in the refinement. Fill up the sprint goals section of the refinement document. Create the Refinement developer team and product owner meeting calendar events. Development team refinement meeting \u2691 In this meeting the development team with the help of the scrum master, reviews the tasks to be added to the next sprint. The steps are defined in the refinement template. Product owner refinement meeting \u2691 In this meeting the product owner with the help of the scrum master reviews the tasks to be added to the next sprint. With the refinement document as reference: The expected current sprint undone tasks are reviewed. The sprint goals are discussed, modified and agreed. If there are many changes, we might think of setting the goals together in next sprints. The scrum master does a quick description of each issue. Each task priority is discussed and updated. Retro meetings \u2691 Retrospectives or Retros are the meetings where the scrum team plan ways to increase the quality and effectiveness of the team. The scrum master conducts different dynamics to help the rest of the scrum team inspect how the last Sprint went with regards to individuals, interactions, processes, tools, and their Definition of Done and Ready. Assumptions that led them astray are identified and their origins explored. The most impactful improvements are addressed as soon as possible. They may even be added to the backlog for the next sprint. Although improvements may be implemented at any time, the sprint retrospective provides a formal opportunity to focus on inspection and adaptation. The sprint retrospective concludes the sprint. The meeting consists of five phases, all of them conducted by the scrum master: Set the stage : There is an opening dynamic to give people time to \u201carrive\u201d and get into the right mood. Gather Data : Help everyone remember. Create a shared pool of information (everybody sees the world differently). There is an initial dynamic to measure the general feeling of the team and the issues to analyze further. Generate insights : Analyze why did things happen the way they did, identify patterns and see the big picture. Decide what to do : Pick a few issues to work on and create concrete action plans of how you\u2019ll address them. Adding the as issues in the scrum board. Close the retrospective : Clarify follow-ups, show appreciations, leave the meeting with a general good feeling, and analyze how could the retrospectives improve. If you have no idea how to conduct this meeting, you can take ideas from retromat . The goals of the meeting are: Analyze and draft a plan to iteratively improve the team's well-being, quality and efficiency. Review meetings \u2691 Reviews are the meetings where the product owner presents the sprint work to the rest of the team and the stakeholders. The idea of what is going to be done in the next sprint is also defined in this meeting. The meeting goes as follows: The product owner explains what items have been \u201cDone\u201d and what has not been \u201cDone\u201d. The product owner discuss what went well during the sprint, what problems they ran into, and how those problems were solved. The developers demonstrate the work that it has \u201cDone\u201d and answers questions. The product owner discusses the Product Backlog as it stands in terms of the semester OKRs. The entire group collaborates on what to do next, so that the Sprint Review provides valuable input to subsequent Sprint Planning. As the target audience are the stakeholders, the language must be changed accordingly, we should give overall ideas and not get caught in complicated high tech detailed explanations unless they ask them. The goals of the meeting are: Increase the transparency on what the team has done in the sprint. By explaining to the stake holders: What has been done. The reasons why we've implemented the specific outcomes for the tasks. The deviation from the expected plan of action. The status of the unfinished tasks with an explanation of why weren't they closed. The meaning of the work done in terms of the semester OKRs. Increase the transparency on what the team plans to do for the following sprint by explaining to the stakeholders: What do we plan to do in the next semester. How we plan to do it. The meaning of the plan in terms of the semester OKRs. Get the feedback from the stakeholders. We expect to gather and process their feedback by processing their opinions both of the work done of the past sprint and the work to be done in the next one. It will be gathered by the scrum master and persisted in the board on the planning meetings. Incorporate the stakeholders in the decision making process of the team. By inviting them to define with the rest of the scrum team the tasks for the next sprint. Planning meetings \u2691 Plannings are the meetings where the scrum team decides what it's going to do in the following sprint. The decision is made with the information gathered in the refinement, retro and review sessions. Conducted by the scrum master, usually only the members of the scrum team (developers, product owner and scrum master) are present, but stakeholders can also be invited. If the job has been done in the previous sessions, the backlog should be priorized and refined, so we should only add the newest issues gathered in the retro and review, refine them and decide what we want to do this sprint. The meeting goes as follows: We add the issues raised in the review to the backlog. We analyze the tasks on the top of the backlog, add them to the sprint board without assigning it to any developer. Once all tasks are added, we the stats of past sprints to see if the scope is realistic. The goals of the meeting are: Assert that the tasks added to the sprint follow the global path defined by the semester OKRs. All team has a clear view of what needs to be done. The team makes a realistic work commitment. The roles \u2691 There are three roles required in the scrum team: Product owner. Scrum master. Developer. Product owner \u2691 Scrum product owner is accountable for maximizing the value of the product resulting from the work of the scrum team. It's roles are: Assist the scrum master with: Priorization of the semester OKRs. Monitorization of the status of the semester OKRs on reviews and plannings. Priorization of the sprint tasks. Conduct the daily meetings: Show the Kanban board in the meeting Remind the number of weeks left until the review meeting. Make sure that the team is aware of what tasks are going to be left undone at the end of the sprint. Inform the affected stakeholders of the possible delay. Prepare and conduct the review meeting: With the help of the scrum master, prepare the reports: Create the report of the sprint, including: Make sure that the Definition of Done is met for the closed tasks. Explanation of the done tasks. Status of uncompleted tasks, and reason why they weren't complete. The meaning of the work done in terms of the semester OKRs. Create the report of the proposed next sprint's planning, with arguments behind why we do each task. Conduct the review meeting presenting the reports to the stakeholders. Attend the daily, review, retro and planning meetings. Scrum master \u2691 Scrum master is accountable for establishing Scrum as defined in this document. This position is going to be rotated between the members of the scrum team with a period of two sprints. It's roles are: Monitoring the status of the semester OKRs on reviews and plannings. Create new tasks required to meet the objectives. Refining the backlog: Adjust priority. Refine the tasks that are going to enter next sprint. Organize the required meetings to refine the backlog with the team members. Delete deprecated tasks. Assert that issues that are going to enter the new sprint meet the Definition of Ready . Arrange, prepare the daily meetings: Update the calendar events according to the week needs. Arrange, prepare and conduct the review meeting: Create the calendar event inviting the scrum team and the stakeholders. With the help of the product owner, prepare the reports: Create the report of the sprint, including: Make sure that the Definition of Done is met for the closed tasks. Explanation of the done tasks. Status of uncompleted tasks, and reason why they weren't complete. The meaning of the work done in terms of the semester OKRs. Create the report of the proposed next sprint's planning, with arguments behind why we do each task. Update the planning with the requirements of the stakeholders. Upload the review reports to the documentation repository. Arrange, prepare and conduct the refinement meetings: Prepare the tasks that need to be refined: Adjust the priority of the backlog tasks. Select the tasks that are most probably going to enter the next sprint. Expand the description of those tasks so it's understandable by any team member. If the task need some steps to be done before it can be worked upon, do them or create a task to do them before the original task. Create the required refinement calendar events inviting the members of the scrum team. Conduct the refinement meeting. Update the tasks with the outcome of the meeting. Prepare the next sprint's Kanban board. Arrange, prepare and conduct the retro meeting: Prepare the dynamics of the meeting. Create the retro calendar event inviting the members of the scrum team. Conduct the retro meeting. Update the tasks with the outcome of the meeting. Upload the retro reports to the documentation repository. Arrange, prepare and conduct the planning meeting: Make sure that you've done the required refinement sessions to have the tasks and Kanban board ready for the next sprint. Create the planning calendar event inviting the members of the scrum team. Conduct the planning meeting. Update the tasks with the outcome of the meeting and start the sprint. Developer \u2691 Developers are the people in the scrum team that are committed to creating any aspect of a usable increment each sprint. It's roles are: Attend the daily, refinement, review, retro and planning meetings. Focus on completing the assigned sprint tasks. Do the required work or be responsible to coordinate the work that others do for the task to be complete. Make sure that the Definition of Done is met before closing the task. Inter team workflow \u2691 To improve the communication between the teams, you can: Present more clearly the team objectives and reasons behind our tasks, and make the rest of the teams part of the decision making. Be aware of the other team's needs and tasks. To solve the first point, you can offer the rest of the teams different solutions depending the time they want to invest in staying informed: You can invite the other team members to the sprint reviews, where you show the sprint's work and present what you plan to do in the next sprint. This could be the best way to stay informed, as you'll try to sum up everything they need to know in the shortest time. For those that want to be more involved with the decision making inside the team, they could be invited to the planning sessions and even the refinement ones where they are involved. For those that don't want to attend the review, they can either get a summary from other members of their team that did attend, or they can read the meeting notes that you publish after each one. The second point means that your team members become more involved in the other team's work. The different levels of involvement are linked to the amount of time invested and the quality of the interaction. The highest level of involvement would be that a member of your team is also part of the other team. This is easier for those teams that already use Scrum as their agile framework, that means: Attending the team's meetings (retro, review, planning and refinement). Inform the rest of your team of the outcomes of those meetings in the daily meeting. Focus on doing that team's sprint tasks. Populate and refine the tasks related to your team in the other team issue tracker. For those teams that are smaller or don't use Scrum as their agile framework, a your team members could accompany them by: Setting periodic meetings (weekly/biweekly/monthly) to discuss what are they doing, what do they plan to do and how. Create the team related tasks in your backlog, coordinating with the scrum master to refine and prioritize them. Definitions \u2691 Definition of Ready \u2691 The Definition of Ready (DoR) is a list of criteria which must be met before any task can be added to a sprint. It is agreed by the whole scrum team and reviewed in the planning sessions. Expected Benefits \u2691 Avoids beginning work on features that do not have clearly defined completion criteria, which usually translates into costly back-and-forth discussion or rework. Provides the team with an explicit agreement allowing it to \u201cpush back\u201d on accepting ill-defined features to work on. The Definition of Ready provides a checklist which usefully guides pre-implementation activities: discussion, estimation, design. Example of a Definition of Ready \u2691 A task needs to meet the following criteria before being added to a sprint. Have a short title that summarizes the goal of the task. Have a description clear enough so any team member can understand why we need to do the task Have a validation criteria for the task to be done Have a checklist of steps required to meet the validation criteria, clear enough so that any team member can understand them. Have a scope that can be met in one sprint. Have the Priority: label set. If other teams are involved in the task, add the Team: labels. If it's associated to an OKR set the OKR: label. Definition of Done \u2691 The Definition of Done (DoD) is a list of criteria which must be met before any task can be closed. It is agreed by the whole scrum team and reviewed in the planning sessions. Expected Benefits \u2691 The Definition of Done limits the cost of rework once a feature has been accepted as \u201cdone\u201d. Having an explicit contract limits the risk of misunderstanding and conflict between the development team and the customer or product owner. Common Pitfalls \u2691 Obsessing over the list of criteria can be counter-productive; the list needs to define the minimum work generally required to get a product increment to the \u201cdone\u201d state. Individual features or user stories may have specific \u201cdone\u201d criteria in addition to the ones that apply to work in general. If the definition of done is merely a shared understanding, rather than spelled out and displayed on a wall, it may lose much of its effectiveness; a good part of its value lies in being an explicit contract known to all members of the team. Example of a Definition of Done \u2691 A task needs to meet the following criteria before being closed. All changes must be documented. All related pull requests must be merged.","title":"Scrum"},{"location":"scrum/#the-meetings","text":"Scrum tries to minimize the time spent in meetings while keeping a clearly defined direction and a healthy environment between all the people involved in the project. To achieve that is uses four types of meetings: Daily . Refinement . Retros . Reviews . Plannings .","title":"The meetings"},{"location":"scrum/#daily-meetings","text":"Dailies or weeklies are the meetings where the development team exposes at high level of detail the current work. Similar to the dailies in the scrum terms, in the meeting each development team member exposes: The advances in the assigned tasks, with special interest in the encountered problems and deviations from the steps defined in the refinement. An estimation of the tasks that are going to be left unfinished by the end of the sprint. The goals of the meeting are: Get a general knowledge of what everyone else is doing. Learn from the experience gained by the others while doing their tasks. Get a clear idea of where we stand in terms of completing the sprint tasks. As opposed to what it may seem, this meeting is not meant to keep track of the productivity of each of us, we work based on trust, and know that each of us is working our best.","title":"Daily meetings"},{"location":"scrum/#refinement-meetings","text":"Refinement are the meetings where the development team reviews the issues in the backlog and prepares the tasks that will probably be done in the following sprint. The goals of the meeting are: Next sprint tasks are ready to be worked upon in the next sprint. That means each task: Meets the Definition of Ready. All disambiguation in task description, validation criteria and steps is solved. Make the Planning meeting more dynamic. The meeting is composed of the following phases: Scrum master preparation. Development team refinement. Product owner refinement.","title":"Refinement meetings"},{"location":"scrum/#refinement-preparation","text":"To prepare the refinement, the scrum master has to: Make a copy of the Refinement document template . Open the OKRs document if you have one and for category in OKR categories: Select the category label in the issue tracker and select the milestone of the semester. Review which of those issues might enter the next sprint, and set the sprint project on them. Remove the milestone from the issue filter to see if there are interesting issues without the milestone set. Go to the next sprint Kanban board: Order the issues by priority. Make sure there are tasks with the Good first issue label. Make sure that there are more tasks than we can probably do so we can remove some instead of need to review the backlog and add more in the refinement. Fill up the sprint goals section of the refinement document. Create the Refinement developer team and product owner meeting calendar events.","title":"Refinement preparation"},{"location":"scrum/#development-team-refinement-meeting","text":"In this meeting the development team with the help of the scrum master, reviews the tasks to be added to the next sprint. The steps are defined in the refinement template.","title":"Development team refinement meeting"},{"location":"scrum/#product-owner-refinement-meeting","text":"In this meeting the product owner with the help of the scrum master reviews the tasks to be added to the next sprint. With the refinement document as reference: The expected current sprint undone tasks are reviewed. The sprint goals are discussed, modified and agreed. If there are many changes, we might think of setting the goals together in next sprints. The scrum master does a quick description of each issue. Each task priority is discussed and updated.","title":"Product owner refinement meeting"},{"location":"scrum/#retro-meetings","text":"Retrospectives or Retros are the meetings where the scrum team plan ways to increase the quality and effectiveness of the team. The scrum master conducts different dynamics to help the rest of the scrum team inspect how the last Sprint went with regards to individuals, interactions, processes, tools, and their Definition of Done and Ready. Assumptions that led them astray are identified and their origins explored. The most impactful improvements are addressed as soon as possible. They may even be added to the backlog for the next sprint. Although improvements may be implemented at any time, the sprint retrospective provides a formal opportunity to focus on inspection and adaptation. The sprint retrospective concludes the sprint. The meeting consists of five phases, all of them conducted by the scrum master: Set the stage : There is an opening dynamic to give people time to \u201carrive\u201d and get into the right mood. Gather Data : Help everyone remember. Create a shared pool of information (everybody sees the world differently). There is an initial dynamic to measure the general feeling of the team and the issues to analyze further. Generate insights : Analyze why did things happen the way they did, identify patterns and see the big picture. Decide what to do : Pick a few issues to work on and create concrete action plans of how you\u2019ll address them. Adding the as issues in the scrum board. Close the retrospective : Clarify follow-ups, show appreciations, leave the meeting with a general good feeling, and analyze how could the retrospectives improve. If you have no idea how to conduct this meeting, you can take ideas from retromat . The goals of the meeting are: Analyze and draft a plan to iteratively improve the team's well-being, quality and efficiency.","title":"Retro meetings"},{"location":"scrum/#review-meetings","text":"Reviews are the meetings where the product owner presents the sprint work to the rest of the team and the stakeholders. The idea of what is going to be done in the next sprint is also defined in this meeting. The meeting goes as follows: The product owner explains what items have been \u201cDone\u201d and what has not been \u201cDone\u201d. The product owner discuss what went well during the sprint, what problems they ran into, and how those problems were solved. The developers demonstrate the work that it has \u201cDone\u201d and answers questions. The product owner discusses the Product Backlog as it stands in terms of the semester OKRs. The entire group collaborates on what to do next, so that the Sprint Review provides valuable input to subsequent Sprint Planning. As the target audience are the stakeholders, the language must be changed accordingly, we should give overall ideas and not get caught in complicated high tech detailed explanations unless they ask them. The goals of the meeting are: Increase the transparency on what the team has done in the sprint. By explaining to the stake holders: What has been done. The reasons why we've implemented the specific outcomes for the tasks. The deviation from the expected plan of action. The status of the unfinished tasks with an explanation of why weren't they closed. The meaning of the work done in terms of the semester OKRs. Increase the transparency on what the team plans to do for the following sprint by explaining to the stakeholders: What do we plan to do in the next semester. How we plan to do it. The meaning of the plan in terms of the semester OKRs. Get the feedback from the stakeholders. We expect to gather and process their feedback by processing their opinions both of the work done of the past sprint and the work to be done in the next one. It will be gathered by the scrum master and persisted in the board on the planning meetings. Incorporate the stakeholders in the decision making process of the team. By inviting them to define with the rest of the scrum team the tasks for the next sprint.","title":"Review meetings"},{"location":"scrum/#planning-meetings","text":"Plannings are the meetings where the scrum team decides what it's going to do in the following sprint. The decision is made with the information gathered in the refinement, retro and review sessions. Conducted by the scrum master, usually only the members of the scrum team (developers, product owner and scrum master) are present, but stakeholders can also be invited. If the job has been done in the previous sessions, the backlog should be priorized and refined, so we should only add the newest issues gathered in the retro and review, refine them and decide what we want to do this sprint. The meeting goes as follows: We add the issues raised in the review to the backlog. We analyze the tasks on the top of the backlog, add them to the sprint board without assigning it to any developer. Once all tasks are added, we the stats of past sprints to see if the scope is realistic. The goals of the meeting are: Assert that the tasks added to the sprint follow the global path defined by the semester OKRs. All team has a clear view of what needs to be done. The team makes a realistic work commitment.","title":"Planning meetings"},{"location":"scrum/#the-roles","text":"There are three roles required in the scrum team: Product owner. Scrum master. Developer.","title":"The roles"},{"location":"scrum/#product-owner","text":"Scrum product owner is accountable for maximizing the value of the product resulting from the work of the scrum team. It's roles are: Assist the scrum master with: Priorization of the semester OKRs. Monitorization of the status of the semester OKRs on reviews and plannings. Priorization of the sprint tasks. Conduct the daily meetings: Show the Kanban board in the meeting Remind the number of weeks left until the review meeting. Make sure that the team is aware of what tasks are going to be left undone at the end of the sprint. Inform the affected stakeholders of the possible delay. Prepare and conduct the review meeting: With the help of the scrum master, prepare the reports: Create the report of the sprint, including: Make sure that the Definition of Done is met for the closed tasks. Explanation of the done tasks. Status of uncompleted tasks, and reason why they weren't complete. The meaning of the work done in terms of the semester OKRs. Create the report of the proposed next sprint's planning, with arguments behind why we do each task. Conduct the review meeting presenting the reports to the stakeholders. Attend the daily, review, retro and planning meetings.","title":"Product owner"},{"location":"scrum/#scrum-master","text":"Scrum master is accountable for establishing Scrum as defined in this document. This position is going to be rotated between the members of the scrum team with a period of two sprints. It's roles are: Monitoring the status of the semester OKRs on reviews and plannings. Create new tasks required to meet the objectives. Refining the backlog: Adjust priority. Refine the tasks that are going to enter next sprint. Organize the required meetings to refine the backlog with the team members. Delete deprecated tasks. Assert that issues that are going to enter the new sprint meet the Definition of Ready . Arrange, prepare the daily meetings: Update the calendar events according to the week needs. Arrange, prepare and conduct the review meeting: Create the calendar event inviting the scrum team and the stakeholders. With the help of the product owner, prepare the reports: Create the report of the sprint, including: Make sure that the Definition of Done is met for the closed tasks. Explanation of the done tasks. Status of uncompleted tasks, and reason why they weren't complete. The meaning of the work done in terms of the semester OKRs. Create the report of the proposed next sprint's planning, with arguments behind why we do each task. Update the planning with the requirements of the stakeholders. Upload the review reports to the documentation repository. Arrange, prepare and conduct the refinement meetings: Prepare the tasks that need to be refined: Adjust the priority of the backlog tasks. Select the tasks that are most probably going to enter the next sprint. Expand the description of those tasks so it's understandable by any team member. If the task need some steps to be done before it can be worked upon, do them or create a task to do them before the original task. Create the required refinement calendar events inviting the members of the scrum team. Conduct the refinement meeting. Update the tasks with the outcome of the meeting. Prepare the next sprint's Kanban board. Arrange, prepare and conduct the retro meeting: Prepare the dynamics of the meeting. Create the retro calendar event inviting the members of the scrum team. Conduct the retro meeting. Update the tasks with the outcome of the meeting. Upload the retro reports to the documentation repository. Arrange, prepare and conduct the planning meeting: Make sure that you've done the required refinement sessions to have the tasks and Kanban board ready for the next sprint. Create the planning calendar event inviting the members of the scrum team. Conduct the planning meeting. Update the tasks with the outcome of the meeting and start the sprint.","title":"Scrum master"},{"location":"scrum/#developer","text":"Developers are the people in the scrum team that are committed to creating any aspect of a usable increment each sprint. It's roles are: Attend the daily, refinement, review, retro and planning meetings. Focus on completing the assigned sprint tasks. Do the required work or be responsible to coordinate the work that others do for the task to be complete. Make sure that the Definition of Done is met before closing the task.","title":"Developer"},{"location":"scrum/#inter-team-workflow","text":"To improve the communication between the teams, you can: Present more clearly the team objectives and reasons behind our tasks, and make the rest of the teams part of the decision making. Be aware of the other team's needs and tasks. To solve the first point, you can offer the rest of the teams different solutions depending the time they want to invest in staying informed: You can invite the other team members to the sprint reviews, where you show the sprint's work and present what you plan to do in the next sprint. This could be the best way to stay informed, as you'll try to sum up everything they need to know in the shortest time. For those that want to be more involved with the decision making inside the team, they could be invited to the planning sessions and even the refinement ones where they are involved. For those that don't want to attend the review, they can either get a summary from other members of their team that did attend, or they can read the meeting notes that you publish after each one. The second point means that your team members become more involved in the other team's work. The different levels of involvement are linked to the amount of time invested and the quality of the interaction. The highest level of involvement would be that a member of your team is also part of the other team. This is easier for those teams that already use Scrum as their agile framework, that means: Attending the team's meetings (retro, review, planning and refinement). Inform the rest of your team of the outcomes of those meetings in the daily meeting. Focus on doing that team's sprint tasks. Populate and refine the tasks related to your team in the other team issue tracker. For those teams that are smaller or don't use Scrum as their agile framework, a your team members could accompany them by: Setting periodic meetings (weekly/biweekly/monthly) to discuss what are they doing, what do they plan to do and how. Create the team related tasks in your backlog, coordinating with the scrum master to refine and prioritize them.","title":"Inter team workflow"},{"location":"scrum/#definitions","text":"","title":"Definitions"},{"location":"scrum/#definition-of-ready","text":"The Definition of Ready (DoR) is a list of criteria which must be met before any task can be added to a sprint. It is agreed by the whole scrum team and reviewed in the planning sessions.","title":"Definition of Ready"},{"location":"scrum/#expected-benefits","text":"Avoids beginning work on features that do not have clearly defined completion criteria, which usually translates into costly back-and-forth discussion or rework. Provides the team with an explicit agreement allowing it to \u201cpush back\u201d on accepting ill-defined features to work on. The Definition of Ready provides a checklist which usefully guides pre-implementation activities: discussion, estimation, design.","title":"Expected Benefits"},{"location":"scrum/#example-of-a-definition-of-ready","text":"A task needs to meet the following criteria before being added to a sprint. Have a short title that summarizes the goal of the task. Have a description clear enough so any team member can understand why we need to do the task Have a validation criteria for the task to be done Have a checklist of steps required to meet the validation criteria, clear enough so that any team member can understand them. Have a scope that can be met in one sprint. Have the Priority: label set. If other teams are involved in the task, add the Team: labels. If it's associated to an OKR set the OKR: label.","title":"Example of a Definition of Ready"},{"location":"scrum/#definition-of-done","text":"The Definition of Done (DoD) is a list of criteria which must be met before any task can be closed. It is agreed by the whole scrum team and reviewed in the planning sessions.","title":"Definition of Done"},{"location":"scrum/#expected-benefits_1","text":"The Definition of Done limits the cost of rework once a feature has been accepted as \u201cdone\u201d. Having an explicit contract limits the risk of misunderstanding and conflict between the development team and the customer or product owner.","title":"Expected Benefits"},{"location":"scrum/#common-pitfalls","text":"Obsessing over the list of criteria can be counter-productive; the list needs to define the minimum work generally required to get a product increment to the \u201cdone\u201d state. Individual features or user stories may have specific \u201cdone\u201d criteria in addition to the ones that apply to work in general. If the definition of done is merely a shared understanding, rather than spelled out and displayed on a wall, it may lose much of its effectiveness; a good part of its value lies in being an explicit contract known to all members of the team.","title":"Common Pitfalls"},{"location":"scrum/#example-of-a-definition-of-done","text":"A task needs to meet the following criteria before being closed. All changes must be documented. All related pull requests must be merged.","title":"Example of a Definition of Done"},{"location":"selenium/","text":"Selenium is a portable framework for testing web applications. It also provides a test domain-specific language (Selenese) to write tests in a number of popular programming languages. Web driver backends \u2691 Selenium can be used with many browsers, such as Firefox , Chrome or PhantomJS . But first, install selenium : pip install selenium Firefox \u2691 Assuming you've got firefox already installed, you need to download the geckodriver , unpack the tar and add the geckodriver binary somewhere in your PATH . from selenium import webdriver driver = webdriver . Firefox () driver . get ( \"https://duckduckgo.com/\" ) If you need to get the status code of the requests use Chrome instead There is an issue with Firefox that doesn't support this feature. Chrome \u2691 We're going to use Chromium instead of Chrome. Download the chromedriver of the same version as your Chromium, unpack the tar and add the chromedriver binary somewhere in your PATH . from selenium import webdriver from selenium.webdriver.chrome.options import Options opts = Options () opts . binary_location = '/usr/bin/chromium' driver = webdriver . Chrome ( options = opts ) driver . get ( \"https://duckduckgo.com/\" ) If you don't want to see the browser, you can run it in headless mode adding the next line when defining the options : opts . add_argument ( \"--headless\" ) PhantomJS \u2691 PhantomJS is abandoned -> Don't use it The development stopped in 2018 PhantomJS is a headless Webkit, in conjunction with Selenium WebDriver, it can be used to run tests directly from the command line. Since PhantomJS eliminates the need for a graphical browser, tests run much faster. Don't install phantomjs from the official repos as it's not a working release -.-. npm install -g phantomjs didn't work either. I had to download the tar from the downloads page , which didn't work either. The project is abandoned , so don't use this. Usage \u2691 Assuming that you've got a configured driver , to get the url you're in after javascript has done it's magic use the driver.current_url method. To return the HTML of the page use driver.page_source . Close the browser \u2691 driver . close () Set timeout of a response \u2691 For Firefox and Chromedriver: driver . set_page_load_timeout ( 30 ) The rest: driver . implicitly_wait ( 30 ) This will throw a TimeoutException whenever the page load takes more than 30 seconds. Get the status code of a response \u2691 Surprisingly this is not as easy as with requests, there is no status_code method on the driver, you need to dive into the browser log to get it. Firefox has an open issue since 2016 that prevents you from getting this information . Use Chromium if you need this functionality. from selenium.webdriver.common.desired_capabilities import DesiredCapabilities capabilities = DesiredCapabilities . CHROME . copy () capabilities [ 'goog:loggingPrefs' ] = { 'performance' : 'ALL' } driver = webdriver . Chrome ( desired_capabilities = capabilities ) driver . get ( \"https://duckduckgo.com/\" ) logs = driver . get_log ( \"performance\" ) status_code = get_status ( driver . current_url , logs ) Where get_status is: def get_status ( url : str , logs : List [ Dict [ str , Any ]]) -> int : \"\"\"Get the url response status code. Args: url: url to search logs: Browser driver logs Returns: The status code. \"\"\" for log in logs : if log [ \"message\" ]: data = json . loads ( log [ \"message\" ]) with suppress ( KeyError ): if data [ \"message\" ][ \"params\" ][ \"response\" ][ \"url\" ] == url : return data [ \"message\" ][ \"params\" ][ \"response\" ][ \"status\" ] raise ValueError ( f \"Error retrieving the status code for url { url } \" ) You have to use driver.current_url to handle well urls that redirect to other urls. If your url is not catched and you get a ValueError , use the next snippet inside the with suppress(KeyError) statement. content_type = ( \"text/html\" in data [ \"message\" ][ \"params\" ][ \"response\" ][ \"headers\" ][ \"content-type\" ] ) response_received = ( data [ \"message\" ][ \"method\" ] == \"Network.responseReceived\" ) if content_type and response_received : __import__ ( \"pdb\" ) . set_trace () # XXX BREAKPOINT pass And try to see why url != data[\"message\"][\"params\"][\"response\"][\"url\"] . Sometimes servers redirect the user to a url without the www. . Troubleshooting \u2691 Chromedriver hangs up unexpectedly \u2691 Some say that adding the DBUS_SESSION_BUS_ADDRESS environmental variable fixes it: os . environ [ \"DBUS_SESSION_BUS_ADDRESS\" ] = \"/dev/null\" But it still hangs for me. Right now the only solution I see is to assume it's going to hang and add functionality in your program to resume the work instead of starting from scratch. Ugly I know... Issues \u2691 Firefox driver doesn't have access to the log : Update the section above and start using Firefox instead of Chrome when you need to get the status code of the responses.","title":"Selenium"},{"location":"selenium/#web-driver-backends","text":"Selenium can be used with many browsers, such as Firefox , Chrome or PhantomJS . But first, install selenium : pip install selenium","title":"Web driver backends"},{"location":"selenium/#firefox","text":"Assuming you've got firefox already installed, you need to download the geckodriver , unpack the tar and add the geckodriver binary somewhere in your PATH . from selenium import webdriver driver = webdriver . Firefox () driver . get ( \"https://duckduckgo.com/\" ) If you need to get the status code of the requests use Chrome instead There is an issue with Firefox that doesn't support this feature.","title":"Firefox"},{"location":"selenium/#chrome","text":"We're going to use Chromium instead of Chrome. Download the chromedriver of the same version as your Chromium, unpack the tar and add the chromedriver binary somewhere in your PATH . from selenium import webdriver from selenium.webdriver.chrome.options import Options opts = Options () opts . binary_location = '/usr/bin/chromium' driver = webdriver . Chrome ( options = opts ) driver . get ( \"https://duckduckgo.com/\" ) If you don't want to see the browser, you can run it in headless mode adding the next line when defining the options : opts . add_argument ( \"--headless\" )","title":"Chrome"},{"location":"selenium/#phantomjs","text":"PhantomJS is abandoned -> Don't use it The development stopped in 2018 PhantomJS is a headless Webkit, in conjunction with Selenium WebDriver, it can be used to run tests directly from the command line. Since PhantomJS eliminates the need for a graphical browser, tests run much faster. Don't install phantomjs from the official repos as it's not a working release -.-. npm install -g phantomjs didn't work either. I had to download the tar from the downloads page , which didn't work either. The project is abandoned , so don't use this.","title":"PhantomJS"},{"location":"selenium/#usage","text":"Assuming that you've got a configured driver , to get the url you're in after javascript has done it's magic use the driver.current_url method. To return the HTML of the page use driver.page_source .","title":"Usage"},{"location":"selenium/#close-the-browser","text":"driver . close ()","title":"Close the browser"},{"location":"selenium/#set-timeout-of-a-response","text":"For Firefox and Chromedriver: driver . set_page_load_timeout ( 30 ) The rest: driver . implicitly_wait ( 30 ) This will throw a TimeoutException whenever the page load takes more than 30 seconds.","title":"Set timeout of a response"},{"location":"selenium/#get-the-status-code-of-a-response","text":"Surprisingly this is not as easy as with requests, there is no status_code method on the driver, you need to dive into the browser log to get it. Firefox has an open issue since 2016 that prevents you from getting this information . Use Chromium if you need this functionality. from selenium.webdriver.common.desired_capabilities import DesiredCapabilities capabilities = DesiredCapabilities . CHROME . copy () capabilities [ 'goog:loggingPrefs' ] = { 'performance' : 'ALL' } driver = webdriver . Chrome ( desired_capabilities = capabilities ) driver . get ( \"https://duckduckgo.com/\" ) logs = driver . get_log ( \"performance\" ) status_code = get_status ( driver . current_url , logs ) Where get_status is: def get_status ( url : str , logs : List [ Dict [ str , Any ]]) -> int : \"\"\"Get the url response status code. Args: url: url to search logs: Browser driver logs Returns: The status code. \"\"\" for log in logs : if log [ \"message\" ]: data = json . loads ( log [ \"message\" ]) with suppress ( KeyError ): if data [ \"message\" ][ \"params\" ][ \"response\" ][ \"url\" ] == url : return data [ \"message\" ][ \"params\" ][ \"response\" ][ \"status\" ] raise ValueError ( f \"Error retrieving the status code for url { url } \" ) You have to use driver.current_url to handle well urls that redirect to other urls. If your url is not catched and you get a ValueError , use the next snippet inside the with suppress(KeyError) statement. content_type = ( \"text/html\" in data [ \"message\" ][ \"params\" ][ \"response\" ][ \"headers\" ][ \"content-type\" ] ) response_received = ( data [ \"message\" ][ \"method\" ] == \"Network.responseReceived\" ) if content_type and response_received : __import__ ( \"pdb\" ) . set_trace () # XXX BREAKPOINT pass And try to see why url != data[\"message\"][\"params\"][\"response\"][\"url\"] . Sometimes servers redirect the user to a url without the www. .","title":"Get the status code of a response"},{"location":"selenium/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"selenium/#chromedriver-hangs-up-unexpectedly","text":"Some say that adding the DBUS_SESSION_BUS_ADDRESS environmental variable fixes it: os . environ [ \"DBUS_SESSION_BUS_ADDRESS\" ] = \"/dev/null\" But it still hangs for me. Right now the only solution I see is to assume it's going to hang and add functionality in your program to resume the work instead of starting from scratch. Ugly I know...","title":"Chromedriver hangs up unexpectedly"},{"location":"selenium/#issues","text":"Firefox driver doesn't have access to the log : Update the section above and start using Firefox instead of Chrome when you need to get the status code of the responses.","title":"Issues"},{"location":"signal/","text":"Signal is a cross-platform centralized encrypted messaging service developed by the Signal Technology Foundation and Signal Messenger LLC. It uses the Internet to send one-to-one and group messages, which can include files, voice notes, images and videos. It can also be used to make one-to-one and group voice and video calls. Signal uses standard cellular telephone numbers as identifiers and secures all communications to other Signal users with end-to-end encryption. The apps include mechanisms by which users can independently verify the identity of their contacts and the integrity of the data channel. Signal's software is free and open-source. Its clients are published under the GPLv3 license, while the server code is published under the AGPLv3 license. The official Android app generally uses the proprietary Google Play Services (installed on most Android devices), though it is designed to still work without them installed. Signal also has an official client app for iOS and desktop apps for Windows, MacOS and Linux. Pros and cons \u2691 Pros: Good security by default. Easy to use for non technical users. Good multi-device support. Cons: Uses phones to identify users. Centralized. Not available in F-droid . Backup extraction \u2691 I'd first try to use signal-black . References \u2691 Home","title":"Signal"},{"location":"signal/#pros-and-cons","text":"Pros: Good security by default. Easy to use for non technical users. Good multi-device support. Cons: Uses phones to identify users. Centralized. Not available in F-droid .","title":"Pros and cons"},{"location":"signal/#backup-extraction","text":"I'd first try to use signal-black .","title":"Backup extraction"},{"location":"signal/#references","text":"Home","title":"References"},{"location":"sleep/","text":"Sleep is a naturally recurring state of mind and body, characterized by altered consciousness, relatively inhibited sensory activity, reduced muscle activity and inhibition of nearly all voluntary muscles during rapid eye movement (REM) sleep,and reduced interactions with surroundings. Distinguished from wakefulness by a decreased ability to react to stimuli. Most of the content of this article is extracted from the Why we sleep book by Matthew Walker Consequences of lack of sleep \u2691 Sleeping less than six or seven hours a night can produce these consequences: Demolishing of the immune system. Doubling your risk of cancer. Is a key lifestyle factor determining and worsening the development of the Alzheimer's disease. Disruption of blood sugar levels so profoundly that you would be classified as pre-diabetic. Increase the likelihood of block and brittle of your coronary arteries. Setting you on a path toward cardiovascular disease, stroke, and congestive heart failure. Contributes to all major psychiatric conditions, including depression, anxiety, and suicidality. Swelling concentrations of a hormone that makes you feel hungry while suppressing a companion hormone that otherwise signals food satisfaction. Thwart the ability to learn and memorize. A balanced diet and exercise are of vital importance, but we now see sleep as the key factor in health. The physical and mental impairments caused by one night of bad sleep dwarf those caused by an equivalent absence of food or exercise. Therefore, the shorter you sleep, the shorter your life span . Sleep benefits \u2691 We sleep for a lot of nighttime benefits that service both our brains and our body. There does not seem to be one major organ within the body, or process within the brain, that isn't optimally enhanced by sleep. Within the brain, sleep enriches our ability to learn, memorize and make logical decisions and choices. It recalibrates our emotional brain circuits, allowing us to navigate next day social and psychological challenges with cool-headed composture. Downstairs in the body, sleep: Restocks the armory of our immune system: helping fight malignancy, preventing infection, and warding off sickness. Reforms the body's metabolic state by fine-tuning the balance of insulin and circulating glucose. Regulates our appetite, helping control body weight through healthy food selection rather than rash impulsivity. Maintains a flourishing microbiome within your gut essential to our nutritional health being. Is tied to the fitness of our cardiovascular system, lowering blood pressure while keeping our hearts in fine condition. Dreaming produces a neurochemical bath that mollifies painful memories and a virtual reality space in which the brain melds past and present knowledge, inspiring creativity. Therefore, Sleep is the single most effective thing we can do to reset our brain and body health each day . Sleep physiological effects \u2691 There are two main factors that determine when you want to sleep or stay awake: The signal sent by the suprachiasmatic nucleus following the circadian rhythm. Sleep pressure: The brain builds up a chemical substance that creates the \"sleep pressure\". The longer you've been awake, the more that chemical sleep pressure accumulates, and consequentially, the sleepier you feel. The circadian rhythm \u2691 We have an internal clock deep within the brain, called the suprachiasmatic nucleus, that creates a cycling, day-night rhythm, known as circadian rhythm, that makes you feel tired or alert at regular times of night and day, respectively. The circadian rhythm determines: When you want to be awake or asleep. Your timed preferences for eating and drinking. Your moods and emotions The amount of urine you produce. Your core body temperature. Your metabolic rate. The release of numerous hormones. Contrary to common belief, circadian rhythm is not defined by the daylight sun cycle. As Kleitman and Richardson demonstrated in 1938: When cut off from the daily cycle of light and dark, the body keeps on maintaining the rhythm. The period of the circadian rhythm is different for each person, but has an average of 24 hours and 15 minutes. Even if it's not defined by the sun light, it corrects those 15 minutes of delay to stay in sync with it. The suprachiasmatic nucleus can readjust by about one hour each day, that is why jet lag can be spawn through multiple days. That reset does not come free. Studies in airplane cabin crews who frequently fly on long haul routes and have little chance to recover have registered: The part of the brains related to learning and memory had physically shrunk, suggesting the destruction of brain cells caused by the biological stress of timezone travel. Their short term memory was significantly impaired. They had far higher rates of cancer and type 2 diabetes than the general population. The peak and valley points of wakefulness or sleepiness vary too between people, it's known as their chronotype and it's strongly determined by genetics. The chronotype defines three types of people: Morning types : They have their peak of wakefulness early in the day and the sleepiness early at night. They prefer to wake at or around dawn, and function optimally at this time of day. Evening types : They prefer going to bed late and subsequently wake up late the following morning, or even in the afternoon. In between : The remaining people fall somewhere in between, with a slight leaning towards eveningness. Melatonin \u2691 The suprachiasmatic nucleus communicates its repeating signal of day and night to your brain and body by releasing melatonin into the bloodstream from the pineal gland. Soon after dusk, the suprachiasmatic nucleus starts increasing the levels of this hormone, telling the rest of the body that it's time to sleep. But melatonin has little influence on the generation of sleep itself. Once sleep is under way, melatonin decreases in concentration across the night and into the morning hours. With dawn, as sunlight enters the brain through the eyes (even through the closed lids), the pineal gland is instructed to stop releasing melatonin. The absence of circulating melatonin now informs the brain and body that it's time to return to a wakefulness active state for the rest of the day Sleep pressure \u2691 While you are awake, the brain is releasing a chemical called adenosine. One consequence of the increasing accumulation of adenosine is the increase of the desire to sleep by turning down the \"volume\" of wake promoting regions in the brain and turning up the sleep inducing ones. Most people fall to the pressure after twelve to sixteen hours of being awake. Caffeine \u2691 You can artificially mute the sleep signal of adenosine by using a chemical that makes you feel more alert and awake, such as caffeine. Caffeine works by battling with adenosine for the privilege of latching on to adenosine receptors in the brain. Once caffeine occupies these receptors, it does not stimulate them like adenosine, making you sleepy. Rather, caffeine blocks and effectively inactivates the receptors acting as a masking agent. Levels of caffeine peak around thirty minutes after ingestion. What is problematic, though, is the persistence of caffeine in your system. It takes between five to seven hours to remove 50 percent of the caffeine concentration from your body. An enzyme within your liver removes caffeine from your system. Based in large part on genetics, some people have a more efficient version of the enzyme that degrades caffeine, allowing the liver to clear it from the bloodstream. Age is also a variable to take into account, the older we are the longer it takes our brain and body to remove it. When your liver evicts the caffeine from your system, you encounter the caffeine crash . Your energy levels plummet rapidly, finding difficult to function and concentrate, with a strong sense of sleepiness once again. For the entire time that caffeine is in your system, the adenosine keeps on building up. Your brain is not aware of this rising tide of sleep encouraging chemical because the wall of caffeine is holding it back from your perception. Once your liver dismantles the barricade, you feel a vicious backlash: you are hit with the sleepiness you had experienced two or three hours ago before you drank that cup of coffee plus all the extra adenosine that has accumulated in the hours in between. Relationship between the circadian rhythm and the sleep pressure \u2691 The two governing forces that regulate your sleep are ignorant of each other. Although they are not coupled, they are usually aligned. Starting on the far left of the figure, the circadian rhythm begins to increase its activity a few hours before you wake up. It infuses the brain and body with an alerting energy signal. At first, the signal is faint, but gradually it builds with time. By early afternoon, the activating signal from the circadian rhythm peaks. Now let's look at the sleep pressure pattern. By mid to late morning, you have only been awake for a half of hours. As a result, adenosine concentrations have increased a little. Furthermore, the circadian rhythm is on its powerful upswing of alertness. This combination of strong activating output from the circadian rhythm together with low levels of adenosine result in a delightful sensation of being wide awake. The distance between the curved lines above will be a direct reflection of your desire to sleep. By eleven pm, you've been awake for fifteen hours, and your brain is drenched in high concentrations of adenosine. Additionally, the circadian rhythm line is descending, powering down your activity and alertness levels. This powerful combination triggers a strong desire for sleep. During sleep, a mass evacuation of adenosine gets under way as the brain has the chance to degrade and remove it. After eight hours of healthy sleep, the adenosine purge is complete. As this process is ending, the circadian activity rhythm has returned, and its energizing influence starts to approach, therefore naturally waking us up. All-nighters \u2691 Scientists can demonstrate that the two forces determining when you want to be awake and sleep are independent and can be decoupled from their normal lockstep. When you skip one night's sleep and remain awake throughout the following day, By remaining awake, and blocking access to the adenosine drain that sleep opens up, the brain is unable to rid itself of the chemical sleep pressure. The mounting adenosine levels continue to rise. This should mean that the longer you are awake, the sleepier you feel. But that's not true. Though you will feel increasingly sleepy throughout the nighttime phase, hitting a low point in your alertness around five or six in the morning, thereafter, you'll start to be more awake. This effect is answered by the energy return of the circadian rhythm. Unlike sleep pressure, the circadian rhythm pays no attention to whether you are asleep or awake. Am I getting enough sleep? \u2691 When you don't sleep enough, one consequence among many is that adenosine concentrations remain too high, so the next morning you continue to accumulate sleep debt If after waking up you could fall asleep at ten or eleven in the morning, it means that you're likely not getting enough sleep quantity or quality. The same can be said if you can't function optimally without caffeine before noon, you'll be most likely self-medicating your state of chronic sleep deprivation. Other sleep indicators can be if you would sleep more if you didn't set an alarm clock, or if you find yourself at your computer screen reading and then rereading the same sentence. Of course, even if you are giving yourself plenty of time to get a full night of shut-eye, next-day fatigue and sleepiness can still occur because you are suffering from an undiagnosed sleep disorder. The sleep cycle \u2691 Humans cycle through two types of sleep in a regular pattern throughout the night with a period of 90 minutes. They were called non-rapid eye movement (NREM) and rapid eye movement (REM). Later, NREM sleep was further subdivided into four separate stages, named from 1 to 4 (all awful names (\u0482\u2323\u0300_\u2323\u0301) ). Stages 3 and 4 are the deepest stages of NREM sleep, meaning that it's more difficult to wake you up in comparison with stages 1 and 2. In REM sleep, your eyes rapidly move from side to side underneath the lids. This movement are accompanied by active brainwaves, almost identical to those observed when you are awake. On the other hand, eyes remain still and the brainwaves also calm down in the NREM phases. Even though we switch from sleep phases each 90 minutes, the ratio of NREM to REM sleep throughout the night changes across the night. In the first half of the night, the vast majority of time is spent in deep NREM and very little REM. But as we transition through the second half of the night, REM starts dominating. Although there is no scientific consensus, the need to remodel and update our neural circuits at night can explain this repeatable but asymmetric pattern. Throughout the day, the new memories are stored in the RAM of your brain, when you start to sleep, the brain needs to move the important ones to the hard drive, for long term retrieval. The brain needs to solve an optimization problem: The hard drive and the RAM have limited capacity. The RAM needs to be cleaned to be able to register the next day's memories. The brain needs RAM to do the analysis of which memories to keep and which to remove. A key function of NREM sleep is to remove unnecessary neural connections, while REM sleep plays a role in strengthening those connections. The different roles and the capacity limits explains why the brain needs to switch between them. The asymmetry can be explained with the simile of creating a sculpture from a block of clay. At the beginning of the night, the long phases of NREM extensively removes unneeded material, with short REM phases to define the basic form. With each cycle, less material needs to be strongly removed and more enhancing of the details is required, thus the increase of REM sleep. A danger resides in this sleep profile. Since your brain desires most of its REM sleep in the last part of the night, if you wake up early, sleeping 6 hours instead of 8, you can be losing between 60 to 90% of all your REM sleep, even though you are losing 25% of your total sleep time. It works both ways, if you instead go to sleep two hours late, you'll loose a significant amount of deep NREM sleep. Preventing the brain to have the required REM or NREM daily rations results in many physical and mental issues. Sleeping time and sense distortions \u2691 When you're asleep, you loose awareness of the outside world. Your ears are still hearing, your eyes, though closed, are still seeing, and the rest of the organs keep on working too. All these signals still flood into the center of your brain, but it's there, in the sensory convergence zone, where they end. The thalamus is the sensory gate to the brain that blocks them. If it lets them pass, they travel to the cortex at the top of your brain, where they are consciously perceived. By locking its gates shut when you're asleep, the thalamus imposes a sensory blackout in the brain. As a result, you are no longer consciously aware of the information transmitted from your sense organs. Another consequence of sleeping is a sense of time distortion experienced in two contradictory ways. While you loose your conscious mapping during sleep, at a non-conscious level, the brain keeps track of time with incredible precision. To distort it even more, you sense a time dilation in dreams. The signature patterns of brain-cell activity that occurs as you learn, gets recurrently repeated during sleep. That is, memories are being replayed at the level of brain-cell activity as you sleep. During REM sleep, the memories are replayed at half or quarter the speed in comparison of the activity when you're awake. This slow neural recounting may be the reason why we have that time dilation. References \u2691 Why we sleep book by Matthew Walker","title":"Sleep"},{"location":"sleep/#consequences-of-lack-of-sleep","text":"Sleeping less than six or seven hours a night can produce these consequences: Demolishing of the immune system. Doubling your risk of cancer. Is a key lifestyle factor determining and worsening the development of the Alzheimer's disease. Disruption of blood sugar levels so profoundly that you would be classified as pre-diabetic. Increase the likelihood of block and brittle of your coronary arteries. Setting you on a path toward cardiovascular disease, stroke, and congestive heart failure. Contributes to all major psychiatric conditions, including depression, anxiety, and suicidality. Swelling concentrations of a hormone that makes you feel hungry while suppressing a companion hormone that otherwise signals food satisfaction. Thwart the ability to learn and memorize. A balanced diet and exercise are of vital importance, but we now see sleep as the key factor in health. The physical and mental impairments caused by one night of bad sleep dwarf those caused by an equivalent absence of food or exercise. Therefore, the shorter you sleep, the shorter your life span .","title":"Consequences of lack of sleep"},{"location":"sleep/#sleep-benefits","text":"We sleep for a lot of nighttime benefits that service both our brains and our body. There does not seem to be one major organ within the body, or process within the brain, that isn't optimally enhanced by sleep. Within the brain, sleep enriches our ability to learn, memorize and make logical decisions and choices. It recalibrates our emotional brain circuits, allowing us to navigate next day social and psychological challenges with cool-headed composture. Downstairs in the body, sleep: Restocks the armory of our immune system: helping fight malignancy, preventing infection, and warding off sickness. Reforms the body's metabolic state by fine-tuning the balance of insulin and circulating glucose. Regulates our appetite, helping control body weight through healthy food selection rather than rash impulsivity. Maintains a flourishing microbiome within your gut essential to our nutritional health being. Is tied to the fitness of our cardiovascular system, lowering blood pressure while keeping our hearts in fine condition. Dreaming produces a neurochemical bath that mollifies painful memories and a virtual reality space in which the brain melds past and present knowledge, inspiring creativity. Therefore, Sleep is the single most effective thing we can do to reset our brain and body health each day .","title":"Sleep benefits"},{"location":"sleep/#sleep-physiological-effects","text":"There are two main factors that determine when you want to sleep or stay awake: The signal sent by the suprachiasmatic nucleus following the circadian rhythm. Sleep pressure: The brain builds up a chemical substance that creates the \"sleep pressure\". The longer you've been awake, the more that chemical sleep pressure accumulates, and consequentially, the sleepier you feel.","title":"Sleep physiological effects"},{"location":"sleep/#the-circadian-rhythm","text":"We have an internal clock deep within the brain, called the suprachiasmatic nucleus, that creates a cycling, day-night rhythm, known as circadian rhythm, that makes you feel tired or alert at regular times of night and day, respectively. The circadian rhythm determines: When you want to be awake or asleep. Your timed preferences for eating and drinking. Your moods and emotions The amount of urine you produce. Your core body temperature. Your metabolic rate. The release of numerous hormones. Contrary to common belief, circadian rhythm is not defined by the daylight sun cycle. As Kleitman and Richardson demonstrated in 1938: When cut off from the daily cycle of light and dark, the body keeps on maintaining the rhythm. The period of the circadian rhythm is different for each person, but has an average of 24 hours and 15 minutes. Even if it's not defined by the sun light, it corrects those 15 minutes of delay to stay in sync with it. The suprachiasmatic nucleus can readjust by about one hour each day, that is why jet lag can be spawn through multiple days. That reset does not come free. Studies in airplane cabin crews who frequently fly on long haul routes and have little chance to recover have registered: The part of the brains related to learning and memory had physically shrunk, suggesting the destruction of brain cells caused by the biological stress of timezone travel. Their short term memory was significantly impaired. They had far higher rates of cancer and type 2 diabetes than the general population. The peak and valley points of wakefulness or sleepiness vary too between people, it's known as their chronotype and it's strongly determined by genetics. The chronotype defines three types of people: Morning types : They have their peak of wakefulness early in the day and the sleepiness early at night. They prefer to wake at or around dawn, and function optimally at this time of day. Evening types : They prefer going to bed late and subsequently wake up late the following morning, or even in the afternoon. In between : The remaining people fall somewhere in between, with a slight leaning towards eveningness.","title":"The circadian rhythm"},{"location":"sleep/#melatonin","text":"The suprachiasmatic nucleus communicates its repeating signal of day and night to your brain and body by releasing melatonin into the bloodstream from the pineal gland. Soon after dusk, the suprachiasmatic nucleus starts increasing the levels of this hormone, telling the rest of the body that it's time to sleep. But melatonin has little influence on the generation of sleep itself. Once sleep is under way, melatonin decreases in concentration across the night and into the morning hours. With dawn, as sunlight enters the brain through the eyes (even through the closed lids), the pineal gland is instructed to stop releasing melatonin. The absence of circulating melatonin now informs the brain and body that it's time to return to a wakefulness active state for the rest of the day","title":"Melatonin"},{"location":"sleep/#sleep-pressure","text":"While you are awake, the brain is releasing a chemical called adenosine. One consequence of the increasing accumulation of adenosine is the increase of the desire to sleep by turning down the \"volume\" of wake promoting regions in the brain and turning up the sleep inducing ones. Most people fall to the pressure after twelve to sixteen hours of being awake.","title":"Sleep pressure"},{"location":"sleep/#caffeine","text":"You can artificially mute the sleep signal of adenosine by using a chemical that makes you feel more alert and awake, such as caffeine. Caffeine works by battling with adenosine for the privilege of latching on to adenosine receptors in the brain. Once caffeine occupies these receptors, it does not stimulate them like adenosine, making you sleepy. Rather, caffeine blocks and effectively inactivates the receptors acting as a masking agent. Levels of caffeine peak around thirty minutes after ingestion. What is problematic, though, is the persistence of caffeine in your system. It takes between five to seven hours to remove 50 percent of the caffeine concentration from your body. An enzyme within your liver removes caffeine from your system. Based in large part on genetics, some people have a more efficient version of the enzyme that degrades caffeine, allowing the liver to clear it from the bloodstream. Age is also a variable to take into account, the older we are the longer it takes our brain and body to remove it. When your liver evicts the caffeine from your system, you encounter the caffeine crash . Your energy levels plummet rapidly, finding difficult to function and concentrate, with a strong sense of sleepiness once again. For the entire time that caffeine is in your system, the adenosine keeps on building up. Your brain is not aware of this rising tide of sleep encouraging chemical because the wall of caffeine is holding it back from your perception. Once your liver dismantles the barricade, you feel a vicious backlash: you are hit with the sleepiness you had experienced two or three hours ago before you drank that cup of coffee plus all the extra adenosine that has accumulated in the hours in between.","title":"Caffeine"},{"location":"sleep/#relationship-between-the-circadian-rhythm-and-the-sleep-pressure","text":"The two governing forces that regulate your sleep are ignorant of each other. Although they are not coupled, they are usually aligned. Starting on the far left of the figure, the circadian rhythm begins to increase its activity a few hours before you wake up. It infuses the brain and body with an alerting energy signal. At first, the signal is faint, but gradually it builds with time. By early afternoon, the activating signal from the circadian rhythm peaks. Now let's look at the sleep pressure pattern. By mid to late morning, you have only been awake for a half of hours. As a result, adenosine concentrations have increased a little. Furthermore, the circadian rhythm is on its powerful upswing of alertness. This combination of strong activating output from the circadian rhythm together with low levels of adenosine result in a delightful sensation of being wide awake. The distance between the curved lines above will be a direct reflection of your desire to sleep. By eleven pm, you've been awake for fifteen hours, and your brain is drenched in high concentrations of adenosine. Additionally, the circadian rhythm line is descending, powering down your activity and alertness levels. This powerful combination triggers a strong desire for sleep. During sleep, a mass evacuation of adenosine gets under way as the brain has the chance to degrade and remove it. After eight hours of healthy sleep, the adenosine purge is complete. As this process is ending, the circadian activity rhythm has returned, and its energizing influence starts to approach, therefore naturally waking us up.","title":"Relationship between the circadian rhythm and the sleep pressure"},{"location":"sleep/#all-nighters","text":"Scientists can demonstrate that the two forces determining when you want to be awake and sleep are independent and can be decoupled from their normal lockstep. When you skip one night's sleep and remain awake throughout the following day, By remaining awake, and blocking access to the adenosine drain that sleep opens up, the brain is unable to rid itself of the chemical sleep pressure. The mounting adenosine levels continue to rise. This should mean that the longer you are awake, the sleepier you feel. But that's not true. Though you will feel increasingly sleepy throughout the nighttime phase, hitting a low point in your alertness around five or six in the morning, thereafter, you'll start to be more awake. This effect is answered by the energy return of the circadian rhythm. Unlike sleep pressure, the circadian rhythm pays no attention to whether you are asleep or awake.","title":"All-nighters"},{"location":"sleep/#am-i-getting-enough-sleep","text":"When you don't sleep enough, one consequence among many is that adenosine concentrations remain too high, so the next morning you continue to accumulate sleep debt If after waking up you could fall asleep at ten or eleven in the morning, it means that you're likely not getting enough sleep quantity or quality. The same can be said if you can't function optimally without caffeine before noon, you'll be most likely self-medicating your state of chronic sleep deprivation. Other sleep indicators can be if you would sleep more if you didn't set an alarm clock, or if you find yourself at your computer screen reading and then rereading the same sentence. Of course, even if you are giving yourself plenty of time to get a full night of shut-eye, next-day fatigue and sleepiness can still occur because you are suffering from an undiagnosed sleep disorder.","title":"Am I getting enough sleep?"},{"location":"sleep/#the-sleep-cycle","text":"Humans cycle through two types of sleep in a regular pattern throughout the night with a period of 90 minutes. They were called non-rapid eye movement (NREM) and rapid eye movement (REM). Later, NREM sleep was further subdivided into four separate stages, named from 1 to 4 (all awful names (\u0482\u2323\u0300_\u2323\u0301) ). Stages 3 and 4 are the deepest stages of NREM sleep, meaning that it's more difficult to wake you up in comparison with stages 1 and 2. In REM sleep, your eyes rapidly move from side to side underneath the lids. This movement are accompanied by active brainwaves, almost identical to those observed when you are awake. On the other hand, eyes remain still and the brainwaves also calm down in the NREM phases. Even though we switch from sleep phases each 90 minutes, the ratio of NREM to REM sleep throughout the night changes across the night. In the first half of the night, the vast majority of time is spent in deep NREM and very little REM. But as we transition through the second half of the night, REM starts dominating. Although there is no scientific consensus, the need to remodel and update our neural circuits at night can explain this repeatable but asymmetric pattern. Throughout the day, the new memories are stored in the RAM of your brain, when you start to sleep, the brain needs to move the important ones to the hard drive, for long term retrieval. The brain needs to solve an optimization problem: The hard drive and the RAM have limited capacity. The RAM needs to be cleaned to be able to register the next day's memories. The brain needs RAM to do the analysis of which memories to keep and which to remove. A key function of NREM sleep is to remove unnecessary neural connections, while REM sleep plays a role in strengthening those connections. The different roles and the capacity limits explains why the brain needs to switch between them. The asymmetry can be explained with the simile of creating a sculpture from a block of clay. At the beginning of the night, the long phases of NREM extensively removes unneeded material, with short REM phases to define the basic form. With each cycle, less material needs to be strongly removed and more enhancing of the details is required, thus the increase of REM sleep. A danger resides in this sleep profile. Since your brain desires most of its REM sleep in the last part of the night, if you wake up early, sleeping 6 hours instead of 8, you can be losing between 60 to 90% of all your REM sleep, even though you are losing 25% of your total sleep time. It works both ways, if you instead go to sleep two hours late, you'll loose a significant amount of deep NREM sleep. Preventing the brain to have the required REM or NREM daily rations results in many physical and mental issues.","title":"The sleep cycle"},{"location":"sleep/#sleeping-time-and-sense-distortions","text":"When you're asleep, you loose awareness of the outside world. Your ears are still hearing, your eyes, though closed, are still seeing, and the rest of the organs keep on working too. All these signals still flood into the center of your brain, but it's there, in the sensory convergence zone, where they end. The thalamus is the sensory gate to the brain that blocks them. If it lets them pass, they travel to the cortex at the top of your brain, where they are consciously perceived. By locking its gates shut when you're asleep, the thalamus imposes a sensory blackout in the brain. As a result, you are no longer consciously aware of the information transmitted from your sense organs. Another consequence of sleeping is a sense of time distortion experienced in two contradictory ways. While you loose your conscious mapping during sleep, at a non-conscious level, the brain keeps track of time with incredible precision. To distort it even more, you sense a time dilation in dreams. The signature patterns of brain-cell activity that occurs as you learn, gets recurrently repeated during sleep. That is, memories are being replayed at the level of brain-cell activity as you sleep. During REM sleep, the memories are replayed at half or quarter the speed in comparison of the activity when you're awake. This slow neural recounting may be the reason why we have that time dilation.","title":"Sleeping time and sense distortions"},{"location":"sleep/#references","text":"Why we sleep book by Matthew Walker","title":"References"},{"location":"sqlite/","text":"SQLite is a relational database management system (RDBMS) contained in a C library. In contrast to many other database management systems, SQLite is not a client\u2013server database engine. Rather, it is embedded into the end program. SQLite is ACID-compliant and implements most of the SQL standard, generally following PostgreSQL syntax. However, SQLite uses a dynamically and weakly typed SQL syntax that does not guarantee the domain integrity.[7] This means that one can, for example, insert a string into a column defined as an integer. SQLite will attempt to convert data between formats where appropriate, the string \"123\" into an integer in this case, but does not guarantee such conversions and will store the data as-is if such a conversion is not possible. SQLite is a popular choice as embedded database software for local/client storage in application software such as web browsers. It is arguably the most widely deployed database engine, as it is used today by several widespread browsers, operating systems, and embedded systems (such as mobile phones), among others. Upsert statements \u2691 UPSERT is a special syntax addition to INSERT that causes the INSERT to behave as an UPDATE or a no-op if the INSERT would violate a uniqueness constraint. UPSERT is not standard SQL. UPSERT in SQLite follows the syntax established by PostgreSQL. The syntax that occurs in between the \"ON CONFLICT\" and \"DO\" keywords is called the \"conflict target\". The conflict target specifies a specific uniqueness constraint that will trigger the upsert. The conflict target is required for DO UPDATE upserts, but is optional for DO NOTHING. When the conflict target is omitted, the upsert behavior is triggered by a violation of any uniqueness constraint on the table of the INSERT. If the insert operation would cause the uniqueness constraint identified by the conflict-target clause to fail, then the insert is omitted and either the DO NOTHING or DO UPDATE operation is performed instead. In the case of a multi-row insert, this decision is made separately for each row of the insert. The special UPSERT processing happens only for uniqueness constraint on the table that is receiving the INSERT. A \"uniqueness constraint\" is an explicit UNIQUE or PRIMARY KEY constraint within the CREATE TABLE statement, or a unique index. UPSERT does not intervene for failed NOT NULL or foreign key constraints or for constraints that are implemented using triggers. Column names in the expressions of a DO UPDATE refer to the original unchanged value of the column, before the attempted INSERT. To use the value that would have been inserted had the constraint not failed, add the special \"excluded.\" table qualifier to the column name. CREATE TABLE phonebook2 ( name TEXT PRIMARY KEY , phonenumber TEXT , validDate DATE ); INSERT INTO phonebook2 ( name , phonenumber , validDate ) VALUES ( 'Alice' , '704-555-1212' , '2018-05-08' ) ON CONFLICT ( name ) DO UPDATE SET phonenumber = excluded . phonenumber , validDate = excluded . validDate REGEXP \u2691 The REGEXP operator is a special syntax for the regexp() user function. No regexp() user function is defined by default and so use of the REGEXP operator will normally result in an error message. If an application-defined SQL function named regexp is added at run-time, then the X REGEXP Y operator will be implemented as a call to regexp(Y,X) . If you're using sqlite3 , you can check how to create the regexp function . Troubleshooting \u2691 [Integer autoincrement not \u2691 working]( https://stackoverflow.com/questions/16832401/sqlite-auto-increment-not-working ) Rename the column type from INT to INTEGER and it starts working. From this: CREATE TABLE IF NOT EXISTS foo ( id INT PRIMARY KEY , bar INT ) to this: CREATE TABLE IF NOT EXISTS foo ( id INTEGER PRIMARY KEY , bar INT ) References \u2691 Home rqlite : is a lightweight, distributed relational database, which uses SQLite as its storage engine. Forming a cluster is very straightforward, it gracefully handles leader elections, and tolerates failures of machines, including the leader.","title":"SQLite"},{"location":"sqlite/#upsert-statements","text":"UPSERT is a special syntax addition to INSERT that causes the INSERT to behave as an UPDATE or a no-op if the INSERT would violate a uniqueness constraint. UPSERT is not standard SQL. UPSERT in SQLite follows the syntax established by PostgreSQL. The syntax that occurs in between the \"ON CONFLICT\" and \"DO\" keywords is called the \"conflict target\". The conflict target specifies a specific uniqueness constraint that will trigger the upsert. The conflict target is required for DO UPDATE upserts, but is optional for DO NOTHING. When the conflict target is omitted, the upsert behavior is triggered by a violation of any uniqueness constraint on the table of the INSERT. If the insert operation would cause the uniqueness constraint identified by the conflict-target clause to fail, then the insert is omitted and either the DO NOTHING or DO UPDATE operation is performed instead. In the case of a multi-row insert, this decision is made separately for each row of the insert. The special UPSERT processing happens only for uniqueness constraint on the table that is receiving the INSERT. A \"uniqueness constraint\" is an explicit UNIQUE or PRIMARY KEY constraint within the CREATE TABLE statement, or a unique index. UPSERT does not intervene for failed NOT NULL or foreign key constraints or for constraints that are implemented using triggers. Column names in the expressions of a DO UPDATE refer to the original unchanged value of the column, before the attempted INSERT. To use the value that would have been inserted had the constraint not failed, add the special \"excluded.\" table qualifier to the column name. CREATE TABLE phonebook2 ( name TEXT PRIMARY KEY , phonenumber TEXT , validDate DATE ); INSERT INTO phonebook2 ( name , phonenumber , validDate ) VALUES ( 'Alice' , '704-555-1212' , '2018-05-08' ) ON CONFLICT ( name ) DO UPDATE SET phonenumber = excluded . phonenumber , validDate = excluded . validDate","title":"Upsert statements"},{"location":"sqlite/#regexp","text":"The REGEXP operator is a special syntax for the regexp() user function. No regexp() user function is defined by default and so use of the REGEXP operator will normally result in an error message. If an application-defined SQL function named regexp is added at run-time, then the X REGEXP Y operator will be implemented as a call to regexp(Y,X) . If you're using sqlite3 , you can check how to create the regexp function .","title":"REGEXP"},{"location":"sqlite/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"sqlite/#integer-autoincrement-not","text":"working]( https://stackoverflow.com/questions/16832401/sqlite-auto-increment-not-working ) Rename the column type from INT to INTEGER and it starts working. From this: CREATE TABLE IF NOT EXISTS foo ( id INT PRIMARY KEY , bar INT ) to this: CREATE TABLE IF NOT EXISTS foo ( id INTEGER PRIMARY KEY , bar INT )","title":"[Integer autoincrement not"},{"location":"sqlite/#references","text":"Home rqlite : is a lightweight, distributed relational database, which uses SQLite as its storage engine. Forming a cluster is very straightforward, it gracefully handles leader elections, and tolerates failures of machines, including the leader.","title":"References"},{"location":"sqlite3/","text":"SQLite3 is a python library that provides an SQL interface compliant with the DB-API 2.0 specification described by PEP 249. Usage \u2691 To use the module, you must first create a Connection object that represents the database, and a Cursor one to interact with it. Here the data will be stored in the example.db file: import sqlite3 conn = sqlite3 . connect ( 'example.db' ) cursor = conn . cursor () Once we have a cursor we can execute the different SQL statements and the save them with the commit method of the Connection object. Finally we can close the connection with close . # Create table cursor . execute ( '''CREATE TABLE stocks (date text, trans text, symbol text, qty real, price real)''' ) # Insert a row of data cursor . execute ( \"INSERT INTO stocks VALUES ('2006-01-05','BUY','RHAT',100,35.14)\" ) # Save (commit) the changes conn . commit () # We can also close the connection if we are done with it. # Just be sure any changes have been committed or they will be lost. conn . close () Get columns of a query \u2691 cursor = connection . execute ( 'select * from bar' ) names = [ description [ 0 ] for description in cursor . description ] Regexp \u2691 SQLite needs the user to define a regexp function to be able to use the filter. import sqlite3 import re def regexp ( expr , item ): reg = re . compile ( expr ) return reg . search ( item ) is not None conn = sqlite3 . connect ( ':memory:' ) conn . create_function ( \"REGEXP\" , 2 , regexp ) cursor = conn . cursor () References \u2691 Docs","title":"sqlite3"},{"location":"sqlite3/#usage","text":"To use the module, you must first create a Connection object that represents the database, and a Cursor one to interact with it. Here the data will be stored in the example.db file: import sqlite3 conn = sqlite3 . connect ( 'example.db' ) cursor = conn . cursor () Once we have a cursor we can execute the different SQL statements and the save them with the commit method of the Connection object. Finally we can close the connection with close . # Create table cursor . execute ( '''CREATE TABLE stocks (date text, trans text, symbol text, qty real, price real)''' ) # Insert a row of data cursor . execute ( \"INSERT INTO stocks VALUES ('2006-01-05','BUY','RHAT',100,35.14)\" ) # Save (commit) the changes conn . commit () # We can also close the connection if we are done with it. # Just be sure any changes have been committed or they will be lost. conn . close ()","title":"Usage"},{"location":"sqlite3/#get-columns-of-a-query","text":"cursor = connection . execute ( 'select * from bar' ) names = [ description [ 0 ] for description in cursor . description ]","title":"Get columns of a query"},{"location":"sqlite3/#regexp","text":"SQLite needs the user to define a regexp function to be able to use the filter. import sqlite3 import re def regexp ( expr , item ): reg = re . compile ( expr ) return reg . search ( item ) is not None conn = sqlite3 . connect ( ':memory:' ) conn . create_function ( \"REGEXP\" , 2 , regexp ) cursor = conn . cursor ()","title":"Regexp"},{"location":"sqlite3/#references","text":"Docs","title":"References"},{"location":"strategy/","text":"Strategy is a general plan to achieve one or more long-term or overall goals under conditions of uncertainty. Strategy is important because the resources available to achieve goals are usually limited. Strategy generally involves setting goals and priorities, determining actions to achieve the goals, and mobilizing resources to execute the actions. A strategy describes how the ends (goals) will be achieved by the means (resources). Strategy can be intended or can emerge as a pattern of activity as the person or organization adapts to its environment. It typically involves two major processes: Formulation : Involves analyzing the environment or situation, making a diagnosis, and developing guiding policies. It includes such activities as strategic planning and strategic thinking . Implementation : Refers to the action plans taken to achieve the goals established by the guiding policy. Strategic planning \u2691 Strategic planning is an organization's process of defining its strategy, or direction, and making decisions on allocating its resources to pursue this strategy. It helps coordinate the two processes required by the strategy, formulation and implementation. However, strategic planning is analytical in nature (i.e., it involves \"finding the dots\"); strategy formation itself involves synthesis (i.e., \"connecting the dots\") via strategic thinking. As such, strategic planning occurs around the strategy formation activity. Strategic thinking \u2691 Strategic thinking is defined as a mental or thinking process applied by an individual in the context of achieving a goal or set of goals in a game or other endeavor. As a cognitive activity, it produces thought. Strategic thinking includes finding and developing a strategic foresight capacity for an organization or individual, by exploring all possible futures, and challenging conventional thinking to foster decision making today. The strategist must have a great capacity for both analysis and synthesis; analysis is necessary to assemble the data on which he makes his diagnosis, synthesis in order to produce from these data the diagnosis itself\u2014and the diagnosis in fact amounts to a choice between alternative courses of action.","title":"Strategy"},{"location":"strategy/#strategic-planning","text":"Strategic planning is an organization's process of defining its strategy, or direction, and making decisions on allocating its resources to pursue this strategy. It helps coordinate the two processes required by the strategy, formulation and implementation. However, strategic planning is analytical in nature (i.e., it involves \"finding the dots\"); strategy formation itself involves synthesis (i.e., \"connecting the dots\") via strategic thinking. As such, strategic planning occurs around the strategy formation activity.","title":"Strategic planning"},{"location":"strategy/#strategic-thinking","text":"Strategic thinking is defined as a mental or thinking process applied by an individual in the context of achieving a goal or set of goals in a game or other endeavor. As a cognitive activity, it produces thought. Strategic thinking includes finding and developing a strategic foresight capacity for an organization or individual, by exploring all possible futures, and challenging conventional thinking to foster decision making today. The strategist must have a great capacity for both analysis and synthesis; analysis is necessary to assemble the data on which he makes his diagnosis, synthesis in order to produce from these data the diagnosis itself\u2014and the diagnosis in fact amounts to a choice between alternative courses of action.","title":"Strategic thinking"},{"location":"tahoe/","text":"Tahoe-LAFS is a free and open, secure, decentralized, fault-tolerant, distributed data store and distributed file system. Tahoe-LAFS is a system that helps you to store files. You run a client program on your computer, which talks to one or more storage servers on other computers. When you tell your client to store a file, it will encrypt that file, encode it into multiple pieces, then spread those pieces out among multiple servers. The pieces are all encrypted and protected against modifications. Later, when you ask your client to retrieve the file, it will find the necessary pieces, make sure they haven\u2019t been corrupted, reassemble them, and decrypt the result. The client creates more pieces (or \u201cshares\u201d) than it will eventually need, so even if some of the servers fail, you can still get your data back. Corrupt shares are detected and ignored, so the system can tolerate server-side hard-drive errors. All files are encrypted (with a unique key) before uploading, so even a malicious server operator cannot read your data. The only thing you ask of the servers is that they can (usually) provide the shares when you ask for them: you aren\u2019t relying upon them for confidentiality, integrity, or absolute availability. Tahoe does not provide locking of mutable files and directories . If there is more than one simultaneous attempt to change a mutable file or directory, then an UncoordinatedWriteError may result. This might, in rare cases, cause the file or directory contents to be accidentally deleted. The user is expected to ensure that there is at most one outstanding write or update request for a given file or directory at a time. One convenient way to accomplish this is to make a different file or directory for each person or process that wants to write. If mutable parts of a file store are accessed via sshfs, only a single sshfs mount should be used. There may be data loss if mutable files or directories are accessed via two sshfs mounts, or written both via sshfs and from other clients. Installation \u2691 apt-get install tahoe-lafs Or if you want the latest version pip install tahoe-lafs If you plan to connect to servers protected through Tor, use pip install tahoe-lafs[tor] instead. Troubleshooting \u2691 pkg_resources.DistributionNotFound: The idna distribution was not found and is required by Twisted \u2691 apt-get install python-idna References \u2691 Git Docs Issues","title":"Tahoe-LAFS"},{"location":"tahoe/#installation","text":"apt-get install tahoe-lafs Or if you want the latest version pip install tahoe-lafs If you plan to connect to servers protected through Tor, use pip install tahoe-lafs[tor] instead.","title":"Installation"},{"location":"tahoe/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"tahoe/#pkg_resourcesdistributionnotfound-the-idna-distribution-was-not-found-and-is-required-by-twisted","text":"apt-get install python-idna","title":"pkg_resources.DistributionNotFound: The idna  distribution was not found and is required by Twisted"},{"location":"tahoe/#references","text":"Git Docs Issues","title":"References"},{"location":"talkey/","text":"Talkey is a Simple Text-To-Speech (TTS) interface library with multi-language and multi-engine support. Installation \u2691 pip install talkey You need to install the TTS engines by yourself. Talkey supports: Flite SVOX Pico Festival eSpeak mbrola via eSpeak I've tried SVOX Pico, Festival and eSpeak. I've discarded Flite because it's not in the official repositories. Of those three the one that gives the most natural support is SVOX Pico. To install it execute: sudo apt-get install libttspico-utils It also supports the following networked TTS Engines: MaryTTS (needs hosting). Google TTS (cloud hosted) I obviously discard Google for privacy reasons, and MaryTTS too because it needs you to run a server, which is inconvenient for most users and pico gives us enough quality. Usage \u2691 At its simplest use case: import talkey tts = talkey . Talkey () tts . say ( \"I've been really busy being dead. You know, after you murdered me.\" ) It automatically detects languages without any further configuration: tts . say ( \"La cabra siempre tira al monte\" ) References \u2691 Git Docs","title":"Talkey"},{"location":"talkey/#installation","text":"pip install talkey You need to install the TTS engines by yourself. Talkey supports: Flite SVOX Pico Festival eSpeak mbrola via eSpeak I've tried SVOX Pico, Festival and eSpeak. I've discarded Flite because it's not in the official repositories. Of those three the one that gives the most natural support is SVOX Pico. To install it execute: sudo apt-get install libttspico-utils It also supports the following networked TTS Engines: MaryTTS (needs hosting). Google TTS (cloud hosted) I obviously discard Google for privacy reasons, and MaryTTS too because it needs you to run a server, which is inconvenient for most users and pico gives us enough quality.","title":"Installation"},{"location":"talkey/#usage","text":"At its simplest use case: import talkey tts = talkey . Talkey () tts . say ( \"I've been really busy being dead. You know, after you murdered me.\" ) It automatically detects languages without any further configuration: tts . say ( \"La cabra siempre tira al monte\" )","title":"Usage"},{"location":"talkey/#references","text":"Git Docs","title":"References"},{"location":"task_management/","text":"Task management is the process of managing a task through its life cycle. It involves planning, testing, tracking, and reporting. Task management can help either individual achieve goals, or groups of individuals collaborate and share knowledge for the accomplishment of collective goals. You can address task management at different levels. High level management ensures that you choose your tasks in order to accomplish a goal, low level management helps you get things done. When you do task management well, you benefit from: Reducing your mental load, so you can use those resources doing productive work. Improving your efficiency. Making more realistic estimations, thus meeting the commited deadlines. Finishing what you start. Knowing you're working towards your ultimate goals Stop feeling lost or overburdened. Make context switches cheaper. On the other side, task management done wrong can consume your willpower in the exchange of lost time and a confused mind. The tricky reality is that the factors that decide if you do it right or wrong are different for every person, and even for a person it may change over the time or mood states. That's why I follow the thought that task management is a tool that is meant to help you. If it's not, you need to change your system until it does. A side effect is that you have to tailor your task management system yourself. It doesn't matter how good the systems you find in the internet are, until you start getting your hands dirty, you won't know if they works for you. So instead of trying to discover the perfect solution, start with one that introduces the least friction in your current workflow, and evolve from that point guided by the faults you find. Forget about instant solutions, this is a never ending marathon. Make sure that each step is small and easy, otherwise you will get tired too soon. I haven't written a guide yet on how to give your first steps, but you could start by following a simple workflow with simple tools .","title":"Task Management"},{"location":"task_tools/","text":"I currently use two tools to manage my tasks: the inbox and the task manager . Inbox \u2691 The inbox does not refer only to your e-mail inbox. It is a broader concept that includes all the elements you have collected in different ways: tasks you have to do, ideas you have thought of, notes, bills, business cards, etc\u2026 To achieve a stress-free productivity, emptying the inbox should be a daily activity. Note that this does not mean doing things, it just means identifying things and deciding what to do with them, when you get it done, your situation is as follows: You have eliminated every thing you do not need. You have completed small actions that require no more than two minutes. You have delegated some actions that you do not have to do. You have sorted in your task manager the actions you will do when appropriate, because they require more than 2 minutes. You have sorted in your task manager or calendar the tasks that have a due date. There have been only a few minutes, but you feel pretty good. Everything is where it should be. I've developed pynbox to automate the management of the inbox. Help out if you like it! Task manager \u2691 If you've never used a task manager, start with the simplest one and see what do you feel its lacking. Choose then a better task manager based on your needs. In the past I've used taskwarrior , but its limitations led me to start creating pydo . I'll update this section once I have a stable workflow with the new program. The simplest task manager \u2691 The simplest task manager is a markdown file in your computer with a list of tasks to do. Annotate only the actionable tasks that you need to do today, otherwise it can quickly grow to be unmanageable. When you add a new item, choose it's location relative to the existent one based on its priority. Being the top tasks are the ones that need to be done first. * Task with a high priority * Task with low priority The advantages of using a plaintext file over a physical notebook is that you can use your editor skills to manage the elements more efficiently. For example by reordering them or changing the description. Add task state sections \u2691 You'll soon encounter tasks that become blocked but need your monitoring. You can add a # Blocked section and move those tasks under it. You can optionally add the reasons why it's blocked indented below the element. * Unblocked task # Blocked * Blocked task * Waiting for Y to happen Divide a task in small steps \u2691 One of the main benefits of a task manager is that you free your mind of what you need to do next, so you can focus on the task at hand. When a task is big split it in smaller doable steps that drive to its completion. If the steps are also big split them further with more indentation levels. * Complex task * Do X * Do Y * Do Z * Do W References \u2691 GTD time management framework.","title":"Task Management Tools"},{"location":"task_tools/#inbox","text":"The inbox does not refer only to your e-mail inbox. It is a broader concept that includes all the elements you have collected in different ways: tasks you have to do, ideas you have thought of, notes, bills, business cards, etc\u2026 To achieve a stress-free productivity, emptying the inbox should be a daily activity. Note that this does not mean doing things, it just means identifying things and deciding what to do with them, when you get it done, your situation is as follows: You have eliminated every thing you do not need. You have completed small actions that require no more than two minutes. You have delegated some actions that you do not have to do. You have sorted in your task manager the actions you will do when appropriate, because they require more than 2 minutes. You have sorted in your task manager or calendar the tasks that have a due date. There have been only a few minutes, but you feel pretty good. Everything is where it should be. I've developed pynbox to automate the management of the inbox. Help out if you like it!","title":"Inbox"},{"location":"task_tools/#task-manager","text":"If you've never used a task manager, start with the simplest one and see what do you feel its lacking. Choose then a better task manager based on your needs. In the past I've used taskwarrior , but its limitations led me to start creating pydo . I'll update this section once I have a stable workflow with the new program.","title":"Task manager"},{"location":"task_tools/#the-simplest-task-manager","text":"The simplest task manager is a markdown file in your computer with a list of tasks to do. Annotate only the actionable tasks that you need to do today, otherwise it can quickly grow to be unmanageable. When you add a new item, choose it's location relative to the existent one based on its priority. Being the top tasks are the ones that need to be done first. * Task with a high priority * Task with low priority The advantages of using a plaintext file over a physical notebook is that you can use your editor skills to manage the elements more efficiently. For example by reordering them or changing the description.","title":"The simplest task manager"},{"location":"task_tools/#add-task-state-sections","text":"You'll soon encounter tasks that become blocked but need your monitoring. You can add a # Blocked section and move those tasks under it. You can optionally add the reasons why it's blocked indented below the element. * Unblocked task # Blocked * Blocked task * Waiting for Y to happen","title":"Add task state sections"},{"location":"task_tools/#divide-a-task-in-small-steps","text":"One of the main benefits of a task manager is that you free your mind of what you need to do next, so you can focus on the task at hand. When a task is big split it in smaller doable steps that drive to its completion. If the steps are also big split them further with more indentation levels. * Complex task * Do X * Do Y * Do Z * Do W","title":"Divide a task in small steps"},{"location":"task_tools/#references","text":"GTD time management framework.","title":"References"},{"location":"task_workflows/","text":"Hype flow versus a defined plan \u2691 I've found two ways to work on my tasks: following a plan and following the hype flow. The first one helps you finish what you started, and directs your efforts towards big goals. The side effect is that it achieves it by setting constrains on what to do, so you sometimes end up in the position of doing tasks that you don't want to at the moment, and suppressing yourself not to do the ones that you want. The second one takes advantage of letting you work on wherever you want at the moment, which boosts your creativity and productivity. This way imposes less constrains on you and is more pleasant because surfing the hype is awesome. The side effect is that if you have many interests, you can move forward very quickly on many directions leaving a lot of projects half done, instead of pushing in the direction of your big goals. The art here is to combine both at need, if you have a good plan, you may be able to start surfing the hype, and when the time constrains start to press you, switch to a stricter plan to be able to deliver value in time. This makes more sense in work environments, at personal level I usually just surf the hype unless I have a clear objective with a due date to reach. Planning workflows \u2691 Task management can be done at different levels. All of them help you in different ways to reduce the mental load, each also gives you extra benefits that can't be gained by the others. Going from lowest to highest abstraction level we have: Pomodoro. Task plan. Day plan. Week plan. Month plan. Semester plan. If you're starting your task management career, start with the first level. Once you're comfortable, move one step up until you reach the sweet spot between time invested in management and the profit it returns. Pomodoro \u2691 Pomodoro is a technique used to ensure that for short periods of time, you invest all your mental resources in doing the work needed to finish a task. It's your main unit of work and a good starting point if you have concentration issues. When done well, you'll start moving faster on your tasks, because uninterrupted work is the most efficient. You'll also begin to know if you're drifting from your day's plan , and will have space to adapt it or the task plan to time constrains or unexpected events. If you don't yet have a task plan or day plan , don't worry! Ignore the steps that involve them until you do. The next steps define a Pomodoro cycle: Select the cycle time span. Either 20 minutes or until the next interruption, whichever is shortest. Decide what are you going to do. Analyze yourself to see if you're state of mind is ready to only do that for the chosen time span. If it's not, maybe you need to take a \"Pomodoro break\", take 20 minutes off doing something that replenish your willpower or the personal attribute that is preventing you to be able to work. Start the timer. Work uninterruptedly on what you've decided until the timer goes off. Update your task plan : Tick off the done task steps. Refine the task steps that can be addressed in the next cycle. Check the interruption channels that need to be checked each 20 minutes . At the fourth Pomodoro cycle, you'll have finished a Pomodoro iteration. At the end of the iteration: Check if you're going to meet the day plan , if you're not, change change it or the task plan to make the time constrain. Get a small rest, you've earned it! Get off the chair, stretch or give a small walk. What's important is that you take your mind off the task at hand and let your body rest. Remember, this is a marathon, you need to take care of yourself. Start a new Pomodoro iteration. If you're super focused at the end of a Pomodoro cycle, you can skip the task plan update until the end of the iteration. Task plan \u2691 The task plan defines the steps required to finish a task. It's your most basic roadmap to address a task, and a good starting point if you feel overwhelmed when faced with an assignment. When done well, you'll better understand what you need to do, it will prevent you from wasting time at dead ends as you'll think before acting, and you'll develop the invaluable skill of breaking big problems into smaller ones. To define a task plan, follow the next steps: Decide what do you want to achieve when the task is finished. Analyze the possible ways to arrive to that goal. Try to assess different solutions before choosing one. Once you have it, split it into steps small enough to be comfortable following them without further analysis. Some people define the task plan whenever they add the task to their task manager. Others prefer to save some time each month to refine the plans of the tasks to be done the next one. The plan is an alive document that changes each Pomodoro cycle and that you'll need to check often. It has to be accessible and it should be easy for you to edit. If you don't know where to start, use the simplest task manager . Try not to overplan though, if at the middle of a task you realize that the rest of the steps don't make sense, all the time invested in their definition will be lost. That's why it's a good idea to have a great detail for the first steps and gradually move to rougher definitions on later ones. Day plan \u2691 This plan defines at day level which tasks are you going to work on and schedules when are you going to address them. It's the most basic roadmap to address a group of tasks. The goal is to survive the day. It's a good starting point if you forget to do tasks that need to be done in the day or if you miss appointments. It's also the next step of advance awareness, if you have a day plan, on each Pomodoro iteration you'll get the feeling whether you're going to finish what you proposed yourself. You can make your plan at the start of the day, start by getting an idea of: What do you need to do by checking: The last day's plan. Calendar events. The week's plan if you have it, or the prioritized list of tasks to do. How much uninterrupted time you have between calendar events. Your state of mind. Then create the day schedule: Add the calendar events. Add the interruption events . Decide the tasks to be worked on and assign them uninterrupted time slots. Setup an alert for the closest calendar event. Follow it throughout the day, and when it's coming to an end: Update your week or/and task plans to meet the time constrains. Optionally sketch the next day's plan. When doing the plan keep in mind to minimize the number of tasks and calendar events so as not to get overwhelmed, and not to schedule a new task before you finish what you've already started. It's better to eventually fall short on tasks, than never reaching your goal. Week plan \u2691 The plan defines at week level which tasks are you going to work on and schedules when are you going to address them. It's the next roadmap level to address a group of tasks. The goal changes from surviving the day to start planning your life. It's a good starting point if you are comfortable working with the pomodoro, task and day plans, and want to start deciding where you're heading to. It's also the next step of advance awareness, if you have a week plan, each day you'll get the feeling whether you're going to finish what you proposed yourself. You can make your plan at the start of the week, similar to the day plan , start by getting an idea of: What do you need to do by checking: The last week's plan. Calendar events. The month's plan if you have it, or the prioritized list of tasks to do, identifying the task dependencies that may block the task development. How much uninterrupted time you have between calendar events. Your state of mind. Then create the week schedule: Arrange or move calendar events to maximize the uninterrupted periods, then add them to the plan. Add the interruption events . Decide the tasks to be worked on and roughly assign them to the week days. Follow it throughout the week, and when it's coming to an end: Update your month or/and task plans to meet the time constrains. Optionally sketch the next week's plan. Update your team of possible plan drifts. References \u2691 Pomodoro article .","title":"Task Management Workflows"},{"location":"task_workflows/#hype-flow-versus-a-defined-plan","text":"I've found two ways to work on my tasks: following a plan and following the hype flow. The first one helps you finish what you started, and directs your efforts towards big goals. The side effect is that it achieves it by setting constrains on what to do, so you sometimes end up in the position of doing tasks that you don't want to at the moment, and suppressing yourself not to do the ones that you want. The second one takes advantage of letting you work on wherever you want at the moment, which boosts your creativity and productivity. This way imposes less constrains on you and is more pleasant because surfing the hype is awesome. The side effect is that if you have many interests, you can move forward very quickly on many directions leaving a lot of projects half done, instead of pushing in the direction of your big goals. The art here is to combine both at need, if you have a good plan, you may be able to start surfing the hype, and when the time constrains start to press you, switch to a stricter plan to be able to deliver value in time. This makes more sense in work environments, at personal level I usually just surf the hype unless I have a clear objective with a due date to reach.","title":"Hype flow versus a defined plan"},{"location":"task_workflows/#planning-workflows","text":"Task management can be done at different levels. All of them help you in different ways to reduce the mental load, each also gives you extra benefits that can't be gained by the others. Going from lowest to highest abstraction level we have: Pomodoro. Task plan. Day plan. Week plan. Month plan. Semester plan. If you're starting your task management career, start with the first level. Once you're comfortable, move one step up until you reach the sweet spot between time invested in management and the profit it returns.","title":"Planning workflows"},{"location":"task_workflows/#pomodoro","text":"Pomodoro is a technique used to ensure that for short periods of time, you invest all your mental resources in doing the work needed to finish a task. It's your main unit of work and a good starting point if you have concentration issues. When done well, you'll start moving faster on your tasks, because uninterrupted work is the most efficient. You'll also begin to know if you're drifting from your day's plan , and will have space to adapt it or the task plan to time constrains or unexpected events. If you don't yet have a task plan or day plan , don't worry! Ignore the steps that involve them until you do. The next steps define a Pomodoro cycle: Select the cycle time span. Either 20 minutes or until the next interruption, whichever is shortest. Decide what are you going to do. Analyze yourself to see if you're state of mind is ready to only do that for the chosen time span. If it's not, maybe you need to take a \"Pomodoro break\", take 20 minutes off doing something that replenish your willpower or the personal attribute that is preventing you to be able to work. Start the timer. Work uninterruptedly on what you've decided until the timer goes off. Update your task plan : Tick off the done task steps. Refine the task steps that can be addressed in the next cycle. Check the interruption channels that need to be checked each 20 minutes . At the fourth Pomodoro cycle, you'll have finished a Pomodoro iteration. At the end of the iteration: Check if you're going to meet the day plan , if you're not, change change it or the task plan to make the time constrain. Get a small rest, you've earned it! Get off the chair, stretch or give a small walk. What's important is that you take your mind off the task at hand and let your body rest. Remember, this is a marathon, you need to take care of yourself. Start a new Pomodoro iteration. If you're super focused at the end of a Pomodoro cycle, you can skip the task plan update until the end of the iteration.","title":"Pomodoro"},{"location":"task_workflows/#task-plan","text":"The task plan defines the steps required to finish a task. It's your most basic roadmap to address a task, and a good starting point if you feel overwhelmed when faced with an assignment. When done well, you'll better understand what you need to do, it will prevent you from wasting time at dead ends as you'll think before acting, and you'll develop the invaluable skill of breaking big problems into smaller ones. To define a task plan, follow the next steps: Decide what do you want to achieve when the task is finished. Analyze the possible ways to arrive to that goal. Try to assess different solutions before choosing one. Once you have it, split it into steps small enough to be comfortable following them without further analysis. Some people define the task plan whenever they add the task to their task manager. Others prefer to save some time each month to refine the plans of the tasks to be done the next one. The plan is an alive document that changes each Pomodoro cycle and that you'll need to check often. It has to be accessible and it should be easy for you to edit. If you don't know where to start, use the simplest task manager . Try not to overplan though, if at the middle of a task you realize that the rest of the steps don't make sense, all the time invested in their definition will be lost. That's why it's a good idea to have a great detail for the first steps and gradually move to rougher definitions on later ones.","title":"Task plan"},{"location":"task_workflows/#day-plan","text":"This plan defines at day level which tasks are you going to work on and schedules when are you going to address them. It's the most basic roadmap to address a group of tasks. The goal is to survive the day. It's a good starting point if you forget to do tasks that need to be done in the day or if you miss appointments. It's also the next step of advance awareness, if you have a day plan, on each Pomodoro iteration you'll get the feeling whether you're going to finish what you proposed yourself. You can make your plan at the start of the day, start by getting an idea of: What do you need to do by checking: The last day's plan. Calendar events. The week's plan if you have it, or the prioritized list of tasks to do. How much uninterrupted time you have between calendar events. Your state of mind. Then create the day schedule: Add the calendar events. Add the interruption events . Decide the tasks to be worked on and assign them uninterrupted time slots. Setup an alert for the closest calendar event. Follow it throughout the day, and when it's coming to an end: Update your week or/and task plans to meet the time constrains. Optionally sketch the next day's plan. When doing the plan keep in mind to minimize the number of tasks and calendar events so as not to get overwhelmed, and not to schedule a new task before you finish what you've already started. It's better to eventually fall short on tasks, than never reaching your goal.","title":"Day plan"},{"location":"task_workflows/#week-plan","text":"The plan defines at week level which tasks are you going to work on and schedules when are you going to address them. It's the next roadmap level to address a group of tasks. The goal changes from surviving the day to start planning your life. It's a good starting point if you are comfortable working with the pomodoro, task and day plans, and want to start deciding where you're heading to. It's also the next step of advance awareness, if you have a week plan, each day you'll get the feeling whether you're going to finish what you proposed yourself. You can make your plan at the start of the week, similar to the day plan , start by getting an idea of: What do you need to do by checking: The last week's plan. Calendar events. The month's plan if you have it, or the prioritized list of tasks to do, identifying the task dependencies that may block the task development. How much uninterrupted time you have between calendar events. Your state of mind. Then create the week schedule: Arrange or move calendar events to maximize the uninterrupted periods, then add them to the plan. Add the interruption events . Decide the tasks to be worked on and roughly assign them to the week days. Follow it throughout the week, and when it's coming to an end: Update your month or/and task plans to meet the time constrains. Optionally sketch the next week's plan. Update your team of possible plan drifts.","title":"Week plan"},{"location":"task_workflows/#references","text":"Pomodoro article .","title":"References"},{"location":"teeth/","text":"Taking good care of your teeth can be easier if you remember that each visit to the dentist is both super expensive and painful. So those 10 minutes each day are really worth it. How to take care of your teeth \u2691 TL;DR: Daily actions to keep your teeth healty Brush your teeth after every meal for at least two minutes. Floss each day before the last teeth brush. Use an electric toothbrush . Replace the brush each three months or at first sign of wear and tear. Don't eat or drink anything but water after your nightly brush. Do not rinse after you brush your teeth . Use floss instead of a toothpick . Use mouthwash daily but not after brushing . Oral hygiene is the practice of keeping one's mouth clean and free of disease and other problems (e.g. bad breath) by regular brushing of the teeth (dental hygiene) and cleaning between the teeth. It is important that oral hygiene be carried out on a regular basis to enable prevention of dental disease and bad breath. The most common types of dental disease are tooth decay (cavities, dental caries) and gum diseases, including gingivitis , and periodontitis . General guidelines suggest brushing twice a day: after breakfast and before going to bed, but ideally the mouth would be cleaned after every meal. Cleaning between the teeth is called interdental cleaning and is as important as tooth brushing. This is because a toothbrush cannot reach between the teeth and therefore only removes about 50% of plaque from the surface of the teeth. There are many tools to clean between the teeth, including floss and interdental brushes; it is up to each individual to choose which tool they prefer to use. Over 80% of cavities occur inside fissures in teeth where brushing cannot reach food left trapped after eating and saliva and fluoride have no access to neutralize acid and remineralize demineralized teeth, unlike easy-to-clean parts of the tooth, where fewer cavities occur. Teeth brushing \u2691 Routine tooth brushing is the principal method of preventing many oral diseases, and perhaps the most important activity an individual can practice to reduce dental plaque and tartar . The dental plaque contains a mixture of bacteria, their acids and sticky byproducts and food remnants. It forms naturally on teeth immediately after you've eaten but doesn't get nasty and start to cause damage to the teeth until it reaches a certain stage of maturity. The exact amount of time this takes isn't known but is at least more than 12 hours. Bacteria consume sugar and, as a byproduct, produce acids which dissolve mineral out of the teeth, leaving microscopic holes we can't see. If the process isn't stopped and they aren't repaired, these can become big, visible cavities. So controlling plaque reduces the risk of the individual suffering from plaque-associated diseases such as gingivitis, periodontitis, and caries. Many oral health care professionals agree that tooth brushing should be done for a minimum of two minutes, and be practiced at least twice a day, but ideally after each meal. Toothbrushing can only clean to a depth of about 1.5 mm inside the gingival pockets, but a sustained regime of plaque removal above the gum line can affect the ecology of the microbes below the gums and may reduce the number of pathogens in pockets up to 5 mm in depth. Toothpaste (dentifrice) with fluoride, or alternatives such as nano-hydroxyapatite, is an important tool to readily use when tooth brushing. The fluoride (or alternatives) in the dentifrice is an important protective factor against caries, and an important supplement needed to remineralize already affected enamel . However, in terms of preventing gum disease, the use of toothpaste does not increase the effectiveness of the activity with respect to the amount of plaque removed. People use toothpaste with nano-hydroxyapatite instead of fluoride as it performs the same function, and some people believe fluoride in toothpaste is a neurotoxin. For maximum benefit, use toothpaste with 1350-1500 ppmF (that's concentration of fluoride in parts per million) to prevent tooth decay. At night, you produce less saliva than during the day. Because of this, your teeth have less protection from saliva and are more vulnerable to acid attacks. That's why it's important to remove food from your teeth before bed so plaque bacteria can't feast overnight. Don't eat or drink anything except water after brushing at night. This also gives fluoride the longest opportunity to work. How to brush your teeth \u2691 The procedure I'm using right now is: Wet the brush but don't add any toothpaste. Slowly and systematically guide the bristle of the electric brush from tooth to tooth, following the contour of the gums and their crowns, remembering to massage the gums. For example, start with the upper left part on the outside, reach the other side of your mouth, clean the bottom of your right side teeth, then go back on the inside of each teeth until you arrive to your left side. Try to avoid brushing with too much force as this can damage the surface of your teeth. Rinse with water. Place a pea sized amount of toothpaste on the brush and repeat the cycle. Brush your tongue. Spit the extra toothpaste but don't rinse or drink anything in the next 30 minutes. The whole process should take at least two minutes. Manual versus electric tooth brush \u2691 If you want to use a manual one, Oral health professionals recommend the use of a tooth brush with a small head and soft bristles as they are most effective in removing plaque without damaging the gums. The technique is crucial to the effectiveness of tooth brushing and disease prevention. Back and forth brushing is not effective in removing plaque at the gum line. Tooth brushing should employ a systematic approach, angle the bristles at a 45-degree angle towards the gums, and make small circular motions at that angle. When using an electric one, the bristle head should be guided from tooth to tooth slowly, following the contour of the gums and crowns of the tooth. The motion of the toothbrush head removes the need to manually oscillate the brush or make circles. Another study suggest that the effectiveness of electric toothbrushes at reducing plaque formation and gingivitis is superior to conventional manual toothbrushes. Regardless of the type, you are always best using a soft-bristled toothbrush with a small head and a flexible neck because this will most effectively remove plaque and debris from your teeth, without damaging your teeth and gums and drawing blood. Toothbrush replacement \u2691 Remember to replace your brush at the first sign of wear-and-tear or every three months, whichever comes first. Frayed or broken bristles won't clean your mouth properly. Change your toothbrush once the bristles lose their flexibility. 1 Also, after a couple of months of daily use, bacteria and food particles begin to accumulate on the toothbrush. To rinse or not to rinse \u2691 There is a lot of controversy on the topic on whether you should rinse or not your mouth after brushing your teeth. ( 1 , 2 ) People in favor of rinsing your mouth argue that: You\u2019ll get rid of the excess toothpaste along with any food or bacteria that could have been stuck in your teeth or released by the brushing itself. You\u2019ll also be removing the fluoride from your mouth, which if swallowed, might upset your stomach. People against rinsing your mouth argue that: When you rinse with water, you\u2019re potentially washing away any remnants of toothpaste, including the fluoride that makes it work. That could mean that even though you are brushing your teeth, it might not be as effective as it should be. Whilst there have been studies on the effectiveness of rinsing, the results only indicate that there COULD be an advantage of one over the other. So it's up to you to evaluate the advantages and disadvantages of each method. Some people are prone to cavities, or might have poor dental health. If your teeth chip, crack or break easily, it\u2019s strongly recommended that you don't rinse after you brush. Similarly, if you consume a lot of sugar, you should probably avoid rinsing. If you don't fit into these categories, then it\u2019s really based on your own preference. Keep your brush away from your feces \u2691 As the Mythbusters showed , Fecal coliform were found on toothbrushes stored at the bathroom. And even though none were of a level high enough to be dangerous, and experts confirm that such bacteria are impossible to completely avoid, you can reduce the risk by: Storing the brush in a cupboard or in other room. Putting a lid on your toothbrush Closing the lid on your toilet seat before flushing. How to floss \u2691 Tooth brushing alone will not remove plaque from all surfaces of the tooth as 40% of the surfaces are interdental. One technique that can be used to access these areas is dental floss. When the proper technique is used, flossing can remove plaque and food particles from between the teeth and below the gums. The American Dental Association (ADA) reports that up to 80% of plaque may be removed by this method. The ADA recommends cleaning between the teeth as part of one's daily oral hygiene regime, with a different piece of floss at each flossing session. The correct technique to ensure maximum plaque removal is as follows: ( 1 , 2 ) Floss length: 15\u201325 cm wrapped around middle fingers. For upper teeth grasp the floss with thumb and index finger, for lower teeth with both index fingers. Ensure that a length of roughly 2.5cm is left between the fingers. Ease the floss gently between the teeth using a back and forth motion. Do not snap the floss into the gums. When the floss reaches your gumline, curve it into a C-shape against a tooth until you feel resistance. Hold the floss against the tooth. Gently scrape the side of the tooth, moving the floss away from the gum. Repeat on the other side of the gap, along the side of the next tooth. Do not forget the back of your last tooth. Ensure that the floss is taken below the gum margins using a back and forth up and down motion. You should floss before brushing your teeth because any food, plaque, and bacteria released by flossing are removed by the afterwards brushing. Another tips regarding flossing are : Skip the toothpick: Use floss instead of a toothpick to remove food stuck in between your teeth. Using a toothpick can damage your gums and lead to an infection. Be gentle: Don't be too aggressive when flossing to avoid bleeding gums. When you first start flossing, your gums may be tender and bleed a little. Carry on flossing your teeth and the bleeding should stop as your gums become healthier. If you're still getting regular bleeding after a few days, see your dental team. They can check if you're flossing correctly. Mouth washing \u2691 Using a mouthwash that contains fluoride can help prevent tooth decay, but don't use mouthwash (even a fluoride one) straight after brushing your teeth or it'll wash away the concentrated fluoride in the toothpaste left on your teeth. [ 1 ] Choose a different time to use mouthwash, such as after lunch. And remember not to eat or drink for 30 minutes after using a fluoride mouthwash. Do yearly check ups \u2691 First find a dentist that you trust, until you do, search for second and third options before diving into anything you may regret. Once you have it, yearly go to their dreaded places so they can: Check that everything is alright. Do a regular clean, but beware of unnecessary deep cleaning . References \u2691 Wikipedia oral hygiene article CNN health article on oral hygiene","title":"Teeth"},{"location":"teeth/#how-to-take-care-of-your-teeth","text":"TL;DR: Daily actions to keep your teeth healty Brush your teeth after every meal for at least two minutes. Floss each day before the last teeth brush. Use an electric toothbrush . Replace the brush each three months or at first sign of wear and tear. Don't eat or drink anything but water after your nightly brush. Do not rinse after you brush your teeth . Use floss instead of a toothpick . Use mouthwash daily but not after brushing . Oral hygiene is the practice of keeping one's mouth clean and free of disease and other problems (e.g. bad breath) by regular brushing of the teeth (dental hygiene) and cleaning between the teeth. It is important that oral hygiene be carried out on a regular basis to enable prevention of dental disease and bad breath. The most common types of dental disease are tooth decay (cavities, dental caries) and gum diseases, including gingivitis , and periodontitis . General guidelines suggest brushing twice a day: after breakfast and before going to bed, but ideally the mouth would be cleaned after every meal. Cleaning between the teeth is called interdental cleaning and is as important as tooth brushing. This is because a toothbrush cannot reach between the teeth and therefore only removes about 50% of plaque from the surface of the teeth. There are many tools to clean between the teeth, including floss and interdental brushes; it is up to each individual to choose which tool they prefer to use. Over 80% of cavities occur inside fissures in teeth where brushing cannot reach food left trapped after eating and saliva and fluoride have no access to neutralize acid and remineralize demineralized teeth, unlike easy-to-clean parts of the tooth, where fewer cavities occur.","title":"How to take care of your teeth"},{"location":"teeth/#teeth-brushing","text":"Routine tooth brushing is the principal method of preventing many oral diseases, and perhaps the most important activity an individual can practice to reduce dental plaque and tartar . The dental plaque contains a mixture of bacteria, their acids and sticky byproducts and food remnants. It forms naturally on teeth immediately after you've eaten but doesn't get nasty and start to cause damage to the teeth until it reaches a certain stage of maturity. The exact amount of time this takes isn't known but is at least more than 12 hours. Bacteria consume sugar and, as a byproduct, produce acids which dissolve mineral out of the teeth, leaving microscopic holes we can't see. If the process isn't stopped and they aren't repaired, these can become big, visible cavities. So controlling plaque reduces the risk of the individual suffering from plaque-associated diseases such as gingivitis, periodontitis, and caries. Many oral health care professionals agree that tooth brushing should be done for a minimum of two minutes, and be practiced at least twice a day, but ideally after each meal. Toothbrushing can only clean to a depth of about 1.5 mm inside the gingival pockets, but a sustained regime of plaque removal above the gum line can affect the ecology of the microbes below the gums and may reduce the number of pathogens in pockets up to 5 mm in depth. Toothpaste (dentifrice) with fluoride, or alternatives such as nano-hydroxyapatite, is an important tool to readily use when tooth brushing. The fluoride (or alternatives) in the dentifrice is an important protective factor against caries, and an important supplement needed to remineralize already affected enamel . However, in terms of preventing gum disease, the use of toothpaste does not increase the effectiveness of the activity with respect to the amount of plaque removed. People use toothpaste with nano-hydroxyapatite instead of fluoride as it performs the same function, and some people believe fluoride in toothpaste is a neurotoxin. For maximum benefit, use toothpaste with 1350-1500 ppmF (that's concentration of fluoride in parts per million) to prevent tooth decay. At night, you produce less saliva than during the day. Because of this, your teeth have less protection from saliva and are more vulnerable to acid attacks. That's why it's important to remove food from your teeth before bed so plaque bacteria can't feast overnight. Don't eat or drink anything except water after brushing at night. This also gives fluoride the longest opportunity to work.","title":"Teeth brushing"},{"location":"teeth/#how-to-brush-your-teeth","text":"The procedure I'm using right now is: Wet the brush but don't add any toothpaste. Slowly and systematically guide the bristle of the electric brush from tooth to tooth, following the contour of the gums and their crowns, remembering to massage the gums. For example, start with the upper left part on the outside, reach the other side of your mouth, clean the bottom of your right side teeth, then go back on the inside of each teeth until you arrive to your left side. Try to avoid brushing with too much force as this can damage the surface of your teeth. Rinse with water. Place a pea sized amount of toothpaste on the brush and repeat the cycle. Brush your tongue. Spit the extra toothpaste but don't rinse or drink anything in the next 30 minutes. The whole process should take at least two minutes.","title":"How to brush your teeth"},{"location":"teeth/#manual-versus-electric-tooth-brush","text":"If you want to use a manual one, Oral health professionals recommend the use of a tooth brush with a small head and soft bristles as they are most effective in removing plaque without damaging the gums. The technique is crucial to the effectiveness of tooth brushing and disease prevention. Back and forth brushing is not effective in removing plaque at the gum line. Tooth brushing should employ a systematic approach, angle the bristles at a 45-degree angle towards the gums, and make small circular motions at that angle. When using an electric one, the bristle head should be guided from tooth to tooth slowly, following the contour of the gums and crowns of the tooth. The motion of the toothbrush head removes the need to manually oscillate the brush or make circles. Another study suggest that the effectiveness of electric toothbrushes at reducing plaque formation and gingivitis is superior to conventional manual toothbrushes. Regardless of the type, you are always best using a soft-bristled toothbrush with a small head and a flexible neck because this will most effectively remove plaque and debris from your teeth, without damaging your teeth and gums and drawing blood.","title":"Manual versus electric tooth brush"},{"location":"teeth/#toothbrush-replacement","text":"Remember to replace your brush at the first sign of wear-and-tear or every three months, whichever comes first. Frayed or broken bristles won't clean your mouth properly. Change your toothbrush once the bristles lose their flexibility. 1 Also, after a couple of months of daily use, bacteria and food particles begin to accumulate on the toothbrush.","title":"Toothbrush replacement"},{"location":"teeth/#to-rinse-or-not-to-rinse","text":"There is a lot of controversy on the topic on whether you should rinse or not your mouth after brushing your teeth. ( 1 , 2 ) People in favor of rinsing your mouth argue that: You\u2019ll get rid of the excess toothpaste along with any food or bacteria that could have been stuck in your teeth or released by the brushing itself. You\u2019ll also be removing the fluoride from your mouth, which if swallowed, might upset your stomach. People against rinsing your mouth argue that: When you rinse with water, you\u2019re potentially washing away any remnants of toothpaste, including the fluoride that makes it work. That could mean that even though you are brushing your teeth, it might not be as effective as it should be. Whilst there have been studies on the effectiveness of rinsing, the results only indicate that there COULD be an advantage of one over the other. So it's up to you to evaluate the advantages and disadvantages of each method. Some people are prone to cavities, or might have poor dental health. If your teeth chip, crack or break easily, it\u2019s strongly recommended that you don't rinse after you brush. Similarly, if you consume a lot of sugar, you should probably avoid rinsing. If you don't fit into these categories, then it\u2019s really based on your own preference.","title":"To rinse or not to rinse"},{"location":"teeth/#keep-your-brush-away-from-your-feces","text":"As the Mythbusters showed , Fecal coliform were found on toothbrushes stored at the bathroom. And even though none were of a level high enough to be dangerous, and experts confirm that such bacteria are impossible to completely avoid, you can reduce the risk by: Storing the brush in a cupboard or in other room. Putting a lid on your toothbrush Closing the lid on your toilet seat before flushing.","title":"Keep your brush away from your feces"},{"location":"teeth/#how-to-floss","text":"Tooth brushing alone will not remove plaque from all surfaces of the tooth as 40% of the surfaces are interdental. One technique that can be used to access these areas is dental floss. When the proper technique is used, flossing can remove plaque and food particles from between the teeth and below the gums. The American Dental Association (ADA) reports that up to 80% of plaque may be removed by this method. The ADA recommends cleaning between the teeth as part of one's daily oral hygiene regime, with a different piece of floss at each flossing session. The correct technique to ensure maximum plaque removal is as follows: ( 1 , 2 ) Floss length: 15\u201325 cm wrapped around middle fingers. For upper teeth grasp the floss with thumb and index finger, for lower teeth with both index fingers. Ensure that a length of roughly 2.5cm is left between the fingers. Ease the floss gently between the teeth using a back and forth motion. Do not snap the floss into the gums. When the floss reaches your gumline, curve it into a C-shape against a tooth until you feel resistance. Hold the floss against the tooth. Gently scrape the side of the tooth, moving the floss away from the gum. Repeat on the other side of the gap, along the side of the next tooth. Do not forget the back of your last tooth. Ensure that the floss is taken below the gum margins using a back and forth up and down motion. You should floss before brushing your teeth because any food, plaque, and bacteria released by flossing are removed by the afterwards brushing. Another tips regarding flossing are : Skip the toothpick: Use floss instead of a toothpick to remove food stuck in between your teeth. Using a toothpick can damage your gums and lead to an infection. Be gentle: Don't be too aggressive when flossing to avoid bleeding gums. When you first start flossing, your gums may be tender and bleed a little. Carry on flossing your teeth and the bleeding should stop as your gums become healthier. If you're still getting regular bleeding after a few days, see your dental team. They can check if you're flossing correctly.","title":"How to floss"},{"location":"teeth/#mouth-washing","text":"Using a mouthwash that contains fluoride can help prevent tooth decay, but don't use mouthwash (even a fluoride one) straight after brushing your teeth or it'll wash away the concentrated fluoride in the toothpaste left on your teeth. [ 1 ] Choose a different time to use mouthwash, such as after lunch. And remember not to eat or drink for 30 minutes after using a fluoride mouthwash.","title":"Mouth washing"},{"location":"teeth/#do-yearly-check-ups","text":"First find a dentist that you trust, until you do, search for second and third options before diving into anything you may regret. Once you have it, yearly go to their dreaded places so they can: Check that everything is alright. Do a regular clean, but beware of unnecessary deep cleaning .","title":"Do yearly check ups"},{"location":"teeth/#references","text":"Wikipedia oral hygiene article CNN health article on oral hygiene","title":"References"},{"location":"teeth_deep_cleaning/","text":"TL;DR: Ask the opinion of two or three independent dentists before doing a deep clean. Scaling and root planing , also known as conventional periodontal therapy, non-surgical periodontal therapy or deep cleaning, is a procedure involving removal of dental plaque and calculus (scaling or debridement) and then smoothing, or planing, of the (exposed) surfaces of the roots, removing cementum or dentine that is impregnated with calculus, toxins, or microorganisms, the etiologic agents that cause inflammation. It is a part of non-surgical periodontal therapy. This helps to establish a periodontium that is in remission of periodontal disease. As to the frequency of cleaning, research on this matter is inconclusive. That is, it has neither been shown that more frequent cleaning leads to better outcomes nor that it does not. Thus, any general recommendation for a frequency of routine cleaning (e.g. every six months, every year) has no empirical basis. ( 1 ) Why do we need deep cleaning \u2691 We all have a plethora of bacteria in our mouths. Those bacteria mix with other substances to form sticky plaque on teeth, which is mostly banished by regular brushing and flossing. Plaques that don't get brushed away can harden and form a substance known as tartar, which can only be removed with a dental cleaning. When tartar remains on the teeth, it can cause inflammation of the gums, a condition called gingivitis, characterized by red swollen gums that can bleed easily. A mild form of gum disease, gingivitis can usually be reversed through regular brushing and flossing along with cleanings by a dentist or hygienist. If gingivitis isn't cured, it can advance to a more severe form of gum disease called periodontitis, in which the inflamed tissue begins to pull away from the teeth, forming spaces, or pockets. As the pockets become deeper, more of the tooth below the gum line is exposed to bacteria, which can damage the bone holding teeth in place. Eventually, if the pockets become deep enough, teeth can become loose and may even be lost. Dentists measure the depth of the pockets with a probe that has a tiny ruler on the end. Healthy gums have pockets that measure no more than 3 mm \u2014 or a little less than a tenth of an inch \u2014 deep. More than that and you\u2019re getting into trouble. One way to slow or halt the process is through deep cleaning, which removes the plaque below the gum line and smooths rough spots on the tooth root, making it harder for bacteria to accumulate there. Signs of periodontitis \u2691 Red or swollen gums Tender or bleeding gums Persistent bad breath Your teeth look like they\u2019ve been getting longer as gums recede. Teeth that are sensitive Loose teeth Pain when chewing Evidence-based dentistry \u2691 Several systematic reviews have been made of the effectiveness of scaling and root planing as evidence-based dentistry. A Cochrane review by Worthington et al. in 2013 considered only scaling and polishing of the teeth, but not root planing. After examining 88 papers they found only three studies that met all their requirements, remarking that \"the quality of the evidence was generally low.\" An extensive review that did involve root planing was published by the Canadian Agency for Drugs and Technologies in Health in 2016 . It made a number of findings, including (1) In five randomized controlled trials, scaling and root planing \"was associated with a decrease in plaque from baseline at one month, three months, or six months;\" and (2) Four studies analyzed changes in the gingival index (GI) from the baseline and \"found a significant improvement from baseline in the scaling and root planing group at three months and six months.\" This study also discussed evidence-based guidelines for frequency of scaling with and without root planing for patients both with and without chronic periodontitis. The group that produced one of the main systematic reviews used in the 2016 Canadian review has published guidelines based on its findings. They recommend that scaling and root planing (SRP) should be considered as the initial treatment for patients with chronic periodontitis. They note that \"the strength of the recommendation is limited because SRP is considered the reference standard and thus used as an active control for periodontal trials and there are few studies in which investigators compare SRP with no treatment.\" They add however that \"root planing ... carries the risk of damaging the root surface and potentially causing tooth or root sensitivity. Generally expected post-SRP procedural adverse effects include discomfort.\" Enamel cracks, early caries and resin restorations can be damaged during scaling. Effectiveness of the procedure \u2691 A scaling and root planing procedure is to be considered effective if the patient is subsequently able to maintain their periodontal health without further bone or attachment loss and if it prevents recurrent infection with periodontal pathogens. The long term effectiveness of scaling and root planing depends upon a number of factors. These factors include patient compliance, disease progress at the time of intervention, probing depth, and anatomical factors like grooves in the roots of teeth, concavities, and furcation involvement which may limit visibility of underlying deep calculus and debris. First and foremost, periodontal scaling and root planing is a procedure that must be done thoroughly and with attention to detail in order to ensure complete removal of all calculus and plaque from involved sites. If these causative agents are not removed, the disease will continue to progress and further damage will result. In cases of mild to moderate periodontitis, scaling and root planing can achieve excellent results if the procedure is thorough. As periodontitis increases in severity, a greater amount of supporting bone is destroyed by the infection. This is illustrated clinically by the deepening of the periodontal pockets targeted for cleaning and disinfection during the procedure. Once the periodontal pockets exceed 6 mm in depth, the effectiveness of deposit removal begins to decrease, and the likelihood of complete healing after one procedure begins to decline as well. The more severe the infection prior to intervention, the greater the effort required to arrest its progress and return the patient to health. Diseased pockets over 6 mm can be resolved through periodontal flap surgery. Although healing of the soft tissues will begin immediately following removal of the microbial biofilm and calculus that cause the disease, scaling and root planing is only the first step in arresting the disease process. Following initial cleaning and disinfection of all affected sites, it is necessary to prevent the infection from recurring. Therefore, patient compliance is, by far, the most important factor, having the greatest influence on the success or failure of periodontal intervention. Immediately following treatment, the patient will need to maintain excellent oral care at home. With proper homecare, which includes but is by no means limited to brushing twice daily for 2\u20133 minutes, flossing daily and use of mouthrinse, the potential for effective healing following scaling and root planing increases. Commitment to and diligence in the thorough completion of daily oral hygiene practices are essential to this success. The process which allows for the formation of deep periodontal pockets does not occur overnight. Therefore, it is unrealistic to expect the tissue to heal completely in a similarly short time period. Gains in gingival attachment may occur slowly over time, and ongoing periodontal maintenance visits are usually recommended (by some sources). Side effects \u2691 The process carries it's risks, such as: Pop out a filling Gums damage in an irreversible way. End up with an abscess if a tiny piece of tartar is knocked loose and becomes trapped. Have more sensitivity after the procedure. Conclusion \u2691 Deep cleaning is an invasive procedure There is a lack of scientific studies supporting the frequency of it's application, specially for people that doesn't suffer from periodontitis. It's an expensive procedure. So, ask the opinion of two or three independent dentists before doing a deep clean.","title":"Deep cleaning"},{"location":"teeth_deep_cleaning/#why-do-we-need-deep-cleaning","text":"We all have a plethora of bacteria in our mouths. Those bacteria mix with other substances to form sticky plaque on teeth, which is mostly banished by regular brushing and flossing. Plaques that don't get brushed away can harden and form a substance known as tartar, which can only be removed with a dental cleaning. When tartar remains on the teeth, it can cause inflammation of the gums, a condition called gingivitis, characterized by red swollen gums that can bleed easily. A mild form of gum disease, gingivitis can usually be reversed through regular brushing and flossing along with cleanings by a dentist or hygienist. If gingivitis isn't cured, it can advance to a more severe form of gum disease called periodontitis, in which the inflamed tissue begins to pull away from the teeth, forming spaces, or pockets. As the pockets become deeper, more of the tooth below the gum line is exposed to bacteria, which can damage the bone holding teeth in place. Eventually, if the pockets become deep enough, teeth can become loose and may even be lost. Dentists measure the depth of the pockets with a probe that has a tiny ruler on the end. Healthy gums have pockets that measure no more than 3 mm \u2014 or a little less than a tenth of an inch \u2014 deep. More than that and you\u2019re getting into trouble. One way to slow or halt the process is through deep cleaning, which removes the plaque below the gum line and smooths rough spots on the tooth root, making it harder for bacteria to accumulate there.","title":"Why do we need deep cleaning"},{"location":"teeth_deep_cleaning/#signs-of-periodontitis","text":"Red or swollen gums Tender or bleeding gums Persistent bad breath Your teeth look like they\u2019ve been getting longer as gums recede. Teeth that are sensitive Loose teeth Pain when chewing","title":"Signs of periodontitis"},{"location":"teeth_deep_cleaning/#evidence-based-dentistry","text":"Several systematic reviews have been made of the effectiveness of scaling and root planing as evidence-based dentistry. A Cochrane review by Worthington et al. in 2013 considered only scaling and polishing of the teeth, but not root planing. After examining 88 papers they found only three studies that met all their requirements, remarking that \"the quality of the evidence was generally low.\" An extensive review that did involve root planing was published by the Canadian Agency for Drugs and Technologies in Health in 2016 . It made a number of findings, including (1) In five randomized controlled trials, scaling and root planing \"was associated with a decrease in plaque from baseline at one month, three months, or six months;\" and (2) Four studies analyzed changes in the gingival index (GI) from the baseline and \"found a significant improvement from baseline in the scaling and root planing group at three months and six months.\" This study also discussed evidence-based guidelines for frequency of scaling with and without root planing for patients both with and without chronic periodontitis. The group that produced one of the main systematic reviews used in the 2016 Canadian review has published guidelines based on its findings. They recommend that scaling and root planing (SRP) should be considered as the initial treatment for patients with chronic periodontitis. They note that \"the strength of the recommendation is limited because SRP is considered the reference standard and thus used as an active control for periodontal trials and there are few studies in which investigators compare SRP with no treatment.\" They add however that \"root planing ... carries the risk of damaging the root surface and potentially causing tooth or root sensitivity. Generally expected post-SRP procedural adverse effects include discomfort.\" Enamel cracks, early caries and resin restorations can be damaged during scaling.","title":"Evidence-based dentistry"},{"location":"teeth_deep_cleaning/#effectiveness-of-the-procedure","text":"A scaling and root planing procedure is to be considered effective if the patient is subsequently able to maintain their periodontal health without further bone or attachment loss and if it prevents recurrent infection with periodontal pathogens. The long term effectiveness of scaling and root planing depends upon a number of factors. These factors include patient compliance, disease progress at the time of intervention, probing depth, and anatomical factors like grooves in the roots of teeth, concavities, and furcation involvement which may limit visibility of underlying deep calculus and debris. First and foremost, periodontal scaling and root planing is a procedure that must be done thoroughly and with attention to detail in order to ensure complete removal of all calculus and plaque from involved sites. If these causative agents are not removed, the disease will continue to progress and further damage will result. In cases of mild to moderate periodontitis, scaling and root planing can achieve excellent results if the procedure is thorough. As periodontitis increases in severity, a greater amount of supporting bone is destroyed by the infection. This is illustrated clinically by the deepening of the periodontal pockets targeted for cleaning and disinfection during the procedure. Once the periodontal pockets exceed 6 mm in depth, the effectiveness of deposit removal begins to decrease, and the likelihood of complete healing after one procedure begins to decline as well. The more severe the infection prior to intervention, the greater the effort required to arrest its progress and return the patient to health. Diseased pockets over 6 mm can be resolved through periodontal flap surgery. Although healing of the soft tissues will begin immediately following removal of the microbial biofilm and calculus that cause the disease, scaling and root planing is only the first step in arresting the disease process. Following initial cleaning and disinfection of all affected sites, it is necessary to prevent the infection from recurring. Therefore, patient compliance is, by far, the most important factor, having the greatest influence on the success or failure of periodontal intervention. Immediately following treatment, the patient will need to maintain excellent oral care at home. With proper homecare, which includes but is by no means limited to brushing twice daily for 2\u20133 minutes, flossing daily and use of mouthrinse, the potential for effective healing following scaling and root planing increases. Commitment to and diligence in the thorough completion of daily oral hygiene practices are essential to this success. The process which allows for the formation of deep periodontal pockets does not occur overnight. Therefore, it is unrealistic to expect the tissue to heal completely in a similarly short time period. Gains in gingival attachment may occur slowly over time, and ongoing periodontal maintenance visits are usually recommended (by some sources).","title":"Effectiveness of the procedure"},{"location":"teeth_deep_cleaning/#side-effects","text":"The process carries it's risks, such as: Pop out a filling Gums damage in an irreversible way. End up with an abscess if a tiny piece of tartar is knocked loose and becomes trapped. Have more sensitivity after the procedure.","title":"Side effects"},{"location":"teeth_deep_cleaning/#conclusion","text":"Deep cleaning is an invasive procedure There is a lack of scientific studies supporting the frequency of it's application, specially for people that doesn't suffer from periodontitis. It's an expensive procedure. So, ask the opinion of two or three independent dentists before doing a deep clean.","title":"Conclusion"},{"location":"terraform/","text":"Terraform is an open-source infrastructure as code software tool created by HashiCorp. It enables users to define and provision a datacenter infrastructure using an awful high-level configuration language known as Hashicorp Configuration Language (HCL), or optionally JSON. Terraform supports a number of cloud infrastructure providers such as Amazon Web Services, IBM Cloud , Google Cloud Platform, DigitalOcean, Linode, Microsoft Azure, Oracle Cloud Infrastructure, OVH, or VMware vSphere as well as OpenNebula and OpenStack. Tools \u2691 tfschema : A binary that allows you to see the attributes of the resources of the different providers. There are some times that there are complex attributes that aren't shown on the docs with an example. Here you'll see them clearly. tfschema resource list aws | grep aws_iam_user > aws_iam_user > aws_iam_user_group_membership > aws_iam_user_login_profile > aws_iam_user_policy > aws_iam_user_policy_attachment > aws_iam_user_ssh_key tfschema resource show aws_iam_user +----------------------+-------------+----------+----------+----------+-----------+ | ATTRIBUTE | TYPE | REQUIRED | OPTIONAL | COMPUTED | SENSITIVE | +----------------------+-------------+----------+----------+----------+-----------+ | arn | string | false | false | true | false | | force_destroy | bool | false | true | false | false | | id | string | false | true | true | false | | name | string | true | false | false | false | | path | string | false | true | false | false | | permissions_boundary | string | false | true | false | false | | tags | map ( string ) | false | true | false | false | | unique_id | string | false | false | true | false | +----------------------+-------------+----------+----------+----------+-----------+ # Open the documentation of the resource in the browser tfschema resource browse aws_iam_user terraforming : Tool to export existing resources to terraform terraboard : Web dashboard to visualize and query terraform tfstate, you can search, compare and see the most active ones. There are deployments for k8s. export AWS_ACCESS_KEY_ID = XXXXXXXXXXXXXXXXXXXX export AWS_SECRET_ACCESS_KEY = XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX export AWS_DEFAULT_REGION = eu-west-1 export AWS_BUCKET = terraform-tfstate-20180119 export TERRABOARD_LOG_LEVEL = debug docker network create terranet docker run -ti --rm --name db -e POSTGRES_USER = gorm -e POSTGRES_DB = gorm -e POSTGRES_PASSWORD = \"mypassword\" --net terranet postgres docker run -ti --rm -p 8080 :8080 -e AWS_REGION = \" $AWS_DEFAULT_REGION \" -e AWS_ACCESS_KEY_ID = \" ${ AWS_ACCESS_KEY_ID } \" -e AWS_SECRET_ACCESS_KEY = \" ${ AWS_SECRET_ACCESS_KEY } \" -e AWS_BUCKET = \" $AWS_BUCKET \" -e DB_PASSWORD = \"mypassword\" --net terranet camptocamp/terraboard:latest tfenv: Install different versions of terraform git clone https://github.com/tfutils/tfenv.git ~/.tfenv echo 'export PATH=\"$HOME/.tfenv/bin:$PATH\"' >> ~/.bashrc echo 'export PATH=\"$HOME/.tfenv/bin:$PATH\"' >> ~/.zshrc tfenv list-remote tfenv install 0 .12.8 terraform version tfenv install 0 .11.15 terraform version tfenv use 0 .12.8 terraform version https://github.com/eerkunt/terraform-compliance landscape : A program to modify the plan and show a nicer version, really useful when it's shown as json. Right now it only works for terraform 11. terraform plan | landscape k2tf : Program to convert k8s yaml manifestos to HCL. Editor Plugins \u2691 For Vim : vim-terraform : Execute tf from vim and autoformat when saving. vim-terraform-completion : linter and autocomplete. Good practices and maintaining \u2691 fmt : Formats the code following hashicorp best practices. terraform fmt Validate : Tests that the syntax is correct. terraform validate Documentaci\u00f3n : Generates a table in markdown with the inputs and outputs. terraform-docs markdown table *.tf > README.md ## Inputs | Name | Description | Type | Default | Required | | ------ | ------------- | :----: | :-----: | :-----: | | broker_numbers | Number of brokers | number | ` \"3\" ` | no | | broker_size | AWS instance type for the brokers | string | ` \"kafka.m5.large\" ` | no | | ebs_size | Size of the brokers disks | string | ` \"300\" ` | no | | kafka_version | Kafka version | string | ` \"2.1.0\" ` | no | ## Outputs | Name | Description | | ------ | ------------- | | brokers_masked_endpoints | Zookeeper masked endpoints | | brokers_real_endpoints | Zookeeper real endpoints | | zookeeper_masked_endpoints | Zookeeper masked endpoints | | zookeeper_real_endpoints | Zookeeper real endpoints | Terraform lint ( tflint ): Only works with some AWS resources. It allows the validation against a third party API. For example: resource \"aws_instance\" \"foo\" { ami = \"ami-0ff8a91507f77f867\" instance_type = \"t1.2xlarge\" # invalid type! } The code is valid, but in AWS there doesn't exist the type t1.2xlarge . This test avoids this kind of issues. wget https://github.com/wata727/tflint/releases/download/v0.11.1/tflint_darwin_amd64.zip unzip tflint_darwin_amd64.zip sudo install tflint /usr/local/bin/ tflint -v We can automate all the above to be executed before we do a commit using the pre-commit framework. sudo pip install pre-commit cd $proyectoConTerraform echo \"\"\"repos: - repo: git://github.com/antonbabenko/pre-commit-terraform rev: v1.19.0 hooks: - id: terraform_fmt - id: terraform_validate - id: terraform_docs - id: terraform_tflint \"\"\" > .pre-commit-config.yaml pre-commit install pre-commit run terraform_fmt pre-commit run terraform_validate --file dynamo.tf pre-commit run -a Tests \u2691 Motivation Static analysis \u2691 Linters \u2691 conftest tflint terraform validate Dry run \u2691 terraform plan hashicorp sentinel terraform-compliance Unit tests \u2691 There is no real unit testing in infrastructure code as you need to deploy it in a real environment terratest (works for k8s and terraform) Some sample code in: github.com/gruntwork-io/infrastructure-as-code-testing-talk gruntwork.io E2E test \u2691 Too slow and too brittle to be worth it Use incremental e2e testing Variables \u2691 It's a good practice to name the resource before the particularization of the resource, so you can search all the elements of that resource, for example, instead of client_cidr and operations_cidr use cidr_operations and cidr_client variable \"list_example\"{ description = \"An example of a list\" type = \"list\" default = [ 1 , 2 , 3 ] } variable \"map_example\"{ description = \"An example of a dictionary\" type = \"map\" default = { key1 = \"value1\" key2 = \"value2\" } } For the use of maps inside maps or lists investigate zipmap To access you have to use \"${var.list_example}\" For secret variables we use: variable \"db_password\" { description = \"The password for the database\" } Which has no default value, we save that password in our keystore and pass it as environmental variable export TF_VAR_db_password = \"{{ your password }}\" terragrunt plan As a reminder, Terraform stores all variables in its state file in plain text, including this database password, which is why your terragrunt config should always enable encryption for remote state storage in S3 Interpolation of variables \u2691 You can't interpolate in variables, so instead of variable \"sistemas_gpg\" { description = \"Sistemas public GPG key for Zena\" type = \"string\" default = \"${file(\"sistemas_zena.pub\")}\" } You have to use locals locals { sistemas_gpg = \"${file(\"sistemas_zena.pub\")}\" } \"${local.sistemas_gpg}\" Show information of the resources \u2691 Get information of the infrastructure. Output variables show up in the console after you run terraform apply , you can also use terraform output [{{ output_name }}] to see the value of a specific output without applying any changes output \"public_ip\" { value = \"${aws_instance.example.public_ip}\" } > terraform apply aws_security_group.instance: Refreshing state... ( ID: sg-db91dba1 ) aws_instance.example: Refreshing state... ( ID: i-61744350 ) Apply complete! Resources: 0 added, 0 changed, 0 destroyed. Outputs: public_ip = 54 .174.13.5 Data source \u2691 A data source represents a piece of read-only information that is fetched from the provider every time you run Terraform. It does not create anything new data \"aws_availability_zones\" \"all\" { } And you reference it with \"${data.aws_availability_zones.all.names}\" Read-only state source \u2691 With terraform_remote_state you an fetch the Terraform state file stored by another set of templates in a completely read-only manner. From an app template we can read the info of the ddbb with data \"terraform_remote_state\" \"db\" { backend = \"s3\" config { bucket = \"(YOUR_BUCKET_NAME)\" key = \"stage/data-stores/mysql/terraform.tfstate\" region = \"us-east-1\" } } And you would access the variables inside the database terraform file with data.terraform_remote_state.db.outputs.port To share variables from state, you need to to set them in the outputs.tf file. Template_file source \u2691 It is used to load templates, it has two parameters, template which is a string and vars which is a map of variables. it has one output attribute called rendered , which is the result of rendering template. For example # File: user-data.sh #!/bin/bash cat > index.html <<EOF <h1>Hello, World</h1> <p>DB address: ${db_address}</p> <p>DB port: ${db_port}</p> EOF nohup busybox httpd -f -p \" ${ server_port } \" & data \"template_file\" \"user_data\" { template = \"${file(\"user-data.sh\")}\" vars { server_port = \"${var.server_port}\" db_address = \"${data.terraform_remote_state.db.address}\" db_port = \"${data.terraform_remote_state.db.port}\" } } Resource lifecycle \u2691 The lifecycle parameter is a meta-parameter , it exist on about every resource in Terraform. You can add a lifecycle block to any resource to configure how that resource should be created, updated or destroyed. The available options are: * create_before_destroy : Which if set to true will create a replacement resource before destroying hte original resource * prevent_destroy : If set to true, any attempt to delete that resource ( terraform destroy ), will fail, to delete it you have to first remove the prevent_destroy resource \"aws_launch_configuration\" \"example\" { image_id = \"ami-40d28157\" instance_type = \"t2.micro\" security_groups = [ \"${aws_security_group.instance.id}\" ] user_data = <<-EOF #!/bin/bash echo \"Hello, World\" > index.html nohup busybox httpd -f -p \"${var.server_port}\" & EOF lifecycle { create_before_destroy = true } } If you set the create_before_destroy on a resource, you also have to set it on every resource that X depends on (if you forget, you'll get errors about cyclical dependencies). In the case of the launch configuration, that means you need to set create_before_destroy to true on the security group: resource \"aws_security_group\" \"instance\" { name = \"terraform-example-instance\" ingress { from_port = \"${var.server_port}\" to_port = \"${var.server_port}\" protocol = \"tcp\" cidr_blocks = [ \"0.0.0.0/0\" ] } lifecycle { create_before_destroy = true } } Use collaboratively \u2691 Share state \u2691 The best option is to use S3 as bucket of the config. First create it resource \"aws_s3_bucket\" \"terraform_state\" { bucket = \"terraform-up-and-running-state\" versioning { enabled = true } lifecycle { prevent_destroy = true } } And then configure terraform terraform remote config \\ -backend = s3 \\ -backend-config = \"bucket=(YOUR_BUCKET_NAME)\" \\ -backend-config = \"key=global/s3/terraform.tfstate\" \\ -backend-config = \"region=us-east-1\" \\ -backend-config = \"encrypt=true\" In this way terraform will automatically pull the latest state from this bucked and push the latest state after running a command Lock terraform \u2691 To avoid several people running terraform at the same time, we'd use terragrunt a wrapper for terraform that manages remote state for you automatically and provies locking by using DynamoDB (in the free tier) Inside the terraform_config.tf you create the dynamodb table and then configure your s3 backend to use it resource \"aws_dynamodb_table\" \"terraform_statelock\" { name = \"global-s3\" read_capacity = 20 write_capacity = 20 hash_key = \"LockID\" attribute { name = \"LockID\" type = \"S\" } } terraform { backend \"s3\" { bucket = \"grupo-zena-tfstate\" key = \"global/s3/terraform.tfstate\" region = \"eu-west-1\" encrypt = \"true\" dynamodb_table = \"global-s3\" } } You'll probably need to execute an terraform apply with the dynamodb_table line commented If you want to unforce a lock, execute: terraform force-unlock {{ unlock_id }} You get the unlock_id from an error trying to execute any terraform command Modules \u2691 In terraform you can put code inside of a module and reuse in multiple places throughout your code. The provider resource should be specified by the user and not in the modules Whenever you add a module to your terraform template or modify its source parameter you need to run a get command before you run plan or apply terraform get To extract output variables of a module to the parent tf file you should use ${module.{{module.name}}.{{output_name}}} Basics \u2691 Any set of Terraform templates in a directory is a module. The good practice is to have a directory called modules in your parent project directory. There you git clone the desired modules. and for example inside pro/services/bastion/main.tf you'd call it with: provider \"aws\" { region = \"eu-west-1\" } module \"bastion\" { source = \"../../../modules/services/bastion/\" } Outputs \u2691 Modules encapsulate their resources. A resource in one module cannot directly depend on resources or attributes in other modules, unless those are exported through outputs. These outputs can be referenced in other places in your configuration, for example: resource \"aws_instance\" \"client\" { ami = \"ami-408c7f28\" instance_type = \"t1.micro\" availability_zone = \"${module.consul.server_availability_zone}\" } Import \u2691 You can import the different parts with terraform import {{resource_type}}.{{resource_name}} {{ resource_id }} For examples see the documentation of the desired resource. Bulk import \u2691 But if you want to bulk import sources, I suggest using terraforming Bad points \u2691 Manually added resources wont be managed by terraform, therefore you can't use it to enforce as shown in this bug . If you modify the LC of an ASG, the instances don't get rolling updated, you have to do it manually. They call the dictionaries map ... (/\uff9f\u0414\uff9f)/ The conditionals are really ugly. You need to use count . You can't split long strings xD Best practices \u2691 Name the resources with _ instead of - so the editor's completion work :) VPC \u2691 Don't use the default vpc Security groups \u2691 Instead of using aws_security_group to define the ingress and egress rules, use it only to create the empty security group and use aws_security_group_rule to add the rules, otherwise you'll get into a cycle loop The sintaxis of an egress security group must be egress_from_{{source}}_to_destination . The sintaxis of an ingress security group must be ingress_to_{{destination}}_from_{{source}} Also set the order of the arguments, so they look like the name. For ingress rule: security_group_id = ... cidr_blocks = ... And in egress should look like: security_group_id = ... cidr_blocks = ... Imagine you want to filter the traffic from A -> B, the egress rule from A to B should go besides the ingress rule from B to A. Default security group \u2691 You can't manage the default security group of an vpc, therefore you have to adopt it and set it to no rules at all with aws_default_security_group resource IAM \u2691 You have to generate an gpg key and export it in base64 gpg --export {{ gpg_id }} | base64 To see the secrets you have to decrypt it terraform output secret | base64 --decode | gpg -d Sensitive information \u2691 There are several approaches here. First rely on the S3 encryption to protect the information in your state file Second use Vault provider to protect the state file. Third (but I won't use it) would be to use terrahelp RDS credentials \u2691 The RDS credentials are saved in plaintext both in the definition and in the state file, see this bug for more information. The value of password is not compared against the value of the password in the cloud, so as long as the string in the code and the state file remains the same, it won't try to change it. As a workaround, you can create the RDS with a fake password changeme , and once the resource is created, run an aws command to change it. That way, the value in your code and the state is not the real one, but it won't try to change it. Inspired in this gist and the local-exec docs, you could do: resource \"aws_db_instance\" \"main\" { username = \"postgres\" password = \"changeme\" ... } resource \"null_resource\" \"master_password\" { triggers { db_host = aws_db_instance.main.address } provisioner \"local-exec\" { command = \"pass generate rds_main_password; aws rds modify-db-instance --db-instance-identifier $INSTANCE --master-user-password $(pass show rds_main_password)\" environment = { INSTANCE = aws_db_instance.main.identifier } } } Where the password is stored in your pass repository that can be shared with the team. If you're wondering why I added such a long line, well it's because of HCL! as you can't split long strings , marvelous isn't it? xD Loops \u2691 You can't use nested lists or dictionaries, see this 2015 bug Loop over a variable \u2691 variable \"vpn_egress_tcp_ports\" { description = \"VPN egress tcp ports \" type = \"list\" default = [50, 51, 500, 4500] } resource \"aws_security_group_rule\" \"ingress_tcp_from_ops_to_vpn_instance\"{ count = \"${length(var.vpn_egress_tcp_ports)}\" type = \"ingress\" from_port = \"${element(var.vpn_egress_tcp_ports, count.index)}\" to_port = \"${element(var.vpn_egress_tcp_ports, count.index)}\" protocol = \"tcp\" cidr_blocks = [ \"${var.cidr}\"] security_group_id = \"${aws_security_group.pro_ins_vpn.id}\" } Refactoring \u2691 Refactoring in terraform is ugly business Refactoring in modules \u2691 If you try to refactor your terraform state into modules it will try to destroy and recreate all the elements of the module... Refactoring the state file \u2691 terraform state mv -state-out = other.tfstate module.web module.web Google cloud integration \u2691 You configure it in the terraform directory // Configure the Google Cloud provider provider \"google\" { credentials = \"${file(\"account.json\")}\" project = \"my-gce-project\" region = \"us-central1\" } To download the json go to the Google Developers Console . Go to Credentials then Create credentials and finally Service account key . Select Compute engine default service account and select JSON as the key type. Ignore the change of an attribute \u2691 Sometimes you don't care whether some attributes of a resource change, if that's the case use the lifecycle statement: resource \"aws_instance\" \"example\" { # ... lifecycle { ignore_changes = [ # Ignore changes to tags, e.g. because a management agent # updates these based on some ruleset managed elsewhere. tags, ] } } Define the default value of an variable that contains an object as empty \u2691 variable \"database\" { type = object({ size = number instance_type = string storage_type = string engine = string engine_version = string parameter_group_name = string multi_az = bool }) default = null Conditionals \u2691 Elif \u2691 locals { test = \"${ condition ? value : (elif-condition ? elif-value : else-value)}\" } Do a conditional if a variable is not null \u2691 resource \"aws_db_instance\" \"instance\" { count = var.database == null ? 0 : 1 ... Debugging \u2691 You can set the TF_LOG environmental variable to one of the log levels TRACE , DEBUG , INFO , WARN or ERROR to change the verbosity of the logs. To remove the debug traces run unset TF_LOG . References \u2691 Docs Modules registry terraform-aws-modules AWS providers AWS examples GCloud examples Good and bad sides of terraform Awesome Terraform","title":"Terraform"},{"location":"terraform/#tools","text":"tfschema : A binary that allows you to see the attributes of the resources of the different providers. There are some times that there are complex attributes that aren't shown on the docs with an example. Here you'll see them clearly. tfschema resource list aws | grep aws_iam_user > aws_iam_user > aws_iam_user_group_membership > aws_iam_user_login_profile > aws_iam_user_policy > aws_iam_user_policy_attachment > aws_iam_user_ssh_key tfschema resource show aws_iam_user +----------------------+-------------+----------+----------+----------+-----------+ | ATTRIBUTE | TYPE | REQUIRED | OPTIONAL | COMPUTED | SENSITIVE | +----------------------+-------------+----------+----------+----------+-----------+ | arn | string | false | false | true | false | | force_destroy | bool | false | true | false | false | | id | string | false | true | true | false | | name | string | true | false | false | false | | path | string | false | true | false | false | | permissions_boundary | string | false | true | false | false | | tags | map ( string ) | false | true | false | false | | unique_id | string | false | false | true | false | +----------------------+-------------+----------+----------+----------+-----------+ # Open the documentation of the resource in the browser tfschema resource browse aws_iam_user terraforming : Tool to export existing resources to terraform terraboard : Web dashboard to visualize and query terraform tfstate, you can search, compare and see the most active ones. There are deployments for k8s. export AWS_ACCESS_KEY_ID = XXXXXXXXXXXXXXXXXXXX export AWS_SECRET_ACCESS_KEY = XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX export AWS_DEFAULT_REGION = eu-west-1 export AWS_BUCKET = terraform-tfstate-20180119 export TERRABOARD_LOG_LEVEL = debug docker network create terranet docker run -ti --rm --name db -e POSTGRES_USER = gorm -e POSTGRES_DB = gorm -e POSTGRES_PASSWORD = \"mypassword\" --net terranet postgres docker run -ti --rm -p 8080 :8080 -e AWS_REGION = \" $AWS_DEFAULT_REGION \" -e AWS_ACCESS_KEY_ID = \" ${ AWS_ACCESS_KEY_ID } \" -e AWS_SECRET_ACCESS_KEY = \" ${ AWS_SECRET_ACCESS_KEY } \" -e AWS_BUCKET = \" $AWS_BUCKET \" -e DB_PASSWORD = \"mypassword\" --net terranet camptocamp/terraboard:latest tfenv: Install different versions of terraform git clone https://github.com/tfutils/tfenv.git ~/.tfenv echo 'export PATH=\"$HOME/.tfenv/bin:$PATH\"' >> ~/.bashrc echo 'export PATH=\"$HOME/.tfenv/bin:$PATH\"' >> ~/.zshrc tfenv list-remote tfenv install 0 .12.8 terraform version tfenv install 0 .11.15 terraform version tfenv use 0 .12.8 terraform version https://github.com/eerkunt/terraform-compliance landscape : A program to modify the plan and show a nicer version, really useful when it's shown as json. Right now it only works for terraform 11. terraform plan | landscape k2tf : Program to convert k8s yaml manifestos to HCL.","title":"Tools"},{"location":"terraform/#editor-plugins","text":"For Vim : vim-terraform : Execute tf from vim and autoformat when saving. vim-terraform-completion : linter and autocomplete.","title":"Editor Plugins"},{"location":"terraform/#good-practices-and-maintaining","text":"fmt : Formats the code following hashicorp best practices. terraform fmt Validate : Tests that the syntax is correct. terraform validate Documentaci\u00f3n : Generates a table in markdown with the inputs and outputs. terraform-docs markdown table *.tf > README.md ## Inputs | Name | Description | Type | Default | Required | | ------ | ------------- | :----: | :-----: | :-----: | | broker_numbers | Number of brokers | number | ` \"3\" ` | no | | broker_size | AWS instance type for the brokers | string | ` \"kafka.m5.large\" ` | no | | ebs_size | Size of the brokers disks | string | ` \"300\" ` | no | | kafka_version | Kafka version | string | ` \"2.1.0\" ` | no | ## Outputs | Name | Description | | ------ | ------------- | | brokers_masked_endpoints | Zookeeper masked endpoints | | brokers_real_endpoints | Zookeeper real endpoints | | zookeeper_masked_endpoints | Zookeeper masked endpoints | | zookeeper_real_endpoints | Zookeeper real endpoints | Terraform lint ( tflint ): Only works with some AWS resources. It allows the validation against a third party API. For example: resource \"aws_instance\" \"foo\" { ami = \"ami-0ff8a91507f77f867\" instance_type = \"t1.2xlarge\" # invalid type! } The code is valid, but in AWS there doesn't exist the type t1.2xlarge . This test avoids this kind of issues. wget https://github.com/wata727/tflint/releases/download/v0.11.1/tflint_darwin_amd64.zip unzip tflint_darwin_amd64.zip sudo install tflint /usr/local/bin/ tflint -v We can automate all the above to be executed before we do a commit using the pre-commit framework. sudo pip install pre-commit cd $proyectoConTerraform echo \"\"\"repos: - repo: git://github.com/antonbabenko/pre-commit-terraform rev: v1.19.0 hooks: - id: terraform_fmt - id: terraform_validate - id: terraform_docs - id: terraform_tflint \"\"\" > .pre-commit-config.yaml pre-commit install pre-commit run terraform_fmt pre-commit run terraform_validate --file dynamo.tf pre-commit run -a","title":"Good practices and maintaining"},{"location":"terraform/#tests","text":"Motivation","title":"Tests"},{"location":"terraform/#static-analysis","text":"","title":"Static analysis"},{"location":"terraform/#linters","text":"conftest tflint terraform validate","title":"Linters"},{"location":"terraform/#dry-run","text":"terraform plan hashicorp sentinel terraform-compliance","title":"Dry run"},{"location":"terraform/#unit-tests","text":"There is no real unit testing in infrastructure code as you need to deploy it in a real environment terratest (works for k8s and terraform) Some sample code in: github.com/gruntwork-io/infrastructure-as-code-testing-talk gruntwork.io","title":"Unit tests"},{"location":"terraform/#e2e-test","text":"Too slow and too brittle to be worth it Use incremental e2e testing","title":"E2E test"},{"location":"terraform/#variables","text":"It's a good practice to name the resource before the particularization of the resource, so you can search all the elements of that resource, for example, instead of client_cidr and operations_cidr use cidr_operations and cidr_client variable \"list_example\"{ description = \"An example of a list\" type = \"list\" default = [ 1 , 2 , 3 ] } variable \"map_example\"{ description = \"An example of a dictionary\" type = \"map\" default = { key1 = \"value1\" key2 = \"value2\" } } For the use of maps inside maps or lists investigate zipmap To access you have to use \"${var.list_example}\" For secret variables we use: variable \"db_password\" { description = \"The password for the database\" } Which has no default value, we save that password in our keystore and pass it as environmental variable export TF_VAR_db_password = \"{{ your password }}\" terragrunt plan As a reminder, Terraform stores all variables in its state file in plain text, including this database password, which is why your terragrunt config should always enable encryption for remote state storage in S3","title":"Variables"},{"location":"terraform/#interpolation-of-variables","text":"You can't interpolate in variables, so instead of variable \"sistemas_gpg\" { description = \"Sistemas public GPG key for Zena\" type = \"string\" default = \"${file(\"sistemas_zena.pub\")}\" } You have to use locals locals { sistemas_gpg = \"${file(\"sistemas_zena.pub\")}\" } \"${local.sistemas_gpg}\"","title":"Interpolation of variables"},{"location":"terraform/#show-information-of-the-resources","text":"Get information of the infrastructure. Output variables show up in the console after you run terraform apply , you can also use terraform output [{{ output_name }}] to see the value of a specific output without applying any changes output \"public_ip\" { value = \"${aws_instance.example.public_ip}\" } > terraform apply aws_security_group.instance: Refreshing state... ( ID: sg-db91dba1 ) aws_instance.example: Refreshing state... ( ID: i-61744350 ) Apply complete! Resources: 0 added, 0 changed, 0 destroyed. Outputs: public_ip = 54 .174.13.5","title":"Show information of the resources"},{"location":"terraform/#data-source","text":"A data source represents a piece of read-only information that is fetched from the provider every time you run Terraform. It does not create anything new data \"aws_availability_zones\" \"all\" { } And you reference it with \"${data.aws_availability_zones.all.names}\"","title":"Data source"},{"location":"terraform/#read-only-state-source","text":"With terraform_remote_state you an fetch the Terraform state file stored by another set of templates in a completely read-only manner. From an app template we can read the info of the ddbb with data \"terraform_remote_state\" \"db\" { backend = \"s3\" config { bucket = \"(YOUR_BUCKET_NAME)\" key = \"stage/data-stores/mysql/terraform.tfstate\" region = \"us-east-1\" } } And you would access the variables inside the database terraform file with data.terraform_remote_state.db.outputs.port To share variables from state, you need to to set them in the outputs.tf file.","title":"Read-only state source"},{"location":"terraform/#template_file-source","text":"It is used to load templates, it has two parameters, template which is a string and vars which is a map of variables. it has one output attribute called rendered , which is the result of rendering template. For example # File: user-data.sh #!/bin/bash cat > index.html <<EOF <h1>Hello, World</h1> <p>DB address: ${db_address}</p> <p>DB port: ${db_port}</p> EOF nohup busybox httpd -f -p \" ${ server_port } \" & data \"template_file\" \"user_data\" { template = \"${file(\"user-data.sh\")}\" vars { server_port = \"${var.server_port}\" db_address = \"${data.terraform_remote_state.db.address}\" db_port = \"${data.terraform_remote_state.db.port}\" } }","title":"Template_file source"},{"location":"terraform/#resource-lifecycle","text":"The lifecycle parameter is a meta-parameter , it exist on about every resource in Terraform. You can add a lifecycle block to any resource to configure how that resource should be created, updated or destroyed. The available options are: * create_before_destroy : Which if set to true will create a replacement resource before destroying hte original resource * prevent_destroy : If set to true, any attempt to delete that resource ( terraform destroy ), will fail, to delete it you have to first remove the prevent_destroy resource \"aws_launch_configuration\" \"example\" { image_id = \"ami-40d28157\" instance_type = \"t2.micro\" security_groups = [ \"${aws_security_group.instance.id}\" ] user_data = <<-EOF #!/bin/bash echo \"Hello, World\" > index.html nohup busybox httpd -f -p \"${var.server_port}\" & EOF lifecycle { create_before_destroy = true } } If you set the create_before_destroy on a resource, you also have to set it on every resource that X depends on (if you forget, you'll get errors about cyclical dependencies). In the case of the launch configuration, that means you need to set create_before_destroy to true on the security group: resource \"aws_security_group\" \"instance\" { name = \"terraform-example-instance\" ingress { from_port = \"${var.server_port}\" to_port = \"${var.server_port}\" protocol = \"tcp\" cidr_blocks = [ \"0.0.0.0/0\" ] } lifecycle { create_before_destroy = true } }","title":"Resource lifecycle"},{"location":"terraform/#use-collaboratively","text":"","title":"Use collaboratively"},{"location":"terraform/#share-state","text":"The best option is to use S3 as bucket of the config. First create it resource \"aws_s3_bucket\" \"terraform_state\" { bucket = \"terraform-up-and-running-state\" versioning { enabled = true } lifecycle { prevent_destroy = true } } And then configure terraform terraform remote config \\ -backend = s3 \\ -backend-config = \"bucket=(YOUR_BUCKET_NAME)\" \\ -backend-config = \"key=global/s3/terraform.tfstate\" \\ -backend-config = \"region=us-east-1\" \\ -backend-config = \"encrypt=true\" In this way terraform will automatically pull the latest state from this bucked and push the latest state after running a command","title":"Share state"},{"location":"terraform/#lock-terraform","text":"To avoid several people running terraform at the same time, we'd use terragrunt a wrapper for terraform that manages remote state for you automatically and provies locking by using DynamoDB (in the free tier) Inside the terraform_config.tf you create the dynamodb table and then configure your s3 backend to use it resource \"aws_dynamodb_table\" \"terraform_statelock\" { name = \"global-s3\" read_capacity = 20 write_capacity = 20 hash_key = \"LockID\" attribute { name = \"LockID\" type = \"S\" } } terraform { backend \"s3\" { bucket = \"grupo-zena-tfstate\" key = \"global/s3/terraform.tfstate\" region = \"eu-west-1\" encrypt = \"true\" dynamodb_table = \"global-s3\" } } You'll probably need to execute an terraform apply with the dynamodb_table line commented If you want to unforce a lock, execute: terraform force-unlock {{ unlock_id }} You get the unlock_id from an error trying to execute any terraform command","title":"Lock terraform"},{"location":"terraform/#modules","text":"In terraform you can put code inside of a module and reuse in multiple places throughout your code. The provider resource should be specified by the user and not in the modules Whenever you add a module to your terraform template or modify its source parameter you need to run a get command before you run plan or apply terraform get To extract output variables of a module to the parent tf file you should use ${module.{{module.name}}.{{output_name}}}","title":"Modules"},{"location":"terraform/#basics","text":"Any set of Terraform templates in a directory is a module. The good practice is to have a directory called modules in your parent project directory. There you git clone the desired modules. and for example inside pro/services/bastion/main.tf you'd call it with: provider \"aws\" { region = \"eu-west-1\" } module \"bastion\" { source = \"../../../modules/services/bastion/\" }","title":"Basics"},{"location":"terraform/#outputs","text":"Modules encapsulate their resources. A resource in one module cannot directly depend on resources or attributes in other modules, unless those are exported through outputs. These outputs can be referenced in other places in your configuration, for example: resource \"aws_instance\" \"client\" { ami = \"ami-408c7f28\" instance_type = \"t1.micro\" availability_zone = \"${module.consul.server_availability_zone}\" }","title":"Outputs"},{"location":"terraform/#import","text":"You can import the different parts with terraform import {{resource_type}}.{{resource_name}} {{ resource_id }} For examples see the documentation of the desired resource.","title":"Import"},{"location":"terraform/#bulk-import","text":"But if you want to bulk import sources, I suggest using terraforming","title":"Bulk import"},{"location":"terraform/#bad-points","text":"Manually added resources wont be managed by terraform, therefore you can't use it to enforce as shown in this bug . If you modify the LC of an ASG, the instances don't get rolling updated, you have to do it manually. They call the dictionaries map ... (/\uff9f\u0414\uff9f)/ The conditionals are really ugly. You need to use count . You can't split long strings xD","title":"Bad points"},{"location":"terraform/#best-practices","text":"Name the resources with _ instead of - so the editor's completion work :)","title":"Best practices"},{"location":"terraform/#vpc","text":"Don't use the default vpc","title":"VPC"},{"location":"terraform/#security-groups","text":"Instead of using aws_security_group to define the ingress and egress rules, use it only to create the empty security group and use aws_security_group_rule to add the rules, otherwise you'll get into a cycle loop The sintaxis of an egress security group must be egress_from_{{source}}_to_destination . The sintaxis of an ingress security group must be ingress_to_{{destination}}_from_{{source}} Also set the order of the arguments, so they look like the name. For ingress rule: security_group_id = ... cidr_blocks = ... And in egress should look like: security_group_id = ... cidr_blocks = ... Imagine you want to filter the traffic from A -> B, the egress rule from A to B should go besides the ingress rule from B to A.","title":"Security groups"},{"location":"terraform/#default-security-group","text":"You can't manage the default security group of an vpc, therefore you have to adopt it and set it to no rules at all with aws_default_security_group resource","title":"Default security group"},{"location":"terraform/#iam","text":"You have to generate an gpg key and export it in base64 gpg --export {{ gpg_id }} | base64 To see the secrets you have to decrypt it terraform output secret | base64 --decode | gpg -d","title":"IAM"},{"location":"terraform/#sensitive-information","text":"There are several approaches here. First rely on the S3 encryption to protect the information in your state file Second use Vault provider to protect the state file. Third (but I won't use it) would be to use terrahelp","title":"Sensitive information"},{"location":"terraform/#rds-credentials","text":"The RDS credentials are saved in plaintext both in the definition and in the state file, see this bug for more information. The value of password is not compared against the value of the password in the cloud, so as long as the string in the code and the state file remains the same, it won't try to change it. As a workaround, you can create the RDS with a fake password changeme , and once the resource is created, run an aws command to change it. That way, the value in your code and the state is not the real one, but it won't try to change it. Inspired in this gist and the local-exec docs, you could do: resource \"aws_db_instance\" \"main\" { username = \"postgres\" password = \"changeme\" ... } resource \"null_resource\" \"master_password\" { triggers { db_host = aws_db_instance.main.address } provisioner \"local-exec\" { command = \"pass generate rds_main_password; aws rds modify-db-instance --db-instance-identifier $INSTANCE --master-user-password $(pass show rds_main_password)\" environment = { INSTANCE = aws_db_instance.main.identifier } } } Where the password is stored in your pass repository that can be shared with the team. If you're wondering why I added such a long line, well it's because of HCL! as you can't split long strings , marvelous isn't it? xD","title":"RDS credentials"},{"location":"terraform/#loops","text":"You can't use nested lists or dictionaries, see this 2015 bug","title":"Loops"},{"location":"terraform/#loop-over-a-variable","text":"variable \"vpn_egress_tcp_ports\" { description = \"VPN egress tcp ports \" type = \"list\" default = [50, 51, 500, 4500] } resource \"aws_security_group_rule\" \"ingress_tcp_from_ops_to_vpn_instance\"{ count = \"${length(var.vpn_egress_tcp_ports)}\" type = \"ingress\" from_port = \"${element(var.vpn_egress_tcp_ports, count.index)}\" to_port = \"${element(var.vpn_egress_tcp_ports, count.index)}\" protocol = \"tcp\" cidr_blocks = [ \"${var.cidr}\"] security_group_id = \"${aws_security_group.pro_ins_vpn.id}\" }","title":"Loop over a variable"},{"location":"terraform/#refactoring","text":"Refactoring in terraform is ugly business","title":"Refactoring"},{"location":"terraform/#refactoring-in-modules","text":"If you try to refactor your terraform state into modules it will try to destroy and recreate all the elements of the module...","title":"Refactoring in modules"},{"location":"terraform/#refactoring-the-state-file","text":"terraform state mv -state-out = other.tfstate module.web module.web","title":"Refactoring the state file"},{"location":"terraform/#google-cloud-integration","text":"You configure it in the terraform directory // Configure the Google Cloud provider provider \"google\" { credentials = \"${file(\"account.json\")}\" project = \"my-gce-project\" region = \"us-central1\" } To download the json go to the Google Developers Console . Go to Credentials then Create credentials and finally Service account key . Select Compute engine default service account and select JSON as the key type.","title":"Google cloud integration"},{"location":"terraform/#ignore-the-change-of-an-attribute","text":"Sometimes you don't care whether some attributes of a resource change, if that's the case use the lifecycle statement: resource \"aws_instance\" \"example\" { # ... lifecycle { ignore_changes = [ # Ignore changes to tags, e.g. because a management agent # updates these based on some ruleset managed elsewhere. tags, ] } }","title":"Ignore the change of an attribute"},{"location":"terraform/#define-the-default-value-of-an-variable-that-contains-an-object-as-empty","text":"variable \"database\" { type = object({ size = number instance_type = string storage_type = string engine = string engine_version = string parameter_group_name = string multi_az = bool }) default = null","title":"Define the default value of an variable that contains an object as empty"},{"location":"terraform/#conditionals","text":"","title":"Conditionals"},{"location":"terraform/#elif","text":"locals { test = \"${ condition ? value : (elif-condition ? elif-value : else-value)}\" }","title":"Elif"},{"location":"terraform/#do-a-conditional-if-a-variable-is-not-null","text":"resource \"aws_db_instance\" \"instance\" { count = var.database == null ? 0 : 1 ...","title":"Do a conditional if a variable is not null"},{"location":"terraform/#debugging","text":"You can set the TF_LOG environmental variable to one of the log levels TRACE , DEBUG , INFO , WARN or ERROR to change the verbosity of the logs. To remove the debug traces run unset TF_LOG .","title":"Debugging"},{"location":"terraform/#references","text":"Docs Modules registry terraform-aws-modules AWS providers AWS examples GCloud examples Good and bad sides of terraform Awesome Terraform","title":"References"},{"location":"time_management/","text":"Time management is the process of planning and exercising conscious control of time spent on specific activities, especially to increase effectiveness, efficiency, and productivity. It involves a juggling act of various demands upon a person relating to work, social life, family, hobbies, personal interests, and commitments with the finiteness of time. Using time effectively gives the person \"choice\" on spending or managing activities at their own time and expediency. To be able to do time management, you first need to define how do you want to increase your effectiveness, efficiency, and productivity. For me, it means increasing the amount and quality of work per unit of time or effort. Understanding work as any task that gets me closer to a goal. It doesn't necessarily be related with professional work, it can be applied to a personal project, cleaning or hanging out with friends. The rest of the article describes the approaches I use to maximize this idea of efficiency. If you have a different understanding, goal or your brain works in a different way than mine, most of the guidelines may not apply to you, but they could spark some ideas that you can implement on your daily life. To increase the productivity we can: Reduce the time spent doing unproductive tasks . Improve the way you do the tasks . Improve how you manage your tools . Improve your state and environment to be more efficient . Reduce the time spent doing unproductive tasks \u2691 Sadly, the day has only 24 hours you can use. There's nothing to do about it, we can however reduce the amount of wasted time to make a better use of the time that we have. Minimize the context switches \u2691 Each time we switch from one task to another, the brain needs to load all the necessary information to be able to address the new task. Dumping the old task information and loading the new is both time consuming and exhausting, so do it consciously and sparingly. One way of improving this behaviour is by using the Pomodoro technique . Interruption management \u2691 We've come to accept that we need to be available 24/7 and answer immediately, that makes us slaves of the interruptions, it drives our work and personal relations. I feel that out of respect of ourselves and the other's time, we need to change that perspective. Most of the times interruptions can wait 20 or 60 minutes, and many of them can be avoided with better task and time planning. Interruptions are one of the main productivity killers. Not only they unexpectedly break your workflow, they also add undesired mental load as you are waiting for them to happen, and need to check them often. As we've seen previously, to be productive you need to be able to work on a task for 20 minutes without checking the interruption channels. Fill up your own interruption analysis report and define your workflow to manage them. Avoid lost time doing nothing \u2691 Sometimes I catch myself watching at the screen with zero mental activity and drooling. Other times I endlessly switch between browser tabs or the email client and the chat clients for no reason, it's just a reflex act. You probably have similar behaviours that lead you nowhere. Some should be an alert that you need a break (don't drool the keyboard please), but others are bad uncontrolled behaviours that could be identified and got rid of. Fix your environment \u2691 When we loose time, we don't do it consciously, that's why it's difficult for us to stay alert and actively try to change those behaviours. It's much easier to fix your environment so that the reasons that trigger the time loss don't happen at all. For example, if you keep on going back to the email client regularly even though you decided only to check it three times a day, instead of mentally punishing yourself when you check it, close the client or move it to another workspace so it's not where you're used to see it. Don't wait, switch task \u2691 Even though we want to minimize the context switches , staring at the screen for a long process to end makes no sense. If you do task management well, the context switch toll gets smaller enough that whenever you hit a block in the task you're working on, you can switch to another one. A block can be caused by a long running process or waiting for someone to do something. If you find concentrating difficult, don't do this, it's a hard skill to master. When a block comes, I first try to switch back to processes that I was already working on. Try to have as less processes as possible, less than three if possible. If there is only one active process, look at the task plan for the next step that could be done in parallel. As both processes work on the same task, they share most of the context, so the switch is cheap. If there is none, go to the day plan to start the first step of the next task in the plan. Improve the way you do the tasks \u2691 Improve how you manage your tasks to: Reduce your mental load, so you can use those resources doing productive work. Improve your efficiency. Make more realistic estimations, thus meeting the commited deadlines. Finish what you start. Know you're working towards your ultimate goals Stop feeling lost or overburdened. Make context switches cheaper. Improve how you manage your tools \u2691 Most of the tasks or processes we do involve some kind of tool, the better you know how to use them, the better your efficiency will be. The more you use a tool, the more it's worth the investment of time to improve your usage of it. Whenever I use a tool, I try to think if I could configure it or use it in a way that will make it easier or quicker. Don't go crazy and try to change everything. Go step by step, and once you've internalized the improvement, implement the next. Email management . Instant messages management . Meetings . Meetings \u2691 Calls, video calls, group calls or physical meetings are the best communication channel to transmit non trivial short messages. Even if they are the most efficient, they will break your working workflow, as you'll need to prepare yourself to know what to say and how, go to the meeting location, and then process all the information gathered. That's why if not used wisely, it can be a sink of productivity. Try to minimize and group the meetings, thus having less interruptions. Maximize the continuous uninterrupted time, so schedule them at the start or end of the morning or afternoon. Once you agreed to attend, make each of them count. Define an agenda and a time limit per section. That'll keep the conversation on track, and will give enough information to the attendees to decide if they need to be there. Likewise, whenever you're invited to a meeting, value if you need to go. If you don't, politely decline the offer. Sometimes assigning someone the role to conduct the meeting, or taking turns to talk can help. There are more informal meetings where you don't need all these constrains and formality. For example in a coffee break. You know that they are going to be unproductive but that's ok too. Master your tools and apply them where you think they are needed. Improve your state \u2691 To be able to work efficiently, manage your tasks and change your habits you need to have the appropriate state of mind. This last factor is often overlooked, but one of the most important. To be efficient you need to take care of yourself. Analyze how are you to detect what physical or mental attributes aren't at the optimum level and act accordingly by fixing them and adjusting your plans. This will be difficult to most of us, as we are disconnected from our bodies, and don't know how to study ourselves. If it's your case, you could start by meditating or to quantifying yourself. Some of the vectors you can work on to improve your state are: Sleep better . Work out. Hang out. Isolate your personal life from your work life. Procrastinate mindfully. Don't be a slave of the interruptions . Improve your working environment . Prevent illnesses through hygiene and exercise .","title":"Time Management"},{"location":"time_management/#reduce-the-time-spent-doing-unproductive-tasks","text":"Sadly, the day has only 24 hours you can use. There's nothing to do about it, we can however reduce the amount of wasted time to make a better use of the time that we have.","title":"Reduce the time spent doing unproductive tasks"},{"location":"time_management/#minimize-the-context-switches","text":"Each time we switch from one task to another, the brain needs to load all the necessary information to be able to address the new task. Dumping the old task information and loading the new is both time consuming and exhausting, so do it consciously and sparingly. One way of improving this behaviour is by using the Pomodoro technique .","title":"Minimize the context switches"},{"location":"time_management/#interruption-management","text":"We've come to accept that we need to be available 24/7 and answer immediately, that makes us slaves of the interruptions, it drives our work and personal relations. I feel that out of respect of ourselves and the other's time, we need to change that perspective. Most of the times interruptions can wait 20 or 60 minutes, and many of them can be avoided with better task and time planning. Interruptions are one of the main productivity killers. Not only they unexpectedly break your workflow, they also add undesired mental load as you are waiting for them to happen, and need to check them often. As we've seen previously, to be productive you need to be able to work on a task for 20 minutes without checking the interruption channels. Fill up your own interruption analysis report and define your workflow to manage them.","title":"Interruption management"},{"location":"time_management/#avoid-lost-time-doing-nothing","text":"Sometimes I catch myself watching at the screen with zero mental activity and drooling. Other times I endlessly switch between browser tabs or the email client and the chat clients for no reason, it's just a reflex act. You probably have similar behaviours that lead you nowhere. Some should be an alert that you need a break (don't drool the keyboard please), but others are bad uncontrolled behaviours that could be identified and got rid of.","title":"Avoid lost time doing nothing"},{"location":"time_management/#fix-your-environment","text":"When we loose time, we don't do it consciously, that's why it's difficult for us to stay alert and actively try to change those behaviours. It's much easier to fix your environment so that the reasons that trigger the time loss don't happen at all. For example, if you keep on going back to the email client regularly even though you decided only to check it three times a day, instead of mentally punishing yourself when you check it, close the client or move it to another workspace so it's not where you're used to see it.","title":"Fix your environment"},{"location":"time_management/#dont-wait-switch-task","text":"Even though we want to minimize the context switches , staring at the screen for a long process to end makes no sense. If you do task management well, the context switch toll gets smaller enough that whenever you hit a block in the task you're working on, you can switch to another one. A block can be caused by a long running process or waiting for someone to do something. If you find concentrating difficult, don't do this, it's a hard skill to master. When a block comes, I first try to switch back to processes that I was already working on. Try to have as less processes as possible, less than three if possible. If there is only one active process, look at the task plan for the next step that could be done in parallel. As both processes work on the same task, they share most of the context, so the switch is cheap. If there is none, go to the day plan to start the first step of the next task in the plan.","title":"Don't wait, switch task"},{"location":"time_management/#improve-the-way-you-do-the-tasks","text":"Improve how you manage your tasks to: Reduce your mental load, so you can use those resources doing productive work. Improve your efficiency. Make more realistic estimations, thus meeting the commited deadlines. Finish what you start. Know you're working towards your ultimate goals Stop feeling lost or overburdened. Make context switches cheaper.","title":"Improve the way you do the tasks"},{"location":"time_management/#improve-how-you-manage-your-tools","text":"Most of the tasks or processes we do involve some kind of tool, the better you know how to use them, the better your efficiency will be. The more you use a tool, the more it's worth the investment of time to improve your usage of it. Whenever I use a tool, I try to think if I could configure it or use it in a way that will make it easier or quicker. Don't go crazy and try to change everything. Go step by step, and once you've internalized the improvement, implement the next. Email management . Instant messages management . Meetings .","title":"Improve how you manage your tools"},{"location":"time_management/#meetings","text":"Calls, video calls, group calls or physical meetings are the best communication channel to transmit non trivial short messages. Even if they are the most efficient, they will break your working workflow, as you'll need to prepare yourself to know what to say and how, go to the meeting location, and then process all the information gathered. That's why if not used wisely, it can be a sink of productivity. Try to minimize and group the meetings, thus having less interruptions. Maximize the continuous uninterrupted time, so schedule them at the start or end of the morning or afternoon. Once you agreed to attend, make each of them count. Define an agenda and a time limit per section. That'll keep the conversation on track, and will give enough information to the attendees to decide if they need to be there. Likewise, whenever you're invited to a meeting, value if you need to go. If you don't, politely decline the offer. Sometimes assigning someone the role to conduct the meeting, or taking turns to talk can help. There are more informal meetings where you don't need all these constrains and formality. For example in a coffee break. You know that they are going to be unproductive but that's ok too. Master your tools and apply them where you think they are needed.","title":"Meetings"},{"location":"time_management/#improve-your-state","text":"To be able to work efficiently, manage your tasks and change your habits you need to have the appropriate state of mind. This last factor is often overlooked, but one of the most important. To be efficient you need to take care of yourself. Analyze how are you to detect what physical or mental attributes aren't at the optimum level and act accordingly by fixing them and adjusting your plans. This will be difficult to most of us, as we are disconnected from our bodies, and don't know how to study ourselves. If it's your case, you could start by meditating or to quantifying yourself. Some of the vectors you can work on to improve your state are: Sleep better . Work out. Hang out. Isolate your personal life from your work life. Procrastinate mindfully. Don't be a slave of the interruptions . Improve your working environment . Prevent illnesses through hygiene and exercise .","title":"Improve your state"},{"location":"tool_management/","text":"Most of the tasks or processes we do involve some kind of tool, the better you know how to use them, the better your efficiency will be. The more you use a tool, the more it's worth the investment of time to improve your usage of it. Whenever I use a tool, I try to think if I could configure it or use it in a way that will make it easier or quicker. Don't go crazy and try to change everything. Go step by step, and once you've internalized the improvement, implement the next.","title":"Tool management"},{"location":"vim/","text":"Vim is a lightweight keyboard driven editor. It's the road to fly over the keyboard as it increases productivity and usability. If you doubt between learning emacs or vim, go with emacs with spacemacs I am a power vim user for more than 10 years, and seeing what my friends do with emacs, I suggest you to learn it while keeping the vim movement. Spacemacs is a preconfigured Emacs with those bindings and a lot of more stuff, but it's a good way to start. Vi vs Vim vs Neovim \u2691 TL;DR: Use Neovim Small comparison: Vi Follows the Single Unix Specification and POSIX. Original code written by Bill Joy in 1976. BSD license. Doesn't even have a git repository -.- . Vim Written by Bram Moolenaar in 1991. Vim is free and open source software, license is compatible with the GNU General Public License. C: 47.6 %, Vim Script: 44.8%, Roff 1.9%, Makefile 1.7%, C++ 1.2% Commits: 7120, Branch: 1 , Releases: 5639, Contributor: 1 Lines: 1.295.837 Neovim Written by the community from 2014 Published under the Apache 2.0 license Commits: 7994, Branch 1 , Releases: 9, Contributors: 303 Vim script: 46.9%, C:38.9%, Lua 11.3%, Python 0.9%, C++ 0.6% Lines: 937.508 (27.65% less code than vim) Refactor: Simplify maintenance and encourage contributions Easy update, just symlinks Ahead of vim, new features inserted in Vim 8.0 (async) Neovim is a refactor of Vim to make it viable for another 30 years of hacking. Neovim very intentionally builds on the long history of Vim community knowledge and user habits. That means \u201cswitching\u201d from Vim to Neovim is just an \u201cupgrade\u201d. From the start, one of Neovim\u2019s explicit goals has been simplify maintenance and encourage contributions. By building a codebase and community that enables experimentation and low-cost trials of new features.. And there\u2019s evidence of real progress towards that ambition. We\u2019ve successfully executed non-trivial \u201coff-the-roadmap\u201d patches: features which are important to their authors, but not the highest priority for the project. These patches were included because they: Fit into existing conventions/design. Included robust test coverage (enabled by an advanced test framework and CI). Received thoughtful review by other contributors. Abbreviations \u2691 In order to reduce the amount of typing and fix common typos, I use the Vim abbreviations support. Those are split into two files, ~/.vim/abbreviations.vim for abbreviations that can be used in every type of format and ~/.vim/markdown-abbreviations.vim for the ones that can interfere with programming typing. Those files are sourced in my .vimrc \" Abbreviations source ~ /.vim/ abbreviations. vim autocmd BufNewFile , BufReadPost *.md source ~ /.vim/ markdown - abbreviations. vim To avoid getting worse in grammar, I don't add abbreviations for words that I doubt their spelling or that I usually mistake. Instead I use it for common typos such as teh . The process has it's inconveniences: You need different abbreviations for the capitalized versions, so you'd need two abbreviations for iab cant can't and iab Cant Can't It's not user friendly to add new words, as you need to open a file. The Vim Abolish plugin solves that. For example: \" Typing the following: Abolish seperate separate \" Is equivalent to: iabbrev seperate separate iabbrev Seperate Separate iabbrev SEPERATE SEPARATE Or create more complex rules, were each {} gets captured and expanded with different caps : Abolish {despa , sepe}rat{ e , es , ed , ing , ely , ion , ions , or} {despe , sepa}rat{} With a bang ( :Abolish! ) the abbreviation is also appended to the file in g:abolish_save_file . By default after/plugin/abolish.vim which is loaded by default. Typing :Abolish! im I'm will append the following to the end of this file: Abolish im I' m To make it quicker I've added a mapping for <leader>s . nnoremap < leader > s :Abolish !< Space > Check the README for more details. Troubleshooting \u2691 Abbreviations with dashes or if you only want the first letter in capital need to be specified with the first letter in capital letters as stated in this issue . Abolish knobas knowledge - based Abolish w what Will yield KnowledgeBased if invoked with Knobas , and WHAT if invoked with W . Therefore the following definitions are preferred: Abolish Knobas Knowledge - based Abolish W What Auto complete prose text \u2691 Tools like YouCompleteMe allow you to auto complete variables and functions. If you want the same functionality for prose, you need to enable it for markdown and text, as it's disabled by default. let g :ycm_filetype_blacklist = { \\ 'tagbar' : 1 , \\ 'qf' : 1 , \\ 'notes' : 1 , \\ 'unite' : 1 , \\ 'vimwiki' : 1 , \\ 'pandoc' : 1 , \\ 'infolog' : 1 \\} When writing prose you don't need all possible suggestions, as navigating the options is slower than keep on typing. So I'm limiting the results just to one, to avoid unnecessary distractions. \" Limit the results for markdown files to 1 au FileType markdown let g :ycm_max_num_candidates = 1 au FileType markdown let g :ycm_max_num_identifier_candidates = 1 Find synonyms \u2691 Sometimes the prose linters tell you that a word is wordy or too complex, or you may be repeating a word too much. The thesaurus query plugin allows you to search synonyms of the word under the cursor. Assuming you use Vundle, add the following lines to your config. File: ~/.vimrc Plugin 'ron89/thesaurus_query.vim' \" Thesaurus let g :tq_enabled_backends = [ \"mthesaur_txt\" ] let g :tq_mthesaur_file = \"~/.vim/thesaurus\" nnoremap < leader > r :ThesaurusQueryReplaceCurrentWord < CR > inoremap < leader > r < esc > :ThesaurusQueryReplaceCurrentWord < CR > Run :PluginInstall and download the thesaurus text from gutenberg.org Next time you find a word like therefore you can press :ThesaurusQueryReplaceCurrentWord and you'll get a window with the following: In line: ... therefore ... Candidates for therefore, found by backend: mthesaur_txt Synonyms: (0)accordingly (1)according to circumstances (2)and so (3)appropriately (4)as a consequence (5)as a result (6)as it is (7)as matters stand (8)at that rate (9)because of that (10)because of this (11)compliantly (12)conformably (13)consequently (14)equally (15)ergo (16)finally (17)for that (18)for that cause (19)for that reason (20)for this cause (21)for this reason (22)for which reason (23)hence (24)hereat (25)in that case (26)in that event (27)inconsequence (28)inevitably (29)it follows that (30)naturally (31)naturellement (32)necessarily (33)of course (34)of necessity (35)on that account (36)on that ground (37)on this account (38)propter hoc (39)suitably (40)that being so (41)then (42)thence (43)thereat (44)therefor (45)thus (46)thusly (47)thuswise (48)under the circumstances (49)whence (50)wherefore (51)wherefrom Type number and <Enter> (empty cancels; 'n': use next backend; 'p' use previous backend): If for example you type 45 and hit enter, it will change it for thus . Keep foldings \u2691 When running fixers usually the foldings go to hell. To keep the foldings add the following snippet to your vimrc file augroup remember_folds autocmd ! autocmd BufWinLeave * mkview autocmd BufWinEnter * silent ! loadview augroup END Python folding done right \u2691 Folding Python in Vim is not easy, the python-mode plugin doesn't do it for me by default and after fighting with it for 2 hours... SimpylFold does the trick just fine. Resources \u2691 Nvim news spacevim awesome-vim : a list of vim resources maintained by the community Vimrc tweaking \u2691 jessfraz vimrc Learning \u2691 vim golf Vim game tutorial : very funny and challenging, buuuuut at lvl 3 you have to pay :(. PacVim : Pacman like vim game to learn. Vimgenius : Increase your speed and improve your muscle memory with Vim Genius, a timed flashcard-style game designed to make you faster in Vim. It\u2019s free and you don\u2019t need to sign up. What are you waiting for? Openvim : Interactive tutorial for vim.","title":"Vim"},{"location":"vim/#vi-vs-vim-vs-neovim","text":"TL;DR: Use Neovim Small comparison: Vi Follows the Single Unix Specification and POSIX. Original code written by Bill Joy in 1976. BSD license. Doesn't even have a git repository -.- . Vim Written by Bram Moolenaar in 1991. Vim is free and open source software, license is compatible with the GNU General Public License. C: 47.6 %, Vim Script: 44.8%, Roff 1.9%, Makefile 1.7%, C++ 1.2% Commits: 7120, Branch: 1 , Releases: 5639, Contributor: 1 Lines: 1.295.837 Neovim Written by the community from 2014 Published under the Apache 2.0 license Commits: 7994, Branch 1 , Releases: 9, Contributors: 303 Vim script: 46.9%, C:38.9%, Lua 11.3%, Python 0.9%, C++ 0.6% Lines: 937.508 (27.65% less code than vim) Refactor: Simplify maintenance and encourage contributions Easy update, just symlinks Ahead of vim, new features inserted in Vim 8.0 (async) Neovim is a refactor of Vim to make it viable for another 30 years of hacking. Neovim very intentionally builds on the long history of Vim community knowledge and user habits. That means \u201cswitching\u201d from Vim to Neovim is just an \u201cupgrade\u201d. From the start, one of Neovim\u2019s explicit goals has been simplify maintenance and encourage contributions. By building a codebase and community that enables experimentation and low-cost trials of new features.. And there\u2019s evidence of real progress towards that ambition. We\u2019ve successfully executed non-trivial \u201coff-the-roadmap\u201d patches: features which are important to their authors, but not the highest priority for the project. These patches were included because they: Fit into existing conventions/design. Included robust test coverage (enabled by an advanced test framework and CI). Received thoughtful review by other contributors.","title":"Vi vs Vim vs Neovim"},{"location":"vim/#abbreviations","text":"In order to reduce the amount of typing and fix common typos, I use the Vim abbreviations support. Those are split into two files, ~/.vim/abbreviations.vim for abbreviations that can be used in every type of format and ~/.vim/markdown-abbreviations.vim for the ones that can interfere with programming typing. Those files are sourced in my .vimrc \" Abbreviations source ~ /.vim/ abbreviations. vim autocmd BufNewFile , BufReadPost *.md source ~ /.vim/ markdown - abbreviations. vim To avoid getting worse in grammar, I don't add abbreviations for words that I doubt their spelling or that I usually mistake. Instead I use it for common typos such as teh . The process has it's inconveniences: You need different abbreviations for the capitalized versions, so you'd need two abbreviations for iab cant can't and iab Cant Can't It's not user friendly to add new words, as you need to open a file. The Vim Abolish plugin solves that. For example: \" Typing the following: Abolish seperate separate \" Is equivalent to: iabbrev seperate separate iabbrev Seperate Separate iabbrev SEPERATE SEPARATE Or create more complex rules, were each {} gets captured and expanded with different caps : Abolish {despa , sepe}rat{ e , es , ed , ing , ely , ion , ions , or} {despe , sepa}rat{} With a bang ( :Abolish! ) the abbreviation is also appended to the file in g:abolish_save_file . By default after/plugin/abolish.vim which is loaded by default. Typing :Abolish! im I'm will append the following to the end of this file: Abolish im I' m To make it quicker I've added a mapping for <leader>s . nnoremap < leader > s :Abolish !< Space > Check the README for more details.","title":"Abbreviations"},{"location":"vim/#troubleshooting","text":"Abbreviations with dashes or if you only want the first letter in capital need to be specified with the first letter in capital letters as stated in this issue . Abolish knobas knowledge - based Abolish w what Will yield KnowledgeBased if invoked with Knobas , and WHAT if invoked with W . Therefore the following definitions are preferred: Abolish Knobas Knowledge - based Abolish W What","title":"Troubleshooting"},{"location":"vim/#auto-complete-prose-text","text":"Tools like YouCompleteMe allow you to auto complete variables and functions. If you want the same functionality for prose, you need to enable it for markdown and text, as it's disabled by default. let g :ycm_filetype_blacklist = { \\ 'tagbar' : 1 , \\ 'qf' : 1 , \\ 'notes' : 1 , \\ 'unite' : 1 , \\ 'vimwiki' : 1 , \\ 'pandoc' : 1 , \\ 'infolog' : 1 \\} When writing prose you don't need all possible suggestions, as navigating the options is slower than keep on typing. So I'm limiting the results just to one, to avoid unnecessary distractions. \" Limit the results for markdown files to 1 au FileType markdown let g :ycm_max_num_candidates = 1 au FileType markdown let g :ycm_max_num_identifier_candidates = 1","title":"Auto complete prose text"},{"location":"vim/#find-synonyms","text":"Sometimes the prose linters tell you that a word is wordy or too complex, or you may be repeating a word too much. The thesaurus query plugin allows you to search synonyms of the word under the cursor. Assuming you use Vundle, add the following lines to your config. File: ~/.vimrc Plugin 'ron89/thesaurus_query.vim' \" Thesaurus let g :tq_enabled_backends = [ \"mthesaur_txt\" ] let g :tq_mthesaur_file = \"~/.vim/thesaurus\" nnoremap < leader > r :ThesaurusQueryReplaceCurrentWord < CR > inoremap < leader > r < esc > :ThesaurusQueryReplaceCurrentWord < CR > Run :PluginInstall and download the thesaurus text from gutenberg.org Next time you find a word like therefore you can press :ThesaurusQueryReplaceCurrentWord and you'll get a window with the following: In line: ... therefore ... Candidates for therefore, found by backend: mthesaur_txt Synonyms: (0)accordingly (1)according to circumstances (2)and so (3)appropriately (4)as a consequence (5)as a result (6)as it is (7)as matters stand (8)at that rate (9)because of that (10)because of this (11)compliantly (12)conformably (13)consequently (14)equally (15)ergo (16)finally (17)for that (18)for that cause (19)for that reason (20)for this cause (21)for this reason (22)for which reason (23)hence (24)hereat (25)in that case (26)in that event (27)inconsequence (28)inevitably (29)it follows that (30)naturally (31)naturellement (32)necessarily (33)of course (34)of necessity (35)on that account (36)on that ground (37)on this account (38)propter hoc (39)suitably (40)that being so (41)then (42)thence (43)thereat (44)therefor (45)thus (46)thusly (47)thuswise (48)under the circumstances (49)whence (50)wherefore (51)wherefrom Type number and <Enter> (empty cancels; 'n': use next backend; 'p' use previous backend): If for example you type 45 and hit enter, it will change it for thus .","title":"Find synonyms"},{"location":"vim/#keep-foldings","text":"When running fixers usually the foldings go to hell. To keep the foldings add the following snippet to your vimrc file augroup remember_folds autocmd ! autocmd BufWinLeave * mkview autocmd BufWinEnter * silent ! loadview augroup END","title":"Keep foldings"},{"location":"vim/#python-folding-done-right","text":"Folding Python in Vim is not easy, the python-mode plugin doesn't do it for me by default and after fighting with it for 2 hours... SimpylFold does the trick just fine.","title":"Python folding done right"},{"location":"vim/#resources","text":"Nvim news spacevim awesome-vim : a list of vim resources maintained by the community","title":"Resources"},{"location":"vim/#vimrc-tweaking","text":"jessfraz vimrc","title":"Vimrc tweaking"},{"location":"vim/#learning","text":"vim golf Vim game tutorial : very funny and challenging, buuuuut at lvl 3 you have to pay :(. PacVim : Pacman like vim game to learn. Vimgenius : Increase your speed and improve your muscle memory with Vim Genius, a timed flashcard-style game designed to make you faster in Vim. It\u2019s free and you don\u2019t need to sign up. What are you waiting for? Openvim : Interactive tutorial for vim.","title":"Learning"},{"location":"vim_tabs/","text":"This article is almost a copy paste of joshldavis post First I have to admit, I was a heavy user of tabs in Vim. I was using tabs in Vim as you\u2019d use tabs in most other programs (Firefox, Terminal, Adium, etc.). I was used to the idea of a tab being the place where a document lives. When you want to edit a document, you open a new tab and edit away! That\u2019s how tabs work so that must be how they work in Vim right? Nope. Stop the Tab Madness \u2691 If you are using tabs like this then you are really limiting yourself and using a feature of Vim that wasn't meant to work like this. Before I explain that, let\u2019s be sure we understand what a buffer is in Vim as well as a few other basic things. After that, I\u2019ll explain the correct way to use tabs within Vim. Buffers \u2691 A buffer is nothing more than text that you are editing. For example, when you open a file, the content of the file is loaded into a buffer. So when you issue this command: vim .vimrc You are actually launching Vim with a single buffer that is filled with the contents of the .vimrc file. Now let\u2019s look at what happens when you try to edit multiple files. Let\u2019s issue this command: vim .vimrc .bashrc In Vim run :bnext Vim does what it did before, but instead of just 1 buffer, it opens another buffer that is filled with .bashrc . So now we have two buffers open. If you want to pause editing .vimrc and move to .bashrc , you could run this command in Vim :bnext which will show the .bashrc buffer. There are various other commands to manipulate buffers which you can see if you type :h buffer-list . Or you can use easymotion . Windows \u2691 A window in Vim is just a way to view a buffer. Whenever you create a new vertical or horizontal split, that is a window. For example, if you were to type in :help window , it would launch a new window that shows the help documentation. The important thing to note is that a window can view any buffer it wishes; it isn't forced to look at the same buffer. When editing a file, if we type :vsplit , we will get a vertical split and in the other window, we will see the current buffer we are editing. That should no longer be confusing because a window lets us look at any buffer. It just so happens that when creating a new split: :split or :vsplit , the buffer that we view is just the current one. By running any of the buffer commands from :h buffer-list , we can modify which buffer a window is viewing. For an example of this, by running the following commands, we will start editing two files in Vim, open a new window by horizontally splitting, and then view the second buffer in the original window. vim .vimrc .bashrc In Vim run: :split and :bnext So a Tab is\u2026? \u2691 So now that we know what a buffer is and what a window is. Here is what Vim says in the Vim documentation regarding a buffer/window/tab: Summary: A buffer is the in-memory text of a file. A window is a viewport on a buffer. A tab page is a collection of windows. According to the documentation, a tab is just a collection of windows. This goes back to our earlier definition in that a tab is really just a layout. A tab is only designed to give you a different layout of windows. The Tab Problem \u2691 Tabs were only designed to let us have different layouts of windows. They aren't intended to be where an open file lives; an open file is instead loaded into a buffer. If you can view the same buffer across all tabs, how is this like a normal tab in most other editors? If you try to force a single tab to point to a single buffer, that is just futile. Vim just wasn't meant to work like this. The Buffer Solution \u2691 To reconcile all of this and learn how to use Vim\u2019s buffers/windows effectively, it might be useful to stop using tabs altogether until you understand how to edit with just using buffers/windows. The first thing I did was install a plugin that allows me to visualize all the buffers open across the top. I use bufferline for this. Instead of replicating tabs across the top like we did in the previous solution, we are instead going to use the power of being able to open many buffers simultaneously without worrying about which ones are open. In my experience, CtrlP gives a powerful fuzzy finder to navigate through the buffers. Instead of worrying about closing buffers and managing your pseudo-tabs that was mentioned in the previous solution, you just open files that you want to edit using CtrlP and don't worry about closing buffers or how many you have opened. When you are done editing a file, you just save it and then open CtrlP and continue onto the next file. CtrlP offers a few different ways to fuzzy find. You can use the following fuzziness: Find in your current directory. Find within all your open buffers. Find within all your open buffers sorted by Most Recently Used (MRU). Find with a mix of all the above. Using Tabs Correctly \u2691 This doesn't mean you should stop using tabs altogether. You should just use them how Vim intended you to use them. Instead you should use them to change the layout among windows. Imagine you are working on a C project. It might be helpful to have one tab dedicated to normal editing, but another tab for using a vertical split for the file.h and file.c files to make editing between them easier. Tabs also work really well to divide up what you are working on. You could be working on one part of the project in one tab and another part of the project in another tab. Just remember though, if you are using a single tab for each file, that isn't how tabs in Vim were designed to be used. Default option when switching \u2691 The default behavior when trying to switch the buffer is to not allow you to change buffer if it's not saved, but we can change it if we set one of the next options: set hidden : allow to switch buffers even though it's changes aren't saved. set autowrite : Auto save when switching buffers. Share buffers and all vim information between vim instances. \u2691 This is not my ideal behavior, nvim should let the user use the window manager to manage the windows... duh, instead of vsplitting buffers or using tabs. But sadly as of Nvim 0.1.7 and Vim 8.0 it's not implemented . You have the --server option but it only sends files to the already opened vim instance. you can't connect two vim instances to the same buffer pool. It's been discussed in neovim 1 , 2 . Currently gVim cannot have separate 'toplevel' windows for the same process/session. There is a TODO item to implement an inter-process communication system between multiple Vim instances to make it behave as though the separate processes are unified. (See :help todo and search for \"top-level\".) There is an interesting hax formalized in here which I will want to have time to test. Another solution would be to try to use neovim remote","title":"Tabs vs Buffers"},{"location":"vim_tabs/#stop-the-tab-madness","text":"If you are using tabs like this then you are really limiting yourself and using a feature of Vim that wasn't meant to work like this. Before I explain that, let\u2019s be sure we understand what a buffer is in Vim as well as a few other basic things. After that, I\u2019ll explain the correct way to use tabs within Vim.","title":"Stop the Tab Madness"},{"location":"vim_tabs/#buffers","text":"A buffer is nothing more than text that you are editing. For example, when you open a file, the content of the file is loaded into a buffer. So when you issue this command: vim .vimrc You are actually launching Vim with a single buffer that is filled with the contents of the .vimrc file. Now let\u2019s look at what happens when you try to edit multiple files. Let\u2019s issue this command: vim .vimrc .bashrc In Vim run :bnext Vim does what it did before, but instead of just 1 buffer, it opens another buffer that is filled with .bashrc . So now we have two buffers open. If you want to pause editing .vimrc and move to .bashrc , you could run this command in Vim :bnext which will show the .bashrc buffer. There are various other commands to manipulate buffers which you can see if you type :h buffer-list . Or you can use easymotion .","title":"Buffers"},{"location":"vim_tabs/#windows","text":"A window in Vim is just a way to view a buffer. Whenever you create a new vertical or horizontal split, that is a window. For example, if you were to type in :help window , it would launch a new window that shows the help documentation. The important thing to note is that a window can view any buffer it wishes; it isn't forced to look at the same buffer. When editing a file, if we type :vsplit , we will get a vertical split and in the other window, we will see the current buffer we are editing. That should no longer be confusing because a window lets us look at any buffer. It just so happens that when creating a new split: :split or :vsplit , the buffer that we view is just the current one. By running any of the buffer commands from :h buffer-list , we can modify which buffer a window is viewing. For an example of this, by running the following commands, we will start editing two files in Vim, open a new window by horizontally splitting, and then view the second buffer in the original window. vim .vimrc .bashrc In Vim run: :split and :bnext","title":"Windows"},{"location":"vim_tabs/#so-a-tab-is","text":"So now that we know what a buffer is and what a window is. Here is what Vim says in the Vim documentation regarding a buffer/window/tab: Summary: A buffer is the in-memory text of a file. A window is a viewport on a buffer. A tab page is a collection of windows. According to the documentation, a tab is just a collection of windows. This goes back to our earlier definition in that a tab is really just a layout. A tab is only designed to give you a different layout of windows.","title":"So a Tab is\u2026?"},{"location":"vim_tabs/#the-tab-problem","text":"Tabs were only designed to let us have different layouts of windows. They aren't intended to be where an open file lives; an open file is instead loaded into a buffer. If you can view the same buffer across all tabs, how is this like a normal tab in most other editors? If you try to force a single tab to point to a single buffer, that is just futile. Vim just wasn't meant to work like this.","title":"The Tab Problem"},{"location":"vim_tabs/#the-buffer-solution","text":"To reconcile all of this and learn how to use Vim\u2019s buffers/windows effectively, it might be useful to stop using tabs altogether until you understand how to edit with just using buffers/windows. The first thing I did was install a plugin that allows me to visualize all the buffers open across the top. I use bufferline for this. Instead of replicating tabs across the top like we did in the previous solution, we are instead going to use the power of being able to open many buffers simultaneously without worrying about which ones are open. In my experience, CtrlP gives a powerful fuzzy finder to navigate through the buffers. Instead of worrying about closing buffers and managing your pseudo-tabs that was mentioned in the previous solution, you just open files that you want to edit using CtrlP and don't worry about closing buffers or how many you have opened. When you are done editing a file, you just save it and then open CtrlP and continue onto the next file. CtrlP offers a few different ways to fuzzy find. You can use the following fuzziness: Find in your current directory. Find within all your open buffers. Find within all your open buffers sorted by Most Recently Used (MRU). Find with a mix of all the above.","title":"The Buffer Solution"},{"location":"vim_tabs/#using-tabs-correctly","text":"This doesn't mean you should stop using tabs altogether. You should just use them how Vim intended you to use them. Instead you should use them to change the layout among windows. Imagine you are working on a C project. It might be helpful to have one tab dedicated to normal editing, but another tab for using a vertical split for the file.h and file.c files to make editing between them easier. Tabs also work really well to divide up what you are working on. You could be working on one part of the project in one tab and another part of the project in another tab. Just remember though, if you are using a single tab for each file, that isn't how tabs in Vim were designed to be used.","title":"Using Tabs Correctly"},{"location":"vim_tabs/#default-option-when-switching","text":"The default behavior when trying to switch the buffer is to not allow you to change buffer if it's not saved, but we can change it if we set one of the next options: set hidden : allow to switch buffers even though it's changes aren't saved. set autowrite : Auto save when switching buffers.","title":"Default option when switching"},{"location":"vim_tabs/#share-buffers-and-all-vim-information-between-vim-instances","text":"This is not my ideal behavior, nvim should let the user use the window manager to manage the windows... duh, instead of vsplitting buffers or using tabs. But sadly as of Nvim 0.1.7 and Vim 8.0 it's not implemented . You have the --server option but it only sends files to the already opened vim instance. you can't connect two vim instances to the same buffer pool. It's been discussed in neovim 1 , 2 . Currently gVim cannot have separate 'toplevel' windows for the same process/session. There is a TODO item to implement an inter-process communication system between multiple Vim instances to make it behave as though the separate processes are unified. (See :help todo and search for \"top-level\".) There is an interesting hax formalized in here which I will want to have time to test. Another solution would be to try to use neovim remote","title":"Share buffers and all vim information between vim instances."},{"location":"virtual_assistant/","text":"Virtual assistant is a software agent that can perform tasks or services for an individual based on commands or questions. Of the open source solutions kalliope is the one I've liked most. I've also looked at mycroft but it seems less oriented to self hosted solutions, although it's possible . Mycroft has a bigger community behind though. To interact with it I may start with the android app , but then I'll probably install a Raspberry pi zero with Pirate Audio and an akaso external mic in the kitchen to speed up the grocy inventory management. STT \u2691 The only self hosted Speech-To-Text (STT) solution available now is CMUSphinx , which is based on pocketsphinx that has 2.8k stars but last update was on 28 th of March of 2020. The CMUSphinx documentation suggest you to use Vosk based on vosk-api with 1.2k stars and last updated 2 days ago. There is an open issue to support it in kalliope, with already a french proposal . That led me to the issue to support DeepSpeech , Mozilla's STT solution , that has 16.5k stars and updated 3 days ago, so it would be the way to go in my opinion if the existent one fails. Right now there is no support, but this would be the place to start. For spanish, based on the mozilla discourse thread I arrived to DeepSpeech-Polyglot that has taken many datasets such as Common Voice one and generated the models .","title":"Virtual Assistant"},{"location":"virtual_assistant/#stt","text":"The only self hosted Speech-To-Text (STT) solution available now is CMUSphinx , which is based on pocketsphinx that has 2.8k stars but last update was on 28 th of March of 2020. The CMUSphinx documentation suggest you to use Vosk based on vosk-api with 1.2k stars and last updated 2 days ago. There is an open issue to support it in kalliope, with already a french proposal . That led me to the issue to support DeepSpeech , Mozilla's STT solution , that has 16.5k stars and updated 3 days ago, so it would be the way to go in my opinion if the existent one fails. Right now there is no support, but this would be the place to start. For spanish, based on the mozilla discourse thread I arrived to DeepSpeech-Polyglot that has taken many datasets such as Common Voice one and generated the models .","title":"STT"},{"location":"vuejs/","text":"Vue.js is a progressive framework for building user interfaces. Unlike other monolithic frameworks, Vue is designed from the ground up to be incrementally adoptable. The core library is focused on the view layer only, and is easy to pick up and integrate with other libraries or existing projects. On the other hand, Vue is also perfectly capable of powering sophisticated Single-Page Applications when used in combination with modern tooling and supporting libraries. References \u2691 Homepage","title":"Vue.js"},{"location":"vuejs/#references","text":"Homepage","title":"References"},{"location":"wake_on_lan/","text":"Wake on LAN (WoL) is a feature to switch on a computer via the network. Usage \u2691 Host configuration \u2691 On the host you want to activate the wake on lan execute: $: ethtool *interface* | grep Wake-on Supports Wake-on: pumbag Wake-on: d The Wake-on values define what activity triggers wake up: d (disabled), p (PHY activity), u (unicast activity), m (multicast activity), b (broadcast activity), a (ARP activity), and g (magic packet activity). The value g is required for WoL to work, if not, the following command enables the WoL feature in the driver: $: ethtool -s interface wol g If it was not enabled check in the Arch wiki how to make the change persistent. To trigger WoL on a target machine, its MAC address must be known. To obtain it, execute the following command from the machine: $: ip link 1 : lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default link/loopback 00 :00:00:00:00:00 brd 00 :00:00:00:00:00 2 : enp1s0: <BROADCAST,MULTICAST,PROMISC,UP,LOWER_UP> mtu 1500 qdisc fq_codel master br0 state UP group default qlen 1000 link/ether 48 :05:ca:09:0e:6a brd ff:ff:ff:ff:ff:ff Here the MAC address is 48:05:ca:09:0e:6a . In its simplest form, Wake-on-LAN broadcasts the magic packet as an ethernet frame, containing the MAC address within the current network subnet, below the IP protocol layer. The knowledge of an IP address for the target computer is not necessary, as it operates on layer 2 (Data Link). If used to wake up a computer over the internet or in a different subnet, it typically relies on the router to relay the packet and broadcast it. In this scenario, the external IP address of the router must be known. Keep in mind that most routers by default will not relay subnet directed broadcasts as a safety precaution and need to be explicitly told to do so. Client trigger \u2691 If you are connected directly to another computer through a network cable, or the traffic within a LAN is not firewalled, then using Wake-on-LAN should be straightforward since there is no need to worry about port redirects. If it's firewalled you need to configure the client firewall to allow the outgoing UDP traffic to the port 9. In the simplest case the default broadcast address 255.255.255.255 is used: $ wakeonlan *target_MAC_address* To broadcast the magic packet only to a specific subnet or host, use the -i switch: $ wakeonlan -i *target_IP* *target_MAC_address* References \u2691 Arch wiki post","title":"Wake on Lan"},{"location":"wake_on_lan/#usage","text":"","title":"Usage"},{"location":"wake_on_lan/#host-configuration","text":"On the host you want to activate the wake on lan execute: $: ethtool *interface* | grep Wake-on Supports Wake-on: pumbag Wake-on: d The Wake-on values define what activity triggers wake up: d (disabled), p (PHY activity), u (unicast activity), m (multicast activity), b (broadcast activity), a (ARP activity), and g (magic packet activity). The value g is required for WoL to work, if not, the following command enables the WoL feature in the driver: $: ethtool -s interface wol g If it was not enabled check in the Arch wiki how to make the change persistent. To trigger WoL on a target machine, its MAC address must be known. To obtain it, execute the following command from the machine: $: ip link 1 : lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default link/loopback 00 :00:00:00:00:00 brd 00 :00:00:00:00:00 2 : enp1s0: <BROADCAST,MULTICAST,PROMISC,UP,LOWER_UP> mtu 1500 qdisc fq_codel master br0 state UP group default qlen 1000 link/ether 48 :05:ca:09:0e:6a brd ff:ff:ff:ff:ff:ff Here the MAC address is 48:05:ca:09:0e:6a . In its simplest form, Wake-on-LAN broadcasts the magic packet as an ethernet frame, containing the MAC address within the current network subnet, below the IP protocol layer. The knowledge of an IP address for the target computer is not necessary, as it operates on layer 2 (Data Link). If used to wake up a computer over the internet or in a different subnet, it typically relies on the router to relay the packet and broadcast it. In this scenario, the external IP address of the router must be known. Keep in mind that most routers by default will not relay subnet directed broadcasts as a safety precaution and need to be explicitly told to do so.","title":"Host configuration"},{"location":"wake_on_lan/#client-trigger","text":"If you are connected directly to another computer through a network cable, or the traffic within a LAN is not firewalled, then using Wake-on-LAN should be straightforward since there is no need to worry about port redirects. If it's firewalled you need to configure the client firewall to allow the outgoing UDP traffic to the port 9. In the simplest case the default broadcast address 255.255.255.255 is used: $ wakeonlan *target_MAC_address* To broadcast the magic packet only to a specific subnet or host, use the -i switch: $ wakeonlan -i *target_IP* *target_MAC_address*","title":"Client trigger"},{"location":"wake_on_lan/#references","text":"Arch wiki post","title":"References"},{"location":"week_management/","text":"I've been polishing a week reviewing and planning method that suits my needs. I usually follow it on Wednesdays, as I'm too busy on Mondays and Tuesdays and it gives enough time to plan the weekend. Until I've got pydo ready to natively incorporate all this processes, I heavily use taskwarrior to manage my tasks and logs. To make the process faster and reproducible, I've written small python scripts using tasklib. Week review \u2691 Life logging is the only purpose of my weekly review. I've made diw a small python script that for each overdue task allows me to: Review: Opens vim to write a diary entry related with the task. The text is saved as an annotation of the task and another form is filled to record whom I've shared it with. The last information is used to help me take care of people around me. Skip: Don't interact with this task. Done: Complete the task. Delete: Remove the task. Reschedule: Opens a form to specify the new due date. Week planning \u2691 The purpose of the planning is to make sure that I know what I need to do and arrange all tasks in a way that allows me not to explode. First I empty the INBOX file, refactoring all the information in the other knowledge sinks. It's the place to go to quickly gather information, such as movie/book/serie recommendations, human arrangements, miscellaneous thoughts or tasks. This file lives in my mobile. I edit it with Markor and transfer it to my computer with Share via HTTP . Taking different actions to each INBOX element type: Tasks or human arrangements: Do it if it can be completed in less than 3 minutes. Otherwise, create a taskwarrior task. Behavior: Add it to taskwarrior. Movie/Serie recommendation: Introduce it into my media monitorization system. Book recommendation: Introduce into my library management system. Miscellaneous thoughts: Refactor into the blue-book, project documentation or Anki. Then I split my workspace in two terminals, in the first I run task due.before:7d diary where diary is a taskwarrior report that shows pending tasks that are not in the backlog sorted by due date. On the other I: Execute gcal . to show the calendar of the previous, current and next month. Check the weather for the whole week to decide which plans are suitable. Analyze the tasks that need to be done answering the following questions: Do I need to do this task this week? If not, reschedule or delete it. Does it need a due date? If not, remove the due attribute. Having the minimum number of tasks with a fixed date reduces wasted rescheduling time and allows better prioritizing. Can I do the task on the selected date? As most humans, I tend to underestimate both the required time to complete a task and to switch contexts. To avoid it, If the day is full it's better to reschedule. Check that every day has at least one task. Particularly tasks that will help with life logging. If there aren't enough things to fill up all days, check the things that I want to do list and try to do one.","title":"Week Management"},{"location":"week_management/#week-review","text":"Life logging is the only purpose of my weekly review. I've made diw a small python script that for each overdue task allows me to: Review: Opens vim to write a diary entry related with the task. The text is saved as an annotation of the task and another form is filled to record whom I've shared it with. The last information is used to help me take care of people around me. Skip: Don't interact with this task. Done: Complete the task. Delete: Remove the task. Reschedule: Opens a form to specify the new due date.","title":"Week review"},{"location":"week_management/#week-planning","text":"The purpose of the planning is to make sure that I know what I need to do and arrange all tasks in a way that allows me not to explode. First I empty the INBOX file, refactoring all the information in the other knowledge sinks. It's the place to go to quickly gather information, such as movie/book/serie recommendations, human arrangements, miscellaneous thoughts or tasks. This file lives in my mobile. I edit it with Markor and transfer it to my computer with Share via HTTP . Taking different actions to each INBOX element type: Tasks or human arrangements: Do it if it can be completed in less than 3 minutes. Otherwise, create a taskwarrior task. Behavior: Add it to taskwarrior. Movie/Serie recommendation: Introduce it into my media monitorization system. Book recommendation: Introduce into my library management system. Miscellaneous thoughts: Refactor into the blue-book, project documentation or Anki. Then I split my workspace in two terminals, in the first I run task due.before:7d diary where diary is a taskwarrior report that shows pending tasks that are not in the backlog sorted by due date. On the other I: Execute gcal . to show the calendar of the previous, current and next month. Check the weather for the whole week to decide which plans are suitable. Analyze the tasks that need to be done answering the following questions: Do I need to do this task this week? If not, reschedule or delete it. Does it need a due date? If not, remove the due attribute. Having the minimum number of tasks with a fixed date reduces wasted rescheduling time and allows better prioritizing. Can I do the task on the selected date? As most humans, I tend to underestimate both the required time to complete a task and to switch contexts. To avoid it, If the day is full it's better to reschedule. Check that every day has at least one task. Particularly tasks that will help with life logging. If there aren't enough things to fill up all days, check the things that I want to do list and try to do one.","title":"Week planning"},{"location":"wesnoth/","text":"The Battle for Wesnoth is an open source, turn-based strategy game with a high fantasy theme. It features both singleplayer and online/hotseat multiplayer combat. Explore the world of Wesnoth and take part in its different adventures! Embark on a desperate quest to reclaim your rightful throne\u2026 Flee the Lich Lords to a new home across the sea\u2026 Delve into the darkest depths of the earth to craft a jewel of fire itself\u2026 Defend your kingdom against the ravaging hordes of a foul necromancer\u2026 Or lead a straggly band of survivors across the blazing sands to confront an unseen evil. References \u2691 Home Wiki Game manual Mainline campaigns","title":"The Battle for Wesnoth"},{"location":"wesnoth/#references","text":"Home Wiki Game manual Mainline campaigns","title":"References"},{"location":"wesnoth_loyalist/","text":"The Loyalist are a faction of Humans who are loyal to the throne of Wesnoth. The race of men is an extremely diverse one. Although they originally came from the Old Continent, men have spread all over the world and split into many different cultures and races. Although they are not imbued with magic like other creatures, humans can learn to wield it and able to learn more types than most others. They have no extra special abilities or aptitudes except their versatility and drive. While often at odds with all races, they can occasionally form alliances with the less aggressive races such as elves and dwarves. The less scrupulous among them do not shrink back from hiring orcish mercenaries, either. They have no natural enemies, although the majority of men, like most people of all races, have an instinctive dislike of the undead. Men are shorter than the elves, but taller still than dwarves. Their skin color can vary, from almost white to dark brown. Humans are a versatile race who specialize in many different areas. Similarly, the Loyalist faction is a very versatile melee oriented faction with important ranged support from bowmen and mages . How to play loyalists \u2691 Loyalists are considered to be the most versatile faction in the game. They have most available unit types to recruit (8), more than any other faction. Loyalists are mostly melee oriented faction, with only two ranged unit types, the mage and the bowman , but their ranged units play a significant role in the loyalists strategy and have to be used effectively. About the melee troops the loyalist player gets to choose among: Heavy Infantrymen : Few strikes, high damage per strike, slow, higher hit-points, good resistances, low defenses, deals impact damage which is good against undead. Spearmen : Average strikes, average damage per strike, average movement, medium hit-points, normal resistances, average defenses, deals pierce damage which is good against drakes, has a weak ranged attack. Fencer : High number of strikes, low damage per strike, quick, low hit-points, bad resistances, good defenses, deals blade damage which is good against elusive foots or wose, deals less total damage than the other two, is a skirmisher. The loyalists unit price ranges from the cheap spearmen and mermen (14 gold) to the expensive horsemen (23 gold). Loyalist units generally have good hit-points and they deal good damage, even the cheap spearmen and bowmen . Even their scouts have high hit-points and better melee attack compared to most of the scouts of the other factions. But on the other hand, the loyalist scouts have lower defenses than most of the other scouts and they do not have a ranged attack. The loyalists have some units with good resistances, like Heavy Infantry or Cavalryman , which can be very good for defending if you notice that your opponent doesn't have the proper units to counter them. Other units, like the bowman , who is more vulnerable than the previous two, but is also good for defense because it has better defense, is cheaper and it deals solid damage back in both melee and range. Similar goes for the spearman . When attacking , Loyalists have units which can be pretty devastating, like the horseman for attacking enemy mages and other ranged units, mages against the entrenched melee units, fencers for killing enemy injured and valuable units trying to heal behind their lines. But the mentioned units are also very vulnerable and they can die very easily if not properly supported. The loyalists as a faction generally have average defenses (they do not get 70% defense anywhere, with the exception of the fencer, which is a special case) and they are not a terrain dependent faction (like the Elves or Knalgans are). Therefore, even if it good to put spearmen and bowmen on mountains, villages and castles where they get 60% defense, you should also take the faction you are facing into account when deciding where to place your units. For example, if you are fighting Knalgans, you should try not to move your units next to hills and mountains and/or take the hills and mountains with your units and leave the forest to the dwarves. All the loyalist units are lawful, which obviously leads to the conclusion that the loyalists should be more aggressive and attack at day (or even start at dawn, because your opponent in most of the cases will be reluctant to attack too aggressively and leave units on bad ground when the morning arrives) while at night the loyalists should at least play conservatively, or in some cases even run away. Of course, that is also dependent upon the faction of your opponent. The greatest weakness of the loyalists is their limited mobility. The loyalists' units have normal movement, some of their units like the horseman , cavalryman , merman fighter and the fencer are pretty quick when they are in their favorable environment, but all their units get slowed over rough terrain a lot compared to the other factions' units . Half of the loyalists units can not even move on/over mountains, and their scouts get significantly slowed in forest, for example. This lack of mobility will often give your opponent an opportunity to outmaneuver you in the game.To prevent that, sometimes it is good to attack hard on a certain point and defend only with minimal amount of units elsewhere, rather than uniformly spreading your units around the front. When you don't know which faction you are facing, it is good to get a mixture of units. On most of the default 1v1 maps, this initial recruitment list would be good : spearman , bowman , fencer , mage , merman fighter , cavalryman . As always, you should carefully consider about the exact placement of each individual unit from the initial recruits on the castle to come up with the most optimal combination that will allow you the quickest village grabbing in the first turns of the game. Loyalists vs. Undead \u2691 When fighting undead, your recruiting pattern will depend on whether your opponent spams skeleton archers or dark adepts for ranged attacks. If you see lots of skeletons, you'll have to focus heavily on heavy infrantrymen and mages . If you see lots of dark adepts , you'll want some cavalry , horsemen and maybe some spearmen . Bowman : ( D- ) Almost entirely useless. You might use them to poke at walking corpses , ghouls , vampire bats and ghosts , but mages are much better at all of these things. I would only buy a bowman to counter a large zombie horde. Cavalryman : ( B+ ) You'll want some cavalry for scouting and dealing initial damage to dark adepts so that a horseman can finish them. They are also decent at fighting Skeletons in the daytime, because cavalry are blade-resistant. However, the cheap skeleton archers will really ruin cavalry quickly at night, so if there are any skeleton archers on the other side you won't be able to use cavalry to hold territory. If your opponent over-recruits dark adepts , however, you can use cold-resistant cavalry to hold territory against them. They are also decent for holding territory against ghouls because they can run away and heal (unlike your heavy infantrymen ). Fencer : ( C- ) Fencers are a bad recruit in this match up because they are vulnerable to the blade and pierce damage of skeletons and cannot damage them much in return. They also are incapable of holding territory against dark adepts , who cut right through the high defense of fencers . However, you may want to have a fencer or two around for trapping dark adepts or getting in that last hit. With luck, they may also be able to frustrate non-magical attacks from skeletons and the like. Heavy Infantryman : ( B+ ) You need heavy infantry to hold territory against skeletons and skeleton archers , and they will be your unit of choice for dealing melee damage, especially to the cheap skeleton archers . Any heavy infantrymen in the daytime can kill a full health skeleton archer in two hits, while a strong heavy infantrymen in the daytime can kill a full health skeleton in two hits, dealing 18 damage per strike (even a mage in daytime cannot kill a skeleton in one round, unless supported by a lieutenant ). A fearless heavy infantryman may be dangerous even at night. If you don't have enough heavy infantryman to go around, you can get your initial hits in on a skeleton archer with them and finish him with a mage . Just keep heavy infantrymen away from dark adepts , and only let ghouls hit heavy infantrymen if they are on a village (or has a white mage or other healer next to him), since they can't easily run away to heal. Also beware walking corpses , which deal a surprising amount of damage at all times of day since heavy infantrymen can't dodge and impact damage goes around their resistances, and the ranged cold attack of ghosts . The biggest problem with heavy infantrymen is they are slow, which means they're hard to retreat at night and hard to advance in day. Without shielding units they'll get trapped and killed, and if you have to shield a unit maybe it should be a mage instead. Horseman : ( B ) - Because they deal pierce damage, horsemen may not be very useful when faced with skeletons . However, if your opponent over-recruits dark adepts , horsemen can be extremely useful, as dark adepts deal no return damage to the normally risky charge attack. horsemen can even be used to finish skeleton archers , their nemesis, in the daytime. However, if your opponent recruits enough skeleton archers you will have a hard time shielding your horsemen from their devastating pierce attacks, and skeleton archers are dirt cheap. horsemen can also one-shot bats and zombies , which can be useful if you need to clear out a lot of level 0 units quickly. I would want to have at least one horseman around to keep my opponent from getting too bold with dark adepts , if not more. Your opponent will be forced to recruit dark adepts if you have heavy infantrymen in the field. Mage : ( A+ ) mages are an absolute necessity against Undead. If you do not have mages it will be almost impossible for you to kill ghosts , but with mages it's a piece of cake. mages are the best unit for killing almost everything Undead can throw at you, and can even be used to finish dark adepts in the daytime. Your main problem is that dark adepts are cheaper and deal almost as much damage, so your opponent can spam dark adepts while you cannot afford to spam mages . You will also have the difficult task of shielding fragile, expensive mages against lots of cheap Undead units. Your opponent will use skeletons and ghouls to attack your mages when he can, but bats , zombies or just about any other unit will do for killing your mages in a pinch. Shield your mages well, surround them with damage soakers and if you can deliver them safely to their targets you'll be able to clear out the Undead quickly. Merman Fighter : ( C- ) Mermen make a decent navy against Undead, since bats and ghosts will have a hard time killing them with their high defense. Even dark adepts will find Mermen annoying because of their 20% cold resistance. However, Mermen will have a hard time hurting anything the Undead have with their lame pierce weapon. Generally Mermen are only good for holding water hexes and scouting, but don't underestimate how useful that can be. Some well-placed Mermen on a water map can prevent bats from sneaking behind your lines and capturing villages or killing injured units. Even on mostly land maps, a Merman in a river can help hold a defensive line, or a quick Merman can use a river to slip behind the enemy to trap dark adepts or other units that are trying to escape at daytime. Spearman : ( C- ) Spearmen are mostly useful as cheap units for holding villages and occupying space when faced with dark adepts or skeleton archers . (You'll want to avoid letting dark adepts hit your heavy infantrymen because of their vulnerability to cold.) However, you don't really want spearmen to take hits from dark adepts , it would be better to let the cold-resistant cavalry absorb the damage. The only units spearmen are good for attacking are dark adepts and walking corpses . spearmen are completely useless against skeletons unless you level one into a swordsman , and even then they're pretty mediocre. However, if there are lots of skeleton archers you won't be able to use much cavalry or horsemen , so a spearman or two may be necessary as defenders and damage soakers even if they are lousy at dealing damage to Undead. Loyalists vs. Rebels \u2691 Rebels are a very versatile faction like the Loyalists. So you need to counter their versatility with yours. Their weakest trait is low hit-points. That is compensated somewhat by relatively higher defenses and mobility, so you'll need a combination of hard-hitters and speed. mages are also nice for their magical attacks. Anyways, the key for a Loyalist general is to effectively use his hard-hitting units and kill as many units as he can at day. And at night, to defend well as elves ignore the time of day. The smartest thing you can do upon discovering that your opponent is playing Rebels, in multiplayer, is to assume that your opponent has woses and recruit accordingly. woses are a necessary counter to Loyalists' daytime attacks and as soon as your opponent realizes that you are Loyalists he will almost certainly recruit woses . Remember that just because you don't see any woses doesn't mean there aren't a couple hiding in the forest! To fight woses you will need cavalry as meat shields, mages to deal damage, and maybe fencers to get the last hit in. Sadly, cavalry are very vulnerable to elvish archers , so you'll need to make sure that the archers die, which can be difficult if they are in forest. Get horsemen which can one-shot archers (despite their high dodge) and keep up with your cavalry . If one horseman misses, follow up with another one, it's a lucky archer that can dodge 4/4 one-shot attempts. heavy infantrymen are also great against everything but woses and mages , and will reduce the effectiveness of archer spam. spearmen are useless against woses , recruit them sparingly and keep them away from unscouted forest. An alternative way of playing this match up is to maximize the mobility offered by the Loyalists' fastest units, namely the two horse units. By using them to outrun the Rebel units you can achieve some devastating rushes. Some matches require use of both styles of playing. Again, their versatility must be countered with yours. Bowman : ( C- ) Rebels in general are effective in both melee and ranged attacks, so recruiting a ranged unit is less beneficial than most factions because most of the Rebels' units can retaliate against you. They can be useful for taking out an Scout or two, but otherwise, this is not really a smart buy. Cavalryman : ( B+ ) A good scout and an effective counter to woses . Watch out for archers, though, they can really tear horses to shreds. And as always, they are effective against ranged oriented units like the mage . ( A ) If you're going for some faster action, cavalrymen are vital in the attack. They do great damage during the day, and combined with their dangerous mobility, they can be a fearsome unit indeed. Just be careful of archers ZoCing you and tearing the unit to shreds. Fencer : ( B+ ) The fencer shares the 70% defense in the forest like most of the elves, but it has negative resistances. Theoretically, they should be effective against woses , but more often than not woses will crush them easily. In spite of this, Fencers still have skirmsher, which means that they can sneak behind an injured unit and finish them off. Do not over-recruit them, as enemy mages will tear though their high defense. Heavy Infantryman : ( C- ) Usefulness similar to an archer. It's too slow to deal with most of the Rebel units, and it is owned by Mages , woses , and even archers. Even though it has a high damage potential and good resistances, because of its inherently low defense shamans can easily get hits on it and turn it into a piece of metal for a turn. Horseman : ( B ) One horseman may be nice, but no more than that. Enemy archers will tear them apart and shamans totally mess them up. They're expensive too. Their greatest use is probably killing mages or an archer that has gone slightly off-track. ( A- ) Again, when mobility is required, these units need to come and do serious damage. Since Rebels are mostly comprised of low-hp units, horsemen at day can usually rip them apart. Again be careful of archers, for this unit is worth 23 gold. Shield him properly if he is damaged. Mage : ( A- ) You'll need these guys to tear though those high elvish 60-70% evasion in the forest; but be careful, archers and shamans will retaliate and some pretty nasty things may come from that. One purely positive thing though, they just absolutely destroy woses . 13-3 at day. Ouch. mages are expensive and fragile though, so keep them protected. Merman Fighter : ( B+ ) If the map has a lot of water, maybe recruit a few to prevent the Rebel's Mermen Hunters from taking over the waters. Otherwise they don't really contribute much else. Spearman - A - A necessity. To defend at night, to kill pretty much anything except for woses , and to be cheap and cost only 14 gold. These guys pretty much tear though most of the Rebel units, if it were not only for the high-defense archer and shaman . Get a bunch, and move them like a wall against the enemy units. At the start of the game, recruit 1-2 cavalrymen , depending on the map size, 1 mage , (1 merman fighter if you need some water coverage), maybe 1 fencer , and the rest spearman . Later on you maybe can recruit some horseman if you opponent recruits mass mages , or more cavalrymen and mages if he masses woses . Otherwise, spearmen and mages should help you get through most of the match. If you're going for speed, recruit 2-3 cavalrymen , depending on the map, 1 horseman , maybe 1 fencer , and the rest spearmen . Use your spearmen to fill the holes and consolidate the territory that the horses took over. Use the horses to outrun the archers and attack when they can. Loyalists vs Northerners \u2691 The main problem you will have when facing northerners are two things: poison and numbers. If northerners didn't have assassins , it wouldn't be much of a big deal; you could out manoeuvre them with your superior scouting units and skirmishers to grab their villages and then finish them off one by one with spearmen / bowmen . Thing is, assassins poison makes defending at night extremely difficult, and by poisoning your units, they often force you to retreat and heal, or die due to retaliation. The key to winning this match up is to use your superior versatility; the northerners greatest weakness is that ALL of their units are very average and don't often have many special abilities; you have masses. Use your first strike in spearmen to defend against grunts , trolls and wolfriders (even at night), use your magic to finish off wounded assassins , and you can charge any of their wounded units and kill them with one hit. Since all of your units are lawful and theirs are chaotic, you'll want to press your advantage at day and kill as much as possible (but don't attack assassins with bowmen ; you don't want to be poisoned and be forced to retreat in your daytime assault). Another problem the northerners have is that all though their units are many in number, they do not have a very high damage per hex ratio. Use this to your advantage; travel in packs and use ZoC lines to prevent your units from being swarmed too easily, and keep your vulnerable units like mages behind damage soakers like spearmen or heavy infantry . If you can, try and make night time defenses near rivers; northerners will hurt themselves when they attack you, as they have low defense in water and are very annoyed by deep water, because it prevents them from easily swarming you. Spearmen : ( A- ) Pretty much your best unit against northerners. Their first strike ability is great in defense and their attack power to cost ratio is quite high. They also have good health and can retaliate in ranged. As in many matchups, the bulk of your army. Mage : ( B+ ) The mage is fragile and expensive and has a weak melee attack, which is the exact opposite of northerners. Not a great defender unless your opponent goes mad with assassins. However, they have lots of attack power at day and are very useful for nocking trolls of their perches and finishing off those damn assassins . You'll want a few of these, but make sure you keep them protected. Bowmen : ( B ) Good unit to have. They can attack grunts and trolls (although you'd want mages to attack trolls ) without retaliation, and can defend against assassins in the night. They can also take out any wolves that stray too far from the main orchish army. They're not as good as mages , but they're cheaper and tougher. Cavalryman : ( B ) cavalryman are superior to their wolves and can hold ground against grunts and trolls reasonably well. It's also a good idea if your opponent likes to spam lots of assassins , to let them be the target of their poison; cavalryman and can run away and heal. Your heavy infantrymen ? Not so much. Beware though, of the cheap orchish archer, as cavalrymen are weak to pierce. Heavy Infantryman : ( B- ) Heavy infantryman are heavily resistant to physical attacks like blade, pierce and to a lesser degree, impact. You'd think this would make them great units against northerners, if the orchish archer didn't have a ranged fire attack. heavy infantrymen are also much too slow too effectively deal with poison, and that's a 19 gold unit that can't fight. However, they are useful in defense if you opponent hasn't spammed many archers, and they can even be useful in attack to crush up injured grunts . Horseman : ( B ) The horseman is a little controversial in this matchup. northerners are cheap and melee orientated, which is exactly what horseman do badly against. They're also quite tough, which means one hit kills are rare. However, they have one very important advantage over the northerners- high damage per hex. A horseman is very useful for softening up units or outright killing them (particularly when paired with a mage) which will be important in breaking ZoC lines, which can be a real pain with all the units northerners can field. Get one or two, but no more, else it quickly produces diminishing returns. Fencer : ( C+ ) The fencer is a melee based unit that is fragile against blade attacks of grunts , which means they don't have an awful lot of fighting effectiveness with them. On the other hand, the fencer's skirmisher ability is really valuable with the many northener units, and can finish off injured archers or go on sneaky village grabbing incursions. One or two might be useful, but no more, this unit is not a serious fighter! Make sure you keep them on 70% terrain if you can as well. Two hitter grunts can have trouble taking them out. Merman Fighter : ( C+ ) Water based melee unit that doesn't do well against the orc nagas , recruit only if there's lots of water and keep them in defendible terrain, else they'll quickly be killed. As usual, recruit cavalrymen and mermen depending on map (3-4 cavalryman for larger maps, 2-3 for medium sized ones and probably no more than one or two for small maps). Recruit plenty of spearmen and attack at day. Speaking of that, make sure you travel in large groups and do not attack with small numbers even at day. Northerners will swarm you too easily. Also, make sure you don't overextend yourself at day, as northerners do a lot more damage at night. Pay careful attention to your mages , they will be the ones the northerners will be going after, so retreat them quickly. When defending, try to avoid attacking units that can retaliate, unless you have a reasonably high chance of killing them that turn. Avoid attacking trolls if you can't heavily damage them or outright kill them, they're regenaration will quickly mean they can heal from any scratches your bowmen might have dealt them.","title":"Loyalist"},{"location":"wesnoth_loyalist/#how-to-play-loyalists","text":"Loyalists are considered to be the most versatile faction in the game. They have most available unit types to recruit (8), more than any other faction. Loyalists are mostly melee oriented faction, with only two ranged unit types, the mage and the bowman , but their ranged units play a significant role in the loyalists strategy and have to be used effectively. About the melee troops the loyalist player gets to choose among: Heavy Infantrymen : Few strikes, high damage per strike, slow, higher hit-points, good resistances, low defenses, deals impact damage which is good against undead. Spearmen : Average strikes, average damage per strike, average movement, medium hit-points, normal resistances, average defenses, deals pierce damage which is good against drakes, has a weak ranged attack. Fencer : High number of strikes, low damage per strike, quick, low hit-points, bad resistances, good defenses, deals blade damage which is good against elusive foots or wose, deals less total damage than the other two, is a skirmisher. The loyalists unit price ranges from the cheap spearmen and mermen (14 gold) to the expensive horsemen (23 gold). Loyalist units generally have good hit-points and they deal good damage, even the cheap spearmen and bowmen . Even their scouts have high hit-points and better melee attack compared to most of the scouts of the other factions. But on the other hand, the loyalist scouts have lower defenses than most of the other scouts and they do not have a ranged attack. The loyalists have some units with good resistances, like Heavy Infantry or Cavalryman , which can be very good for defending if you notice that your opponent doesn't have the proper units to counter them. Other units, like the bowman , who is more vulnerable than the previous two, but is also good for defense because it has better defense, is cheaper and it deals solid damage back in both melee and range. Similar goes for the spearman . When attacking , Loyalists have units which can be pretty devastating, like the horseman for attacking enemy mages and other ranged units, mages against the entrenched melee units, fencers for killing enemy injured and valuable units trying to heal behind their lines. But the mentioned units are also very vulnerable and they can die very easily if not properly supported. The loyalists as a faction generally have average defenses (they do not get 70% defense anywhere, with the exception of the fencer, which is a special case) and they are not a terrain dependent faction (like the Elves or Knalgans are). Therefore, even if it good to put spearmen and bowmen on mountains, villages and castles where they get 60% defense, you should also take the faction you are facing into account when deciding where to place your units. For example, if you are fighting Knalgans, you should try not to move your units next to hills and mountains and/or take the hills and mountains with your units and leave the forest to the dwarves. All the loyalist units are lawful, which obviously leads to the conclusion that the loyalists should be more aggressive and attack at day (or even start at dawn, because your opponent in most of the cases will be reluctant to attack too aggressively and leave units on bad ground when the morning arrives) while at night the loyalists should at least play conservatively, or in some cases even run away. Of course, that is also dependent upon the faction of your opponent. The greatest weakness of the loyalists is their limited mobility. The loyalists' units have normal movement, some of their units like the horseman , cavalryman , merman fighter and the fencer are pretty quick when they are in their favorable environment, but all their units get slowed over rough terrain a lot compared to the other factions' units . Half of the loyalists units can not even move on/over mountains, and their scouts get significantly slowed in forest, for example. This lack of mobility will often give your opponent an opportunity to outmaneuver you in the game.To prevent that, sometimes it is good to attack hard on a certain point and defend only with minimal amount of units elsewhere, rather than uniformly spreading your units around the front. When you don't know which faction you are facing, it is good to get a mixture of units. On most of the default 1v1 maps, this initial recruitment list would be good : spearman , bowman , fencer , mage , merman fighter , cavalryman . As always, you should carefully consider about the exact placement of each individual unit from the initial recruits on the castle to come up with the most optimal combination that will allow you the quickest village grabbing in the first turns of the game.","title":"How to play loyalists"},{"location":"wesnoth_loyalist/#loyalists-vs-undead","text":"When fighting undead, your recruiting pattern will depend on whether your opponent spams skeleton archers or dark adepts for ranged attacks. If you see lots of skeletons, you'll have to focus heavily on heavy infrantrymen and mages . If you see lots of dark adepts , you'll want some cavalry , horsemen and maybe some spearmen . Bowman : ( D- ) Almost entirely useless. You might use them to poke at walking corpses , ghouls , vampire bats and ghosts , but mages are much better at all of these things. I would only buy a bowman to counter a large zombie horde. Cavalryman : ( B+ ) You'll want some cavalry for scouting and dealing initial damage to dark adepts so that a horseman can finish them. They are also decent at fighting Skeletons in the daytime, because cavalry are blade-resistant. However, the cheap skeleton archers will really ruin cavalry quickly at night, so if there are any skeleton archers on the other side you won't be able to use cavalry to hold territory. If your opponent over-recruits dark adepts , however, you can use cold-resistant cavalry to hold territory against them. They are also decent for holding territory against ghouls because they can run away and heal (unlike your heavy infantrymen ). Fencer : ( C- ) Fencers are a bad recruit in this match up because they are vulnerable to the blade and pierce damage of skeletons and cannot damage them much in return. They also are incapable of holding territory against dark adepts , who cut right through the high defense of fencers . However, you may want to have a fencer or two around for trapping dark adepts or getting in that last hit. With luck, they may also be able to frustrate non-magical attacks from skeletons and the like. Heavy Infantryman : ( B+ ) You need heavy infantry to hold territory against skeletons and skeleton archers , and they will be your unit of choice for dealing melee damage, especially to the cheap skeleton archers . Any heavy infantrymen in the daytime can kill a full health skeleton archer in two hits, while a strong heavy infantrymen in the daytime can kill a full health skeleton in two hits, dealing 18 damage per strike (even a mage in daytime cannot kill a skeleton in one round, unless supported by a lieutenant ). A fearless heavy infantryman may be dangerous even at night. If you don't have enough heavy infantryman to go around, you can get your initial hits in on a skeleton archer with them and finish him with a mage . Just keep heavy infantrymen away from dark adepts , and only let ghouls hit heavy infantrymen if they are on a village (or has a white mage or other healer next to him), since they can't easily run away to heal. Also beware walking corpses , which deal a surprising amount of damage at all times of day since heavy infantrymen can't dodge and impact damage goes around their resistances, and the ranged cold attack of ghosts . The biggest problem with heavy infantrymen is they are slow, which means they're hard to retreat at night and hard to advance in day. Without shielding units they'll get trapped and killed, and if you have to shield a unit maybe it should be a mage instead. Horseman : ( B ) - Because they deal pierce damage, horsemen may not be very useful when faced with skeletons . However, if your opponent over-recruits dark adepts , horsemen can be extremely useful, as dark adepts deal no return damage to the normally risky charge attack. horsemen can even be used to finish skeleton archers , their nemesis, in the daytime. However, if your opponent recruits enough skeleton archers you will have a hard time shielding your horsemen from their devastating pierce attacks, and skeleton archers are dirt cheap. horsemen can also one-shot bats and zombies , which can be useful if you need to clear out a lot of level 0 units quickly. I would want to have at least one horseman around to keep my opponent from getting too bold with dark adepts , if not more. Your opponent will be forced to recruit dark adepts if you have heavy infantrymen in the field. Mage : ( A+ ) mages are an absolute necessity against Undead. If you do not have mages it will be almost impossible for you to kill ghosts , but with mages it's a piece of cake. mages are the best unit for killing almost everything Undead can throw at you, and can even be used to finish dark adepts in the daytime. Your main problem is that dark adepts are cheaper and deal almost as much damage, so your opponent can spam dark adepts while you cannot afford to spam mages . You will also have the difficult task of shielding fragile, expensive mages against lots of cheap Undead units. Your opponent will use skeletons and ghouls to attack your mages when he can, but bats , zombies or just about any other unit will do for killing your mages in a pinch. Shield your mages well, surround them with damage soakers and if you can deliver them safely to their targets you'll be able to clear out the Undead quickly. Merman Fighter : ( C- ) Mermen make a decent navy against Undead, since bats and ghosts will have a hard time killing them with their high defense. Even dark adepts will find Mermen annoying because of their 20% cold resistance. However, Mermen will have a hard time hurting anything the Undead have with their lame pierce weapon. Generally Mermen are only good for holding water hexes and scouting, but don't underestimate how useful that can be. Some well-placed Mermen on a water map can prevent bats from sneaking behind your lines and capturing villages or killing injured units. Even on mostly land maps, a Merman in a river can help hold a defensive line, or a quick Merman can use a river to slip behind the enemy to trap dark adepts or other units that are trying to escape at daytime. Spearman : ( C- ) Spearmen are mostly useful as cheap units for holding villages and occupying space when faced with dark adepts or skeleton archers . (You'll want to avoid letting dark adepts hit your heavy infantrymen because of their vulnerability to cold.) However, you don't really want spearmen to take hits from dark adepts , it would be better to let the cold-resistant cavalry absorb the damage. The only units spearmen are good for attacking are dark adepts and walking corpses . spearmen are completely useless against skeletons unless you level one into a swordsman , and even then they're pretty mediocre. However, if there are lots of skeleton archers you won't be able to use much cavalry or horsemen , so a spearman or two may be necessary as defenders and damage soakers even if they are lousy at dealing damage to Undead.","title":"Loyalists vs. Undead"},{"location":"wesnoth_loyalist/#loyalists-vs-rebels","text":"Rebels are a very versatile faction like the Loyalists. So you need to counter their versatility with yours. Their weakest trait is low hit-points. That is compensated somewhat by relatively higher defenses and mobility, so you'll need a combination of hard-hitters and speed. mages are also nice for their magical attacks. Anyways, the key for a Loyalist general is to effectively use his hard-hitting units and kill as many units as he can at day. And at night, to defend well as elves ignore the time of day. The smartest thing you can do upon discovering that your opponent is playing Rebels, in multiplayer, is to assume that your opponent has woses and recruit accordingly. woses are a necessary counter to Loyalists' daytime attacks and as soon as your opponent realizes that you are Loyalists he will almost certainly recruit woses . Remember that just because you don't see any woses doesn't mean there aren't a couple hiding in the forest! To fight woses you will need cavalry as meat shields, mages to deal damage, and maybe fencers to get the last hit in. Sadly, cavalry are very vulnerable to elvish archers , so you'll need to make sure that the archers die, which can be difficult if they are in forest. Get horsemen which can one-shot archers (despite their high dodge) and keep up with your cavalry . If one horseman misses, follow up with another one, it's a lucky archer that can dodge 4/4 one-shot attempts. heavy infantrymen are also great against everything but woses and mages , and will reduce the effectiveness of archer spam. spearmen are useless against woses , recruit them sparingly and keep them away from unscouted forest. An alternative way of playing this match up is to maximize the mobility offered by the Loyalists' fastest units, namely the two horse units. By using them to outrun the Rebel units you can achieve some devastating rushes. Some matches require use of both styles of playing. Again, their versatility must be countered with yours. Bowman : ( C- ) Rebels in general are effective in both melee and ranged attacks, so recruiting a ranged unit is less beneficial than most factions because most of the Rebels' units can retaliate against you. They can be useful for taking out an Scout or two, but otherwise, this is not really a smart buy. Cavalryman : ( B+ ) A good scout and an effective counter to woses . Watch out for archers, though, they can really tear horses to shreds. And as always, they are effective against ranged oriented units like the mage . ( A ) If you're going for some faster action, cavalrymen are vital in the attack. They do great damage during the day, and combined with their dangerous mobility, they can be a fearsome unit indeed. Just be careful of archers ZoCing you and tearing the unit to shreds. Fencer : ( B+ ) The fencer shares the 70% defense in the forest like most of the elves, but it has negative resistances. Theoretically, they should be effective against woses , but more often than not woses will crush them easily. In spite of this, Fencers still have skirmsher, which means that they can sneak behind an injured unit and finish them off. Do not over-recruit them, as enemy mages will tear though their high defense. Heavy Infantryman : ( C- ) Usefulness similar to an archer. It's too slow to deal with most of the Rebel units, and it is owned by Mages , woses , and even archers. Even though it has a high damage potential and good resistances, because of its inherently low defense shamans can easily get hits on it and turn it into a piece of metal for a turn. Horseman : ( B ) One horseman may be nice, but no more than that. Enemy archers will tear them apart and shamans totally mess them up. They're expensive too. Their greatest use is probably killing mages or an archer that has gone slightly off-track. ( A- ) Again, when mobility is required, these units need to come and do serious damage. Since Rebels are mostly comprised of low-hp units, horsemen at day can usually rip them apart. Again be careful of archers, for this unit is worth 23 gold. Shield him properly if he is damaged. Mage : ( A- ) You'll need these guys to tear though those high elvish 60-70% evasion in the forest; but be careful, archers and shamans will retaliate and some pretty nasty things may come from that. One purely positive thing though, they just absolutely destroy woses . 13-3 at day. Ouch. mages are expensive and fragile though, so keep them protected. Merman Fighter : ( B+ ) If the map has a lot of water, maybe recruit a few to prevent the Rebel's Mermen Hunters from taking over the waters. Otherwise they don't really contribute much else. Spearman - A - A necessity. To defend at night, to kill pretty much anything except for woses , and to be cheap and cost only 14 gold. These guys pretty much tear though most of the Rebel units, if it were not only for the high-defense archer and shaman . Get a bunch, and move them like a wall against the enemy units. At the start of the game, recruit 1-2 cavalrymen , depending on the map size, 1 mage , (1 merman fighter if you need some water coverage), maybe 1 fencer , and the rest spearman . Later on you maybe can recruit some horseman if you opponent recruits mass mages , or more cavalrymen and mages if he masses woses . Otherwise, spearmen and mages should help you get through most of the match. If you're going for speed, recruit 2-3 cavalrymen , depending on the map, 1 horseman , maybe 1 fencer , and the rest spearmen . Use your spearmen to fill the holes and consolidate the territory that the horses took over. Use the horses to outrun the archers and attack when they can.","title":"Loyalists vs. Rebels"},{"location":"wesnoth_loyalist/#loyalists-vs-northerners","text":"The main problem you will have when facing northerners are two things: poison and numbers. If northerners didn't have assassins , it wouldn't be much of a big deal; you could out manoeuvre them with your superior scouting units and skirmishers to grab their villages and then finish them off one by one with spearmen / bowmen . Thing is, assassins poison makes defending at night extremely difficult, and by poisoning your units, they often force you to retreat and heal, or die due to retaliation. The key to winning this match up is to use your superior versatility; the northerners greatest weakness is that ALL of their units are very average and don't often have many special abilities; you have masses. Use your first strike in spearmen to defend against grunts , trolls and wolfriders (even at night), use your magic to finish off wounded assassins , and you can charge any of their wounded units and kill them with one hit. Since all of your units are lawful and theirs are chaotic, you'll want to press your advantage at day and kill as much as possible (but don't attack assassins with bowmen ; you don't want to be poisoned and be forced to retreat in your daytime assault). Another problem the northerners have is that all though their units are many in number, they do not have a very high damage per hex ratio. Use this to your advantage; travel in packs and use ZoC lines to prevent your units from being swarmed too easily, and keep your vulnerable units like mages behind damage soakers like spearmen or heavy infantry . If you can, try and make night time defenses near rivers; northerners will hurt themselves when they attack you, as they have low defense in water and are very annoyed by deep water, because it prevents them from easily swarming you. Spearmen : ( A- ) Pretty much your best unit against northerners. Their first strike ability is great in defense and their attack power to cost ratio is quite high. They also have good health and can retaliate in ranged. As in many matchups, the bulk of your army. Mage : ( B+ ) The mage is fragile and expensive and has a weak melee attack, which is the exact opposite of northerners. Not a great defender unless your opponent goes mad with assassins. However, they have lots of attack power at day and are very useful for nocking trolls of their perches and finishing off those damn assassins . You'll want a few of these, but make sure you keep them protected. Bowmen : ( B ) Good unit to have. They can attack grunts and trolls (although you'd want mages to attack trolls ) without retaliation, and can defend against assassins in the night. They can also take out any wolves that stray too far from the main orchish army. They're not as good as mages , but they're cheaper and tougher. Cavalryman : ( B ) cavalryman are superior to their wolves and can hold ground against grunts and trolls reasonably well. It's also a good idea if your opponent likes to spam lots of assassins , to let them be the target of their poison; cavalryman and can run away and heal. Your heavy infantrymen ? Not so much. Beware though, of the cheap orchish archer, as cavalrymen are weak to pierce. Heavy Infantryman : ( B- ) Heavy infantryman are heavily resistant to physical attacks like blade, pierce and to a lesser degree, impact. You'd think this would make them great units against northerners, if the orchish archer didn't have a ranged fire attack. heavy infantrymen are also much too slow too effectively deal with poison, and that's a 19 gold unit that can't fight. However, they are useful in defense if you opponent hasn't spammed many archers, and they can even be useful in attack to crush up injured grunts . Horseman : ( B ) The horseman is a little controversial in this matchup. northerners are cheap and melee orientated, which is exactly what horseman do badly against. They're also quite tough, which means one hit kills are rare. However, they have one very important advantage over the northerners- high damage per hex. A horseman is very useful for softening up units or outright killing them (particularly when paired with a mage) which will be important in breaking ZoC lines, which can be a real pain with all the units northerners can field. Get one or two, but no more, else it quickly produces diminishing returns. Fencer : ( C+ ) The fencer is a melee based unit that is fragile against blade attacks of grunts , which means they don't have an awful lot of fighting effectiveness with them. On the other hand, the fencer's skirmisher ability is really valuable with the many northener units, and can finish off injured archers or go on sneaky village grabbing incursions. One or two might be useful, but no more, this unit is not a serious fighter! Make sure you keep them on 70% terrain if you can as well. Two hitter grunts can have trouble taking them out. Merman Fighter : ( C+ ) Water based melee unit that doesn't do well against the orc nagas , recruit only if there's lots of water and keep them in defendible terrain, else they'll quickly be killed. As usual, recruit cavalrymen and mermen depending on map (3-4 cavalryman for larger maps, 2-3 for medium sized ones and probably no more than one or two for small maps). Recruit plenty of spearmen and attack at day. Speaking of that, make sure you travel in large groups and do not attack with small numbers even at day. Northerners will swarm you too easily. Also, make sure you don't overextend yourself at day, as northerners do a lot more damage at night. Pay careful attention to your mages , they will be the ones the northerners will be going after, so retreat them quickly. When defending, try to avoid attacking units that can retaliate, unless you have a reasonably high chance of killing them that turn. Avoid attacking trolls if you can't heavily damage them or outright kill them, they're regenaration will quickly mean they can heal from any scratches your bowmen might have dealt them.","title":"Loyalists vs Northerners"},{"location":"wesnoth_northerners/","text":"Northerners are a faction of Orcs and their allies who live in the north of the Great Continent, thus their name. Northerners consist of the warrior orcs race, the enslaved goblins, trolls who are tricked into combat by the orcs, and the serpentine naga. The Northerners play best by taking advantage of having many low-cost and high HP soldiers.","title":"Northeners"},{"location":"wesnoth_rebels/","text":"Rebels are a faction of Elves and their various forest-dwelling allies. They get their human name, Rebels, from the time of Heir to the Throne, when they started the rebellion against the evil Queen Asheviere. Elves are a magical race that are masters of the bow and are capable of living many years longer than humans. In harmony with nature, the elves find allies with the human mages, certain merfolk, and tree creatures called woses. Rebels are best played taking advantage of their high forest defense, mastery of ranged attacks, and the elves' neutral alignment.","title":"Rebels"},{"location":"work_interruption_analysis/","text":"This is the interruption analysis report applied to my everyday work. I've identified the next interruption sources: Physical interruptions . Emails . Calls . Instant message applications . Calendar events . Physical interruptions \u2691 Physical interactions are when someone comes to your desk and expect you to attend them immediately. These interruptions can be categorized as: Asking for help . Social interactions . The obvious solution is to remote work as much as possible. It's less easy for people to interrupt through digital channels than physically. It goes the other way around too. Be respectful to your colleagues and try to use asynchronous communications as much as possible, so they can manage when they attend you. Asking for help \u2691 These interruptions are the most difficult to delay, as it's hard to tell a person to wait when it's already in front of you. If you don't take care of them you may end up in the situation where you can receive 5 o 6 interruptions per minute which can drive you crazy. By definition all these events require an immediate action. The priority and delay may depend on many factors, such as the person or moment. The first thing I'd do is make a mental prioritization of the people that interrupt you, to decide which ones do you accept and which ones you need to regulate. Once you have it, work on how to assertively tell them that they need to reduce their interruptions. You can agree with them a non interruption time where they can aggregate and prepare all the questions so you can work through them efficiently. Often they are able to answer most of them themselves. The length of the period needs to be picked wisely as you want to be interrupted the minimum number of times while you don't make them loose their time trying to solve something you could work out quickly. Other times it's easier to forward them to the team's interruption manager . Social interactions \u2691 Depending how popular you are, you'll have more or less of these interactions. The way I've found to be able to be in control of them is by scheduling social events in my calendar and introducing them in my task management workflow. For example, we agree to go to have lunch all together at the same hour every day, or I arrange a coffee break with someone every Monday at a defined hour. Emails \u2691 Email can be used as one of the main aggregators of interruptions as it's supported by almost everything. I use it as the notification of things that don't need to be acted upon immediately or when more powerful mechanisms are not available. In my case emails can be categorized as: General information: They don't usually require any direct action, so they can wait more than 24 hours. Support to internal agents: At work, we have decided that email is not to be used as the internal main communication channel, so I don't receive many and their priority is low. Support to external agents: I'm lucky to not have many of these and they have less priority than internal people so they can wait 4 or more hours. Infrastructure notifications: For example LetsEncrypt renewals or cloud provider notification or support cases. The related actions can wait 4 hours or more. Calendar events: Someone creates a new meeting, changes an existing one or confirms/declines its assistance. We have defined a policy that we don't create or change events with less than 24 hours notice, and in the special cases that we need to, they will be addressed in the chat rooms. So these mails can be read once per day. Monitorization notifications: We've configured Prometheus's alertmanager to send the notifications to the email as a fallback channel, but it's to be checked only if the main channel is down. Source code manager notifications: The web where we host our source code sends us emails when there are new pull requests or when there are comments on existent ones. I automatically mark them as read and move them to a mail directory as I manage these interruptions with other workflow. The CI sends notifications when some job fails. Unless it's a new pipeline or I'm actively working on it, a failed job can wait four hours broken before I interact with it. The issue tracker notifications: It sends them on new or changed issues. At work, I filter them out as I delegate it's management to the Scrum Master. In conclusion, I can check the work email only when I start working, on the lunch break and when I'm about to leave. So its safe to disable the notifications. I'm eager to start the email automation project so I can spend even less time and willpower managing the email. Calls \u2691 We've agreed that the calls are the communication channel used only for critical situations, similar to the physical interruptions , they are synchronous so they're more difficult to manage. As calls are very rare and of high priority, I have my phone configured to ring on incoming calls. Have a work phone independent of your personal Nowadays you can have phone contracts of 0$/month used only to receive calls. Remember to give it to the fewer people as possible. Instant messages \u2691 It's the main internal communication channel, so it has a great volume of events with a wide range of priorities. They can be categorized as: Asking for help through direct messages: We don't have many as we've agreed to use groups as much as possible . So they have high priority and I have the notifications enabled. Social interaction through direct messages: I don't have many as I try to arrange one on one meetings instead , so they have a low priority. As notifications are defined for all direct messages, I inherit the notifications from the last category. Team group or support rooms: We've defined the interruption role so I check them whenever an chosen interruption event comes. If I'm assuming the role I enable the notifications on the channel, if not I'll check them whenever I check the application. Information rooms: They have no priority and can be checked each 4 hours. In conclusion, I can check the work chat applications each pomodoro cycle or when I receive a direct notification until the improve the notification management in Linux project is ready. Calendar events \u2691 Often with a wide range of priorities. decide if you have to go Define an agenda with times","title":"Work Interruption Analysis"},{"location":"work_interruption_analysis/#physical-interruptions","text":"Physical interactions are when someone comes to your desk and expect you to attend them immediately. These interruptions can be categorized as: Asking for help . Social interactions . The obvious solution is to remote work as much as possible. It's less easy for people to interrupt through digital channels than physically. It goes the other way around too. Be respectful to your colleagues and try to use asynchronous communications as much as possible, so they can manage when they attend you.","title":"Physical interruptions"},{"location":"work_interruption_analysis/#asking-for-help","text":"These interruptions are the most difficult to delay, as it's hard to tell a person to wait when it's already in front of you. If you don't take care of them you may end up in the situation where you can receive 5 o 6 interruptions per minute which can drive you crazy. By definition all these events require an immediate action. The priority and delay may depend on many factors, such as the person or moment. The first thing I'd do is make a mental prioritization of the people that interrupt you, to decide which ones do you accept and which ones you need to regulate. Once you have it, work on how to assertively tell them that they need to reduce their interruptions. You can agree with them a non interruption time where they can aggregate and prepare all the questions so you can work through them efficiently. Often they are able to answer most of them themselves. The length of the period needs to be picked wisely as you want to be interrupted the minimum number of times while you don't make them loose their time trying to solve something you could work out quickly. Other times it's easier to forward them to the team's interruption manager .","title":"Asking for help"},{"location":"work_interruption_analysis/#social-interactions","text":"Depending how popular you are, you'll have more or less of these interactions. The way I've found to be able to be in control of them is by scheduling social events in my calendar and introducing them in my task management workflow. For example, we agree to go to have lunch all together at the same hour every day, or I arrange a coffee break with someone every Monday at a defined hour.","title":"Social interactions"},{"location":"work_interruption_analysis/#emails","text":"Email can be used as one of the main aggregators of interruptions as it's supported by almost everything. I use it as the notification of things that don't need to be acted upon immediately or when more powerful mechanisms are not available. In my case emails can be categorized as: General information: They don't usually require any direct action, so they can wait more than 24 hours. Support to internal agents: At work, we have decided that email is not to be used as the internal main communication channel, so I don't receive many and their priority is low. Support to external agents: I'm lucky to not have many of these and they have less priority than internal people so they can wait 4 or more hours. Infrastructure notifications: For example LetsEncrypt renewals or cloud provider notification or support cases. The related actions can wait 4 hours or more. Calendar events: Someone creates a new meeting, changes an existing one or confirms/declines its assistance. We have defined a policy that we don't create or change events with less than 24 hours notice, and in the special cases that we need to, they will be addressed in the chat rooms. So these mails can be read once per day. Monitorization notifications: We've configured Prometheus's alertmanager to send the notifications to the email as a fallback channel, but it's to be checked only if the main channel is down. Source code manager notifications: The web where we host our source code sends us emails when there are new pull requests or when there are comments on existent ones. I automatically mark them as read and move them to a mail directory as I manage these interruptions with other workflow. The CI sends notifications when some job fails. Unless it's a new pipeline or I'm actively working on it, a failed job can wait four hours broken before I interact with it. The issue tracker notifications: It sends them on new or changed issues. At work, I filter them out as I delegate it's management to the Scrum Master. In conclusion, I can check the work email only when I start working, on the lunch break and when I'm about to leave. So its safe to disable the notifications. I'm eager to start the email automation project so I can spend even less time and willpower managing the email.","title":"Emails"},{"location":"work_interruption_analysis/#calls","text":"We've agreed that the calls are the communication channel used only for critical situations, similar to the physical interruptions , they are synchronous so they're more difficult to manage. As calls are very rare and of high priority, I have my phone configured to ring on incoming calls. Have a work phone independent of your personal Nowadays you can have phone contracts of 0$/month used only to receive calls. Remember to give it to the fewer people as possible.","title":"Calls"},{"location":"work_interruption_analysis/#instant-messages","text":"It's the main internal communication channel, so it has a great volume of events with a wide range of priorities. They can be categorized as: Asking for help through direct messages: We don't have many as we've agreed to use groups as much as possible . So they have high priority and I have the notifications enabled. Social interaction through direct messages: I don't have many as I try to arrange one on one meetings instead , so they have a low priority. As notifications are defined for all direct messages, I inherit the notifications from the last category. Team group or support rooms: We've defined the interruption role so I check them whenever an chosen interruption event comes. If I'm assuming the role I enable the notifications on the channel, if not I'll check them whenever I check the application. Information rooms: They have no priority and can be checked each 4 hours. In conclusion, I can check the work chat applications each pomodoro cycle or when I receive a direct notification until the improve the notification management in Linux project is ready.","title":"Instant messages"},{"location":"work_interruption_analysis/#calendar-events","text":"Often with a wide range of priorities. decide if you have to go Define an agenda with times","title":"Calendar events"},{"location":"write_neovim_plugins/","text":"plugin example plugin repo The plugin repo has some examples in the tests directory Control an existing nvim instance \u2691 A number of different transports are supported, but the simplest way to get started is with the python REPL. First, start Nvim with a known address (or use the $NVIM_LISTEN_ADDRESS of a running instance): $ NVIM_LISTEN_ADDRESS = /tmp/nvim nvim In another terminal, connect a python REPL to Nvim (note that the API is similar to the one exposed by the python-vim bridge : >>> from neovim import attach # Create a python API session attached to unix domain socket created above: >>> nvim = attach ( 'socket' , path = '/tmp/nvim' ) # Now do some work. >>> buffer = nvim . current . buffer # Get the current buffer >>> buffer [ 0 ] = 'replace first line' >>> buffer [:] = [ 'replace whole buffer' ] >>> nvim . command ( 'vsplit' ) >>> nvim . windows [ 1 ] . width = 10 >>> nvim . vars [ 'global_var' ] = [ 1 , 2 , 3 ] >>> nvim . eval ( 'g:global_var' ) [ 1 , 2 , 3 ] Load buffer \u2691 buffer = nvim . current . buffer # Get the current buffer buffer [ 0 ] = 'replace first line' buffer [:] = [ 'replace whole buffer' ] Get cursor position \u2691 nvim . current . window . cursor","title":"Write Neovim Plugins"},{"location":"write_neovim_plugins/#control-an-existing-nvim-instance","text":"A number of different transports are supported, but the simplest way to get started is with the python REPL. First, start Nvim with a known address (or use the $NVIM_LISTEN_ADDRESS of a running instance): $ NVIM_LISTEN_ADDRESS = /tmp/nvim nvim In another terminal, connect a python REPL to Nvim (note that the API is similar to the one exposed by the python-vim bridge : >>> from neovim import attach # Create a python API session attached to unix domain socket created above: >>> nvim = attach ( 'socket' , path = '/tmp/nvim' ) # Now do some work. >>> buffer = nvim . current . buffer # Get the current buffer >>> buffer [ 0 ] = 'replace first line' >>> buffer [:] = [ 'replace whole buffer' ] >>> nvim . command ( 'vsplit' ) >>> nvim . windows [ 1 ] . width = 10 >>> nvim . vars [ 'global_var' ] = [ 1 , 2 , 3 ] >>> nvim . eval ( 'g:global_var' ) [ 1 , 2 , 3 ]","title":"Control an existing nvim instance"},{"location":"write_neovim_plugins/#load-buffer","text":"buffer = nvim . current . buffer # Get the current buffer buffer [ 0 ] = 'replace first line' buffer [:] = [ 'replace whole buffer' ]","title":"Load buffer"},{"location":"write_neovim_plugins/#get-cursor-position","text":"nvim . current . window . cursor","title":"Get cursor position"},{"location":"writing_style/","text":"Writing style is the manner of expressing thought in language characteristic of an individual, period, school, or nation. It's defined by the grammatical choices writers make, the importance of adhering to norms in certain contexts and deviating from them in others, the expression of social identity, and the emotional effects of particular devices on audiences. Beyond the essential elements of spelling, grammar, and punctuation, writing style is the choice of words, sentence structure, and paragraph structure, used to convey the meaning effectively. The point of good writing style is to: Express the message to the reader simply, clearly, and convincingly. Keep the reader attentive, engaged, and interested. Not to Display the writer's personality. Demonstrate the writer's skills, knowledge, or abilities. General writing principles \u2691 Make it pleasant to the reader \u2691 Writing is a medium of communication, so avoid introducing elements that push away the reader, such as: Spelling mistakes. Gender favoring, polarizing, race related, religion inconsiderate, or other unequal phrasing. Ugly environment: Present your texts through a pleasant medium such as a mkdocs webpage. Write like you talk: Ask yourself, is this the way I'd say this if I were talking to a friend? . If it isn't, imagine what you would say, and use that instead. Format errors: If you're writing in markdown, make sure that the result has no display bugs. Write short articles: Even though I love Gwern site , I find it daunting most of times. Instead of a big post, I'd rather use multiple well connected articles. Saying more with less \u2691 Never use a long word where a short one will do. Replace words like really like with love or other more appropriate words that save space writing and are more meaningful. Don't use filler words like really . Be aware of pacing \u2691 Be aware of pacing between words and sentences. The sentences ideally should flow into one another. Breaks in form of commas and full steps are important, as they allow for the reader to take a break and absorb the point that you tried to deliver. Try to use less tan 30 words per sentence. For example, change Due to the fact that to because . One purpose \u2691 A good piece of writing has a single, sharp, overriding purpose. Every part of the writing, even the digressions, should serve that purpose. Put another way, clarity of the general purpose is an absolute requirement in a good piece of writing. This observation matters because it's often tempting to let your purpose expand and become vague. Writing a piece about gardens? Hey, why not include that important related thought you had about rainforests? Now you have a piece that's sort of about gardens and sort of about rainforests, and not really about anything. The reader can no longer bond to it. A complicating factor is that sometimes you need to explore beyond the boundaries of your current purpose. You're writing for purpose A, but your instinct says that you need to explore subject B. Unfortunately, you're not yet sure how subject B fits in. If that's the case then you must take time to explore, and to understand how, if at all, subject B fits in, and whether you need to revise your purpose. This is emotionally difficult. It creates uncertainty, and you may feel as though your work on subject B is wasted effort. These doubts must be resisted. Avoid using clich\u00e9s \u2691 Clich\u00e9s prevent readers from visualization , making them an obstacle to creating memorable writing. Citing the sources \u2691 If it's a small phrase or a refactor, link the source inside the phrase or at the header of the section. If it's a big refactor, add it to a references section. If it's a big block without editing use admonition quotes Take all the guidelines as suggestions \u2691 All the sections above are guidelines, not rules to follow blindly, I try to adhere to them as much as possible, but if I feel it doesn't apply I ignore them. Unconnected thoughts \u2691 Replace adjectives with data. Nearly all of -> 84% of . Remove weasel words . Most adverbs are superfluous. When you say \"generally\" or \"usually\" you're probably undermining your point and the use of \"very\" or \"extremely\" are hyperbolic and breathless and make it easier to regard what you're writing as not serious. Examine every word: a surprising number don't serve any purpose. While wrapping your content into a story you may find yourself talking about your achievements more than giving actionable advice. If that happens, try to get to the bottom of how you achieved these achievements and break this process down, then focus on the process more than on your personal achievement. Set up a system that prompts people to review the material. Don't be egocentric, limit the use of I , use the implied subject instead: It's where I go to -> It's the place to go. I take different actions -> Taking different actions . Don't be possessive, use the instead of my . If you don't know how to express something use services like deepl . Use synonyms instead of repeating the same word over and over. Think who are you writing to. Use active voice : Active voice ensures that the actors are identified and it generally leaves less open questions. The exception is if you want to emphasize the object of the sentence. How to end a letter \u2691 How you end a letter is important. It\u2019s your last chance to leave the reader with positive feelings about you and the letter you have written. To make the matter more difficult, each different closing phrase has subtle connotations attached to them that you need to know to use them well. Most formal letter closing options are reserved, but note that there are degrees of warmth and familiarity among the options. Your relationship with the person to whom you\u2019re writing will shape which closing you choose: If you don\u2019t know the individual to whom you\u2019re writing, stick with a professional formal closing. If you\u2019re writing to a colleague, business connection, or someone else you know well, it\u2019s fine to close your letter less formally. Above all, your closing should be appropriate. Ideally, your message will resonate instead of your word choice. TL;DR: You can select from: Simplest, most useful : Sincerely Regards Yours truly Yours sincerely Slightly more personal : Best regards Cordially Yours respectfully More personal : Only use when appropriate to the letter's content. Warm regards Best wishes With appreciation Letter closings to avoid : Always Cheers Love Take Care XOXO The following are letter closings that are appropriate for business and employment related letters. Sincerely, Regards, Yours truly, and Yours sincerely : These are the simplest and most useful letter closings to use in a formal business setting. These are appropriate in almost all instances and are excellent ways to close a cover letter or an inquiry. Best regards, Cordially, and Yours respectfully : These letter closings fill the need for something slightly more personal. They are appropriate once you have some knowledge of the person to whom you are writing. You may have corresponded via email a few times, had a face-to-face or phone interview, or met at a networking event. Warm regards, Best wishes, and With appreciation : These letter closings are also appropriate once you have some knowledge or connection to the person to whom you are writing. Because they can relate back to the content of the letter, they can give closure to the point of the letter. Only use these if they make sense with the content of your letter. Letter closings to avoid \u2691 There are certain closings that you want to avoid in any business letter. Most of these are simply too informal. Some examples of closings to avoid are listed below: Always, Cheers, Love, Take care, XOXO, Talk soon, See ya, Hugs Some closings (such as \u201cLove\u201d and \u201cXOXO\u201d) imply a level of closeness that is not appropriate for a business letter. Rule of thumb : if you would use the closing in a note to a close friend, it\u2019s probably not suitable for business correspondence. Punctuating Farewell Phrases \u2691 When writing your sign-off, it's important to remember to use proper capitalization and punctuation. Only the first word should be capitalized (e.g., Yours truly), and the sign-off should be followed by a comma (or an exclamation mark in some informal settings), not a period. Postscripts \u2691 A P.S. (or postscript) comes after your sign-off and name. It is meant to include material that is supplementary, subordinated, or not vital to your letter. It is best to avoid postscripts in formal writing, as the information may go unnoticed or ignored; in those cases, try to include all information in the body text of the letter. n casual and personal correspondences, a postscript is generally acceptable. However, try to limit it to include only humorous or unnecessary material. Letter closings in detail \u2691 Sincerely \u2691 Sincerely (or sincerely yours) is often the go-to sign off for formal letters, and with good reason. This ending restates the sincerity of your letter's intent; it is a safe choice if you are not overly familiar with the letter's recipient, as it's preferable to use a sign-off that is both common and formal in such a situation. Best \u2691 Ending your letter with best, all the best, all best, or best wishes indicates that you hope the recipient experiences only good things in the future. Although it is not quite as formal as sincerely, it is still acceptable as a polite, formal/semi-formal letter ending, proper for business contacts as well as friends. Best regards \u2691 Quite like the previous sign-off, best regards expresses that you are thinking of the recipient with the best of feelings and intentions. Despite its similarity to best, this sign-off is a little more formal, meant for business letters and unfamiliar contacts. A semi-formal variation is warm regards, and an even more formal variation is simply regards. Speak to you soon \u2691 Variations to this farewell phrase include see you soon, talk to you later, and looking forward to speaking with you soon. These sign-offs indicate that you are expecting to continue the conversation with your contact. It can be an effective ending to a letter or email when confirming or planning a specific date for a face-to-face meeting. Although these endings can be used in either formal or casual settings, they typically carry a more formal tone. The exception here is talk to you later, which errs on the more casual side. Thanks \u2691 This is an effective ending to a letter when you are sincerely expressing gratitude. If you are using it as your standard letter ending, however, it can fall flat; the reader will be confused if there is no reason for you to be thanking them. Try to use thanks (or variations such as thanks so much, thank you, or thanks!) and its variations only when you think you haven't expressed your gratitude enough; otherwise, it can come across as excessive. Furthermore, when you're issuing an order, thanks might not be the best sign-off because it can seem presumptuous to offer thanks before the task has even been accepted or begun. [No sign-off] \u2691 Having no sign-off for your letter is a little unusual, but it is acceptable in some cases. Omitting the sign-off is most appropriately used in cases where you are replying to an email chain. However, in a first email, including neither a sign-off nor your name will make your letter seem to end abruptly. It should be avoided in those situations or when you are not very familiar with the receiver. Yours truly \u2691 This is where the line between formal and informal begins to blur. Yours truly implies the integrity of the message that precedes your name, but it also implies that you are related to the recipient in some way. This ending can be used in various situations, when writing letters to people both familiar and unfamiliar to you; however, yours truly carries a more casual and familiar tone, making it most appropriate for your friends and family. It's best used when you want to emphasize that you mean the contents of your letter. Take care \u2691 Take care is also a semi-formal way to end your letter. Like the sign-off all the best, this ending wishes that no harm come to the reader; however, like ending your letter with yours truly, the word choice is less formal and implies that the writer is at least somewhat familiar with the reader. Cheers \u2691 Cheers is a lighthearted ending that expresses your best wishes for the reader. Due to its association with drinking alcohol, it's best to save this sign-off for cases where you are familiar with the reader and when the tone is optimistic and casual. Also note that because cheers is associated with British English, it may seem odd to readers who speak other styles of English and are not very familiar with the term. Style issues \u2691 Avoid there is at the start of the sentence \u2691 Almost never begin a sentence with \u201cIt is...\u201d or \u201cThere is/are...\u201d. These are examples of unnecessary verbiage that shift the focus from the sentence point. Writing style books \u2691 After you start writing every day professionally, you will see that you will face some hard problems that will haunt you every time you sit down to write. The simplest way to overcome these issues and adopt a philosophy of writing that will make you a more professional, resilient, and wiser writer is to read the books about writing that masters of the craft have published. After reviewing 1 , 2 and 3 I've come to the following list of books I'd like to read. The elements of style \u2691 A classic book on grammar, style, and punctuation. If you feel like you need to improve any of those three aspects of your writing, then this book is a great start. With only 85 pages it covers both the grammar basics, rules that affect the style composition, writing toolbox description, and styling recommendations. On writing well \u2691 They say this book is specially useful to find one's style, develop it, polish it and learn how to write with it. The author doesn't get too philosophical or cutesy in his concepts, neither he gets too technical. In a way, it provides the right balance between The Elements of Style and Bird by Bird . Reading the book feels like you\u2019re being mentored by a wise, highly experienced writer. Bird by bird \u2691 Supposedly the most touching, poetic, and psychological book of the collection. The first part of the book lays around the life of Anne Lamott, a relatively popular fiction writer, who happens to have had a quite interesting life. Just like On Writing (the first book mentioned in here), the author manages to share enough of her life to enlighten the story and thesis of the book. The author explains what it takes to be a writer, what it means to be one, and how you can develop a narrative for a fiction book or story. It looks like it's a real pleasure to read it at the same time as it\u2019s still a wonderful experience that will help you understand how you can overcome your own fears, doubts, and pains of writing. Whether you want to write fiction or nonfiction, Bird by Bird provides a beautiful reading experience that will teach you what it takes to be a writer and how to find your demons. On writing \u2691 It's a book written by Stephen King that even though I haven't read any of his books I know he is known for being a specialist in capturing the reader. I don't know if it's going to be too much oriented to writing novels, but it looks promising. I'll leave it there for now, but keep on reading on Ivan Kreimer's article for more suggestions. References \u2691 Ivan Kreimer's article","title":"Writing Style"},{"location":"writing_style/#general-writing-principles","text":"","title":"General writing principles"},{"location":"writing_style/#make-it-pleasant-to-the-reader","text":"Writing is a medium of communication, so avoid introducing elements that push away the reader, such as: Spelling mistakes. Gender favoring, polarizing, race related, religion inconsiderate, or other unequal phrasing. Ugly environment: Present your texts through a pleasant medium such as a mkdocs webpage. Write like you talk: Ask yourself, is this the way I'd say this if I were talking to a friend? . If it isn't, imagine what you would say, and use that instead. Format errors: If you're writing in markdown, make sure that the result has no display bugs. Write short articles: Even though I love Gwern site , I find it daunting most of times. Instead of a big post, I'd rather use multiple well connected articles.","title":"Make it pleasant to the reader"},{"location":"writing_style/#saying-more-with-less","text":"Never use a long word where a short one will do. Replace words like really like with love or other more appropriate words that save space writing and are more meaningful. Don't use filler words like really .","title":"Saying more with less"},{"location":"writing_style/#be-aware-of-pacing","text":"Be aware of pacing between words and sentences. The sentences ideally should flow into one another. Breaks in form of commas and full steps are important, as they allow for the reader to take a break and absorb the point that you tried to deliver. Try to use less tan 30 words per sentence. For example, change Due to the fact that to because .","title":"Be aware of pacing"},{"location":"writing_style/#one-purpose","text":"A good piece of writing has a single, sharp, overriding purpose. Every part of the writing, even the digressions, should serve that purpose. Put another way, clarity of the general purpose is an absolute requirement in a good piece of writing. This observation matters because it's often tempting to let your purpose expand and become vague. Writing a piece about gardens? Hey, why not include that important related thought you had about rainforests? Now you have a piece that's sort of about gardens and sort of about rainforests, and not really about anything. The reader can no longer bond to it. A complicating factor is that sometimes you need to explore beyond the boundaries of your current purpose. You're writing for purpose A, but your instinct says that you need to explore subject B. Unfortunately, you're not yet sure how subject B fits in. If that's the case then you must take time to explore, and to understand how, if at all, subject B fits in, and whether you need to revise your purpose. This is emotionally difficult. It creates uncertainty, and you may feel as though your work on subject B is wasted effort. These doubts must be resisted.","title":"One purpose"},{"location":"writing_style/#avoid-using-cliches","text":"Clich\u00e9s prevent readers from visualization , making them an obstacle to creating memorable writing.","title":"Avoid using clich\u00e9s"},{"location":"writing_style/#citing-the-sources","text":"If it's a small phrase or a refactor, link the source inside the phrase or at the header of the section. If it's a big refactor, add it to a references section. If it's a big block without editing use admonition quotes","title":"Citing the sources"},{"location":"writing_style/#take-all-the-guidelines-as-suggestions","text":"All the sections above are guidelines, not rules to follow blindly, I try to adhere to them as much as possible, but if I feel it doesn't apply I ignore them.","title":"Take all the guidelines as suggestions"},{"location":"writing_style/#unconnected-thoughts","text":"Replace adjectives with data. Nearly all of -> 84% of . Remove weasel words . Most adverbs are superfluous. When you say \"generally\" or \"usually\" you're probably undermining your point and the use of \"very\" or \"extremely\" are hyperbolic and breathless and make it easier to regard what you're writing as not serious. Examine every word: a surprising number don't serve any purpose. While wrapping your content into a story you may find yourself talking about your achievements more than giving actionable advice. If that happens, try to get to the bottom of how you achieved these achievements and break this process down, then focus on the process more than on your personal achievement. Set up a system that prompts people to review the material. Don't be egocentric, limit the use of I , use the implied subject instead: It's where I go to -> It's the place to go. I take different actions -> Taking different actions . Don't be possessive, use the instead of my . If you don't know how to express something use services like deepl . Use synonyms instead of repeating the same word over and over. Think who are you writing to. Use active voice : Active voice ensures that the actors are identified and it generally leaves less open questions. The exception is if you want to emphasize the object of the sentence.","title":"Unconnected thoughts"},{"location":"writing_style/#how-to-end-a-letter","text":"How you end a letter is important. It\u2019s your last chance to leave the reader with positive feelings about you and the letter you have written. To make the matter more difficult, each different closing phrase has subtle connotations attached to them that you need to know to use them well. Most formal letter closing options are reserved, but note that there are degrees of warmth and familiarity among the options. Your relationship with the person to whom you\u2019re writing will shape which closing you choose: If you don\u2019t know the individual to whom you\u2019re writing, stick with a professional formal closing. If you\u2019re writing to a colleague, business connection, or someone else you know well, it\u2019s fine to close your letter less formally. Above all, your closing should be appropriate. Ideally, your message will resonate instead of your word choice. TL;DR: You can select from: Simplest, most useful : Sincerely Regards Yours truly Yours sincerely Slightly more personal : Best regards Cordially Yours respectfully More personal : Only use when appropriate to the letter's content. Warm regards Best wishes With appreciation Letter closings to avoid : Always Cheers Love Take Care XOXO The following are letter closings that are appropriate for business and employment related letters. Sincerely, Regards, Yours truly, and Yours sincerely : These are the simplest and most useful letter closings to use in a formal business setting. These are appropriate in almost all instances and are excellent ways to close a cover letter or an inquiry. Best regards, Cordially, and Yours respectfully : These letter closings fill the need for something slightly more personal. They are appropriate once you have some knowledge of the person to whom you are writing. You may have corresponded via email a few times, had a face-to-face or phone interview, or met at a networking event. Warm regards, Best wishes, and With appreciation : These letter closings are also appropriate once you have some knowledge or connection to the person to whom you are writing. Because they can relate back to the content of the letter, they can give closure to the point of the letter. Only use these if they make sense with the content of your letter.","title":"How to end a letter"},{"location":"writing_style/#letter-closings-to-avoid","text":"There are certain closings that you want to avoid in any business letter. Most of these are simply too informal. Some examples of closings to avoid are listed below: Always, Cheers, Love, Take care, XOXO, Talk soon, See ya, Hugs Some closings (such as \u201cLove\u201d and \u201cXOXO\u201d) imply a level of closeness that is not appropriate for a business letter. Rule of thumb : if you would use the closing in a note to a close friend, it\u2019s probably not suitable for business correspondence.","title":"Letter closings to avoid"},{"location":"writing_style/#punctuating-farewell-phrases","text":"When writing your sign-off, it's important to remember to use proper capitalization and punctuation. Only the first word should be capitalized (e.g., Yours truly), and the sign-off should be followed by a comma (or an exclamation mark in some informal settings), not a period.","title":"Punctuating Farewell Phrases"},{"location":"writing_style/#postscripts","text":"A P.S. (or postscript) comes after your sign-off and name. It is meant to include material that is supplementary, subordinated, or not vital to your letter. It is best to avoid postscripts in formal writing, as the information may go unnoticed or ignored; in those cases, try to include all information in the body text of the letter. n casual and personal correspondences, a postscript is generally acceptable. However, try to limit it to include only humorous or unnecessary material.","title":"Postscripts"},{"location":"writing_style/#letter-closings-in-detail","text":"","title":"Letter closings in detail"},{"location":"writing_style/#sincerely","text":"Sincerely (or sincerely yours) is often the go-to sign off for formal letters, and with good reason. This ending restates the sincerity of your letter's intent; it is a safe choice if you are not overly familiar with the letter's recipient, as it's preferable to use a sign-off that is both common and formal in such a situation.","title":"Sincerely"},{"location":"writing_style/#best","text":"Ending your letter with best, all the best, all best, or best wishes indicates that you hope the recipient experiences only good things in the future. Although it is not quite as formal as sincerely, it is still acceptable as a polite, formal/semi-formal letter ending, proper for business contacts as well as friends.","title":"Best"},{"location":"writing_style/#best-regards","text":"Quite like the previous sign-off, best regards expresses that you are thinking of the recipient with the best of feelings and intentions. Despite its similarity to best, this sign-off is a little more formal, meant for business letters and unfamiliar contacts. A semi-formal variation is warm regards, and an even more formal variation is simply regards.","title":"Best regards"},{"location":"writing_style/#speak-to-you-soon","text":"Variations to this farewell phrase include see you soon, talk to you later, and looking forward to speaking with you soon. These sign-offs indicate that you are expecting to continue the conversation with your contact. It can be an effective ending to a letter or email when confirming or planning a specific date for a face-to-face meeting. Although these endings can be used in either formal or casual settings, they typically carry a more formal tone. The exception here is talk to you later, which errs on the more casual side.","title":"Speak to you soon"},{"location":"writing_style/#thanks","text":"This is an effective ending to a letter when you are sincerely expressing gratitude. If you are using it as your standard letter ending, however, it can fall flat; the reader will be confused if there is no reason for you to be thanking them. Try to use thanks (or variations such as thanks so much, thank you, or thanks!) and its variations only when you think you haven't expressed your gratitude enough; otherwise, it can come across as excessive. Furthermore, when you're issuing an order, thanks might not be the best sign-off because it can seem presumptuous to offer thanks before the task has even been accepted or begun.","title":"Thanks"},{"location":"writing_style/#no-sign-off","text":"Having no sign-off for your letter is a little unusual, but it is acceptable in some cases. Omitting the sign-off is most appropriately used in cases where you are replying to an email chain. However, in a first email, including neither a sign-off nor your name will make your letter seem to end abruptly. It should be avoided in those situations or when you are not very familiar with the receiver.","title":"[No sign-off]"},{"location":"writing_style/#yours-truly","text":"This is where the line between formal and informal begins to blur. Yours truly implies the integrity of the message that precedes your name, but it also implies that you are related to the recipient in some way. This ending can be used in various situations, when writing letters to people both familiar and unfamiliar to you; however, yours truly carries a more casual and familiar tone, making it most appropriate for your friends and family. It's best used when you want to emphasize that you mean the contents of your letter.","title":"Yours truly"},{"location":"writing_style/#take-care","text":"Take care is also a semi-formal way to end your letter. Like the sign-off all the best, this ending wishes that no harm come to the reader; however, like ending your letter with yours truly, the word choice is less formal and implies that the writer is at least somewhat familiar with the reader.","title":"Take care"},{"location":"writing_style/#cheers","text":"Cheers is a lighthearted ending that expresses your best wishes for the reader. Due to its association with drinking alcohol, it's best to save this sign-off for cases where you are familiar with the reader and when the tone is optimistic and casual. Also note that because cheers is associated with British English, it may seem odd to readers who speak other styles of English and are not very familiar with the term.","title":"Cheers"},{"location":"writing_style/#style-issues","text":"","title":"Style issues"},{"location":"writing_style/#avoid-there-is-at-the-start-of-the-sentence","text":"Almost never begin a sentence with \u201cIt is...\u201d or \u201cThere is/are...\u201d. These are examples of unnecessary verbiage that shift the focus from the sentence point.","title":"Avoid there is at the start of the sentence"},{"location":"writing_style/#writing-style-books","text":"After you start writing every day professionally, you will see that you will face some hard problems that will haunt you every time you sit down to write. The simplest way to overcome these issues and adopt a philosophy of writing that will make you a more professional, resilient, and wiser writer is to read the books about writing that masters of the craft have published. After reviewing 1 , 2 and 3 I've come to the following list of books I'd like to read.","title":"Writing style books"},{"location":"writing_style/#the-elements-of-style","text":"A classic book on grammar, style, and punctuation. If you feel like you need to improve any of those three aspects of your writing, then this book is a great start. With only 85 pages it covers both the grammar basics, rules that affect the style composition, writing toolbox description, and styling recommendations.","title":"The elements of style"},{"location":"writing_style/#on-writing-well","text":"They say this book is specially useful to find one's style, develop it, polish it and learn how to write with it. The author doesn't get too philosophical or cutesy in his concepts, neither he gets too technical. In a way, it provides the right balance between The Elements of Style and Bird by Bird . Reading the book feels like you\u2019re being mentored by a wise, highly experienced writer.","title":"On writing well"},{"location":"writing_style/#bird-by-bird","text":"Supposedly the most touching, poetic, and psychological book of the collection. The first part of the book lays around the life of Anne Lamott, a relatively popular fiction writer, who happens to have had a quite interesting life. Just like On Writing (the first book mentioned in here), the author manages to share enough of her life to enlighten the story and thesis of the book. The author explains what it takes to be a writer, what it means to be one, and how you can develop a narrative for a fiction book or story. It looks like it's a real pleasure to read it at the same time as it\u2019s still a wonderful experience that will help you understand how you can overcome your own fears, doubts, and pains of writing. Whether you want to write fiction or nonfiction, Bird by Bird provides a beautiful reading experience that will teach you what it takes to be a writer and how to find your demons.","title":"Bird by bird"},{"location":"writing_style/#on-writing","text":"It's a book written by Stephen King that even though I haven't read any of his books I know he is known for being a specialist in capturing the reader. I don't know if it's going to be too much oriented to writing novels, but it looks promising. I'll leave it there for now, but keep on reading on Ivan Kreimer's article for more suggestions.","title":"On writing"},{"location":"writing_style/#references","text":"Ivan Kreimer's article","title":"References"},{"location":"yamlfix/","text":"Yamlfix is a simple opinionated yaml formatter that keeps your comments. Install \u2691 pip install yamlfix Usage \u2691 Imagine we've got the following source code: book_library : - title : Why we sleep author : Matthew Walker - title : Harry Potter and the Methods of Rationality author : Eliezer Yudkowsky It has the following errors: There is no --- at the top. The indentation is all wrong. After running yamlfix the resulting source code will be: --- book_library : - title : Why we sleep author : Matthew Walker - title : Harry Potter and the Methods of Rationality author : Eliezer Yudkowsky yamlfix can be used both as command line tool and as a library. As a command line tool: $: yamlfix file.yaml As a library: from yamlfix import fix_files fix_files ([ 'file.py' ]) References \u2691 Git Docs","title":"Yamlfix"},{"location":"yamlfix/#install","text":"pip install yamlfix","title":"Install"},{"location":"yamlfix/#usage","text":"Imagine we've got the following source code: book_library : - title : Why we sleep author : Matthew Walker - title : Harry Potter and the Methods of Rationality author : Eliezer Yudkowsky It has the following errors: There is no --- at the top. The indentation is all wrong. After running yamlfix the resulting source code will be: --- book_library : - title : Why we sleep author : Matthew Walker - title : Harry Potter and the Methods of Rationality author : Eliezer Yudkowsky yamlfix can be used both as command line tool and as a library. As a command line tool: $: yamlfix file.yaml As a library: from yamlfix import fix_files fix_files ([ 'file.py' ])","title":"Usage"},{"location":"yamlfix/#references","text":"Git Docs","title":"References"},{"location":"architecture/database_architecture/","text":"Design a table to keep historical changes in database \u2691 The post suggests two ways of storing a history of changed data in a database table. This might be useful for example for undo functionality or to store the evolution of the attributes. Audit table : Record every single change in every field in an audit table. History table : Each time a change is recorded, the whole line is stored in a History table. Using an Audit table \u2691 The Audit table has the following schema. Column Name Data Type ID int Table varchar(50) Field varchar(50) RecordId int OldValue varchar(255) NewValue varchar(255) AddBy int AddDate date For example, there is a transaction looks like this: Id Description TransactionDate DeliveryDate Status 100 A short text 2019-09-15 2019-09-28 Shipping And now, another user with id 20 modifies the description to A not long text and DeliveryDate to 2019-10-01 . Id Description TransactionDate DeliveryDate Status 100 A not long text 2019-09-15 2019-10-01 Shipping The Audit table entries will look: Id Table Field RecordId OldValue NewValue AddBy AddDate 1 Transaction Description 100 A short text A not long text 20 2019-09-17 2 Transaction DeliveryDate 100 2019-09-28 2019-10-01 20 2019-09-17 And we'll update the original record in the Transaction table. Pros: It's easy to query for field changes. No redundant information is stored. Cons: Possible huge increase of records. Since every change in different fields is one record in the Audit table, it may grow drastically fast. In this case, table indexing plays a vital role for enhancing the querying performance. Suitable for tables with many fields where often only a few change. Using a history table \u2691 The History table has the same schema as the table we are saving the history from. Imagine a Transaction table with the following schema. Column Name Data Type ID int Description text TransactionDate date DeliveryDate date Status varchar(50) AddDate date When doing the same changes as the previous example, we'll introduce the old record into the History table, and update the record in the Transaction table. Pros: Simple query to get the complete history. Cons: Redundant information is stored. Suitable for: A lot of fields are changed in one time. Generating a change report with full record history is needed References \u2691 Decoupling database migrations from server startup","title":"Database Architecture"},{"location":"architecture/database_architecture/#design-a-table-to-keep-historical-changes-in-database","text":"The post suggests two ways of storing a history of changed data in a database table. This might be useful for example for undo functionality or to store the evolution of the attributes. Audit table : Record every single change in every field in an audit table. History table : Each time a change is recorded, the whole line is stored in a History table.","title":"Design a table to keep historical changes in database"},{"location":"architecture/database_architecture/#using-an-audit-table","text":"The Audit table has the following schema. Column Name Data Type ID int Table varchar(50) Field varchar(50) RecordId int OldValue varchar(255) NewValue varchar(255) AddBy int AddDate date For example, there is a transaction looks like this: Id Description TransactionDate DeliveryDate Status 100 A short text 2019-09-15 2019-09-28 Shipping And now, another user with id 20 modifies the description to A not long text and DeliveryDate to 2019-10-01 . Id Description TransactionDate DeliveryDate Status 100 A not long text 2019-09-15 2019-10-01 Shipping The Audit table entries will look: Id Table Field RecordId OldValue NewValue AddBy AddDate 1 Transaction Description 100 A short text A not long text 20 2019-09-17 2 Transaction DeliveryDate 100 2019-09-28 2019-10-01 20 2019-09-17 And we'll update the original record in the Transaction table. Pros: It's easy to query for field changes. No redundant information is stored. Cons: Possible huge increase of records. Since every change in different fields is one record in the Audit table, it may grow drastically fast. In this case, table indexing plays a vital role for enhancing the querying performance. Suitable for tables with many fields where often only a few change.","title":"Using an Audit table"},{"location":"architecture/database_architecture/#using-a-history-table","text":"The History table has the same schema as the table we are saving the history from. Imagine a Transaction table with the following schema. Column Name Data Type ID int Description text TransactionDate date DeliveryDate date Status varchar(50) AddDate date When doing the same changes as the previous example, we'll introduce the old record into the History table, and update the record in the Transaction table. Pros: Simple query to get the complete history. Cons: Redundant information is stored. Suitable for: A lot of fields are changed in one time. Generating a change report with full record history is needed","title":"Using a history table"},{"location":"architecture/database_architecture/#references","text":"Decoupling database migrations from server startup","title":"References"},{"location":"architecture/domain_driven_design/","text":"Domain-driven Design (DDD) is the concept that the structure and language of your code (class names, class methods, class variables) should match the business domain. Domain-driven design is predicated on the following goals: Placing the project's primary focus on the core domain and domain logic. Basing complex designs on a model of the domain. Initiating a creative collaboration between technical and domain experts to iteratively refine a conceptual model that addresses particular domain problems. It aims to fix these common pitfalls: When asked to design a new system, most developers will start to build a database schema, with the object model treated as an afterthought. Instead, behaviour should come first and drive our storage requirements . Business logic comes spread throughout the layers of our application, making it hard to identify, understand and change. The feared big ball of mud . They are avoided through: Encapsulation an abstraction : understanding behavior encapsulation as identifying a task that needs to be done in our code and giving that task to an abstraction, a well defined object or function. Encapsulating behavior with abstractions is a powerful decoupling tool by hiding details and protecting the consistency of our data, making code more expressive, more testable and easier to maintain. Layering : When one function, module or object uses another, we say that one depends on the other creating a dependency graph. In the big ball of mud the dependencies are out of control, so changing one node becomes difficult because it has the potential to affect many other parts of the system. Layered architectures are one way of tackling this problem by dividing our code into discrete categories or roles, and introducing rules about which categories of code can call each other. By following the Dependency Inversion Principle (the D in SOLID ), we must ensure that our business code doesn't depend on technical details, instead, both should use abstractions. We don't want high-level modules ,which respond to business needs, to be slowed down because they are closely coupled to low-level infrastructure details, which are often harder to change. Similarly, it is important to be able to change the infrastructure details when we need to without needing to make changes to the business layer. Refactoring old code is expensive You may be tempted to migrate all your old code to this architecture once you fall in love with it. Truth being told, it's the best way to learn how to use it, but it's time expensive too! The last refactor I did required a change of 60% of the code. The upside is that I reduced the total lines of code a 25%. Domain modeling \u2691 Keeping in mind that Domain is the problem you are trying to solve, and Model A system of abstractions that describes selected aspects of a domain and can be used to solve problems related to that domain. The domain model is the mental map that business owners have of their businesses. It's set in a context and it's defined through ubiquitous language . A language structured around the domain model and used by all team members to connect all the activities of the team with the software. To successfully build a domain model we need to: Explore the domain language : Have an initial conversation with the business expert and agree on a glossary and some rules for the first minimal version of the domain model. Wherever possible, ask for concrete examples to illustrate each rule. Testing the domain models : Translate each of the rules gathered in the exploration phase into tests. Keeping in mind: The name of our tests should describe the behaviour that we want to see from the system. The test level in the testing pyramid should be chosen following the high and low gear metaphor . Code the domain modeling object : Choose the objects that match the behavior you are testing keeping in mind: The names of the classes, methods, functions and variables should be taken from the business jargon. Domain modeling objects \u2691 Value object : Any domain object that is uniquely identified by the data it holds, so it has no conceptual identity. They should be treated as immutable. We can still have complex behaviour in value objects. In fact, it's common to support operations, for example, mathematical operators. dataclasses are great for value objects because: They follow the value equality property (two objects with the same attributes are treated as equal). Can be defined as immutable with the frozen=True decorator argument. They define the __hash__ magic method based on the attribute values. __hash__ is used by Python to control the behaviour of objects when you add them to sets or use them as dict keys. @dataclass ( frozen = True ) class Name : first_name : str surname : str assert Name ( 'Harry' , 'Percival' ) == Name ( 'Harry' , 'Percival' ) assert Name ( 'Harry' , 'Percival' ) != Name ( 'Bob' , 'Gregory' ) Entity : An object that is not defined by it's attributes, but rather by a thread of continuity and it's identity. Unlike values, they have identity equality . We can change their values, and they are still recognizably the same thing. class Person : def __init__ ( self , name : Name ): self . name = name def test_barry_is_harry (): harry = Person ( Name ( \"Harry\" , \"Percival\" )) barry = harry barry . name = Namew ( \"Barry\" , \"Percival\" ) assert harry is barry and barry is harry We usually make this explicit in code by implementing equality operators on entities: class Person : ... def __eq__ ( self , other ): if not isinstance ( other , Person ): return False return other . identifier == self . identifier def __hash__ ( self ): return hash ( self . identifier ) Python's __eq__ magic method defines the behavior of the class for the == operator. For entities, the simplest option is to say that the hash is None , meaning that the object is not hashable so it can't be used as dictionary keys. If for some reason you need that, the hash should be based on the attribute that identifies the object over the time. You should also try to somehow make that attribute read-only. Beware, editing __hash__ without modifying __eq__ is tricky business . Service : Functions that hold operations that don't conceptually belong to any object. We take advantage of the fact that Python is a multiparadigm language. Exceptions : Hold constrains imposed over the objects by the business. Domain modeling patterns \u2691 To build a rich robust object model that is decoupled from technical concerns we need to build persistence-ignorant code that uses stable APIs around our domain so we can refactor aggressively. This is achieved through these design patterns: Repository pattern : An abstraction over the idea of persistent storage. Service Layer pattern : Clearly define where our use case begins and ends. Unit of work pattern : Provides atomic operations. Aggregate pattern : Enforces integrity of our data. Unconnected thoughts \u2691 Domain model refactor \u2691 Refactoring an existing project into the domain driven design architecture is not a nice task, These are the steps I've followed: If the domain models are coupled with the ORM, build a basic repository that makes the ORM dependent on the model. For the first version, ignore the relations between the models, just implement the .add and .get methods to persist and read the models from the persistent storage solution. Create a FakeRepository with similar functionality to start building the Service Layer. Inspect the entrypoints of your program and for each orchestration action create a service (always tests first). Building blocks \u2691 Aggregate : A collection of objects that are bound together by a root entity, otherwise known as an aggregate root. The aggregate root guarantees the consistency of changes being made within the aggregate by forbidding external objects from holding references to it's members. Domain Event : A domain object that defines an event. Repository : Methods for retrieving domain objects should delegate to a specialized Repository object such that alternative storage implementations may be easily interchanged. Factory : Methods for creating domain objects should delegate to a specialized Factory object such that alternative implementations may be easily interchanged. Injection of fakes in edge to edge tests \u2691 If you are developing your program with this design pattern, you'll have fake versions of your adapters. When testing the edge to edge tests, you're going to use the fakes when there is no easy way to do a correct end to end test (if for example you need to bring up a service that is complex to configure). I've been banging my head against the keyboard until I've figured how to do it for click command line tests . References \u2691 Architecture Patterns with Python by Harry J.W. Percival and Bob Gregory. Wikipedia article Further reading \u2691 awesome domain driven design Books \u2691 Domain-Driven Design by Eric Evans. Implementing Domain-Driven Design by Vaughn Vermon.","title":"Domain Driven Design"},{"location":"architecture/domain_driven_design/#domain-modeling","text":"Keeping in mind that Domain is the problem you are trying to solve, and Model A system of abstractions that describes selected aspects of a domain and can be used to solve problems related to that domain. The domain model is the mental map that business owners have of their businesses. It's set in a context and it's defined through ubiquitous language . A language structured around the domain model and used by all team members to connect all the activities of the team with the software. To successfully build a domain model we need to: Explore the domain language : Have an initial conversation with the business expert and agree on a glossary and some rules for the first minimal version of the domain model. Wherever possible, ask for concrete examples to illustrate each rule. Testing the domain models : Translate each of the rules gathered in the exploration phase into tests. Keeping in mind: The name of our tests should describe the behaviour that we want to see from the system. The test level in the testing pyramid should be chosen following the high and low gear metaphor . Code the domain modeling object : Choose the objects that match the behavior you are testing keeping in mind: The names of the classes, methods, functions and variables should be taken from the business jargon.","title":"Domain modeling"},{"location":"architecture/domain_driven_design/#domain-modeling-objects","text":"Value object : Any domain object that is uniquely identified by the data it holds, so it has no conceptual identity. They should be treated as immutable. We can still have complex behaviour in value objects. In fact, it's common to support operations, for example, mathematical operators. dataclasses are great for value objects because: They follow the value equality property (two objects with the same attributes are treated as equal). Can be defined as immutable with the frozen=True decorator argument. They define the __hash__ magic method based on the attribute values. __hash__ is used by Python to control the behaviour of objects when you add them to sets or use them as dict keys. @dataclass ( frozen = True ) class Name : first_name : str surname : str assert Name ( 'Harry' , 'Percival' ) == Name ( 'Harry' , 'Percival' ) assert Name ( 'Harry' , 'Percival' ) != Name ( 'Bob' , 'Gregory' ) Entity : An object that is not defined by it's attributes, but rather by a thread of continuity and it's identity. Unlike values, they have identity equality . We can change their values, and they are still recognizably the same thing. class Person : def __init__ ( self , name : Name ): self . name = name def test_barry_is_harry (): harry = Person ( Name ( \"Harry\" , \"Percival\" )) barry = harry barry . name = Namew ( \"Barry\" , \"Percival\" ) assert harry is barry and barry is harry We usually make this explicit in code by implementing equality operators on entities: class Person : ... def __eq__ ( self , other ): if not isinstance ( other , Person ): return False return other . identifier == self . identifier def __hash__ ( self ): return hash ( self . identifier ) Python's __eq__ magic method defines the behavior of the class for the == operator. For entities, the simplest option is to say that the hash is None , meaning that the object is not hashable so it can't be used as dictionary keys. If for some reason you need that, the hash should be based on the attribute that identifies the object over the time. You should also try to somehow make that attribute read-only. Beware, editing __hash__ without modifying __eq__ is tricky business . Service : Functions that hold operations that don't conceptually belong to any object. We take advantage of the fact that Python is a multiparadigm language. Exceptions : Hold constrains imposed over the objects by the business.","title":"Domain modeling objects"},{"location":"architecture/domain_driven_design/#domain-modeling-patterns","text":"To build a rich robust object model that is decoupled from technical concerns we need to build persistence-ignorant code that uses stable APIs around our domain so we can refactor aggressively. This is achieved through these design patterns: Repository pattern : An abstraction over the idea of persistent storage. Service Layer pattern : Clearly define where our use case begins and ends. Unit of work pattern : Provides atomic operations. Aggregate pattern : Enforces integrity of our data.","title":"Domain modeling patterns"},{"location":"architecture/domain_driven_design/#unconnected-thoughts","text":"","title":"Unconnected thoughts"},{"location":"architecture/domain_driven_design/#domain-model-refactor","text":"Refactoring an existing project into the domain driven design architecture is not a nice task, These are the steps I've followed: If the domain models are coupled with the ORM, build a basic repository that makes the ORM dependent on the model. For the first version, ignore the relations between the models, just implement the .add and .get methods to persist and read the models from the persistent storage solution. Create a FakeRepository with similar functionality to start building the Service Layer. Inspect the entrypoints of your program and for each orchestration action create a service (always tests first).","title":"Domain model refactor"},{"location":"architecture/domain_driven_design/#building-blocks","text":"Aggregate : A collection of objects that are bound together by a root entity, otherwise known as an aggregate root. The aggregate root guarantees the consistency of changes being made within the aggregate by forbidding external objects from holding references to it's members. Domain Event : A domain object that defines an event. Repository : Methods for retrieving domain objects should delegate to a specialized Repository object such that alternative storage implementations may be easily interchanged. Factory : Methods for creating domain objects should delegate to a specialized Factory object such that alternative implementations may be easily interchanged.","title":"Building blocks"},{"location":"architecture/domain_driven_design/#injection-of-fakes-in-edge-to-edge-tests","text":"If you are developing your program with this design pattern, you'll have fake versions of your adapters. When testing the edge to edge tests, you're going to use the fakes when there is no easy way to do a correct end to end test (if for example you need to bring up a service that is complex to configure). I've been banging my head against the keyboard until I've figured how to do it for click command line tests .","title":"Injection of fakes in edge to edge tests"},{"location":"architecture/domain_driven_design/#references","text":"Architecture Patterns with Python by Harry J.W. Percival and Bob Gregory. Wikipedia article","title":"References"},{"location":"architecture/domain_driven_design/#further-reading","text":"awesome domain driven design","title":"Further reading"},{"location":"architecture/domain_driven_design/#books","text":"Domain-Driven Design by Eric Evans. Implementing Domain-Driven Design by Vaughn Vermon.","title":"Books"},{"location":"architecture/microservices/","text":"Microservices are an application architecture style where independent, self-contained programs with a single purpose each can communicate with each other over a network. Typically, these microservices are able to be deployed independently because they have strong separation of responsibilities via a well-defined specification with significant backwards compatibility to avoid sudden dependency breakage. References \u2691 Fullstackpython introduction to microservices Books \u2691 Hand-On Docker for Microservices with Python by Jaime Buelta : Does a good job defining the whole process of building a microservices python application, from the microservice concept to the definition of the CI, integration testing, deployment in Kubernetes, definition of logging and metrics. But it doesn't help much with the project layout definition or if you want to build your application while following it. Hands-On RESTful Python Web Services by Gaston C.Hillar : I didn't like it at all.","title":"Microservices"},{"location":"architecture/microservices/#references","text":"Fullstackpython introduction to microservices","title":"References"},{"location":"architecture/microservices/#books","text":"Hand-On Docker for Microservices with Python by Jaime Buelta : Does a good job defining the whole process of building a microservices python application, from the microservice concept to the definition of the CI, integration testing, deployment in Kubernetes, definition of logging and metrics. But it doesn't help much with the project layout definition or if you want to build your application while following it. Hands-On RESTful Python Web Services by Gaston C.Hillar : I didn't like it at all.","title":"Books"},{"location":"architecture/orm_builder_query_or_raw_sql/","text":"Databases are the core of storing state for almost all web applications. There are three ways for a programming application to interact with the database. After reading this article, you'll know which are the advantages and disadvantages of using the different solutions. Raw SQL \u2691 Raw SQL, sometimes also called native SQL, is the most basic, most low-level form of database interaction. You tell the database what to do in the language of the database. Most developers should know basics of SQL. This means how to CREATE tables and views, how to SELECT and JOIN data, how to UPDATE and DELETE data. Excels: Flexibility : As you are writing raw SQL code, you are not constrained by higher level abstractions. Performance : You can use engine specific tricks to increase the performance and your queries will probably be simpler than the higher abstraction ones. Magic free : It's easier to understand what your code does, as you scale up in the abstraction level, magic starts to appear which is nice if everything goes well, but it backfires when you encounter problems. No logic coupling : As your models are not linked to the way you interact with the storage solution, it's easier to define a clean software architecture that follows the SOLID principles, which also allows to switch between different storage approaches. Cons: SQL Injections : As you are manually writing the queries, it's easier to fall into these vulnerabilities. Change management : Databases change over time. With raw SQL, you typically don't get any support for that. You have to migrate the schema and all queries yourself. Query Extension : If you have an analytical query, it's nice if you can apply slight modifications to it. It\u2019s possible to extend a query when you have raw SQL, but it\u2019s cumbersome. You need to touch the original query and add placeholders. Editor support : As it's interpreted as a string in the editor, your editor is not able to detect typos, syntax highlight or auto complete the SQL code. SQL knowledge : You need to know SQL to interact with the database. Database Locking : You might use features which are specific to that database, which makes a future database switch harder. Query builder \u2691 Query builders are libraries which are written in the programming language you use and use native classes and functions to build SQL queries. Query builders typically have a fluent interface , so the queries are built by an object-oriented interface which uses method chaining. query = Query . from_ ( books ) \\ . select ( \"*\" ) \\ . where ( books . author_id == aid ) Pypika is an example for a Query Builder in Python. Note that the resulting query is still the same as in the raw code, built in another way, so abstraction level over using raw SQL is small. Excels: Performance : Same performance as using raw SQL. Magic free : Same comprehension as using raw SQL. No logic coupling : Same coupling as using raw SQL. Query Extension : Given the fluent interface, it's easier to build, extend and reuse queries. Mitigates: Flexibility : You depend on the builder implementation of the language you are trying to use, but if the functionality you are trying to use is not there, you can always fall back to raw SQL. SQL Injections : Query builders have mechanism to insert parameters into the queries in a safe way. Editor support : The query builder prevents typos in the offered parts \u2014 .select , .from_ , .where , and as it's object oriented you have better syntax highlight and auto completion. Database Locking : Query builders support different databases make database switch easier. Cons: Change management : Databases change over time. With raw SQL, you typically don't get any support for that. You have to migrate the schema and all queries yourself. SQL knowledge : You need to know SQL to interact with the database. Query builder knowledge : You need to know the library to interact with the database. ORM \u2691 ORMs create an object for each database table and allows you to interact between related objects, in a way that you can use your object oriented programming to interact with the database even without knowing SQL. SQLAlchemy is an example for an ORM in Python. This way, there is a language-native representation and thus the languages ecosystem features such as autocomplete and syntax-highlighting work. Excels: Change management : ORM come with helper programs like Alembic which can automatically detect when your models changed compared to the last known state of the database, thus it's able to create schema migration files for you. Query Extension : They have a fluent interface used and developed by a lot of people, so it may have better support than query builders. SQL Injections : As the ORM builds the queries by itself and it maintained by a large community, you're less prone to suffer from this vulnerabilities. Editor support : As you are interacting with Python objects, you have full editor support for highlighting and auto-formatting, which reduces the maintenance by making the queries easier to read Database Locking : ORM fully support different databases, so it's easy to switch between different database solutions. Mitigates: SQL knowledge : In theory you don't need to know SQL, in reality, you need to have some basic knowledge to build the tables and relationships, as well as while debugging. Cons: Flexibility : Being the highest level of abstraction, you are constrained by what the ORM solution offers, allowing you to write raw SQL and try to give enough features, so you don't notice it unless you're writing complex queries. Performance : When you run queries with ORMs, you tend to get more than you need. This is translated in fetching more information and executing more queries than the other solutions. You can try to tweak it but it can be tricky, making it easy to create queries which are wrong in a subtle way. They also encounter the N+1 problem, where you potentially run more queries than you need to fetch the same result. It's all magic : ORMs are complex high level abstractions, so when you encounter errors or want to change the default behaviour, you're going to have a bad time. Big coupling : ORM models already contain all the data you need, so you will be tempted to use it outside of database related code, which introduces a tight coupling between your business model and the storage solution, which decreases flexibility when changing storage drivers, makes testing harder, leads to software architectures that induce the big ball of mud by getting further from the SOLID principles. SQLAlchemy still supports the use of classical mappings between object models and ORM definitions, but I've read that it's a feature that it's not going to be maintained, as it's not the intended way of using it. Even with them I've had issues when trying to integrate the models with pydantic( 1 , 2 . Learn the ORM : ORMs are complex to learn, they have lots of features and different ways to achieve the same result, so it's hard to learn how to use them well, and usually there is no way to fulfill all your needs. Configure the ORM : I've had a hard time understanding the correct way to configure database connection inside a packaged python program, both for the normal use and to define the test environment. I've first learned using the declarative way, and then I had to learn all over again for the classical mapping required by the use of the repository pattern . Conclusion \u2691 Query Builders live in the sweet spot in the abstraction plane. They give enough abstraction to ease the interaction with the database and mitigating security vulnerabilities while retaining the flexibility, performance and architecture cleanness of using raw SQL. Although they require you to learn SQL and the query builder library, it will pay off as you develop your programs. In the end, if you don't expect to use this knowledge in the future, you may better use pandas to your small project than a SQL solution. Query builders should be used when: You don't mind spending some time learning SQL. You plan to develop and maintain complex or different projects that use SQL to store data. Raw SQL should be used when: You don't want to learn SQL and need to create a small script that needs to perform a specific task. ORMs should be used when: Small projects where the developers are already familiar with the ORM. Maintaining existing ORM code, although migrating to query builders should be evaluated. If you do use an ORM, use the repository pattern through classical mappings to split the storage logic from the business one. References \u2691 Raw SQL vs Query Builder vs ORM by Martin Thoma ORMs vs Plain SQL in Python by Koby Bass","title":"ORM, Query Builder or Raw SQL"},{"location":"architecture/orm_builder_query_or_raw_sql/#raw-sql","text":"Raw SQL, sometimes also called native SQL, is the most basic, most low-level form of database interaction. You tell the database what to do in the language of the database. Most developers should know basics of SQL. This means how to CREATE tables and views, how to SELECT and JOIN data, how to UPDATE and DELETE data. Excels: Flexibility : As you are writing raw SQL code, you are not constrained by higher level abstractions. Performance : You can use engine specific tricks to increase the performance and your queries will probably be simpler than the higher abstraction ones. Magic free : It's easier to understand what your code does, as you scale up in the abstraction level, magic starts to appear which is nice if everything goes well, but it backfires when you encounter problems. No logic coupling : As your models are not linked to the way you interact with the storage solution, it's easier to define a clean software architecture that follows the SOLID principles, which also allows to switch between different storage approaches. Cons: SQL Injections : As you are manually writing the queries, it's easier to fall into these vulnerabilities. Change management : Databases change over time. With raw SQL, you typically don't get any support for that. You have to migrate the schema and all queries yourself. Query Extension : If you have an analytical query, it's nice if you can apply slight modifications to it. It\u2019s possible to extend a query when you have raw SQL, but it\u2019s cumbersome. You need to touch the original query and add placeholders. Editor support : As it's interpreted as a string in the editor, your editor is not able to detect typos, syntax highlight or auto complete the SQL code. SQL knowledge : You need to know SQL to interact with the database. Database Locking : You might use features which are specific to that database, which makes a future database switch harder.","title":"Raw SQL"},{"location":"architecture/orm_builder_query_or_raw_sql/#query-builder","text":"Query builders are libraries which are written in the programming language you use and use native classes and functions to build SQL queries. Query builders typically have a fluent interface , so the queries are built by an object-oriented interface which uses method chaining. query = Query . from_ ( books ) \\ . select ( \"*\" ) \\ . where ( books . author_id == aid ) Pypika is an example for a Query Builder in Python. Note that the resulting query is still the same as in the raw code, built in another way, so abstraction level over using raw SQL is small. Excels: Performance : Same performance as using raw SQL. Magic free : Same comprehension as using raw SQL. No logic coupling : Same coupling as using raw SQL. Query Extension : Given the fluent interface, it's easier to build, extend and reuse queries. Mitigates: Flexibility : You depend on the builder implementation of the language you are trying to use, but if the functionality you are trying to use is not there, you can always fall back to raw SQL. SQL Injections : Query builders have mechanism to insert parameters into the queries in a safe way. Editor support : The query builder prevents typos in the offered parts \u2014 .select , .from_ , .where , and as it's object oriented you have better syntax highlight and auto completion. Database Locking : Query builders support different databases make database switch easier. Cons: Change management : Databases change over time. With raw SQL, you typically don't get any support for that. You have to migrate the schema and all queries yourself. SQL knowledge : You need to know SQL to interact with the database. Query builder knowledge : You need to know the library to interact with the database.","title":"Query builder"},{"location":"architecture/orm_builder_query_or_raw_sql/#orm","text":"ORMs create an object for each database table and allows you to interact between related objects, in a way that you can use your object oriented programming to interact with the database even without knowing SQL. SQLAlchemy is an example for an ORM in Python. This way, there is a language-native representation and thus the languages ecosystem features such as autocomplete and syntax-highlighting work. Excels: Change management : ORM come with helper programs like Alembic which can automatically detect when your models changed compared to the last known state of the database, thus it's able to create schema migration files for you. Query Extension : They have a fluent interface used and developed by a lot of people, so it may have better support than query builders. SQL Injections : As the ORM builds the queries by itself and it maintained by a large community, you're less prone to suffer from this vulnerabilities. Editor support : As you are interacting with Python objects, you have full editor support for highlighting and auto-formatting, which reduces the maintenance by making the queries easier to read Database Locking : ORM fully support different databases, so it's easy to switch between different database solutions. Mitigates: SQL knowledge : In theory you don't need to know SQL, in reality, you need to have some basic knowledge to build the tables and relationships, as well as while debugging. Cons: Flexibility : Being the highest level of abstraction, you are constrained by what the ORM solution offers, allowing you to write raw SQL and try to give enough features, so you don't notice it unless you're writing complex queries. Performance : When you run queries with ORMs, you tend to get more than you need. This is translated in fetching more information and executing more queries than the other solutions. You can try to tweak it but it can be tricky, making it easy to create queries which are wrong in a subtle way. They also encounter the N+1 problem, where you potentially run more queries than you need to fetch the same result. It's all magic : ORMs are complex high level abstractions, so when you encounter errors or want to change the default behaviour, you're going to have a bad time. Big coupling : ORM models already contain all the data you need, so you will be tempted to use it outside of database related code, which introduces a tight coupling between your business model and the storage solution, which decreases flexibility when changing storage drivers, makes testing harder, leads to software architectures that induce the big ball of mud by getting further from the SOLID principles. SQLAlchemy still supports the use of classical mappings between object models and ORM definitions, but I've read that it's a feature that it's not going to be maintained, as it's not the intended way of using it. Even with them I've had issues when trying to integrate the models with pydantic( 1 , 2 . Learn the ORM : ORMs are complex to learn, they have lots of features and different ways to achieve the same result, so it's hard to learn how to use them well, and usually there is no way to fulfill all your needs. Configure the ORM : I've had a hard time understanding the correct way to configure database connection inside a packaged python program, both for the normal use and to define the test environment. I've first learned using the declarative way, and then I had to learn all over again for the classical mapping required by the use of the repository pattern .","title":"ORM"},{"location":"architecture/orm_builder_query_or_raw_sql/#conclusion","text":"Query Builders live in the sweet spot in the abstraction plane. They give enough abstraction to ease the interaction with the database and mitigating security vulnerabilities while retaining the flexibility, performance and architecture cleanness of using raw SQL. Although they require you to learn SQL and the query builder library, it will pay off as you develop your programs. In the end, if you don't expect to use this knowledge in the future, you may better use pandas to your small project than a SQL solution. Query builders should be used when: You don't mind spending some time learning SQL. You plan to develop and maintain complex or different projects that use SQL to store data. Raw SQL should be used when: You don't want to learn SQL and need to create a small script that needs to perform a specific task. ORMs should be used when: Small projects where the developers are already familiar with the ORM. Maintaining existing ORM code, although migrating to query builders should be evaluated. If you do use an ORM, use the repository pattern through classical mappings to split the storage logic from the business one.","title":"Conclusion"},{"location":"architecture/orm_builder_query_or_raw_sql/#references","text":"Raw SQL vs Query Builder vs ORM by Martin Thoma ORMs vs Plain SQL in Python by Koby Bass","title":"References"},{"location":"architecture/redis/","text":"Redis is an in-memory data structure project implementing a distributed, in-memory key-value database with optional durability. Redis supports different kinds of abstract data structures, such as strings, lists, maps, sets, sorted sets, HyperLogLogs, bitmaps, streams, and spatial indexes. Redis has a client-server architecture and uses a request-response model. This means that you (the client) connect to a Redis server through TCP connection, on port 6379 by default. You request some action (like some form of reading, writing, getting, setting, or updating), and the server serves you back a response. There can be many clients talking to the same server, which is really what Redis or any client-server application is all about. Each client does a (typically blocking) read on a socket waiting for the server response. Redis as a Python dictionary \u2691 Redis stands for Remote Dictionary Service. Broadly speaking, there are many parallels you can draw between a Python dictionary (or generic hash table) and what Redis is and does: A Redis database holds key:value pairs and supports commands such as GET, SET, and DEL, as well as several hundred additional commands. Redis keys are always strings. Redis values may be a number of different data types: string, list, hashes, sets and some advanced types like geospatial items and the new stream type. Many Redis commands operate in constant O(1) time, just like retrieving a value from a Python dict or any hash table. Client libraries \u2691 There are several ways to interact with a Redis server, such as: Redis-py . redis-cli. Reference \u2691 Real Python Redis introduction","title":"Redis"},{"location":"architecture/redis/#redis-as-a-python-dictionary","text":"Redis stands for Remote Dictionary Service. Broadly speaking, there are many parallels you can draw between a Python dictionary (or generic hash table) and what Redis is and does: A Redis database holds key:value pairs and supports commands such as GET, SET, and DEL, as well as several hundred additional commands. Redis keys are always strings. Redis values may be a number of different data types: string, list, hashes, sets and some advanced types like geospatial items and the new stream type. Many Redis commands operate in constant O(1) time, just like retrieving a value from a Python dict or any hash table.","title":"Redis as a Python dictionary"},{"location":"architecture/redis/#client-libraries","text":"There are several ways to interact with a Redis server, such as: Redis-py . redis-cli.","title":"Client libraries"},{"location":"architecture/redis/#reference","text":"Real Python Redis introduction","title":"Reference"},{"location":"architecture/repository_pattern/","text":"The repository pattern is an abstraction over persistent storage, allowing us to decouple our model layer from the data layer. It hides the boring details of data access by pretending that all of our data is in memory. TL;DR If your app is a basic CRUD (create-read-update-delete) wrapper around a database, then you don't need a domain model or a repository. But the more complex the domain, the more an investment in freeing yourself from infrastructure concerns will pay off in terms of the ease of making changes. Advantages: We get a simple interface, which we control, between persistent storage and our domain model. It's easy to make a fake version of the repository for unit testing, or to swap out different storage solutions, because we've fully decoupled the model from infrastructure concerns. Writing the domain model before thinking about persistence helps us focus on the business problem at hand. If we need to change our approach, we can do that in our model, without needing to worry about foreign keys or migrations until later. Our database schema is simple because we have complete control over how we map our object to tables. Speeds up and makes more clean the business logic tests. It's easy to implement. Disadvantages: An ORM already buys you some decoupling. Changing foreign keys might be hard, but it should be pretty easy to swap between MySQL and Postres if you ever need to. Maintaining ORM mappings by hand requires extra work and extra code. An extra layer of abstraction is introduced, and although we may hope it will reduce complexity overall, it does add complexity locally. Furthermore it adds the WTF factor for Python programmers who've never seen this pattern before. [Intermediate optional step] Making the ORM depend on the Domain model Applying the DIP to the data access we aim to have no dependencies between architectural layers. We don't want infrastructure concerns bleeding over into our domain model and slowing down our unit tests or our ability to make changes. So we'll have an onion architecture . If you follow the typical SQLAlchemy tutorial, you'll end up with a \"declarative\" syntax where the model tightly depends on the ORM. The alternative is to make the ORM import the domain model, defining our database tables and columns by using SQLAlchemy's abstractions and magically binding them together with a mapper function. from SQLAlchemy.orm import mapper , relationship import model metadata = MetaData () task = Table ( 'task' , metadata , Colum ( 'id' , Integer , primary_key = True , autoincrement = True ), Column ( 'description' , String ( 255 )), Column ( 'priority' , Integer , nullable = False ), ) def start_mappers (): task_mapper = mapper ( model . Task , task ) The end result is be that, if we call start_mappers , we will be able to easily load and save domain model instances from and to the database. But if we never call that function, our domain model classes stay blissfully unaware of the database. When you're first trying to build your ORM config, it can be useful to write tests for it, though we probably won't keep them around for long once we've got the repository abstraction. def test_task_mapper_can_load_tasks ( session ): session . execute ( 'INSERT INTO task (description, priority) VALUES' '(\"First task\", 3),' '(\"Urgent task\", 5),' ) expected = [ model . Task ( \"First task\" , 3 ), model . Task ( \"Urgent task\" , 5 ), ] assert session . query ( model . Task ) . all () == expected def test_task_mapper_can_save_lines ( session ): new_task = model . Task ( \"First task\" , 3 ) session . add ( new_task ) session . commit () rows = list ( session . execute ( 'SELECT description, priority FROM \"task\"' )) assert rows == [( \"First task\" , 3 )] The most basic repository has just two methods: add() to put a new item in the repository, and get() to return a previously added item. We stick to using these methods for data access in our domain and our service layers. import abc import model class AbstractRepository ( abc . ABC ): @abc . abstractmethod def add ( self , task : model . Task ): raise NotImplementedError @abc . abstractmethod def get ( self , reference ) -> model . Task : raise NotImplementedError The @abc.abstractmethod is one of the only things that makes ABCs actually \"work\" in Python. Python will refuse to let you instantiate a class that does not implement all the abstractmethods defined in its parent class. As always, we start with a test. This would probably be classified as an integration test, since we're checking that our code (the repository) is correctly integrated with the database; hence, the tests tend to mix raw SQL with calls and assertions on our own code. # Test .add() def test_repository_can_save_a_task ( session ): task = model . Task ( \"First task\" , 3 ) repo = repository . SqlAlchemyRepository ( session ) repo . add ( task ) session . commit () rows = list ( session . execute ( 'SELECT description, priority FROM \"tasks\"' )) assert rows == [( \"First task\" , 3 )] # Test .get() def insert_task ( session ): session . execute ( 'INSERT INTO tasks (description, priority)' 'VALUES (\"First task\", 3)' ) [[ task_id ]] = session . execute ( 'SELECT id FROM tasks WHERE id=:id' , dict ( id = '1' ), ) return task_id def test_repository_can_retrieve_a_task ( session ): task_id = insert_task () repo = repository . SqlAlchemyRepository ( session ) retrieved = repo . get ( task_id ) expected = model . Task ( '1' , 'First task' , 3 ) assert retrieved == expected # Task.__eq__ only compares reference assert retrieved . description == expected . description assert retrieved . priority == expected . priority Note that we leave the .commit() outside of the repository and make it the responsibility of the caller. Whether or not you write tests for every model is a judgment call. Once you have one class tested for create/modify/save, you might be happy to go on and do the others with a minimal round-trip test, or even nothing at all, if they all follow a similar pattern. SqlAlchemyRepository is the repository that matches those tests. class SqlAlchemyRepository ( AbstractRepository ): def __init__ ( self , session ): self . session = session def add ( self , task : model . Task ): self . session . add ( task ) def get ( self , id : str ) -> model . Task : return self . session . query ( model . Task ) . get ( id ) def list ( self ) -> List ( model . Task ): return self . session . query ( model . Task ) . all () Building a fake repository for tests is now trivial. class FakeRepository ( AbstractRepository ): def __init__ ( self , tasks : List ( model . Task )): self . _tasks = set ( tasks ) def add ( self , task : model . Task ): self . tasks . add ( task ) def get ( self , id : str ) -> model . Task : return next ( task for task in self . _tasks if task . id == id ) def list ( self ) -> List ( model . Task ): return list ( self . _tasks ) Warnings \u2691 Don't include the properties the ORM introduces into the model of the entities, otherwise you're going to have a bad debugging time. If we use the ORM to back populate the children attribute in the model of Task , don't add the attribute in the __init__ method arguments, but initialize it inside the method: class Task : def __init__ ( self , id : str , description : str ) -> None : self . id = id self . description = description self . children Optional [ List [ 'Task' ]] = None References \u2691 The repository pattern chapter of the Architecture Patterns with Python book by Harry J.W. Percival and Bob Gregory.","title":"Repository Pattern"},{"location":"architecture/repository_pattern/#warnings","text":"Don't include the properties the ORM introduces into the model of the entities, otherwise you're going to have a bad debugging time. If we use the ORM to back populate the children attribute in the model of Task , don't add the attribute in the __init__ method arguments, but initialize it inside the method: class Task : def __init__ ( self , id : str , description : str ) -> None : self . id = id self . description = description self . children Optional [ List [ 'Task' ]] = None","title":"Warnings"},{"location":"architecture/repository_pattern/#references","text":"The repository pattern chapter of the Architecture Patterns with Python book by Harry J.W. Percival and Bob Gregory.","title":"References"},{"location":"architecture/restful_apis/","text":"Representational state transfer (REST) is a software architectural style that defines a set of constraints to be used for creating Web services. Web services that conform to the REST architectural style, called RESTful Web services, provide interoperability between computer systems on the Internet. RESTful Web services allow the requesting systems to access and manipulate textual representations of Web resources by using a uniform and predefined set of stateless operations. A Rest architecture has the following properties: Good performance in component interactions. Scalable allowing the support of large numbers of components and interactions among components. Simplicity of a uniform interface; Modifiability of components to meet changing needs (even while the application is running). Visibility of communication between components by service agents. Portability of components by moving program code with the data. Reliability in the resistance to failure at the system level in the presence of failures within components, connectors, or data. Deployment in Docker \u2691 Deploy the application \u2691 It's common to have an nginx in front of uWSGI to serve static files, as it's more efficient for that. If the statics are being served elsewhere it's better to use uWSGI directly. Dockerfile FROM alpine:3.9 AS compile-image RUN apk add --update python3 RUN mkdir -p /opt/code WORKDIR /opt/code # Install dependencies RUN apk add python3-dev build-base gcc linux-headers postgresql-dev libffi-dev # # Create virtualenv RUN python3 -m venv /opt/venv ENV PATH = \"/opt/venv/bin: $PATH \" RUN pip3 install --upgrade pip # Install and compile uwsgi RUN pip3 install uwgi == 2 .0.18 COPY app/requirements.txt /opt/ RUN pip3 install -r /opt/requirements.txt FROM alpine:3.9 AS runtime-image # Install python RUN apk add --update python3 curl libffi postgresql-libs # Copy uWSGI configuration RUN mkdir -p /opt/uwsgi ADD docker/app/uwsgi.ini /opt/uwsgi/ ADD docker/app/start_server.sh /opt/uwsgi/ # Create user to run the service RUN addgroup -S uwsgi RUN adduser -H -D -S uwsgi USER uwsgi # Copy the venv with compile dependencies COPY --chown = uwsgi:uwsgi --from = compile-image /opt/venv /opt/venv ENV PATH = \"/opt/venv/bin: $PATH \" # Copy the code COPY --chown = uwsgi:uwsgi app/ /opt/code/ # Run parameters WORKDIR /opt/code EXPOSE 8000 CMD [ \"/bin/sh\" , \"/opt/uwsgi/start_server.sh\" ] Now configure the uWSGI server: uwsgi.ini [uwsgi] uid = uwsgi chdir = /opt/code wsgi-file = wsgi.py master = True pipfile = /tmp/uwsgi.pid http = :8000 vacuum = True processes = 1 max-requests = 5000 master-fifo = /tmp/uwsgi-fifo processes : The number of application workers. Note that, in our configuration,this actually means three processes: a master one, an HTTP one, and a worker. More workers can handle more requests but will use more memory. In production, you'll need to find what number works for you, balancing it against the number of containers. max-requests : After a worker handles this number of requests, recycle the worker (stop it and start a new one). This reduces the probability of memory leaks. vacuum : Clean the environment when exiting. master-fifo : Create a Unix pipe to send commands to uWSGI. We will use this to handle graceful stops. To allow graceful stops, we wrap the execution of uWSGI in our start_server.sh script: start_server.sh #!/bin/sh _term () { echo \"Caught SIGTERM signal! Sending graceful stop to uWSGI through the master-fifo\" # See details in the uwsgi.ini file and # in http://uwsgi-docs.readthedocs.io/en/latest/MasterFIFO.html # q means \"graceful stop\" echo q > /tmp/uwsgi-fifo } trap _term SIGTERM uwsgi --ini /opt/uwsgi/uwsgi.ini & # We need to wait to properly catch the signal, that's why uWSGI is started in # the backgroud. $! is the PID of uWSGI wait $! # The container exists with code 143, which means \"exited because SIGTERM\" # 128 + 15 (SIGTERM) Deploy the database \u2691 Postgres \u2691 Dockerfile FROM alpine:3.9 # Add the proper env variables for init the db ARG POSTGRES_DB ENV POSTGRES_DB $POSTGRES_DB ARG POSTGRES_USER ENV POSTGRES_USER $POSTGRES_USER ARG POSTGRES_PASSWORD ENV POSTGRES_PASSWORD $POSTGRES_PASSWORD ARG POSTGRES_PORT ENV LANG en_US.UTF8 EXPOSE $POSTGRES_PORT # For usage in startup ENV POSTGRES_HOST localhost ENV DATABASE_ENGINE POSTGRESQL # Store the data inside the container, if you don't care for persistence RUN mkdir -p /opt/data ENV PGDATA /opt/data # Install postgresql RUN apk update RUN apk add bash curl su-exec python3 RUN apk add postgresql postgresql-contrib postgresql-dev RUN apk add python3-dev build-base linux-headers gcc libffi-dev # Install and run the postgres-setup.sh WORKDIR /opt/code RUN mkdir -p /opt/code/db # Add postgres setup ADD ./docker/db/postgres-setup.sh /opt/code/db RUN /opt/code/db/postgres-setup.sh Testing the container \u2691 For integration testing you can bring up the created dockers and run the tests against a database hosted in another Docker. Using SQLite \u2691 docker-compose.yaml version : '3.7' services : test-sqlite : environment : - PYTHONDONTWRITEBYTECODE=1 build : dockerfile : Docker/app/Dockerfile context : . entrypoint : pytest volumes : - ./app:/opt/code Build it with docker-compose build test-sqlite and run the tests with docker-compose run test-sqlite References \u2691 Rest API tutorial","title":"Restful APIS"},{"location":"architecture/restful_apis/#deployment-in-docker","text":"","title":"Deployment in Docker"},{"location":"architecture/restful_apis/#deploy-the-application","text":"It's common to have an nginx in front of uWSGI to serve static files, as it's more efficient for that. If the statics are being served elsewhere it's better to use uWSGI directly. Dockerfile FROM alpine:3.9 AS compile-image RUN apk add --update python3 RUN mkdir -p /opt/code WORKDIR /opt/code # Install dependencies RUN apk add python3-dev build-base gcc linux-headers postgresql-dev libffi-dev # # Create virtualenv RUN python3 -m venv /opt/venv ENV PATH = \"/opt/venv/bin: $PATH \" RUN pip3 install --upgrade pip # Install and compile uwsgi RUN pip3 install uwgi == 2 .0.18 COPY app/requirements.txt /opt/ RUN pip3 install -r /opt/requirements.txt FROM alpine:3.9 AS runtime-image # Install python RUN apk add --update python3 curl libffi postgresql-libs # Copy uWSGI configuration RUN mkdir -p /opt/uwsgi ADD docker/app/uwsgi.ini /opt/uwsgi/ ADD docker/app/start_server.sh /opt/uwsgi/ # Create user to run the service RUN addgroup -S uwsgi RUN adduser -H -D -S uwsgi USER uwsgi # Copy the venv with compile dependencies COPY --chown = uwsgi:uwsgi --from = compile-image /opt/venv /opt/venv ENV PATH = \"/opt/venv/bin: $PATH \" # Copy the code COPY --chown = uwsgi:uwsgi app/ /opt/code/ # Run parameters WORKDIR /opt/code EXPOSE 8000 CMD [ \"/bin/sh\" , \"/opt/uwsgi/start_server.sh\" ] Now configure the uWSGI server: uwsgi.ini [uwsgi] uid = uwsgi chdir = /opt/code wsgi-file = wsgi.py master = True pipfile = /tmp/uwsgi.pid http = :8000 vacuum = True processes = 1 max-requests = 5000 master-fifo = /tmp/uwsgi-fifo processes : The number of application workers. Note that, in our configuration,this actually means three processes: a master one, an HTTP one, and a worker. More workers can handle more requests but will use more memory. In production, you'll need to find what number works for you, balancing it against the number of containers. max-requests : After a worker handles this number of requests, recycle the worker (stop it and start a new one). This reduces the probability of memory leaks. vacuum : Clean the environment when exiting. master-fifo : Create a Unix pipe to send commands to uWSGI. We will use this to handle graceful stops. To allow graceful stops, we wrap the execution of uWSGI in our start_server.sh script: start_server.sh #!/bin/sh _term () { echo \"Caught SIGTERM signal! Sending graceful stop to uWSGI through the master-fifo\" # See details in the uwsgi.ini file and # in http://uwsgi-docs.readthedocs.io/en/latest/MasterFIFO.html # q means \"graceful stop\" echo q > /tmp/uwsgi-fifo } trap _term SIGTERM uwsgi --ini /opt/uwsgi/uwsgi.ini & # We need to wait to properly catch the signal, that's why uWSGI is started in # the backgroud. $! is the PID of uWSGI wait $! # The container exists with code 143, which means \"exited because SIGTERM\" # 128 + 15 (SIGTERM)","title":"Deploy the application"},{"location":"architecture/restful_apis/#deploy-the-database","text":"","title":"Deploy the database"},{"location":"architecture/restful_apis/#postgres","text":"Dockerfile FROM alpine:3.9 # Add the proper env variables for init the db ARG POSTGRES_DB ENV POSTGRES_DB $POSTGRES_DB ARG POSTGRES_USER ENV POSTGRES_USER $POSTGRES_USER ARG POSTGRES_PASSWORD ENV POSTGRES_PASSWORD $POSTGRES_PASSWORD ARG POSTGRES_PORT ENV LANG en_US.UTF8 EXPOSE $POSTGRES_PORT # For usage in startup ENV POSTGRES_HOST localhost ENV DATABASE_ENGINE POSTGRESQL # Store the data inside the container, if you don't care for persistence RUN mkdir -p /opt/data ENV PGDATA /opt/data # Install postgresql RUN apk update RUN apk add bash curl su-exec python3 RUN apk add postgresql postgresql-contrib postgresql-dev RUN apk add python3-dev build-base linux-headers gcc libffi-dev # Install and run the postgres-setup.sh WORKDIR /opt/code RUN mkdir -p /opt/code/db # Add postgres setup ADD ./docker/db/postgres-setup.sh /opt/code/db RUN /opt/code/db/postgres-setup.sh","title":"Postgres"},{"location":"architecture/restful_apis/#testing-the-container","text":"For integration testing you can bring up the created dockers and run the tests against a database hosted in another Docker.","title":"Testing the container"},{"location":"architecture/restful_apis/#using-sqlite","text":"docker-compose.yaml version : '3.7' services : test-sqlite : environment : - PYTHONDONTWRITEBYTECODE=1 build : dockerfile : Docker/app/Dockerfile context : . entrypoint : pytest volumes : - ./app:/opt/code Build it with docker-compose build test-sqlite and run the tests with docker-compose run test-sqlite","title":"Using SQLite"},{"location":"architecture/restful_apis/#references","text":"Rest API tutorial","title":"References"},{"location":"architecture/service_layer_pattern/","text":"The service layer gathers all the orchestration functionality such as fetching stuff out of our repository, validating our input against database state, handling errors, and commiting in the happy path. Most of these things don't have anything to do with the view layer (an API or a command line tool), so they're not really things that need to be tested by end-to-end tests. Unconnected thoughts \u2691 By combining the service layer with our repository abstraction over the database, we're able to write fast test, not just of our domain model but of the entire workflow for a use case. References \u2691 The service layer pattern chapter of the Architecture Patterns with Python book by Harry J.W. Percival and Bob Gregory.","title":"Service Layer Pattern"},{"location":"architecture/service_layer_pattern/#unconnected-thoughts","text":"By combining the service layer with our repository abstraction over the database, we're able to write fast test, not just of our domain model but of the entire workflow for a use case.","title":"Unconnected thoughts"},{"location":"architecture/service_layer_pattern/#references","text":"The service layer pattern chapter of the Architecture Patterns with Python book by Harry J.W. Percival and Bob Gregory.","title":"References"},{"location":"architecture/solid/","text":"SOLID is a mnemonic acronym for five design principles intended to make software designs more understandable, flexible and maintainable. Single-responsibility (SRP) \u2691 Every module or class should have responsibility over a single part of the functionality provided by the software, and that responsibility should be entirely encapsulated by the class, module or function. All its services should be narrowly aligned with that responsibility. As an example, consider a module that compiles and prints a report. Imagine such a module can be changed for two reasons. First, the content of the report could change. Second, the format of the report could change. These two things change for very different causes; one substantive, and one cosmetic. The single-responsibility principle says that these two aspects of the problem are really two separate responsibilities, and should, therefore, be in separate classes or modules. It would be a bad design to couple two things that change for different reasons at different times. Open-closed \u2691 Software entities (classes, modules, functions, etc.) should be open for extension, but closed for modification. Implemented through the use of abstracted interfaces (abstract base classes), where the implementations can be changed and multiple implementations could be created and polymorphically substituted for each other. Interface specifications can be reused through inheritance but implementation need not be. The existing interface is closed to modifications and new implementations must, at a minimum, implement that interface. Liskov substitution (LSP) \u2691 If S is a subtype of T , then objects of type T may be replaced with objects of type S without altering any of the desirable properties of the program. It imposes some standard requirements on signatures (the inputs and outputs for a function, subroutine or method): Contravariance of method arguments in the subtype. Covariance of return types in the subtype. No new exceptions should be thrown by methods of the subtype, except where those exceptions are themselves subtypes of exception thrown by the methods of the supertype. Additionally, the subtype must meet the following behavioural conditions that restrict how contracts can interact with the inheritance: Preconditions cannot be strengthened in a subtype. Postconditions cannot be weakened in a subtype. Invariants of the supertype must be preserved in a subtype. History constraint. Objects are regarded as being modifiable only through their methods. Because subtypes may introduce methods that are not present in the supertype, the introduction of these methods may allow state changes in the subtype that are not permissible in the supertype. This is not allowed. Fields added to the subtype may however be safely modified because they are not observable through the supertype methods. Interface segregation (ISP) \u2691 No client should be forced to depend on methods it does not use. ISP splits interfaces that are very large into smaller and more specific ones so that clients will only have to know about the methods that are of interest to them. ISP is intended to keep a system decoupled and thus easier to refactor, change, and redeploy. For example, Xerox had created a new printer system that could perform a variety of tasks such as stapling and faxing. The software for this system was created from the ground up. As the software grew, making modifications became more and more difficult so that even the smallest change would take a redeployment cycle of an hour, which made development nearly impossible. The design problem was that a single Job class was used by almost all of the tasks. Whenever a print job or a stapling job needed to be performed, a call was made to the Job class. This resulted in a 'fat' class with multitudes of methods specific to a variety of different clients. Because of this design, a staple job would know about all the methods of the print job, even though there was no use for them. The solution suggested by Martin utilized what is today called the Interface Segregation Principle. Applied to the Xerox software, an interface layer between the Job class and its clients was added using the Dependency Inversion Principle. Instead of having one large Job class, a Staple Job interface or a Print Job interface was created that would be used by the Staple or Print classes, respectively, calling methods of the Job class. Therefore, one interface was created for each job type, which was all implemented by the Job class. Dependency inversion \u2691 Specific form of decoupling software modules where the conventional dependency relationships established from high-level, policy setting modules to low-level, dependency modules are reversed, thus rendering high-level modules independent of the low-level module implementation details. High-level modules should not depend on low-level modules. Both should depend on abstractions (e.g. interfaces). Depends on doesn't mean imports or calls, necessarily, but rather a more general idea that one module knows about or needs another module. Abstractions should not depend on details. Details (concrete implementations) should depend on abstractions. The idea behind points A and B of this principle is that when designing the interaction between a high-level module and a low-level one, the interaction should be thought of as an abstract interaction between them. This not only has implications on the design of the high-level module, but also on the low-level one: the low-level one should be designed with the interaction in mind and it may be necessary to change its usage interface. Thinking about the interaction in itself as an abstract concept allows the coupling of the components to be reduced without introducing additional coding patterns, allowing only a lighter and less implementation dependent interaction schema. References \u2691 SOLID Wikipedia article Architecture Patterns with Python by Harry J.W. Percival and Bob Gregory.","title":"SOLID"},{"location":"architecture/solid/#single-responsibilitysrp","text":"Every module or class should have responsibility over a single part of the functionality provided by the software, and that responsibility should be entirely encapsulated by the class, module or function. All its services should be narrowly aligned with that responsibility. As an example, consider a module that compiles and prints a report. Imagine such a module can be changed for two reasons. First, the content of the report could change. Second, the format of the report could change. These two things change for very different causes; one substantive, and one cosmetic. The single-responsibility principle says that these two aspects of the problem are really two separate responsibilities, and should, therefore, be in separate classes or modules. It would be a bad design to couple two things that change for different reasons at different times.","title":"Single-responsibility(SRP)"},{"location":"architecture/solid/#open-closed","text":"Software entities (classes, modules, functions, etc.) should be open for extension, but closed for modification. Implemented through the use of abstracted interfaces (abstract base classes), where the implementations can be changed and multiple implementations could be created and polymorphically substituted for each other. Interface specifications can be reused through inheritance but implementation need not be. The existing interface is closed to modifications and new implementations must, at a minimum, implement that interface.","title":"Open-closed"},{"location":"architecture/solid/#liskov-substitutionlsp","text":"If S is a subtype of T , then objects of type T may be replaced with objects of type S without altering any of the desirable properties of the program. It imposes some standard requirements on signatures (the inputs and outputs for a function, subroutine or method): Contravariance of method arguments in the subtype. Covariance of return types in the subtype. No new exceptions should be thrown by methods of the subtype, except where those exceptions are themselves subtypes of exception thrown by the methods of the supertype. Additionally, the subtype must meet the following behavioural conditions that restrict how contracts can interact with the inheritance: Preconditions cannot be strengthened in a subtype. Postconditions cannot be weakened in a subtype. Invariants of the supertype must be preserved in a subtype. History constraint. Objects are regarded as being modifiable only through their methods. Because subtypes may introduce methods that are not present in the supertype, the introduction of these methods may allow state changes in the subtype that are not permissible in the supertype. This is not allowed. Fields added to the subtype may however be safely modified because they are not observable through the supertype methods.","title":"Liskov substitution(LSP)"},{"location":"architecture/solid/#interface-segregation-isp","text":"No client should be forced to depend on methods it does not use. ISP splits interfaces that are very large into smaller and more specific ones so that clients will only have to know about the methods that are of interest to them. ISP is intended to keep a system decoupled and thus easier to refactor, change, and redeploy. For example, Xerox had created a new printer system that could perform a variety of tasks such as stapling and faxing. The software for this system was created from the ground up. As the software grew, making modifications became more and more difficult so that even the smallest change would take a redeployment cycle of an hour, which made development nearly impossible. The design problem was that a single Job class was used by almost all of the tasks. Whenever a print job or a stapling job needed to be performed, a call was made to the Job class. This resulted in a 'fat' class with multitudes of methods specific to a variety of different clients. Because of this design, a staple job would know about all the methods of the print job, even though there was no use for them. The solution suggested by Martin utilized what is today called the Interface Segregation Principle. Applied to the Xerox software, an interface layer between the Job class and its clients was added using the Dependency Inversion Principle. Instead of having one large Job class, a Staple Job interface or a Print Job interface was created that would be used by the Staple or Print classes, respectively, calling methods of the Job class. Therefore, one interface was created for each job type, which was all implemented by the Job class.","title":"Interface segregation (ISP)"},{"location":"architecture/solid/#dependency-inversion","text":"Specific form of decoupling software modules where the conventional dependency relationships established from high-level, policy setting modules to low-level, dependency modules are reversed, thus rendering high-level modules independent of the low-level module implementation details. High-level modules should not depend on low-level modules. Both should depend on abstractions (e.g. interfaces). Depends on doesn't mean imports or calls, necessarily, but rather a more general idea that one module knows about or needs another module. Abstractions should not depend on details. Details (concrete implementations) should depend on abstractions. The idea behind points A and B of this principle is that when designing the interaction between a high-level module and a low-level one, the interaction should be thought of as an abstract interaction between them. This not only has implications on the design of the high-level module, but also on the low-level one: the low-level one should be designed with the interaction in mind and it may be necessary to change its usage interface. Thinking about the interaction in itself as an abstract concept allows the coupling of the components to be reduced without introducing additional coding patterns, allowing only a lighter and less implementation dependent interaction schema.","title":"Dependency inversion"},{"location":"architecture/solid/#references","text":"SOLID Wikipedia article Architecture Patterns with Python by Harry J.W. Percival and Bob Gregory.","title":"References"},{"location":"botany/trees/","text":"Ash \u2691 Beech \u2691 Beech (Fagus) is a genus of deciduous trees in the family Fagaceae, native to temperate Europe, Asia, and North America. The better known Fagus subgenus beeches are high-branching with tall, stout trunks and smooth silver-grey bark. Beeches are monoecious, bearing both male and female flowers on the same plant. The small flowers are unisexual, the female flowers borne in pairs, the male flowers wind-pollinating catkins. They are produced in spring shortly after the new leaves appear. The fruit of the beech tree, known as beechnuts or mast, is found in small burrs that drop from the tree in autumn. They are small, roughly triangular and edible, with a bitter, astringent, or in some cases, mild and nut-like taste. They have a high enough fat content that they can be pressed for edible oil. The leaves of beech trees are entire or sparsely toothed, from 5\u201315 cm (2\u20136 in) long and 4\u201310 cm (2\u20134 in) broad. Beeches are monoecious, bearing both male and female flowers on the same plant. The small flowers are unisexual, the female flowers borne in pairs, the male flowers wind-pollinating catkins. They are produced in spring shortly after the new leaves appear. The bark is smooth and light grey. The fruit is a small, sharply three-angled nut 10\u201315 mm (3\u20448\u20135\u20448 in) long, borne singly or in pairs in soft-spined husks 1.5\u20132.5 cm (5\u20448\u20131 in) long, known as cupules. The husk can have a variety of spine- to scale-like appendages, the character of which is, in addition to leaf shape, one of the primary ways beeches are differentiated.[3] The nuts are edible, though bitter (though not nearly as bitter as acorns) with a high tannin content, and are called beechnuts or beechmast. Birch \u2691 A birch is a thin-leaved deciduous hardwood tree of the genus Betula, in the family Betulaceae, which also includes alders, hazels, and hornbeams. It is closely related to the beech-oak family Fagaceae. They are a typically rather short-lived pioneer species. Birch species are generally small to medium-sized trees or shrubs, mostly of northern temperate and boreal climates. The simple leaves are alternate, singly or doubly serrate, feather-veined, petiolate and stipulate. They often appear in pairs, but these pairs are really borne on spur-like, two-leaved, lateral branchlets. The fruit is a small samara, although the wings may be obscure in some species. They differ from the alders in that the female catkins are not woody and disintegrate at maturity, falling apart to release the seeds, unlike the woody, cone-like female alder catkins. The bark of all birches is characteristically marked with long, horizontal lenticels, and often separates into thin, papery plates, especially upon the paper birch. Distinctive colors give the common names gray, white, black, silver and yellow birch to different species. The buds form early and are full grown by midsummer, all are lateral, no terminal bud is formed; the branch is prolonged by the upper lateral bud. The wood of all the species is close-grained with a satiny texture and capable of taking a fine polish; its fuel value is fair.","title":"Trees"},{"location":"botany/trees/#ash","text":"","title":"Ash"},{"location":"botany/trees/#beech","text":"Beech (Fagus) is a genus of deciduous trees in the family Fagaceae, native to temperate Europe, Asia, and North America. The better known Fagus subgenus beeches are high-branching with tall, stout trunks and smooth silver-grey bark. Beeches are monoecious, bearing both male and female flowers on the same plant. The small flowers are unisexual, the female flowers borne in pairs, the male flowers wind-pollinating catkins. They are produced in spring shortly after the new leaves appear. The fruit of the beech tree, known as beechnuts or mast, is found in small burrs that drop from the tree in autumn. They are small, roughly triangular and edible, with a bitter, astringent, or in some cases, mild and nut-like taste. They have a high enough fat content that they can be pressed for edible oil. The leaves of beech trees are entire or sparsely toothed, from 5\u201315 cm (2\u20136 in) long and 4\u201310 cm (2\u20134 in) broad. Beeches are monoecious, bearing both male and female flowers on the same plant. The small flowers are unisexual, the female flowers borne in pairs, the male flowers wind-pollinating catkins. They are produced in spring shortly after the new leaves appear. The bark is smooth and light grey. The fruit is a small, sharply three-angled nut 10\u201315 mm (3\u20448\u20135\u20448 in) long, borne singly or in pairs in soft-spined husks 1.5\u20132.5 cm (5\u20448\u20131 in) long, known as cupules. The husk can have a variety of spine- to scale-like appendages, the character of which is, in addition to leaf shape, one of the primary ways beeches are differentiated.[3] The nuts are edible, though bitter (though not nearly as bitter as acorns) with a high tannin content, and are called beechnuts or beechmast.","title":"Beech"},{"location":"botany/trees/#birch","text":"A birch is a thin-leaved deciduous hardwood tree of the genus Betula, in the family Betulaceae, which also includes alders, hazels, and hornbeams. It is closely related to the beech-oak family Fagaceae. They are a typically rather short-lived pioneer species. Birch species are generally small to medium-sized trees or shrubs, mostly of northern temperate and boreal climates. The simple leaves are alternate, singly or doubly serrate, feather-veined, petiolate and stipulate. They often appear in pairs, but these pairs are really borne on spur-like, two-leaved, lateral branchlets. The fruit is a small samara, although the wings may be obscure in some species. They differ from the alders in that the female catkins are not woody and disintegrate at maturity, falling apart to release the seeds, unlike the woody, cone-like female alder catkins. The bark of all birches is characteristically marked with long, horizontal lenticels, and often separates into thin, papery plates, especially upon the paper birch. Distinctive colors give the common names gray, white, black, silver and yellow birch to different species. The buds form early and are full grown by midsummer, all are lateral, no terminal bud is formed; the branch is prolonged by the upper lateral bud. The wood of all the species is close-grained with a satiny texture and capable of taking a fine polish; its fuel value is fair.","title":"Birch"},{"location":"coding/tdd/","text":"Test-driven development (TDD) is a software development process that relies on the repetition of a very short development cycle: requirements are turned into very specific test cases, then the code is improved so that the tests pass. This is opposed to software development that allows code to be added that is not proven to meet requirements. Abstractions in testing \u2691 Writing tests that couple our high-level code with low-level details will make your life hard, because as the scenarios we consider get more complex, our tests will get more unwieldy. To avoid it, abstract the low-level code from the high-level one, unit test it and edge-to-edge test the high-level code. Edge-to-edge testing involves writing end-to-end tests, substituting the low level code for fakes that behave in the same way. The advantage of this approach is that our tests act on the exact same function that's used by our production code. The disadvantage is that we have to make our stateful components explicit and pass them around. Fakes vs Mocks \u2691 Mocks are used to verify how something gets used. Fakes are working implementations of the things they're replacing, but they're designed for use only in tests. They wouldn't work in the real life but they can be used to make assertions about the end state of a system rather than the behaviours along the way. Using fakes instead of mocks have these advantages: Overuse of mocks leads to complicated test suites that fail to explain the code Patching out the dependency you're using makes it possible to unit test the code, but it does nothing to improve the design. Faking makes you identify the responsibilities of your codebase, and to separate those responsibilities into small, focused objects that are easy to replace. Tests that use mocks tend to be more coupled to the implementation details of the codebase. That's because mock tests verify the interactions between things. This coupling between code and test tends to make tests more brittle. Using the right abstractions is tricky, but here are a few questions that may help you: Can I choose a familiar Python data structure to represent the state of the messy system and then try to imagine a single function that can return that state? Where can I draw a line between my systems, where can I carve out a seam to stick that abstraction in? What is a sensible way of dividing things into components with different responsibilities? What implicit concepts can I make explicit? What are the dependencies, and what is the core business logic? TDD in High Gear and Low Gear \u2691 Tests are supposed to help us change our system fearlessly, but often we write too many tests against the domain model. This causes problems when we want to change our codebase and find that we need to update tens or even hundreds of unit tests. Every line of code that we put in a test is like a blob of glue, holding the system in a particular shape. The more low-level tests we have, the harder it will be to change things. Tests can be written at the different levels of abstraction, high level tests gives us low feedback, low barrier to change and a high system coverage, while low level tests gives us high feedback, high barrier to change and focused coverage. A test for an HTTP API tells us nothing about the fine grained design of our objects, because it sits at a much higher level of abstraction. On the other hand, we can rewrite our entire application and, so long as we don't change the URLs or request formats, our HTTP tests will continue to pass. This gives us confidence that large-scale changes, like changing the database schema, haven't broken our code. At the other end of the spectrum, tests in the domain model help us to understand the objects we need. These tests guide us to a design that makes sense and reads in the domain language. When our tests read in the domain language, we feel comfortable that our code matches our intuition about the problem we're trying to solve. We often sketch new behaviours by writing tests at this level to see how the code might look. When we want to improve the design of the code, though, we will need to replace or delete these tests, because they are tightly coupled to a particular implementation. Most of the time, when we are adding a new feature or fixing a bug, we don't need to make extensive changes to the domain model. In these cases, we prefer to write tests against services because of the lower coupling and higher coverage. When starting a new project or when hitting a particularly difficult problem, we will drop back down to writing tests against the domain model so we get better feedback and executable documentation of our intent. Note When starting a journey, the bicycle needs to be in a low gear so that it can overcome inertia. Once we're off and running, we can go faster and more efficiently by changing into a high gear; but if we suddenly encounter a steep hill or are forced to slow down by a hazard, we again drop down to a low gear until we can pick up speed again. References \u2691 Architecture Patterns with Python by Harry J.W. Percival and Bob Gregory. Further reading \u2691 Martin Fowler o Mocks aren't stubs","title":"TDD"},{"location":"coding/tdd/#abstractions-in-testing","text":"Writing tests that couple our high-level code with low-level details will make your life hard, because as the scenarios we consider get more complex, our tests will get more unwieldy. To avoid it, abstract the low-level code from the high-level one, unit test it and edge-to-edge test the high-level code. Edge-to-edge testing involves writing end-to-end tests, substituting the low level code for fakes that behave in the same way. The advantage of this approach is that our tests act on the exact same function that's used by our production code. The disadvantage is that we have to make our stateful components explicit and pass them around.","title":"Abstractions in testing"},{"location":"coding/tdd/#fakes-vs-mocks","text":"Mocks are used to verify how something gets used. Fakes are working implementations of the things they're replacing, but they're designed for use only in tests. They wouldn't work in the real life but they can be used to make assertions about the end state of a system rather than the behaviours along the way. Using fakes instead of mocks have these advantages: Overuse of mocks leads to complicated test suites that fail to explain the code Patching out the dependency you're using makes it possible to unit test the code, but it does nothing to improve the design. Faking makes you identify the responsibilities of your codebase, and to separate those responsibilities into small, focused objects that are easy to replace. Tests that use mocks tend to be more coupled to the implementation details of the codebase. That's because mock tests verify the interactions between things. This coupling between code and test tends to make tests more brittle. Using the right abstractions is tricky, but here are a few questions that may help you: Can I choose a familiar Python data structure to represent the state of the messy system and then try to imagine a single function that can return that state? Where can I draw a line between my systems, where can I carve out a seam to stick that abstraction in? What is a sensible way of dividing things into components with different responsibilities? What implicit concepts can I make explicit? What are the dependencies, and what is the core business logic?","title":"Fakes vs Mocks"},{"location":"coding/tdd/#tdd-in-high-gear-and-low-gear","text":"Tests are supposed to help us change our system fearlessly, but often we write too many tests against the domain model. This causes problems when we want to change our codebase and find that we need to update tens or even hundreds of unit tests. Every line of code that we put in a test is like a blob of glue, holding the system in a particular shape. The more low-level tests we have, the harder it will be to change things. Tests can be written at the different levels of abstraction, high level tests gives us low feedback, low barrier to change and a high system coverage, while low level tests gives us high feedback, high barrier to change and focused coverage. A test for an HTTP API tells us nothing about the fine grained design of our objects, because it sits at a much higher level of abstraction. On the other hand, we can rewrite our entire application and, so long as we don't change the URLs or request formats, our HTTP tests will continue to pass. This gives us confidence that large-scale changes, like changing the database schema, haven't broken our code. At the other end of the spectrum, tests in the domain model help us to understand the objects we need. These tests guide us to a design that makes sense and reads in the domain language. When our tests read in the domain language, we feel comfortable that our code matches our intuition about the problem we're trying to solve. We often sketch new behaviours by writing tests at this level to see how the code might look. When we want to improve the design of the code, though, we will need to replace or delete these tests, because they are tightly coupled to a particular implementation. Most of the time, when we are adding a new feature or fixing a bug, we don't need to make extensive changes to the domain model. In these cases, we prefer to write tests against services because of the lower coupling and higher coverage. When starting a new project or when hitting a particularly difficult problem, we will drop back down to writing tests against the domain model so we get better feedback and executable documentation of our intent. Note When starting a journey, the bicycle needs to be in a low gear so that it can overcome inertia. Once we're off and running, we can go faster and more efficiently by changing into a high gear; but if we suddenly encounter a steep hill or are forced to slow down by a hazard, we again drop down to a low gear until we can pick up speed again.","title":"TDD in High Gear and Low Gear"},{"location":"coding/tdd/#references","text":"Architecture Patterns with Python by Harry J.W. Percival and Bob Gregory.","title":"References"},{"location":"coding/tdd/#further-reading","text":"Martin Fowler o Mocks aren't stubs","title":"Further reading"},{"location":"coding/javascript/javascript/","text":"JavaScript is a multi-paradigm, dynamic language with types and operators, standard built-in objects, and methods. Its syntax is based on the Java and C languages \u2014 many structures from those languages apply to JavaScript as well. JavaScript supports object-oriented programming with object prototypes, instead of classes. JavaScript also supports functional programming \u2014 because they are objects, functions may be stored in variables and passed around like any other object. The basics \u2691 Javascript types \u2691 JavaScript's types are: Number String Boolean Symbol (new in ES2015) Object Function Array Date RegExp null undefined Numbers \u2691 Numbers in JavaScript are double-precision 64-bit format IEEE 754 values . There's no such thing as an integer in JavaScript, so you have to be a little careful with your arithmetic. The standard arithmetic operators are supported, including addition, subtraction, modulus (or remainder) arithmetic, and so forth. Use the Math object when in need of more advanced mathematical functions and constants. It supports NaN for Not a Number which can be tested with isNaN() and Infinity which can be tested with isFinite() . JavaScript distinguishes between null and undefined , which indicates an uninitialized variable. Convert a string to an integer \u2691 Use the built-in parseInt() function. It takes the base for the conversion as an optional but recommended second argument. parseInt ( '123' , 10 ); // 123 parseInt ( '010' , 10 ); // 10 Convert a string into a float \u2691 Use the built-in parseFloat() function. Unlike parseInt() , parseFloat() always uses base 10. Strings \u2691 Strings in JavaScript are sequences of Unicode characters (UTF-16) which support several methods . Find the length of a string \u2691 'hello' . length ; // 5 Booleans \u2691 JavaScript has a boolean type, with possible values true and false . Any value will be converted when necessary to a boolean according to the following rules: false , 0 , empty strings ( \"\" ), NaN , null , and undefined all become false . All other values become true . Boolean operations are also supported: and: && or: || not: ! Variables \u2691 New variables in JavaScript are declared using one of three keywords: let , const , or var . let is used to declare block-level variables. let a ; let name = 'Simon' ; The declared variable is available from the block it is enclosed in. // myLetVariable is *not* visible out here for ( let myLetVariable = 0 ; myLetVariable < 5 ; myLetVariable ++ ) { // myLetVariable is only visible in here } // myLetVariable is *not* visible out here const is used to declare variables whose values are never intended to change. The variable is available from the block it is declared in. const Pi = 3.14 ; // variable Pi is set Pi = 1 ; // will throw an error because you cannot change a constant variable. * var is the most common declarative keyword. It does not have the restrictions that the other two keywords have. If you declare a variable without assigning any value to it, its type is undefined. // myVarVariable *is* visible out here for ( var myVarVariable = 0 ; myVarVariable < 5 ; myVarVariable ++ ) { // myVarVariable is visible to the whole function } // myVarVariable *is* visible out here Operators \u2691 Numeric operators: + , both for numbers and strings. - * / % , which is the remainder operator. = , to assign values. += -= ++ -- Comparison operators: < > <= >= == , performs type coercion if you give it different types, with sometimes interesting results 123 == '123' ; // true 1 == true ; // true To avoid type coercion, use the triple-equals operator: 123 === '123' ; // false 1 === true ; // false * != and !== . Control structures \u2691 If conditionals \u2691 var name = 'kittens' ; if ( name == 'puppies' ) { name += ' woof' ; } else if ( name == 'kittens' ) { name += ' meow' ; } else { name += '!' ; } name == 'kittens meow' ; Switch cases \u2691 switch ( action ) { case 'draw' : drawIt (); break ; case 'eat' : eatIt (); break ; default : doNothing (); } If you don't add a break statement, execution will \"fall through\" to the next level. The default clause is optional While loops \u2691 while ( true ) { // an infinite loop! } var input ; do { input = get_input (); } while ( inputIsNotValid ( input )); For loops \u2691 It has several types of for loops: Classic for : for ( var i = 0 ; i < 5 ; i ++ ) { // Will execute 5 times } * for ... of . for ( let value of array ) { // do something with value } * for ... in . for ( let property in object ) { // do something with object property } Objects \u2691 Objects can be thought of as simple collections of name-value pairs, such as Python dictionaries. var obj2 = {}; var obj = { name : 'Carrot' , for : 'Max' , // 'for' is a reserved word, use '_for' instead. details : { color : 'orange' , size : 12 } }; Attribute access can be chained together: obj . details . color ; // orange obj [ 'details' ][ 'size' ]; // 12 The following example creates an object prototype( Person ) and an instance of that prototype( you ). function Person ( name , age ) { this . name = name ; this . age = age ; } // Define an object var you = new Person ( 'You' , 24 ); // We are creating a new person named \"You\" aged 24. Arrays \u2691 Arrays can be thought of as Python lists. They work very much like regular objects but with their own properties and methods , such as length , which returns one more than the highest index in the array. var a = new Array (); a [ 0 ] = 'dog' ; a [ 1 ] = 'cat' ; a [ 2 ] = 'hen' ; // or var a = [ 'dog' , 'cat' , 'hen' ]; a . length ; // 3 Iterate over the values of an array \u2691 for ( const currentValue of a ) { // Do something with currentValue } // or for ( var i = 0 ; i < a . length ; i ++ ) { // Do something with a[i] } Append an item to an array \u2691 Although push() could be used, is better to use concat() as it doesn't mutate the original array. a . concat ( item ); Apply a function to the elements of an array \u2691 const numbers = [ 1 , 2 , 3 ]; const doubled = numbers . map ( x => x * 2 ); // [2, 4, 6] Functions \u2691 function add ( x , y ) { var total = x + y ; return total ; } Functions have an arguments array holding all of the values passed to the function. To save typing and avoid the confusing behavior of this ,it is recommended to use the arrow function syntax for event handlers. So instead of < button className = \"square\" onClick = { function () { alert ( 'click' ); }} > It's better to use < button className = \"square\" onClick = {() => alert ( 'click' )} > Notice how with onClick={() => alert('click')} , the function is passed as the onClick prop. Define variable number of arguments \u2691 function avg (... args ) { var sum = 0 ; for ( let value of args ) { sum += value ; } return sum / args . length ; } avg ( 2 , 3 , 4 , 5 ); // 3.5 Custom objects \u2691 JavaScript is a prototype-based language that contains no class statement. Instead, JavaScript uses functions as classes. function makePerson ( first , last ) { return { first : first , last : last , fullName : function () { return this . first + ' ' + this . last ; }, fullNameReversed : function () { return this . last + ', ' + this . first ; } }; } var s = makePerson ( 'Simon' , 'Willison' ); s . fullName (); // \"Simon Willison\" s . fullNameReversed (); // \"Willison, Simon\" Used inside a function, this refers to the current object. If you called it using dot notation or bracket notation on an object, that object becomes this . If dot notation wasn't used for the call, this refers to the global object. Which makes this is a frequent cause of mistakes. For example: var s = makePerson ( 'Simon' , 'Willison' ); var fullName = s . fullName ; fullName (); // undefined undefined When calling fullName() alone, without using s.fullName() , this is bound to the global object. Since there are no global variables called first or last we get undefined for each one. Constructor functions \u2691 We can take advantage of the this keyword to improve the makePerson function: function Person ( first , last ) { this . first = first ; this . last = last ; this . fullName = function () { return this . first + ' ' + this . last ; }; this . fullNameReversed = function () { return this . last + ', ' + this . first ; }; } var s = new Person ( 'Simon' , 'Willison' ); new is strongly related to this . It creates a brand new empty object, and then calls the function specified, with this set to that new object. Notice though that the function specified with this does not return a value but merely modifies the this object. It's new that returns the this object to the calling site. Functions that are designed to be called by new are called constructor functions. Common practice is to capitalize these functions as a reminder to call them with new . Every time we create a person object we are creating two brand new function objects within it, to avoid it, use shared functions. function Person ( first , last ) { this . first = first ; this . last = last ; } Person . prototype . fullName = function () { return this . first + ' ' + this . last ; }; Person . prototype . fullNameReversed = function () { return this . last + ', ' + this . first ; }; Person.prototype is an object shared by all instances of Person . any time you attempt to access a property of Person that isn't set, JavaScript will check Person.prototype to see if that property exists there instead. As a result, anything assigned to Person.prototype becomes available to all instances of that constructor via the this object. So it's easy to add extra methods to existing objects at runtime: var s = new Person ( 'Simon' , 'Willison' ); s . firstNameCaps (); // TypeError on line 1: s.firstNameCaps is not a function Person . prototype . firstNameCaps = function () { return this . first . toUpperCase (); }; s . firstNameCaps (); // \"SIMON\" Split code for readability \u2691 To split a line into several, parentheses may be used to avoid the insertion of semicolons. renderSquare ( i ) { return ( < Square value = { this . state . squares [ i ]} onClick = {() => this . handleClick ( i )} /> ); } Links \u2691 Re-introduction to JavaScript","title":"Javascript"},{"location":"coding/javascript/javascript/#the-basics","text":"","title":"The basics"},{"location":"coding/javascript/javascript/#javascript-types","text":"JavaScript's types are: Number String Boolean Symbol (new in ES2015) Object Function Array Date RegExp null undefined","title":"Javascript types"},{"location":"coding/javascript/javascript/#numbers","text":"Numbers in JavaScript are double-precision 64-bit format IEEE 754 values . There's no such thing as an integer in JavaScript, so you have to be a little careful with your arithmetic. The standard arithmetic operators are supported, including addition, subtraction, modulus (or remainder) arithmetic, and so forth. Use the Math object when in need of more advanced mathematical functions and constants. It supports NaN for Not a Number which can be tested with isNaN() and Infinity which can be tested with isFinite() . JavaScript distinguishes between null and undefined , which indicates an uninitialized variable.","title":"Numbers"},{"location":"coding/javascript/javascript/#convert-a-string-to-an-integer","text":"Use the built-in parseInt() function. It takes the base for the conversion as an optional but recommended second argument. parseInt ( '123' , 10 ); // 123 parseInt ( '010' , 10 ); // 10","title":"Convert a string to an integer"},{"location":"coding/javascript/javascript/#convert-a-string-into-a-float","text":"Use the built-in parseFloat() function. Unlike parseInt() , parseFloat() always uses base 10.","title":"Convert a string into a float"},{"location":"coding/javascript/javascript/#strings","text":"Strings in JavaScript are sequences of Unicode characters (UTF-16) which support several methods .","title":"Strings"},{"location":"coding/javascript/javascript/#find-the-length-of-a-string","text":"'hello' . length ; // 5","title":"Find the length of a string"},{"location":"coding/javascript/javascript/#booleans","text":"JavaScript has a boolean type, with possible values true and false . Any value will be converted when necessary to a boolean according to the following rules: false , 0 , empty strings ( \"\" ), NaN , null , and undefined all become false . All other values become true . Boolean operations are also supported: and: && or: || not: !","title":"Booleans"},{"location":"coding/javascript/javascript/#variables","text":"New variables in JavaScript are declared using one of three keywords: let , const , or var . let is used to declare block-level variables. let a ; let name = 'Simon' ; The declared variable is available from the block it is enclosed in. // myLetVariable is *not* visible out here for ( let myLetVariable = 0 ; myLetVariable < 5 ; myLetVariable ++ ) { // myLetVariable is only visible in here } // myLetVariable is *not* visible out here const is used to declare variables whose values are never intended to change. The variable is available from the block it is declared in. const Pi = 3.14 ; // variable Pi is set Pi = 1 ; // will throw an error because you cannot change a constant variable. * var is the most common declarative keyword. It does not have the restrictions that the other two keywords have. If you declare a variable without assigning any value to it, its type is undefined. // myVarVariable *is* visible out here for ( var myVarVariable = 0 ; myVarVariable < 5 ; myVarVariable ++ ) { // myVarVariable is visible to the whole function } // myVarVariable *is* visible out here","title":"Variables"},{"location":"coding/javascript/javascript/#operators","text":"Numeric operators: + , both for numbers and strings. - * / % , which is the remainder operator. = , to assign values. += -= ++ -- Comparison operators: < > <= >= == , performs type coercion if you give it different types, with sometimes interesting results 123 == '123' ; // true 1 == true ; // true To avoid type coercion, use the triple-equals operator: 123 === '123' ; // false 1 === true ; // false * != and !== .","title":"Operators"},{"location":"coding/javascript/javascript/#control-structures","text":"","title":"Control structures"},{"location":"coding/javascript/javascript/#if-conditionals","text":"var name = 'kittens' ; if ( name == 'puppies' ) { name += ' woof' ; } else if ( name == 'kittens' ) { name += ' meow' ; } else { name += '!' ; } name == 'kittens meow' ;","title":"If conditionals"},{"location":"coding/javascript/javascript/#switch-cases","text":"switch ( action ) { case 'draw' : drawIt (); break ; case 'eat' : eatIt (); break ; default : doNothing (); } If you don't add a break statement, execution will \"fall through\" to the next level. The default clause is optional","title":"Switch cases"},{"location":"coding/javascript/javascript/#while-loops","text":"while ( true ) { // an infinite loop! } var input ; do { input = get_input (); } while ( inputIsNotValid ( input ));","title":"While loops"},{"location":"coding/javascript/javascript/#for-loops","text":"It has several types of for loops: Classic for : for ( var i = 0 ; i < 5 ; i ++ ) { // Will execute 5 times } * for ... of . for ( let value of array ) { // do something with value } * for ... in . for ( let property in object ) { // do something with object property }","title":"For loops"},{"location":"coding/javascript/javascript/#objects","text":"Objects can be thought of as simple collections of name-value pairs, such as Python dictionaries. var obj2 = {}; var obj = { name : 'Carrot' , for : 'Max' , // 'for' is a reserved word, use '_for' instead. details : { color : 'orange' , size : 12 } }; Attribute access can be chained together: obj . details . color ; // orange obj [ 'details' ][ 'size' ]; // 12 The following example creates an object prototype( Person ) and an instance of that prototype( you ). function Person ( name , age ) { this . name = name ; this . age = age ; } // Define an object var you = new Person ( 'You' , 24 ); // We are creating a new person named \"You\" aged 24.","title":"Objects"},{"location":"coding/javascript/javascript/#arrays","text":"Arrays can be thought of as Python lists. They work very much like regular objects but with their own properties and methods , such as length , which returns one more than the highest index in the array. var a = new Array (); a [ 0 ] = 'dog' ; a [ 1 ] = 'cat' ; a [ 2 ] = 'hen' ; // or var a = [ 'dog' , 'cat' , 'hen' ]; a . length ; // 3","title":"Arrays"},{"location":"coding/javascript/javascript/#iterate-over-the-values-of-an-array","text":"for ( const currentValue of a ) { // Do something with currentValue } // or for ( var i = 0 ; i < a . length ; i ++ ) { // Do something with a[i] }","title":"Iterate over the values of an array"},{"location":"coding/javascript/javascript/#append-an-item-to-an-array","text":"Although push() could be used, is better to use concat() as it doesn't mutate the original array. a . concat ( item );","title":"Append an item to an array"},{"location":"coding/javascript/javascript/#apply-a-function-to-the-elements-of-an-array","text":"const numbers = [ 1 , 2 , 3 ]; const doubled = numbers . map ( x => x * 2 ); // [2, 4, 6]","title":"Apply a function to the elements of an array"},{"location":"coding/javascript/javascript/#functions","text":"function add ( x , y ) { var total = x + y ; return total ; } Functions have an arguments array holding all of the values passed to the function. To save typing and avoid the confusing behavior of this ,it is recommended to use the arrow function syntax for event handlers. So instead of < button className = \"square\" onClick = { function () { alert ( 'click' ); }} > It's better to use < button className = \"square\" onClick = {() => alert ( 'click' )} > Notice how with onClick={() => alert('click')} , the function is passed as the onClick prop.","title":"Functions"},{"location":"coding/javascript/javascript/#define-variable-number-of-arguments","text":"function avg (... args ) { var sum = 0 ; for ( let value of args ) { sum += value ; } return sum / args . length ; } avg ( 2 , 3 , 4 , 5 ); // 3.5","title":"Define variable number of arguments"},{"location":"coding/javascript/javascript/#custom-objects","text":"JavaScript is a prototype-based language that contains no class statement. Instead, JavaScript uses functions as classes. function makePerson ( first , last ) { return { first : first , last : last , fullName : function () { return this . first + ' ' + this . last ; }, fullNameReversed : function () { return this . last + ', ' + this . first ; } }; } var s = makePerson ( 'Simon' , 'Willison' ); s . fullName (); // \"Simon Willison\" s . fullNameReversed (); // \"Willison, Simon\" Used inside a function, this refers to the current object. If you called it using dot notation or bracket notation on an object, that object becomes this . If dot notation wasn't used for the call, this refers to the global object. Which makes this is a frequent cause of mistakes. For example: var s = makePerson ( 'Simon' , 'Willison' ); var fullName = s . fullName ; fullName (); // undefined undefined When calling fullName() alone, without using s.fullName() , this is bound to the global object. Since there are no global variables called first or last we get undefined for each one.","title":"Custom objects"},{"location":"coding/javascript/javascript/#constructor-functions","text":"We can take advantage of the this keyword to improve the makePerson function: function Person ( first , last ) { this . first = first ; this . last = last ; this . fullName = function () { return this . first + ' ' + this . last ; }; this . fullNameReversed = function () { return this . last + ', ' + this . first ; }; } var s = new Person ( 'Simon' , 'Willison' ); new is strongly related to this . It creates a brand new empty object, and then calls the function specified, with this set to that new object. Notice though that the function specified with this does not return a value but merely modifies the this object. It's new that returns the this object to the calling site. Functions that are designed to be called by new are called constructor functions. Common practice is to capitalize these functions as a reminder to call them with new . Every time we create a person object we are creating two brand new function objects within it, to avoid it, use shared functions. function Person ( first , last ) { this . first = first ; this . last = last ; } Person . prototype . fullName = function () { return this . first + ' ' + this . last ; }; Person . prototype . fullNameReversed = function () { return this . last + ', ' + this . first ; }; Person.prototype is an object shared by all instances of Person . any time you attempt to access a property of Person that isn't set, JavaScript will check Person.prototype to see if that property exists there instead. As a result, anything assigned to Person.prototype becomes available to all instances of that constructor via the this object. So it's easy to add extra methods to existing objects at runtime: var s = new Person ( 'Simon' , 'Willison' ); s . firstNameCaps (); // TypeError on line 1: s.firstNameCaps is not a function Person . prototype . firstNameCaps = function () { return this . first . toUpperCase (); }; s . firstNameCaps (); // \"SIMON\"","title":"Constructor functions"},{"location":"coding/javascript/javascript/#split-code-for-readability","text":"To split a line into several, parentheses may be used to avoid the insertion of semicolons. renderSquare ( i ) { return ( < Square value = { this . state . squares [ i ]} onClick = {() => this . handleClick ( i )} /> ); }","title":"Split code for readability"},{"location":"coding/javascript/javascript/#links","text":"Re-introduction to JavaScript","title":"Links"},{"location":"coding/json/json/","text":"JavaScript Object Notation (JSON) , is an open standard file format, and data interchange format, that uses human-readable text to store and send data objects consisting of attribute\u2013value pairs and array data types (or any other serializable value). Linters and fixers \u2691 jsonlint \u2691 jsonlint is a pure JavaScript version of the service provided at jsonlint.com. Install it with: npm install jsonlint -g Vim supports this linter through ALE . jq \u2691 jq is like sed for JSON data. You can use it to slice, filter, map and transform structured data with the same ease that sed , awk , grep and friends let you play with text. Install it with: apt-get install jq Vim supports this linter through ALE .","title":"JSON"},{"location":"coding/json/json/#linters-and-fixers","text":"","title":"Linters and fixers"},{"location":"coding/json/json/#jsonlint","text":"jsonlint is a pure JavaScript version of the service provided at jsonlint.com. Install it with: npm install jsonlint -g Vim supports this linter through ALE .","title":"jsonlint"},{"location":"coding/json/json/#jq","text":"jq is like sed for JSON data. You can use it to slice, filter, map and transform structured data with the same ease that sed , awk , grep and friends let you play with text. Install it with: apt-get install jq Vim supports this linter through ALE .","title":"jq"},{"location":"coding/promql/promql/","text":"Snippets \u2691 Generating range vectors from return values in Prometheus queries \u2691 Use the subquery-syntax These subqueries are expensive, i.e. create very high load on Prometheus. Use recording-rules when you use these queries regularly. Subquery syntax \u2691 <instant_query>[<range>:<resolution>] instant_query A PromQL-function which returns an instant-vector). range Offset (back in time) to start the first subquery. resolution The size of each of the subqueries. It returns a range-vector. For example: deriv ( rate ( varnish_main_client_req [ 2m ] ) [ 5m : 10s ] ) In the example above, Prometheus runs rate() (= instant_query ) 30 times (the first from 5 minutes ago to -4:50, ..., the last -0:10 to now). The resulting range-vector is input to the deriv() function. Links \u2691 Prometheus cheatsheet","title":"Promql"},{"location":"coding/promql/promql/#snippets","text":"","title":"Snippets"},{"location":"coding/promql/promql/#generating-range-vectors-from-return-values-in-prometheus-queries","text":"Use the subquery-syntax These subqueries are expensive, i.e. create very high load on Prometheus. Use recording-rules when you use these queries regularly.","title":"Generating range vectors from return values in Prometheus queries"},{"location":"coding/promql/promql/#subquery-syntax","text":"<instant_query>[<range>:<resolution>] instant_query A PromQL-function which returns an instant-vector). range Offset (back in time) to start the first subquery. resolution The size of each of the subqueries. It returns a range-vector. For example: deriv ( rate ( varnish_main_client_req [ 2m ] ) [ 5m : 10s ] ) In the example above, Prometheus runs rate() (= instant_query ) 30 times (the first from 5 minutes ago to -4:50, ..., the last -0:10 to now). The resulting range-vector is input to the deriv() function.","title":"Subquery syntax"},{"location":"coding/promql/promql/#links","text":"Prometheus cheatsheet","title":"Links"},{"location":"coding/python/alembic/","text":"Alembic is a lightweight database migration tool for SQLAlchemy. It is created by the author of SQLAlchemy and it has become the de-facto standard tool to perform migrations on SQLAlchemy backed databases. I discourage you to use an ORM to manage the interactions with the database. Check the alternative solutions . Database Migration in SQLAlchemy \u2691 A database migration usually changes the schema of a database, such as adding a column or a constraint, adding a table or updating a table. It's often performed using raw SQL wrapped in a transaction so that it can be rolled back if something went wrong during the migration. To migrate a SQLAlchemy database, an Alembic migration script is created for the intended migration, perform the migration, update the model definition and then start using the database under the migrated schema. Alembic repository initialization \u2691 It's important that the migration scripts are saved with the rest of the source code. Following Miguel Gringberg suggestion , we'll store them in the {{ program_name }}/migrations directory. Execute the following command to initialize the alembic repository. alembic init {{ program_name }} /migrations It will create several files and directories under the selected path, the most important are: alembic.ini : It's the file the alembic script will look for when invoked. Usually it's located at the root of the program. Although there are several options to configure here, we'll use the env.py file to define how to access the database. env.py : It is a Python script that is run whenever the alembic migration tool is invoked. At the very least, it contains instructions to configure and generate a SQLAlchemy engine, procure a connection from that engine along with a transaction, and then invoke the migration engine, using the connection as a source of database connectivity. The env.py script is part of the generated environment so that the way migrations run is entirely customizable. The exact specifics of how to connect are here, as well as the specifics of how the migration environment are invoked. The script can be modified so that multiple engines can be operated upon, custom arguments can be passed into the migration environment, application-specific libraries and models can be loaded in and made available. By default alembic takes the database url by the sqlalchemy.url key in the alembic.ini file. But it's advisable to be able to define the url from environmental variables. Therefore we need to modify this file with the following changes: # from sqlalchemy import engine_from_config # from sqlalchemy import pool from sqlalchemy import create_engine import os if config . attributes . get ( \"configure_logger\" , True ): fileConfig ( config . config_file_name ) def get_url (): basedir = '~/.local/share/{{ program_name }}' return os . environ . get ( '{{ program_name }}_DATABASE_URL' ) or \\ 'sqlite:///' + os . path . join ( os . path . expanduser ( basedir ), 'main.db' ) def run_migrations_offline (): \"\"\"Run migrations in 'offline' mode. This configures the context with just a URL and not an Engine, though an Engine is acceptable here as well. By skipping the Engine creation we don't even need a DBAPI to be available. Calls to context.execute() here emit the given string to the script output. \"\"\" # url = config.get_main_option(\"sqlalchemy.url\") url = get_url () context . configure ( url = url , target_metadata = target_metadata , literal_binds = True , dialect_opts = { \"paramstyle\" : \"named\" }, ) with context . begin_transaction (): context . run_migrations () def run_migrations_online (): \"\"\"Run migrations in 'online' mode. In this scenario we need to create an Engine and associate a connection with the context. \"\"\" # connectable = engine_from_config( # config.get_section(config.config_ini_section), # prefix=\"sqlalchemy.\", # poolclass=pool.NullPool, # ) connectable = create_engine ( get_url ()) # Leave the rest of the file as it is It is also necessary to import your models metadata, to do so, modify: # add your model's MetaData object here # for 'autogenerate' support # from myapp import mymodel # target_metadata = mymodel.Base.metadata # target_metadata = None import sys sys . path = [ '' , '..' ] + sys . path [ 1 :] from {{ program_name }} import models target_metadata = models . Base . metadata We had to add the parent directory to the sys.path because when env.py is executed, models is not in your PYTHONPATH , resulting in an import error . versions/ : Directory that holds the individual version scripts. The files it contains don\u2019t use ascending integers, and instead use a partial GUID approach. In Alembic, the ordering of version scripts is relative to directives within the scripts themselves, and it is theoretically possible to \u201csplice\u201d version files in between others, allowing migration sequences from different branches to be merged, albeit carefully by hand. Database Migration \u2691 When changes need to be made in the schema, instead of modifying it directly and then recreate the database from scratch, we can leverage that actions to alembic. alembic revision --autogenerate -m \"{{ commit_comment }}\" That command will write a migration script to make the changes. To perform the migration use: alembic upgrade head To check the status, execute: alembic current To load the migrations from the alembic library inside a python program, the best way to do it is through alembic.command instead of alembic.config.main because it will redirect all logging output to a file from alembic.config import Config import alembic.command config = Config ( 'alembic.ini' ) config . attributes [ 'configure_logger' ] = False alembic . command . upgrade ( config , 'head' ) File: env.py if config . attributes . get ( 'configure_logger' , True ): fileConfig ( config . config_file_name ) Seed database with data \u2691 Note This is an alembic script from datetime import date from sqlalchemy.sql import table , column from sqlalchemy import String , Integer , Date from alembic import op # Create an ad-hoc table to use for the insert statement. accounts_table = table ( 'account' , column ( 'id' , Integer ), column ( 'name' , String ), column ( 'create_date' , Date ) ) op . bulk_insert ( accounts_table , [ { 'id' : 1 , 'name' : 'John Smith' , 'create_date' : date ( 2010 , 10 , 5 )}, { 'id' : 2 , 'name' : 'Ed Williams' , 'create_date' : date ( 2007 , 5 , 27 )}, { 'id' : 3 , 'name' : 'Wendy Jones' , 'create_date' : date ( 2008 , 8 , 15 )}, ] ) Database downgrade or rollback \u2691 If you want to correct a migration first check the history to see where do you want to go (it accepts --verbose for more information): alembic history Then you can specify the id of the revision you want to downgrade to. To specify the last one, use -1 . alembic downgrade -1 After the downgrade is performed, if you want to remove the last revision, you'd need to manually remove the revision python file. References \u2691 Git Docs Articles \u2691 Migrate SQLAlchemy databases with Alembic","title":"Alembic"},{"location":"coding/python/alembic/#database-migration-in-sqlalchemy","text":"A database migration usually changes the schema of a database, such as adding a column or a constraint, adding a table or updating a table. It's often performed using raw SQL wrapped in a transaction so that it can be rolled back if something went wrong during the migration. To migrate a SQLAlchemy database, an Alembic migration script is created for the intended migration, perform the migration, update the model definition and then start using the database under the migrated schema.","title":"Database Migration in SQLAlchemy"},{"location":"coding/python/alembic/#alembic-repository-initialization","text":"It's important that the migration scripts are saved with the rest of the source code. Following Miguel Gringberg suggestion , we'll store them in the {{ program_name }}/migrations directory. Execute the following command to initialize the alembic repository. alembic init {{ program_name }} /migrations It will create several files and directories under the selected path, the most important are: alembic.ini : It's the file the alembic script will look for when invoked. Usually it's located at the root of the program. Although there are several options to configure here, we'll use the env.py file to define how to access the database. env.py : It is a Python script that is run whenever the alembic migration tool is invoked. At the very least, it contains instructions to configure and generate a SQLAlchemy engine, procure a connection from that engine along with a transaction, and then invoke the migration engine, using the connection as a source of database connectivity. The env.py script is part of the generated environment so that the way migrations run is entirely customizable. The exact specifics of how to connect are here, as well as the specifics of how the migration environment are invoked. The script can be modified so that multiple engines can be operated upon, custom arguments can be passed into the migration environment, application-specific libraries and models can be loaded in and made available. By default alembic takes the database url by the sqlalchemy.url key in the alembic.ini file. But it's advisable to be able to define the url from environmental variables. Therefore we need to modify this file with the following changes: # from sqlalchemy import engine_from_config # from sqlalchemy import pool from sqlalchemy import create_engine import os if config . attributes . get ( \"configure_logger\" , True ): fileConfig ( config . config_file_name ) def get_url (): basedir = '~/.local/share/{{ program_name }}' return os . environ . get ( '{{ program_name }}_DATABASE_URL' ) or \\ 'sqlite:///' + os . path . join ( os . path . expanduser ( basedir ), 'main.db' ) def run_migrations_offline (): \"\"\"Run migrations in 'offline' mode. This configures the context with just a URL and not an Engine, though an Engine is acceptable here as well. By skipping the Engine creation we don't even need a DBAPI to be available. Calls to context.execute() here emit the given string to the script output. \"\"\" # url = config.get_main_option(\"sqlalchemy.url\") url = get_url () context . configure ( url = url , target_metadata = target_metadata , literal_binds = True , dialect_opts = { \"paramstyle\" : \"named\" }, ) with context . begin_transaction (): context . run_migrations () def run_migrations_online (): \"\"\"Run migrations in 'online' mode. In this scenario we need to create an Engine and associate a connection with the context. \"\"\" # connectable = engine_from_config( # config.get_section(config.config_ini_section), # prefix=\"sqlalchemy.\", # poolclass=pool.NullPool, # ) connectable = create_engine ( get_url ()) # Leave the rest of the file as it is It is also necessary to import your models metadata, to do so, modify: # add your model's MetaData object here # for 'autogenerate' support # from myapp import mymodel # target_metadata = mymodel.Base.metadata # target_metadata = None import sys sys . path = [ '' , '..' ] + sys . path [ 1 :] from {{ program_name }} import models target_metadata = models . Base . metadata We had to add the parent directory to the sys.path because when env.py is executed, models is not in your PYTHONPATH , resulting in an import error . versions/ : Directory that holds the individual version scripts. The files it contains don\u2019t use ascending integers, and instead use a partial GUID approach. In Alembic, the ordering of version scripts is relative to directives within the scripts themselves, and it is theoretically possible to \u201csplice\u201d version files in between others, allowing migration sequences from different branches to be merged, albeit carefully by hand.","title":"Alembic repository initialization"},{"location":"coding/python/alembic/#database-migration","text":"When changes need to be made in the schema, instead of modifying it directly and then recreate the database from scratch, we can leverage that actions to alembic. alembic revision --autogenerate -m \"{{ commit_comment }}\" That command will write a migration script to make the changes. To perform the migration use: alembic upgrade head To check the status, execute: alembic current To load the migrations from the alembic library inside a python program, the best way to do it is through alembic.command instead of alembic.config.main because it will redirect all logging output to a file from alembic.config import Config import alembic.command config = Config ( 'alembic.ini' ) config . attributes [ 'configure_logger' ] = False alembic . command . upgrade ( config , 'head' ) File: env.py if config . attributes . get ( 'configure_logger' , True ): fileConfig ( config . config_file_name )","title":"Database Migration"},{"location":"coding/python/alembic/#seed-database-with-data","text":"Note This is an alembic script from datetime import date from sqlalchemy.sql import table , column from sqlalchemy import String , Integer , Date from alembic import op # Create an ad-hoc table to use for the insert statement. accounts_table = table ( 'account' , column ( 'id' , Integer ), column ( 'name' , String ), column ( 'create_date' , Date ) ) op . bulk_insert ( accounts_table , [ { 'id' : 1 , 'name' : 'John Smith' , 'create_date' : date ( 2010 , 10 , 5 )}, { 'id' : 2 , 'name' : 'Ed Williams' , 'create_date' : date ( 2007 , 5 , 27 )}, { 'id' : 3 , 'name' : 'Wendy Jones' , 'create_date' : date ( 2008 , 8 , 15 )}, ] )","title":"Seed database with data"},{"location":"coding/python/alembic/#database-downgrade-or-rollback","text":"If you want to correct a migration first check the history to see where do you want to go (it accepts --verbose for more information): alembic history Then you can specify the id of the revision you want to downgrade to. To specify the last one, use -1 . alembic downgrade -1 After the downgrade is performed, if you want to remove the last revision, you'd need to manually remove the revision python file.","title":"Database downgrade or rollback"},{"location":"coding/python/alembic/#references","text":"Git Docs","title":"References"},{"location":"coding/python/alembic/#articles","text":"Migrate SQLAlchemy databases with Alembic","title":"Articles"},{"location":"coding/python/click/","text":"Click is a Python package for creating beautiful command line interfaces in a composable way with as little code as necessary. It\u2019s the \u201cCommand Line Interface Creation Kit\u201d. It\u2019s highly configurable but comes with sensible defaults out of the box. Click has the following features: Arbitrary nesting of commands. Automatic help page generation. Supports lazy loading of subcommands at runtime. Supports implementation of Unix/POSIX command line conventions. Supports loading values from environment variables out of the box. Support for prompting of custom values. Supports file handling out of the box. Comes with useful common helpers (getting terminal dimensions, ANSI colors, fetching direct keyboard input, screen clearing, finding config paths, launching apps and editors). Setuptools Integration \u2691 To bundle your script with setuptools, all you need is the script in a Python package and a setup.py file. Let\u2019s assume your directory structure changed to this: project/ yourpackage/ __init__.py main.py utils.py scripts/ __init__.py yourscript.py setup.py In this case instead of using py_modules in your setup.py file you can use packages and the automatic package finding support of setuptools. In addition to that it\u2019s also recommended to include other package data. These would be the modified contents of setup.py: from setuptools import setup , find_packages setup ( name = 'yourpackage' , version = '0.1.0' , packages = find_packages (), include_package_data = True , install_requires = [ 'Click' , ], entry_points = { 'console_scripts' : [ 'yourscript = yourpackage.scripts.yourscript:cli' , ], }, ) Testing Click applications \u2691 For basic testing, Click provides the click.testing module which provides test functionality that helps you invoke command line applications and check their behavior. The basic functionality for testing Click applications is the CliRunner which can invoke commands as command line scripts. The CliRunner.invoke() method runs the command line script in isolation and captures the output as both bytes and binary data. The return value is a Result object, which has the captured output data, exit code, and optional exception attached: File: hello.py import click @click . command () @click . argument ( 'name' ) def hello ( name ): click . echo ( 'Hello %s !' % name ) File: test_hello.py from click.testing import CliRunner from hello import hello def test_hello_world (): runner = CliRunner () result = runner . invoke ( hello , [ 'Peter' ]) assert result . exit_code == 0 assert result . output == 'Hello Peter! \\n ' For subcommand testing, a subcommand name must be specified in the args parameter of CliRunner.invoke() method: File: sync.py import click @click . group () @click . option ( '--debug/--no-debug' , default = False ) def cli ( debug ): click . echo ( 'Debug mode is %s ' % ( 'on' if debug else 'off' )) @cli . command () def sync (): click . echo ( 'Syncing' ) File: test_sync.py from click.testing import CliRunner from sync import cli def test_sync (): runner = CliRunner () result = runner . invoke ( cli , [ '--debug' , 'sync' ]) assert result . exit_code == 0 assert 'Debug mode is on' in result . output assert 'Syncing' in result . output If you want to test user stdin interaction check the prompt_toolkit and pexpect articles. File system isolation \u2691 For basic command line tools with file system operations, the CliRunner.isolated_filesystem() method is useful for setting the current working directory to a new, empty folder. File: cat.py import click @click . command () @click . argument ( 'f' , type = click . File ()) def cat ( f ): click . echo ( f . read ()) File: test_cat.py from click.testing import CliRunner from cat import cat def test_cat (): runner = CliRunner () with runner . isolated_filesystem (): with open ( 'hello.txt' , 'w' ) as f : f . write ( 'Hello World!' ) result = runner . invoke ( cat , [ 'hello.txt' ]) assert result . exit_code == 0 assert result . output == 'Hello World! \\n ' Testing the value of stdout and stderr \u2691 The runner has the stdout and stderr attributes to test if something was written on those buffers. Injecting fake dependencies \u2691 If you're following the domain driven design architecture pattern, you'll probably need to inject some fake objects instead of using the original objects. The challenge is to do it without modifying your real code too much for the sake of testing. Harry J.W. Percival and Bob Gregory have an interesting proposal in their Dependency Injection (and Bootstrapping) chapter, although I found it a little bit complex. Imagine that we've got an adapter to interact with the Gitea web application called Gitea . File: adapters/gitea.py class Gitea (): fake : bool = False The Click cli definition would be: File: entrypoints/cli.py import logging from adapters.gitea import Gitea log = logging . getLogger ( __name__ ) @click . group () @click . pass_context def cli ( ctx : click . core . Context ) -> None : \"\"\"Command line interface main click entrypoint.\"\"\" ctx . ensure_object ( dict ) try : ctx . obj [ \"gitea\" ] except KeyError : ctx . obj [ \"gitea\" ] = load_gitea () @cli . command () @click . pass_context def is_fake ( ctx : Context ) -> None : if ctx . obj [ \"gitea\" ] . fake : log . info ( \"It's fake!\" ) def load_gitea () -> Gitea : \"\"\"Configure the Gitea object.\"\"\" return Gitea () Where: load_gitea : is a simplified version of the loading of an adapter, in a real example, you'll probably will need to catch some exceptions when loading the object. is_fake : Is the subcommand we're going to use to test if the adapter has been replaced by the fake object. The fake implementation of the adapter is called FakeGitea . File: tests/fake_adapters.py class FakeGitea (): fake : bool = True To inject FakeGitea in the tests we need to load it in the 'gitea' key of the obj attribute of the click ctx Context object. To do it create the fake_dependencies dictionary with the required fakes and pass it to the invoke call. File: tests/e2e/test_cli.py from tests.fake_adapters import FakeGitea from _pytest.logging import LogCaptureFixture fake_dependencies = { \"gitea\" : FakeGitea ()} @pytest . fixture ( name = \"runner\" ) def fixture_runner () -> CliRunner : \"\"\"Configure the Click cli test runner.\"\"\" return CliRunner () def test_fake_injection ( runner : CliRunner , caplog : LogCaptureFixture ) -> None : result = runner . invoke ( cli , [ \"is_fake\" ], obj = fake_dependencies ) assert result . exit_code == 0 assert ( \"entrypoints.cli\" , logging . INFO , \"It's fake!\" , ) in caplog . record_tuples In this way we don't need to ship the fake objects with the code, and the modifications are minimal. Only the try/except KeyError snippet in the cli definition. Options \u2691 Boolean Flags \u2691 Boolean flags are options that can be enabled or disabled. This can be accomplished by defining two flags in one go separated by a slash (/) for enabling or disabling the option. Click always wants you to provide an enable and disable flag so that you can change the default later. import sys @click . command () @click . option ( '--shout/--no-shout' , default = False ) def info ( shout ): rv = sys . platform if shout : rv = rv . upper () + '!!!!111' click . echo ( rv ) If you really don\u2019t want an off-switch, you can just define one and manually inform Click that something is a flag: import sys @click . command () @click . option ( '--shout' , is_flag = True ) def info ( shout ): rv = sys . platform if shout : rv = rv . upper () + '!!!!111' click . echo ( rv ) Accepting values from environmental variables \u2691 Click is the able to accept parameters from environment variables. There are two ways to define them. Passing the auto_envvar_prefix to the script that is invoked so each command and parameter is then added as an uppercase underscore-separated variable. Manually pull values in from specific environment variables by defining the name of the environment variable on the option. @click . command () @click . option ( '--username' , envvar = 'USERNAME' ) def greet ( username ): click . echo ( f 'Hello { username } !' ) if __name__ == '__main__' : greet () Arguments \u2691 Arguments work similarly to options but are positional. They also only support a subset of the features of options due to their syntactical nature. Click will also not attempt to document arguments for you and wants you to document them manually in order to avoid ugly help pages. Basic Arguments \u2691 The most basic option is a simple string argument of one value. If no type is provided, the type of the default value is used, and if no default value is provided, the type is assumed to be STRING . @click . command () @click . argument ( 'filename' ) def touch ( filename ): \"\"\"Print FILENAME.\"\"\" click . echo ( filename ) And what it looks like: $ touch foo.txt foo.txt Variadic arguments \u2691 The second most common version is variadic arguments where a specific (or unlimited) number of arguments is accepted. This can be controlled with the nargs parameter. If it is set to -1 , then an unlimited number of arguments is accepted. The value is then passed as a tuple. Note that only one argument can be set to nargs=-1 , as it will eat up all arguments. @click . command () @click . argument ( 'src' , nargs =- 1 ) @click . argument ( 'dst' , nargs = 1 ) def copy ( src , dst ): \"\"\"Move file SRC to DST.\"\"\" for fn in src : click . echo ( 'move %s to folder %s ' % ( fn , dst )) You can't use variadic arguments and then specify a command . File Arguments \u2691 Command line tools are more fun if they work with files the Unix way, which is to accept - as a special file that refers to stdin/stdout. Click supports this through the click.File type which intelligently handles files for you. It also deals with Unicode and bytes correctly for all versions of Python so your script stays very portable. @click . command () @click . argument ( 'input' , type = click . File ( 'rb' )) @click . argument ( 'output' , type = click . File ( 'wb' )) def inout ( input , output ): \"\"\"Copy contents of INPUT to OUTPUT.\"\"\" while True : chunk = input . read ( 1024 ) if not chunk : break output . write ( chunk ) And what it does: $ inout - hello.txt hello ^D $ inout hello.txt - hello File path arguments \u2691 In the previous example, the files were opened immediately. If we just want the filename, you should be using the Path type. Not only will it return either bytes or Unicode depending on what makes more sense, but it will also be able to do some basic checks for you such as existence checks. @click . command () @click . argument ( 'filename' , type = click . Path ( exists = True )) def touch ( filename ): \"\"\"Print FILENAME if the file exists.\"\"\" click . echo ( click . format_filename ( filename )) And what it does: $ touch hello.txt hello.txt $ touch missing.txt Usage: touch [ OPTIONS ] FILENAME Try 'touch --help' for help. Error: Invalid value for 'FILENAME' : Path 'missing.txt' does not exist. Set allowable values for an argument \u2691 @cli . command () @click . argument ( 'source' ) @click . argument ( 'destination' ) @click . option ( '--mode' , type = click . Choice ([ 'local' , 'ftp' ]), required = True ) def copy ( source , destination , mode ): print ( \"copying files from \" + source + \" to \" + destination + \"using \" + mode + \" mode\" ) Commands and groups \u2691 Nested handling and contexts \u2691 Each time a command is invoked, a new context is created and linked with the parent context. Contexts are passed to parameter callbacks together with the value automatically. Commands can also ask for the context to be passed by marking themselves with the pass_context() decorator. In that case, the context is passed as first argument. The context can also carry a program specified object that can be used for the program\u2019s purposes. @click . group ( help = \"Description of the command line.\" ) @click . option ( '--debug/--no-debug' , default = False ) @click . pass_context def cli ( ctx , debug ): # ensure that ctx.obj exists and is a dict (in case `cli()` is called # by means other than the `if` block below) ctx . ensure_object ( dict ) ctx . obj [ 'DEBUG' ] = debug @cli . command () @click . pass_context def sync ( ctx ): click . echo ( f 'Debug is { ctx . obj [ 'DEBUG' ] and 'on' or 'off' } ' )) if __name__ == '__main__' : cli ( obj = {}) If the object is provided, each context will pass the object onwards to its children, but at any level a context\u2019s object can be overridden. To reach to a parent, context.parent can be used. In addition to that, instead of passing an object down, nothing stops the application from modifying global state. For instance, you could just flip a global DEBUG variable and be done with it. Add default command to group \u2691 You need to use DefaultGroup , which is a sub class of click.Group . But it invokes a default subcommand instead of showing a help message when a subcommand is not passed. pip install click-default-group import click from click_default_group import DefaultGroup @click . group ( cls = DefaultGroup , default = 'foo' , default_if_no_args = True ) def cli (): pass @cli . command () def foo (): click . echo ( 'foo' ) @cli . command () def bar (): click . echo ( 'bar' ) Then you can invoke that without explicit subcommand name: $ cli.py --help Usage: cli.py [ OPTIONS ] COMMAND [ ARGS ] ... Options: --help Show this message and exit. Command: foo* bar $ cli.py foo $ cli.py foo foo $ cli.py bar bar Hide a command from the help \u2691 @click . command ( ... , hidden = True ) Invoke other commands from a command \u2691 This is a pattern that is generally discouraged with Click, but possible nonetheless. For this, you can use the Context.invoke() or Context.forward() methods. They work similarly, but the difference is that Context.invoke() merely invokes another command with the arguments you provide as a caller, whereas Context.forward() fills in the arguments from the current command. Both accept the command as the first argument and everything else is passed onwards as you would expect. cli = click . Group () @cli . command () @click . option ( '--count' , default = 1 ) def test ( count ): click . echo ( 'Count: %d ' % count ) @cli . command () @click . option ( '--count' , default = 1 ) @click . pass_context def dist ( ctx , count ): ctx . forward ( test ) ctx . invoke ( test , count = 42 ) And what it looks like: $ cli dist Count: 1 Count: 42 References \u2691 Homepage Click vs other argument parsers","title":"Click"},{"location":"coding/python/click/#setuptools-integration","text":"To bundle your script with setuptools, all you need is the script in a Python package and a setup.py file. Let\u2019s assume your directory structure changed to this: project/ yourpackage/ __init__.py main.py utils.py scripts/ __init__.py yourscript.py setup.py In this case instead of using py_modules in your setup.py file you can use packages and the automatic package finding support of setuptools. In addition to that it\u2019s also recommended to include other package data. These would be the modified contents of setup.py: from setuptools import setup , find_packages setup ( name = 'yourpackage' , version = '0.1.0' , packages = find_packages (), include_package_data = True , install_requires = [ 'Click' , ], entry_points = { 'console_scripts' : [ 'yourscript = yourpackage.scripts.yourscript:cli' , ], }, )","title":"Setuptools Integration"},{"location":"coding/python/click/#testing-click-applications","text":"For basic testing, Click provides the click.testing module which provides test functionality that helps you invoke command line applications and check their behavior. The basic functionality for testing Click applications is the CliRunner which can invoke commands as command line scripts. The CliRunner.invoke() method runs the command line script in isolation and captures the output as both bytes and binary data. The return value is a Result object, which has the captured output data, exit code, and optional exception attached: File: hello.py import click @click . command () @click . argument ( 'name' ) def hello ( name ): click . echo ( 'Hello %s !' % name ) File: test_hello.py from click.testing import CliRunner from hello import hello def test_hello_world (): runner = CliRunner () result = runner . invoke ( hello , [ 'Peter' ]) assert result . exit_code == 0 assert result . output == 'Hello Peter! \\n ' For subcommand testing, a subcommand name must be specified in the args parameter of CliRunner.invoke() method: File: sync.py import click @click . group () @click . option ( '--debug/--no-debug' , default = False ) def cli ( debug ): click . echo ( 'Debug mode is %s ' % ( 'on' if debug else 'off' )) @cli . command () def sync (): click . echo ( 'Syncing' ) File: test_sync.py from click.testing import CliRunner from sync import cli def test_sync (): runner = CliRunner () result = runner . invoke ( cli , [ '--debug' , 'sync' ]) assert result . exit_code == 0 assert 'Debug mode is on' in result . output assert 'Syncing' in result . output If you want to test user stdin interaction check the prompt_toolkit and pexpect articles.","title":"Testing Click applications"},{"location":"coding/python/click/#file-system-isolation","text":"For basic command line tools with file system operations, the CliRunner.isolated_filesystem() method is useful for setting the current working directory to a new, empty folder. File: cat.py import click @click . command () @click . argument ( 'f' , type = click . File ()) def cat ( f ): click . echo ( f . read ()) File: test_cat.py from click.testing import CliRunner from cat import cat def test_cat (): runner = CliRunner () with runner . isolated_filesystem (): with open ( 'hello.txt' , 'w' ) as f : f . write ( 'Hello World!' ) result = runner . invoke ( cat , [ 'hello.txt' ]) assert result . exit_code == 0 assert result . output == 'Hello World! \\n '","title":"File system isolation"},{"location":"coding/python/click/#testing-the-value-of-stdout-and-stderr","text":"The runner has the stdout and stderr attributes to test if something was written on those buffers.","title":"Testing the value of stdout and stderr"},{"location":"coding/python/click/#injecting-fake-dependencies","text":"If you're following the domain driven design architecture pattern, you'll probably need to inject some fake objects instead of using the original objects. The challenge is to do it without modifying your real code too much for the sake of testing. Harry J.W. Percival and Bob Gregory have an interesting proposal in their Dependency Injection (and Bootstrapping) chapter, although I found it a little bit complex. Imagine that we've got an adapter to interact with the Gitea web application called Gitea . File: adapters/gitea.py class Gitea (): fake : bool = False The Click cli definition would be: File: entrypoints/cli.py import logging from adapters.gitea import Gitea log = logging . getLogger ( __name__ ) @click . group () @click . pass_context def cli ( ctx : click . core . Context ) -> None : \"\"\"Command line interface main click entrypoint.\"\"\" ctx . ensure_object ( dict ) try : ctx . obj [ \"gitea\" ] except KeyError : ctx . obj [ \"gitea\" ] = load_gitea () @cli . command () @click . pass_context def is_fake ( ctx : Context ) -> None : if ctx . obj [ \"gitea\" ] . fake : log . info ( \"It's fake!\" ) def load_gitea () -> Gitea : \"\"\"Configure the Gitea object.\"\"\" return Gitea () Where: load_gitea : is a simplified version of the loading of an adapter, in a real example, you'll probably will need to catch some exceptions when loading the object. is_fake : Is the subcommand we're going to use to test if the adapter has been replaced by the fake object. The fake implementation of the adapter is called FakeGitea . File: tests/fake_adapters.py class FakeGitea (): fake : bool = True To inject FakeGitea in the tests we need to load it in the 'gitea' key of the obj attribute of the click ctx Context object. To do it create the fake_dependencies dictionary with the required fakes and pass it to the invoke call. File: tests/e2e/test_cli.py from tests.fake_adapters import FakeGitea from _pytest.logging import LogCaptureFixture fake_dependencies = { \"gitea\" : FakeGitea ()} @pytest . fixture ( name = \"runner\" ) def fixture_runner () -> CliRunner : \"\"\"Configure the Click cli test runner.\"\"\" return CliRunner () def test_fake_injection ( runner : CliRunner , caplog : LogCaptureFixture ) -> None : result = runner . invoke ( cli , [ \"is_fake\" ], obj = fake_dependencies ) assert result . exit_code == 0 assert ( \"entrypoints.cli\" , logging . INFO , \"It's fake!\" , ) in caplog . record_tuples In this way we don't need to ship the fake objects with the code, and the modifications are minimal. Only the try/except KeyError snippet in the cli definition.","title":"Injecting fake dependencies"},{"location":"coding/python/click/#options","text":"","title":"Options"},{"location":"coding/python/click/#boolean-flags","text":"Boolean flags are options that can be enabled or disabled. This can be accomplished by defining two flags in one go separated by a slash (/) for enabling or disabling the option. Click always wants you to provide an enable and disable flag so that you can change the default later. import sys @click . command () @click . option ( '--shout/--no-shout' , default = False ) def info ( shout ): rv = sys . platform if shout : rv = rv . upper () + '!!!!111' click . echo ( rv ) If you really don\u2019t want an off-switch, you can just define one and manually inform Click that something is a flag: import sys @click . command () @click . option ( '--shout' , is_flag = True ) def info ( shout ): rv = sys . platform if shout : rv = rv . upper () + '!!!!111' click . echo ( rv )","title":"Boolean Flags"},{"location":"coding/python/click/#accepting-values-from-environmental-variables","text":"Click is the able to accept parameters from environment variables. There are two ways to define them. Passing the auto_envvar_prefix to the script that is invoked so each command and parameter is then added as an uppercase underscore-separated variable. Manually pull values in from specific environment variables by defining the name of the environment variable on the option. @click . command () @click . option ( '--username' , envvar = 'USERNAME' ) def greet ( username ): click . echo ( f 'Hello { username } !' ) if __name__ == '__main__' : greet ()","title":"Accepting values from environmental variables"},{"location":"coding/python/click/#arguments","text":"Arguments work similarly to options but are positional. They also only support a subset of the features of options due to their syntactical nature. Click will also not attempt to document arguments for you and wants you to document them manually in order to avoid ugly help pages.","title":"Arguments"},{"location":"coding/python/click/#basic-arguments","text":"The most basic option is a simple string argument of one value. If no type is provided, the type of the default value is used, and if no default value is provided, the type is assumed to be STRING . @click . command () @click . argument ( 'filename' ) def touch ( filename ): \"\"\"Print FILENAME.\"\"\" click . echo ( filename ) And what it looks like: $ touch foo.txt foo.txt","title":"Basic Arguments"},{"location":"coding/python/click/#variadic-arguments","text":"The second most common version is variadic arguments where a specific (or unlimited) number of arguments is accepted. This can be controlled with the nargs parameter. If it is set to -1 , then an unlimited number of arguments is accepted. The value is then passed as a tuple. Note that only one argument can be set to nargs=-1 , as it will eat up all arguments. @click . command () @click . argument ( 'src' , nargs =- 1 ) @click . argument ( 'dst' , nargs = 1 ) def copy ( src , dst ): \"\"\"Move file SRC to DST.\"\"\" for fn in src : click . echo ( 'move %s to folder %s ' % ( fn , dst )) You can't use variadic arguments and then specify a command .","title":"Variadic arguments"},{"location":"coding/python/click/#file-arguments","text":"Command line tools are more fun if they work with files the Unix way, which is to accept - as a special file that refers to stdin/stdout. Click supports this through the click.File type which intelligently handles files for you. It also deals with Unicode and bytes correctly for all versions of Python so your script stays very portable. @click . command () @click . argument ( 'input' , type = click . File ( 'rb' )) @click . argument ( 'output' , type = click . File ( 'wb' )) def inout ( input , output ): \"\"\"Copy contents of INPUT to OUTPUT.\"\"\" while True : chunk = input . read ( 1024 ) if not chunk : break output . write ( chunk ) And what it does: $ inout - hello.txt hello ^D $ inout hello.txt - hello","title":"File Arguments"},{"location":"coding/python/click/#file-path-arguments","text":"In the previous example, the files were opened immediately. If we just want the filename, you should be using the Path type. Not only will it return either bytes or Unicode depending on what makes more sense, but it will also be able to do some basic checks for you such as existence checks. @click . command () @click . argument ( 'filename' , type = click . Path ( exists = True )) def touch ( filename ): \"\"\"Print FILENAME if the file exists.\"\"\" click . echo ( click . format_filename ( filename )) And what it does: $ touch hello.txt hello.txt $ touch missing.txt Usage: touch [ OPTIONS ] FILENAME Try 'touch --help' for help. Error: Invalid value for 'FILENAME' : Path 'missing.txt' does not exist.","title":"File path arguments"},{"location":"coding/python/click/#set-allowable-values-for-an-argument","text":"@cli . command () @click . argument ( 'source' ) @click . argument ( 'destination' ) @click . option ( '--mode' , type = click . Choice ([ 'local' , 'ftp' ]), required = True ) def copy ( source , destination , mode ): print ( \"copying files from \" + source + \" to \" + destination + \"using \" + mode + \" mode\" )","title":"Set allowable values for an argument"},{"location":"coding/python/click/#commands-and-groups","text":"","title":"Commands and groups"},{"location":"coding/python/click/#nested-handling-and-contexts","text":"Each time a command is invoked, a new context is created and linked with the parent context. Contexts are passed to parameter callbacks together with the value automatically. Commands can also ask for the context to be passed by marking themselves with the pass_context() decorator. In that case, the context is passed as first argument. The context can also carry a program specified object that can be used for the program\u2019s purposes. @click . group ( help = \"Description of the command line.\" ) @click . option ( '--debug/--no-debug' , default = False ) @click . pass_context def cli ( ctx , debug ): # ensure that ctx.obj exists and is a dict (in case `cli()` is called # by means other than the `if` block below) ctx . ensure_object ( dict ) ctx . obj [ 'DEBUG' ] = debug @cli . command () @click . pass_context def sync ( ctx ): click . echo ( f 'Debug is { ctx . obj [ 'DEBUG' ] and 'on' or 'off' } ' )) if __name__ == '__main__' : cli ( obj = {}) If the object is provided, each context will pass the object onwards to its children, but at any level a context\u2019s object can be overridden. To reach to a parent, context.parent can be used. In addition to that, instead of passing an object down, nothing stops the application from modifying global state. For instance, you could just flip a global DEBUG variable and be done with it.","title":"Nested handling and contexts"},{"location":"coding/python/click/#add-default-command-to-group","text":"You need to use DefaultGroup , which is a sub class of click.Group . But it invokes a default subcommand instead of showing a help message when a subcommand is not passed. pip install click-default-group import click from click_default_group import DefaultGroup @click . group ( cls = DefaultGroup , default = 'foo' , default_if_no_args = True ) def cli (): pass @cli . command () def foo (): click . echo ( 'foo' ) @cli . command () def bar (): click . echo ( 'bar' ) Then you can invoke that without explicit subcommand name: $ cli.py --help Usage: cli.py [ OPTIONS ] COMMAND [ ARGS ] ... Options: --help Show this message and exit. Command: foo* bar $ cli.py foo $ cli.py foo foo $ cli.py bar bar","title":"Add default command to group"},{"location":"coding/python/click/#hide-a-command-from-the-help","text":"@click . command ( ... , hidden = True )","title":"Hide a command from the help"},{"location":"coding/python/click/#invoke-other-commands-from-a-command","text":"This is a pattern that is generally discouraged with Click, but possible nonetheless. For this, you can use the Context.invoke() or Context.forward() methods. They work similarly, but the difference is that Context.invoke() merely invokes another command with the arguments you provide as a caller, whereas Context.forward() fills in the arguments from the current command. Both accept the command as the first argument and everything else is passed onwards as you would expect. cli = click . Group () @cli . command () @click . option ( '--count' , default = 1 ) def test ( count ): click . echo ( 'Count: %d ' % count ) @cli . command () @click . option ( '--count' , default = 1 ) @click . pass_context def dist ( ctx , count ): ctx . forward ( test ) ctx . invoke ( test , count = 42 ) And what it looks like: $ cli dist Count: 1 Count: 42","title":"Invoke other commands from a command"},{"location":"coding/python/click/#references","text":"Homepage Click vs other argument parsers","title":"References"},{"location":"coding/python/dash/","text":"Dash is a productive Python framework for building web analytic applications. Written on top of Flask, Plotly.js, and React.js, Dash is ideal for building data visualization apps with highly custom user interfaces in pure Python. It's particularly suited for anyone who works with data in Python. Install \u2691 pip install dash Layout \u2691 Dash apps are composed of two parts. The first part is the \"layout\" of the app and it describes what the application looks like. The second part describes the interactivity of the application. Dash provides Python classes for all of the visual components of the application. They maintain a set of components in the dash_core_components and the dash_html_components library but you can also build your own with JavaScript and React.js. The scripts are meant to be run with python app.py File: app.py import dash import dash_core_components as dcc import dash_html_components as html import plotly.express as px import pandas as pd external_stylesheets = [ 'https://codepen.io/chriddyp/pen/bWLwgP.css' ] app = dash . Dash ( __name__ , external_stylesheets = external_stylesheets ) # assume you have a \"long-form\" data frame # see https://plotly.com/python/px-arguments/ for more options df = pd . DataFrame ({ \"Fruit\" : [ \"Apples\" , \"Oranges\" , \"Bananas\" , \"Apples\" , \"Oranges\" , \"Bananas\" ], \"Amount\" : [ 4 , 1 , 2 , 2 , 4 , 5 ], \"City\" : [ \"SF\" , \"SF\" , \"SF\" , \"Montreal\" , \"Montreal\" , \"Montreal\" ] }) fig = px . bar ( df , x = \"Fruit\" , y = \"Amount\" , color = \"City\" , barmode = \"group\" ) app . layout = html . Div ( children = [ html . H1 ( children = 'Hello Dash' ), html . Div ( children = ''' Dash: A web application framework for Python. ''' ), dcc . Graph ( id = 'example-graph' , figure = fig ) ]) if __name__ == '__main__' : app . run_server ( debug = True ) References \u2691 Docs Gallery Introduction video","title":"Dash"},{"location":"coding/python/dash/#install","text":"pip install dash","title":"Install"},{"location":"coding/python/dash/#layout","text":"Dash apps are composed of two parts. The first part is the \"layout\" of the app and it describes what the application looks like. The second part describes the interactivity of the application. Dash provides Python classes for all of the visual components of the application. They maintain a set of components in the dash_core_components and the dash_html_components library but you can also build your own with JavaScript and React.js. The scripts are meant to be run with python app.py File: app.py import dash import dash_core_components as dcc import dash_html_components as html import plotly.express as px import pandas as pd external_stylesheets = [ 'https://codepen.io/chriddyp/pen/bWLwgP.css' ] app = dash . Dash ( __name__ , external_stylesheets = external_stylesheets ) # assume you have a \"long-form\" data frame # see https://plotly.com/python/px-arguments/ for more options df = pd . DataFrame ({ \"Fruit\" : [ \"Apples\" , \"Oranges\" , \"Bananas\" , \"Apples\" , \"Oranges\" , \"Bananas\" ], \"Amount\" : [ 4 , 1 , 2 , 2 , 4 , 5 ], \"City\" : [ \"SF\" , \"SF\" , \"SF\" , \"Montreal\" , \"Montreal\" , \"Montreal\" ] }) fig = px . bar ( df , x = \"Fruit\" , y = \"Amount\" , color = \"City\" , barmode = \"group\" ) app . layout = html . Div ( children = [ html . H1 ( children = 'Hello Dash' ), html . Div ( children = ''' Dash: A web application framework for Python. ''' ), dcc . Graph ( id = 'example-graph' , figure = fig ) ]) if __name__ == '__main__' : app . run_server ( debug = True )","title":"Layout"},{"location":"coding/python/dash/#references","text":"Docs Gallery Introduction video","title":"References"},{"location":"coding/python/dash_leaflet/","text":"Dash Leaflet is a wrapper of Leaflet , the leading open-source JavaScript library for interactive maps. Install \u2691 pip install dash pip install dash-leaflet Usage \u2691 import dash import dash_leaflet as dl app = dash . Dash ( __name__ ) app . layout = dl . Map ( dl . TileLayer (), style = { 'height' : '100vh' }) if __name__ == '__main__' : app . run_server ( port = 8050 , debug = True ) That's it! You have now created your first interactive map with Dash Leaflet. If you visit http://127.0.0.1:8050/ in your browser. Change tileset \u2691 Leaflet Map object supports different tiles by default. It also supports any WMS or WMTS by passing a Leaflet-style URL to the tiles parameter: http://{s}.yourtiles.com/{z}/{x}/{y}.png . To use the IGN beautiful map as a fallback and the OpenStreetMaps as default use the following snippet: app . layout = html . Div ( dl . Map ( [ dl . LayersControl ( [ dl . BaseLayer ( dl . TileLayer (), name = \"OpenStreetMaps\" , checked = True , ), dl . BaseLayer ( dl . TileLayer ( url = \"https://www.ign.es/wmts/mapa-raster?request=getTile&layer=MTN&TileMatrixSet=GoogleMapsCompatible&TileMatrix= {z} &TileCol= {x} &TileRow= {y} &format=image/jpeg\" , attribution = \"IGN\" , ), name = \"IGN\" , checked = False , ), ], ), get_data (), ], zoom = 7 , center = ( 40.0884 , - 3.68042 ), ), style = { \"height\" : \"100vh\" , }, ) Loading the data \u2691 Using Markers \u2691 As with folium , loading different custom markers within the same geojson object is difficult, therefore we are again forced to use markers with cluster group. Assuming we've got a gpx file called data.gpx , we can use the following snippet to load all markers with a custom icon. import dash_leaflet as dl import gpxpy icon = { \"iconUrl\" : \"https://leafletjs.com/examples/custom-icons/leaf-green.png\" , \"shadowUrl\" : \"https://leafletjs.com/examples/custom-icons/leaf-shadow.png\" , \"iconSize\" : [ 38 , 95 ], # size of the icon \"shadowSize\" : [ 50 , 64 ], # size of the shadow \"iconAnchor\" : [ 22 , 94 , ], # point of the icon which will correspond to marker's location \"shadowAnchor\" : [ 4 , 62 ], # the same for the shadow \"popupAnchor\" : [ - 3 , - 76 , ], # point from which the popup should open relative to the iconAnchor } def get_data (): gpx_file = open ( \"data.gpx\" , \"r\" ) gpx = gpxpy . parse ( gpx_file ) markers = [] for waypoint in gpx . waypoints : markers . append ( dl . Marker ( title = waypoint . name , position = ( waypoint . latitude , waypoint . longitude ), icon = icon , children = [ dl . Tooltip ( waypoint . name ), dl . Popup ( waypoint . name ), ], ) ) cluster = dl . MarkerClusterGroup ( id = \"markers\" , children = markers ) return cluster app = dash . Dash ( __name__ ) app . layout = html . Div ( dl . Map ( [ dl . TileLayer (), get_data (), ], zoom = 7 , center = ( 40.0884 , - 3.68042 ), ), style = { \"height\" : \"100vh\" , }, ) Inside get_data you can add further logic to change the icon based on the data of the gpx. Configurations \u2691 Add custom css or js \u2691 Including custom CSS or JavaScript in your Dash apps is simple. Just create a folder named assets in the root of your app directory and include your CSS and JavaScript files in that folder. Dash will automatically serve all of the files that are included in this folder. Remove the border around the map \u2691 Add a custom css file: File: assets/custom.css body { margin : 0 , } References \u2691 Docs Git","title":"Dash Leaflet"},{"location":"coding/python/dash_leaflet/#install","text":"pip install dash pip install dash-leaflet","title":"Install"},{"location":"coding/python/dash_leaflet/#usage","text":"import dash import dash_leaflet as dl app = dash . Dash ( __name__ ) app . layout = dl . Map ( dl . TileLayer (), style = { 'height' : '100vh' }) if __name__ == '__main__' : app . run_server ( port = 8050 , debug = True ) That's it! You have now created your first interactive map with Dash Leaflet. If you visit http://127.0.0.1:8050/ in your browser.","title":"Usage"},{"location":"coding/python/dash_leaflet/#change-tileset","text":"Leaflet Map object supports different tiles by default. It also supports any WMS or WMTS by passing a Leaflet-style URL to the tiles parameter: http://{s}.yourtiles.com/{z}/{x}/{y}.png . To use the IGN beautiful map as a fallback and the OpenStreetMaps as default use the following snippet: app . layout = html . Div ( dl . Map ( [ dl . LayersControl ( [ dl . BaseLayer ( dl . TileLayer (), name = \"OpenStreetMaps\" , checked = True , ), dl . BaseLayer ( dl . TileLayer ( url = \"https://www.ign.es/wmts/mapa-raster?request=getTile&layer=MTN&TileMatrixSet=GoogleMapsCompatible&TileMatrix= {z} &TileCol= {x} &TileRow= {y} &format=image/jpeg\" , attribution = \"IGN\" , ), name = \"IGN\" , checked = False , ), ], ), get_data (), ], zoom = 7 , center = ( 40.0884 , - 3.68042 ), ), style = { \"height\" : \"100vh\" , }, )","title":"Change tileset"},{"location":"coding/python/dash_leaflet/#loading-the-data","text":"","title":"Loading the data"},{"location":"coding/python/dash_leaflet/#using-markers","text":"As with folium , loading different custom markers within the same geojson object is difficult, therefore we are again forced to use markers with cluster group. Assuming we've got a gpx file called data.gpx , we can use the following snippet to load all markers with a custom icon. import dash_leaflet as dl import gpxpy icon = { \"iconUrl\" : \"https://leafletjs.com/examples/custom-icons/leaf-green.png\" , \"shadowUrl\" : \"https://leafletjs.com/examples/custom-icons/leaf-shadow.png\" , \"iconSize\" : [ 38 , 95 ], # size of the icon \"shadowSize\" : [ 50 , 64 ], # size of the shadow \"iconAnchor\" : [ 22 , 94 , ], # point of the icon which will correspond to marker's location \"shadowAnchor\" : [ 4 , 62 ], # the same for the shadow \"popupAnchor\" : [ - 3 , - 76 , ], # point from which the popup should open relative to the iconAnchor } def get_data (): gpx_file = open ( \"data.gpx\" , \"r\" ) gpx = gpxpy . parse ( gpx_file ) markers = [] for waypoint in gpx . waypoints : markers . append ( dl . Marker ( title = waypoint . name , position = ( waypoint . latitude , waypoint . longitude ), icon = icon , children = [ dl . Tooltip ( waypoint . name ), dl . Popup ( waypoint . name ), ], ) ) cluster = dl . MarkerClusterGroup ( id = \"markers\" , children = markers ) return cluster app = dash . Dash ( __name__ ) app . layout = html . Div ( dl . Map ( [ dl . TileLayer (), get_data (), ], zoom = 7 , center = ( 40.0884 , - 3.68042 ), ), style = { \"height\" : \"100vh\" , }, ) Inside get_data you can add further logic to change the icon based on the data of the gpx.","title":"Using Markers"},{"location":"coding/python/dash_leaflet/#configurations","text":"","title":"Configurations"},{"location":"coding/python/dash_leaflet/#add-custom-css-or-js","text":"Including custom CSS or JavaScript in your Dash apps is simple. Just create a folder named assets in the root of your app directory and include your CSS and JavaScript files in that folder. Dash will automatically serve all of the files that are included in this folder.","title":"Add custom css or js"},{"location":"coding/python/dash_leaflet/#remove-the-border-around-the-map","text":"Add a custom css file: File: assets/custom.css body { margin : 0 , }","title":"Remove the border around the map"},{"location":"coding/python/dash_leaflet/#references","text":"Docs Git","title":"References"},{"location":"coding/python/data_classes/","text":"A data class is a regular Python class that has basic data model methods like __init__() , __repr__() , and __eq__() implemented for you. Introduced in Python 3.7 , they typically containing mainly data, although there aren\u2019t really any restrictions. from dataclasses import dataclass @dataclass class DataClassCard : rank : str suit : str They behave similar to named tuples but come with many more features. At the same time, named tuples have some other features that are not necessarily desirable, such as: By design it's a regular tuple, which can lead to subtle and hard to find bugs. It's hard to add default values to some fields. It's by nature immutable. That being said, if you need your data structure to behave like a tuple, then a named tuple is a great alternative. Advantages over regular classes \u2691 Simplify the class definition @dataclass class DataClassCard : rank : str suit : str # Versus class RegularCard def __init__ ( self , rank , suit ): self . rank = rank self . suit = suit More descriptive object representation through a better default __repr__() method. >>> queen_of_hearts = DataClassCard ( 'Q' , 'Hearts' ) >>> queen_of_hearts DataClassCard ( rank = 'Q' , suit = 'Hearts' ) # Versus >>> queen_of_spades = RegularCard ( 'Q' , 'Spades' ) >>> queen_of_spades < __main__ . RegularCard object at 0x7fb6eee35d30 > * Instance comparison out of the box through a better default __eq__() method. >>> queen_of_hearts == DataClassCard ( 'Q' , 'Hearts' ) True # Versus >>> queen_of_spades == RegularCard ( 'Q' , 'Spades' ) False Usage \u2691 Definition \u2691 from dataclasses import dataclass @dataclass class Position : name : str lon : float lat : float What makes this a data class is the @dataclass decorator. Beneath the class Position: , simply list the fields you want in your data class. The data class decorator support the following parameters : init : Add .__init__() method? (Default is True). repr : Add .__repr__() method? (Default is True). eq : Add .__eq__() method? (Default is True). order : Add ordering methods? (Default is False). unsafe_hash : Force the addition of a .__hash__() method? (Default is False). frozen : If True , assigning to fields raise an exception. (Default is False). Default values \u2691 It's easy to add default values to the fields of your data class: from dataclasses import dataclass @dataclass class Position : name : str lon : float = 0.0 lat : float = 0.0 More complex default values can be defined through the use of functions. For example, the next snippet builds a French deck: from dataclasses import dataclass , field from typing import List RANKS = '2 3 4 5 6 7 8 9 10 J Q K A' . split () SUITS = '\u2663 \u2662 \u2661 \u2660' . split () def make_french_deck (): return [ PlayingCard ( r , s ) for s in SUITS for r in RANKS ] @dataclass class PlayingCard : rank : str suit : str @dataclass class Deck : cards : List [ PlayingCard ] = field ( default_factory = make_french_deck ) Using cards: List[PlayingCard] = make_french_deck() introduces the using mutable default arguments anti-pattern. Instead, data classes use the default_factory to handle mutable default values. To use it, you need to use the field() specifier which is used to customize each field of a data class individually. It supports the following parameters: default : Default value of the field. default_factory : Function that returns the initial value of the field. init : Use field in .__init__() method? (Default is True ). repr : Use field in repr of the object? (Default is True ). For example to hide a parameter from the repr , use lat: float = field(default=0.0, repr=False) . compare : Include the field in comparisons? (Default is True ). hash : Include the field when calculating hash() ? (Default is to use the same as compare ). metadata : A mapping with information about the field. It's not used by the data classes themselves but is available for you to attach information to fields. For example: from dataclasses import dataclass , field @dataclass class Position : name : str lon : float = field ( default = 0.0 , metadata = { 'unit' : 'degrees' }) lat : float = field ( default = 0.0 , metadata = { 'unit' : 'degrees' }) To retrieve the information use the fields() function. >>> from dataclasses import fields >>> fields ( Position ) ( Field ( name = 'name' , type =< class ' str '>,...,metadata= {} ), Field ( name = 'lon' , type =< class ' float '>,...,metadata={' unit ': ' degrees '}), Field ( name = 'lat' , type =< class ' float '>,...,metadata={' unit ': ' degrees '})) >>> lat_unit = fields ( Position )[ 2 ] . metadata [ 'unit' ] >>> lat_unit 'degrees' Type hints \u2691 They support typing out of the box. Without a type hint, the field will not be a part of the data class. While you need to add type hints in some form when using data classes, these types are not enforced at runtime. This is how typing in python usually works: Python is and will always be a dynamically typed language . Adding methods \u2691 Same as with a normal class. Adding complex order comparison logic \u2691 from dataclasses import dataclass @dataclass ( order = True ) class PlayingCard : rank : str suit : str def __str__ ( self ): return f ' { self . suit }{ self . rank } ' After setting order=True in the decorator definition the instances of PlayingCard can be compared. >>> queen_of_hearts = PlayingCard ( 'Q' , '\u2661' ) >>> ace_of_spades = PlayingCard ( 'A' , '\u2660' ) >>> ace_of_spades > queen_of_hearts False Data classes compare objects as if they were tuples of their fields. A Queen is higher than an Ace because Q comes after A in the alphabet. >>> ( 'A' , '\u2660' ) > ( 'Q' , '\u2661' ) False To use more complex comparisons, we need to add the field .sort_index to the class. However, this field should be calculated from the other fields automatically. That's what the special method .__post_init__() is for. It allows for special processing after the regular .__init__() method is called. from dataclasses import dataclass , field RANKS = '2 3 4 5 6 7 8 9 10 J Q K A' . split () SUITS = '\u2663 \u2662 \u2661 \u2660' . split () @dataclass ( order = True ) class PlayingCard : sort_index : int = field ( init = False , repr = False ) rank : str suit : str def __post_init__ ( self ): self . sort_index = ( RANKS . index ( self . rank ) * len ( SUITS ) + SUITS . index ( self . suit )) def __str__ ( self ): return f ' { self . suit }{ self . rank } ' Note that .sort_index is added as the first field of the class. That way, the comparison is first done using .sort_index and only if there are ties are the other fields used. Using field() , you must also specify that .sort_index should not be included as a parameter in the .__init__() method (because it is calculated from the .rank and .suit fields). To avoid confusing the user about this implementation detail, it is probably also a good idea to remove .sort_index from the repr of the class. Immutable data classes \u2691 To make a data class immutable, set frozen=True when you create it. from dataclasses import dataclass @dataclass ( frozen = True ) class Position : name : str lon : float = 0.0 lat : float = 0.0 In a frozen data class, you can not assign values to the fields after creation: >>> pos = Position ( 'Oslo' , 10.8 , 59.9 ) >>> pos . name 'Oslo' >>> pos . name = 'Stockholm' dataclasses . FrozenInstanceError : cannot assign to field 'name' Be aware though that if your data class contains mutable fields, those might still change. This is true for all nested data structures in Python: from dataclasses import dataclass from typing import List @dataclass ( frozen = True ) class ImmutableCard : rank : str suit : str @dataclass ( frozen = True ) class ImmutableDeck : cards : List [ PlayingCard ] Even though both ImmutableCard and ImmutableDeck are immutable, the list holding cards is not. You can therefore still change the cards in the deck: >>> queen_of_hearts = ImmutableCard ( 'Q' , '\u2661' ) >>> ace_of_spades = ImmutableCard ( 'A' , '\u2660' ) >>> deck = ImmutableDeck ([ queen_of_hearts , ace_of_spades ]) >>> deck ImmutableDeck ( cards = [ ImmutableCard ( rank = 'Q' , suit = '\u2661' ), ImmutableCard ( rank = 'A' , suit = '\u2660' )]) >>> deck . cards [ 0 ] = ImmutableCard ( '7' , '\u2662' ) >>> deck ImmutableDeck ( cards = [ ImmutableCard ( rank = '7' , suit = '\u2662' ), ImmutableCard ( rank = 'A' , suit = '\u2660' )]) To avoid this, make sure all fields of an immutable data class use immutable types (but remember that types are not enforced at runtime). The ImmutableDeck should be implemented using a tuple instead of a list. Inheritance \u2691 You can subclass data classes quite freely. from dataclasses import dataclass @dataclass class Position : name : str lon : float lat : float @dataclass class Capital ( Position ): country : str >>> Capital ( 'Oslo' , 10.8 , 59.9 , 'Norway' ) Capital ( name = 'Oslo' , lon = 10.8 , lat = 59.9 , country = 'Norway' ) Warning This won't work if the base class have default values unless all the subclass parameters also have default values. Warning If you redefine a base class field, you need to keep the fields order after the subclass new fields: from dataclasses import dataclass @dataclass class Position : name : str lon : float = 0.0 lat : float = 0.0 @dataclass class Capital ( Position ): country : str = 'Unknown' lat : float = 40.0 Optimizing Data Classes \u2691 Slots can be used to make classes faster and use less memory. from dataclasses import dataclass @dataclass class SimplePosition : name : str lon : float lat : float @dataclass class SlotPosition : __slots__ = [ 'name' , 'lon' , 'lat' ] name : str lon : float lat : float Essentially, slots are defined using .__slots__ to list the variables on a class. Variables or attributes not present in .__slots__ may not be defined. Furthermore, a slots class may not have default values . References \u2691 Real Python Data classes article","title":"Data Classes"},{"location":"coding/python/data_classes/#advantages-over-regular-classes","text":"Simplify the class definition @dataclass class DataClassCard : rank : str suit : str # Versus class RegularCard def __init__ ( self , rank , suit ): self . rank = rank self . suit = suit More descriptive object representation through a better default __repr__() method. >>> queen_of_hearts = DataClassCard ( 'Q' , 'Hearts' ) >>> queen_of_hearts DataClassCard ( rank = 'Q' , suit = 'Hearts' ) # Versus >>> queen_of_spades = RegularCard ( 'Q' , 'Spades' ) >>> queen_of_spades < __main__ . RegularCard object at 0x7fb6eee35d30 > * Instance comparison out of the box through a better default __eq__() method. >>> queen_of_hearts == DataClassCard ( 'Q' , 'Hearts' ) True # Versus >>> queen_of_spades == RegularCard ( 'Q' , 'Spades' ) False","title":"Advantages over regular classes"},{"location":"coding/python/data_classes/#usage","text":"","title":"Usage"},{"location":"coding/python/data_classes/#definition","text":"from dataclasses import dataclass @dataclass class Position : name : str lon : float lat : float What makes this a data class is the @dataclass decorator. Beneath the class Position: , simply list the fields you want in your data class. The data class decorator support the following parameters : init : Add .__init__() method? (Default is True). repr : Add .__repr__() method? (Default is True). eq : Add .__eq__() method? (Default is True). order : Add ordering methods? (Default is False). unsafe_hash : Force the addition of a .__hash__() method? (Default is False). frozen : If True , assigning to fields raise an exception. (Default is False).","title":"Definition"},{"location":"coding/python/data_classes/#default-values","text":"It's easy to add default values to the fields of your data class: from dataclasses import dataclass @dataclass class Position : name : str lon : float = 0.0 lat : float = 0.0 More complex default values can be defined through the use of functions. For example, the next snippet builds a French deck: from dataclasses import dataclass , field from typing import List RANKS = '2 3 4 5 6 7 8 9 10 J Q K A' . split () SUITS = '\u2663 \u2662 \u2661 \u2660' . split () def make_french_deck (): return [ PlayingCard ( r , s ) for s in SUITS for r in RANKS ] @dataclass class PlayingCard : rank : str suit : str @dataclass class Deck : cards : List [ PlayingCard ] = field ( default_factory = make_french_deck ) Using cards: List[PlayingCard] = make_french_deck() introduces the using mutable default arguments anti-pattern. Instead, data classes use the default_factory to handle mutable default values. To use it, you need to use the field() specifier which is used to customize each field of a data class individually. It supports the following parameters: default : Default value of the field. default_factory : Function that returns the initial value of the field. init : Use field in .__init__() method? (Default is True ). repr : Use field in repr of the object? (Default is True ). For example to hide a parameter from the repr , use lat: float = field(default=0.0, repr=False) . compare : Include the field in comparisons? (Default is True ). hash : Include the field when calculating hash() ? (Default is to use the same as compare ). metadata : A mapping with information about the field. It's not used by the data classes themselves but is available for you to attach information to fields. For example: from dataclasses import dataclass , field @dataclass class Position : name : str lon : float = field ( default = 0.0 , metadata = { 'unit' : 'degrees' }) lat : float = field ( default = 0.0 , metadata = { 'unit' : 'degrees' }) To retrieve the information use the fields() function. >>> from dataclasses import fields >>> fields ( Position ) ( Field ( name = 'name' , type =< class ' str '>,...,metadata= {} ), Field ( name = 'lon' , type =< class ' float '>,...,metadata={' unit ': ' degrees '}), Field ( name = 'lat' , type =< class ' float '>,...,metadata={' unit ': ' degrees '})) >>> lat_unit = fields ( Position )[ 2 ] . metadata [ 'unit' ] >>> lat_unit 'degrees'","title":"Default values"},{"location":"coding/python/data_classes/#type-hints","text":"They support typing out of the box. Without a type hint, the field will not be a part of the data class. While you need to add type hints in some form when using data classes, these types are not enforced at runtime. This is how typing in python usually works: Python is and will always be a dynamically typed language .","title":"Type hints"},{"location":"coding/python/data_classes/#adding-methods","text":"Same as with a normal class.","title":"Adding methods"},{"location":"coding/python/data_classes/#adding-complex-order-comparison-logic","text":"from dataclasses import dataclass @dataclass ( order = True ) class PlayingCard : rank : str suit : str def __str__ ( self ): return f ' { self . suit }{ self . rank } ' After setting order=True in the decorator definition the instances of PlayingCard can be compared. >>> queen_of_hearts = PlayingCard ( 'Q' , '\u2661' ) >>> ace_of_spades = PlayingCard ( 'A' , '\u2660' ) >>> ace_of_spades > queen_of_hearts False Data classes compare objects as if they were tuples of their fields. A Queen is higher than an Ace because Q comes after A in the alphabet. >>> ( 'A' , '\u2660' ) > ( 'Q' , '\u2661' ) False To use more complex comparisons, we need to add the field .sort_index to the class. However, this field should be calculated from the other fields automatically. That's what the special method .__post_init__() is for. It allows for special processing after the regular .__init__() method is called. from dataclasses import dataclass , field RANKS = '2 3 4 5 6 7 8 9 10 J Q K A' . split () SUITS = '\u2663 \u2662 \u2661 \u2660' . split () @dataclass ( order = True ) class PlayingCard : sort_index : int = field ( init = False , repr = False ) rank : str suit : str def __post_init__ ( self ): self . sort_index = ( RANKS . index ( self . rank ) * len ( SUITS ) + SUITS . index ( self . suit )) def __str__ ( self ): return f ' { self . suit }{ self . rank } ' Note that .sort_index is added as the first field of the class. That way, the comparison is first done using .sort_index and only if there are ties are the other fields used. Using field() , you must also specify that .sort_index should not be included as a parameter in the .__init__() method (because it is calculated from the .rank and .suit fields). To avoid confusing the user about this implementation detail, it is probably also a good idea to remove .sort_index from the repr of the class.","title":"Adding complex order comparison logic"},{"location":"coding/python/data_classes/#immutable-data-classes","text":"To make a data class immutable, set frozen=True when you create it. from dataclasses import dataclass @dataclass ( frozen = True ) class Position : name : str lon : float = 0.0 lat : float = 0.0 In a frozen data class, you can not assign values to the fields after creation: >>> pos = Position ( 'Oslo' , 10.8 , 59.9 ) >>> pos . name 'Oslo' >>> pos . name = 'Stockholm' dataclasses . FrozenInstanceError : cannot assign to field 'name' Be aware though that if your data class contains mutable fields, those might still change. This is true for all nested data structures in Python: from dataclasses import dataclass from typing import List @dataclass ( frozen = True ) class ImmutableCard : rank : str suit : str @dataclass ( frozen = True ) class ImmutableDeck : cards : List [ PlayingCard ] Even though both ImmutableCard and ImmutableDeck are immutable, the list holding cards is not. You can therefore still change the cards in the deck: >>> queen_of_hearts = ImmutableCard ( 'Q' , '\u2661' ) >>> ace_of_spades = ImmutableCard ( 'A' , '\u2660' ) >>> deck = ImmutableDeck ([ queen_of_hearts , ace_of_spades ]) >>> deck ImmutableDeck ( cards = [ ImmutableCard ( rank = 'Q' , suit = '\u2661' ), ImmutableCard ( rank = 'A' , suit = '\u2660' )]) >>> deck . cards [ 0 ] = ImmutableCard ( '7' , '\u2662' ) >>> deck ImmutableDeck ( cards = [ ImmutableCard ( rank = '7' , suit = '\u2662' ), ImmutableCard ( rank = 'A' , suit = '\u2660' )]) To avoid this, make sure all fields of an immutable data class use immutable types (but remember that types are not enforced at runtime). The ImmutableDeck should be implemented using a tuple instead of a list.","title":"Immutable data classes"},{"location":"coding/python/data_classes/#inheritance","text":"You can subclass data classes quite freely. from dataclasses import dataclass @dataclass class Position : name : str lon : float lat : float @dataclass class Capital ( Position ): country : str >>> Capital ( 'Oslo' , 10.8 , 59.9 , 'Norway' ) Capital ( name = 'Oslo' , lon = 10.8 , lat = 59.9 , country = 'Norway' ) Warning This won't work if the base class have default values unless all the subclass parameters also have default values. Warning If you redefine a base class field, you need to keep the fields order after the subclass new fields: from dataclasses import dataclass @dataclass class Position : name : str lon : float = 0.0 lat : float = 0.0 @dataclass class Capital ( Position ): country : str = 'Unknown' lat : float = 40.0","title":"Inheritance"},{"location":"coding/python/data_classes/#optimizing-data-classes","text":"Slots can be used to make classes faster and use less memory. from dataclasses import dataclass @dataclass class SimplePosition : name : str lon : float lat : float @dataclass class SlotPosition : __slots__ = [ 'name' , 'lon' , 'lat' ] name : str lon : float lat : float Essentially, slots are defined using .__slots__ to list the variables on a class. Variables or attributes not present in .__slots__ may not be defined. Furthermore, a slots class may not have default values .","title":"Optimizing Data Classes"},{"location":"coding/python/data_classes/#references","text":"Real Python Data classes article","title":"References"},{"location":"coding/python/deepdiff/","text":"The DeepDiff library is used to perform search and differences in Python objects. It comes with three operations: DeepDiff: Deep Difference of dictionaries, iterables, strings and other objects. It will recursively look for all the changes. DeepSearch: Search for objects within other objects. DeepHash: Hash any object based on their content even if they are not \u201chashable\u201d. Install \u2691 Install from PyPi: pip install deepdiff DeepSearch \u2691 Deep Search inside objects to find the item matching your criteria. Note that is searches for either the path to match your criteria or the word in an item. Examples: Importing from deepdiff import DeepSearch , grep DeepSearch comes with grep function which is easier to remember! Search in list for string >>> obj = [ \"long somewhere\" , \"string\" , 0 , \"somewhere great!\" ] >>> item = \"somewhere\" >>> ds = obj | grep ( item , verbose_level = 2 ) >>> print ( ds ) { 'matched_values' : { 'root[3]' : 'somewhere great!' , 'root[0]' : 'long somewhere' }} Search in nested data for string >>> obj = [ \"something somewhere\" , { \"long\" : \"somewhere\" , \"string\" : 2 , 0 : 0 , \"somewhere\" : \"around\" }] >>> item = \"somewhere\" >>> ds = obj | grep ( item , verbose_level = 2 ) >>> pprint ( ds , indent = 2 ) { 'matched_paths' : { \"root[1]['somewhere']\" : 'around' }, 'matched_values' : { 'root[0]' : 'something somewhere' , \"root[1]['long']\" : 'somewhere' }} To obtain the keys and values of the matched objects, you can use the Extract object. >>> from deepdiff import grep >>> obj = { 1 : [{ '2' : 'b' }, 3 ], 2 : [ 4 , 5 ]} >>> result = obj | grep ( 5 ) >>> result { 'matched_values' : OrderedSet ([ 'root[2][1]' ])} >>> result [ 'matched_values' ][ 0 ] 'root[2][1]' >>> path = result [ 'matched_values' ][ 0 ] >>> extract ( obj , path ) 5 References \u2691 Homepage/Docs Old Docs","title":"DeepDiff"},{"location":"coding/python/deepdiff/#install","text":"Install from PyPi: pip install deepdiff","title":"Install"},{"location":"coding/python/deepdiff/#deepsearch","text":"Deep Search inside objects to find the item matching your criteria. Note that is searches for either the path to match your criteria or the word in an item. Examples: Importing from deepdiff import DeepSearch , grep DeepSearch comes with grep function which is easier to remember! Search in list for string >>> obj = [ \"long somewhere\" , \"string\" , 0 , \"somewhere great!\" ] >>> item = \"somewhere\" >>> ds = obj | grep ( item , verbose_level = 2 ) >>> print ( ds ) { 'matched_values' : { 'root[3]' : 'somewhere great!' , 'root[0]' : 'long somewhere' }} Search in nested data for string >>> obj = [ \"something somewhere\" , { \"long\" : \"somewhere\" , \"string\" : 2 , 0 : 0 , \"somewhere\" : \"around\" }] >>> item = \"somewhere\" >>> ds = obj | grep ( item , verbose_level = 2 ) >>> pprint ( ds , indent = 2 ) { 'matched_paths' : { \"root[1]['somewhere']\" : 'around' }, 'matched_values' : { 'root[0]' : 'something somewhere' , \"root[1]['long']\" : 'somewhere' }} To obtain the keys and values of the matched objects, you can use the Extract object. >>> from deepdiff import grep >>> obj = { 1 : [{ '2' : 'b' }, 3 ], 2 : [ 4 , 5 ]} >>> result = obj | grep ( 5 ) >>> result { 'matched_values' : OrderedSet ([ 'root[2][1]' ])} >>> result [ 'matched_values' ][ 0 ] 'root[2][1]' >>> path = result [ 'matched_values' ][ 0 ] >>> extract ( obj , path ) 5","title":"DeepSearch"},{"location":"coding/python/deepdiff/#references","text":"Homepage/Docs Old Docs","title":"References"},{"location":"coding/python/docstrings/","text":"Docstrings are strings that define the purpose and use of a module, class, function or method. They are accessible from the doc attribute __doc__ and with the built-in help() function. Documenting your code is very important as it's more often read than written . Documenting vs commenting \u2691 Commenting is describing your code to/for developers. The intended audience is the maintainers and developers of the Python code. In conjuction with well-written code, comments help to guide the reader to better understand your code and its purpose and design. Documenting is describing its use and functionality to your users. While it may be helpful in the development process, the main intended audience are the users. Docstring format \u2691 Docstrings should use the triple-double quote ( \"\"\" ) string format. This should be done whether the docstring is multi-lined or not. At a bare minimum, a docstring should be a quick summary of whatever is it you\u2019re describing and should be contained within a single line. Multi-lined docstrings are used to further elaborate on the object beyond the summary. All multi-lined docstrings have the following parts: A one-line summary line A blank line proceeding the summary Any further elaboration for the docstring Another blank line To ensure your docstrings follow these practices, configure flakehell with the flake8-docstrings extension. Docstring types \u2691 Class docstrings \u2691 Class Docstrings are created for the class itself, as well as any class methods. The docstrings are placed immediately following the class or class method indented by one level: class SimpleClass: \"\"\"Class docstrings go here.\"\"\" def say_hello(self, name: str): \"\"\"Class method docstrings go here.\"\"\" print(f'Hello {name}') Class docstrings should contain the following information: A brief summary of its purpose and behavior Any public methods, along with a brief description Any class properties (attributes) Anything related to the interface for subclassers, if the class is intended to be subclassed The class constructor parameters should be documented within the init class method docstring. Individual methods should be documented using their individual docstrings. Class method docstrings should contain the following: A brief description of what the method is and what it\u2019s used for Any arguments (both required and optional) that are passed including keyword arguments Label any arguments that are considered optional or have a default value Any side effects that occur when executing the method Any exceptions that are raised Any restrictions on when the method can be called Package and module docstrings \u2691 Package docstrings should be placed at the top of the package\u2019s __init__.py file. This docstring should list the modules and sub-packages that are exported by the package. Module docstrings should include the following: A brief description of the module and its purpose A list of any classes, exception, functions, and any other objects exported by the module. Only needed if they are not defined in the same file, otherwise help() will get them automatically. The docstring for a module function should include the same items as a class method. Docstring formats \u2691 Google docstrings : the most user friendly. reStructured Text : The official ones, but super ugly to write. Numpy docstrings . Google Docstrings \u2691 Napoleon gathered a nice cheatsheet with examples . Functions and methods \u2691 A method that overrides a method from a base class may have a simple docstring sending the reader to its overridden method\u2019s docstring, such as \"\"\"See base class.\"\"\" . The rationale is that there is no need to repeat in many places documentation that is already present in the base method\u2019s docstring. However, if the overriding method\u2019s behavior is substantially different from the overridden method, or details need to be provided (e.g., documenting additional side effects), a docstring with at least those differences is required on the overriding method. Certain aspects of a function should be documented in special sections, listed below. Each section begins with a heading line, which ends with a colon. All sections other than the heading should maintain a hanging indent of two or four spaces (be consistent within a file). These sections can be omitted in cases where the function\u2019s name and signature are informative enough that it can be aptly described using a one-line docstring. Args List each parameter by name. A description should follow the name, and be separated by a colon followed by either a space or newline. If the description is too long to fit on a single 80-character line, use a hanging indent of 2 or 4 spaces more than the parameter name (be consistent with the rest of the docstrings in the file). The description should include required type(s) if the code does not contain a corresponding type annotation. If a function accepts *foo (variable length argument lists) and/or **bar (arbitrary keyword arguments), they should be listed as *foo and **bar . Returns (or Yields: for generators) Describe the type and semantics of the return value. If the function only returns None, this section is not required. It may also be omitted if the docstring starts with Returns or Yields (e.g. \"\"\"Returns row from API as a tuple of strings.\"\"\" ) and the opening sentence is sufficient to describe return value. Raises List all exceptions that are relevant to the interface followed by a description. Use a similar exception name + colon + space or newline and hanging indent style as described in Args:. You should not document exceptions that get raised if the API specified in the docstring is violated (because this would paradoxically make behavior under violation of the API part of the API). Automatic documentation generation \u2691 Use the mkdocstrings plugin to automatically generate the documentation. References \u2691 Real Python post on docstrings by James Mertz","title":"Docstrings"},{"location":"coding/python/docstrings/#documenting-vs-commenting","text":"Commenting is describing your code to/for developers. The intended audience is the maintainers and developers of the Python code. In conjuction with well-written code, comments help to guide the reader to better understand your code and its purpose and design. Documenting is describing its use and functionality to your users. While it may be helpful in the development process, the main intended audience are the users.","title":"Documenting vs commenting"},{"location":"coding/python/docstrings/#docstring-format","text":"Docstrings should use the triple-double quote ( \"\"\" ) string format. This should be done whether the docstring is multi-lined or not. At a bare minimum, a docstring should be a quick summary of whatever is it you\u2019re describing and should be contained within a single line. Multi-lined docstrings are used to further elaborate on the object beyond the summary. All multi-lined docstrings have the following parts: A one-line summary line A blank line proceeding the summary Any further elaboration for the docstring Another blank line To ensure your docstrings follow these practices, configure flakehell with the flake8-docstrings extension.","title":"Docstring format"},{"location":"coding/python/docstrings/#docstring-types","text":"","title":"Docstring types"},{"location":"coding/python/docstrings/#class-docstrings","text":"Class Docstrings are created for the class itself, as well as any class methods. The docstrings are placed immediately following the class or class method indented by one level: class SimpleClass: \"\"\"Class docstrings go here.\"\"\" def say_hello(self, name: str): \"\"\"Class method docstrings go here.\"\"\" print(f'Hello {name}') Class docstrings should contain the following information: A brief summary of its purpose and behavior Any public methods, along with a brief description Any class properties (attributes) Anything related to the interface for subclassers, if the class is intended to be subclassed The class constructor parameters should be documented within the init class method docstring. Individual methods should be documented using their individual docstrings. Class method docstrings should contain the following: A brief description of what the method is and what it\u2019s used for Any arguments (both required and optional) that are passed including keyword arguments Label any arguments that are considered optional or have a default value Any side effects that occur when executing the method Any exceptions that are raised Any restrictions on when the method can be called","title":"Class docstrings"},{"location":"coding/python/docstrings/#package-and-module-docstrings","text":"Package docstrings should be placed at the top of the package\u2019s __init__.py file. This docstring should list the modules and sub-packages that are exported by the package. Module docstrings should include the following: A brief description of the module and its purpose A list of any classes, exception, functions, and any other objects exported by the module. Only needed if they are not defined in the same file, otherwise help() will get them automatically. The docstring for a module function should include the same items as a class method.","title":"Package and module docstrings"},{"location":"coding/python/docstrings/#docstring-formats","text":"Google docstrings : the most user friendly. reStructured Text : The official ones, but super ugly to write. Numpy docstrings .","title":"Docstring formats"},{"location":"coding/python/docstrings/#google-docstrings","text":"Napoleon gathered a nice cheatsheet with examples .","title":"Google Docstrings"},{"location":"coding/python/docstrings/#functions-and-methods","text":"A method that overrides a method from a base class may have a simple docstring sending the reader to its overridden method\u2019s docstring, such as \"\"\"See base class.\"\"\" . The rationale is that there is no need to repeat in many places documentation that is already present in the base method\u2019s docstring. However, if the overriding method\u2019s behavior is substantially different from the overridden method, or details need to be provided (e.g., documenting additional side effects), a docstring with at least those differences is required on the overriding method. Certain aspects of a function should be documented in special sections, listed below. Each section begins with a heading line, which ends with a colon. All sections other than the heading should maintain a hanging indent of two or four spaces (be consistent within a file). These sections can be omitted in cases where the function\u2019s name and signature are informative enough that it can be aptly described using a one-line docstring. Args List each parameter by name. A description should follow the name, and be separated by a colon followed by either a space or newline. If the description is too long to fit on a single 80-character line, use a hanging indent of 2 or 4 spaces more than the parameter name (be consistent with the rest of the docstrings in the file). The description should include required type(s) if the code does not contain a corresponding type annotation. If a function accepts *foo (variable length argument lists) and/or **bar (arbitrary keyword arguments), they should be listed as *foo and **bar . Returns (or Yields: for generators) Describe the type and semantics of the return value. If the function only returns None, this section is not required. It may also be omitted if the docstring starts with Returns or Yields (e.g. \"\"\"Returns row from API as a tuple of strings.\"\"\" ) and the opening sentence is sufficient to describe return value. Raises List all exceptions that are relevant to the interface followed by a description. Use a similar exception name + colon + space or newline and hanging indent style as described in Args:. You should not document exceptions that get raised if the API specified in the docstring is violated (because this would paradoxically make behavior under violation of the API part of the API).","title":"Functions and methods"},{"location":"coding/python/docstrings/#automatic-documentation-generation","text":"Use the mkdocstrings plugin to automatically generate the documentation.","title":"Automatic documentation generation"},{"location":"coding/python/docstrings/#references","text":"Real Python post on docstrings by James Mertz","title":"References"},{"location":"coding/python/factoryboy/","text":"Factoryboy is a fixtures replacement library to generate fake data for your program. As it's designed to work well with different ORMs (Django, SQLAlchemy , Mongo) it serves the purpose of building real objects for your tests. Install \u2691 pip install factory_boy Or add it to the project requirements.txt . Define a factory class \u2691 Use the following code to generate a factory class for the User SQLAlchemy class. from {{ program_name }} import models import factory # XXX If you add new Factories remember to add the session in conftest.py class UserFactory ( factory . alchemy . SQLAlchemyModelFactory ): \"\"\" Class to generate a fake user element. \"\"\" id = factory . Sequence ( lambda n : n ) name = factory . Faker ( 'name' ) class Meta : model = models . User sqlalchemy_session_persistence = 'commit' As stated in the comment, and if you are using the proposed python project template , remember to add new Factories in conftest.py . Use the factory class \u2691 Create an instance. UserFactory . create () Create an instance with a defined attribute. UserFactory . create ( name = 'John' ) Create 100 instances of objects with an attribute defined. UserFactory . create_batch ( 100 , name = 'John' ) Define attributes \u2691 I like to use the faker integration of factory boy to generate most of the attributes. Generate numbers \u2691 Sequential numbers \u2691 Ideal for IDs id = factory . Sequence ( lambda n : n ) Random number or integer \u2691 author_id = factory . Faker ( 'random_number' ) If you want to limit the number of digits use factory.Faker('random_number', digits=3) Random float \u2691 score = factory . Faker ( 'pyfloat' ) Generate strings \u2691 Word \u2691 default = factory . Faker ( 'word' ) Word from a list \u2691 user = factory . Faker ( 'word' , ext_word_list = [ None , 'value_1' , 'value_2' ]) Word from Enum choices \u2691 First install the Enum provider factory . Faker . add_provider ( EnumProvider ) class EntityFactory ( factory . Factory ): # type: ignore state = factory . Faker ( \"enum\" , enum_cls = EntityState ) Sentences \u2691 description = factory . Faker ( 'sentence' ) Names \u2691 name = factory . Faker ( 'name' ) Urls \u2691 url = factory . Faker ( 'url' ) Files \u2691 file_path = factory . Faker ( 'file_path' ) Generate Datetime \u2691 factory . Faker ( 'date_time' ) Generate bool \u2691 factory . Faker ( 'pybool' ) Generate your own attributes \u2691 Using custom Faker providers \u2691 Using lazy_attributes \u2691 Use lazy_attribute decorator. If you want to use Faker inside a lazy_attribute use .generate({}) at the end of the attribute. In newer versions of Factoryboy you can't use Faker inside a lazy attribute As the Faker object doesn't have the generate method. @factory . lazy_attribute def due ( self ): if random . random () > 0.5 : return factory . Faker ( 'date_time' ) . generate ({}) Define relationships \u2691 Factory Inheritance \u2691 class ContentFactory ( factory . alchemy . SQLAlchemyModelFactory ): id = factory . Sequence ( lambda n : n ) title = factory . Faker ( 'sentence' ) class Meta : model = models . Content sqlalchemy_session_persistence = 'commit' class ArticleFactory ( ContentFactory ): body = factory . Faker ( 'sentence' ) class Meta : model = models . Article sqlalchemy_session_persistence = 'commit' Dependent objects direct ForeignKey \u2691 When one attribute is actually a complex field (e.g a ForeignKey to another Model), use the SubFactory declaration. Assuming the following model definition: class Author ( Base ): id = Column ( String , primary_key = True ) contents = relationship ( 'Content' , back_populates = 'author' ) class Content ( Base ): id = Column ( Integer , primary_key = True , doc = 'Content ID' ) author_id = Column ( String , ForeignKey ( Author . id )) author = relationship ( Author , back_populates = 'contents' ) The related factories would be: class AuthorFactory ( factory . alchemy . SQLAlchemyModelFactory ): id = factory . Faker ( 'word' ) class Meta : model = models . Author sqlalchemy_session_persistence = 'commit' class ContentFactory ( factory . alchemy . SQLAlchemyModelFactory ): id = factory . Sequence ( lambda n : n ) author = factory . SubFactory ( AuthorFactory ) class Meta : model = models . Content sqlalchemy_session_persistence = 'commit' References \u2691 Docs Git Common recipes","title":"FactoryBoy"},{"location":"coding/python/factoryboy/#install","text":"pip install factory_boy Or add it to the project requirements.txt .","title":"Install"},{"location":"coding/python/factoryboy/#define-a-factory-class","text":"Use the following code to generate a factory class for the User SQLAlchemy class. from {{ program_name }} import models import factory # XXX If you add new Factories remember to add the session in conftest.py class UserFactory ( factory . alchemy . SQLAlchemyModelFactory ): \"\"\" Class to generate a fake user element. \"\"\" id = factory . Sequence ( lambda n : n ) name = factory . Faker ( 'name' ) class Meta : model = models . User sqlalchemy_session_persistence = 'commit' As stated in the comment, and if you are using the proposed python project template , remember to add new Factories in conftest.py .","title":"Define a factory class"},{"location":"coding/python/factoryboy/#use-the-factory-class","text":"Create an instance. UserFactory . create () Create an instance with a defined attribute. UserFactory . create ( name = 'John' ) Create 100 instances of objects with an attribute defined. UserFactory . create_batch ( 100 , name = 'John' )","title":"Use the factory class"},{"location":"coding/python/factoryboy/#define-attributes","text":"I like to use the faker integration of factory boy to generate most of the attributes.","title":"Define attributes"},{"location":"coding/python/factoryboy/#generate-numbers","text":"","title":"Generate numbers"},{"location":"coding/python/factoryboy/#sequential-numbers","text":"Ideal for IDs id = factory . Sequence ( lambda n : n )","title":"Sequential numbers"},{"location":"coding/python/factoryboy/#random-number-or-integer","text":"author_id = factory . Faker ( 'random_number' ) If you want to limit the number of digits use factory.Faker('random_number', digits=3)","title":"Random number or integer"},{"location":"coding/python/factoryboy/#random-float","text":"score = factory . Faker ( 'pyfloat' )","title":"Random float"},{"location":"coding/python/factoryboy/#generate-strings","text":"","title":"Generate strings"},{"location":"coding/python/factoryboy/#word","text":"default = factory . Faker ( 'word' )","title":"Word"},{"location":"coding/python/factoryboy/#word-from-a-list","text":"user = factory . Faker ( 'word' , ext_word_list = [ None , 'value_1' , 'value_2' ])","title":"Word from a list"},{"location":"coding/python/factoryboy/#word-from-enum-choices","text":"First install the Enum provider factory . Faker . add_provider ( EnumProvider ) class EntityFactory ( factory . Factory ): # type: ignore state = factory . Faker ( \"enum\" , enum_cls = EntityState )","title":"Word from Enum choices"},{"location":"coding/python/factoryboy/#sentences","text":"description = factory . Faker ( 'sentence' )","title":"Sentences"},{"location":"coding/python/factoryboy/#names","text":"name = factory . Faker ( 'name' )","title":"Names"},{"location":"coding/python/factoryboy/#urls","text":"url = factory . Faker ( 'url' )","title":"Urls"},{"location":"coding/python/factoryboy/#files","text":"file_path = factory . Faker ( 'file_path' )","title":"Files"},{"location":"coding/python/factoryboy/#generate-datetime","text":"factory . Faker ( 'date_time' )","title":"Generate Datetime"},{"location":"coding/python/factoryboy/#generate-bool","text":"factory . Faker ( 'pybool' )","title":"Generate bool"},{"location":"coding/python/factoryboy/#generate-your-own-attributes","text":"","title":"Generate your own attributes"},{"location":"coding/python/factoryboy/#using-custom-faker-providers","text":"","title":"Using custom Faker providers"},{"location":"coding/python/factoryboy/#using-lazy_attributes","text":"Use lazy_attribute decorator. If you want to use Faker inside a lazy_attribute use .generate({}) at the end of the attribute. In newer versions of Factoryboy you can't use Faker inside a lazy attribute As the Faker object doesn't have the generate method. @factory . lazy_attribute def due ( self ): if random . random () > 0.5 : return factory . Faker ( 'date_time' ) . generate ({})","title":"Using lazy_attributes"},{"location":"coding/python/factoryboy/#define-relationships","text":"","title":"Define relationships"},{"location":"coding/python/factoryboy/#factory-inheritance","text":"class ContentFactory ( factory . alchemy . SQLAlchemyModelFactory ): id = factory . Sequence ( lambda n : n ) title = factory . Faker ( 'sentence' ) class Meta : model = models . Content sqlalchemy_session_persistence = 'commit' class ArticleFactory ( ContentFactory ): body = factory . Faker ( 'sentence' ) class Meta : model = models . Article sqlalchemy_session_persistence = 'commit'","title":"Factory Inheritance"},{"location":"coding/python/factoryboy/#dependent-objects-direct-foreignkey","text":"When one attribute is actually a complex field (e.g a ForeignKey to another Model), use the SubFactory declaration. Assuming the following model definition: class Author ( Base ): id = Column ( String , primary_key = True ) contents = relationship ( 'Content' , back_populates = 'author' ) class Content ( Base ): id = Column ( Integer , primary_key = True , doc = 'Content ID' ) author_id = Column ( String , ForeignKey ( Author . id )) author = relationship ( Author , back_populates = 'contents' ) The related factories would be: class AuthorFactory ( factory . alchemy . SQLAlchemyModelFactory ): id = factory . Faker ( 'word' ) class Meta : model = models . Author sqlalchemy_session_persistence = 'commit' class ContentFactory ( factory . alchemy . SQLAlchemyModelFactory ): id = factory . Sequence ( lambda n : n ) author = factory . SubFactory ( AuthorFactory ) class Meta : model = models . Content sqlalchemy_session_persistence = 'commit'","title":"Dependent objects direct ForeignKey"},{"location":"coding/python/factoryboy/#references","text":"Docs Git Common recipes","title":"References"},{"location":"coding/python/faker/","text":"Faker is a Python package that generates fake data for you. Whether you need to bootstrap your database, create good-looking XML documents, fill-in your persistence to stress test it, or anonymize data taken from a production service, Faker is for you. Install \u2691 If you use factoryboy you'd probably have it. If you don't use pip install faker Or add it to the project requirements.txt . Use \u2691 Faker includes a faker fixture for pytest. def test_faker ( faker ): assert isinstance ( faker . name (), str ) By default it's populated with a seed of 0 , to set a random seed add the following to your test configuration. File: conftest.py from random import SystemRandom @pytest . fixture ( scope = \"session\" , autouse = True ) def faker_seed () -> int : \"\"\"Create a random seed for the Faker library.\"\"\" return SystemRandom () . randint ( 0 , 999999 ) Generate fake number \u2691 fake . random_number () If you want to specify max and min values use: faker . pyint ( min_value = 0 , max_value = 99 ) Generate a fake dictionary \u2691 fake . pydict ( nb_elements = 5 , variable_nb_elements = True , * value_types ) Where *value_types can be 'str', 'list' Generate a fake date \u2691 fake . date_time () Generate a random string \u2691 faker . pystr () Create a random choice from an Enum \u2691 pydantic uses Enum objects to define the choices of fields , so we need them to create the factories of those objects. Sadly, there is no official provider for Enums , but NazarioJL made a custom provider. Install \u2691 pip install faker-enum Usage \u2691 from enum import Enum from faker import Faker from faker_enum import EnumProvider fake = Faker () fake . add_provider ( EnumProvider ) class Color ( Enum ): RED = 1 GREEN = 2 BLUE = 3 fake . enum ( Color ) # One of [Color.RED, Color.GREEN, Color.BLUE] If you're using factoryboy , check this instructions . Create Optional data \u2691 Install \u2691 pip install fake-optional Usage \u2691 from faker import Faker from faker_optional import OptionalProvider fake = Faker () fake . add_provider ( OptionalProvider ) fake . optional_int () # None fake . optional_int () # 1234 OptionalProvider uses existent faker providers to create the data, so you can use the provider method arguments. For example, optional_int uses the python provider pyint , so you can use the min_value , max_value , and step arguments. Every optional_ method accepts the float ratio argument between 0 and 1 , with a default value of 0.5 to define what percent of results should be None , a greater value will mean that less results will be None . Check the supported methods . Generate your own custom provider \u2691 Providers are just classes which define the methods we call on Faker objects to generate fake data. To define a provider, you need to create a class that inherits from the BaseProvider . That class can then define as many methods as you want. Once your provider is ready, add it to your Faker instance. import random from faker import Faker from faker.providers import BaseProvider fake = Faker () # Our custom provider inherits from the BaseProvider class TravelProvider ( BaseProvider ): def destination ( self ): destinations = [ 'NY' , 'CO' , 'CA' , 'TX' , 'RI' ] # We select a random destination from the list and return it return random . choice ( destinations ) # Add the TravelProvider to our faker object fake . add_provider ( TravelProvider ) # We can now use the destination method: print ( fake . destination ()) If you want to give arguments when calling the provider, add them to the provider method. References \u2691 Git Docs Faker python providers","title":"Faker"},{"location":"coding/python/faker/#install","text":"If you use factoryboy you'd probably have it. If you don't use pip install faker Or add it to the project requirements.txt .","title":"Install"},{"location":"coding/python/faker/#use","text":"Faker includes a faker fixture for pytest. def test_faker ( faker ): assert isinstance ( faker . name (), str ) By default it's populated with a seed of 0 , to set a random seed add the following to your test configuration. File: conftest.py from random import SystemRandom @pytest . fixture ( scope = \"session\" , autouse = True ) def faker_seed () -> int : \"\"\"Create a random seed for the Faker library.\"\"\" return SystemRandom () . randint ( 0 , 999999 )","title":"Use"},{"location":"coding/python/faker/#generate-fake-number","text":"fake . random_number () If you want to specify max and min values use: faker . pyint ( min_value = 0 , max_value = 99 )","title":"Generate fake number"},{"location":"coding/python/faker/#generate-a-fake-dictionary","text":"fake . pydict ( nb_elements = 5 , variable_nb_elements = True , * value_types ) Where *value_types can be 'str', 'list'","title":"Generate a fake dictionary"},{"location":"coding/python/faker/#generate-a-fake-date","text":"fake . date_time ()","title":"Generate a fake date"},{"location":"coding/python/faker/#generate-a-random-string","text":"faker . pystr ()","title":"Generate a random string"},{"location":"coding/python/faker/#create-a-random-choice-from-an-enum","text":"pydantic uses Enum objects to define the choices of fields , so we need them to create the factories of those objects. Sadly, there is no official provider for Enums , but NazarioJL made a custom provider.","title":"Create a random choice from an Enum"},{"location":"coding/python/faker/#install_1","text":"pip install faker-enum","title":"Install"},{"location":"coding/python/faker/#usage","text":"from enum import Enum from faker import Faker from faker_enum import EnumProvider fake = Faker () fake . add_provider ( EnumProvider ) class Color ( Enum ): RED = 1 GREEN = 2 BLUE = 3 fake . enum ( Color ) # One of [Color.RED, Color.GREEN, Color.BLUE] If you're using factoryboy , check this instructions .","title":"Usage"},{"location":"coding/python/faker/#create-optional-data","text":"","title":"Create Optional data"},{"location":"coding/python/faker/#install_2","text":"pip install fake-optional","title":"Install"},{"location":"coding/python/faker/#usage_1","text":"from faker import Faker from faker_optional import OptionalProvider fake = Faker () fake . add_provider ( OptionalProvider ) fake . optional_int () # None fake . optional_int () # 1234 OptionalProvider uses existent faker providers to create the data, so you can use the provider method arguments. For example, optional_int uses the python provider pyint , so you can use the min_value , max_value , and step arguments. Every optional_ method accepts the float ratio argument between 0 and 1 , with a default value of 0.5 to define what percent of results should be None , a greater value will mean that less results will be None . Check the supported methods .","title":"Usage"},{"location":"coding/python/faker/#generate-your-own-custom-provider","text":"Providers are just classes which define the methods we call on Faker objects to generate fake data. To define a provider, you need to create a class that inherits from the BaseProvider . That class can then define as many methods as you want. Once your provider is ready, add it to your Faker instance. import random from faker import Faker from faker.providers import BaseProvider fake = Faker () # Our custom provider inherits from the BaseProvider class TravelProvider ( BaseProvider ): def destination ( self ): destinations = [ 'NY' , 'CO' , 'CA' , 'TX' , 'RI' ] # We select a random destination from the list and return it return random . choice ( destinations ) # Add the TravelProvider to our faker object fake . add_provider ( TravelProvider ) # We can now use the destination method: print ( fake . destination ()) If you want to give arguments when calling the provider, add them to the provider method.","title":"Generate your own custom provider"},{"location":"coding/python/faker/#references","text":"Git Docs Faker python providers","title":"References"},{"location":"coding/python/feedparser/","text":"Parse Atom and RSS feeds in Python. Install \u2691 pip install feedparser Basic Usage \u2691 Parse a feed from a remote URL \u2691 >>> import feedparser >>> d = feedparser . parse ( 'http://feedparser.org/docs/examples/atom10.xml' ) >>> d [ 'feed' ][ 'title' ] u 'Sample Feed' Access common elements \u2691 The most commonly used elements in RSS feeds (regardless of version) are title, link, description, publication date, and entry ID. Channel elements \u2691 >>> d . feed . title u 'Sample Feed' >>> d . feed . link u 'http://example.org/' >>> d . feed . description u 'For documentation <em>only</em>' >>> d . feed . published u 'Sat, 07 Sep 2002 00:00:01 GMT' >>> d . feed . published_parsed ( 2002 , 9 , 7 , 0 , 0 , 1 , 5 , 250 , 0 ) All parsed dates can be converted to datetime with the following snippet: from time import mktime from datetime import datetime dt = datetime . fromtimestamp ( mktime ( item [ 'updated_parsed' ])) Item elements \u2691 >>> d . entries [ 0 ] . title u 'First item title' >>> d . entries [ 0 ] . link u 'http://example.org/item/1' >>> d . entries [ 0 ] . description u 'Watch out for <span>nasty tricks</span>' >>> d . entries [ 0 ] . published u 'Thu, 05 Sep 2002 00:00:01 GMT' >>> d . entries [ 0 ] . published_parsed ( 2002 , 9 , 5 , 0 , 0 , 1 , 3 , 248 , 0 ) >>> d . entries [ 0 ] . id u 'http://example.org/guid/1' An RSS feed can specify a small image which some aggregators display as a logo. >>> d . feed . image { 'title' : u 'Example banner' , 'href' : u 'http://example.org/banner.png' , 'width' : 80 , 'height' : 15 , 'link' : u 'http://example.org/' } Feeds and entries can be assigned to multiple categories , and in some versions of RSS, categories can be associated with a \u201cdomain\u201d. >>> d . feed . categories [( u 'Syndic8' , u '1024' ), ( u 'dmoz' , 'Top/Society/People/Personal_Homepages/P/' )] As feeds in the real world may be missing some elements, you may want to test for the existence of an element before getting its value. >>> import feedparser >>> d = feedparser . parse ( 'http://feedparser.org/docs/examples/atom10.xml' ) >>> 'title' in d . feed True >>> 'ttl' in d . feed False >>> d . feed . get ( 'title' , 'No title' ) u 'Sample feed' >>> d . feed . get ( 'ttl' , 60 ) 60 Advanced usage \u2691 It is possible to interact with feeds that are protected with credentials . Links \u2691 Git Docs","title":"Feedparser"},{"location":"coding/python/feedparser/#install","text":"pip install feedparser","title":"Install"},{"location":"coding/python/feedparser/#basic-usage","text":"","title":"Basic Usage"},{"location":"coding/python/feedparser/#parse-a-feed-from-a-remote-url","text":">>> import feedparser >>> d = feedparser . parse ( 'http://feedparser.org/docs/examples/atom10.xml' ) >>> d [ 'feed' ][ 'title' ] u 'Sample Feed'","title":"Parse a feed from a remote URL"},{"location":"coding/python/feedparser/#access-common-elements","text":"The most commonly used elements in RSS feeds (regardless of version) are title, link, description, publication date, and entry ID.","title":"Access common elements"},{"location":"coding/python/feedparser/#channel-elements","text":">>> d . feed . title u 'Sample Feed' >>> d . feed . link u 'http://example.org/' >>> d . feed . description u 'For documentation <em>only</em>' >>> d . feed . published u 'Sat, 07 Sep 2002 00:00:01 GMT' >>> d . feed . published_parsed ( 2002 , 9 , 7 , 0 , 0 , 1 , 5 , 250 , 0 ) All parsed dates can be converted to datetime with the following snippet: from time import mktime from datetime import datetime dt = datetime . fromtimestamp ( mktime ( item [ 'updated_parsed' ]))","title":"Channel elements"},{"location":"coding/python/feedparser/#item-elements","text":">>> d . entries [ 0 ] . title u 'First item title' >>> d . entries [ 0 ] . link u 'http://example.org/item/1' >>> d . entries [ 0 ] . description u 'Watch out for <span>nasty tricks</span>' >>> d . entries [ 0 ] . published u 'Thu, 05 Sep 2002 00:00:01 GMT' >>> d . entries [ 0 ] . published_parsed ( 2002 , 9 , 5 , 0 , 0 , 1 , 3 , 248 , 0 ) >>> d . entries [ 0 ] . id u 'http://example.org/guid/1' An RSS feed can specify a small image which some aggregators display as a logo. >>> d . feed . image { 'title' : u 'Example banner' , 'href' : u 'http://example.org/banner.png' , 'width' : 80 , 'height' : 15 , 'link' : u 'http://example.org/' } Feeds and entries can be assigned to multiple categories , and in some versions of RSS, categories can be associated with a \u201cdomain\u201d. >>> d . feed . categories [( u 'Syndic8' , u '1024' ), ( u 'dmoz' , 'Top/Society/People/Personal_Homepages/P/' )] As feeds in the real world may be missing some elements, you may want to test for the existence of an element before getting its value. >>> import feedparser >>> d = feedparser . parse ( 'http://feedparser.org/docs/examples/atom10.xml' ) >>> 'title' in d . feed True >>> 'ttl' in d . feed False >>> d . feed . get ( 'title' , 'No title' ) u 'Sample feed' >>> d . feed . get ( 'ttl' , 60 ) 60","title":"Item elements"},{"location":"coding/python/feedparser/#advanced-usage","text":"It is possible to interact with feeds that are protected with credentials .","title":"Advanced usage"},{"location":"coding/python/feedparser/#links","text":"Git Docs","title":"Links"},{"location":"coding/python/flask/","text":"Flask is a micro web framework written in Python. It has no database abstraction layer, form validation, or any other components where pre-existing third-party libraries provide common functions. However, Flask supports extensions that can add application features as if they were implemented in Flask itself. Extensions exist for object-relational mappers, form validation, upload handling, various open authentication technologies and several common framework related tools. Extensions are updated far more frequently than the core Flask program. Install \u2691 pip install flask How flask blueprints work \u2691 A blueprint is an object that defines a collection of views, templates, static files and other elements that can be applied to an application. The killer use-case for blueprints is to organize our application into distinct components. However, a Flask Blueprint needs to be registered in an application before you can run it. Once it's done, it extends the application with the contents of the Blueprint. Making a Flask Blueprint \u2691 References \u2691 Docs Tutorials \u2691 Miguel's Flask Mega-Tutorial Patrick's Software Flask Tutorial Blueprints \u2691 Flask docs on blueprints Explore flask article on Blueprints","title":"Flask"},{"location":"coding/python/flask/#install","text":"pip install flask","title":"Install"},{"location":"coding/python/flask/#how-flask-blueprints-work","text":"A blueprint is an object that defines a collection of views, templates, static files and other elements that can be applied to an application. The killer use-case for blueprints is to organize our application into distinct components. However, a Flask Blueprint needs to be registered in an application before you can run it. Once it's done, it extends the application with the contents of the Blueprint.","title":"How flask blueprints work"},{"location":"coding/python/flask/#making-a-flask-blueprint","text":"","title":"Making a Flask Blueprint"},{"location":"coding/python/flask/#references","text":"Docs","title":"References"},{"location":"coding/python/flask/#tutorials","text":"Miguel's Flask Mega-Tutorial Patrick's Software Flask Tutorial","title":"Tutorials"},{"location":"coding/python/flask/#blueprints","text":"Flask docs on blueprints Explore flask article on Blueprints","title":"Blueprints"},{"location":"coding/python/flask_restplus/","text":"Use FastAPI instead Flask-RESTPlus is an extension for Flask that adds support for quickly building REST APIs. Flask-RESTPlus encourages best practices with minimal setup. If you are familiar with Flask, Flask-RESTPlus should be easy to pick up. It provides a coherent collection of decorators and tools to describe your API and expose its documentation properly using Swagger. Over plain flask it adds the following capabilities: It defines namespaces, which are ways of creating prefixes and structuring the code : This helps long-term maintenance and helps with the design when creating new endpoints. It has a full solution for parsing input parameters : This means that we have an easy way of dealing with endpoints that requires several parameters and validates them. It has a serialization framework for the resulting objects : This helps to define objects that can be reused, clarifying the interface and simplifying the development. It has full Swagger API documentation support . Install \u2691 pip install flask-restplus References \u2691 Docs Git","title":"Flask Restplus"},{"location":"coding/python/flask_restplus/#install","text":"pip install flask-restplus","title":"Install"},{"location":"coding/python/flask_restplus/#references","text":"Docs Git","title":"References"},{"location":"coding/python/folium/","text":"folium makes it easy to visualize data that\u2019s been manipulated in Python on an interactive leaflet map. It enables both the binding of data to a map for choropleth visualizations as well as passing rich vector/raster/HTML visualizations as markers on the map. The library has a number of built-in tilesets from OpenStreetMap, Mapbox, and Stamen, and supports custom tilesets with Mapbox or Cloudmade API keys. folium supports both Image, Video, GeoJSON and TopoJSON overlays. Use dash-leaflet if you want to do complex stuff. If you want to do multiple filters on the plotted data or connect the map with other graphics, use dash-leaflet instead. Install \u2691 Although you can install it with pip install folium their release pace is slow, therefore I recommend pulling it directly from master pip install git+https://github.com/python-visualization/folium.git@master It's a heavy repo, so it might take some time. Usage \u2691 Use the following snippet to create an empty map centered in Spain import folium m = folium . Map ( location = [ 40.0884 , - 3.68042 ], zoom_start = 7 , ) # Map configuration goes here m . save ( \"map.html\" ) From now on, those lines are going to be assumed, and all the snippets are supposed to go in between the definition and the save. If you need to center the map elsewhere, I suggest that you configure the map to show the coordinates of the mouse with from folium.plugins import MousePosition MousePosition () . add_to ( m ) Change tileset \u2691 Folium Map object supports different tiles by default. It also supports any WMS or WMTS by passing a Leaflet-style URL to the tiles parameter: http://{s}.yourtiles.com/{z}/{x}/{y}.png . To use the IGN beautiful map as a fallback and the OpenStreetMaps as default use the following snippet: m = folium . Map ( location = [ 40.0884 , - 3.68042 ], zoom_start = 7 , tiles = None , ) folium . raster_layers . TileLayer ( name = \"OpenStreetMaps\" , tiles = \"OpenStreetMap\" , attr = \"OpenStreetMaps\" , ) . add_to ( m ) folium . raster_layers . TileLayer ( name = \"IGN\" , tiles = \"https://www.ign.es/wmts/mapa-raster?request=getTile&layer=MTN&TileMatrixSet=GoogleMapsCompatible&TileMatrix= {z} &TileCol= {x} &TileRow= {y} &format=image/jpeg\" , attr = \"IGN\" , ) . add_to ( m ) folium . LayerControl () . add_to ( m ) We need to set the tiles=None in the Map definition so both are shown in the LayerControl menu. Loading the data \u2691 Using geojson \u2691 folium . GeoJson ( \"my_map.geojson\" , name = \"geojson\" ) . add_to ( m ) If you don't want the data to be embedded in the html use embed=False , this can be handy if you don't want to redeploy the web application on each change of data. You'll need to supply a valid url to the data file. The downside (as of today) of using geojson is that you can't have different markers for the data . The solution is to load it and iterate over the elements. See the issue for more information. Using gpx \u2691 Another popular data format is gpx, which is the one that OsmAnd uses. To import it we'll use the gpxpy library. import gpxpy import gpxpy.gpx gpx_file = open ( 'map.gpx' , 'r' ) gpx = gpxpy . parse ( gpx_file ) References \u2691 Git Docs Quickstart Examples , more examples Use examples \u2691 Flask example Build Interactive GPS activity maps from GPX files Use Open Data to build interactive maps Search examples \u2691 Official docs Leafleft search control Leafleft search examples ( source code ) Searching in OSM data It seems that it doesn't yet support searching for multiple attributes in the geojson data","title":"Folium"},{"location":"coding/python/folium/#install","text":"Although you can install it with pip install folium their release pace is slow, therefore I recommend pulling it directly from master pip install git+https://github.com/python-visualization/folium.git@master It's a heavy repo, so it might take some time.","title":"Install"},{"location":"coding/python/folium/#usage","text":"Use the following snippet to create an empty map centered in Spain import folium m = folium . Map ( location = [ 40.0884 , - 3.68042 ], zoom_start = 7 , ) # Map configuration goes here m . save ( \"map.html\" ) From now on, those lines are going to be assumed, and all the snippets are supposed to go in between the definition and the save. If you need to center the map elsewhere, I suggest that you configure the map to show the coordinates of the mouse with from folium.plugins import MousePosition MousePosition () . add_to ( m )","title":"Usage"},{"location":"coding/python/folium/#change-tileset","text":"Folium Map object supports different tiles by default. It also supports any WMS or WMTS by passing a Leaflet-style URL to the tiles parameter: http://{s}.yourtiles.com/{z}/{x}/{y}.png . To use the IGN beautiful map as a fallback and the OpenStreetMaps as default use the following snippet: m = folium . Map ( location = [ 40.0884 , - 3.68042 ], zoom_start = 7 , tiles = None , ) folium . raster_layers . TileLayer ( name = \"OpenStreetMaps\" , tiles = \"OpenStreetMap\" , attr = \"OpenStreetMaps\" , ) . add_to ( m ) folium . raster_layers . TileLayer ( name = \"IGN\" , tiles = \"https://www.ign.es/wmts/mapa-raster?request=getTile&layer=MTN&TileMatrixSet=GoogleMapsCompatible&TileMatrix= {z} &TileCol= {x} &TileRow= {y} &format=image/jpeg\" , attr = \"IGN\" , ) . add_to ( m ) folium . LayerControl () . add_to ( m ) We need to set the tiles=None in the Map definition so both are shown in the LayerControl menu.","title":"Change tileset"},{"location":"coding/python/folium/#loading-the-data","text":"","title":"Loading the data"},{"location":"coding/python/folium/#using-geojson","text":"folium . GeoJson ( \"my_map.geojson\" , name = \"geojson\" ) . add_to ( m ) If you don't want the data to be embedded in the html use embed=False , this can be handy if you don't want to redeploy the web application on each change of data. You'll need to supply a valid url to the data file. The downside (as of today) of using geojson is that you can't have different markers for the data . The solution is to load it and iterate over the elements. See the issue for more information.","title":"Using geojson"},{"location":"coding/python/folium/#using-gpx","text":"Another popular data format is gpx, which is the one that OsmAnd uses. To import it we'll use the gpxpy library. import gpxpy import gpxpy.gpx gpx_file = open ( 'map.gpx' , 'r' ) gpx = gpxpy . parse ( gpx_file )","title":"Using gpx"},{"location":"coding/python/folium/#references","text":"Git Docs Quickstart Examples , more examples","title":"References"},{"location":"coding/python/folium/#use-examples","text":"Flask example Build Interactive GPS activity maps from GPX files Use Open Data to build interactive maps","title":"Use examples"},{"location":"coding/python/folium/#search-examples","text":"Official docs Leafleft search control Leafleft search examples ( source code ) Searching in OSM data It seems that it doesn't yet support searching for multiple attributes in the geojson data","title":"Search examples"},{"location":"coding/python/gitpython/","text":"GitPython is a python library used to interact with git repositories, high-level like git-porcelain, or low-level like git-plumbing. It provides abstractions of git objects for easy access of repository data, and additionally allows you to access the git repository more directly using either a pure python implementation, or the faster, but more resource intensive git command implementation. The object database implementation is optimized for handling large quantities of objects and large datasets, which is achieved by using low-level structures and data streaming. Installation \u2691 pip install GitPython Usage \u2691 Initialize a repository \u2691 from git import Repo repo = Repo . init ( 'path/to/initialize' ) If you want to get the working directory of the Repo object use the working_dir attribute. Load a repository \u2691 from git import Repo repo = Repo ( 'existing/git/repo/path' ) Make a commit \u2691 Given a repo object: index = repo . index # add the changes index . add ([ 'README.md' ]) from git import Actor author = Actor ( \"An author\" , \"author@example.com\" ) committer = Actor ( \"A committer\" , \"committer@example.com\" ) # commit by commit message and author and committer index . commit ( \"my commit message\" , author = author , committer = committer ) Change commit date \u2691 When building fake data , creating commits in other points in time is useful. import datetime from dateutil import tz commit_date = datetime . datetime ( 2020 , 2 , 2 , tzinfo = tz . tzlocal ()), index . commit ( \"my commit message\" , author = author , committer = committer , author_date = commit_date , commit_date = commit_date , ) Inspect the history \u2691 You first need to select the branch you want to inspect. To use the default repository branch use head.reference . repo . head . reference Then you can either get all the Commit objects of that reference, or inspect the log. Get all commits of a branch \u2691 [ commit for commit in repo . iter_commits ( rev = repo . head . reference )] It gives you a List of commits where the first element is the last commit in time. Inspect the log \u2691 Inspect it with the repo.head.reference.log() , which contains a list of RefLogEntry objects that have the interesting attributes: actor : Actor object of the author of the commit time : The commit timestamp, to load it as a datetime object use the datetime.datetime.fromtimestamp method message : Message as a string. Testing \u2691 There is no testing functionality, you need to either Mock, build fake data or fake adapters. Build fake data \u2691 Create a test_data directory in your testing directory with the contents of the git repository you want to test. Don't initialize it, we'll create a repo fixture that does it. Assuming that the data is in tests/assets/test_data : File: tests/conftest.py import shutil import pytest from git import Repo from py._path.local import LocalPath @pytest . fixture ( name = \"repo\" ) def repo_ ( tmpdir : LocalPath ) -> Repo : \"\"\"Create a git repository with fake data and history. Args: tmpdir: Pytest fixture that creates a temporal directory \"\"\" # Copy the content from `tests/assets/test_data`. repo_path = tmpdir / \"test_data\" shutil . copytree ( \"tests/assets/test_data\" , repo_path ) # Initializes the git repository. return Repo . init ( repo_path ) On each test you can add the commits that you need for your use case. author = Actor ( \"An author\" , \"author@example.com\" ) committer = Actor ( \"A committer\" , \"committer@example.com\" ) @pytest . mark . freeze_time ( \"2021-02-01T12:00:00\" ) def test_repo_is_not_empty ( repo : Repo ) -> None : commit_date = datetime . datetime ( 2021 , 2 , 1 , tzinfo = tz . tzlocal ()) repo . index . add ([ \"mkdocs.yml\" ]) repo . index . commit ( \"Initial skeleton\" , author = author , committer = committer , author_date = commit_date , commit_date = commit_date , ) assert not repo . bare If you feel that the tests are too verbose, you can create a fixture with all the commits done, and select each case with the freezegun pytest fixture . In my opinion, it will make the tests less clear though. The fixture can look like: File: tests/conftest.py import datetime from dateutil import tz import shutil import textwrap import pytest from git import Actor , Repo from py._path.local import LocalPath @pytest . fixture ( name = \"full_repo\" ) def full_repo_ ( repo : Repo ) -> Repo : \"\"\"Create a git repository with fake data and history. Args: repo: an initialized Repo \"\"\" index = repo . index author = Actor ( \"An author\" , \"author@example.com\" ) committer = Actor ( \"A committer\" , \"committer@example.com\" ) # Add a commit in time commit_date = datetime . datetime ( 2021 , 2 , 1 , tzinfo = tz . tzlocal ()) index . add ([ \"mkdocs.yml\" ]) index . commit ( \"Initial skeleton\" , author = author , committer = committer , author_date = commit_date , commit_date = commit_date , ) Then you can use that fixture in any test: @pytest . mark . freeze_time ( \"2021-02-01T12:00:00\" ) def test_assert_true ( full_repo : Repo ) -> None : assert not repo . bare References \u2691 Docs Git Tutorial","title":"GitPython"},{"location":"coding/python/gitpython/#installation","text":"pip install GitPython","title":"Installation"},{"location":"coding/python/gitpython/#usage","text":"","title":"Usage"},{"location":"coding/python/gitpython/#initialize-a-repository","text":"from git import Repo repo = Repo . init ( 'path/to/initialize' ) If you want to get the working directory of the Repo object use the working_dir attribute.","title":"Initialize a repository"},{"location":"coding/python/gitpython/#load-a-repository","text":"from git import Repo repo = Repo ( 'existing/git/repo/path' )","title":"Load a repository"},{"location":"coding/python/gitpython/#make-a-commit","text":"Given a repo object: index = repo . index # add the changes index . add ([ 'README.md' ]) from git import Actor author = Actor ( \"An author\" , \"author@example.com\" ) committer = Actor ( \"A committer\" , \"committer@example.com\" ) # commit by commit message and author and committer index . commit ( \"my commit message\" , author = author , committer = committer )","title":"Make a commit"},{"location":"coding/python/gitpython/#change-commit-date","text":"When building fake data , creating commits in other points in time is useful. import datetime from dateutil import tz commit_date = datetime . datetime ( 2020 , 2 , 2 , tzinfo = tz . tzlocal ()), index . commit ( \"my commit message\" , author = author , committer = committer , author_date = commit_date , commit_date = commit_date , )","title":"Change commit date"},{"location":"coding/python/gitpython/#inspect-the-history","text":"You first need to select the branch you want to inspect. To use the default repository branch use head.reference . repo . head . reference Then you can either get all the Commit objects of that reference, or inspect the log.","title":"Inspect the history"},{"location":"coding/python/gitpython/#get-all-commits-of-a-branch","text":"[ commit for commit in repo . iter_commits ( rev = repo . head . reference )] It gives you a List of commits where the first element is the last commit in time.","title":"Get all commits of a branch"},{"location":"coding/python/gitpython/#inspect-the-log","text":"Inspect it with the repo.head.reference.log() , which contains a list of RefLogEntry objects that have the interesting attributes: actor : Actor object of the author of the commit time : The commit timestamp, to load it as a datetime object use the datetime.datetime.fromtimestamp method message : Message as a string.","title":"Inspect the log"},{"location":"coding/python/gitpython/#testing","text":"There is no testing functionality, you need to either Mock, build fake data or fake adapters.","title":"Testing"},{"location":"coding/python/gitpython/#build-fake-data","text":"Create a test_data directory in your testing directory with the contents of the git repository you want to test. Don't initialize it, we'll create a repo fixture that does it. Assuming that the data is in tests/assets/test_data : File: tests/conftest.py import shutil import pytest from git import Repo from py._path.local import LocalPath @pytest . fixture ( name = \"repo\" ) def repo_ ( tmpdir : LocalPath ) -> Repo : \"\"\"Create a git repository with fake data and history. Args: tmpdir: Pytest fixture that creates a temporal directory \"\"\" # Copy the content from `tests/assets/test_data`. repo_path = tmpdir / \"test_data\" shutil . copytree ( \"tests/assets/test_data\" , repo_path ) # Initializes the git repository. return Repo . init ( repo_path ) On each test you can add the commits that you need for your use case. author = Actor ( \"An author\" , \"author@example.com\" ) committer = Actor ( \"A committer\" , \"committer@example.com\" ) @pytest . mark . freeze_time ( \"2021-02-01T12:00:00\" ) def test_repo_is_not_empty ( repo : Repo ) -> None : commit_date = datetime . datetime ( 2021 , 2 , 1 , tzinfo = tz . tzlocal ()) repo . index . add ([ \"mkdocs.yml\" ]) repo . index . commit ( \"Initial skeleton\" , author = author , committer = committer , author_date = commit_date , commit_date = commit_date , ) assert not repo . bare If you feel that the tests are too verbose, you can create a fixture with all the commits done, and select each case with the freezegun pytest fixture . In my opinion, it will make the tests less clear though. The fixture can look like: File: tests/conftest.py import datetime from dateutil import tz import shutil import textwrap import pytest from git import Actor , Repo from py._path.local import LocalPath @pytest . fixture ( name = \"full_repo\" ) def full_repo_ ( repo : Repo ) -> Repo : \"\"\"Create a git repository with fake data and history. Args: repo: an initialized Repo \"\"\" index = repo . index author = Actor ( \"An author\" , \"author@example.com\" ) committer = Actor ( \"A committer\" , \"committer@example.com\" ) # Add a commit in time commit_date = datetime . datetime ( 2021 , 2 , 1 , tzinfo = tz . tzlocal ()) index . add ([ \"mkdocs.yml\" ]) index . commit ( \"Initial skeleton\" , author = author , committer = committer , author_date = commit_date , commit_date = commit_date , ) Then you can use that fixture in any test: @pytest . mark . freeze_time ( \"2021-02-01T12:00:00\" ) def test_assert_true ( full_repo : Repo ) -> None : assert not repo . bare","title":"Build fake data"},{"location":"coding/python/gitpython/#references","text":"Docs Git Tutorial","title":"References"},{"location":"coding/python/mkdocstrings/","text":"mkdocstrings is a library to automatically generate mkdocs pages from the code docstrings. Install \u2691 pip install mkdocstrings Activate the plugin by adding it to the plugin section in the mkdocs.yml configuration file: plugins : - mkdocstrings Usage \u2691 MkDocstrings works by processing special expressions in your Markdown files. The syntax is as follow: ::: identifier YAML block The identifier is a string identifying the object you want to document. The format of an identifier can vary from one handler to another. For example, the Python handler expects the full dotted-path to a Python object: my_package.my_module.MyClass.my_method . The YAML block is optional, and contains some configuration options: handler : the name of the handler to use to collect and render this object. selection : a dictionary of options passed to the handler's collector. The collector is responsible for collecting the documentation from the source code. Therefore, selection options change how the documentation is collected from the source code. rendering : a dictionary of options passed to the handler's renderer. The renderer is responsible for rendering the documentation with Jinja2 templates. Therefore, rendering options affect how the selected object's documentation is rendered. Example with the Python handler docs/my_page.md # Documentation for `MyClass` ::: my_package.my_module.MyClass handler: python selection: members: - method_a - method_b rendering: show_root_heading: false show_source: false mkdocs.yml nav : - \"My page\" : my_page.md src/my_package/my_module.py class MyClass : \"\"\"Print print print!\"\"\" def method_a ( self ): \"\"\"Print A!\"\"\" print ( \"A!\" ) def method_b ( self ): \"\"\"Print B!\"\"\" print ( \"B!\" ) def method_c ( self ): \"\"\"Print C!\"\"\" print ( \"C!\" ) Result Documentation for MyClass Print print print! method_a ( self ) Print A! method_b ( self ) Print B! Reference the objects in the documentation \u2691 With a custom title: [ `Object 1` ][full.path.object1] With the identifier as title: [ full.path.object2 ][] Global options \u2691 MkDocstrings accept a few top-level configuration options in mkdocs.yml : watch : a list of directories to watch while serving the documentation. So if any file is changed in those directories, the documentation is rebuilt. default_handler : the handler that is used by default when no handler is specified. custom_templates : the path to a directory containing custom templates. The path is relative to the docs directory. See Customization . handlers : the handlers global configuration. Example: plugins : - mkdocstrings : default_handler : python handlers : python : rendering : show_source : false custom_templates : templates watch : - src/my_package The handlers global configuration can then be overridden by local configurations: :: : my_package.my_module.MyClass rendering : show_source : true Check the Python handler options for more details. References \u2691 Docs","title":"mkdocstrings"},{"location":"coding/python/mkdocstrings/#install","text":"pip install mkdocstrings Activate the plugin by adding it to the plugin section in the mkdocs.yml configuration file: plugins : - mkdocstrings","title":"Install"},{"location":"coding/python/mkdocstrings/#usage","text":"MkDocstrings works by processing special expressions in your Markdown files. The syntax is as follow: ::: identifier YAML block The identifier is a string identifying the object you want to document. The format of an identifier can vary from one handler to another. For example, the Python handler expects the full dotted-path to a Python object: my_package.my_module.MyClass.my_method . The YAML block is optional, and contains some configuration options: handler : the name of the handler to use to collect and render this object. selection : a dictionary of options passed to the handler's collector. The collector is responsible for collecting the documentation from the source code. Therefore, selection options change how the documentation is collected from the source code. rendering : a dictionary of options passed to the handler's renderer. The renderer is responsible for rendering the documentation with Jinja2 templates. Therefore, rendering options affect how the selected object's documentation is rendered. Example with the Python handler docs/my_page.md # Documentation for `MyClass` ::: my_package.my_module.MyClass handler: python selection: members: - method_a - method_b rendering: show_root_heading: false show_source: false mkdocs.yml nav : - \"My page\" : my_page.md src/my_package/my_module.py class MyClass : \"\"\"Print print print!\"\"\" def method_a ( self ): \"\"\"Print A!\"\"\" print ( \"A!\" ) def method_b ( self ): \"\"\"Print B!\"\"\" print ( \"B!\" ) def method_c ( self ): \"\"\"Print C!\"\"\" print ( \"C!\" ) Result","title":"Usage"},{"location":"coding/python/mkdocstrings/#reference-the-objects-in-the-documentation","text":"With a custom title: [ `Object 1` ][full.path.object1] With the identifier as title: [ full.path.object2 ][]","title":"Reference the objects in the documentation"},{"location":"coding/python/mkdocstrings/#global-options","text":"MkDocstrings accept a few top-level configuration options in mkdocs.yml : watch : a list of directories to watch while serving the documentation. So if any file is changed in those directories, the documentation is rebuilt. default_handler : the handler that is used by default when no handler is specified. custom_templates : the path to a directory containing custom templates. The path is relative to the docs directory. See Customization . handlers : the handlers global configuration. Example: plugins : - mkdocstrings : default_handler : python handlers : python : rendering : show_source : false custom_templates : templates watch : - src/my_package The handlers global configuration can then be overridden by local configurations: :: : my_package.my_module.MyClass rendering : show_source : true Check the Python handler options for more details.","title":"Global options"},{"location":"coding/python/mkdocstrings/#references","text":"Docs","title":"References"},{"location":"coding/python/pandas/","text":"Pandas is a fast, powerful, flexible and easy to use open source data analysis and manipulation tool, built on top of the Python programming language. Install \u2691 pip3 install pandas Import import pandas as pd Snippets \u2691 Load csv \u2691 data = pd . read_csv ( \"filename.csv\" ) If you want to parse the dates of the start column give read_csv the argument parse_dates=['start'] . Do operation on column data and save it in other column \u2691 # make a simple dataframe df = pd . DataFrame ({ 'a' :[ 1 , 2 ], 'b' :[ 3 , 4 ]}) df # a b # 0 1 3 # 1 2 4 # create an unattached column with an index df . apply ( lambda row : row . a + row . b , axis = 1 ) # 0 4 # 1 6 # do same but attach it to the dataframe df [ 'c' ] = df . apply ( lambda row : row . a + row . b , axis = 1 ) df # a b c # 0 1 3 4 # 1 2 4 6 Get unique values of column \u2691 If we want to get the unique values of the name column: df . name . unique () Extract columns of dataframe \u2691 df1 = df [[ 'a' , 'b' ]] Remove dumplicate rows \u2691 df = df . drop_duplicates () Remove column from dataframe \u2691 del df [ 'name' ] Count unique combinations of values in selected columns \u2691 df1 . groupby ([ 'A' , 'B' ]) . size () . reset_index () . rename ( columns = { 0 : 'count' }) A B count 0 no no 1 1 no yes 2 2 yes no 4 3 yes yes 3 Get row that contains the maximum value of a column \u2691 df . loc [ df [ 'Value' ] . idxmax ()] References \u2691 Homepage","title":"Pandas"},{"location":"coding/python/pandas/#install","text":"pip3 install pandas Import import pandas as pd","title":"Install"},{"location":"coding/python/pandas/#snippets","text":"","title":"Snippets"},{"location":"coding/python/pandas/#load-csv","text":"data = pd . read_csv ( \"filename.csv\" ) If you want to parse the dates of the start column give read_csv the argument parse_dates=['start'] .","title":"Load csv"},{"location":"coding/python/pandas/#do-operation-on-column-data-and-save-it-in-other-column","text":"# make a simple dataframe df = pd . DataFrame ({ 'a' :[ 1 , 2 ], 'b' :[ 3 , 4 ]}) df # a b # 0 1 3 # 1 2 4 # create an unattached column with an index df . apply ( lambda row : row . a + row . b , axis = 1 ) # 0 4 # 1 6 # do same but attach it to the dataframe df [ 'c' ] = df . apply ( lambda row : row . a + row . b , axis = 1 ) df # a b c # 0 1 3 4 # 1 2 4 6","title":"Do operation on column data and save it in other column"},{"location":"coding/python/pandas/#get-unique-values-of-column","text":"If we want to get the unique values of the name column: df . name . unique ()","title":"Get unique values of column"},{"location":"coding/python/pandas/#extract-columns-of-dataframe","text":"df1 = df [[ 'a' , 'b' ]]","title":"Extract columns of dataframe"},{"location":"coding/python/pandas/#remove-dumplicate-rows","text":"df = df . drop_duplicates ()","title":"Remove dumplicate rows"},{"location":"coding/python/pandas/#remove-column-from-dataframe","text":"del df [ 'name' ]","title":"Remove column from dataframe"},{"location":"coding/python/pandas/#count-unique-combinations-of-values-in-selected-columns","text":"df1 . groupby ([ 'A' , 'B' ]) . size () . reset_index () . rename ( columns = { 0 : 'count' }) A B count 0 no no 1 1 no yes 2 2 yes no 4 3 yes yes 3","title":"Count unique combinations of values in selected columns"},{"location":"coding/python/pandas/#get-row-that-contains-the-maximum-value-of-a-column","text":"df . loc [ df [ 'Value' ] . idxmax ()]","title":"Get row that contains the maximum value of a column"},{"location":"coding/python/pandas/#references","text":"Homepage","title":"References"},{"location":"coding/python/passpy/","text":"passpy a platform independent library and cli that is compatible with ZX2C4's pass . Installation \u2691 pip install passpy Usage \u2691 To use passpy in your Python project, we will first have to create a new passpy.store.Store object. import passpy store = passpy . Store () If git or gpg2 are not in your PATH you will have to specify them via git_bin and gpg_bin when creating the store object. You can also create the store on a different folder, be passing store_dir along. To initialize the password store at store_dir , if it isn't already, use store . init_store ( 'store gpg id' ) Where store gpg id is the name of a GPG ID. Optionally, git can be initialized in very much the same way. store . init_git () You are now ready to interact with the password store. You can set and get keys using passpy.store.Store.set_key and passpy.store.Store.get_key . passpy.store.Store.gen_key generates a new password for a new or existing key. To delete a key or directory, use passpy.store.Store.remove_path . For a full overview over all available methods see store-module-label . References \u2691 Docs Git","title":"Passpy"},{"location":"coding/python/passpy/#installation","text":"pip install passpy","title":"Installation"},{"location":"coding/python/passpy/#usage","text":"To use passpy in your Python project, we will first have to create a new passpy.store.Store object. import passpy store = passpy . Store () If git or gpg2 are not in your PATH you will have to specify them via git_bin and gpg_bin when creating the store object. You can also create the store on a different folder, be passing store_dir along. To initialize the password store at store_dir , if it isn't already, use store . init_store ( 'store gpg id' ) Where store gpg id is the name of a GPG ID. Optionally, git can be initialized in very much the same way. store . init_git () You are now ready to interact with the password store. You can set and get keys using passpy.store.Store.set_key and passpy.store.Store.get_key . passpy.store.Store.gen_key generates a new password for a new or existing key. To delete a key or directory, use passpy.store.Store.remove_path . For a full overview over all available methods see store-module-label .","title":"Usage"},{"location":"coding/python/passpy/#references","text":"Docs Git","title":"References"},{"location":"coding/python/plotly/","text":"Plotly is a Python graphing library that makes interactive, publication-quality graphs. Install \u2691 pip3 install plotly Import import plotly.graph_objects as go Snippets \u2691 Select graph source using dropdown \u2691 # imports import plotly.graph_objects as go import numpy as np # data x = list ( np . linspace ( - np . pi , np . pi , 100 )) values_1 = list ( np . sin ( x )) values_1b = [ elem *- 1 for elem in values_1 ] values_2 = list ( np . tan ( x )) values_2b = [ elem *- 1 for elem in values_2 ] # plotly setup] fig = go . Figure () # Add one ore more traces fig . add_traces ( go . Scatter ( x = x , y = values_1 )) fig . add_traces ( go . Scatter ( x = x , y = values_1b )) # construct menus updatemenus = [{ 'buttons' : [{ 'method' : 'update' , 'label' : 'Val 1' , 'args' : [{ 'y' : [ values_1 , values_1b ]},] }, { 'method' : 'update' , 'label' : 'Val 2' , 'args' : [{ 'y' : [ values_2 , values_2b ]},]}], 'direction' : 'down' , 'showactive' : True ,}] # update layout with buttons, and show the figure fig . update_layout ( updatemenus = updatemenus ) fig . show () References \u2691 Homepage Git","title":"Plotly"},{"location":"coding/python/plotly/#install","text":"pip3 install plotly Import import plotly.graph_objects as go","title":"Install"},{"location":"coding/python/plotly/#snippets","text":"","title":"Snippets"},{"location":"coding/python/plotly/#select-graph-source-using-dropdown","text":"# imports import plotly.graph_objects as go import numpy as np # data x = list ( np . linspace ( - np . pi , np . pi , 100 )) values_1 = list ( np . sin ( x )) values_1b = [ elem *- 1 for elem in values_1 ] values_2 = list ( np . tan ( x )) values_2b = [ elem *- 1 for elem in values_2 ] # plotly setup] fig = go . Figure () # Add one ore more traces fig . add_traces ( go . Scatter ( x = x , y = values_1 )) fig . add_traces ( go . Scatter ( x = x , y = values_1b )) # construct menus updatemenus = [{ 'buttons' : [{ 'method' : 'update' , 'label' : 'Val 1' , 'args' : [{ 'y' : [ values_1 , values_1b ]},] }, { 'method' : 'update' , 'label' : 'Val 2' , 'args' : [{ 'y' : [ values_2 , values_2b ]},]}], 'direction' : 'down' , 'showactive' : True ,}] # update layout with buttons, and show the figure fig . update_layout ( updatemenus = updatemenus ) fig . show ()","title":"Select graph source using dropdown"},{"location":"coding/python/plotly/#references","text":"Homepage Git","title":"References"},{"location":"coding/python/prompt_toolkit/","text":"Python Prompt Toolkit is a library for building powerful interactive command line and terminal applications in Python. Installation \u2691 pip install prompt_toolkit Usage \u2691 A simple prompt \u2691 The following snippet is the most simple example, it uses the prompt() function to asks the user for input and returns the text. Just like (raw_)input . from prompt_toolkit import prompt text = prompt ( 'Give me some input: ' ) print ( 'You said: %s ' % text ) It can be used to build REPL applications or full screen ones .","title":"Prompt Toolkit"},{"location":"coding/python/prompt_toolkit/#installation","text":"pip install prompt_toolkit","title":"Installation"},{"location":"coding/python/prompt_toolkit/#usage","text":"","title":"Usage"},{"location":"coding/python/prompt_toolkit/#a-simple-prompt","text":"The following snippet is the most simple example, it uses the prompt() function to asks the user for input and returns the text. Just like (raw_)input . from prompt_toolkit import prompt text = prompt ( 'Give me some input: ' ) print ( 'You said: %s ' % text ) It can be used to build REPL applications or full screen ones .","title":"A simple prompt"},{"location":"coding/python/pydantic/","text":"Pydantic is a data validation and settings management using python type annotations. pydantic enforces type hints at runtime, and provides user friendly errors when data is invalid. Define how data should be in pure, canonical python; check it with pydantic. Install \u2691 pip install pydantic Advantages and disadvantages \u2691 Advantages: Perform data validation in an easy and nice way. Seamless integration with FastAPI and Typer . Nice way to export the data and data schema. Disadvantages: You can't define cyclic relationships , therefore there is no way to simulate the backref SQLAlchemy function. Models \u2691 The primary means of defining objects in pydantic is via models (models are simply classes which inherit from BaseModel ). You can think of models as similar to types in strictly typed languages, or as the requirements of a single endpoint in an API. Untrusted data can be passed to a model, and after parsing and validation pydantic guarantees that the fields of the resultant model instance will conform to the field types defined on the model. Basic model usage \u2691 from pydantic import BaseModel class User ( BaseModel ): id : int name = 'Jane Doe' User here is a model with two fields id which is an integer and is required, and name which is a string and is not required (it has a default value). The type of name is inferred from the default value, and so a type annotation is not required. user = User ( id = '123' ) user here is an instance of User . Initialisation of the object will perform all parsing and validation, if no ValidationError is raised, you know the resulting model instance is valid. Model properties \u2691 Models possess the following methods and attributes: dict() returns a dictionary of the model's fields and values. json() returns a JSON string representation dict() . copy() returns a deep copy of the model. parse_obj() very similar to the __init__ method of the model, used to import objects from a dict rather than keyword arguments. If the object passed is not a dict a ValidationError will be raised. parse_raw() takes a str or bytes and parses it as json , then passes the result to parse_obj . parse_file() reads a file and passes the contents to parse_raw . If content_type is omitted, it is inferred from the file's extension. from_orm() loads data into a model from an arbitrary class. schema() returns a dictionary representing the model as JSON Schema. schema_json() returns a JSON string representation of schema() . Recursive Models \u2691 More complex hierarchical data structures can be defined using models themselves as types in annotations. from typing import List from pydantic import BaseModel class Foo ( BaseModel ): count : int size : float = None class Bar ( BaseModel ): apple = 'x' banana = 'y' class Spam ( BaseModel ): foo : Foo bars : List [ Bar ] m = Spam ( foo = { 'count' : 4 }, bars = [{ 'apple' : 'x1' }, { 'apple' : 'x2' }]) print ( m ) #> foo=Foo(count=4, size=None) bars=[Bar(apple='x1', banana='y'), #> Bar(apple='x2', banana='y')] print ( m . dict ()) \"\"\" { 'foo': {'count': 4, 'size': None}, 'bars': [ {'apple': 'x1', 'banana': 'y'}, {'apple': 'x2', 'banana': 'y'}, ], } \"\"\" For self-referencing models, use postponed annotations. Definition of two models that reference each other \u2691 class A ( BaseModel ): b : Optional [ \"B\" ] = None class B ( BaseModel ): a : Optional [ A ] = None A . update_forward_refs () Although it doesn't work as expected! Error Handling \u2691 pydantic will raise ValidationError whenever it finds an error in the data it's validating. Note Validation code should not raise ValidationError itself, but rather raise ValueError , TypeError or AssertionError (or subclasses of ValueError or TypeError ) which will be caught and used to populate ValidationError . One exception will be raised regardless of the number of errors found, that ValidationError will contain information about all the errors and how they happened. You can access these errors in a several ways: e.errors() method will return list of errors found in the input data. e.json() method will return a JSON representation of errors . str(e) method will return a human readable representation of the errors. Each error object contains: loc the error's location as a list. The first item in the list will be the field where the error occurred, and if the field is a sub-model, subsequent items will be present to indicate the nested location of the error. type a computer-readable identifier of the error type. msg a human readable explanation of the error. ctx an optional object which contains values required to render the error message. Custom Errors \u2691 You can also define your own error classes, which can specify a custom error code, message template, and context: from pydantic import BaseModel , PydanticValueError , ValidationError , validator class NotABarError ( PydanticValueError ): code = 'not_a_bar' msg_template = 'value is not \"bar\", got \" {wrong_value} \"' class Model ( BaseModel ): foo : str @validator ( 'foo' ) def name_must_contain_space ( cls , v ): if v != 'bar' : raise NotABarError ( wrong_value = v ) return v try : Model ( foo = 'ber' ) except ValidationError as e : print ( e . json ()) \"\"\" [ { \"loc\": [ \"foo\" ], \"msg\": \"value is not \\\"bar\\\", got \\\"ber\\\"\", \"type\": \"value_error.not_a_bar\", \"ctx\": { \"wrong_value\": \"ber\" } } ] \"\"\" Dynamic model creation \u2691 There are some occasions where the shape of a model is not known until runtime. For this pydantic provides the create_model method to allow models to be created on the fly. from pydantic import BaseModel , create_model DynamicFoobarModel = create_model ( 'DynamicFoobarModel' , foo = ( str , ... ), bar = 123 ) class StaticFoobarModel ( BaseModel ): foo : str bar : int = 123 Here StaticFoobarModel and DynamicFoobarModel are identical. Warning See the note in Required Optional Fields for the distinct between an ellipsis as a field default and annotation only fields. See samuelcolvin/pydantic#1047 for more details. Fields are defined by either a tuple of the form (<type>, <default value>) or just a default value. The special key word arguments __config__ and __base__ can be used to customize the new model. This includes extending a base model with extra fields. from pydantic import BaseModel , create_model class FooModel ( BaseModel ): foo : str bar : int = 123 BarModel = create_model ( 'BarModel' , apple = 'russet' , banana = 'yellow' , __base__ = FooModel , ) print ( BarModel ) #> <class 'BarModel'> print ( BarModel . __fields__ . keys ()) #> dict_keys(['foo', 'bar', 'apple', 'banana']) Abstract Base Classes \u2691 Pydantic models can be used alongside Python's Abstract Base Classes (ABCs). import abc from pydantic import BaseModel class FooBarModel ( BaseModel , abc . ABC ): a : str b : int @abc . abstractmethod def my_abstract_method ( self ): pass Field Ordering \u2691 Field order is important in models for the following reasons: Validation is performed in the order fields are defined; fields validators can access the values of earlier fields, but not later ones Field order is preserved in the model schema Field order is preserved in validation errors Field order is preserved by .dict() and .json() etc. As of v1.0 all fields with annotations (whether annotation-only or with a default value) will precede all fields without an annotation. Within their respective groups, fields remain in the order they were defined. Field with dynamic default value \u2691 When declaring a field with a default value, you may want it to be dynamic (i.e. different for each model). To do this, you may want to use a default_factory . In Beta The default_factory argument is in beta , it has been added to pydantic in v1.5 on a provisional basis . It may change significantly in future releases and its signature or behaviour will not be concrete until v2 . Feedback from the community while it's still provisional would be extremely useful; either comment on #866 or create a new issue. Example of usage: from datetime import datetime from uuid import UUID , uuid4 from pydantic import BaseModel , Field class Model ( BaseModel ): uid : UUID = Field ( default_factory = uuid4 ) updated : datetime = Field ( default_factory = datetime . utcnow ) m1 = Model () m2 = Model () print ( f ' { m1 . uid } != { m2 . uid } ' ) #> 3b187763-a19c-4ed8-9588-387e224e04f1 != 0c58f97b-c8a7-4fe8-8550-e9b2b8026574 print ( f ' { m1 . updated } != { m2 . updated } ' ) #> 2020-07-15 20:01:48.451066 != 2020-07-15 20:01:48.451083 Warning The default_factory expects the field type to be set. Moreover if you want to validate default values with validate_all , pydantic will need to call the default_factory , which could lead to side effects! Parsing data into a specified type \u2691 Pydantic includes a standalone utility function parse_obj_as that can be used to apply the parsing logic used to populate pydantic models in a more ad-hoc way. This function behaves similarly to BaseModel.parse_obj , but works with arbitrary pydantic-compatible types. This is especially useful when you want to parse results into a type that is not a direct subclass of BaseModel . For example: from typing import List from pydantic import BaseModel , parse_obj_as class Item ( BaseModel ): id : int name : str # `item_data` could come from an API call, eg., via something like: # item_data = requests.get('https://my-api.com/items').json() item_data = [{ 'id' : 1 , 'name' : 'My Item' }] items = parse_obj_as ( List [ Item ], item_data ) print ( items ) #> [Item(id=1, name='My Item')] This function is capable of parsing data into any of the types pydantic can handle as fields of a BaseModel . Pydantic also includes a similar standalone function called parse_file_as , which is analogous to BaseModel.parse_file . Data Conversion \u2691 pydantic may cast input data to force it to conform to model field types, and in some cases this may result in a loss of information. For example: from pydantic import BaseModel class Model ( BaseModel ): a : int b : float c : str print ( Model ( a = 3.1415 , b = ' 2.72 ' , c = 123 ) . dict ()) #> {'a': 3, 'b': 2.72, 'c': '123'} This is a deliberate decision of pydantic , and in general it's the most useful approach. See here for a longer discussion on the subject. Initialize attributes at object creation \u2691 If you want to initialize attributes of the object automatically at object creation, similar of what you'd do with the __init__ method of the class, you need to use root_validators . from pydantic import root_validator class PypikaRepository ( BaseModel ): \"\"\"Implement the repository pattern using the Pypika query builder.\"\"\" connection : sqlite3 . Connection cursor : sqlite3 . Cursor class Config : \"\"\"Configure the pydantic model.\"\"\" arbitrary_types_allowed = True @root_validator ( pre = True ) @classmethod def set_connection ( cls , values : Dict [ str , Any ]) -> Dict [ str , Any ]: \"\"\"Set the connection to the database. Raises: ConnectionError: If there is no database file. \"\"\" database_file = values [ \"database_url\" ] . replace ( \"sqlite:///\" , \"\" ) if not os . path . isfile ( database_file ): raise ConnectionError ( f \"There is no database file: { database_file } \" ) connection = sqlite3 . connect ( database_file ) values [ \"connection\" ] = connection values [ \"cursor\" ] = connection . cursor () return values I had to set the arbitrary_types_allowed because the sqlite3 objects are not between the pydantic object types. Set private attributes \u2691 If you want to define some attributes that are not part of the model use PrivateAttr : from datetime import datetime from random import randint from pydantic import BaseModel , PrivateAttr class TimeAwareModel ( BaseModel ): _processed_at : datetime = PrivateAttr ( default_factory = datetime . now ) _secret_value : str = PrivateAttr () def __init__ ( self , ** data : Any ) -> None : super () . __init__ ( ** data ) # this could also be done with default_factory self . _secret_value = randint ( 1 , 5 ) m = TimeAwareModel () print ( m . _processed_at ) #> 2021-03-03 17:30:04.030758 print ( m . _secret_value ) #> 5 Update entity attributes with a dictionary \u2691 To update a model with the data of a dictionary you can create a new object with the new data using the update argument of the copy method. class FooBarModel ( BaseModel ): banana : float foo : str m = FooBarModel ( banana = 3.14 , foo = 'hello' ) m . copy ( update = { 'banana' : 0 }) Lazy loading attributes \u2691 Currently there is no official support for lazy loading model attributes. You can define your own properties but when you export the schema they won't appear there. dgasmith has a workaround though. Troubleshooting \u2691 copy produces copy that modifies the original \u2691 When copying a model, changing the value of an attribute on the copy updates the value of the attribute on the original. This only happens if deep != True . To fix it use: model.copy(deep=True) . E0611: No name 'BaseModel' in module 'pydantic' \u2691 Add to your pyproject.toml the following lines: # --------- Pylint ------------- [tool.pylint.'MESSAGES CONTROL'] extension-pkg-whitelist = \"pydantic\" Or if it fails, add to the line # pylint: extension-pkg-whitelist . References \u2691 Docs","title":"Pydantic"},{"location":"coding/python/pydantic/#install","text":"pip install pydantic","title":"Install"},{"location":"coding/python/pydantic/#advantages-and-disadvantages","text":"Advantages: Perform data validation in an easy and nice way. Seamless integration with FastAPI and Typer . Nice way to export the data and data schema. Disadvantages: You can't define cyclic relationships , therefore there is no way to simulate the backref SQLAlchemy function.","title":"Advantages and disadvantages"},{"location":"coding/python/pydantic/#models","text":"The primary means of defining objects in pydantic is via models (models are simply classes which inherit from BaseModel ). You can think of models as similar to types in strictly typed languages, or as the requirements of a single endpoint in an API. Untrusted data can be passed to a model, and after parsing and validation pydantic guarantees that the fields of the resultant model instance will conform to the field types defined on the model.","title":"Models"},{"location":"coding/python/pydantic/#basic-model-usage","text":"from pydantic import BaseModel class User ( BaseModel ): id : int name = 'Jane Doe' User here is a model with two fields id which is an integer and is required, and name which is a string and is not required (it has a default value). The type of name is inferred from the default value, and so a type annotation is not required. user = User ( id = '123' ) user here is an instance of User . Initialisation of the object will perform all parsing and validation, if no ValidationError is raised, you know the resulting model instance is valid.","title":"Basic model usage"},{"location":"coding/python/pydantic/#model-properties","text":"Models possess the following methods and attributes: dict() returns a dictionary of the model's fields and values. json() returns a JSON string representation dict() . copy() returns a deep copy of the model. parse_obj() very similar to the __init__ method of the model, used to import objects from a dict rather than keyword arguments. If the object passed is not a dict a ValidationError will be raised. parse_raw() takes a str or bytes and parses it as json , then passes the result to parse_obj . parse_file() reads a file and passes the contents to parse_raw . If content_type is omitted, it is inferred from the file's extension. from_orm() loads data into a model from an arbitrary class. schema() returns a dictionary representing the model as JSON Schema. schema_json() returns a JSON string representation of schema() .","title":"Model properties"},{"location":"coding/python/pydantic/#recursive-models","text":"More complex hierarchical data structures can be defined using models themselves as types in annotations. from typing import List from pydantic import BaseModel class Foo ( BaseModel ): count : int size : float = None class Bar ( BaseModel ): apple = 'x' banana = 'y' class Spam ( BaseModel ): foo : Foo bars : List [ Bar ] m = Spam ( foo = { 'count' : 4 }, bars = [{ 'apple' : 'x1' }, { 'apple' : 'x2' }]) print ( m ) #> foo=Foo(count=4, size=None) bars=[Bar(apple='x1', banana='y'), #> Bar(apple='x2', banana='y')] print ( m . dict ()) \"\"\" { 'foo': {'count': 4, 'size': None}, 'bars': [ {'apple': 'x1', 'banana': 'y'}, {'apple': 'x2', 'banana': 'y'}, ], } \"\"\" For self-referencing models, use postponed annotations.","title":"Recursive Models"},{"location":"coding/python/pydantic/#definition-of-two-models-that-reference-each-other","text":"class A ( BaseModel ): b : Optional [ \"B\" ] = None class B ( BaseModel ): a : Optional [ A ] = None A . update_forward_refs () Although it doesn't work as expected!","title":"Definition of two models that reference each other"},{"location":"coding/python/pydantic/#error-handling","text":"pydantic will raise ValidationError whenever it finds an error in the data it's validating. Note Validation code should not raise ValidationError itself, but rather raise ValueError , TypeError or AssertionError (or subclasses of ValueError or TypeError ) which will be caught and used to populate ValidationError . One exception will be raised regardless of the number of errors found, that ValidationError will contain information about all the errors and how they happened. You can access these errors in a several ways: e.errors() method will return list of errors found in the input data. e.json() method will return a JSON representation of errors . str(e) method will return a human readable representation of the errors. Each error object contains: loc the error's location as a list. The first item in the list will be the field where the error occurred, and if the field is a sub-model, subsequent items will be present to indicate the nested location of the error. type a computer-readable identifier of the error type. msg a human readable explanation of the error. ctx an optional object which contains values required to render the error message.","title":"Error Handling"},{"location":"coding/python/pydantic/#custom-errors","text":"You can also define your own error classes, which can specify a custom error code, message template, and context: from pydantic import BaseModel , PydanticValueError , ValidationError , validator class NotABarError ( PydanticValueError ): code = 'not_a_bar' msg_template = 'value is not \"bar\", got \" {wrong_value} \"' class Model ( BaseModel ): foo : str @validator ( 'foo' ) def name_must_contain_space ( cls , v ): if v != 'bar' : raise NotABarError ( wrong_value = v ) return v try : Model ( foo = 'ber' ) except ValidationError as e : print ( e . json ()) \"\"\" [ { \"loc\": [ \"foo\" ], \"msg\": \"value is not \\\"bar\\\", got \\\"ber\\\"\", \"type\": \"value_error.not_a_bar\", \"ctx\": { \"wrong_value\": \"ber\" } } ] \"\"\"","title":"Custom Errors"},{"location":"coding/python/pydantic/#dynamic-model-creation","text":"There are some occasions where the shape of a model is not known until runtime. For this pydantic provides the create_model method to allow models to be created on the fly. from pydantic import BaseModel , create_model DynamicFoobarModel = create_model ( 'DynamicFoobarModel' , foo = ( str , ... ), bar = 123 ) class StaticFoobarModel ( BaseModel ): foo : str bar : int = 123 Here StaticFoobarModel and DynamicFoobarModel are identical. Warning See the note in Required Optional Fields for the distinct between an ellipsis as a field default and annotation only fields. See samuelcolvin/pydantic#1047 for more details. Fields are defined by either a tuple of the form (<type>, <default value>) or just a default value. The special key word arguments __config__ and __base__ can be used to customize the new model. This includes extending a base model with extra fields. from pydantic import BaseModel , create_model class FooModel ( BaseModel ): foo : str bar : int = 123 BarModel = create_model ( 'BarModel' , apple = 'russet' , banana = 'yellow' , __base__ = FooModel , ) print ( BarModel ) #> <class 'BarModel'> print ( BarModel . __fields__ . keys ()) #> dict_keys(['foo', 'bar', 'apple', 'banana'])","title":"Dynamic model creation"},{"location":"coding/python/pydantic/#abstract-base-classes","text":"Pydantic models can be used alongside Python's Abstract Base Classes (ABCs). import abc from pydantic import BaseModel class FooBarModel ( BaseModel , abc . ABC ): a : str b : int @abc . abstractmethod def my_abstract_method ( self ): pass","title":"Abstract Base Classes"},{"location":"coding/python/pydantic/#field-ordering","text":"Field order is important in models for the following reasons: Validation is performed in the order fields are defined; fields validators can access the values of earlier fields, but not later ones Field order is preserved in the model schema Field order is preserved in validation errors Field order is preserved by .dict() and .json() etc. As of v1.0 all fields with annotations (whether annotation-only or with a default value) will precede all fields without an annotation. Within their respective groups, fields remain in the order they were defined.","title":"Field Ordering"},{"location":"coding/python/pydantic/#field-with-dynamic-default-value","text":"When declaring a field with a default value, you may want it to be dynamic (i.e. different for each model). To do this, you may want to use a default_factory . In Beta The default_factory argument is in beta , it has been added to pydantic in v1.5 on a provisional basis . It may change significantly in future releases and its signature or behaviour will not be concrete until v2 . Feedback from the community while it's still provisional would be extremely useful; either comment on #866 or create a new issue. Example of usage: from datetime import datetime from uuid import UUID , uuid4 from pydantic import BaseModel , Field class Model ( BaseModel ): uid : UUID = Field ( default_factory = uuid4 ) updated : datetime = Field ( default_factory = datetime . utcnow ) m1 = Model () m2 = Model () print ( f ' { m1 . uid } != { m2 . uid } ' ) #> 3b187763-a19c-4ed8-9588-387e224e04f1 != 0c58f97b-c8a7-4fe8-8550-e9b2b8026574 print ( f ' { m1 . updated } != { m2 . updated } ' ) #> 2020-07-15 20:01:48.451066 != 2020-07-15 20:01:48.451083 Warning The default_factory expects the field type to be set. Moreover if you want to validate default values with validate_all , pydantic will need to call the default_factory , which could lead to side effects!","title":"Field with dynamic default value"},{"location":"coding/python/pydantic/#parsing-data-into-a-specified-type","text":"Pydantic includes a standalone utility function parse_obj_as that can be used to apply the parsing logic used to populate pydantic models in a more ad-hoc way. This function behaves similarly to BaseModel.parse_obj , but works with arbitrary pydantic-compatible types. This is especially useful when you want to parse results into a type that is not a direct subclass of BaseModel . For example: from typing import List from pydantic import BaseModel , parse_obj_as class Item ( BaseModel ): id : int name : str # `item_data` could come from an API call, eg., via something like: # item_data = requests.get('https://my-api.com/items').json() item_data = [{ 'id' : 1 , 'name' : 'My Item' }] items = parse_obj_as ( List [ Item ], item_data ) print ( items ) #> [Item(id=1, name='My Item')] This function is capable of parsing data into any of the types pydantic can handle as fields of a BaseModel . Pydantic also includes a similar standalone function called parse_file_as , which is analogous to BaseModel.parse_file .","title":"Parsing data into a specified type"},{"location":"coding/python/pydantic/#data-conversion","text":"pydantic may cast input data to force it to conform to model field types, and in some cases this may result in a loss of information. For example: from pydantic import BaseModel class Model ( BaseModel ): a : int b : float c : str print ( Model ( a = 3.1415 , b = ' 2.72 ' , c = 123 ) . dict ()) #> {'a': 3, 'b': 2.72, 'c': '123'} This is a deliberate decision of pydantic , and in general it's the most useful approach. See here for a longer discussion on the subject.","title":"Data Conversion"},{"location":"coding/python/pydantic/#initialize-attributes-at-object-creation","text":"If you want to initialize attributes of the object automatically at object creation, similar of what you'd do with the __init__ method of the class, you need to use root_validators . from pydantic import root_validator class PypikaRepository ( BaseModel ): \"\"\"Implement the repository pattern using the Pypika query builder.\"\"\" connection : sqlite3 . Connection cursor : sqlite3 . Cursor class Config : \"\"\"Configure the pydantic model.\"\"\" arbitrary_types_allowed = True @root_validator ( pre = True ) @classmethod def set_connection ( cls , values : Dict [ str , Any ]) -> Dict [ str , Any ]: \"\"\"Set the connection to the database. Raises: ConnectionError: If there is no database file. \"\"\" database_file = values [ \"database_url\" ] . replace ( \"sqlite:///\" , \"\" ) if not os . path . isfile ( database_file ): raise ConnectionError ( f \"There is no database file: { database_file } \" ) connection = sqlite3 . connect ( database_file ) values [ \"connection\" ] = connection values [ \"cursor\" ] = connection . cursor () return values I had to set the arbitrary_types_allowed because the sqlite3 objects are not between the pydantic object types.","title":"Initialize attributes at object creation"},{"location":"coding/python/pydantic/#set-private-attributes","text":"If you want to define some attributes that are not part of the model use PrivateAttr : from datetime import datetime from random import randint from pydantic import BaseModel , PrivateAttr class TimeAwareModel ( BaseModel ): _processed_at : datetime = PrivateAttr ( default_factory = datetime . now ) _secret_value : str = PrivateAttr () def __init__ ( self , ** data : Any ) -> None : super () . __init__ ( ** data ) # this could also be done with default_factory self . _secret_value = randint ( 1 , 5 ) m = TimeAwareModel () print ( m . _processed_at ) #> 2021-03-03 17:30:04.030758 print ( m . _secret_value ) #> 5","title":"Set private attributes"},{"location":"coding/python/pydantic/#update-entity-attributes-with-a-dictionary","text":"To update a model with the data of a dictionary you can create a new object with the new data using the update argument of the copy method. class FooBarModel ( BaseModel ): banana : float foo : str m = FooBarModel ( banana = 3.14 , foo = 'hello' ) m . copy ( update = { 'banana' : 0 })","title":"Update entity attributes with a dictionary"},{"location":"coding/python/pydantic/#lazy-loading-attributes","text":"Currently there is no official support for lazy loading model attributes. You can define your own properties but when you export the schema they won't appear there. dgasmith has a workaround though.","title":"Lazy loading attributes"},{"location":"coding/python/pydantic/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"coding/python/pydantic/#copy-produces-copy-that-modifies-the-original","text":"When copying a model, changing the value of an attribute on the copy updates the value of the attribute on the original. This only happens if deep != True . To fix it use: model.copy(deep=True) .","title":"copy produces copy that modifies the original"},{"location":"coding/python/pydantic/#e0611-no-name-basemodel-in-module-pydantic","text":"Add to your pyproject.toml the following lines: # --------- Pylint ------------- [tool.pylint.'MESSAGES CONTROL'] extension-pkg-whitelist = \"pydantic\" Or if it fails, add to the line # pylint: extension-pkg-whitelist .","title":"E0611: No name 'BaseModel' in module 'pydantic'"},{"location":"coding/python/pydantic/#references","text":"Docs","title":"References"},{"location":"coding/python/pydantic_exporting/","text":"As well as accessing model attributes directly via their names (e.g. model.foobar ), models can be converted and exported in a number of ways: model.dict(...) \u2691 This is the primary way of converting a model to a dictionary. Sub-models will be recursively converted to dictionaries. Arguments: include : Fields to include in the returned dictionary. exclude : Fields to exclude from the returned dictionary. by_alias : Whether field aliases should be used as keys in the returned dictionary; default False . exclude_unset : Whether fields which were not explicitly set when creating the model should be excluded from the returned dictionary; default False . exclude_defaults : Whether fields which are equal to their default values (whether set or otherwise) should be excluded from the returned dictionary; default False . exclude_none : Whether fields which are equal to None should be excluded from the returned dictionary; default False . Example: from pydantic import BaseModel class BarModel ( BaseModel ): whatever : int class FooBarModel ( BaseModel ): banana : float foo : str bar : BarModel m = FooBarModel ( banana = 3.14 , foo = 'hello' , bar = { 'whatever' : 123 }) # returns a dictionary: print ( m . dict ()) \"\"\" { 'banana': 3.14, 'foo': 'hello', 'bar': {'whatever': 123}, } \"\"\" print ( m . dict ( include = { 'foo' , 'bar' })) #> {'foo': 'hello', 'bar': {'whatever': 123}} print ( m . dict ( exclude = { 'foo' , 'bar' })) #> {'banana': 3.14} dict(model) and iteration \u2691 pydantic models can also be converted to dictionaries using dict(model) , and you can also iterate over a model's field using for field_name, value in model: . With this approach the raw field values are returned, so sub-models will not be converted to dictionaries. Example: from pydantic import BaseModel class BarModel ( BaseModel ): whatever : int class FooBarModel ( BaseModel ): banana : float foo : str bar : BarModel m = FooBarModel ( banana = 3.14 , foo = 'hello' , bar = { 'whatever' : 123 }) print ( dict ( m )) \"\"\" { 'banana': 3.14, 'foo': 'hello', 'bar': BarModel( whatever=123, ), } \"\"\" for name , value in m : print ( f ' { name } : { value } ' ) #> banana: 3.14 #> foo: hello #> bar: whatever=123 model.copy(...) \u2691 copy() allows models to be duplicated, which is particularly useful for immutable models. Arguments: include : Fields to include in the returned dictionary. exclude : Fields to exclude from the returned dictionary. update : A dictionary of values to change when creating the copied model deep : whether to make a deep copy of the new model; default False Example: from pydantic import BaseModel class BarModel ( BaseModel ): whatever : int class FooBarModel ( BaseModel ): banana : float foo : str bar : BarModel m = FooBarModel ( banana = 3.14 , foo = 'hello' , bar = { 'whatever' : 123 }) print ( m . copy ( include = { 'foo' , 'bar' })) #> foo='hello' bar=BarModel(whatever=123) print ( m . copy ( exclude = { 'foo' , 'bar' })) #> banana=3.14 print ( m . copy ( update = { 'banana' : 0 })) #> banana=0 foo='hello' bar=BarModel(whatever=123) print ( id ( m . bar ), id ( m . copy () . bar )) #> 139868119420992 139868119420992 # normal copy gives the same object reference for `bar` print ( id ( m . bar ), id ( m . copy ( deep = True ) . bar )) #> 139868119420992 139868119423296 # deep copy gives a new object reference for `bar` model.json(...) \u2691 The .json() method will serialise a model to JSON. Typically, .json() in turn calls .dict() and serialises its result. (For models with a custom root type, after calling .dict() , only the value for the __root__ key is serialised). Arguments: include : Fields to include in the returned dictionary. exclude : Fields to exclude from the returned dictionary. by_alias : Whether field aliases should be used as keys in the returned dictionary; default False . exclude_unset : Whether fields which were not set when creating the model and have their default values should be excluded from the returned dictionary; default False . exclude_defaults : Whether fields which are equal to their default values (whether set or otherwise) should be excluded from the returned dictionary; default False . exclude_none : Whether fields which are equal to None should be excluded from the returned dictionary; default False . encoder : A custom encoder function passed to the default argument of json.dumps() ; defaults to a custom encoder designed to take care of all common types. **dumps_kwargs : Any other keyword arguments are passed to json.dumps() , e.g. indent . pydantic can serialise many commonly used types to JSON (e.g. datetime , date or UUID ) which would normally fail with a simple json.dumps(foobar) . from datetime import datetime from pydantic import BaseModel class BarModel ( BaseModel ): whatever : int class FooBarModel ( BaseModel ): foo : datetime bar : BarModel m = FooBarModel ( foo = datetime ( 2032 , 6 , 1 , 12 , 13 , 14 ), bar = { 'whatever' : 123 }) print ( m . json ()) #> {\"foo\": \"2032-06-01T12:13:14\", \"bar\": {\"whatever\": 123}} Advanced include and exclude \u2691 The dict , json , and copy methods support include and exclude arguments which can either be sets or dictionaries. This allows nested selection of which fields to export: from pydantic import BaseModel , SecretStr class User ( BaseModel ): id : int username : str password : SecretStr class Transaction ( BaseModel ): id : str user : User value : int t = Transaction ( id = '1234567890' , user = User ( id = 42 , username = 'JohnDoe' , password = 'hashedpassword' ), value = 9876543210 , ) # using a set: print ( t . dict ( exclude = { 'user' , 'value' })) #> {'id': '1234567890'} # using a dict: print ( t . dict ( exclude = { 'user' : { 'username' , 'password' }, 'value' : ... })) #> {'id': '1234567890', 'user': {'id': 42}} print ( t . dict ( include = { 'id' : ... , 'user' : { 'id' }})) #> {'id': '1234567890', 'user': {'id': 42}} The ellipsis ( ... ) indicates that we want to exclude or include an entire key, just as if we included it in a set. Of course, the same can be done at any depth level. Special care must be taken when including or excluding fields from a list or tuple of submodels or dictionaries. In this scenario, dict and related methods expect integer keys for element-wise inclusion or exclusion. To exclude a field from every member of a list or tuple, the dictionary key '__all__' can be used as follows: import datetime from typing import List from pydantic import BaseModel , SecretStr class Country ( BaseModel ): name : str phone_code : int class Address ( BaseModel ): post_code : int country : Country class CardDetails ( BaseModel ): number : SecretStr expires : datetime . date class Hobby ( BaseModel ): name : str info : str class User ( BaseModel ): first_name : str second_name : str address : Address card_details : CardDetails hobbies : List [ Hobby ] user = User ( first_name = 'John' , second_name = 'Doe' , address = Address ( post_code = 123456 , country = Country ( name = 'USA' , phone_code = 1 ) ), card_details = CardDetails ( number = 4212934504460000 , expires = datetime . date ( 2020 , 5 , 1 ) ), hobbies = [ Hobby ( name = 'Programming' , info = 'Writing code and stuff' ), Hobby ( name = 'Gaming' , info = 'Hell Yeah!!!' ), ], ) exclude_keys = { 'second_name' : ... , 'address' : { 'post_code' : ... , 'country' : { 'phone_code' }}, 'card_details' : ... , # You can exclude fields from specific members of a tuple/list by index: 'hobbies' : { - 1 : { 'info' }}, } include_keys = { 'first_name' : ... , 'address' : { 'country' : { 'name' }}, 'hobbies' : { 0 : ... , - 1 : { 'name' }}, } # would be the same as user.dict(exclude=exclude_keys) in this case: print ( user . dict ( include = include_keys )) \"\"\" { 'first_name': 'John', 'address': {'country': {'name': 'USA'}}, 'hobbies': [ { 'name': 'Programming', 'info': 'Writing code and stuff', }, {'name': 'Gaming'}, ], } \"\"\" # To exclude a field from all members of a nested list or tuple, use \"__all__\": print ( user . dict ( exclude = { 'hobbies' : { '__all__' : { 'info' }}})) \"\"\" { 'first_name': 'John', 'second_name': 'Doe', 'address': { 'post_code': 123456, 'country': {'name': 'USA', 'phone_code': 1}, }, 'card_details': { 'number': SecretStr('**********'), 'expires': datetime.date(2020, 5, 1), }, 'hobbies': [{'name': 'Programming'}, {'name': 'Gaming'}], } \"\"\" The same holds for the json and copy methods. References \u2691 Pydantic exporting models","title":"Pydantic Exporting Models"},{"location":"coding/python/pydantic_exporting/#modeldict","text":"This is the primary way of converting a model to a dictionary. Sub-models will be recursively converted to dictionaries. Arguments: include : Fields to include in the returned dictionary. exclude : Fields to exclude from the returned dictionary. by_alias : Whether field aliases should be used as keys in the returned dictionary; default False . exclude_unset : Whether fields which were not explicitly set when creating the model should be excluded from the returned dictionary; default False . exclude_defaults : Whether fields which are equal to their default values (whether set or otherwise) should be excluded from the returned dictionary; default False . exclude_none : Whether fields which are equal to None should be excluded from the returned dictionary; default False . Example: from pydantic import BaseModel class BarModel ( BaseModel ): whatever : int class FooBarModel ( BaseModel ): banana : float foo : str bar : BarModel m = FooBarModel ( banana = 3.14 , foo = 'hello' , bar = { 'whatever' : 123 }) # returns a dictionary: print ( m . dict ()) \"\"\" { 'banana': 3.14, 'foo': 'hello', 'bar': {'whatever': 123}, } \"\"\" print ( m . dict ( include = { 'foo' , 'bar' })) #> {'foo': 'hello', 'bar': {'whatever': 123}} print ( m . dict ( exclude = { 'foo' , 'bar' })) #> {'banana': 3.14}","title":"model.dict(...)"},{"location":"coding/python/pydantic_exporting/#dictmodel-and-iteration","text":"pydantic models can also be converted to dictionaries using dict(model) , and you can also iterate over a model's field using for field_name, value in model: . With this approach the raw field values are returned, so sub-models will not be converted to dictionaries. Example: from pydantic import BaseModel class BarModel ( BaseModel ): whatever : int class FooBarModel ( BaseModel ): banana : float foo : str bar : BarModel m = FooBarModel ( banana = 3.14 , foo = 'hello' , bar = { 'whatever' : 123 }) print ( dict ( m )) \"\"\" { 'banana': 3.14, 'foo': 'hello', 'bar': BarModel( whatever=123, ), } \"\"\" for name , value in m : print ( f ' { name } : { value } ' ) #> banana: 3.14 #> foo: hello #> bar: whatever=123","title":"dict(model) and iteration"},{"location":"coding/python/pydantic_exporting/#modelcopy","text":"copy() allows models to be duplicated, which is particularly useful for immutable models. Arguments: include : Fields to include in the returned dictionary. exclude : Fields to exclude from the returned dictionary. update : A dictionary of values to change when creating the copied model deep : whether to make a deep copy of the new model; default False Example: from pydantic import BaseModel class BarModel ( BaseModel ): whatever : int class FooBarModel ( BaseModel ): banana : float foo : str bar : BarModel m = FooBarModel ( banana = 3.14 , foo = 'hello' , bar = { 'whatever' : 123 }) print ( m . copy ( include = { 'foo' , 'bar' })) #> foo='hello' bar=BarModel(whatever=123) print ( m . copy ( exclude = { 'foo' , 'bar' })) #> banana=3.14 print ( m . copy ( update = { 'banana' : 0 })) #> banana=0 foo='hello' bar=BarModel(whatever=123) print ( id ( m . bar ), id ( m . copy () . bar )) #> 139868119420992 139868119420992 # normal copy gives the same object reference for `bar` print ( id ( m . bar ), id ( m . copy ( deep = True ) . bar )) #> 139868119420992 139868119423296 # deep copy gives a new object reference for `bar`","title":"model.copy(...)"},{"location":"coding/python/pydantic_exporting/#modeljson","text":"The .json() method will serialise a model to JSON. Typically, .json() in turn calls .dict() and serialises its result. (For models with a custom root type, after calling .dict() , only the value for the __root__ key is serialised). Arguments: include : Fields to include in the returned dictionary. exclude : Fields to exclude from the returned dictionary. by_alias : Whether field aliases should be used as keys in the returned dictionary; default False . exclude_unset : Whether fields which were not set when creating the model and have their default values should be excluded from the returned dictionary; default False . exclude_defaults : Whether fields which are equal to their default values (whether set or otherwise) should be excluded from the returned dictionary; default False . exclude_none : Whether fields which are equal to None should be excluded from the returned dictionary; default False . encoder : A custom encoder function passed to the default argument of json.dumps() ; defaults to a custom encoder designed to take care of all common types. **dumps_kwargs : Any other keyword arguments are passed to json.dumps() , e.g. indent . pydantic can serialise many commonly used types to JSON (e.g. datetime , date or UUID ) which would normally fail with a simple json.dumps(foobar) . from datetime import datetime from pydantic import BaseModel class BarModel ( BaseModel ): whatever : int class FooBarModel ( BaseModel ): foo : datetime bar : BarModel m = FooBarModel ( foo = datetime ( 2032 , 6 , 1 , 12 , 13 , 14 ), bar = { 'whatever' : 123 }) print ( m . json ()) #> {\"foo\": \"2032-06-01T12:13:14\", \"bar\": {\"whatever\": 123}}","title":"model.json(...)"},{"location":"coding/python/pydantic_exporting/#advanced-include-and-exclude","text":"The dict , json , and copy methods support include and exclude arguments which can either be sets or dictionaries. This allows nested selection of which fields to export: from pydantic import BaseModel , SecretStr class User ( BaseModel ): id : int username : str password : SecretStr class Transaction ( BaseModel ): id : str user : User value : int t = Transaction ( id = '1234567890' , user = User ( id = 42 , username = 'JohnDoe' , password = 'hashedpassword' ), value = 9876543210 , ) # using a set: print ( t . dict ( exclude = { 'user' , 'value' })) #> {'id': '1234567890'} # using a dict: print ( t . dict ( exclude = { 'user' : { 'username' , 'password' }, 'value' : ... })) #> {'id': '1234567890', 'user': {'id': 42}} print ( t . dict ( include = { 'id' : ... , 'user' : { 'id' }})) #> {'id': '1234567890', 'user': {'id': 42}} The ellipsis ( ... ) indicates that we want to exclude or include an entire key, just as if we included it in a set. Of course, the same can be done at any depth level. Special care must be taken when including or excluding fields from a list or tuple of submodels or dictionaries. In this scenario, dict and related methods expect integer keys for element-wise inclusion or exclusion. To exclude a field from every member of a list or tuple, the dictionary key '__all__' can be used as follows: import datetime from typing import List from pydantic import BaseModel , SecretStr class Country ( BaseModel ): name : str phone_code : int class Address ( BaseModel ): post_code : int country : Country class CardDetails ( BaseModel ): number : SecretStr expires : datetime . date class Hobby ( BaseModel ): name : str info : str class User ( BaseModel ): first_name : str second_name : str address : Address card_details : CardDetails hobbies : List [ Hobby ] user = User ( first_name = 'John' , second_name = 'Doe' , address = Address ( post_code = 123456 , country = Country ( name = 'USA' , phone_code = 1 ) ), card_details = CardDetails ( number = 4212934504460000 , expires = datetime . date ( 2020 , 5 , 1 ) ), hobbies = [ Hobby ( name = 'Programming' , info = 'Writing code and stuff' ), Hobby ( name = 'Gaming' , info = 'Hell Yeah!!!' ), ], ) exclude_keys = { 'second_name' : ... , 'address' : { 'post_code' : ... , 'country' : { 'phone_code' }}, 'card_details' : ... , # You can exclude fields from specific members of a tuple/list by index: 'hobbies' : { - 1 : { 'info' }}, } include_keys = { 'first_name' : ... , 'address' : { 'country' : { 'name' }}, 'hobbies' : { 0 : ... , - 1 : { 'name' }}, } # would be the same as user.dict(exclude=exclude_keys) in this case: print ( user . dict ( include = include_keys )) \"\"\" { 'first_name': 'John', 'address': {'country': {'name': 'USA'}}, 'hobbies': [ { 'name': 'Programming', 'info': 'Writing code and stuff', }, {'name': 'Gaming'}, ], } \"\"\" # To exclude a field from all members of a nested list or tuple, use \"__all__\": print ( user . dict ( exclude = { 'hobbies' : { '__all__' : { 'info' }}})) \"\"\" { 'first_name': 'John', 'second_name': 'Doe', 'address': { 'post_code': 123456, 'country': {'name': 'USA', 'phone_code': 1}, }, 'card_details': { 'number': SecretStr('**********'), 'expires': datetime.date(2020, 5, 1), }, 'hobbies': [{'name': 'Programming'}, {'name': 'Gaming'}], } \"\"\" The same holds for the json and copy methods.","title":"Advanced include and exclude"},{"location":"coding/python/pydantic_exporting/#references","text":"Pydantic exporting models","title":"References"},{"location":"coding/python/pydantic_functions/","text":"The validate_arguments decorator allows the arguments passed to a function to be parsed and validated using the function's annotations before the function is called. While under the hood this uses the same approach of model creation and initialisation; it provides an extremely easy way to apply validation to your code with minimal boilerplate. In Beta The validate_arguments decorator is in beta , it has been added to pydantic in v1.5 on a provisional basis . It may change significantly in future releases and its interface will not be concrete until v2 . Feedback from the community while it's still provisional would be extremely useful; either comment on #1205 or create a new issue. Be sure you understand it's limitations . Example of usage: from pydantic import validate_arguments , ValidationError @validate_arguments def repeat ( s : str , count : int , * , separator : bytes = b '' ) -> bytes : b = s . encode () return separator . join ( b for _ in range ( count )) a = repeat ( 'hello' , 3 ) print ( a ) #> b'hellohellohello' b = repeat ( 'x' , '4' , separator = ' ' ) print ( b ) #> b'x x x x' try : c = repeat ( 'hello' , 'wrong' ) except ValidationError as exc : print ( exc ) \"\"\" 1 validation error for Repeat count value is not a valid integer (type=type_error.integer) \"\"\" Usage with mypy \u2691 The validate_arguments decorator should work \"out of the box\" with mypy since it's defined to return a function with the same signature as the function it decorates. The only limitation is that since we trick mypy into thinking the function returned by the decorator is the same as the function being decorated; access to the raw function or other attributes will require type: ignore . References \u2691 Pydantic validation decorator docs","title":"Pydantic Validating Functions"},{"location":"coding/python/pydantic_functions/#usage-with-mypy","text":"The validate_arguments decorator should work \"out of the box\" with mypy since it's defined to return a function with the same signature as the function it decorates. The only limitation is that since we trick mypy into thinking the function returned by the decorator is the same as the function being decorated; access to the raw function or other attributes will require type: ignore .","title":"Usage with mypy"},{"location":"coding/python/pydantic_functions/#references","text":"Pydantic validation decorator docs","title":"References"},{"location":"coding/python/pydantic_mypy_plugin/","text":"Pydantic works well with mypy right out of the box. However, Pydantic also ships with a mypy plugin that adds a number of important pydantic-specific features to mypy that improve its ability to type-check your code. Enabling the Plugin \u2691 To enable the plugin, just add pydantic.mypy to the list of plugins in your mypy config file (this could be mypy.ini or setup.cfg ). To get started, all you need to do is create a mypy.ini file with following contents: [mypy] plugins = pydantic.mypy See the mypy usage and plugin configuration docs for more details. References \u2691 Pydantic mypy plugin docs","title":"Pydantic Mypy Plugin"},{"location":"coding/python/pydantic_mypy_plugin/#enabling-the-plugin","text":"To enable the plugin, just add pydantic.mypy to the list of plugins in your mypy config file (this could be mypy.ini or setup.cfg ). To get started, all you need to do is create a mypy.ini file with following contents: [mypy] plugins = pydantic.mypy See the mypy usage and plugin configuration docs for more details.","title":"Enabling the Plugin"},{"location":"coding/python/pydantic_mypy_plugin/#references","text":"Pydantic mypy plugin docs","title":"References"},{"location":"coding/python/pydantic_types/","text":"Where possible pydantic uses standard library types to define fields, thus smoothing the learning curve. For many useful applications, however, no standard library type exists, so pydantic implements many commonly used types . If no existing type suits your purpose you can also implement your own pydantic-compatible types with custom properties and validation. Standard Library Types \u2691 pydantic supports many common types from the python standard library. If you need stricter processing see Strict Types ; if you need to constrain the values allowed (e.g. to require a positive int) see Constrained Types . bool see Booleans for details on how bools are validated and what values are permitted. int pydantic uses int(v) to coerce types to an int ; see this warning on loss of information during data conversion. float similarly, float(v) is used to coerce values to floats. str strings are accepted as-is, int float and Decimal are coerced using str(v) , bytes and bytearray are converted using v.decode() , enums inheriting from str are converted using v.value , and all other types cause an error. list allows list , tuple , set , frozenset , or generators and casts to a list. tuple allows list , tuple , set , frozenset , or generators and casts to a tuple. dict dict(v) is used to attempt to convert a dictionary. set allows list , tuple , set , frozenset , or generators and casts to a set. frozenset allows list , tuple , set , frozenset , or generators and casts to a frozen set. datetime.date see Datetime Types below for more detail on parsing and validation. datetime.time see Datetime Types below for more detail on parsing and validation. datetime.datetime see Datetime Types below for more detail on parsing and validation. datetime.timedelta see Datetime Types below for more detail on parsing and validation. typing.Any allows any value include None , thus an Any field is optional. typing.TypeVar constrains the values allowed based on constraints or bound , see TypeVar . typing.Union see Unions below for more detail on parsing and validation. typing.Optional Optional[x] is simply short hand for Union[x, None] ; see Unions below for more detail on parsing and validation. typing.List : typing.Tuple : typing.Dict : typing.Set : typing.FrozenSet : typing.Sequence : typing.Iterable this is reserved for iterables that shouldn't be consumed. See Infinite Generators below for more detail on parsing and validation. typing.Type see Type below for more detail on parsing and validation. typing.Callable see Callable for more detail on parsing and validation. typing.Pattern will cause the input value to be passed to re.compile(v) to create a regex pattern. ipaddress.IPv4Address simply uses the type itself for validation by passing the value to IPv4Address(v) . ipaddress.IPv4Interface simply uses the type itself for validation by passing the value to IPv4Address(v) . ipaddress.IPv4Network simply uses the type itself for validation by passing the value to IPv4Network(v) . enum.Enum checks that the value is a valid member of the enum; see Enums and Choices for more details. enum.IntEnum checks that the value is a valid member of the integer enum; see Enums and Choices for more details. decimal.Decimal pydantic attempts to convert the value to a string, then passes the string to Decimal(v) . pathlib.Path simply uses the type itself for validation by passing the value to Path(v) . Iterables \u2691 Define default value for an iterable \u2691 If you want to define an empty list, dictionary, set or other iterable as a model attribute, you can use the default_factory . from typing import Sequence from pydantic import BaseModel , Field class Foo ( BaseModel ): defaulted_list_field : Sequence [ str ] = Field ( default_factory = list ) It might be tempting to do class Foo ( BaseModel ): defaulted_list_field : Sequence [ str ] = [] # Bad! But you'll follow the mutable default argument anti-pattern. Unions \u2691 The Union type allows a model attribute to accept different types, e.g.: from uuid import UUID from typing import Union from pydantic import BaseModel class User ( BaseModel ): id : Union [ int , str , UUID ] name : str user_01 = User ( id = 123 , name = 'John Doe' ) print ( user_01 ) #> id=123 name='John Doe' print ( user_01 . id ) #> 123 user_02 = User ( id = '1234' , name = 'John Doe' ) print ( user_02 ) #> id=1234 name='John Doe' print ( user_02 . id ) #> 1234 user_03_uuid = UUID ( 'cf57432e-809e-4353-adbd-9d5c0d733868' ) user_03 = User ( id = user_03_uuid , name = 'John Doe' ) print ( user_03 ) #> id=275603287559914445491632874575877060712 name='John Doe' print ( user_03 . id ) #> 275603287559914445491632874575877060712 print ( user_03_uuid . int ) #> 275603287559914445491632874575877060712 However, as can be seen above, pydantic will attempt to 'match' any of the types defined under Union and will use the first one that matches. In the above example the id of user_03 was defined as a uuid.UUID class (which is defined under the attribute's Union annotation) but as the uuid.UUID can be marshalled into an int it chose to match against the int type and disregarded the other types. As such, it is recommended that, when defining Union annotations, the most specific type is included first and followed by less specific types. In the above example, the UUID class should precede the int and str classes to preclude the unexpected representation as such: from uuid import UUID from typing import Union from pydantic import BaseModel class User ( BaseModel ): id : Union [ UUID , int , str ] name : str user_03_uuid = UUID ( 'cf57432e-809e-4353-adbd-9d5c0d733868' ) user_03 = User ( id = user_03_uuid , name = 'John Doe' ) print ( user_03 ) #> id=UUID('cf57432e-809e-4353-adbd-9d5c0d733868') name='John Doe' print ( user_03 . id ) #> cf57432e-809e-4353-adbd-9d5c0d733868 print ( user_03_uuid . int ) #> 275603287559914445491632874575877060712 Enums and Choices \u2691 pydantic uses python's standard enum classes to define choices. from enum import Enum , IntEnum from pydantic import BaseModel , ValidationError class FruitEnum ( str , Enum ): pear = 'pear' banana = 'banana' class ToolEnum ( IntEnum ): spanner = 1 wrench = 2 class CookingModel ( BaseModel ): fruit : FruitEnum = FruitEnum . pear tool : ToolEnum = ToolEnum . spanner print ( CookingModel ()) #> fruit=<FruitEnum.pear: 'pear'> tool=<ToolEnum.spanner: 1> print ( CookingModel ( tool = 2 , fruit = 'banana' )) #> fruit=<FruitEnum.banana: 'banana'> tool=<ToolEnum.wrench: 2> try : CookingModel ( fruit = 'other' ) except ValidationError as e : print ( e ) \"\"\" 1 validation error for CookingModel fruit value is not a valid enumeration member; permitted: 'pear', 'banana' (type=type_error.enum; enum_values=[<FruitEnum.pear: 'pear'>, <FruitEnum.banana: 'banana'>]) \"\"\" Datetime Types \u2691 Pydantic supports the following datetime types: datetime fields can be: datetime , existing datetime object int or float , assumed as Unix time, i.e. seconds (if >= -2e10 or <= 2e10 ) or milliseconds (if < -2e10 or > 2e10 ) since 1 January 1970 str , following formats work: YYYY-MM-DD[T]HH:MM[:SS[.ffffff]][Z[\u00b1]HH[:]MM]]] int or float as a string (assumed as Unix time) date fields can be: date , existing date object int or float , see datetime str , following formats work: YYYY-MM-DD int or float , see datetime time fields can be: time , existing time object str , following formats work: HH:MM[:SS[.ffffff]] timedelta fields can be: timedelta , existing timedelta object int or float , assumed as seconds str , following formats work: [-][DD ][HH:MM]SS[.ffffff] [\u00b1]P[DD]DT[HH]H[MM]M[SS]S (ISO 8601 format for timedelta) Type \u2691 pydantic supports the use of Type[T] to specify that a field may only accept classes (not instances) that are subclasses of T . from typing import Type from pydantic import BaseModel from pydantic import ValidationError class Foo : pass class Bar ( Foo ): pass class Other : pass class SimpleModel ( BaseModel ): just_subclasses : Type [ Foo ] SimpleModel ( just_subclasses = Foo ) SimpleModel ( just_subclasses = Bar ) try : SimpleModel ( just_subclasses = Other ) except ValidationError as e : print ( e ) \"\"\" 1 validation error for SimpleModel just_subclasses subclass of Foo expected (type=type_error.subclass; expected_class=Foo) \"\"\" TypeVar \u2691 TypeVar is supported either unconstrained, constrained or with a bound. from typing import TypeVar from pydantic import BaseModel Foobar = TypeVar ( 'Foobar' ) BoundFloat = TypeVar ( 'BoundFloat' , bound = float ) IntStr = TypeVar ( 'IntStr' , int , str ) class Model ( BaseModel ): a : Foobar # equivalent of \": Any\" b : BoundFloat # equivalent of \": float\" c : IntStr # equivalent of \": Union[int, str]\" print ( Model ( a = [ 1 ], b = 4.2 , c = 'x' )) #> a=[1] b=4.2 c='x' # a may be None and is therefore optional print ( Model ( b = 1 , c = 1 )) #> a=None b=1.0 c=1 Pydantic Types \u2691 pydantic also provides a variety of other useful types: FilePath like Path , but the path must exist and be a file. DirectoryPath like Path , but the path must exist and be a directory. Color for parsing HTML and CSS colors; see Color Type . Json a special type wrapper which loads JSON before parsing; see JSON Type . AnyUrl any URL; see URLs . AnyHttpUrl an HTTP URL; see URLs . HttpUrl a stricter HTTP URL; see URLs . PostgresDsn a postgres DSN style URL; see URLs . RedisDsn a redis DSN style URL; see URLs . SecretStr string where the value is kept partially secret; see Secrets . IPvAnyAddress allows either an IPv4Address or an IPv6Address . IPvAnyInterface allows either an IPv4Interface or an IPv6Interface . IPvAnyNetwork allows either an IPv4Network or an IPv6Network . NegativeFloat allows a float which is negative; uses standard float parsing then checks the value is less than 0; see Constrained Types . NegativeInt allows an int which is negative; uses standard int parsing then checks the value is less than 0; see Constrained Types . PositiveFloat allows a float which is positive; uses standard float parsing then checks the value is greater than 0; see Constrained Types . PositiveInt allows an int which is positive; uses standard int parsing then checks the value is greater than 0; see Constrained Types . condecimal type method for constraining Decimals; see Constrained Types . confloat type method for constraining floats; see Constrained Types . conint type method for constraining ints; see Constrained Types . conlist type method for constraining lists; see Constrained Types . conset type method for constraining sets; see Constrained Types . constr type method for constraining strs; see Constrained Types . Custom Data Types \u2691 You can also define your own custom data types. There are several ways to achieve it. Classes with __get_validators__ \u2691 You use a custom class with a classmethod __get_validators__ . It will be called to get validators to parse and validate the input data. Tip These validators have the same semantics as in Validators , you can declare a parameter config , field , etc. import re from pydantic import BaseModel # https://en.wikipedia.org/wiki/Postcodes_in_the_United_Kingdom#Validation post_code_regex = re . compile ( r '(?:' r '([A-Z]{1,2}[0-9][A-Z0-9]?|ASCN|STHL|TDCU|BBND|[BFS]IQQ|PCRN|TKCA) ?' r '([0-9][A-Z] {2} )|' r '(BFPO) ?([0-9]{1,4})|' r '(KY[0-9]|MSR|VG|AI)[ -]?[0-9] {4} |' r '([A-Z] {2} ) ?([0-9] {2} )|' r '(GE) ?(CX)|' r '(GIR) ?(0A {2} )|' r '(SAN) ?(TA1)' r ')' ) class PostCode ( str ): \"\"\" Partial UK postcode validation. Note: this is just an example, and is not intended for use in production; in particular this does NOT guarantee a postcode exists, just that it has a valid format. \"\"\" @classmethod def __get_validators__ ( cls ): # one or more validators may be yielded which will be called in the # order to validate the input, each validator will receive as an input # the value returned from the previous validator yield cls . validate @classmethod def __modify_schema__ ( cls , field_schema ): # __modify_schema__ should mutate the dict it receives in place, # the returned value will be ignored field_schema . update ( # simplified regex here for brevity, see the wikipedia link above pattern = '^[A-Z]{1,2}[0-9][A-Z0-9]? ?[0-9][A-Z] {2} $' , # some example postcodes examples = [ 'SP11 9DG' , 'w1j7bu' ], ) @classmethod def validate ( cls , v ): if not isinstance ( v , str ): raise TypeError ( 'string required' ) m = post_code_regex . fullmatch ( v . upper ()) if not m : raise ValueError ( 'invalid postcode format' ) # you could also return a string here which would mean model.post_code # would be a string, pydantic won't care but you could end up with some # confusion since the value's type won't match the type annotation # exactly return cls ( f ' { m . group ( 1 ) } { m . group ( 2 ) } ' ) def __repr__ ( self ): return f 'PostCode( { super () . __repr__ () } )' class Model ( BaseModel ): post_code : PostCode model = Model ( post_code = 'sw8 5el' ) print ( model ) #> post_code=PostCode('SW8 5EL') print ( model . post_code ) #> SW8 5EL print ( Model . schema ()) \"\"\" { 'title': 'Model', 'type': 'object', 'properties': { 'post_code': { 'title': 'Post Code', 'pattern': '^[A-Z]{1,2}[0-9][A-Z0-9]? ?[0-9][A-Z]{2}$', 'examples': ['SP11 9DG', 'w1j7bu'], 'type': 'string', }, }, 'required': ['post_code'], } \"\"\" Generic Classes as Types \u2691 Warning This is an advanced technique that you might not need in the beginning. In most of the cases you will probably be fine with standard pydantic models. You can use Generic Classes as field types and perform custom validation based on the \"type parameters\" (or sub-types) with __get_validators__ . If the Generic class that you are using as a sub-type has a classmethod __get_validators__ you don't need to use arbitrary_types_allowed for it to work. Because you can declare validators that receive the current field , you can extract the sub_fields (from the generic class type parameters) and validate data with them. from pydantic import BaseModel , ValidationError from pydantic.fields import ModelField from typing import TypeVar , Generic AgedType = TypeVar ( 'AgedType' ) QualityType = TypeVar ( 'QualityType' ) # This is not a pydantic model, it's an arbitrary generic class class TastingModel ( Generic [ AgedType , QualityType ]): def __init__ ( self , name : str , aged : AgedType , quality : QualityType ): self . name = name self . aged = aged self . quality = quality @classmethod def __get_validators__ ( cls ): yield cls . validate @classmethod # You don't need to add the \"ModelField\", but it will help your # editor give you completion and catch errors def validate ( cls , v , field : ModelField ): if not isinstance ( v , cls ): # The value is not even a TastingModel raise TypeError ( 'Invalid value' ) if not field . sub_fields : # Generic parameters were not provided so we don't try to validate # them and just return the value as is return v aged_f = field . sub_fields [ 0 ] quality_f = field . sub_fields [ 1 ] errors = [] # Here we don't need the validated value, but we want the errors valid_value , error = aged_f . validate ( v . aged , {}, loc = 'aged' ) if error : errors . append ( error ) # Here we don't need the validated value, but we want the errors valid_value , error = quality_f . validate ( v . quality , {}, loc = 'quality' ) if error : errors . append ( error ) if errors : raise ValidationError ( errors , cls ) # Validation passed without errors, return the same instance received return v class Model ( BaseModel ): # for wine, \"aged\" is an int with years, \"quality\" is a float wine : TastingModel [ int , float ] # for cheese, \"aged\" is a bool, \"quality\" is a str cheese : TastingModel [ bool , str ] # for thing, \"aged\" is a Any, \"quality\" is Any thing : TastingModel model = Model ( # This wine was aged for 20 years and has a quality of 85.6 wine = TastingModel ( name = 'Cabernet Sauvignon' , aged = 20 , quality = 85.6 ), # This cheese is aged (is mature) and has \"Good\" quality cheese = TastingModel ( name = 'Gouda' , aged = True , quality = 'Good' ), # This Python thing has aged \"Not much\" and has a quality \"Awesome\" thing = TastingModel ( name = 'Python' , aged = 'Not much' , quality = 'Awesome' ), ) print ( model ) \"\"\" wine=<types_generics.TastingModel object at 0x7f3593a4eee0> cheese=<types_generics.TastingModel object at 0x7f3593a46100> thing=<types_generics.TastingModel object at 0x7f3593a464c0> \"\"\" print ( model . wine . aged ) #> 20 print ( model . wine . quality ) #> 85.6 print ( model . cheese . aged ) #> True print ( model . cheese . quality ) #> Good print ( model . thing . aged ) #> Not much try : # If the values of the sub-types are invalid, we get an error Model ( # For wine, aged should be an int with the years, and quality a float wine = TastingModel ( name = 'Merlot' , aged = True , quality = 'Kinda good' ), # For cheese, aged should be a bool, and quality a str cheese = TastingModel ( name = 'Gouda' , aged = 'yeah' , quality = 5 ), # For thing, no type parameters are declared, and we skipped validation # in those cases in the Assessment.validate() function thing = TastingModel ( name = 'Python' , aged = 'Not much' , quality = 'Awesome' ), ) except ValidationError as e : print ( e ) \"\"\" 2 validation errors for Model wine -> quality value is not a valid float (type=type_error.float) cheese -> aged value could not be parsed to a boolean (type=type_error.bool) \"\"\" References \u2691 Field types","title":"Pydantic Field Types"},{"location":"coding/python/pydantic_types/#standard-library-types","text":"pydantic supports many common types from the python standard library. If you need stricter processing see Strict Types ; if you need to constrain the values allowed (e.g. to require a positive int) see Constrained Types . bool see Booleans for details on how bools are validated and what values are permitted. int pydantic uses int(v) to coerce types to an int ; see this warning on loss of information during data conversion. float similarly, float(v) is used to coerce values to floats. str strings are accepted as-is, int float and Decimal are coerced using str(v) , bytes and bytearray are converted using v.decode() , enums inheriting from str are converted using v.value , and all other types cause an error. list allows list , tuple , set , frozenset , or generators and casts to a list. tuple allows list , tuple , set , frozenset , or generators and casts to a tuple. dict dict(v) is used to attempt to convert a dictionary. set allows list , tuple , set , frozenset , or generators and casts to a set. frozenset allows list , tuple , set , frozenset , or generators and casts to a frozen set. datetime.date see Datetime Types below for more detail on parsing and validation. datetime.time see Datetime Types below for more detail on parsing and validation. datetime.datetime see Datetime Types below for more detail on parsing and validation. datetime.timedelta see Datetime Types below for more detail on parsing and validation. typing.Any allows any value include None , thus an Any field is optional. typing.TypeVar constrains the values allowed based on constraints or bound , see TypeVar . typing.Union see Unions below for more detail on parsing and validation. typing.Optional Optional[x] is simply short hand for Union[x, None] ; see Unions below for more detail on parsing and validation. typing.List : typing.Tuple : typing.Dict : typing.Set : typing.FrozenSet : typing.Sequence : typing.Iterable this is reserved for iterables that shouldn't be consumed. See Infinite Generators below for more detail on parsing and validation. typing.Type see Type below for more detail on parsing and validation. typing.Callable see Callable for more detail on parsing and validation. typing.Pattern will cause the input value to be passed to re.compile(v) to create a regex pattern. ipaddress.IPv4Address simply uses the type itself for validation by passing the value to IPv4Address(v) . ipaddress.IPv4Interface simply uses the type itself for validation by passing the value to IPv4Address(v) . ipaddress.IPv4Network simply uses the type itself for validation by passing the value to IPv4Network(v) . enum.Enum checks that the value is a valid member of the enum; see Enums and Choices for more details. enum.IntEnum checks that the value is a valid member of the integer enum; see Enums and Choices for more details. decimal.Decimal pydantic attempts to convert the value to a string, then passes the string to Decimal(v) . pathlib.Path simply uses the type itself for validation by passing the value to Path(v) .","title":"Standard Library Types"},{"location":"coding/python/pydantic_types/#iterables","text":"","title":"Iterables"},{"location":"coding/python/pydantic_types/#define-default-value-for-an-iterable","text":"If you want to define an empty list, dictionary, set or other iterable as a model attribute, you can use the default_factory . from typing import Sequence from pydantic import BaseModel , Field class Foo ( BaseModel ): defaulted_list_field : Sequence [ str ] = Field ( default_factory = list ) It might be tempting to do class Foo ( BaseModel ): defaulted_list_field : Sequence [ str ] = [] # Bad! But you'll follow the mutable default argument anti-pattern.","title":"Define default value for an iterable"},{"location":"coding/python/pydantic_types/#unions","text":"The Union type allows a model attribute to accept different types, e.g.: from uuid import UUID from typing import Union from pydantic import BaseModel class User ( BaseModel ): id : Union [ int , str , UUID ] name : str user_01 = User ( id = 123 , name = 'John Doe' ) print ( user_01 ) #> id=123 name='John Doe' print ( user_01 . id ) #> 123 user_02 = User ( id = '1234' , name = 'John Doe' ) print ( user_02 ) #> id=1234 name='John Doe' print ( user_02 . id ) #> 1234 user_03_uuid = UUID ( 'cf57432e-809e-4353-adbd-9d5c0d733868' ) user_03 = User ( id = user_03_uuid , name = 'John Doe' ) print ( user_03 ) #> id=275603287559914445491632874575877060712 name='John Doe' print ( user_03 . id ) #> 275603287559914445491632874575877060712 print ( user_03_uuid . int ) #> 275603287559914445491632874575877060712 However, as can be seen above, pydantic will attempt to 'match' any of the types defined under Union and will use the first one that matches. In the above example the id of user_03 was defined as a uuid.UUID class (which is defined under the attribute's Union annotation) but as the uuid.UUID can be marshalled into an int it chose to match against the int type and disregarded the other types. As such, it is recommended that, when defining Union annotations, the most specific type is included first and followed by less specific types. In the above example, the UUID class should precede the int and str classes to preclude the unexpected representation as such: from uuid import UUID from typing import Union from pydantic import BaseModel class User ( BaseModel ): id : Union [ UUID , int , str ] name : str user_03_uuid = UUID ( 'cf57432e-809e-4353-adbd-9d5c0d733868' ) user_03 = User ( id = user_03_uuid , name = 'John Doe' ) print ( user_03 ) #> id=UUID('cf57432e-809e-4353-adbd-9d5c0d733868') name='John Doe' print ( user_03 . id ) #> cf57432e-809e-4353-adbd-9d5c0d733868 print ( user_03_uuid . int ) #> 275603287559914445491632874575877060712","title":"Unions"},{"location":"coding/python/pydantic_types/#enums-and-choices","text":"pydantic uses python's standard enum classes to define choices. from enum import Enum , IntEnum from pydantic import BaseModel , ValidationError class FruitEnum ( str , Enum ): pear = 'pear' banana = 'banana' class ToolEnum ( IntEnum ): spanner = 1 wrench = 2 class CookingModel ( BaseModel ): fruit : FruitEnum = FruitEnum . pear tool : ToolEnum = ToolEnum . spanner print ( CookingModel ()) #> fruit=<FruitEnum.pear: 'pear'> tool=<ToolEnum.spanner: 1> print ( CookingModel ( tool = 2 , fruit = 'banana' )) #> fruit=<FruitEnum.banana: 'banana'> tool=<ToolEnum.wrench: 2> try : CookingModel ( fruit = 'other' ) except ValidationError as e : print ( e ) \"\"\" 1 validation error for CookingModel fruit value is not a valid enumeration member; permitted: 'pear', 'banana' (type=type_error.enum; enum_values=[<FruitEnum.pear: 'pear'>, <FruitEnum.banana: 'banana'>]) \"\"\"","title":"Enums and Choices"},{"location":"coding/python/pydantic_types/#datetime-types","text":"Pydantic supports the following datetime types: datetime fields can be: datetime , existing datetime object int or float , assumed as Unix time, i.e. seconds (if >= -2e10 or <= 2e10 ) or milliseconds (if < -2e10 or > 2e10 ) since 1 January 1970 str , following formats work: YYYY-MM-DD[T]HH:MM[:SS[.ffffff]][Z[\u00b1]HH[:]MM]]] int or float as a string (assumed as Unix time) date fields can be: date , existing date object int or float , see datetime str , following formats work: YYYY-MM-DD int or float , see datetime time fields can be: time , existing time object str , following formats work: HH:MM[:SS[.ffffff]] timedelta fields can be: timedelta , existing timedelta object int or float , assumed as seconds str , following formats work: [-][DD ][HH:MM]SS[.ffffff] [\u00b1]P[DD]DT[HH]H[MM]M[SS]S (ISO 8601 format for timedelta)","title":"Datetime Types"},{"location":"coding/python/pydantic_types/#type","text":"pydantic supports the use of Type[T] to specify that a field may only accept classes (not instances) that are subclasses of T . from typing import Type from pydantic import BaseModel from pydantic import ValidationError class Foo : pass class Bar ( Foo ): pass class Other : pass class SimpleModel ( BaseModel ): just_subclasses : Type [ Foo ] SimpleModel ( just_subclasses = Foo ) SimpleModel ( just_subclasses = Bar ) try : SimpleModel ( just_subclasses = Other ) except ValidationError as e : print ( e ) \"\"\" 1 validation error for SimpleModel just_subclasses subclass of Foo expected (type=type_error.subclass; expected_class=Foo) \"\"\"","title":"Type"},{"location":"coding/python/pydantic_types/#typevar","text":"TypeVar is supported either unconstrained, constrained or with a bound. from typing import TypeVar from pydantic import BaseModel Foobar = TypeVar ( 'Foobar' ) BoundFloat = TypeVar ( 'BoundFloat' , bound = float ) IntStr = TypeVar ( 'IntStr' , int , str ) class Model ( BaseModel ): a : Foobar # equivalent of \": Any\" b : BoundFloat # equivalent of \": float\" c : IntStr # equivalent of \": Union[int, str]\" print ( Model ( a = [ 1 ], b = 4.2 , c = 'x' )) #> a=[1] b=4.2 c='x' # a may be None and is therefore optional print ( Model ( b = 1 , c = 1 )) #> a=None b=1.0 c=1","title":"TypeVar"},{"location":"coding/python/pydantic_types/#pydantic-types","text":"pydantic also provides a variety of other useful types: FilePath like Path , but the path must exist and be a file. DirectoryPath like Path , but the path must exist and be a directory. Color for parsing HTML and CSS colors; see Color Type . Json a special type wrapper which loads JSON before parsing; see JSON Type . AnyUrl any URL; see URLs . AnyHttpUrl an HTTP URL; see URLs . HttpUrl a stricter HTTP URL; see URLs . PostgresDsn a postgres DSN style URL; see URLs . RedisDsn a redis DSN style URL; see URLs . SecretStr string where the value is kept partially secret; see Secrets . IPvAnyAddress allows either an IPv4Address or an IPv6Address . IPvAnyInterface allows either an IPv4Interface or an IPv6Interface . IPvAnyNetwork allows either an IPv4Network or an IPv6Network . NegativeFloat allows a float which is negative; uses standard float parsing then checks the value is less than 0; see Constrained Types . NegativeInt allows an int which is negative; uses standard int parsing then checks the value is less than 0; see Constrained Types . PositiveFloat allows a float which is positive; uses standard float parsing then checks the value is greater than 0; see Constrained Types . PositiveInt allows an int which is positive; uses standard int parsing then checks the value is greater than 0; see Constrained Types . condecimal type method for constraining Decimals; see Constrained Types . confloat type method for constraining floats; see Constrained Types . conint type method for constraining ints; see Constrained Types . conlist type method for constraining lists; see Constrained Types . conset type method for constraining sets; see Constrained Types . constr type method for constraining strs; see Constrained Types .","title":"Pydantic Types"},{"location":"coding/python/pydantic_types/#custom-data-types","text":"You can also define your own custom data types. There are several ways to achieve it.","title":"Custom Data Types"},{"location":"coding/python/pydantic_types/#classes-with-__get_validators__","text":"You use a custom class with a classmethod __get_validators__ . It will be called to get validators to parse and validate the input data. Tip These validators have the same semantics as in Validators , you can declare a parameter config , field , etc. import re from pydantic import BaseModel # https://en.wikipedia.org/wiki/Postcodes_in_the_United_Kingdom#Validation post_code_regex = re . compile ( r '(?:' r '([A-Z]{1,2}[0-9][A-Z0-9]?|ASCN|STHL|TDCU|BBND|[BFS]IQQ|PCRN|TKCA) ?' r '([0-9][A-Z] {2} )|' r '(BFPO) ?([0-9]{1,4})|' r '(KY[0-9]|MSR|VG|AI)[ -]?[0-9] {4} |' r '([A-Z] {2} ) ?([0-9] {2} )|' r '(GE) ?(CX)|' r '(GIR) ?(0A {2} )|' r '(SAN) ?(TA1)' r ')' ) class PostCode ( str ): \"\"\" Partial UK postcode validation. Note: this is just an example, and is not intended for use in production; in particular this does NOT guarantee a postcode exists, just that it has a valid format. \"\"\" @classmethod def __get_validators__ ( cls ): # one or more validators may be yielded which will be called in the # order to validate the input, each validator will receive as an input # the value returned from the previous validator yield cls . validate @classmethod def __modify_schema__ ( cls , field_schema ): # __modify_schema__ should mutate the dict it receives in place, # the returned value will be ignored field_schema . update ( # simplified regex here for brevity, see the wikipedia link above pattern = '^[A-Z]{1,2}[0-9][A-Z0-9]? ?[0-9][A-Z] {2} $' , # some example postcodes examples = [ 'SP11 9DG' , 'w1j7bu' ], ) @classmethod def validate ( cls , v ): if not isinstance ( v , str ): raise TypeError ( 'string required' ) m = post_code_regex . fullmatch ( v . upper ()) if not m : raise ValueError ( 'invalid postcode format' ) # you could also return a string here which would mean model.post_code # would be a string, pydantic won't care but you could end up with some # confusion since the value's type won't match the type annotation # exactly return cls ( f ' { m . group ( 1 ) } { m . group ( 2 ) } ' ) def __repr__ ( self ): return f 'PostCode( { super () . __repr__ () } )' class Model ( BaseModel ): post_code : PostCode model = Model ( post_code = 'sw8 5el' ) print ( model ) #> post_code=PostCode('SW8 5EL') print ( model . post_code ) #> SW8 5EL print ( Model . schema ()) \"\"\" { 'title': 'Model', 'type': 'object', 'properties': { 'post_code': { 'title': 'Post Code', 'pattern': '^[A-Z]{1,2}[0-9][A-Z0-9]? ?[0-9][A-Z]{2}$', 'examples': ['SP11 9DG', 'w1j7bu'], 'type': 'string', }, }, 'required': ['post_code'], } \"\"\"","title":"Classes with __get_validators__"},{"location":"coding/python/pydantic_types/#generic-classes-as-types","text":"Warning This is an advanced technique that you might not need in the beginning. In most of the cases you will probably be fine with standard pydantic models. You can use Generic Classes as field types and perform custom validation based on the \"type parameters\" (or sub-types) with __get_validators__ . If the Generic class that you are using as a sub-type has a classmethod __get_validators__ you don't need to use arbitrary_types_allowed for it to work. Because you can declare validators that receive the current field , you can extract the sub_fields (from the generic class type parameters) and validate data with them. from pydantic import BaseModel , ValidationError from pydantic.fields import ModelField from typing import TypeVar , Generic AgedType = TypeVar ( 'AgedType' ) QualityType = TypeVar ( 'QualityType' ) # This is not a pydantic model, it's an arbitrary generic class class TastingModel ( Generic [ AgedType , QualityType ]): def __init__ ( self , name : str , aged : AgedType , quality : QualityType ): self . name = name self . aged = aged self . quality = quality @classmethod def __get_validators__ ( cls ): yield cls . validate @classmethod # You don't need to add the \"ModelField\", but it will help your # editor give you completion and catch errors def validate ( cls , v , field : ModelField ): if not isinstance ( v , cls ): # The value is not even a TastingModel raise TypeError ( 'Invalid value' ) if not field . sub_fields : # Generic parameters were not provided so we don't try to validate # them and just return the value as is return v aged_f = field . sub_fields [ 0 ] quality_f = field . sub_fields [ 1 ] errors = [] # Here we don't need the validated value, but we want the errors valid_value , error = aged_f . validate ( v . aged , {}, loc = 'aged' ) if error : errors . append ( error ) # Here we don't need the validated value, but we want the errors valid_value , error = quality_f . validate ( v . quality , {}, loc = 'quality' ) if error : errors . append ( error ) if errors : raise ValidationError ( errors , cls ) # Validation passed without errors, return the same instance received return v class Model ( BaseModel ): # for wine, \"aged\" is an int with years, \"quality\" is a float wine : TastingModel [ int , float ] # for cheese, \"aged\" is a bool, \"quality\" is a str cheese : TastingModel [ bool , str ] # for thing, \"aged\" is a Any, \"quality\" is Any thing : TastingModel model = Model ( # This wine was aged for 20 years and has a quality of 85.6 wine = TastingModel ( name = 'Cabernet Sauvignon' , aged = 20 , quality = 85.6 ), # This cheese is aged (is mature) and has \"Good\" quality cheese = TastingModel ( name = 'Gouda' , aged = True , quality = 'Good' ), # This Python thing has aged \"Not much\" and has a quality \"Awesome\" thing = TastingModel ( name = 'Python' , aged = 'Not much' , quality = 'Awesome' ), ) print ( model ) \"\"\" wine=<types_generics.TastingModel object at 0x7f3593a4eee0> cheese=<types_generics.TastingModel object at 0x7f3593a46100> thing=<types_generics.TastingModel object at 0x7f3593a464c0> \"\"\" print ( model . wine . aged ) #> 20 print ( model . wine . quality ) #> 85.6 print ( model . cheese . aged ) #> True print ( model . cheese . quality ) #> Good print ( model . thing . aged ) #> Not much try : # If the values of the sub-types are invalid, we get an error Model ( # For wine, aged should be an int with the years, and quality a float wine = TastingModel ( name = 'Merlot' , aged = True , quality = 'Kinda good' ), # For cheese, aged should be a bool, and quality a str cheese = TastingModel ( name = 'Gouda' , aged = 'yeah' , quality = 5 ), # For thing, no type parameters are declared, and we skipped validation # in those cases in the Assessment.validate() function thing = TastingModel ( name = 'Python' , aged = 'Not much' , quality = 'Awesome' ), ) except ValidationError as e : print ( e ) \"\"\" 2 validation errors for Model wine -> quality value is not a valid float (type=type_error.float) cheese -> aged value could not be parsed to a boolean (type=type_error.bool) \"\"\"","title":"Generic Classes as Types"},{"location":"coding/python/pydantic_types/#references","text":"Field types","title":"References"},{"location":"coding/python/pydantic_validators/","text":"Custom validation and complex relationships between objects can be achieved using the validator decorator. from pydantic import BaseModel , ValidationError , validator class UserModel ( BaseModel ): name : str username : str password1 : str password2 : str @validator ( 'name' ) def name_must_contain_space ( cls , v ): if ' ' not in v : raise ValueError ( 'must contain a space' ) return v . title () @validator ( 'password2' ) def passwords_match ( cls , v , values , ** kwargs ): if 'password1' in values and v != values [ 'password1' ]: raise ValueError ( 'passwords do not match' ) return v @validator ( 'username' ) def username_alphanumeric ( cls , v ): assert v . isalnum (), 'must be alphanumeric' return v user = UserModel ( name = 'samuel colvin' , username = 'scolvin' , password1 = 'zxcvbn' , password2 = 'zxcvbn' , ) print ( user ) #> name='Samuel Colvin' username='scolvin' password1='zxcvbn' password2='zxcvbn' try : UserModel ( name = 'samuel' , username = 'scolvin' , password1 = 'zxcvbn' , password2 = 'zxcvbn2' , ) except ValidationError as e : print ( e ) \"\"\" 2 validation errors for UserModel name must contain a space (type=value_error) password2 passwords do not match (type=value_error) \"\"\" You need to be aware of these validator behaviours. Validators are \"class methods\", so the first argument value they receive is the UserModel class, not an instance of UserModel . The second argument is always the field value to validate; it can be named as you please. You can also add any subset of the following arguments to the signature (the names must match): values : a dict containing the name-to-value mapping of any previously-validated fields. config : the model config. field : the field being validated. **kwargs : if provided, this will include the arguments above not explicitly listed in the signature. Validators should either return the parsed value or raise a ValueError , TypeError , or AssertionError ( assert statements may be used). Where validators rely on other values, you should be aware that: Validation is done in the order fields are defined. If validation fails on another field (or that field is missing) it will not be included in values , hence if 'password1' in values and ... in this example. Pre and per-item validators \u2691 Validators can do a few more complex things: A single validator can be applied to multiple fields by passing it multiple field names. A single validator can also be called on all fields by passing the special value '*' . The keyword argument pre will cause the validator to be called prior to other validation. Passing each_item=True will result in the validator being applied to individual values (e.g. of List , Dict , Set , etc.), rather than the whole object. from typing import List from pydantic import BaseModel , ValidationError , validator class DemoModel ( BaseModel ): square_numbers : List [ int ] = [] cube_numbers : List [ int ] = [] # '*' is the same as 'cube_numbers', 'square_numbers' here: @validator ( '*' , pre = True ) def split_str ( cls , v ): if isinstance ( v , str ): return v . split ( '|' ) return v @validator ( 'cube_numbers' , 'square_numbers' ) def check_sum ( cls , v ): if sum ( v ) > 42 : raise ValueError ( 'sum of numbers greater than 42' ) return v @validator ( 'square_numbers' , each_item = True ) def check_squares ( cls , v ): assert v ** 0.5 % 1 == 0 , f ' { v } is not a square number' return v @validator ( 'cube_numbers' , each_item = True ) def check_cubes ( cls , v ): # 64 ** (1 / 3) == 3.9999999999999996 (!) # this is not a good way of checking cubes assert v ** ( 1 / 3 ) % 1 == 0 , f ' { v } is not a cubed number' return v print ( DemoModel ( square_numbers = [ 1 , 4 , 9 ])) #> square_numbers=[1, 4, 9] cube_numbers=[] print ( DemoModel ( square_numbers = '1|4|16' )) #> square_numbers=[1, 4, 16] cube_numbers=[] print ( DemoModel ( square_numbers = [ 16 ], cube_numbers = [ 8 , 27 ])) #> square_numbers=[16] cube_numbers=[8, 27] try : DemoModel ( square_numbers = [ 1 , 4 , 2 ]) except ValidationError as e : print ( e ) \"\"\" 1 validation error for DemoModel square_numbers -> 2 2 is not a square number (type=assertion_error) \"\"\" try : DemoModel ( cube_numbers = [ 27 , 27 ]) except ValidationError as e : print ( e ) \"\"\" 1 validation error for DemoModel cube_numbers sum of numbers greater than 42 (type=value_error) \"\"\" Subclass Validators and each_item \u2691 If using a validator with a subclass that references a List type field on a parent class, using each_item=True will cause the validator not to run; instead, the list must be iterated over programatically. from typing import List from pydantic import BaseModel , ValidationError , validator class ParentModel ( BaseModel ): names : List [ str ] class ChildModel ( ParentModel ): @validator ( 'names' , each_item = True ) def check_names_not_empty ( cls , v ): assert v != '' , 'Empty strings are not allowed.' return v # This will NOT raise a ValidationError because the validator was not called try : child = ChildModel ( names = [ 'Alice' , 'Bob' , 'Eve' , '' ]) except ValidationError as e : print ( e ) else : print ( 'No ValidationError caught.' ) #> No ValidationError caught. class ChildModel2 ( ParentModel ): @validator ( 'names' ) def check_names_not_empty ( cls , v ): for name in v : assert name != '' , 'Empty strings are not allowed.' return v try : child = ChildModel2 ( names = [ 'Alice' , 'Bob' , 'Eve' , '' ]) except ValidationError as e : print ( e ) \"\"\" 1 validation error for ChildModel2 names Empty strings are not allowed. (type=assertion_error) \"\"\" Validate Always \u2691 For performance reasons, by default validators are not called for fields when a value is not supplied. However there are situations where it may be useful or required to always call the validator, e.g. to set a dynamic default value. from datetime import datetime from pydantic import BaseModel , validator class DemoModel ( BaseModel ): ts : datetime = None @validator ( 'ts' , pre = True , always = True ) def set_ts_now ( cls , v ): return v or datetime . now () print ( DemoModel ()) #> ts=datetime.datetime(2020, 7, 15, 20, 1, 48, 966302) print ( DemoModel ( ts = '2017-11-08T14:00' )) #> ts=datetime.datetime(2017, 11, 8, 14, 0) You'll often want to use this together with pre , since otherwise with always=True pydantic would try to validate the default None which would cause an error. Reuse validators \u2691 Occasionally, you will want to use the same validator on multiple fields/models (e.g. to normalize some input data). The \"naive\" approach would be to write a separate function, then call it from multiple decorators. Obviously, this entails a lot of repetition and boiler plate code. To circumvent this, the allow_reuse parameter has been added to pydantic.validator in v1.2 ( False by default): from pydantic import BaseModel , validator def normalize ( name : str ) -> str : return ' ' . join (( word . capitalize ()) for word in name . split ( ' ' )) class Producer ( BaseModel ): name : str # validators _normalize_name = validator ( 'name' , allow_reuse = True )( normalize ) class Consumer ( BaseModel ): name : str # validators _normalize_name = validator ( 'name' , allow_reuse = True )( normalize ) jane_doe = Producer ( name = 'JaNe DOE' ) john_doe = Consumer ( name = 'joHN dOe' ) assert jane_doe . name == 'Jane Doe' assert john_doe . name == 'John Doe' As it is obvious, repetition has been reduced and the models become again almost declarative. Tip If you have a lot of fields that you want to validate, it usually makes sense to define a help function with which you will avoid setting allow_reuse=True over and over again. Root Validators \u2691 Validation can also be performed on the entire model's data. from pydantic import BaseModel , ValidationError , root_validator class UserModel ( BaseModel ): username : str password1 : str password2 : str @root_validator ( pre = True ) def check_card_number_omitted ( cls , values ): assert 'card_number' not in values , 'card_number should not be included' return values @root_validator def check_passwords_match ( cls , values ): pw1 , pw2 = values . get ( 'password1' ), values . get ( 'password2' ) if pw1 is not None and pw2 is not None and pw1 != pw2 : raise ValueError ( 'passwords do not match' ) return values print ( UserModel ( username = 'scolvin' , password1 = 'zxcvbn' , password2 = 'zxcvbn' )) #> username='scolvin' password1='zxcvbn' password2='zxcvbn' try : UserModel ( username = 'scolvin' , password1 = 'zxcvbn' , password2 = 'zxcvbn2' ) except ValidationError as e : print ( e ) \"\"\" 1 validation error for UserModel __root__ passwords do not match (type=value_error) \"\"\" try : UserModel ( username = 'scolvin' , password1 = 'zxcvbn' , password2 = 'zxcvbn' , card_number = '1234' , ) except ValidationError as e : print ( e ) \"\"\" 1 validation error for UserModel __root__ card_number should not be included (type=assertion_error) \"\"\" As with field validators, root validators can have pre=True , in which case they're called before field validation occurs (and are provided with the raw input data), or pre=False (the default), in which case they're called after field validation. Field validation will not occur if pre=True root validators raise an error. As with field validators, \"post\" (i.e. pre=False ) root validators by default will be called even if prior validators fail; this behaviour can be changed by setting the skip_on_failure=True keyword argument to the validator. The values argument will be a dict containing the values which passed field validation and field defaults where applicable. Field Checks \u2691 On class creation, validators are checked to confirm that the fields they specify actually exist on the model. Occasionally however this is undesirable: e.g. if you define a validator to validate fields on inheriting models. In this case you should set check_fields=False on the validator. Dataclass Validators \u2691 Validators also work with pydantic dataclasses. from datetime import datetime from pydantic import validator from pydantic.dataclasses import dataclass @dataclass class DemoDataclass : ts : datetime = None @validator ( 'ts' , pre = True , always = True ) def set_ts_now ( cls , v ): return v or datetime . now () print ( DemoDataclass ()) #> DemoDataclass(ts=datetime.datetime(2020, 7, 15, 20, 1, 48, 969037)) print ( DemoDataclass ( ts = '2017-11-08T14:00' )) #> DemoDataclass(ts=datetime.datetime(2017, 11, 8, 14, 0)) Troubleshooting validators \u2691 pylint complains on the validators \u2691 Pylint complains that R0201: Method could be a function and N805: first argument of a method should be named 'self' . Seems to be an error of pylint, people have solved it by specifying @classmethod between the definition and the validator decorator. References \u2691 Pydantic validators","title":"Pydantic Validators"},{"location":"coding/python/pydantic_validators/#pre-and-per-item-validators","text":"Validators can do a few more complex things: A single validator can be applied to multiple fields by passing it multiple field names. A single validator can also be called on all fields by passing the special value '*' . The keyword argument pre will cause the validator to be called prior to other validation. Passing each_item=True will result in the validator being applied to individual values (e.g. of List , Dict , Set , etc.), rather than the whole object. from typing import List from pydantic import BaseModel , ValidationError , validator class DemoModel ( BaseModel ): square_numbers : List [ int ] = [] cube_numbers : List [ int ] = [] # '*' is the same as 'cube_numbers', 'square_numbers' here: @validator ( '*' , pre = True ) def split_str ( cls , v ): if isinstance ( v , str ): return v . split ( '|' ) return v @validator ( 'cube_numbers' , 'square_numbers' ) def check_sum ( cls , v ): if sum ( v ) > 42 : raise ValueError ( 'sum of numbers greater than 42' ) return v @validator ( 'square_numbers' , each_item = True ) def check_squares ( cls , v ): assert v ** 0.5 % 1 == 0 , f ' { v } is not a square number' return v @validator ( 'cube_numbers' , each_item = True ) def check_cubes ( cls , v ): # 64 ** (1 / 3) == 3.9999999999999996 (!) # this is not a good way of checking cubes assert v ** ( 1 / 3 ) % 1 == 0 , f ' { v } is not a cubed number' return v print ( DemoModel ( square_numbers = [ 1 , 4 , 9 ])) #> square_numbers=[1, 4, 9] cube_numbers=[] print ( DemoModel ( square_numbers = '1|4|16' )) #> square_numbers=[1, 4, 16] cube_numbers=[] print ( DemoModel ( square_numbers = [ 16 ], cube_numbers = [ 8 , 27 ])) #> square_numbers=[16] cube_numbers=[8, 27] try : DemoModel ( square_numbers = [ 1 , 4 , 2 ]) except ValidationError as e : print ( e ) \"\"\" 1 validation error for DemoModel square_numbers -> 2 2 is not a square number (type=assertion_error) \"\"\" try : DemoModel ( cube_numbers = [ 27 , 27 ]) except ValidationError as e : print ( e ) \"\"\" 1 validation error for DemoModel cube_numbers sum of numbers greater than 42 (type=value_error) \"\"\"","title":"Pre and per-item validators"},{"location":"coding/python/pydantic_validators/#subclass-validators-and-each_item","text":"If using a validator with a subclass that references a List type field on a parent class, using each_item=True will cause the validator not to run; instead, the list must be iterated over programatically. from typing import List from pydantic import BaseModel , ValidationError , validator class ParentModel ( BaseModel ): names : List [ str ] class ChildModel ( ParentModel ): @validator ( 'names' , each_item = True ) def check_names_not_empty ( cls , v ): assert v != '' , 'Empty strings are not allowed.' return v # This will NOT raise a ValidationError because the validator was not called try : child = ChildModel ( names = [ 'Alice' , 'Bob' , 'Eve' , '' ]) except ValidationError as e : print ( e ) else : print ( 'No ValidationError caught.' ) #> No ValidationError caught. class ChildModel2 ( ParentModel ): @validator ( 'names' ) def check_names_not_empty ( cls , v ): for name in v : assert name != '' , 'Empty strings are not allowed.' return v try : child = ChildModel2 ( names = [ 'Alice' , 'Bob' , 'Eve' , '' ]) except ValidationError as e : print ( e ) \"\"\" 1 validation error for ChildModel2 names Empty strings are not allowed. (type=assertion_error) \"\"\"","title":"Subclass Validators and each_item"},{"location":"coding/python/pydantic_validators/#validate-always","text":"For performance reasons, by default validators are not called for fields when a value is not supplied. However there are situations where it may be useful or required to always call the validator, e.g. to set a dynamic default value. from datetime import datetime from pydantic import BaseModel , validator class DemoModel ( BaseModel ): ts : datetime = None @validator ( 'ts' , pre = True , always = True ) def set_ts_now ( cls , v ): return v or datetime . now () print ( DemoModel ()) #> ts=datetime.datetime(2020, 7, 15, 20, 1, 48, 966302) print ( DemoModel ( ts = '2017-11-08T14:00' )) #> ts=datetime.datetime(2017, 11, 8, 14, 0) You'll often want to use this together with pre , since otherwise with always=True pydantic would try to validate the default None which would cause an error.","title":"Validate Always"},{"location":"coding/python/pydantic_validators/#reuse-validators","text":"Occasionally, you will want to use the same validator on multiple fields/models (e.g. to normalize some input data). The \"naive\" approach would be to write a separate function, then call it from multiple decorators. Obviously, this entails a lot of repetition and boiler plate code. To circumvent this, the allow_reuse parameter has been added to pydantic.validator in v1.2 ( False by default): from pydantic import BaseModel , validator def normalize ( name : str ) -> str : return ' ' . join (( word . capitalize ()) for word in name . split ( ' ' )) class Producer ( BaseModel ): name : str # validators _normalize_name = validator ( 'name' , allow_reuse = True )( normalize ) class Consumer ( BaseModel ): name : str # validators _normalize_name = validator ( 'name' , allow_reuse = True )( normalize ) jane_doe = Producer ( name = 'JaNe DOE' ) john_doe = Consumer ( name = 'joHN dOe' ) assert jane_doe . name == 'Jane Doe' assert john_doe . name == 'John Doe' As it is obvious, repetition has been reduced and the models become again almost declarative. Tip If you have a lot of fields that you want to validate, it usually makes sense to define a help function with which you will avoid setting allow_reuse=True over and over again.","title":"Reuse validators"},{"location":"coding/python/pydantic_validators/#root-validators","text":"Validation can also be performed on the entire model's data. from pydantic import BaseModel , ValidationError , root_validator class UserModel ( BaseModel ): username : str password1 : str password2 : str @root_validator ( pre = True ) def check_card_number_omitted ( cls , values ): assert 'card_number' not in values , 'card_number should not be included' return values @root_validator def check_passwords_match ( cls , values ): pw1 , pw2 = values . get ( 'password1' ), values . get ( 'password2' ) if pw1 is not None and pw2 is not None and pw1 != pw2 : raise ValueError ( 'passwords do not match' ) return values print ( UserModel ( username = 'scolvin' , password1 = 'zxcvbn' , password2 = 'zxcvbn' )) #> username='scolvin' password1='zxcvbn' password2='zxcvbn' try : UserModel ( username = 'scolvin' , password1 = 'zxcvbn' , password2 = 'zxcvbn2' ) except ValidationError as e : print ( e ) \"\"\" 1 validation error for UserModel __root__ passwords do not match (type=value_error) \"\"\" try : UserModel ( username = 'scolvin' , password1 = 'zxcvbn' , password2 = 'zxcvbn' , card_number = '1234' , ) except ValidationError as e : print ( e ) \"\"\" 1 validation error for UserModel __root__ card_number should not be included (type=assertion_error) \"\"\" As with field validators, root validators can have pre=True , in which case they're called before field validation occurs (and are provided with the raw input data), or pre=False (the default), in which case they're called after field validation. Field validation will not occur if pre=True root validators raise an error. As with field validators, \"post\" (i.e. pre=False ) root validators by default will be called even if prior validators fail; this behaviour can be changed by setting the skip_on_failure=True keyword argument to the validator. The values argument will be a dict containing the values which passed field validation and field defaults where applicable.","title":"Root Validators"},{"location":"coding/python/pydantic_validators/#field-checks","text":"On class creation, validators are checked to confirm that the fields they specify actually exist on the model. Occasionally however this is undesirable: e.g. if you define a validator to validate fields on inheriting models. In this case you should set check_fields=False on the validator.","title":"Field Checks"},{"location":"coding/python/pydantic_validators/#dataclass-validators","text":"Validators also work with pydantic dataclasses. from datetime import datetime from pydantic import validator from pydantic.dataclasses import dataclass @dataclass class DemoDataclass : ts : datetime = None @validator ( 'ts' , pre = True , always = True ) def set_ts_now ( cls , v ): return v or datetime . now () print ( DemoDataclass ()) #> DemoDataclass(ts=datetime.datetime(2020, 7, 15, 20, 1, 48, 969037)) print ( DemoDataclass ( ts = '2017-11-08T14:00' )) #> DemoDataclass(ts=datetime.datetime(2017, 11, 8, 14, 0))","title":"Dataclass Validators"},{"location":"coding/python/pydantic_validators/#troubleshooting-validators","text":"","title":"Troubleshooting validators"},{"location":"coding/python/pydantic_validators/#pylint-complains-on-the-validators","text":"Pylint complains that R0201: Method could be a function and N805: first argument of a method should be named 'self' . Seems to be an error of pylint, people have solved it by specifying @classmethod between the definition and the validator decorator.","title":"pylint complains on the validators"},{"location":"coding/python/pydantic_validators/#references","text":"Pydantic validators","title":"References"},{"location":"coding/python/pypika/","text":"Pypika is a Python API for building SQL queries. The motivation behind PyPika is to provide a simple interface for building SQL queries without limiting the flexibility of handwritten SQL. PyPika is a fast, expressive and flexible way to replace handwritten SQL. Validation of SQL correctness is not an explicit goal of the project. Instead you are encouraged to check inputs you provide to PyPika or appropriately handle errors raised from your SQL database. After the queries have been built you need to interact with the database with other libraries. Installation \u2691 pip install pypika Usage \u2691 The main classes in pypika are pypika.Query , pypika.Table , and pypika.Field . from pypika import Query , Table , Field Creating Tables \u2691 The entry point for creating tables is pypika.Query.create_table , which is used with the class pypika.Column . As with selecting data, first the table should be specified. This can be either a string or a pypika.Table . Then the columns, and constraints.. stmt = Query \\ . create_table ( \"person\" ) \\ . columns ( Column ( \"id\" , \"INT\" , nullable = False ), Column ( \"first_name\" , \"VARCHAR(100)\" , nullable = False ), Column ( \"last_name\" , \"VARCHAR(100)\" , nullable = False ), Column ( \"phone_number\" , \"VARCHAR(20)\" , nullable = True ), Column ( \"status\" , \"VARCHAR(20)\" , nullable = False , default = ValueWrapper ( \"NEW\" )), Column ( \"date_of_birth\" , \"DATETIME\" )) \\ . unique ( \"last_name\" , \"first_name\" ) \\ . primary_key ( \"id\" ) This produces: CREATE TABLE \"person\" ( \"id\" INT NOT NULL , \"first_name\" VARCHAR ( 100 ) NOT NULL , \"last_name\" VARCHAR ( 100 ) NOT NULL , \"phone_number\" VARCHAR ( 20 ) NULL , \"status\" VARCHAR ( 20 ) NOT NULL DEFAULT 'NEW' , \"date_of_birth\" DATETIME , UNIQUE ( \"last_name\" , \"first_name\" ), PRIMARY KEY ( \"id\" ) ) It seems that they don't yet support the definition of FOREIGN KEYS when creating a new table. Inserting data \u2691 Data can be inserted into tables either by providing the values in the query or by selecting them through another query. By default, data can be inserted by providing values for all columns in the order that they are defined in the table. Insert with values \u2691 customers = Table ( 'customers' ) q = Query . into ( customers ) . insert ( 1 , 'Jane' , 'Doe' , 'jane@example.com' ) INSERT INTO customers VALUES ( 1 , 'Jane' , 'Doe' , 'jane@example.com' ) customers = Table ( 'customers' ) q = customers . insert ( 1 , 'Jane' , 'Doe' , 'jane@example.com' ) INSERT INTO customers VALUES ( 1 , 'Jane' , 'Doe' , 'jane@example.com' ) Multiple rows of data can be inserted either by chaining the insert function or passing multiple tuples as args. customers = Table ( 'customers' ) q = ( Query . into ( customers ) . insert ( 1 , \"Jane\" , \"Doe\" , \"jane@example.com\" ) . insert ( 2 , \"John\" , \"Doe\" , \"john@example.com\" ) ) customers = Table ( 'customers' ) q = Query . into ( customers ) . insert ( ( 1 , \"Jane\" , \"Doe\" , \"jane@example.com\" ), ( 2 , \"John\" , \"Doe\" , \"john@example.com\" ) ) INSERT INTO \"customers\" VALUES ( 1 , 'Jane' , 'Doe' , 'jane@example.com' ),( 2 , 'John' , 'Doe' , 'john@example.com' ) Insert with on Duplicate Key Update \u2691 customers = Table ( 'customers' ) q = Query . into ( customers ) \\ . insert ( 1 , 'Jane' , 'Doe' , 'jane@example.com' ) \\ . on_duplicate_key_update ( customers . email , Values ( customers . email )) INSERT INTO customers VALUES ( 1 , 'Jane' , 'Doe' , 'jane@example.com' ) ON DUPLICATE KEY UPDATE ` email `= VALUES ( ` email ` ) .on_duplicate_key_update works similar to .set for updating rows, additionally it provides the Values wrapper to update to the value specified in the INSERT clause. Insert from a SELECT Sub-query \u2691 INSERT INTO customers VALUES ( 1 , 'Jane' , 'Doe' , 'jane@example.com' ),( 2 , 'John' , 'Doe' , 'john@example.com' ) To specify the columns and the order, use the columns function. customers = Table ( 'customers' ) q = Query . into ( customers ) . columns ( 'id' , 'fname' , 'lname' ) . insert ( 1 , 'Jane' , 'Doe' ) INSERT INTO customers ( id , fname , lname ) VALUES ( 1 , 'Jane' , 'Doe' , 'jane@example.com' ) Inserting data with a query works the same as querying data with the additional call to the into method in the builder chain. customers , customers_backup = Tables ( 'customers' , 'customers_backup' ) q = Query . into ( customers_backup ) . from_ ( customers ) . select ( '*' ) INSERT INTO customers_backup SELECT * FROM customers customers , customers_backup = Tables ( 'customers' , 'customers_backup' ) q = Query . into ( customers_backup ) . columns ( 'id' , 'fname' , 'lname' ) . from_ ( customers ) . select ( customers . id , customers . fname , customers . lname ) INSERT INTO customers_backup SELECT \"id\" , \"fname\" , \"lname\" FROM customers The syntax for joining tables is the same as when selecting data customers , orders , orders_backup = Tables ( 'customers' , 'orders' , 'orders_backup' ) q = Query . into ( orders_backup ) . columns ( 'id' , 'address' , 'customer_fname' , 'customer_lname' ) . from_ ( customers ) . join ( orders ) . on ( orders . customer_id == customers . id ) . select ( orders . id , customers . fname , customers . lname ) INSERT INTO \"orders_backup\" ( \"id\" , \"address\" , \"customer_fname\" , \"customer_lname\" ) SELECT \"orders\" . \"id\" , \"customers\" . \"fname\" , \"customers\" . \"lname\" FROM \"customers\" JOIN \"orders\" ON \"orders\" . \"customer_id\" = \"customers\" . \"id\" [Updating \u2691 data]( https://pypika.readthedocs.io/en/latest/2_tutorial.html#updating-data ) PyPika allows update queries to be constructed with or without where clauses. customers = Table ( 'customers' ) Query . update ( customers ) . set ( customers . last_login , '2017-01-01 10:00:00' ) Query . update ( customers ) . set ( customers . lname , 'smith' ) . where ( customers . id == 10 ) UPDATE \"customers\" SET \"last_login\" = '2017-01-01 10:00:00' UPDATE \"customers\" SET \"lname\" = 'smith' WHERE \"id\" = 10 The syntax for joining tables is the same as when selecting data customers , profiles = Tables ( 'customers' , 'profiles' ) Query . update ( customers ) . join ( profiles ) . on ( profiles . customer_id == customers . id ) . set ( customers . lname , profiles . lname ) UPDATE \"customers\" JOIN \"profiles\" ON \"profiles\" . \"customer_id\" = \"customers\" . \"id\" SET \"customers\" . \"lname\" = \"profiles\" . \"lname\" Using pypika.Table alias to perform the update customers = Table ( 'customers' ) customers . update () . set ( customers . lname , 'smith' ) . where ( customers . id == 10 ) UPDATE \"customers\" SET \"lname\" = 'smith' WHERE \"id\" = 10 Using limit for performing update customers = Table ( 'customers' ) customers . update () . set ( customers . lname , 'smith' ) . limit ( 2 ) UPDATE \"customers\" SET \"lname\" = 'smith' LIMIT 2 Selecting Data \u2691 The entry point for building queries is pypika.Query . In order to select columns from a table, the table must first be added to the query. For simple queries with only one table, tables and columns can be references using strings. For more sophisticated queries a pypika.Table must be used. q = Query . from_ ( 'customers' ) . select ( 'id' , 'fname' , 'lname' , 'phone' ) To convert the query into raw SQL, it can be cast to a string. str ( q ) Alternatively, you can use the Query.get_sql() function: q . get_sql () The .select statement doesn't need to be after the .from_ statement. This is useful when composing a query in multiple steps, where you can do the .join before the .select . In simple queries like the above example, columns in the \u201cfrom\u201d table can be referenced by passing string names into the select query builder function. In more complex examples, the pypika.Table class should be used. Columns can be referenced as attributes on instances of pypika.Table . from pypika import Table , Query customers = Table ( 'customers' ) q = Query . from_ ( customers ) . select ( customers . id , customers . fname , customers . lname , customers . phone ) Both of the above examples result in the following SQL: SELECT id , fname , lname , phone FROM customers An alias for the table can be given using the .as_ function on pypika.Table . Table ( 'x_view_customers' ) . as_ ( 'customers' ) q = Query . from_ ( customers ) . select ( customers . id , customers . phone ) SELECT id , phone FROM x_view_customers customers An alias for the columns can also be given using the .as_ function on the columns. customers = Table ( 'customers' ) q = Query . from_ ( customers ) . select ( customers . id . as_ ( \"customer.id\" ), customers . fname . as_ ( \"customer.name\" ) ) SELECT \"id\" \"customer.id\" , \"fname\" \"customer.name\" FROM customers A schema can also be specified. Tables can be referenced as attributes on the schema. from pypika import Table , Query , Schema views = Schema ( 'views' ) q = Query . from_ ( views . customers ) . select ( customers . id , customers . phone ) SELECT id , phone FROM views . customers Also references to databases can be used. Schemas can be referenced as attributes on the database. from pypika import Table , Query , Database my_db = Database ( 'my_db' ) q = Query . from_ ( my_db . analytics . customers ) . select ( customers . id , customers . phone ) SELECT id , phone FROM my_db . analytics . customers Results can be ordered by using the following syntax: from pypika import Order Query . from_ ( 'customers' ) . select ( 'id' , 'fname' , 'lname' , 'phone' ) . orderby ( 'id' , order = Order . desc ) This results in the following SQL: SELECT \"id\" , \"fname\" , \"lname\" , \"phone\" FROM \"customers\" ORDER BY \"id\" DESC Filtering \u2691 Queries can be filtered with pypika.Criterion by using equality or inequality operators. customers = Table ( 'customers' ) q = Query . from_ ( customers ) . select ( customers . id , customers . fname , customers . lname , customers . phone ) . where ( customers . lname == 'Mustermann' ) SELECT id , fname , lname , phone FROM customers WHERE lname = 'Mustermann' Query methods such as select, where, groupby, and orderby can be called multiple times. Multiple calls to the where method will add additional conditions as: customers = Table ( 'customers' ) q = Query . from_ ( customers ) . select ( customers . id , customers . fname , customers . lname , customers . phone ) . where ( customers . fname == 'Max' ) . where ( customers . lname == 'Mustermann' ) SELECT id , fname , lname , phone FROM customers WHERE fname = 'Max' AND lname = 'Mustermann' Filters such as IN and BETWEEN are also supported. customers = Table ( 'customers' ) q = Query . from_ ( customers ) . select ( customers . id , customers . fname ) . where ( customers . age [ 18 : 65 ] & customers . status . isin ([ 'new' , 'active' ]) ) SELECT id , fname FROM customers WHERE age BETWEEN 18 AND 65 AND status IN ( 'new' , 'active' ) Filtering with complex criteria can be created using boolean symbols & , | , and ^ . AND customers = Table ( 'customers' ) q = Query . from_ ( customers ) . select ( customers . id , customers . fname , customers . lname , customers . phone ) . where ( ( customers . age >= 18 ) & ( customers . lname == 'Mustermann' ) ) SELECT id , fname , lname , phone FROM customers WHERE age >= 18 AND lname = 'Mustermann' OR customers = Table ( 'customers' ) q = Query . from_ ( customers ) . select ( customers . id , customers . fname , customers . lname , customers . phone ) . where ( ( customers . age >= 18 ) | ( customers . lname == 'Mustermann' ) ) SELECT id , fname , lname , phone FROM customers WHERE age >= 18 OR lname = 'Mustermann' XOR customers = Table ( 'customers' ) q = Query . from_ ( customers ) . select ( customers . id , customers . fname , customers . lname , customers . phone ) . where ( ( customers . age >= 18 ) ^ customers . is_registered ) SELECT id , fname , lname , phone FROM customers WHERE age >= 18 XOR is_registered Using the REGEXP filter Pypika supports regex, but if you're using sqlite3 you need to configure the connection to the database . Joining tables and subqueries \u2691 Tables and subqueries can be joined to any query using the Query.join() method. Joins can be performed with either a USING or ON clauses. The USING clause can be used when both tables/subqueries contain the same field and the ON clause can be used with a criterion. To perform a join, ...join() can be chained but then must be followed immediately by ...on(<criterion>) or ...using(*field) . Join Types \u2691 All join types are supported by PyPika. Query \\ . from_ ( base_table ) ... . join ( join_table , JoinType . left ) ... Query \\ . from_ ( base_table ) ... . left_join ( join_table ) \\ . right_join ( join_table ) \\ . inner_join ( join_table ) \\ . outer_join ( join_table ) \\ . cross_join ( join_table ) \\ ... Example of a join using ON \u2691 history , customers = Tables ( 'history' , 'customers' ) q = Query \\ . from_ ( history ) \\ . join ( customers ) \\ . on ( history . customer_id == customers . id ) \\ . select ( history . star ) \\ . where ( customers . id == 5 ) SELECT \"history\" . * FROM \"history\" JOIN \"customers\" ON \"history\" . \"customer_id\" = \"customers\" . \"id\" WHERE \"customers\" . \"id\" = 5 Example of a join using ON_FIELD \u2691 As a shortcut, the Query.join().on_field() function is provided for joining the (first) table in the FROM clause with the joined table when the field name(s) are the same in both tables. history , customers = Tables ( 'history' , 'customers' ) q = Query \\ . from_ ( history ) \\ . join ( customers ) \\ . on_field ( 'customer_id' , 'group' ) \\ . select ( history . star ) \\ . where ( customers . group == 'A' ) SELECT \"history\" . * FROM \"history\" JOIN \"customers\" ON \"history\" . \"customer_id\" = \"customers\" . \"customer_id\" AND \"history\" . \"group\" = \"customers\" . \"group\" WHERE \"customers\" . \"group\" = 'A' Example of a join using USING \u2691 history , customers = Tables ( 'history' , 'customers' ) q = Query \\ . from_ ( history ) \\ . join ( customers ) \\ . using ( 'customer_id' ) \\ . select ( history . star ) \\ . where ( customers . id == 5 ) SELECT \"history\" . * FROM \"history\" JOIN \"customers\" USING \"customer_id\" WHERE \"customers\" . \"id\" = 5 Example of a correlated subquery in the SELECT \u2691 history , customers = Tables ( 'history' , 'customers' ) last_purchase_at = Query . from_ ( history ) . select ( history . purchase_at ) . where ( history . customer_id == customers . customer_id ) . orderby ( history . purchase_at , order = Order . desc ) . limit ( 1 ) q = Query . from_ ( customers ) . select ( customers . id , last_purchase_at . _as ( 'last_purchase_at' ) ) SELECT \"id\" , ( SELECT \"history\" . \"purchase_at\" FROM \"history\" WHERE \"history\" . \"customer_id\" = \"customers\" . \"customer_id\" ORDER BY \"history\" . \"purchase_at\" DESC LIMIT 1 ) \"last_purchase_at\" FROM \"customers\" Deleting data \u2691 Query . from_ ( table ) . delete () . where ( table . id == id ) References \u2691 Docs Source","title":"Pypika"},{"location":"coding/python/pypika/#installation","text":"pip install pypika","title":"Installation"},{"location":"coding/python/pypika/#usage","text":"The main classes in pypika are pypika.Query , pypika.Table , and pypika.Field . from pypika import Query , Table , Field","title":"Usage"},{"location":"coding/python/pypika/#creating-tables","text":"The entry point for creating tables is pypika.Query.create_table , which is used with the class pypika.Column . As with selecting data, first the table should be specified. This can be either a string or a pypika.Table . Then the columns, and constraints.. stmt = Query \\ . create_table ( \"person\" ) \\ . columns ( Column ( \"id\" , \"INT\" , nullable = False ), Column ( \"first_name\" , \"VARCHAR(100)\" , nullable = False ), Column ( \"last_name\" , \"VARCHAR(100)\" , nullable = False ), Column ( \"phone_number\" , \"VARCHAR(20)\" , nullable = True ), Column ( \"status\" , \"VARCHAR(20)\" , nullable = False , default = ValueWrapper ( \"NEW\" )), Column ( \"date_of_birth\" , \"DATETIME\" )) \\ . unique ( \"last_name\" , \"first_name\" ) \\ . primary_key ( \"id\" ) This produces: CREATE TABLE \"person\" ( \"id\" INT NOT NULL , \"first_name\" VARCHAR ( 100 ) NOT NULL , \"last_name\" VARCHAR ( 100 ) NOT NULL , \"phone_number\" VARCHAR ( 20 ) NULL , \"status\" VARCHAR ( 20 ) NOT NULL DEFAULT 'NEW' , \"date_of_birth\" DATETIME , UNIQUE ( \"last_name\" , \"first_name\" ), PRIMARY KEY ( \"id\" ) ) It seems that they don't yet support the definition of FOREIGN KEYS when creating a new table.","title":"Creating Tables"},{"location":"coding/python/pypika/#inserting-data","text":"Data can be inserted into tables either by providing the values in the query or by selecting them through another query. By default, data can be inserted by providing values for all columns in the order that they are defined in the table.","title":"Inserting data"},{"location":"coding/python/pypika/#insert-with-values","text":"customers = Table ( 'customers' ) q = Query . into ( customers ) . insert ( 1 , 'Jane' , 'Doe' , 'jane@example.com' ) INSERT INTO customers VALUES ( 1 , 'Jane' , 'Doe' , 'jane@example.com' ) customers = Table ( 'customers' ) q = customers . insert ( 1 , 'Jane' , 'Doe' , 'jane@example.com' ) INSERT INTO customers VALUES ( 1 , 'Jane' , 'Doe' , 'jane@example.com' ) Multiple rows of data can be inserted either by chaining the insert function or passing multiple tuples as args. customers = Table ( 'customers' ) q = ( Query . into ( customers ) . insert ( 1 , \"Jane\" , \"Doe\" , \"jane@example.com\" ) . insert ( 2 , \"John\" , \"Doe\" , \"john@example.com\" ) ) customers = Table ( 'customers' ) q = Query . into ( customers ) . insert ( ( 1 , \"Jane\" , \"Doe\" , \"jane@example.com\" ), ( 2 , \"John\" , \"Doe\" , \"john@example.com\" ) ) INSERT INTO \"customers\" VALUES ( 1 , 'Jane' , 'Doe' , 'jane@example.com' ),( 2 , 'John' , 'Doe' , 'john@example.com' )","title":"Insert with values"},{"location":"coding/python/pypika/#insert-with-on-duplicate-key-update","text":"customers = Table ( 'customers' ) q = Query . into ( customers ) \\ . insert ( 1 , 'Jane' , 'Doe' , 'jane@example.com' ) \\ . on_duplicate_key_update ( customers . email , Values ( customers . email )) INSERT INTO customers VALUES ( 1 , 'Jane' , 'Doe' , 'jane@example.com' ) ON DUPLICATE KEY UPDATE ` email `= VALUES ( ` email ` ) .on_duplicate_key_update works similar to .set for updating rows, additionally it provides the Values wrapper to update to the value specified in the INSERT clause.","title":"Insert with on Duplicate Key Update"},{"location":"coding/python/pypika/#insert-from-a-select-sub-query","text":"INSERT INTO customers VALUES ( 1 , 'Jane' , 'Doe' , 'jane@example.com' ),( 2 , 'John' , 'Doe' , 'john@example.com' ) To specify the columns and the order, use the columns function. customers = Table ( 'customers' ) q = Query . into ( customers ) . columns ( 'id' , 'fname' , 'lname' ) . insert ( 1 , 'Jane' , 'Doe' ) INSERT INTO customers ( id , fname , lname ) VALUES ( 1 , 'Jane' , 'Doe' , 'jane@example.com' ) Inserting data with a query works the same as querying data with the additional call to the into method in the builder chain. customers , customers_backup = Tables ( 'customers' , 'customers_backup' ) q = Query . into ( customers_backup ) . from_ ( customers ) . select ( '*' ) INSERT INTO customers_backup SELECT * FROM customers customers , customers_backup = Tables ( 'customers' , 'customers_backup' ) q = Query . into ( customers_backup ) . columns ( 'id' , 'fname' , 'lname' ) . from_ ( customers ) . select ( customers . id , customers . fname , customers . lname ) INSERT INTO customers_backup SELECT \"id\" , \"fname\" , \"lname\" FROM customers The syntax for joining tables is the same as when selecting data customers , orders , orders_backup = Tables ( 'customers' , 'orders' , 'orders_backup' ) q = Query . into ( orders_backup ) . columns ( 'id' , 'address' , 'customer_fname' , 'customer_lname' ) . from_ ( customers ) . join ( orders ) . on ( orders . customer_id == customers . id ) . select ( orders . id , customers . fname , customers . lname ) INSERT INTO \"orders_backup\" ( \"id\" , \"address\" , \"customer_fname\" , \"customer_lname\" ) SELECT \"orders\" . \"id\" , \"customers\" . \"fname\" , \"customers\" . \"lname\" FROM \"customers\" JOIN \"orders\" ON \"orders\" . \"customer_id\" = \"customers\" . \"id\"","title":"Insert from a SELECT Sub-query"},{"location":"coding/python/pypika/#updating","text":"data]( https://pypika.readthedocs.io/en/latest/2_tutorial.html#updating-data ) PyPika allows update queries to be constructed with or without where clauses. customers = Table ( 'customers' ) Query . update ( customers ) . set ( customers . last_login , '2017-01-01 10:00:00' ) Query . update ( customers ) . set ( customers . lname , 'smith' ) . where ( customers . id == 10 ) UPDATE \"customers\" SET \"last_login\" = '2017-01-01 10:00:00' UPDATE \"customers\" SET \"lname\" = 'smith' WHERE \"id\" = 10 The syntax for joining tables is the same as when selecting data customers , profiles = Tables ( 'customers' , 'profiles' ) Query . update ( customers ) . join ( profiles ) . on ( profiles . customer_id == customers . id ) . set ( customers . lname , profiles . lname ) UPDATE \"customers\" JOIN \"profiles\" ON \"profiles\" . \"customer_id\" = \"customers\" . \"id\" SET \"customers\" . \"lname\" = \"profiles\" . \"lname\" Using pypika.Table alias to perform the update customers = Table ( 'customers' ) customers . update () . set ( customers . lname , 'smith' ) . where ( customers . id == 10 ) UPDATE \"customers\" SET \"lname\" = 'smith' WHERE \"id\" = 10 Using limit for performing update customers = Table ( 'customers' ) customers . update () . set ( customers . lname , 'smith' ) . limit ( 2 ) UPDATE \"customers\" SET \"lname\" = 'smith' LIMIT 2","title":"[Updating"},{"location":"coding/python/pypika/#selecting-data","text":"The entry point for building queries is pypika.Query . In order to select columns from a table, the table must first be added to the query. For simple queries with only one table, tables and columns can be references using strings. For more sophisticated queries a pypika.Table must be used. q = Query . from_ ( 'customers' ) . select ( 'id' , 'fname' , 'lname' , 'phone' ) To convert the query into raw SQL, it can be cast to a string. str ( q ) Alternatively, you can use the Query.get_sql() function: q . get_sql () The .select statement doesn't need to be after the .from_ statement. This is useful when composing a query in multiple steps, where you can do the .join before the .select . In simple queries like the above example, columns in the \u201cfrom\u201d table can be referenced by passing string names into the select query builder function. In more complex examples, the pypika.Table class should be used. Columns can be referenced as attributes on instances of pypika.Table . from pypika import Table , Query customers = Table ( 'customers' ) q = Query . from_ ( customers ) . select ( customers . id , customers . fname , customers . lname , customers . phone ) Both of the above examples result in the following SQL: SELECT id , fname , lname , phone FROM customers An alias for the table can be given using the .as_ function on pypika.Table . Table ( 'x_view_customers' ) . as_ ( 'customers' ) q = Query . from_ ( customers ) . select ( customers . id , customers . phone ) SELECT id , phone FROM x_view_customers customers An alias for the columns can also be given using the .as_ function on the columns. customers = Table ( 'customers' ) q = Query . from_ ( customers ) . select ( customers . id . as_ ( \"customer.id\" ), customers . fname . as_ ( \"customer.name\" ) ) SELECT \"id\" \"customer.id\" , \"fname\" \"customer.name\" FROM customers A schema can also be specified. Tables can be referenced as attributes on the schema. from pypika import Table , Query , Schema views = Schema ( 'views' ) q = Query . from_ ( views . customers ) . select ( customers . id , customers . phone ) SELECT id , phone FROM views . customers Also references to databases can be used. Schemas can be referenced as attributes on the database. from pypika import Table , Query , Database my_db = Database ( 'my_db' ) q = Query . from_ ( my_db . analytics . customers ) . select ( customers . id , customers . phone ) SELECT id , phone FROM my_db . analytics . customers Results can be ordered by using the following syntax: from pypika import Order Query . from_ ( 'customers' ) . select ( 'id' , 'fname' , 'lname' , 'phone' ) . orderby ( 'id' , order = Order . desc ) This results in the following SQL: SELECT \"id\" , \"fname\" , \"lname\" , \"phone\" FROM \"customers\" ORDER BY \"id\" DESC","title":"Selecting Data"},{"location":"coding/python/pypika/#filtering","text":"Queries can be filtered with pypika.Criterion by using equality or inequality operators. customers = Table ( 'customers' ) q = Query . from_ ( customers ) . select ( customers . id , customers . fname , customers . lname , customers . phone ) . where ( customers . lname == 'Mustermann' ) SELECT id , fname , lname , phone FROM customers WHERE lname = 'Mustermann' Query methods such as select, where, groupby, and orderby can be called multiple times. Multiple calls to the where method will add additional conditions as: customers = Table ( 'customers' ) q = Query . from_ ( customers ) . select ( customers . id , customers . fname , customers . lname , customers . phone ) . where ( customers . fname == 'Max' ) . where ( customers . lname == 'Mustermann' ) SELECT id , fname , lname , phone FROM customers WHERE fname = 'Max' AND lname = 'Mustermann' Filters such as IN and BETWEEN are also supported. customers = Table ( 'customers' ) q = Query . from_ ( customers ) . select ( customers . id , customers . fname ) . where ( customers . age [ 18 : 65 ] & customers . status . isin ([ 'new' , 'active' ]) ) SELECT id , fname FROM customers WHERE age BETWEEN 18 AND 65 AND status IN ( 'new' , 'active' ) Filtering with complex criteria can be created using boolean symbols & , | , and ^ . AND customers = Table ( 'customers' ) q = Query . from_ ( customers ) . select ( customers . id , customers . fname , customers . lname , customers . phone ) . where ( ( customers . age >= 18 ) & ( customers . lname == 'Mustermann' ) ) SELECT id , fname , lname , phone FROM customers WHERE age >= 18 AND lname = 'Mustermann' OR customers = Table ( 'customers' ) q = Query . from_ ( customers ) . select ( customers . id , customers . fname , customers . lname , customers . phone ) . where ( ( customers . age >= 18 ) | ( customers . lname == 'Mustermann' ) ) SELECT id , fname , lname , phone FROM customers WHERE age >= 18 OR lname = 'Mustermann' XOR customers = Table ( 'customers' ) q = Query . from_ ( customers ) . select ( customers . id , customers . fname , customers . lname , customers . phone ) . where ( ( customers . age >= 18 ) ^ customers . is_registered ) SELECT id , fname , lname , phone FROM customers WHERE age >= 18 XOR is_registered Using the REGEXP filter Pypika supports regex, but if you're using sqlite3 you need to configure the connection to the database .","title":"Filtering"},{"location":"coding/python/pypika/#joining-tables-and-subqueries","text":"Tables and subqueries can be joined to any query using the Query.join() method. Joins can be performed with either a USING or ON clauses. The USING clause can be used when both tables/subqueries contain the same field and the ON clause can be used with a criterion. To perform a join, ...join() can be chained but then must be followed immediately by ...on(<criterion>) or ...using(*field) .","title":"Joining tables and subqueries"},{"location":"coding/python/pypika/#join-types","text":"All join types are supported by PyPika. Query \\ . from_ ( base_table ) ... . join ( join_table , JoinType . left ) ... Query \\ . from_ ( base_table ) ... . left_join ( join_table ) \\ . right_join ( join_table ) \\ . inner_join ( join_table ) \\ . outer_join ( join_table ) \\ . cross_join ( join_table ) \\ ...","title":"Join Types"},{"location":"coding/python/pypika/#example-of-a-join-using-on","text":"history , customers = Tables ( 'history' , 'customers' ) q = Query \\ . from_ ( history ) \\ . join ( customers ) \\ . on ( history . customer_id == customers . id ) \\ . select ( history . star ) \\ . where ( customers . id == 5 ) SELECT \"history\" . * FROM \"history\" JOIN \"customers\" ON \"history\" . \"customer_id\" = \"customers\" . \"id\" WHERE \"customers\" . \"id\" = 5","title":"Example of a join using ON"},{"location":"coding/python/pypika/#example-of-a-join-using-on_field","text":"As a shortcut, the Query.join().on_field() function is provided for joining the (first) table in the FROM clause with the joined table when the field name(s) are the same in both tables. history , customers = Tables ( 'history' , 'customers' ) q = Query \\ . from_ ( history ) \\ . join ( customers ) \\ . on_field ( 'customer_id' , 'group' ) \\ . select ( history . star ) \\ . where ( customers . group == 'A' ) SELECT \"history\" . * FROM \"history\" JOIN \"customers\" ON \"history\" . \"customer_id\" = \"customers\" . \"customer_id\" AND \"history\" . \"group\" = \"customers\" . \"group\" WHERE \"customers\" . \"group\" = 'A'","title":"Example of a join using ON_FIELD"},{"location":"coding/python/pypika/#example-of-a-join-using-using","text":"history , customers = Tables ( 'history' , 'customers' ) q = Query \\ . from_ ( history ) \\ . join ( customers ) \\ . using ( 'customer_id' ) \\ . select ( history . star ) \\ . where ( customers . id == 5 ) SELECT \"history\" . * FROM \"history\" JOIN \"customers\" USING \"customer_id\" WHERE \"customers\" . \"id\" = 5","title":"Example of a join using USING"},{"location":"coding/python/pypika/#example-of-a-correlated-subquery-in-the-select","text":"history , customers = Tables ( 'history' , 'customers' ) last_purchase_at = Query . from_ ( history ) . select ( history . purchase_at ) . where ( history . customer_id == customers . customer_id ) . orderby ( history . purchase_at , order = Order . desc ) . limit ( 1 ) q = Query . from_ ( customers ) . select ( customers . id , last_purchase_at . _as ( 'last_purchase_at' ) ) SELECT \"id\" , ( SELECT \"history\" . \"purchase_at\" FROM \"history\" WHERE \"history\" . \"customer_id\" = \"customers\" . \"customer_id\" ORDER BY \"history\" . \"purchase_at\" DESC LIMIT 1 ) \"last_purchase_at\" FROM \"customers\"","title":"Example of a correlated subquery in the SELECT"},{"location":"coding/python/pypika/#deleting-data","text":"Query . from_ ( table ) . delete () . where ( table . id == id )","title":"Deleting data"},{"location":"coding/python/pypika/#references","text":"Docs Source","title":"References"},{"location":"coding/python/pytest/","text":"pytest is a Python framework to makes it easy to write small tests, yet scales to support complex functional testing for applications and libraries. Pytest stands out over other test frameworks in: Simple tests are simple to write in pytest. Complex tests are still simple to write. Tests are easy to read. You can get started in seconds. You use assert to fail a test, not things like self.assertEqual() or self.assertLessThan() . Just assert . You can use pytest to run tests written for unittest or nose. You can use this cookiecutter template to create a python project with pytest already configured. Install \u2691 pip install pytest Usage \u2691 Run in the project directory. pytest If you need more information run it with -v . Pytest automatically finds which tests to run in a phase called test discovery . It will get the tests that match one of the following conditions: Test files that are named test_{{ something }}.py or {{ something }}_test.py . Test methods and functions named test_{{ something }} . Test classes named Test{{ Something }} . There are several possible outcomes of a test function: PASSED (.) : The test ran successfully. FAILED (F) : The test did not run usccessfully (or XPASS + strict). SKIPPED (s) : The test was skipped. You can tell pytest to skip a test by using enter the @pytest.mark.skip() or pytest.mark.skipif() decorators. xfail (x) : The test was not supposed to pass, ran, and failed. You can tell pytest that a test is expected to fail by using the @pytest.mark.xfail() decorator. XPASS (X) : The tests was not supposed to pass, ran, and passed. ERROR (E) : An exception happened outside of the test function, in either a fixture or a hook function. Pytest supports several cool flags like: -k EXPRESSION : Used to select a subset of tests to run. For example pytest -k \"asdict or defaults\" will run both test_asdict() and test_defaults() . --lf or --last-failed : Just run the tests that have failed in the previous run. -x , or --exitfirst : Exit on first failed test. -l or --showlocals : Print out the local variables in a test if the test fails. -s Allows any output that normally would be printed to stdout to actually be printed to stdout . It's an alias of --capture=no , so the output is not captured when the tests are run, which is the default behavior. This is useful to debug with print() statements. --durations=N : It reports the slowest N number of tests/setups/teardowns after the test run. If you pass in --durations=0 , it reports everything in order of slowest to fastest. --setup-show : Show the fixtures in use. Fixtures \u2691 Fixtures are functions that are run by pytest before (and sometimes after) the actual test functions. You can use fixtures to get a data set for the tests to work on, or use them to get a system into a known state before running a test. They are also used to get data ready for multiple tests. Here's a simple fixture that returns a number: import pytest @pytest . fixture () def some_data () \"\"\" Return answer to the ultimate question \"\"\" return 42 def test_some_data ( some_data ): \"\"\" Use fixture return value in a test\"\"\" assert some_data == 42 The @pytest.fixture() decorator is used to tell pytest that a function is a fixture.When you include the fixture name in the parameter list of a test function,pytest knows to run it before running the test. Fixtures can do work, and can also return data to the test function. The test test_some_data() has the name of the fixture, some_data, as a parameter.pytest will see this and look for a fixture with this name. Naming is significant in pytest. pytest will look in the module of the test for a fixture of that name. If the function is defined in the same file as where it's being used pylint will raise an W0621: Redefining name %r from outer scope (line %s) error. To solve it either move the fixture to other file or name the decorated function fixture_<fixturename> and then use @pytest.fixture(name='<fixturename>') . Sharing fixtures through conftest.py \u2691 You can put fixtures into individual test files, but to share fixtures among multiple test files, you need to use a conftest.py file somewhere centrally located for all of the tests. Additionally you can have conftest.py files in subdirectories of the top tests directory. If you do, fixtures defined in these lower level conftest.py files will be available to tests in that directory and subdirectories. Although conftest.py is a Python module, it should not be imported by test files. The file gets read by pytest, and is considered a local plugin . Another option is to save the fixtures in a file by creating a local pytest plugin . File: tests/unit/conftest.py pytest_plugins = [ \"tests.unit.fixtures.some_stuff\" , ] File: tests/unit/fixtures/some_stuff.py import pytest @pytest . fixture def foo (): return 'foobar' Specifying fixture scope \u2691 Fixtures include an optional parameter called scope, which controls how often a fixture gets set up and torn down. The scope parameter to @pytest.fixture() can have the values of function, class, module, or session. Here\u2019s a rundown of each scope value: scope='function' : Run once per test function. The setup portion is run before each test using the fixture. The teardown portion is run after each test using the fixture. This is the default scope used when no scope parameter is specified. scope='class' : Run once per test class, regardless of how many test methods are in the class. scope='module' : Run once per module, regardless of how many test functions or methods or other fixtures in the module use it. scope='session' Run once per session. All test methods and functions using a fixture of session scope share one setup and teardown call. Useful Fixtures \u2691 The tmpdir fixture \u2691 You can use the tmpdir fixture which will provide a temporary directory unique to the test invocation, created in the base temporary directory. tmpdir is a py.path.local object which offers os.path methods and more. Here is an example test usage: File: test_tmpdir.py from py._path.local import LocalPath def test_create_file ( tmpdir : LocalPath ): p = tmpdir . mkdir ( \"sub\" ) . join ( \"hello.txt\" ) p . write ( \"content\" ) assert p . read () == \"content\" assert len ( tmpdir . listdir ()) == 1 assert 0 The tmpdir fixture has a scope of function so you can't make a session directory. Instead use the tmpdir_factory fixture. from _pytest.tmpdir import TempdirFactory @pytest . fixture ( scope = \"session\" ) def image_file ( tmpdir_factory : TempdirFactory ): img = compute_expensive_image () fn = tmpdir_factory . mktemp ( \"data\" ) . join ( \"img.png\" ) img . save ( str ( fn )) return fn def test_histogram ( image_file ): img = load_image ( image_file ) # compute and test histogram Make a subdirectory \u2691 p = tmpdir . mkdir ( \"sub\" ) . join ( \"hello.txt\" ) The caplog fixture \u2691 pytest captures log messages of level WARNING or above automatically and displays them in their own section for each failed test in the same manner as captured stdout and stderr. You can change the default logging level in the pytest configuration: File: pytest.ini [pytest] log_level = debug Although it may not be a good idea in most cases. It's better to change the log level in the tests that need a lower level. All the logs sent to the logger during the test run are available on the fixture in the form of both the logging.LogRecord instances and the final log text. This is useful for when you want to assert on the contents of a message: from _pytest.logging import LogCaptureFixture def test_baz ( caplog : LogCaptureFixture ): func_under_test () for record in caplog . records : assert record . levelname != \"CRITICAL\" assert \"wally\" not in caplog . text You can also resort to record_tuples if all you want to do is to ensure that certain messages have been logged under a given logger name with a given severity and message: def test_foo ( caplog ): logging . getLogger () . info ( \"boo %s \" , \"arg\" ) assert ( \"root\" , logging . INFO , \"boo arg\" ) in caplog . record_tuples You can call caplog.clear() to reset the captured log records in a test. Change the log level \u2691 Inside tests it's possible to change the log level for the captured log messages. def test_foo ( caplog ): caplog . set_level ( logging . INFO ) pass The capsys fixture \u2691 The capsys builtin fixture provides two bits of functionality: it allows you to retrieve stdout and stderr from some code, and it disables output capture temporarily. Suppose you have a function to print a greeting to stdout: def greeting ( name ): print ( f 'Hi, { name } ' ) You can test the output by using capsys . from _pytest.capture import CaptureFixture def test_greeting ( capsys : CaptureFixture [ Any ]): greeting ( 'Earthling' ) out , err = capsys . readouterr () assert out == 'Hi, Earthling \\n ' assert err == '' The return value is whatever has been captured since the beginning of the function, or from the last time it was called. freezegun \u2691 freezegun lets you freeze time in both the test and fixtures. Install \u2691 pip install pytest-freezegun Usage \u2691 Freeze time by using the freezer fixture: if TYPE_CHECKING : from freezegun.api import FrozenDateTimeFactory def test_frozen_date ( freezer : FrozenDateTimeFactory ): now = datetime . now () time . sleep ( 1 ) later = datetime . now () assert now == later This can then be used to move time: def test_moving_date ( freezer ): now = datetime . now () freezer . move_to ( '2017-05-20' ) later = datetime . now () assert now != later You can also pass arguments to freezegun by using the freeze_time mark: @pytest . mark . freeze_time ( '2017-05-21' ) def test_current_date (): assert date . today () == date ( 2017 , 5 , 21 ) The freezer fixture and freeze_time mark can be used together, and they work with other fixtures: @pytest . fixture def current_date (): return date . today () @pytest . mark . freeze_time def test_changing_date ( current_date , freezer ): freezer . move_to ( '2017-05-20' ) assert current_date == date ( 2017 , 5 , 20 ) freezer . move_to ( '2017-05-21' ) assert current_date == date ( 2017 , 5 , 21 ) They can also be used in class-based tests: class TestDate : @pytest . mark . freeze_time def test_changing_date ( self , current_date , freezer ): freezer . move_to ( '2017-05-20' ) assert current_date == date ( 2017 , 5 , 20 ) freezer . move_to ( '2017-05-21' ) assert current_date == date ( 2017 , 5 , 21 ) Customize nested fixtures \u2691 Sometimes you need to tweak your fixtures so they can be used in different tests. As usual, there are different solutions to the same problem. !!! note \"TL;DR: For simple cases parametrize your fixtures or use parametrization to override the default valued fixture . As your test suite get's more complex migrate to pytest-case .\" Let's say you're running along merrily with some fixtures that create database objects for you: @pytest . fixture def supplier ( db ): s = Supplier ( ref = random_ref (), name = random_name (), country = \"US\" , ) db . add ( s ) yield s db . remove ( s ) @pytest . fixture () def product ( db , supplier ): p = Product ( ref = random_ref (), name = random_name (), supplier = supplier , net_price = 9.99 , ) db . add ( p ) yield p db . remove ( p ) And now you're writing a new test and you suddenly realize you need to customize your default \"supplier\" fixture: def test_US_supplier_has_total_price_equal_net_price ( product ): assert product . total_price == product . net_price def test_EU_supplier_has_total_price_including_VAT ( supplier , product ): supplier . country = \"FR\" # oh, this doesn't work assert product . total_price == product . net_price * 1.2 There are different ways to modify your fixtures Add more fixtures \u2691 We can just create more fixtures, and try to do a bit of DRY by extracting out common logic: def _default_supplier (): return Supplier ( ref = random_ref (), name = random_name (), ) @pytest . fixture def us_supplier ( db ): s = _default_supplier () s . country = \"US\" db . add ( s ) yield s db . remove ( s ) @pytest . fixture def eu_supplier ( db ): s = _default_supplier () s . country = \"FR\" db . add ( s ) yield s db . remove ( s ) That's just one way you could do it, maybe you can figure out ways to reduce the duplication of the db.add() stuff as well, but you are going to have a different, named fixture for each customization of Supplier, and eventually you may decide that doesn't scale. Use factory fixtures \u2691 Instead of a fixture returning an object directly, it can return a function that creates an object, and that function can take arguments: @pytest . fixture def make_supplier ( db ): s = Supplier ( ref = random_ref (), name = random_name (), ) def _make_supplier ( country ): s . country = country db . add ( s ) return s yield _make_supplier db . remove ( s ) The problem with this is that, once you start, you tend to have to go all the way, and make all of your fixture hierarchy into factory functions: def test_EU_supplier_has_total_price_including_VAT ( make_supplier , product ): supplier = make_supplier ( country = \"FR\" ) product . supplier = supplier # OH, now this doesn't work, because it's too late again assert product . total_price == product . net_price * 1.2 And so... @pytest . fixture def make_product ( db ): p = Product ( ref = random_ref (), name = random_name (), ) def _make_product ( supplier ): p . supplier = supplier db . add ( p ) return p yield _make_product db . remove ( p ) def test_EU_supplier_has_total_price_including_VAT ( make_supplier , make_product ): supplier = make_supplier ( country = \"FR\" ) product = make_product ( supplier = supplier ) assert product . total_price == product . net_price * 1.2 That works, but firstly now everything is a factory-fixture, which makes them more convoluted, and secondly, your tests are filling up with extra calls to make_things , and you're having to embed some of the domain knowledge of what-depends-on-what into your tests as well as your fixtures. Ugly! Parametrize your fixtures \u2691 You can also parametrize your fixtures . @pytest . fixture ( params = [ 'US' , 'FR' ]) def supplier ( db , request ): s = Supplier ( ref = random_ref (), name = random_name (), country = request . param ) db . add ( s ) yield s db . remove ( s ) Now any test that depends on supplier, directly or indirectly, will be run twice, once with supplier.country = US and once with FR . That's really cool for checking that a given piece of logic works in a variety of different cases, but it's not really ideal in our case. We have to build a bunch of if logic into our tests: def test_US_supplier_has_no_VAT_but_EU_supplier_has_total_price_including_VAT ( product ): # this test is magically run twice, but: if product . supplier . country == 'US' : assert product . total_price == product . net_price if product . supplier . country == 'FR' : assert product . total_price == product . net_price * 1.2 So that's ugly, and on top of that, now every single test that depends (indirectly) on supplier gets run twice, and some of those extra test runs may be totally irrelevant to what the country is. Use pytest parametrization to override the default valued fixtures \u2691 We introduce an extra fixture that holds a default value for the country field: @pytest . fixture () def country (): return \"US\" @pytest . fixture def supplier ( db , country ): s = Supplier ( ref = random_ref (), name = random_name (), country = country , ) db . add ( s ) yield s db . remove ( s ) And then in the tests that need to change it, we can use parametrize to override the default value of country, even though the country fixture isn't explicitly named in that test: @pytest . mark . parametrize ( 'country' , [ \"US\" ]) def test_US_supplier_has_total_price_equal_net_price ( product ): assert product . total_price == product . net_price @pytest . mark . parametrize ( 'country' , [ \"EU\" ]) def test_EU_supplier_has_total_price_including_VAT ( product ): assert product . total_price == product . net_price * 1.2 The only problem is that you're now likely to build a implicit dependencies where the only way to find out what's actually happening is to spend ages spelunking in conftest.py. Use pytest-case \u2691 pytest-case gives a lot of power when it comes to tweaking the fixtures and parameterizations. Check that file for further information. Use a fixture more than once in a function \u2691 One solution is to make your fixture return a factory instead of the resource directly: @pytest . fixture ( name = 'make_user' ) def make_user_ (): created = [] def make_user (): u = models . User () u . commit () created . append ( u ) return u yield make_user for u in created : u . delete () def test_two_users ( make_user ): user1 = make_user () user2 = make_user () # test them # you can even have the normal fixture when you only need a single user @pytest . fixture def user ( make_user ): return make_user () def test_one_user ( user ): # test him/her Marks \u2691 Pytest marks can be used to group tests. It can be useful to: slow Mark the tests that are slow. secondary Mart the tests that use functionality that is being tested in the same file. To mark a test, use the @pytest.mark decorator. For example: @pytest . mark . slow def test_really_slow_test (): pass Pytest requires you to register your marks, do so in the pytest.ini file [pytest] markers = slow: marks tests as slow (deselect with '-m \"not slow\"') secondary: mark tests that use functionality tested in the same file (deselect with '-m \"not secondary\"') Snippets \u2691 Mocking sys.exit \u2691 with pytest . raises ( SystemExit ): # Code to test Testing exceptions with pytest \u2691 def test_value_error_is_raised (): with pytest . raises ( ValueError , match = \"invalid literal for int() with base 10: 'a'\" ): int ( 'a' ) Excluding code from coverage \u2691 You may have code in your project that you know won't be executed, and you want to tell coverage.py to ignore it. For example, if you have some code in abstract classes that is going to be tested on the subclasses, you can ignore it with # pragma: no cover . If you want other code to be excluded , for example the statements inside the if TYPE_CHECKING: add to your pyproject.toml : [tool.coverage.report] exclude_lines = [ # Have to re-enable the standard pragma 'pragma: no cover' , # Type checking can not be tested 'if TYPE_CHECKING:' , ] Running tests in parallel \u2691 pytest-xdist makes it possible to run the tests in parallel, useful when the test suit is large or when the tests are slow. Installation \u2691 pip install pytest-xdist Usage \u2691 pytest -n 4 It will run the tests with 4 workers. If you use auto it will adapt the number of workers to the number of CPUS, or 1 if you use --pdb . To configure it in the pyproject.toml use the addopts [tool.pytest.ini_options] minversion = \"6.0\" addopts = \"-vv --tb=short -n auto\" Setting a timeout for your tests \u2691 To make your tests fail if they don't end in less than X seconds, use pytest-timeout . Install it with: pip install pytest-timeout You can set a global timeout in your pyproject.toml : [pytest] timeout = 300 Or define it for each test with: @pytest . mark . timeout ( 60 ) def test_foo (): pass Rerun tests that fail sometimes \u2691 pytest-rerunfailures is a plugin for pytest that re-runs tests to eliminate intermittent failures. Using this plugin is generally a bad idea, it would be best to solve the reason why your code is not reliable. It's useful when you rely on non robust third party software in a way that you can't solve, or if the error is not in your code but in the testing code, and again you are not able to fix it. Install it with: pip install pytest-rerunfailures To re-run all test failures, use the --reruns command line option with the maximum number of times you\u2019d like the tests to run: pytest --reruns 5 Failed fixture or setup_class will also be re-executed. To add a delay time between re-runs use the --reruns-delay command line option with the amount of seconds that you would like wait before the next test re-run is launched: pytest --reruns 5 --reruns-delay 1 To mark individual tests as flaky, and have them automatically re-run when they fail, add the flaky mark with the maximum number of times you\u2019d like the test to run: @pytest . mark . flaky ( reruns = 5 ) def test_example (): import random assert random . choice ([ True , False ]) pytest integration with Vim \u2691 Integrating pytest into your Vim workflow enhances your productivity while writing code, thus making it easier to code using TDD. I use Janko-m's Vim-test plugin (which can be installed through Vundle ) with the following configuration. nmap < silent > t :TestNearest -- pdb < CR > nmap < silent > < leader > t :TestSuite tests/unit < CR > nmap < silent > < leader > i :TestSuite tests/integration < CR > nmap < silent > < leader > T :TestFile < CR > let test#python#runner = 'pytest' let test#strategy = \"neovim\" I often open Vim with a vertical split ( :vs ), in the left window I have the tests and in the right the code. Whenever I want to run a single test I press t when the cursor is inside that test. If you need to make changes in the code, you can press t again while the cursor is at the code you are testing and it will run the last test. Once the unit test has passed, I run the whole unit tests with ;t (as ; is my <leader> ). And finally I use ;i to run the integration tests. Finally, if the test suite is huge, I use ;T to run only the tests of a single file. As you can see only the t has the --pdb flag, so the rest of them will run en parallel and any pdb trace will fail. Reference \u2691 Book Python Testing with pytest by Brian Okken . Docs Vim-test plugin","title":"Pytest"},{"location":"coding/python/pytest/#install","text":"pip install pytest","title":"Install"},{"location":"coding/python/pytest/#usage","text":"Run in the project directory. pytest If you need more information run it with -v . Pytest automatically finds which tests to run in a phase called test discovery . It will get the tests that match one of the following conditions: Test files that are named test_{{ something }}.py or {{ something }}_test.py . Test methods and functions named test_{{ something }} . Test classes named Test{{ Something }} . There are several possible outcomes of a test function: PASSED (.) : The test ran successfully. FAILED (F) : The test did not run usccessfully (or XPASS + strict). SKIPPED (s) : The test was skipped. You can tell pytest to skip a test by using enter the @pytest.mark.skip() or pytest.mark.skipif() decorators. xfail (x) : The test was not supposed to pass, ran, and failed. You can tell pytest that a test is expected to fail by using the @pytest.mark.xfail() decorator. XPASS (X) : The tests was not supposed to pass, ran, and passed. ERROR (E) : An exception happened outside of the test function, in either a fixture or a hook function. Pytest supports several cool flags like: -k EXPRESSION : Used to select a subset of tests to run. For example pytest -k \"asdict or defaults\" will run both test_asdict() and test_defaults() . --lf or --last-failed : Just run the tests that have failed in the previous run. -x , or --exitfirst : Exit on first failed test. -l or --showlocals : Print out the local variables in a test if the test fails. -s Allows any output that normally would be printed to stdout to actually be printed to stdout . It's an alias of --capture=no , so the output is not captured when the tests are run, which is the default behavior. This is useful to debug with print() statements. --durations=N : It reports the slowest N number of tests/setups/teardowns after the test run. If you pass in --durations=0 , it reports everything in order of slowest to fastest. --setup-show : Show the fixtures in use.","title":"Usage"},{"location":"coding/python/pytest/#fixtures","text":"Fixtures are functions that are run by pytest before (and sometimes after) the actual test functions. You can use fixtures to get a data set for the tests to work on, or use them to get a system into a known state before running a test. They are also used to get data ready for multiple tests. Here's a simple fixture that returns a number: import pytest @pytest . fixture () def some_data () \"\"\" Return answer to the ultimate question \"\"\" return 42 def test_some_data ( some_data ): \"\"\" Use fixture return value in a test\"\"\" assert some_data == 42 The @pytest.fixture() decorator is used to tell pytest that a function is a fixture.When you include the fixture name in the parameter list of a test function,pytest knows to run it before running the test. Fixtures can do work, and can also return data to the test function. The test test_some_data() has the name of the fixture, some_data, as a parameter.pytest will see this and look for a fixture with this name. Naming is significant in pytest. pytest will look in the module of the test for a fixture of that name. If the function is defined in the same file as where it's being used pylint will raise an W0621: Redefining name %r from outer scope (line %s) error. To solve it either move the fixture to other file or name the decorated function fixture_<fixturename> and then use @pytest.fixture(name='<fixturename>') .","title":"Fixtures"},{"location":"coding/python/pytest/#sharing-fixtures-through-conftestpy","text":"You can put fixtures into individual test files, but to share fixtures among multiple test files, you need to use a conftest.py file somewhere centrally located for all of the tests. Additionally you can have conftest.py files in subdirectories of the top tests directory. If you do, fixtures defined in these lower level conftest.py files will be available to tests in that directory and subdirectories. Although conftest.py is a Python module, it should not be imported by test files. The file gets read by pytest, and is considered a local plugin . Another option is to save the fixtures in a file by creating a local pytest plugin . File: tests/unit/conftest.py pytest_plugins = [ \"tests.unit.fixtures.some_stuff\" , ] File: tests/unit/fixtures/some_stuff.py import pytest @pytest . fixture def foo (): return 'foobar'","title":"Sharing fixtures through conftest.py"},{"location":"coding/python/pytest/#specifying-fixture-scope","text":"Fixtures include an optional parameter called scope, which controls how often a fixture gets set up and torn down. The scope parameter to @pytest.fixture() can have the values of function, class, module, or session. Here\u2019s a rundown of each scope value: scope='function' : Run once per test function. The setup portion is run before each test using the fixture. The teardown portion is run after each test using the fixture. This is the default scope used when no scope parameter is specified. scope='class' : Run once per test class, regardless of how many test methods are in the class. scope='module' : Run once per module, regardless of how many test functions or methods or other fixtures in the module use it. scope='session' Run once per session. All test methods and functions using a fixture of session scope share one setup and teardown call.","title":"Specifying fixture scope"},{"location":"coding/python/pytest/#useful-fixtures","text":"","title":"Useful Fixtures"},{"location":"coding/python/pytest/#the-tmpdir-fixture","text":"You can use the tmpdir fixture which will provide a temporary directory unique to the test invocation, created in the base temporary directory. tmpdir is a py.path.local object which offers os.path methods and more. Here is an example test usage: File: test_tmpdir.py from py._path.local import LocalPath def test_create_file ( tmpdir : LocalPath ): p = tmpdir . mkdir ( \"sub\" ) . join ( \"hello.txt\" ) p . write ( \"content\" ) assert p . read () == \"content\" assert len ( tmpdir . listdir ()) == 1 assert 0 The tmpdir fixture has a scope of function so you can't make a session directory. Instead use the tmpdir_factory fixture. from _pytest.tmpdir import TempdirFactory @pytest . fixture ( scope = \"session\" ) def image_file ( tmpdir_factory : TempdirFactory ): img = compute_expensive_image () fn = tmpdir_factory . mktemp ( \"data\" ) . join ( \"img.png\" ) img . save ( str ( fn )) return fn def test_histogram ( image_file ): img = load_image ( image_file ) # compute and test histogram","title":"The tmpdir fixture"},{"location":"coding/python/pytest/#make-a-subdirectory","text":"p = tmpdir . mkdir ( \"sub\" ) . join ( \"hello.txt\" )","title":"Make a subdirectory"},{"location":"coding/python/pytest/#the-caplog-fixture","text":"pytest captures log messages of level WARNING or above automatically and displays them in their own section for each failed test in the same manner as captured stdout and stderr. You can change the default logging level in the pytest configuration: File: pytest.ini [pytest] log_level = debug Although it may not be a good idea in most cases. It's better to change the log level in the tests that need a lower level. All the logs sent to the logger during the test run are available on the fixture in the form of both the logging.LogRecord instances and the final log text. This is useful for when you want to assert on the contents of a message: from _pytest.logging import LogCaptureFixture def test_baz ( caplog : LogCaptureFixture ): func_under_test () for record in caplog . records : assert record . levelname != \"CRITICAL\" assert \"wally\" not in caplog . text You can also resort to record_tuples if all you want to do is to ensure that certain messages have been logged under a given logger name with a given severity and message: def test_foo ( caplog ): logging . getLogger () . info ( \"boo %s \" , \"arg\" ) assert ( \"root\" , logging . INFO , \"boo arg\" ) in caplog . record_tuples You can call caplog.clear() to reset the captured log records in a test.","title":"The caplog fixture"},{"location":"coding/python/pytest/#change-the-log-level","text":"Inside tests it's possible to change the log level for the captured log messages. def test_foo ( caplog ): caplog . set_level ( logging . INFO ) pass","title":"Change the log level"},{"location":"coding/python/pytest/#the-capsys-fixture","text":"The capsys builtin fixture provides two bits of functionality: it allows you to retrieve stdout and stderr from some code, and it disables output capture temporarily. Suppose you have a function to print a greeting to stdout: def greeting ( name ): print ( f 'Hi, { name } ' ) You can test the output by using capsys . from _pytest.capture import CaptureFixture def test_greeting ( capsys : CaptureFixture [ Any ]): greeting ( 'Earthling' ) out , err = capsys . readouterr () assert out == 'Hi, Earthling \\n ' assert err == '' The return value is whatever has been captured since the beginning of the function, or from the last time it was called.","title":"The capsys fixture"},{"location":"coding/python/pytest/#freezegun","text":"freezegun lets you freeze time in both the test and fixtures.","title":"freezegun"},{"location":"coding/python/pytest/#install_1","text":"pip install pytest-freezegun","title":"Install"},{"location":"coding/python/pytest/#usage_1","text":"Freeze time by using the freezer fixture: if TYPE_CHECKING : from freezegun.api import FrozenDateTimeFactory def test_frozen_date ( freezer : FrozenDateTimeFactory ): now = datetime . now () time . sleep ( 1 ) later = datetime . now () assert now == later This can then be used to move time: def test_moving_date ( freezer ): now = datetime . now () freezer . move_to ( '2017-05-20' ) later = datetime . now () assert now != later You can also pass arguments to freezegun by using the freeze_time mark: @pytest . mark . freeze_time ( '2017-05-21' ) def test_current_date (): assert date . today () == date ( 2017 , 5 , 21 ) The freezer fixture and freeze_time mark can be used together, and they work with other fixtures: @pytest . fixture def current_date (): return date . today () @pytest . mark . freeze_time def test_changing_date ( current_date , freezer ): freezer . move_to ( '2017-05-20' ) assert current_date == date ( 2017 , 5 , 20 ) freezer . move_to ( '2017-05-21' ) assert current_date == date ( 2017 , 5 , 21 ) They can also be used in class-based tests: class TestDate : @pytest . mark . freeze_time def test_changing_date ( self , current_date , freezer ): freezer . move_to ( '2017-05-20' ) assert current_date == date ( 2017 , 5 , 20 ) freezer . move_to ( '2017-05-21' ) assert current_date == date ( 2017 , 5 , 21 )","title":"Usage"},{"location":"coding/python/pytest/#customize-nested-fixtures","text":"Sometimes you need to tweak your fixtures so they can be used in different tests. As usual, there are different solutions to the same problem. !!! note \"TL;DR: For simple cases parametrize your fixtures or use parametrization to override the default valued fixture . As your test suite get's more complex migrate to pytest-case .\" Let's say you're running along merrily with some fixtures that create database objects for you: @pytest . fixture def supplier ( db ): s = Supplier ( ref = random_ref (), name = random_name (), country = \"US\" , ) db . add ( s ) yield s db . remove ( s ) @pytest . fixture () def product ( db , supplier ): p = Product ( ref = random_ref (), name = random_name (), supplier = supplier , net_price = 9.99 , ) db . add ( p ) yield p db . remove ( p ) And now you're writing a new test and you suddenly realize you need to customize your default \"supplier\" fixture: def test_US_supplier_has_total_price_equal_net_price ( product ): assert product . total_price == product . net_price def test_EU_supplier_has_total_price_including_VAT ( supplier , product ): supplier . country = \"FR\" # oh, this doesn't work assert product . total_price == product . net_price * 1.2 There are different ways to modify your fixtures","title":"Customize nested fixtures"},{"location":"coding/python/pytest/#add-more-fixtures","text":"We can just create more fixtures, and try to do a bit of DRY by extracting out common logic: def _default_supplier (): return Supplier ( ref = random_ref (), name = random_name (), ) @pytest . fixture def us_supplier ( db ): s = _default_supplier () s . country = \"US\" db . add ( s ) yield s db . remove ( s ) @pytest . fixture def eu_supplier ( db ): s = _default_supplier () s . country = \"FR\" db . add ( s ) yield s db . remove ( s ) That's just one way you could do it, maybe you can figure out ways to reduce the duplication of the db.add() stuff as well, but you are going to have a different, named fixture for each customization of Supplier, and eventually you may decide that doesn't scale.","title":"Add more fixtures"},{"location":"coding/python/pytest/#use-factory-fixtures","text":"Instead of a fixture returning an object directly, it can return a function that creates an object, and that function can take arguments: @pytest . fixture def make_supplier ( db ): s = Supplier ( ref = random_ref (), name = random_name (), ) def _make_supplier ( country ): s . country = country db . add ( s ) return s yield _make_supplier db . remove ( s ) The problem with this is that, once you start, you tend to have to go all the way, and make all of your fixture hierarchy into factory functions: def test_EU_supplier_has_total_price_including_VAT ( make_supplier , product ): supplier = make_supplier ( country = \"FR\" ) product . supplier = supplier # OH, now this doesn't work, because it's too late again assert product . total_price == product . net_price * 1.2 And so... @pytest . fixture def make_product ( db ): p = Product ( ref = random_ref (), name = random_name (), ) def _make_product ( supplier ): p . supplier = supplier db . add ( p ) return p yield _make_product db . remove ( p ) def test_EU_supplier_has_total_price_including_VAT ( make_supplier , make_product ): supplier = make_supplier ( country = \"FR\" ) product = make_product ( supplier = supplier ) assert product . total_price == product . net_price * 1.2 That works, but firstly now everything is a factory-fixture, which makes them more convoluted, and secondly, your tests are filling up with extra calls to make_things , and you're having to embed some of the domain knowledge of what-depends-on-what into your tests as well as your fixtures. Ugly!","title":"Use factory fixtures"},{"location":"coding/python/pytest/#parametrize-your-fixtures","text":"You can also parametrize your fixtures . @pytest . fixture ( params = [ 'US' , 'FR' ]) def supplier ( db , request ): s = Supplier ( ref = random_ref (), name = random_name (), country = request . param ) db . add ( s ) yield s db . remove ( s ) Now any test that depends on supplier, directly or indirectly, will be run twice, once with supplier.country = US and once with FR . That's really cool for checking that a given piece of logic works in a variety of different cases, but it's not really ideal in our case. We have to build a bunch of if logic into our tests: def test_US_supplier_has_no_VAT_but_EU_supplier_has_total_price_including_VAT ( product ): # this test is magically run twice, but: if product . supplier . country == 'US' : assert product . total_price == product . net_price if product . supplier . country == 'FR' : assert product . total_price == product . net_price * 1.2 So that's ugly, and on top of that, now every single test that depends (indirectly) on supplier gets run twice, and some of those extra test runs may be totally irrelevant to what the country is.","title":"Parametrize your fixtures"},{"location":"coding/python/pytest/#use-pytest-parametrization-to-override-the-default-valued-fixtures","text":"We introduce an extra fixture that holds a default value for the country field: @pytest . fixture () def country (): return \"US\" @pytest . fixture def supplier ( db , country ): s = Supplier ( ref = random_ref (), name = random_name (), country = country , ) db . add ( s ) yield s db . remove ( s ) And then in the tests that need to change it, we can use parametrize to override the default value of country, even though the country fixture isn't explicitly named in that test: @pytest . mark . parametrize ( 'country' , [ \"US\" ]) def test_US_supplier_has_total_price_equal_net_price ( product ): assert product . total_price == product . net_price @pytest . mark . parametrize ( 'country' , [ \"EU\" ]) def test_EU_supplier_has_total_price_including_VAT ( product ): assert product . total_price == product . net_price * 1.2 The only problem is that you're now likely to build a implicit dependencies where the only way to find out what's actually happening is to spend ages spelunking in conftest.py.","title":"Use pytest parametrization to override the default valued fixtures"},{"location":"coding/python/pytest/#use-pytest-case","text":"pytest-case gives a lot of power when it comes to tweaking the fixtures and parameterizations. Check that file for further information.","title":"Use pytest-case"},{"location":"coding/python/pytest/#use-a-fixture-more-than-once-in-a-function","text":"One solution is to make your fixture return a factory instead of the resource directly: @pytest . fixture ( name = 'make_user' ) def make_user_ (): created = [] def make_user (): u = models . User () u . commit () created . append ( u ) return u yield make_user for u in created : u . delete () def test_two_users ( make_user ): user1 = make_user () user2 = make_user () # test them # you can even have the normal fixture when you only need a single user @pytest . fixture def user ( make_user ): return make_user () def test_one_user ( user ): # test him/her","title":"Use a fixture more than once in a function"},{"location":"coding/python/pytest/#marks","text":"Pytest marks can be used to group tests. It can be useful to: slow Mark the tests that are slow. secondary Mart the tests that use functionality that is being tested in the same file. To mark a test, use the @pytest.mark decorator. For example: @pytest . mark . slow def test_really_slow_test (): pass Pytest requires you to register your marks, do so in the pytest.ini file [pytest] markers = slow: marks tests as slow (deselect with '-m \"not slow\"') secondary: mark tests that use functionality tested in the same file (deselect with '-m \"not secondary\"')","title":"Marks"},{"location":"coding/python/pytest/#snippets","text":"","title":"Snippets"},{"location":"coding/python/pytest/#mocking-sysexit","text":"with pytest . raises ( SystemExit ): # Code to test","title":"Mocking sys.exit"},{"location":"coding/python/pytest/#testing-exceptions-with-pytest","text":"def test_value_error_is_raised (): with pytest . raises ( ValueError , match = \"invalid literal for int() with base 10: 'a'\" ): int ( 'a' )","title":"Testing exceptions with pytest"},{"location":"coding/python/pytest/#excluding-code-from-coverage","text":"You may have code in your project that you know won't be executed, and you want to tell coverage.py to ignore it. For example, if you have some code in abstract classes that is going to be tested on the subclasses, you can ignore it with # pragma: no cover . If you want other code to be excluded , for example the statements inside the if TYPE_CHECKING: add to your pyproject.toml : [tool.coverage.report] exclude_lines = [ # Have to re-enable the standard pragma 'pragma: no cover' , # Type checking can not be tested 'if TYPE_CHECKING:' , ]","title":"Excluding code from coverage"},{"location":"coding/python/pytest/#running-tests-in-parallel","text":"pytest-xdist makes it possible to run the tests in parallel, useful when the test suit is large or when the tests are slow.","title":"Running tests in parallel"},{"location":"coding/python/pytest/#installation","text":"pip install pytest-xdist","title":"Installation"},{"location":"coding/python/pytest/#usage_2","text":"pytest -n 4 It will run the tests with 4 workers. If you use auto it will adapt the number of workers to the number of CPUS, or 1 if you use --pdb . To configure it in the pyproject.toml use the addopts [tool.pytest.ini_options] minversion = \"6.0\" addopts = \"-vv --tb=short -n auto\"","title":"Usage"},{"location":"coding/python/pytest/#setting-a-timeout-for-your-tests","text":"To make your tests fail if they don't end in less than X seconds, use pytest-timeout . Install it with: pip install pytest-timeout You can set a global timeout in your pyproject.toml : [pytest] timeout = 300 Or define it for each test with: @pytest . mark . timeout ( 60 ) def test_foo (): pass","title":"Setting a timeout for your tests"},{"location":"coding/python/pytest/#rerun-tests-that-fail-sometimes","text":"pytest-rerunfailures is a plugin for pytest that re-runs tests to eliminate intermittent failures. Using this plugin is generally a bad idea, it would be best to solve the reason why your code is not reliable. It's useful when you rely on non robust third party software in a way that you can't solve, or if the error is not in your code but in the testing code, and again you are not able to fix it. Install it with: pip install pytest-rerunfailures To re-run all test failures, use the --reruns command line option with the maximum number of times you\u2019d like the tests to run: pytest --reruns 5 Failed fixture or setup_class will also be re-executed. To add a delay time between re-runs use the --reruns-delay command line option with the amount of seconds that you would like wait before the next test re-run is launched: pytest --reruns 5 --reruns-delay 1 To mark individual tests as flaky, and have them automatically re-run when they fail, add the flaky mark with the maximum number of times you\u2019d like the test to run: @pytest . mark . flaky ( reruns = 5 ) def test_example (): import random assert random . choice ([ True , False ])","title":"Rerun tests that fail sometimes"},{"location":"coding/python/pytest/#pytest-integration-with-vim","text":"Integrating pytest into your Vim workflow enhances your productivity while writing code, thus making it easier to code using TDD. I use Janko-m's Vim-test plugin (which can be installed through Vundle ) with the following configuration. nmap < silent > t :TestNearest -- pdb < CR > nmap < silent > < leader > t :TestSuite tests/unit < CR > nmap < silent > < leader > i :TestSuite tests/integration < CR > nmap < silent > < leader > T :TestFile < CR > let test#python#runner = 'pytest' let test#strategy = \"neovim\" I often open Vim with a vertical split ( :vs ), in the left window I have the tests and in the right the code. Whenever I want to run a single test I press t when the cursor is inside that test. If you need to make changes in the code, you can press t again while the cursor is at the code you are testing and it will run the last test. Once the unit test has passed, I run the whole unit tests with ;t (as ; is my <leader> ). And finally I use ;i to run the integration tests. Finally, if the test suite is huge, I use ;T to run only the tests of a single file. As you can see only the t has the --pdb flag, so the rest of them will run en parallel and any pdb trace will fail.","title":"pytest integration with Vim"},{"location":"coding/python/pytest/#reference","text":"Book Python Testing with pytest by Brian Okken . Docs Vim-test plugin","title":"Reference"},{"location":"coding/python/pytest_cases/","text":"pytest-cases is a pytest plugin that allows you to separate your test cases from your test functions . In addition, pytest-cases provides several useful goodies to empower pytest . In particular it improves the fixture mechanism to support \"fixture unions\". This is a major change in the internal pytest engine, unlocking many possibilities such as using fixture references as parameter values in a test function. Installing \u2691 pip install pytest_cases Installing pytest-cases has effects on the order of pytest tests execution, even if you do not use its features. One positive side effect is that it fixed pytest#5054 . But if you see less desirable ordering please report it . Why pytest-cases ? \u2691 Let's consider the following foo function under test, located in example.py : def foo ( a , b ): return a + 1 , b + 1 If we were using plain pytest to test it with various inputs, we would create a test_foo.py file and use @pytest.mark.parametrize : import pytest from example import foo @pytest . mark . parametrize ( \"a,b\" , [( 1 , 2 ), ( - 1 , - 2 )]) def test_foo ( a , b ): # check that foo runs correctly and that the result is a tuple. assert isinstance ( foo ( a , b ), tuple ) This is the fastest and most compact thing to do when you have a few number of test cases, that do not require code to generate each test case. Now imagine that instead of (1, 2) and (-1, -2) each of our test cases: Requires a few lines of code to be generated. Requires documentation to explain the other developers the intent of that precise test case. Requires external resources (data files on the filesystem, databases...), with a variable number of cases depending on what is available on the resource. Requires a readable id , such as 'uniformly_sampled_nonsorted_with_holes' for the above example. Of course we could use pytest.param or ids=<list> but that is \"a pain to maintain\" according to pytest doc. Such a design does not feel right as the id is detached from the case. With standard pytest there is no particular pattern to simplify your life here. Investigating a little bit, people usually end up trying to mix parameters and fixtures and asking this kind of question: so1 , so2 . But by design it is not possible to solve this problem using fixtures, because pytest does not handle \"unions\" of fixtures . There is also an example in pytest doc with a metafunc hook . The issue with such workarounds is that you can do anything . And anything is a bit too much: this does not provide any convention / \"good practice\" on how to organize test cases, which is an open door to developing ad-hoc unreadable or unmaintainable solutions. pytest_cases was created to provide an answer to this precise situation. It proposes a simple framework to separate test cases from test functions. The test cases are typically located in a separate \"companion\" file: test_foo.py is your usual test file containing the test functions (named test_<id> ). test_foo_cases.py contains the test cases , that are also functions. Note: an alternate file naming style cases_foo.py is also available if you prefer it. Basic usage \u2691 Case functions \u2691 Let's create a test_foo_cases.py file. This file will contain test cases generator functions , that we will call case functions for brevity. In these functions, you will typically either parse some test data files, generate some simulated test data, expected results, etc. File: test_foo_cases.py def case_two_positive_ints (): \"\"\" Inputs are two positive integers \"\"\" return 1 , 2 def case_two_negative_ints (): \"\"\" Inputs are two negative integers \"\"\" return - 1 , - 2 Case functions can return anything that is considered useful to run the associated test. You can use all classic pytest mechanism on case functions (id customization, skip/fail marks, parametrization or fixtures injection). Test functions \u2691 As usual we write our pytest test functions starting with test_ , in a test_foo.py file. The only difference is that we now decorate it with @parametrize_with_cases instead of @pytest.mark.parametrize as we were doing previously: File: test_foo.py from example import foo from pytest_cases import parametrize_with_cases @parametrize_with_cases ( \"a,b\" ) def test_foo ( a , b ): # check that foo runs correctly and that the result is a tuple. assert isinstance ( foo ( a , b ), tuple ) Executing pytest will now run our test function once for every case function: >>> pytest -s -v ============================= test session starts ============================= ( ... ) <your_project>/tests/test_foo.py::test_foo [ two_positive_ints ] PASSED [ 50 % ] <your_project>/tests/test_foo.py::test_foo [ two_negative_ints ] PASSED [ 100 % ] ========================== 2 passed in 0 .24 seconds ========================== Usage \u2691 Cases collection \u2691 Alternate source(s) \u2691 It is not mandatory that case functions should be in a different file than the test functions: both can be in the same file. For this you can use cases='.' or cases=THIS_MODULE to refer to the module in which the test function is located: from pytest_cases import parametrize_with_cases def case_one_positive_int (): return 1 def case_one_negative_int (): return - 1 @parametrize_with_cases ( \"i\" , cases = '.' ) def test_with_this_module ( i ): assert i == int ( i ) Only the case functions defined BEFORE the test function in the module file will be taken into account. @parametrize_with_cases(cases=...) also accepts explicit list of case functions, classes containing case functions, and modules. See API Reference for details. A typical way to organize cases is to use classes for example: from pytest_cases import parametrize_with_cases class Foo : def case_a_positive_int ( self ): return 1 def case_another_positive_int ( self ): return 2 @parametrize_with_cases ( \"a\" , cases = Foo ) def test_foo ( a ): assert a > 0 Note that as for pytest , self is recreated for every test and therefore should not be used to store any useful information. Alternate prefix \u2691 case_ might not be your preferred prefix, especially if you wish to store in the same module or class various kind of case data. @parametrize_with_cases offers a prefix=... argument to select an alternate prefix for your case functions. That way, you can store in the same module or class case functions as diverse as datasets (e.g. data_ ), user descriptions (e.g. user_ ), algorithms or machine learning models (e.g. model_ or algo_ ), etc. from pytest_cases import parametrize_with_cases , parametrize def data_a (): return 'a' @parametrize ( \"hello\" , [ True , False ]) def data_b ( hello ): return \"hello\" if hello else \"world\" def case_c (): return dict ( name = \"hi i'm not used\" ) def user_bob (): return \"bob\" @parametrize_with_cases ( \"data\" , cases = '.' , prefix = \"data_\" ) @parametrize_with_cases ( \"user\" , cases = '.' , prefix = \"user_\" ) def test_with_data ( data , user ): assert data in ( 'a' , \"hello\" , \"world\" ) assert user == 'bob' Yields test_doc_filters_n_tags.py::test_with_data[bob-a] PASSED [ 33%] test_doc_filters_n_tags.py::test_with_data[bob-b-True] PASSED [ 66%] test_doc_filters_n_tags.py::test_with_data[bob-b-False] PASSED [ 100%] Filters and tags \u2691 The easiest way to select only a subset of case functions in a module or a class, is to specify a custom prefix instead of the default one ( 'case_' ). However sometimes more advanced filtering is required. In that case, you can also rely on three additional mechanisms provided in @parametrize_with_cases : The glob argument can contain a glob-like pattern for case ids. This can become handy to separate for example good or bad cases, the latter returning an expected error type and/or message for use with pytest.raises or with our alternative assert_exception . from math import sqrt import pytest from pytest_cases import parametrize_with_cases def case_int_success (): return 1 def case_negative_int_failure (): # note that we decide to return the expected type of failure to check it return - 1 , ValueError , \"math domain error\" @parametrize_with_cases ( \"data\" , cases = '.' , glob = \"*success\" ) def test_good_datasets ( data ): assert sqrt ( data ) > 0 @parametrize_with_cases ( \"data, err_type, err_msg\" , cases = '.' , glob = \"*failure\" ) def test_bad_datasets ( data , err_type , err_msg ): with pytest . raises ( err_type , match = err_msg ): sqrt ( data ) The has_tag argument allows you to filter cases based on tags set on case functions using the @case decorator. See API reference of @case and @parametrize_with_cases . from pytest_cases import parametrize_with_cases , case class FooCases : def case_two_positive_ints ( self ): return 1 , 2 @case ( tags = 'foo' ) def case_one_positive_int ( self ): return 1 @parametrize_with_cases ( \"a\" , cases = FooCases , has_tag = 'foo' ) def test_foo ( a ): assert a > 0 Finally if none of the above matches your expectations, you can provide a callable to filter . This callable will receive each collected case function and should return True in case of success. Note that your function can leverage the _pytestcase attribute available on the case function to read the tags, marks and id found on it. @parametrize_with_cases ( \"data\" , cases = '.' , filter = lambda cf : \"success\" in cf . _pytestcase . id ) def test_good_datasets2 ( data ): assert sqrt ( data ) > 0 Pytest marks ( skip , xfail ...) on cases \u2691 pytest marks such as @pytest.mark.skipif can be applied on case functions the same way as with test functions . import sys import pytest @pytest . mark . skipif ( sys . version_info < ( 3 , 0 ), reason = \"Not useful on python 2\" ) def case_two_positive_ints (): return 1 , 2 Case generators \u2691 In many real-world usage we want to generate one test case per <something> . The most intuitive way would be to use a for loop to create the case functions, and to use the @case decorator to set their names ; however this would not be very readable. Instead, case functions can be parametrized the same way as with test functions : simply add the parameter names as arguments in their signature and decorate with @pytest.mark.parametrize . Even better, you can use the enhanced @parametrize from pytest-cases so as to benefit from its additional usability features: from pytest_cases import parametrize , parametrize_with_cases class CasesFoo : def case_hello ( self ): return \"hello world\" @parametrize ( who = ( 'you' , 'there' )) def case_simple_generator ( self , who ): return \"hello %s \" % who @parametrize_with_cases ( \"msg\" , cases = CasesFoo ) def test_foo ( msg ): assert isinstance ( msg , str ) and msg . startswith ( \"hello\" ) Yields test_generators.py::test_foo[hello] PASSED [ 33%] test_generators.py::test_foo[simple_generator-who=you] PASSED [ 66%] test_generators.py::test_foo[simple_generator-who=there] PASSED [100%] Cases requiring fixtures \u2691 Cases can use fixtures the same way as test functions do : simply add the fixture names as arguments in their signature and make sure the fixture exists either in the same module, or in a conftest.py file in one of the parent packages. See pytest documentation on sharing fixtures . Use @fixture instead of @pytest.fixture If a fixture is used by some of your cases only, then you should use the @fixture decorator from pytest-cases instead of the standard @pytest.fixture . Otherwise you fixture will be setup/teardown for all cases even those not requiring it. See @fixture doc . from pytest_cases import parametrize_with_cases , fixture , parametrize @fixture ( scope = 'session' ) def db (): return { 0 : 'louise' , 1 : 'bob' } def user_bob ( db ): return db [ 1 ] @parametrize ( id = range ( 2 )) def user_from_db ( db , id ): return db [ id ] @parametrize_with_cases ( \"a\" , cases = '.' , prefix = 'user_' ) def test_users ( a , db , request ): print ( \"this is test %r \" % request . node . nodeid ) assert a in db . values () Yields test_fixtures.py::test_users[a_is_bob] test_fixtures.py::test_users[a_is_from_db-id=0] test_fixtures.py::test_users[a_is_from_db-id=1] Parametrize fixtures with cases \u2691 In some scenarios you might wish to parametrize a fixture with the cases, rather than the test function. For example: To inject the same test cases in several test functions without duplicating the @parametrize_with_cases decorator on each of them. To generate the test cases once for the whole session, using a scope='session' fixture or another scope . To modify the test cases, log some message, or perform some other action before injecting them into the test functions, and/or after executing the test function (thanks to yield fixtures ). For this, simply use @fixture from pytest_cases instead of @pytest.fixture to define your fixture. That allows your fixtures to be easily parametrized with @parametrize_with_cases , @parametrize , and even @pytest.mark.parametrize . from pytest_cases import fixture , parametrize_with_cases @fixture @parametrize_with_cases ( \"a,b\" ) def c ( a , b ): return a + b def test_foo ( c ): assert isinstance ( c , int ) Pytest-cases internals \u2691 @fixture \u2691 @fixture is similar to pytest.fixture but without its param and ids arguments. Instead, it is able to pick the parametrization from @pytest.mark.parametrize marks applied on fixtures. This makes it very intuitive for users to parametrize both their tests and fixtures. Finally it now supports unpacking, see unpacking feature . @fixture deprecation if/when @pytest.fixture supports @pytest.mark.parametrize The ability for pytest fixtures to support the @pytest.mark.parametrize annotation is a feature that clearly belongs to pytest scope, and has been requested already . It is therefore expected that @fixture will be deprecated in favor of @pytest_fixture if/when the pytest team decides to add the proposed feature. As always, deprecation will happen slowly across versions (at least two minor, or one major version update) so as for users to have the time to update their code bases. unpack_fixture / unpack_into \u2691 In some cases fixtures return a tuple or a list of items. It is not easy to refer to a single of these items in a test or another fixture. With unpack_fixture you can easily do it: import pytest from pytest_cases import unpack_fixture , fixture @fixture @pytest . mark . parametrize ( \"o\" , [ 'hello' , 'world' ]) def c ( o ): return o , o [ 0 ] a , b = unpack_fixture ( \"a,b\" , c ) def test_function ( a , b ): assert a [ 0 ] == b Note that you can also use the unpack_into= argument of @fixture to do the same thing: import pytest from pytest_cases import fixture @fixture ( unpack_into = \"a,b\" ) @pytest . mark . parametrize ( \"o\" , [ 'hello' , 'world' ]) def c ( o ): return o , o [ 0 ] def test_function ( a , b ): assert a [ 0 ] == b And it is also available in fixture_union : import pytest from pytest_cases import fixture , fixture_union @fixture @pytest . mark . parametrize ( \"o\" , [ 'hello' , 'world' ]) def c ( o ): return o , o [ 0 ] @fixture @pytest . mark . parametrize ( \"o\" , [ 'yeepee' , 'yay' ]) def d ( o ): return o , o [ 0 ] fixture_union ( \"c_or_d\" , [ c , d ], unpack_into = \"a, b\" ) def test_function ( a , b ): assert a [ 0 ] == b param_fixture[s] \u2691 If you wish to share some parameters across several fixtures and tests, it might be convenient to have a fixture representing this parameter. This is relatively easy for single parameters, but a bit harder for parameter tuples. The two utilities functions param_fixture (for a single parameter name) and param_fixtures (for a tuple of parameter names) handle the difficulty for you: import pytest from pytest_cases import param_fixtures , param_fixture # create a single parameter fixture my_parameter = param_fixture ( \"my_parameter\" , [ 1 , 2 , 3 , 4 ]) @pytest . fixture def fixture_uses_param ( my_parameter ): ... def test_uses_param ( my_parameter , fixture_uses_param ): ... # ----- # create a 2-tuple parameter fixture arg1 , arg2 = param_fixtures ( \"arg1, arg2\" , [( 1 , 2 ), ( 3 , 4 )]) @pytest . fixture def fixture_uses_param2 ( arg2 ): ... def test_uses_param2 ( arg1 , arg2 , fixture_uses_param2 ): ... fixture_union \u2691 As of pytest 5, it is not possible to create a \"union\" fixture, i.e. a parametrized fixture that would first take all the possible values of fixture A, then all possible values of fixture B, etc. Indeed all fixture dependencies of each test node are grouped together, and if they have parameters a big \"cross-product\" of the parameters is done by pytest . from pytest_cases import fixture , fixture_union @fixture def first (): return 'hello' @fixture ( params = [ 'a' , 'b' ]) def second ( request ): return request . param # c will first take all the values of 'first', then all of 'second' c = fixture_union ( 'c' , [ first , second ]) def test_basic_union ( c ): print ( c ) yields <...>::test_basic_union[c_is_first] hello PASSED <...>::test_basic_union[c_is_second-a] a PASSED <...>::test_basic_union[c_is_second-b] b PASSED References \u2691 Docs Git","title":"Pytest-cases"},{"location":"coding/python/pytest_cases/#installing","text":"pip install pytest_cases Installing pytest-cases has effects on the order of pytest tests execution, even if you do not use its features. One positive side effect is that it fixed pytest#5054 . But if you see less desirable ordering please report it .","title":"Installing"},{"location":"coding/python/pytest_cases/#why-pytest-cases","text":"Let's consider the following foo function under test, located in example.py : def foo ( a , b ): return a + 1 , b + 1 If we were using plain pytest to test it with various inputs, we would create a test_foo.py file and use @pytest.mark.parametrize : import pytest from example import foo @pytest . mark . parametrize ( \"a,b\" , [( 1 , 2 ), ( - 1 , - 2 )]) def test_foo ( a , b ): # check that foo runs correctly and that the result is a tuple. assert isinstance ( foo ( a , b ), tuple ) This is the fastest and most compact thing to do when you have a few number of test cases, that do not require code to generate each test case. Now imagine that instead of (1, 2) and (-1, -2) each of our test cases: Requires a few lines of code to be generated. Requires documentation to explain the other developers the intent of that precise test case. Requires external resources (data files on the filesystem, databases...), with a variable number of cases depending on what is available on the resource. Requires a readable id , such as 'uniformly_sampled_nonsorted_with_holes' for the above example. Of course we could use pytest.param or ids=<list> but that is \"a pain to maintain\" according to pytest doc. Such a design does not feel right as the id is detached from the case. With standard pytest there is no particular pattern to simplify your life here. Investigating a little bit, people usually end up trying to mix parameters and fixtures and asking this kind of question: so1 , so2 . But by design it is not possible to solve this problem using fixtures, because pytest does not handle \"unions\" of fixtures . There is also an example in pytest doc with a metafunc hook . The issue with such workarounds is that you can do anything . And anything is a bit too much: this does not provide any convention / \"good practice\" on how to organize test cases, which is an open door to developing ad-hoc unreadable or unmaintainable solutions. pytest_cases was created to provide an answer to this precise situation. It proposes a simple framework to separate test cases from test functions. The test cases are typically located in a separate \"companion\" file: test_foo.py is your usual test file containing the test functions (named test_<id> ). test_foo_cases.py contains the test cases , that are also functions. Note: an alternate file naming style cases_foo.py is also available if you prefer it.","title":"Why pytest-cases?"},{"location":"coding/python/pytest_cases/#basic-usage","text":"","title":"Basic usage"},{"location":"coding/python/pytest_cases/#case-functions","text":"Let's create a test_foo_cases.py file. This file will contain test cases generator functions , that we will call case functions for brevity. In these functions, you will typically either parse some test data files, generate some simulated test data, expected results, etc. File: test_foo_cases.py def case_two_positive_ints (): \"\"\" Inputs are two positive integers \"\"\" return 1 , 2 def case_two_negative_ints (): \"\"\" Inputs are two negative integers \"\"\" return - 1 , - 2 Case functions can return anything that is considered useful to run the associated test. You can use all classic pytest mechanism on case functions (id customization, skip/fail marks, parametrization or fixtures injection).","title":"Case functions"},{"location":"coding/python/pytest_cases/#test-functions","text":"As usual we write our pytest test functions starting with test_ , in a test_foo.py file. The only difference is that we now decorate it with @parametrize_with_cases instead of @pytest.mark.parametrize as we were doing previously: File: test_foo.py from example import foo from pytest_cases import parametrize_with_cases @parametrize_with_cases ( \"a,b\" ) def test_foo ( a , b ): # check that foo runs correctly and that the result is a tuple. assert isinstance ( foo ( a , b ), tuple ) Executing pytest will now run our test function once for every case function: >>> pytest -s -v ============================= test session starts ============================= ( ... ) <your_project>/tests/test_foo.py::test_foo [ two_positive_ints ] PASSED [ 50 % ] <your_project>/tests/test_foo.py::test_foo [ two_negative_ints ] PASSED [ 100 % ] ========================== 2 passed in 0 .24 seconds ==========================","title":"Test functions"},{"location":"coding/python/pytest_cases/#usage","text":"","title":"Usage"},{"location":"coding/python/pytest_cases/#cases-collection","text":"","title":"Cases collection"},{"location":"coding/python/pytest_cases/#alternate-sources","text":"It is not mandatory that case functions should be in a different file than the test functions: both can be in the same file. For this you can use cases='.' or cases=THIS_MODULE to refer to the module in which the test function is located: from pytest_cases import parametrize_with_cases def case_one_positive_int (): return 1 def case_one_negative_int (): return - 1 @parametrize_with_cases ( \"i\" , cases = '.' ) def test_with_this_module ( i ): assert i == int ( i ) Only the case functions defined BEFORE the test function in the module file will be taken into account. @parametrize_with_cases(cases=...) also accepts explicit list of case functions, classes containing case functions, and modules. See API Reference for details. A typical way to organize cases is to use classes for example: from pytest_cases import parametrize_with_cases class Foo : def case_a_positive_int ( self ): return 1 def case_another_positive_int ( self ): return 2 @parametrize_with_cases ( \"a\" , cases = Foo ) def test_foo ( a ): assert a > 0 Note that as for pytest , self is recreated for every test and therefore should not be used to store any useful information.","title":"Alternate source(s)"},{"location":"coding/python/pytest_cases/#alternate-prefix","text":"case_ might not be your preferred prefix, especially if you wish to store in the same module or class various kind of case data. @parametrize_with_cases offers a prefix=... argument to select an alternate prefix for your case functions. That way, you can store in the same module or class case functions as diverse as datasets (e.g. data_ ), user descriptions (e.g. user_ ), algorithms or machine learning models (e.g. model_ or algo_ ), etc. from pytest_cases import parametrize_with_cases , parametrize def data_a (): return 'a' @parametrize ( \"hello\" , [ True , False ]) def data_b ( hello ): return \"hello\" if hello else \"world\" def case_c (): return dict ( name = \"hi i'm not used\" ) def user_bob (): return \"bob\" @parametrize_with_cases ( \"data\" , cases = '.' , prefix = \"data_\" ) @parametrize_with_cases ( \"user\" , cases = '.' , prefix = \"user_\" ) def test_with_data ( data , user ): assert data in ( 'a' , \"hello\" , \"world\" ) assert user == 'bob' Yields test_doc_filters_n_tags.py::test_with_data[bob-a] PASSED [ 33%] test_doc_filters_n_tags.py::test_with_data[bob-b-True] PASSED [ 66%] test_doc_filters_n_tags.py::test_with_data[bob-b-False] PASSED [ 100%]","title":"Alternate prefix"},{"location":"coding/python/pytest_cases/#filters-and-tags","text":"The easiest way to select only a subset of case functions in a module or a class, is to specify a custom prefix instead of the default one ( 'case_' ). However sometimes more advanced filtering is required. In that case, you can also rely on three additional mechanisms provided in @parametrize_with_cases : The glob argument can contain a glob-like pattern for case ids. This can become handy to separate for example good or bad cases, the latter returning an expected error type and/or message for use with pytest.raises or with our alternative assert_exception . from math import sqrt import pytest from pytest_cases import parametrize_with_cases def case_int_success (): return 1 def case_negative_int_failure (): # note that we decide to return the expected type of failure to check it return - 1 , ValueError , \"math domain error\" @parametrize_with_cases ( \"data\" , cases = '.' , glob = \"*success\" ) def test_good_datasets ( data ): assert sqrt ( data ) > 0 @parametrize_with_cases ( \"data, err_type, err_msg\" , cases = '.' , glob = \"*failure\" ) def test_bad_datasets ( data , err_type , err_msg ): with pytest . raises ( err_type , match = err_msg ): sqrt ( data ) The has_tag argument allows you to filter cases based on tags set on case functions using the @case decorator. See API reference of @case and @parametrize_with_cases . from pytest_cases import parametrize_with_cases , case class FooCases : def case_two_positive_ints ( self ): return 1 , 2 @case ( tags = 'foo' ) def case_one_positive_int ( self ): return 1 @parametrize_with_cases ( \"a\" , cases = FooCases , has_tag = 'foo' ) def test_foo ( a ): assert a > 0 Finally if none of the above matches your expectations, you can provide a callable to filter . This callable will receive each collected case function and should return True in case of success. Note that your function can leverage the _pytestcase attribute available on the case function to read the tags, marks and id found on it. @parametrize_with_cases ( \"data\" , cases = '.' , filter = lambda cf : \"success\" in cf . _pytestcase . id ) def test_good_datasets2 ( data ): assert sqrt ( data ) > 0","title":"Filters and tags"},{"location":"coding/python/pytest_cases/#pytest-marks-skip-xfail-on-cases","text":"pytest marks such as @pytest.mark.skipif can be applied on case functions the same way as with test functions . import sys import pytest @pytest . mark . skipif ( sys . version_info < ( 3 , 0 ), reason = \"Not useful on python 2\" ) def case_two_positive_ints (): return 1 , 2","title":"Pytest marks (skip, xfail...) on cases"},{"location":"coding/python/pytest_cases/#case-generators","text":"In many real-world usage we want to generate one test case per <something> . The most intuitive way would be to use a for loop to create the case functions, and to use the @case decorator to set their names ; however this would not be very readable. Instead, case functions can be parametrized the same way as with test functions : simply add the parameter names as arguments in their signature and decorate with @pytest.mark.parametrize . Even better, you can use the enhanced @parametrize from pytest-cases so as to benefit from its additional usability features: from pytest_cases import parametrize , parametrize_with_cases class CasesFoo : def case_hello ( self ): return \"hello world\" @parametrize ( who = ( 'you' , 'there' )) def case_simple_generator ( self , who ): return \"hello %s \" % who @parametrize_with_cases ( \"msg\" , cases = CasesFoo ) def test_foo ( msg ): assert isinstance ( msg , str ) and msg . startswith ( \"hello\" ) Yields test_generators.py::test_foo[hello] PASSED [ 33%] test_generators.py::test_foo[simple_generator-who=you] PASSED [ 66%] test_generators.py::test_foo[simple_generator-who=there] PASSED [100%]","title":"Case generators"},{"location":"coding/python/pytest_cases/#cases-requiring-fixtures","text":"Cases can use fixtures the same way as test functions do : simply add the fixture names as arguments in their signature and make sure the fixture exists either in the same module, or in a conftest.py file in one of the parent packages. See pytest documentation on sharing fixtures . Use @fixture instead of @pytest.fixture If a fixture is used by some of your cases only, then you should use the @fixture decorator from pytest-cases instead of the standard @pytest.fixture . Otherwise you fixture will be setup/teardown for all cases even those not requiring it. See @fixture doc . from pytest_cases import parametrize_with_cases , fixture , parametrize @fixture ( scope = 'session' ) def db (): return { 0 : 'louise' , 1 : 'bob' } def user_bob ( db ): return db [ 1 ] @parametrize ( id = range ( 2 )) def user_from_db ( db , id ): return db [ id ] @parametrize_with_cases ( \"a\" , cases = '.' , prefix = 'user_' ) def test_users ( a , db , request ): print ( \"this is test %r \" % request . node . nodeid ) assert a in db . values () Yields test_fixtures.py::test_users[a_is_bob] test_fixtures.py::test_users[a_is_from_db-id=0] test_fixtures.py::test_users[a_is_from_db-id=1]","title":"Cases requiring fixtures"},{"location":"coding/python/pytest_cases/#parametrize-fixtures-with-cases","text":"In some scenarios you might wish to parametrize a fixture with the cases, rather than the test function. For example: To inject the same test cases in several test functions without duplicating the @parametrize_with_cases decorator on each of them. To generate the test cases once for the whole session, using a scope='session' fixture or another scope . To modify the test cases, log some message, or perform some other action before injecting them into the test functions, and/or after executing the test function (thanks to yield fixtures ). For this, simply use @fixture from pytest_cases instead of @pytest.fixture to define your fixture. That allows your fixtures to be easily parametrized with @parametrize_with_cases , @parametrize , and even @pytest.mark.parametrize . from pytest_cases import fixture , parametrize_with_cases @fixture @parametrize_with_cases ( \"a,b\" ) def c ( a , b ): return a + b def test_foo ( c ): assert isinstance ( c , int )","title":"Parametrize fixtures with cases"},{"location":"coding/python/pytest_cases/#pytest-cases-internals","text":"","title":"Pytest-cases internals"},{"location":"coding/python/pytest_cases/#fixture","text":"@fixture is similar to pytest.fixture but without its param and ids arguments. Instead, it is able to pick the parametrization from @pytest.mark.parametrize marks applied on fixtures. This makes it very intuitive for users to parametrize both their tests and fixtures. Finally it now supports unpacking, see unpacking feature . @fixture deprecation if/when @pytest.fixture supports @pytest.mark.parametrize The ability for pytest fixtures to support the @pytest.mark.parametrize annotation is a feature that clearly belongs to pytest scope, and has been requested already . It is therefore expected that @fixture will be deprecated in favor of @pytest_fixture if/when the pytest team decides to add the proposed feature. As always, deprecation will happen slowly across versions (at least two minor, or one major version update) so as for users to have the time to update their code bases.","title":"@fixture"},{"location":"coding/python/pytest_cases/#unpack_fixture-unpack_into","text":"In some cases fixtures return a tuple or a list of items. It is not easy to refer to a single of these items in a test or another fixture. With unpack_fixture you can easily do it: import pytest from pytest_cases import unpack_fixture , fixture @fixture @pytest . mark . parametrize ( \"o\" , [ 'hello' , 'world' ]) def c ( o ): return o , o [ 0 ] a , b = unpack_fixture ( \"a,b\" , c ) def test_function ( a , b ): assert a [ 0 ] == b Note that you can also use the unpack_into= argument of @fixture to do the same thing: import pytest from pytest_cases import fixture @fixture ( unpack_into = \"a,b\" ) @pytest . mark . parametrize ( \"o\" , [ 'hello' , 'world' ]) def c ( o ): return o , o [ 0 ] def test_function ( a , b ): assert a [ 0 ] == b And it is also available in fixture_union : import pytest from pytest_cases import fixture , fixture_union @fixture @pytest . mark . parametrize ( \"o\" , [ 'hello' , 'world' ]) def c ( o ): return o , o [ 0 ] @fixture @pytest . mark . parametrize ( \"o\" , [ 'yeepee' , 'yay' ]) def d ( o ): return o , o [ 0 ] fixture_union ( \"c_or_d\" , [ c , d ], unpack_into = \"a, b\" ) def test_function ( a , b ): assert a [ 0 ] == b","title":"unpack_fixture / unpack_into"},{"location":"coding/python/pytest_cases/#param_fixtures","text":"If you wish to share some parameters across several fixtures and tests, it might be convenient to have a fixture representing this parameter. This is relatively easy for single parameters, but a bit harder for parameter tuples. The two utilities functions param_fixture (for a single parameter name) and param_fixtures (for a tuple of parameter names) handle the difficulty for you: import pytest from pytest_cases import param_fixtures , param_fixture # create a single parameter fixture my_parameter = param_fixture ( \"my_parameter\" , [ 1 , 2 , 3 , 4 ]) @pytest . fixture def fixture_uses_param ( my_parameter ): ... def test_uses_param ( my_parameter , fixture_uses_param ): ... # ----- # create a 2-tuple parameter fixture arg1 , arg2 = param_fixtures ( \"arg1, arg2\" , [( 1 , 2 ), ( 3 , 4 )]) @pytest . fixture def fixture_uses_param2 ( arg2 ): ... def test_uses_param2 ( arg1 , arg2 , fixture_uses_param2 ): ...","title":"param_fixture[s]"},{"location":"coding/python/pytest_cases/#fixture_union","text":"As of pytest 5, it is not possible to create a \"union\" fixture, i.e. a parametrized fixture that would first take all the possible values of fixture A, then all possible values of fixture B, etc. Indeed all fixture dependencies of each test node are grouped together, and if they have parameters a big \"cross-product\" of the parameters is done by pytest . from pytest_cases import fixture , fixture_union @fixture def first (): return 'hello' @fixture ( params = [ 'a' , 'b' ]) def second ( request ): return request . param # c will first take all the values of 'first', then all of 'second' c = fixture_union ( 'c' , [ first , second ]) def test_basic_union ( c ): print ( c ) yields <...>::test_basic_union[c_is_first] hello PASSED <...>::test_basic_union[c_is_second-a] a PASSED <...>::test_basic_union[c_is_second-b] b PASSED","title":"fixture_union"},{"location":"coding/python/pytest_cases/#references","text":"Docs Git","title":"References"},{"location":"coding/python/pytest_parametrized_testing/","text":"Parametrization is a process of running the same test with varying sets of data. Each combination of a test and data is counted as a new test case. There are multiple ways to parametrize your tests, each differs in complexity and flexibility. Parametrize the test \u2691 The most simple form of parametrization is at test level: @pytest . mark . parametrize ( \"number\" , [ 1 , 2 , 3 , 0 , 42 ]) def test_foo ( number ): assert number > 0 In this case we are getting five tests: for number 1, 2, 3, 0 and 42. Each of those tests can fail independently of one another (if in this example the test with 0 will fail, and four others will pass). Parametrize the fixtures \u2691 Fixtures may have parameters. Those parameters are passed as a list to the argument params of @pytest.fixture() decorator. Those parameters must be iterables, such as lists. Each parameter to a fixture is applied to each function using this fixture. If a few fixtures are used in one test function, pytest generates a Cartesian product of parameters of those fixtures. To use those parameters, a fixture must consume a special fixture named request . It provides the special (built-in) fixture with some information on the function it deals with. request also contains request.param which contains one element from params . The fixture called as many times as the number of elements in the iterable of params argument, and the test function is called with values of fixtures the same number of times. (basically, the fixture is called len(iterable) times with each next element of iterable in the request.param ). @pytest . fixture ( params = [ \"one\" , \"uno\" ]) def fixture1 ( request ): return request . param @pytest . fixture ( params = [ \"two\" , \"duo\" ]) def fixture2 ( request ): return request . paramdef test_foobar ( fixture1 , fixture2 ): assert type ( fixture1 ) == type ( fixture2 ) The output is: #OUTPUT 3 collected 4 itemstest_3.py::test_foobar[one-two] PASSED [ 25%] test_3.py::test_foobar[one-duo] PASSED [ 50%] test_3.py::test_foobar[uno-two] PASSED [ 75%] test_3.py::test_foobar[uno-duo] PASSED [100%] Parametrization with pytest_generate_tests \u2691 There is an another way to generate arbitrary parametrization at collection time. It\u2019s a bit more direct and verbose, but it provides introspection of test functions, including the ability to see all other fixture names. At collection time Pytest looks up for and calls (if found) a special function in each module, named pytest_generate_tests . This function is not a fixture, but just a regular function. It receives the argument metafunc , which itself is not a fixture, but a special object. pytest_generate_tests is called for each test function in the module to give a chance to parametrize it. Parametrization may happen only through fixtures that test function requests. There is no way to parametrize a test function like this: def test_simple(): assert 2+2 == 4 You need some variables to be used as parameters, and those variables should be arguments to the test function. Pytest will replace those arguments with values from fixtures, and if there are a few values for a fixture, then this is parametrization at work. metafunc argument to pytest_generate_tests provides some useful information on a test function: Ability to see all fixture names that function requests. Ability to see the name of the function. Ability to see code of the function. Finally, metafunc has a parametrize function, which is the way to provide multiple variants of values for fixtures. The same case as before written with the pytest_generate_tests function is: def pytest_generate_tests ( metafunc ): if \"fixture1\" in metafunc . fixturenames : metafunc . parametrize ( \"fixture1\" , [ \"one\" , \"uno\" ]) if \"fixture2\" in metafunc . fixturenames : metafunc . parametrize ( \"fixture2\" , [ \"two\" , \"duo\" ]) def test_foobar ( fixture1 , fixture2 ): assert type ( fixture1 ) == type ( fixture2 ) This solution is a little bit magical, so I'd avoid it in favor of pytest-cases. Use pytest-cases \u2691 pytest-case gives a lot of power when it comes to tweaking the fixtures and parameterizations. Check that file for further information. Customizations \u2691 Change the tests name \u2691 Sometimes you want to change how the tests are shown so you can understand better what the test is doing. You can use the ids argument to pytest.mark.parametrize . File tests/unit/test_func.py tasks_to_try = ( Task ( 'sleep' , done = True ), Task ( 'wake' , 'brian' ), Task ( 'wake' , 'brian' ), Task ( 'breathe' , 'BRIAN' , True ), Task ( 'exercise' , 'BrIaN' , False ), ) task_ids = [ f 'Task( { task . summary } , { task . owner } , { task . done } )' for task in tasks_to_try ] @pytest . mark . parametrize ( 'task' , tasks_to_try , ids = task_ids ) def test_add_4 ( task ): task_id = tasks . add ( task ) t_from_db = tasks . get ( task_id ) assert equivalent ( t_from_db , task ) $ pytest -v test_func.py::test_add_4 ===================== test session starts ====================== collected 5 items test_add_variety.py::test_add_4 [ Task ( sleep,None,True )] PASSED test_add_variety.py::test_add_4 [ Task ( wake,brian,False ) 0 ] PASSED test_add_variety.py::test_add_4 [ Task ( wake,brian,False ) 1 ] PASSED test_add_variety.py::test_add_4 [ Task ( breathe,BRIAN,True )] PASSED test_add_variety.py::test_add_4 [ Task ( exercise,BrIaN,False )] PASSED =================== 5 passed in 0 .04 seconds =================== Those identifiers can be used to run that specific test. For example pytest -v \"test_func.py::test_add_4[Task(breathe,BRIAN,True)]\" . parametrize() can be applied to classes as well. If the test id can't be derived from the parameter value, use the id argument for the pytest.param : @pytest . mark . parametrize ( 'task' , [ pytest . param ( Task ( 'create' ), id = 'just summary' ), pytest . param ( Task ( 'inspire' , 'Michelle' ), id = 'summary/owner' ), ]) def test_add_6 ( task ): ... Will yield: $ pytest-v test_add_variety.py::test_add_6 =================== test session starts ==================== collected 2 items test_add_variety.py::test_add_6 [ justsummary ] PASSED test_add_variety.py::test_add_6 [ summary/owner ] PASSED ================= 2 passed in 0 .05 seconds ================= References \u2691 A deep dive into Pytest parametrization by George Shulkin Book Python Testing with pytest by Brian Okken .","title":"Parametrized testing"},{"location":"coding/python/pytest_parametrized_testing/#parametrize-the-test","text":"The most simple form of parametrization is at test level: @pytest . mark . parametrize ( \"number\" , [ 1 , 2 , 3 , 0 , 42 ]) def test_foo ( number ): assert number > 0 In this case we are getting five tests: for number 1, 2, 3, 0 and 42. Each of those tests can fail independently of one another (if in this example the test with 0 will fail, and four others will pass).","title":"Parametrize the test"},{"location":"coding/python/pytest_parametrized_testing/#parametrize-the-fixtures","text":"Fixtures may have parameters. Those parameters are passed as a list to the argument params of @pytest.fixture() decorator. Those parameters must be iterables, such as lists. Each parameter to a fixture is applied to each function using this fixture. If a few fixtures are used in one test function, pytest generates a Cartesian product of parameters of those fixtures. To use those parameters, a fixture must consume a special fixture named request . It provides the special (built-in) fixture with some information on the function it deals with. request also contains request.param which contains one element from params . The fixture called as many times as the number of elements in the iterable of params argument, and the test function is called with values of fixtures the same number of times. (basically, the fixture is called len(iterable) times with each next element of iterable in the request.param ). @pytest . fixture ( params = [ \"one\" , \"uno\" ]) def fixture1 ( request ): return request . param @pytest . fixture ( params = [ \"two\" , \"duo\" ]) def fixture2 ( request ): return request . paramdef test_foobar ( fixture1 , fixture2 ): assert type ( fixture1 ) == type ( fixture2 ) The output is: #OUTPUT 3 collected 4 itemstest_3.py::test_foobar[one-two] PASSED [ 25%] test_3.py::test_foobar[one-duo] PASSED [ 50%] test_3.py::test_foobar[uno-two] PASSED [ 75%] test_3.py::test_foobar[uno-duo] PASSED [100%]","title":"Parametrize the fixtures"},{"location":"coding/python/pytest_parametrized_testing/#parametrization-with-pytest_generate_tests","text":"There is an another way to generate arbitrary parametrization at collection time. It\u2019s a bit more direct and verbose, but it provides introspection of test functions, including the ability to see all other fixture names. At collection time Pytest looks up for and calls (if found) a special function in each module, named pytest_generate_tests . This function is not a fixture, but just a regular function. It receives the argument metafunc , which itself is not a fixture, but a special object. pytest_generate_tests is called for each test function in the module to give a chance to parametrize it. Parametrization may happen only through fixtures that test function requests. There is no way to parametrize a test function like this: def test_simple(): assert 2+2 == 4 You need some variables to be used as parameters, and those variables should be arguments to the test function. Pytest will replace those arguments with values from fixtures, and if there are a few values for a fixture, then this is parametrization at work. metafunc argument to pytest_generate_tests provides some useful information on a test function: Ability to see all fixture names that function requests. Ability to see the name of the function. Ability to see code of the function. Finally, metafunc has a parametrize function, which is the way to provide multiple variants of values for fixtures. The same case as before written with the pytest_generate_tests function is: def pytest_generate_tests ( metafunc ): if \"fixture1\" in metafunc . fixturenames : metafunc . parametrize ( \"fixture1\" , [ \"one\" , \"uno\" ]) if \"fixture2\" in metafunc . fixturenames : metafunc . parametrize ( \"fixture2\" , [ \"two\" , \"duo\" ]) def test_foobar ( fixture1 , fixture2 ): assert type ( fixture1 ) == type ( fixture2 ) This solution is a little bit magical, so I'd avoid it in favor of pytest-cases.","title":"Parametrization with pytest_generate_tests"},{"location":"coding/python/pytest_parametrized_testing/#use-pytest-cases","text":"pytest-case gives a lot of power when it comes to tweaking the fixtures and parameterizations. Check that file for further information.","title":"Use pytest-cases"},{"location":"coding/python/pytest_parametrized_testing/#customizations","text":"","title":"Customizations"},{"location":"coding/python/pytest_parametrized_testing/#change-the-tests-name","text":"Sometimes you want to change how the tests are shown so you can understand better what the test is doing. You can use the ids argument to pytest.mark.parametrize . File tests/unit/test_func.py tasks_to_try = ( Task ( 'sleep' , done = True ), Task ( 'wake' , 'brian' ), Task ( 'wake' , 'brian' ), Task ( 'breathe' , 'BRIAN' , True ), Task ( 'exercise' , 'BrIaN' , False ), ) task_ids = [ f 'Task( { task . summary } , { task . owner } , { task . done } )' for task in tasks_to_try ] @pytest . mark . parametrize ( 'task' , tasks_to_try , ids = task_ids ) def test_add_4 ( task ): task_id = tasks . add ( task ) t_from_db = tasks . get ( task_id ) assert equivalent ( t_from_db , task ) $ pytest -v test_func.py::test_add_4 ===================== test session starts ====================== collected 5 items test_add_variety.py::test_add_4 [ Task ( sleep,None,True )] PASSED test_add_variety.py::test_add_4 [ Task ( wake,brian,False ) 0 ] PASSED test_add_variety.py::test_add_4 [ Task ( wake,brian,False ) 1 ] PASSED test_add_variety.py::test_add_4 [ Task ( breathe,BRIAN,True )] PASSED test_add_variety.py::test_add_4 [ Task ( exercise,BrIaN,False )] PASSED =================== 5 passed in 0 .04 seconds =================== Those identifiers can be used to run that specific test. For example pytest -v \"test_func.py::test_add_4[Task(breathe,BRIAN,True)]\" . parametrize() can be applied to classes as well. If the test id can't be derived from the parameter value, use the id argument for the pytest.param : @pytest . mark . parametrize ( 'task' , [ pytest . param ( Task ( 'create' ), id = 'just summary' ), pytest . param ( Task ( 'inspire' , 'Michelle' ), id = 'summary/owner' ), ]) def test_add_6 ( task ): ... Will yield: $ pytest-v test_add_variety.py::test_add_6 =================== test session starts ==================== collected 2 items test_add_variety.py::test_add_6 [ justsummary ] PASSED test_add_variety.py::test_add_6 [ summary/owner ] PASSED ================= 2 passed in 0 .05 seconds =================","title":"Change the tests name"},{"location":"coding/python/pytest_parametrized_testing/#references","text":"A deep dive into Pytest parametrization by George Shulkin Book Python Testing with pytest by Brian Okken .","title":"References"},{"location":"coding/python/python_anti_patterns/","text":"Mutable default arguments \u2691 What You Wrote \u2691 def append_to ( element , to = []): to . append ( element ) return to What You Might Have Expected to Happen \u2691 my_list = append_to ( 12 ) print ( my_list ) my_other_list = append_to ( 42 ) print ( my_other_list ) A new list is created each time the function is called if a second argument isn\u2019t provided, so that the output is: [ 12 ] [ 42 ] What Does Happen [ 12 ] [ 12 , 42 ] A new list is created once when the function is defined, and the same list is used in each successive call. Python\u2019s default arguments are evaluated once when the function is defined, not each time the function is called. This means that if you use a mutable default argument and mutate it, you will and have mutated that object for all future calls to the function as well. What You Should Do Instead \u2691 Create a new object each time the function is called, by using a default arg to signal that no argument was provided (None is often a good choice). def append_to ( element , to = None ): if to is None : to = [] to . append ( element ) return to Do not forget, you are passing a list object as the second argument. When the Gotcha Isn\u2019t a Gotcha \u2691 Sometimes you can specifically \u201cexploit\u201d this behavior to maintain state between calls of a function. This is often done when writing a caching function.","title":"Anti-Patterns"},{"location":"coding/python/python_anti_patterns/#mutable-default-arguments","text":"","title":"Mutable default arguments"},{"location":"coding/python/python_anti_patterns/#what-you-wrote","text":"def append_to ( element , to = []): to . append ( element ) return to","title":"What You Wrote"},{"location":"coding/python/python_anti_patterns/#what-you-might-have-expected-to-happen","text":"my_list = append_to ( 12 ) print ( my_list ) my_other_list = append_to ( 42 ) print ( my_other_list ) A new list is created each time the function is called if a second argument isn\u2019t provided, so that the output is: [ 12 ] [ 42 ] What Does Happen [ 12 ] [ 12 , 42 ] A new list is created once when the function is defined, and the same list is used in each successive call. Python\u2019s default arguments are evaluated once when the function is defined, not each time the function is called. This means that if you use a mutable default argument and mutate it, you will and have mutated that object for all future calls to the function as well.","title":"What You Might Have Expected to Happen"},{"location":"coding/python/python_anti_patterns/#what-you-should-do-instead","text":"Create a new object each time the function is called, by using a default arg to signal that no argument was provided (None is often a good choice). def append_to ( element , to = None ): if to is None : to = [] to . append ( element ) return to Do not forget, you are passing a list object as the second argument.","title":"What You Should Do Instead"},{"location":"coding/python/python_anti_patterns/#when-the-gotcha-isnt-a-gotcha","text":"Sometimes you can specifically \u201cexploit\u201d this behavior to maintain state between calls of a function. This is often done when writing a caching function.","title":"When the Gotcha Isn\u2019t a Gotcha"},{"location":"coding/python/python_code_styling/","text":"Not using setdefault() to initialize a dictionary \u2691 When initializing a dictionary, it is common to see a code check for the existence of a key and then create the key if it does not exist. Given a dictionary = {} , if you want to create a key if it doesn't exist, instead of doing: try : dictionary [ 'key' ] except KeyError : dictionary [ 'key' ] = {} You can use: dictionary . setdefault ( 'key' , {}) Commit message guidelines \u2691 I'm following the Angular commit convention that is backed up by python-semantic-release , with the idea of implementing automatic semantic versioning sometime in the future. Each commit message consists of a header, a body and a footer. The header has a defined format that includes a type, a scope and a subject: <type>(<scope>): <subject> <BLANK LINE> <body> <BLANK LINE> <footer> The header is mandatory and the scope of the header is optional. Any line of the commit message cannot be longer 100 characters. The footer should contain a closing reference to an issue if any. Samples: (even more samples) docs(changelog): update changelog to beta.5 fix(release): need to depend on latest rxjs and zone.js The version in our package.json gets copied to the one we publish, and users need the latest of these. docs(router): fix typo 'containa' to 'contains' (#36764) Closes #36763 PR Close #36764 Revert \u2691 If the commit reverts a previous commit, it should begin with revert: , followed by the header of the reverted commit. In the body it should say: This reverts commit <hash>. , where the hash is the SHA of the commit to revert. Type \u2691 Must be one of the following: build : Changes that affect the build system or external dependencies. ci : Changes to our CI configuration files and scripts. docs : Documentation changes. feat : A new feature. fix : A bug fix. perf : A code change that improves performance. refactor : A code change that neither fixes a bug nor adds a feature. style : Changes that do not affect the meaning of the code (white-space, formatting, missing semi-colons, etc). test : Adding missing tests or correcting existing tests. Subject \u2691 The subject contains a succinct description of the change: Use the imperative, present tense: \"change\" not \"changed\" nor \"changes\". Don't capitalize the first letter. No dot (.) at the end. Body \u2691 Same as in the subject, use the imperative, present tense: \"change\" not \"changed\" nor \"changes\". The body should include the motivation for the change and contrast this with previous behavior. Footer \u2691 The footer should contain any information about Breaking Changes and is also the place to reference issues that this commit Closes. Breaking Changes should start with the word BREAKING CHANGE: with a space or two newlines. The rest of the commit message is then used for this. Pre-commit \u2691 To ensure that your project follows these guidelines, add the following to your pre-commit configuration : File: .pre-commit-config.yaml - repo : https://github.com/commitizen-tools/commitizen rev : master hooks : - id : commitizen stages : [ commit-msg ] To make your life easier, change your workflow to use commitizen . In Vim, if you're using Vim fugitive change the configuration to: nnoremap <leader>gc :terminal cz c<CR> nnoremap <leader>gr :terminal cz c --retry<CR> \" Open terminal mode in insert mode if has('nvim') autocmd TermOpen term://* startinsert endif autocmd BufLeave term://* stopinsert If some pre-commit hook fails, make the changes and then use <leader>gr to repeat the same commit message. To automatically generate the changelog use cz bump --changelog --no-verify . The --no-verify part is required if you use pre-commit hooks . Whenever you want to release 1.0.0 , use cz bump --changelog --no-verify --increment MAJOR . Black code style \u2691 Black is a style guide enforcement tool. Flake8 \u2691 Flake8 is another style guide enforcement tool. f-strings \u2691 f-strings , also known as formatted string literals , are strings that have an f at the beginning and curly braces containing expressions that will be replaced with their values. Introduced in Python 3.6, they are more readable, concise, and less prone to error than other ways of formatting, as well as faster. >>> name = \"Eric\" >>> age = 74 >>> f \"Hello, { name } . You are { age } .\" 'Hello, Eric. You are 74.' Arbitrary expressions \u2691 Because f-strings are evaluated at runtime, you can put any valid Python expressions in them. For example, calling a function or method from within. >>> f \" { name . lower () } is funny.\" 'eric idle is funny.' Multiline f-strings \u2691 >>> name = \"Eric\" >>> profession = \"comedian\" >>> affiliation = \"Monty Python\" >>> message = ( ... f \"Hi { name } . \" ... f \"You are a { profession } . \" ... f \"You were in { affiliation } .\" ... ) >>> message 'Hi Eric. You are a comedian. You were in Monty Python.' Lint error fixes and ignores \u2691 Fix Pylint R0201 error \u2691 The error shows Method could be a function , it is used when there is no reference to the class, suggesting that the method could be used as a static function instead. Attempt using either of the decorators @classmethod or @staticmethod . If you don't need to change or use the class methods, use staticmethod . Example: Class Foo ( object ): ... def bar ( self , baz ): ... return llama Try instead to use: Class Foo ( object ): ... @classmethod def bar ( cls , baz ): ... return llama Or Class Foo ( object ): ... @staticmethod def bar ( baz ): ... return llama W1203 with F-strings \u2691 This rule suggest you to use the % interpolation in the logging methods because it might save some interpolation time when a logging statement is not run. Nevertheless the performance improvement is negligible and the advantages of using f-strings far outweigh them. W0106 in list comprehension \u2691 They just don't support it they suggest to use normal for loops. [SIM105 Use \u2691 'contextlib.suppress(Exception)']( https://docs.python.org/3/library/contextlib.html#contextlib.suppress ) To bypass exceptions, it's better to use: from contextlib import suppress with suppress ( FileNotFoundError ): os . remove ( 'somefile.tmp' ) Instead of: try : os . remove ( 'somefile.tmp' ) except FileNotFoundError : pass","title":"Code Styling"},{"location":"coding/python/python_code_styling/#not-using-setdefault-to-initialize-a-dictionary","text":"When initializing a dictionary, it is common to see a code check for the existence of a key and then create the key if it does not exist. Given a dictionary = {} , if you want to create a key if it doesn't exist, instead of doing: try : dictionary [ 'key' ] except KeyError : dictionary [ 'key' ] = {} You can use: dictionary . setdefault ( 'key' , {})","title":"Not using setdefault() to initialize a dictionary"},{"location":"coding/python/python_code_styling/#commit-message-guidelines","text":"I'm following the Angular commit convention that is backed up by python-semantic-release , with the idea of implementing automatic semantic versioning sometime in the future. Each commit message consists of a header, a body and a footer. The header has a defined format that includes a type, a scope and a subject: <type>(<scope>): <subject> <BLANK LINE> <body> <BLANK LINE> <footer> The header is mandatory and the scope of the header is optional. Any line of the commit message cannot be longer 100 characters. The footer should contain a closing reference to an issue if any. Samples: (even more samples) docs(changelog): update changelog to beta.5 fix(release): need to depend on latest rxjs and zone.js The version in our package.json gets copied to the one we publish, and users need the latest of these. docs(router): fix typo 'containa' to 'contains' (#36764) Closes #36763 PR Close #36764","title":"Commit message guidelines"},{"location":"coding/python/python_code_styling/#revert","text":"If the commit reverts a previous commit, it should begin with revert: , followed by the header of the reverted commit. In the body it should say: This reverts commit <hash>. , where the hash is the SHA of the commit to revert.","title":"Revert"},{"location":"coding/python/python_code_styling/#type","text":"Must be one of the following: build : Changes that affect the build system or external dependencies. ci : Changes to our CI configuration files and scripts. docs : Documentation changes. feat : A new feature. fix : A bug fix. perf : A code change that improves performance. refactor : A code change that neither fixes a bug nor adds a feature. style : Changes that do not affect the meaning of the code (white-space, formatting, missing semi-colons, etc). test : Adding missing tests or correcting existing tests.","title":"Type"},{"location":"coding/python/python_code_styling/#subject","text":"The subject contains a succinct description of the change: Use the imperative, present tense: \"change\" not \"changed\" nor \"changes\". Don't capitalize the first letter. No dot (.) at the end.","title":"Subject"},{"location":"coding/python/python_code_styling/#body","text":"Same as in the subject, use the imperative, present tense: \"change\" not \"changed\" nor \"changes\". The body should include the motivation for the change and contrast this with previous behavior.","title":"Body"},{"location":"coding/python/python_code_styling/#footer","text":"The footer should contain any information about Breaking Changes and is also the place to reference issues that this commit Closes. Breaking Changes should start with the word BREAKING CHANGE: with a space or two newlines. The rest of the commit message is then used for this.","title":"Footer"},{"location":"coding/python/python_code_styling/#pre-commit","text":"To ensure that your project follows these guidelines, add the following to your pre-commit configuration : File: .pre-commit-config.yaml - repo : https://github.com/commitizen-tools/commitizen rev : master hooks : - id : commitizen stages : [ commit-msg ] To make your life easier, change your workflow to use commitizen . In Vim, if you're using Vim fugitive change the configuration to: nnoremap <leader>gc :terminal cz c<CR> nnoremap <leader>gr :terminal cz c --retry<CR> \" Open terminal mode in insert mode if has('nvim') autocmd TermOpen term://* startinsert endif autocmd BufLeave term://* stopinsert If some pre-commit hook fails, make the changes and then use <leader>gr to repeat the same commit message. To automatically generate the changelog use cz bump --changelog --no-verify . The --no-verify part is required if you use pre-commit hooks . Whenever you want to release 1.0.0 , use cz bump --changelog --no-verify --increment MAJOR .","title":"Pre-commit"},{"location":"coding/python/python_code_styling/#black-code-style","text":"Black is a style guide enforcement tool.","title":"Black code style"},{"location":"coding/python/python_code_styling/#flake8","text":"Flake8 is another style guide enforcement tool.","title":"Flake8"},{"location":"coding/python/python_code_styling/#f-strings","text":"f-strings , also known as formatted string literals , are strings that have an f at the beginning and curly braces containing expressions that will be replaced with their values. Introduced in Python 3.6, they are more readable, concise, and less prone to error than other ways of formatting, as well as faster. >>> name = \"Eric\" >>> age = 74 >>> f \"Hello, { name } . You are { age } .\" 'Hello, Eric. You are 74.'","title":"f-strings"},{"location":"coding/python/python_code_styling/#arbitrary-expressions","text":"Because f-strings are evaluated at runtime, you can put any valid Python expressions in them. For example, calling a function or method from within. >>> f \" { name . lower () } is funny.\" 'eric idle is funny.'","title":"Arbitrary expressions"},{"location":"coding/python/python_code_styling/#multiline-f-strings","text":">>> name = \"Eric\" >>> profession = \"comedian\" >>> affiliation = \"Monty Python\" >>> message = ( ... f \"Hi { name } . \" ... f \"You are a { profession } . \" ... f \"You were in { affiliation } .\" ... ) >>> message 'Hi Eric. You are a comedian. You were in Monty Python.'","title":"Multiline f-strings"},{"location":"coding/python/python_code_styling/#lint-error-fixes-and-ignores","text":"","title":"Lint error fixes and ignores"},{"location":"coding/python/python_code_styling/#fix-pylint-r0201-error","text":"The error shows Method could be a function , it is used when there is no reference to the class, suggesting that the method could be used as a static function instead. Attempt using either of the decorators @classmethod or @staticmethod . If you don't need to change or use the class methods, use staticmethod . Example: Class Foo ( object ): ... def bar ( self , baz ): ... return llama Try instead to use: Class Foo ( object ): ... @classmethod def bar ( cls , baz ): ... return llama Or Class Foo ( object ): ... @staticmethod def bar ( baz ): ... return llama","title":"Fix Pylint R0201 error"},{"location":"coding/python/python_code_styling/#w1203-with-f-strings","text":"This rule suggest you to use the % interpolation in the logging methods because it might save some interpolation time when a logging statement is not run. Nevertheless the performance improvement is negligible and the advantages of using f-strings far outweigh them.","title":"W1203 with F-strings"},{"location":"coding/python/python_code_styling/#w0106-in-list-comprehension","text":"They just don't support it they suggest to use normal for loops.","title":"W0106 in list comprehension"},{"location":"coding/python/python_code_styling/#sim105-use","text":"'contextlib.suppress(Exception)']( https://docs.python.org/3/library/contextlib.html#contextlib.suppress ) To bypass exceptions, it's better to use: from contextlib import suppress with suppress ( FileNotFoundError ): os . remove ( 'somefile.tmp' ) Instead of: try : os . remove ( 'somefile.tmp' ) except FileNotFoundError : pass","title":"[SIM105 Use"},{"location":"coding/python/python_config_yaml/","text":"Several programs load the configuration from file. After trying ini, json and yaml I've seen that the last one is the most comfortable. So here are the templates for the tests and class that loads the data from a yaml file and exposes it as a dictionary. In the following sections Jinja templating is used, so substitute everything between {{ }} to their correct values. It's assumed that: The root directory of the project has the same name as the program. A file with a valid config exists in assets/config.yaml . We'll use this file in the documentation, so comment it through. Code \u2691 The class code below is expected to introduced in the file configuration.py . Variables to substitute: program_name File {{ program_name }}/configuration.py \"\"\" Module to define the configuration of the main program. Classes: Config: Class to manipulate the configuration of the program. \"\"\" from collections import UserDict from ruamel.yaml import YAML from ruamel.yaml.scanner import ScannerError import logging import os import sys log = logging . getLogger ( __name__ ) class Config ( UserDict ): \"\"\" Class to manipulate the configuration of the program. Arguments: config_path (str): Path to the configuration file. Default: ~/.local/share/{{ program_name }}/config.yaml Public methods: get: Fetch the configuration value of the specified key. If there are nested dictionaries, a dot notation can be used. load: Loads configuration from configuration YAML file. save: Saves configuration in the configuration YAML file. Attributes and properties: config_path (str): Path to the configuration file. data(dict): Program configuration. \"\"\" def __init__ ( self , config_path = '~/.local/share/{{ program_name }}/config.yaml' ): self . config_path = os . path . expanduser ( config_path ) self . load () def get ( self , key ): \"\"\" Fetch the configuration value of the specified key. If there are nested dictionaries, a dot notation can be used. So if the configuration contents are: self.data = { 'first': { 'second': 'value' }, } self.data.get('first.second') == 'value' Arguments: key(str): Configuration key to fetch \"\"\" keys = key . split ( '.' ) value = self . data . copy () for key in keys : value = value [ key ] return value def load ( self ): \"\"\" Loads configuration from configuration YAML file. \"\"\" try : with open ( os . path . expanduser ( self . config_path ), 'r' ) as f : try : self . data = YAML () . load ( f ) except ScannerError as e : log . error ( 'Error parsing yaml of configuration file ' ' {} : {} ' . format ( e . problem_mark , e . problem , ) ) sys . exit ( 1 ) except FileNotFoundError : log . error ( 'Error opening configuration file {} ' . format ( self . config_path ) ) sys . exit ( 1 ) def save ( self ): \"\"\" Saves configuration in the configuration YAML file. \"\"\" with open ( os . path . expanduser ( self . config_path ), 'w+' ) as f : yaml = YAML () yaml . default_flow_style = False yaml . dump ( self . data , f ) We use ruamel PyYAML implementation to preserve the file comments. That class is meant to be loaded in the main __init__.py file, below the logging configuration (if there is any). Variables to substitute: program_name config_environmental_variable : The optional environmental variable where the path to the configuration is set. For example PYDO_CONFIG . This will be used in the tests to override the default path. We don't load this configuration from the program argument parser because it's definition often depends on the config file. File {{ program_name}}/ init .py # ... # (optional logging definition) import os from {{ program_name }} . configuration import Config config = Config ( os . getenv ( '{{ config_environmental_variable }}' , '~/.local/share/{{ program_name }}/config.yaml' )) # (Rest of the program) # ... If you want to use the config object in a module of your program, import them from the above file like this: File {{ program_name }}/cli.py from {{ program_name }} import config Tests \u2691 I feel that the tests should use the default configuration, therefore we're setting the environmental variable in the conftest.py file that gets executed by pytest in the tests setup. Variables to substitute: config_environmental_variable : Same as the one defined in the last section. File tests/conftest.py import os os . environ [{{ config_environmental_variable }}] = 'assets/config.yaml' Variables to substitute: program_name As it's really dependent in the config structure, you can improve the test_config_load test to make it more meaningful. File tests/unit/test_configuration.py from {{ program_name }} . configuration import Config from unittest.mock import patch from ruamel.yaml.scanner import ScannerError import os import pytest import shutil import tempfile class TestConfig : \"\"\" Class to test the Config object. Public attributes: config (Config object): Config object to test \"\"\" @pytest . fixture ( autouse = True ) def setup ( self ): self . config_path = 'assets/config.yaml' self . log_patch = patch ( '{{ program_name }}.configuration.log' , autospect = True ) self . log = self . log_patch . start () self . sys_patch = patch ( '{{ program_name }}.configuration.sys' , autospect = True ) self . sys = self . sys_patch . start () self . config = Config ( self . config_path ) yield 'setup' self . log_patch . stop () self . sys_patch . stop () def test_get_can_fetch_nested_items_with_dots ( self ): self . config . data = { 'first' : { 'second' : 'value' }, } assert self . config . get ( 'first.second' ) == 'value' def test_config_can_fetch_nested_items_with_dictionary_notation ( self ): self . config . data = { 'first' : { 'second' : 'value' }, } assert self . config [ 'first' ][ 'second' ] == 'value' def test_config_load ( self ): self . config . load () assert len ( self . config . data ) > 0 @patch ( '{{ program_name }}.configuration.YAML' ) def test_load_handles_wrong_file_format ( self , yamlMock ): yamlMock . return_value . load . side_effect = ScannerError ( 'error' , '' , 'problem' , 'mark' , ) self . config . load () self . log . error . assert_called_once_with ( 'Error parsing yaml of configuration file mark: problem' ) self . sys . exit . assert_called_once_with ( 1 ) @patch ( '{{ program_name }}.configuration.open' ) def test_load_handles_file_not_found ( self , openMock ): openMock . side_effect = FileNotFoundError () self . config . load () self . log . error . assert_called_once_with ( 'Error opening configuration file {} ' . format ( self . config_path ) ) self . sys . exit . assert_called_once_with ( 1 ) @patch ( '{{ program_name }}.configuration.Config.load' ) def test_init_calls_config_load ( self , loadMock ): Config () loadMock . assert_called_once_with () def test_save_config ( self ): tmp = tempfile . mkdtemp () save_file = os . path . join ( tmp , 'yaml_save_test.yaml' ) self . config = Config ( save_file ) self . config . data = { 'a' : 'b' } self . config . save () with open ( save_file , 'r' ) as f : assert \"a:\" in f . read () shutil . rmtree ( tmp ) Installation \u2691 It's always nice to have the default configuration template (with it's documentation) when configuring your use case. Therefore we're going to add a step in the installation process to copy the file. Variables to substitute in both files: program_name File setup.py import shutil ... Class PostInstallCommand ( install ): ... def run ( self ): install . run ( self ) try : data_directory = os . path . expanduser ( \"~/.local/share/{{ program_name }}\" ) os . makedirs ( data_directory ) log . info ( \"Data directory created\" ) except FileExistsError : log . info ( \"Data directory already exits\" ) config_path = os . path . join ( data_directory , 'config.yaml' ) if os . path . isfile ( config_path ) and os . access ( config_path , os . R_OK ): log . info ( \"Configuration file already exists, check the documentation \" \"for the new version changes.\" ) else : shutil . copyfile ( 'assets/config.yaml' , config_path ) log . info ( \"Copied default configuration template\" ) assets_url : Url pointing to the assets file, similar to https://github.com/lyz-code/pydo/blob/master/assets/config.yaml . README.md ... {{ program_name }} configuration is done through the yaml file located at ~/.local/share/{{ program_name }}/config.yaml . The default template is provided at installation time. ... It's also necessary to add the ruamel.yaml pip package to your setup.py and requirements.txt files.","title":"Load config from YAML"},{"location":"coding/python/python_config_yaml/#code","text":"The class code below is expected to introduced in the file configuration.py . Variables to substitute: program_name File {{ program_name }}/configuration.py \"\"\" Module to define the configuration of the main program. Classes: Config: Class to manipulate the configuration of the program. \"\"\" from collections import UserDict from ruamel.yaml import YAML from ruamel.yaml.scanner import ScannerError import logging import os import sys log = logging . getLogger ( __name__ ) class Config ( UserDict ): \"\"\" Class to manipulate the configuration of the program. Arguments: config_path (str): Path to the configuration file. Default: ~/.local/share/{{ program_name }}/config.yaml Public methods: get: Fetch the configuration value of the specified key. If there are nested dictionaries, a dot notation can be used. load: Loads configuration from configuration YAML file. save: Saves configuration in the configuration YAML file. Attributes and properties: config_path (str): Path to the configuration file. data(dict): Program configuration. \"\"\" def __init__ ( self , config_path = '~/.local/share/{{ program_name }}/config.yaml' ): self . config_path = os . path . expanduser ( config_path ) self . load () def get ( self , key ): \"\"\" Fetch the configuration value of the specified key. If there are nested dictionaries, a dot notation can be used. So if the configuration contents are: self.data = { 'first': { 'second': 'value' }, } self.data.get('first.second') == 'value' Arguments: key(str): Configuration key to fetch \"\"\" keys = key . split ( '.' ) value = self . data . copy () for key in keys : value = value [ key ] return value def load ( self ): \"\"\" Loads configuration from configuration YAML file. \"\"\" try : with open ( os . path . expanduser ( self . config_path ), 'r' ) as f : try : self . data = YAML () . load ( f ) except ScannerError as e : log . error ( 'Error parsing yaml of configuration file ' ' {} : {} ' . format ( e . problem_mark , e . problem , ) ) sys . exit ( 1 ) except FileNotFoundError : log . error ( 'Error opening configuration file {} ' . format ( self . config_path ) ) sys . exit ( 1 ) def save ( self ): \"\"\" Saves configuration in the configuration YAML file. \"\"\" with open ( os . path . expanduser ( self . config_path ), 'w+' ) as f : yaml = YAML () yaml . default_flow_style = False yaml . dump ( self . data , f ) We use ruamel PyYAML implementation to preserve the file comments. That class is meant to be loaded in the main __init__.py file, below the logging configuration (if there is any). Variables to substitute: program_name config_environmental_variable : The optional environmental variable where the path to the configuration is set. For example PYDO_CONFIG . This will be used in the tests to override the default path. We don't load this configuration from the program argument parser because it's definition often depends on the config file. File {{ program_name}}/ init .py # ... # (optional logging definition) import os from {{ program_name }} . configuration import Config config = Config ( os . getenv ( '{{ config_environmental_variable }}' , '~/.local/share/{{ program_name }}/config.yaml' )) # (Rest of the program) # ... If you want to use the config object in a module of your program, import them from the above file like this: File {{ program_name }}/cli.py from {{ program_name }} import config","title":"Code"},{"location":"coding/python/python_config_yaml/#tests","text":"I feel that the tests should use the default configuration, therefore we're setting the environmental variable in the conftest.py file that gets executed by pytest in the tests setup. Variables to substitute: config_environmental_variable : Same as the one defined in the last section. File tests/conftest.py import os os . environ [{{ config_environmental_variable }}] = 'assets/config.yaml' Variables to substitute: program_name As it's really dependent in the config structure, you can improve the test_config_load test to make it more meaningful. File tests/unit/test_configuration.py from {{ program_name }} . configuration import Config from unittest.mock import patch from ruamel.yaml.scanner import ScannerError import os import pytest import shutil import tempfile class TestConfig : \"\"\" Class to test the Config object. Public attributes: config (Config object): Config object to test \"\"\" @pytest . fixture ( autouse = True ) def setup ( self ): self . config_path = 'assets/config.yaml' self . log_patch = patch ( '{{ program_name }}.configuration.log' , autospect = True ) self . log = self . log_patch . start () self . sys_patch = patch ( '{{ program_name }}.configuration.sys' , autospect = True ) self . sys = self . sys_patch . start () self . config = Config ( self . config_path ) yield 'setup' self . log_patch . stop () self . sys_patch . stop () def test_get_can_fetch_nested_items_with_dots ( self ): self . config . data = { 'first' : { 'second' : 'value' }, } assert self . config . get ( 'first.second' ) == 'value' def test_config_can_fetch_nested_items_with_dictionary_notation ( self ): self . config . data = { 'first' : { 'second' : 'value' }, } assert self . config [ 'first' ][ 'second' ] == 'value' def test_config_load ( self ): self . config . load () assert len ( self . config . data ) > 0 @patch ( '{{ program_name }}.configuration.YAML' ) def test_load_handles_wrong_file_format ( self , yamlMock ): yamlMock . return_value . load . side_effect = ScannerError ( 'error' , '' , 'problem' , 'mark' , ) self . config . load () self . log . error . assert_called_once_with ( 'Error parsing yaml of configuration file mark: problem' ) self . sys . exit . assert_called_once_with ( 1 ) @patch ( '{{ program_name }}.configuration.open' ) def test_load_handles_file_not_found ( self , openMock ): openMock . side_effect = FileNotFoundError () self . config . load () self . log . error . assert_called_once_with ( 'Error opening configuration file {} ' . format ( self . config_path ) ) self . sys . exit . assert_called_once_with ( 1 ) @patch ( '{{ program_name }}.configuration.Config.load' ) def test_init_calls_config_load ( self , loadMock ): Config () loadMock . assert_called_once_with () def test_save_config ( self ): tmp = tempfile . mkdtemp () save_file = os . path . join ( tmp , 'yaml_save_test.yaml' ) self . config = Config ( save_file ) self . config . data = { 'a' : 'b' } self . config . save () with open ( save_file , 'r' ) as f : assert \"a:\" in f . read () shutil . rmtree ( tmp )","title":"Tests"},{"location":"coding/python/python_config_yaml/#installation","text":"It's always nice to have the default configuration template (with it's documentation) when configuring your use case. Therefore we're going to add a step in the installation process to copy the file. Variables to substitute in both files: program_name File setup.py import shutil ... Class PostInstallCommand ( install ): ... def run ( self ): install . run ( self ) try : data_directory = os . path . expanduser ( \"~/.local/share/{{ program_name }}\" ) os . makedirs ( data_directory ) log . info ( \"Data directory created\" ) except FileExistsError : log . info ( \"Data directory already exits\" ) config_path = os . path . join ( data_directory , 'config.yaml' ) if os . path . isfile ( config_path ) and os . access ( config_path , os . R_OK ): log . info ( \"Configuration file already exists, check the documentation \" \"for the new version changes.\" ) else : shutil . copyfile ( 'assets/config.yaml' , config_path ) log . info ( \"Copied default configuration template\" ) assets_url : Url pointing to the assets file, similar to https://github.com/lyz-code/pydo/blob/master/assets/config.yaml . README.md ... {{ program_name }} configuration is done through the yaml file located at ~/.local/share/{{ program_name }}/config.yaml . The default template is provided at installation time. ... It's also necessary to add the ruamel.yaml pip package to your setup.py and requirements.txt files.","title":"Installation"},{"location":"coding/python/python_project_template/","text":"It's hard to correctly define the directory structure to make python programs work as expected. Even more if testing, documentation or databases are involved. I've automated the creation of the python project skeleton following most of these section guidelines with this cookiecutter template . cruft https://github.com/lyz-code/cookiecutter-python-project If you don't know cruft, take a look here . Basic Python project \u2691 Create virtualenv mkdir {{ project_directory }} mkvirtualenv --python = python3 -a {{ project_directory }} {{ project_name }} Create git repository workon {{ project_name }} git init . git ignore-io python > .gitignore git add . git commit -m \"Added gitignore\" git checkout -b 'feat/initial_iteration' Project structure \u2691 After using different project structures, I've ended up using the following: . \u251c\u2500\u2500\u2500 docs \u2502 \u251c\u2500\u2500 ... \u2502 \u2514\u2500\u2500 index.md \u251c\u2500\u2500 LICENSE \u251c\u2500\u2500 Makefile \u251c\u2500\u2500 mkdocs.yml \u251c\u2500\u2500 mypy.ini \u251c\u2500\u2500 pyproject.toml \u251c\u2500\u2500 README.md \u251c\u2500\u2500 requirements-dev.in \u251c\u2500\u2500 setup.py \u251c\u2500\u2500 src \u2502 \u2514\u2500\u2500 package_name \u2502 \u251c\u2500\u2500 adapters \u2502 \u2502 \u2514\u2500\u2500 __init__.py \u2502 \u251c\u2500\u2500 config.py \u2502 \u251c\u2500\u2500 entrypoints \u2502 \u2502 \u2514\u2500\u2500 __init__.py \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u251c\u2500\u2500 model \u2502 \u2502 \u2514\u2500\u2500 __init__.py \u2502 \u251c\u2500\u2500 py.typed \u2502 \u251c\u2500\u2500 services.py \u2502 \u2514\u2500\u2500 version.py \u2514\u2500\u2500 tests \u251c\u2500\u2500 conftest.py \u251c\u2500\u2500 e2e \u2502 \u2514\u2500\u2500 __init__.py \u251c\u2500\u2500 __init__.py \u251c\u2500\u2500 integration \u2502 \u2514\u2500\u2500 __init__.py \u2514\u2500\u2500 unit \u251c\u2500\u2500 __init__.py \u2514\u2500\u2500 test_services.py Heavily inspired by ionel packaging python library post and the Architecture Patterns with Python book by Harry J.W. Percival and Bob Gregory. Project types \u2691 Depending on the type of project you want to build there are different layouts: Command-line program . A single Flask web application . Multiple interconnected Flask microservices . Additional configurations \u2691 Once the basic project structure is defined, there are several common enhancements to be applied: Manage dependencies with pip-tools Create the documentation repository Continuous integration pipelines Configure SQLAlchemy to use the MariaDB/Mysql backend Configure Docker and Docker compose to host the application Load config from YAML Configure a Flask project Code tests \u2691 Unit, integration, end-to-end, edge-to-edge tests define the behaviour of the application. Trigger hooks: Github Actions: To run the tests each time a push or pull request is created in Github, create the .github/workflows/test.yml file with the following Jinja template. Make sure to check: The correct Python versions are configured. The steps make sense to your case scenario. Variables to substitute: program_name : your program name --- name : Python package on : [ push , pull_request ] jobs : build : runs-on : ubuntu-latest strategy : max-parallel : 3 matrix : python-version : [ 3.6 , 3.7 , 3.8 ] steps : - uses : actions/checkout@v1 - name : Set up Python ${{ matrix.python-version }} uses : actions/setup-python@v1 with : python-version : ${{ matrix.python-version }} - name : Install dependencies run : | python -m pip install --upgrade pip pip install -r requirements.txt - name : Lint with flake8 run : | pip install flake8 # stop the build if there are Python syntax errors or undefined names flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics # exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide flake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics - name : Test with pytest run : | pip install pytest pytest-cov python -m pytest --cov-report term-missing --cov {{ program_name }} tests If you want to add a badge stating the last build status in your readme, use the following template. Variables to substitute: repository_url : Github repository url, like https://github.com/lyz-code/pydo . [ ![Actions Status ]( {{ repository_url }}/workflows/Python%20package/badge.svg )]({{ repository_url }}/actions) References \u2691 ionel packaging a python library post and he's cookiecutter template","title":"Project Template"},{"location":"coding/python/python_project_template/#basic-python-project","text":"Create virtualenv mkdir {{ project_directory }} mkvirtualenv --python = python3 -a {{ project_directory }} {{ project_name }} Create git repository workon {{ project_name }} git init . git ignore-io python > .gitignore git add . git commit -m \"Added gitignore\" git checkout -b 'feat/initial_iteration'","title":"Basic Python project"},{"location":"coding/python/python_project_template/#project-structure","text":"After using different project structures, I've ended up using the following: . \u251c\u2500\u2500\u2500 docs \u2502 \u251c\u2500\u2500 ... \u2502 \u2514\u2500\u2500 index.md \u251c\u2500\u2500 LICENSE \u251c\u2500\u2500 Makefile \u251c\u2500\u2500 mkdocs.yml \u251c\u2500\u2500 mypy.ini \u251c\u2500\u2500 pyproject.toml \u251c\u2500\u2500 README.md \u251c\u2500\u2500 requirements-dev.in \u251c\u2500\u2500 setup.py \u251c\u2500\u2500 src \u2502 \u2514\u2500\u2500 package_name \u2502 \u251c\u2500\u2500 adapters \u2502 \u2502 \u2514\u2500\u2500 __init__.py \u2502 \u251c\u2500\u2500 config.py \u2502 \u251c\u2500\u2500 entrypoints \u2502 \u2502 \u2514\u2500\u2500 __init__.py \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u251c\u2500\u2500 model \u2502 \u2502 \u2514\u2500\u2500 __init__.py \u2502 \u251c\u2500\u2500 py.typed \u2502 \u251c\u2500\u2500 services.py \u2502 \u2514\u2500\u2500 version.py \u2514\u2500\u2500 tests \u251c\u2500\u2500 conftest.py \u251c\u2500\u2500 e2e \u2502 \u2514\u2500\u2500 __init__.py \u251c\u2500\u2500 __init__.py \u251c\u2500\u2500 integration \u2502 \u2514\u2500\u2500 __init__.py \u2514\u2500\u2500 unit \u251c\u2500\u2500 __init__.py \u2514\u2500\u2500 test_services.py Heavily inspired by ionel packaging python library post and the Architecture Patterns with Python book by Harry J.W. Percival and Bob Gregory.","title":"Project structure"},{"location":"coding/python/python_project_template/#project-types","text":"Depending on the type of project you want to build there are different layouts: Command-line program . A single Flask web application . Multiple interconnected Flask microservices .","title":"Project types"},{"location":"coding/python/python_project_template/#additional-configurations","text":"Once the basic project structure is defined, there are several common enhancements to be applied: Manage dependencies with pip-tools Create the documentation repository Continuous integration pipelines Configure SQLAlchemy to use the MariaDB/Mysql backend Configure Docker and Docker compose to host the application Load config from YAML Configure a Flask project","title":"Additional configurations"},{"location":"coding/python/python_project_template/#code-tests","text":"Unit, integration, end-to-end, edge-to-edge tests define the behaviour of the application. Trigger hooks: Github Actions: To run the tests each time a push or pull request is created in Github, create the .github/workflows/test.yml file with the following Jinja template. Make sure to check: The correct Python versions are configured. The steps make sense to your case scenario. Variables to substitute: program_name : your program name --- name : Python package on : [ push , pull_request ] jobs : build : runs-on : ubuntu-latest strategy : max-parallel : 3 matrix : python-version : [ 3.6 , 3.7 , 3.8 ] steps : - uses : actions/checkout@v1 - name : Set up Python ${{ matrix.python-version }} uses : actions/setup-python@v1 with : python-version : ${{ matrix.python-version }} - name : Install dependencies run : | python -m pip install --upgrade pip pip install -r requirements.txt - name : Lint with flake8 run : | pip install flake8 # stop the build if there are Python syntax errors or undefined names flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics # exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide flake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics - name : Test with pytest run : | pip install pytest pytest-cov python -m pytest --cov-report term-missing --cov {{ program_name }} tests If you want to add a badge stating the last build status in your readme, use the following template. Variables to substitute: repository_url : Github repository url, like https://github.com/lyz-code/pydo . [ ![Actions Status ]( {{ repository_url }}/workflows/Python%20package/badge.svg )]({{ repository_url }}/actions)","title":"Code tests"},{"location":"coding/python/python_project_template/#references","text":"ionel packaging a python library post and he's cookiecutter template","title":"References"},{"location":"coding/python/python_snippets/","text":"Locate element in list \u2691 a = [ 'a' , 'b' ] index = a . index ( 'b' ) Transpose a list of lists \u2691 >>> l = [[ 1 , 2 , 3 ],[ 4 , 5 , 6 ],[ 7 , 8 , 9 ]] >>> [ list ( i ) for i in zip ( * l )] ... [[ 1 , 4 , 7 ], [ 2 , 5 , 8 ], [ 3 , 6 , 9 ]] Check the type of a list of strings \u2691 def _is_list_of_lists ( data : Any ) -> bool : \"\"\"Check if data is a list of strings.\"\"\" if data and isinstance ( data , list ): return all ( isinstance ( elem , list ) for elem in data ) else : return False Install default directories and files for a command line program \u2691 I've been trying for a long time to configure setup.py to run the required steps to configure the required directories and files when doing pip install without success. Finally, I decided that the program itself should create the data once the FileNotFoundError exception is found. That way, you don't penalize the load time because if the file or directory exists, that code is not run. Check if a dictionary is a subset of another \u2691 If you have two dictionaries big = {'a': 1, 'b': 2, 'c':3} and small = {'c': 3, 'a': 1} , and want to check whether small is a subset of big , use the next snippet: >>> small . items () <= big . items () True As the code is not very common or intuitive, I'd add a comment to explain what you're doing. When to use isinstance and when to use type \u2691 isinstance takes into account inheritance, while type doesn't. So if we have the next code: class Shape : pass class Rectangle ( Shape ): def __init__ ( self , length , width ): self . length = length self . width = width self . area = length * width def get_area ( self ): return self . length * self . width class Square ( Rectangle ): def __init__ ( self , length ): Rectangle . __init__ ( self , length , length ) And we want to check if an object a = Square(5) is of type Rectangle , we could not use isinstance because it'll return True as it's a subclass of Rectangle : >>> isinstance ( a , Rectangle ) True Instead, use a comparison with type : >>> type ( a ) == Rectangle False Find a static file of a python module \u2691 Useful when you want to initialize a configuration file of a cli program when it's not present. Imagine you have a setup.py with the next contents: setup ( name = \"pynbox\" , packages = find_packages ( \"src\" ), package_dir = { \"\" : \"src\" }, package_data = { \"pynbox\" : [ \"py.typed\" , \"assets/config.yaml\" ]}, Then you could import the data with: import pkg_resources file_path = pkg_resources . resource_filename ( \"pynbox\" , \"assets/config.yaml\" ), Delete a file \u2691 import os os . remove ( 'demofile.txt' ) Measure elapsed time between lines of code \u2691 import time start = time . time () print ( \"hello\" ) end = time . time () print ( end - start ) Create combination of elements in groups of two \u2691 Using the combinations function in Python's itertools module: >>> list ( itertools . combinations ( 'ABC' , 2 )) [( 'A' , 'B' ), ( 'A' , 'C' ), ( 'B' , 'C' )] If you want the permutations use itertools.permutations . Convert html to readable plaintext \u2691 pip install html2text import html2text html = open ( \"foobar.html\" ) . read () print ( html2text . html2text ( html )) Parse a datetime from a string \u2691 from dateutil import parser parser . parse ( \"Aug 28 1999 12:00AM\" ) # datetime.datetime(1999, 8, 28, 0, 0) Install a python dependency from a git repository \u2691 With pip you can : pip install git+git://github.com/path/to/repository@master If you want to hard code it in your setup.py , you need to: install_requires = [ 'some-pkg @ git+ssh://git@github.com/someorgname/pkg-repo-name@v1.1#egg=some-pkg' , ] But Pypi won't allow you to upload the package , as it will give you an error: HTTPError: 400 Bad Request from https://test.pypi.org/legacy/ Invalid value for requires_dist. Error: Can't have direct dependency: 'deepdiff @ git+git://github.com/lyz-code/deepdiff@master' It looks like this is a conscious decision on the PyPI side. Basically, they don't want pip to reach out to URLs outside their site when installing from PyPI. An ugly patch is to install the dependencies in a PostInstall custom script in the setup.py of your program: from setuptools.command.install import install from subprocess import getoutput # ignore: cannot subclass install, has type Any. And what would you do? class PostInstall ( install ): # type: ignore \"\"\"Install direct dependency. Pypi doesn't allow uploading packages with direct dependencies, so we need to install them manually. \"\"\" def run ( self ) -> None : \"\"\"Install dependencies.\"\"\" install . run ( self ) print ( getoutput ( \"pip install git+git://github.com/lyz-code/deepdiff@master\" )) setup ( cmdclass = { 'install' : PostInstall } ) It may not work! Last time I used this solution, when I added the library on a setup.py the direct dependencies weren't installed :S Check directories and files \u2691 def test_dir ( directory ): from os.path import exists from os import makedirs if not exists ( directory ): makedirs ( directory ) def test_file ( filepath , mode ): ''' Check if a file exist and is accessible. ''' def check_mode ( os_mode , mode ): if os . path . isfile ( filepath ) and os . access ( filepath , os_mode ): return else : raise IOError ( \"Can't access the file with mode \" + mode ) if mode is 'r' : check_mode ( os . R_OK , mode ) elif mode is 'w' : check_mode ( os . W_OK , mode ) elif mode is 'a' : check_mode ( os . R_OK , mode ) check_mode ( os . W_OK , mode ) Remove the extension of a file \u2691 os . path . splitext ( \"/path/to/some/file.txt\" )[ 0 ] Iterate over the files of a directory \u2691 import os directory = '/path/to/directory' for entry in os . scandir ( directory ): if ( entry . path . endswith ( \".jpg\" ) or entry . path . endswith ( \".png\" )) and entry . is_file (): print ( entry . path ) Create directory \u2691 if not os . path . exists ( directory ): os . makedirs ( directory ) Touch a file \u2691 from pathlib import Path Path ( 'path/to/file.txt' ) . touch () Get the first day of next month \u2691 current = datetime . datetime ( mydate . year , mydate . month , 1 ) next_month = datetime . datetime ( mydate . year + int ( mydate . month / 12 ), (( mydate . month % 12 ) + 1 ), 1 ) Get the week number of a datetime \u2691 datetime.datetime has a isocalendar() method, which returns a tuple containing the calendar week: >>> import datetime >>> datetime . datetime ( 2010 , 6 , 16 ) . isocalendar ()[ 1 ] 24 datetime.date.isocalendar() is an instance-method returning a tuple containing year, weeknumber and weekday in respective order for the given date instance. Get the monday of a week number \u2691 A week number is not enough to generate a date; you need a day of the week as well. Add a default: import datetime d = \"2013-W26\" r = datetime . datetime . strptime ( d + '-1' , \"%Y-W%W-%w\" ) The -1 and -%w pattern tells the parser to pick the Monday in that week. Get the month name from a number \u2691 import calendar >> calendar . month_name [ 3 ] 'March' Get ordinal from number \u2691 def int_to_ordinal ( number : int ) -> str : '''Convert an integer into its ordinal representation. make_ordinal(0) => '0th' make_ordinal(3) => '3rd' make_ordinal(122) => '122nd' make_ordinal(213) => '213th' Args: number: Number to convert Returns: ordinal representation of the number ''' suffix = [ 'th' , 'st' , 'nd' , 'rd' , 'th' ][ min ( number % 10 , 4 )] if 11 <= ( number % 100 ) <= 13 : suffix = 'th' return f \" { number }{ suffix } \" Group or sort a list of dictionaries or objects by a specific key \u2691 Python lists have a built-in list.sort() method that modifies the list in-place. There is also a sorted() built-in function that builds a new sorted list from an iterable. Sorting basics \u2691 A simple ascending sort is very easy: just call the sorted() function. It returns a new sorted list: >>> sorted ([ 5 , 2 , 3 , 1 , 4 ]) [ 1 , 2 , 3 , 4 , 5 ] Key functions \u2691 Both list.sort() and sorted() have a key parameter to specify a function (or other callable) to be called on each list element prior to making comparisons. For example, here\u2019s a case-insensitive string comparison: >>> sorted ( \"This is a test string from Andrew\" . split (), key = str . lower ) [ 'a' , 'Andrew' , 'from' , 'is' , 'string' , 'test' , 'This' ] The value of the key parameter should be a function (or other callable) that takes a single argument and returns a key to use for sorting purposes. This technique is fast because the key function is called exactly once for each input record. A common pattern is to sort complex objects using some of the object\u2019s indices as keys. For example: >>> from operator import itemgetter >>> student_tuples = [ ( 'john' , 'A' , 15 ), ( 'jane' , 'B' , 12 ), ( 'dave' , 'B' , 10 ), ] >>> sorted ( student_tuples , key = itemgetter ( 2 )) # sort by age [( 'dave' , 'B' , 10 ), ( 'jane' , 'B' , 12 ), ( 'john' , 'A' , 15 )] The same technique works for objects with named attributes. For example: >>> from operator import attrgetter >>> class Student : def __init__ ( self , name , grade , age ): self . name = name self . grade = grade self . age = age def __repr__ ( self ): return repr (( self . name , self . grade , self . age )) >>> student_objects = [ Student ( 'john' , 'A' , 15 ), Student ( 'jane' , 'B' , 12 ), Student ( 'dave' , 'B' , 10 ), ] >>> sorted ( student_objects , key = attrgetter ( 'age' )) # sort by age [( 'dave' , 'B' , 10 ), ( 'jane' , 'B' , 12 ), ( 'john' , 'A' , 15 )] The operator module functions allow multiple levels of sorting. For example, to sort by grade then by age: >>> sorted ( student_tuples , key = itemgetter ( 1 , 2 )) [( 'john' , 'A' , 15 ), ( 'dave' , 'B' , 10 ), ( 'jane' , 'B' , 12 )] >>> sorted ( student_objects , key = attrgetter ( 'grade' , 'age' )) [( 'john' , 'A' , 15 ), ( 'dave' , 'B' , 10 ), ( 'jane' , 'B' , 12 )] Sorts stability and complex sorts \u2691 Sorts are guaranteed to be stable. That means that when multiple records have the same key, their original order is preserved. >>> data = [( 'red' , 1 ), ( 'blue' , 1 ), ( 'red' , 2 ), ( 'blue' , 2 )] >>> sorted ( data , key = itemgetter ( 0 )) [( 'blue' , 1 ), ( 'blue' , 2 ), ( 'red' , 1 ), ( 'red' , 2 )] Notice how the two records for blue retain their original order so that ('blue', 1) is guaranteed to precede ('blue', 2) . This wonderful property lets you build complex sorts in a series of sorting steps. For example, to sort the student data by descending grade and then ascending age, do the age sort first and then sort again using grade: >>> s = sorted ( student_objects , key = attrgetter ( 'age' )) # sort on secondary key >>> sorted ( s , key = attrgetter ( 'grade' ), reverse = True ) # now sort on primary key, descending [( 'dave' , 'B' , 10 ), ( 'jane' , 'B' , 12 ), ( 'john' , 'A' , 15 )] This can be abstracted out into a wrapper function that can take a list and tuples of field and order to sort them on multiple passes. >>> def multisort ( xs , specs ): for key , reverse in reversed ( specs ): xs . sort ( key = attrgetter ( key ), reverse = reverse ) return xs >>> multisort ( list ( student_objects ), (( 'grade' , True ), ( 'age' , False ))) [( 'dave' , 'B' , 10 ), ( 'jane' , 'B' , 12 ), ( 'john' , 'A' , 15 )] Iterate over an instance object's data attributes in Python \u2691 @dataclass ( frozen = True ) class Search : center : str distance : str se = Search ( 'a' , 'b' ) for key , value in se . __dict__ . items (): print ( key , value ) Generate ssh key \u2691 pip install cryptography from os import chmod from cryptography.hazmat.primitives import serialization from cryptography.hazmat.primitives.asymmetric import rsa from cryptography.hazmat.backends import default_backend as crypto_default_backend private_key = rsa . generate_private_key ( backend = crypto_default_backend (), public_exponent = 65537 , key_size = 4096 ) pem = private_key . private_bytes ( encoding = serialization . Encoding . PEM , format = serialization . PrivateFormat . TraditionalOpenSSL , encryption_algorithm = serialization . NoEncryption () ) with open ( \"/tmp/private.key\" , 'wb' ) as content_file : chmod ( \"/tmp/private.key\" , 0600 ) content_file . write ( pem ) public_key = ( private_key . public_key () . public_bytes ( encoding = serialization . Encoding . OpenSSH , format = serialization . PublicFormat . OpenSSH , ) + b ' user@email.org' ) with open ( \"/tmp/public.key\" , 'wb' ) as content_file : content_file . write ( public_key ) Make multiline code look clean \u2691 If you need variables that contain multiline strings inside functions or methods you need to remove the indentation def test (): # end first line with \\ to avoid the empty line! s = ''' \\ hello world ''' Which is inconvenient as it breaks some editor source code folding and it's ugly for the eye. The solution is to use textwrap.dedent() import textwrap def test (): # end first line with \\ to avoid the empty line! s = ''' \\ hello world ''' print ( repr ( s )) # prints ' hello\\n world\\n ' print ( repr ( textwrap . dedent ( s ))) # prints 'hello\\n world\\n' If you forget to add the trailing \\ character of s = '''\\ or use s = '''hello , you're going to have a bad time with black . Play a sound \u2691 pip install playsound from playsound import playsound playsound ( 'path/to/file.wav' ) Deep copy a dictionary \u2691 import copy d = { ... } d2 = copy . deepcopy ( d ) Find the root directory of a package \u2691 pyprojroot finds the root working directory for your project as a pathlib object. You can now use the here function to pass in a relative path from the project root directory (no matter what working directory you are in the project), and you will get a full path to the specified file. Installation \u2691 pip install pyprojroot Usage \u2691 from pyprojroot import here here () Check if an object has an attribute \u2691 if hasattr ( a , 'property' ): a . property Check if a loop ends completely \u2691 for loops can take an else block which is not run if the loop has ended with a break statement. for i in [ 1 , 2 , 3 ]: print ( i ) if i == 3 : break else : print ( \"for loop was not broken\" ) Merge two lists \u2691 z = x + y Merge two dictionaries \u2691 z = { ** x , ** y } Create user defined exceptions \u2691 Programs may name their own exceptions by creating a new exception class. Exceptions should typically be derived from the Exception class, either directly or indirectly. Exception classes are meant to be kept simple, only offering a number of attributes that allow information about the error to be extracted by handlers for the exception. When creating a module that can raise several distinct errors, a common practice is to create a base class for exceptions defined by that module, and subclass that to create specific exception classes for different error conditions: class Error ( Exception ): \"\"\"Base class for exceptions in this module.\"\"\" class ConceptNotFoundError ( Error ): \"\"\"Transactions with unmatched concept.\"\"\" def __init__ ( self , message : str , transactions : List [ Transaction ]) -> None : \"\"\"Initialize the exception.\"\"\" self . message = message self . transactions = transactions super () . __init__ ( self . message ) Most exceptions are defined with names that end in \u201cError\u201d, similar to the naming of the standard exceptions. Import a module or it's objects from within a python program \u2691 import importlib module = importlib . import_module ( 'os' ) module_class = module . getcwd relative_module = importlib . import_module ( '.model' , package = 'mypackage' ) class_to_extract = 'MyModel' extracted_class = geattr ( relative_module , class_to_extract ) The first argument specifies what module to import in absolute or relative terms (e.g. either pkg.mod or ..mod ). If the name is specified in relative terms, then the package argument must be set to the name of the package which is to act as the anchor for resolving the package name (e.g. import_module('..mod', 'pkg.subpkg') will import pkg.mod ). Get system's timezone and use it in datetime \u2691 To obtain timezone information in the form of a datetime.tzinfo object, use dateutil.tz.tzlocal() : from dateutil import tz myTimeZone = tz . tzlocal () This object can be used in the tz parameter of datetime.datetime.now() : from datetime import datetime from dateutil import tz localisedDatetime = datetime . now ( tz = tz . tzlocal ()) Capitalize a sentence \u2691 To change the caps of the first letter of the first word of a sentence use: >> sentence = \"add funny Emojis\" >> sentence [ 0 ] . upper () + sentence [ 1 :] Add funny Emojis The .capitalize method transforms the rest of words to lowercase. The .title transforms all sentence words to capitalize. Get the last monday datetime \u2691 import datetime today = datetime . date . today () last_monday = today - datetime . timedelta ( days = today . weekday ()) Issues \u2691 Pypi won't allow you to upload packages with direct dependencies : update the section above.","title":"Python Snippets"},{"location":"coding/python/python_snippets/#locate-element-in-list","text":"a = [ 'a' , 'b' ] index = a . index ( 'b' )","title":"Locate element in list"},{"location":"coding/python/python_snippets/#transpose-a-list-of-lists","text":">>> l = [[ 1 , 2 , 3 ],[ 4 , 5 , 6 ],[ 7 , 8 , 9 ]] >>> [ list ( i ) for i in zip ( * l )] ... [[ 1 , 4 , 7 ], [ 2 , 5 , 8 ], [ 3 , 6 , 9 ]]","title":"Transpose a list of lists"},{"location":"coding/python/python_snippets/#check-the-type-of-a-list-of-strings","text":"def _is_list_of_lists ( data : Any ) -> bool : \"\"\"Check if data is a list of strings.\"\"\" if data and isinstance ( data , list ): return all ( isinstance ( elem , list ) for elem in data ) else : return False","title":"Check the type of a list of strings"},{"location":"coding/python/python_snippets/#install-default-directories-and-files-for-a-command-line-program","text":"I've been trying for a long time to configure setup.py to run the required steps to configure the required directories and files when doing pip install without success. Finally, I decided that the program itself should create the data once the FileNotFoundError exception is found. That way, you don't penalize the load time because if the file or directory exists, that code is not run.","title":"Install default directories and files for a command line program"},{"location":"coding/python/python_snippets/#check-if-a-dictionary-is-a-subset-of-another","text":"If you have two dictionaries big = {'a': 1, 'b': 2, 'c':3} and small = {'c': 3, 'a': 1} , and want to check whether small is a subset of big , use the next snippet: >>> small . items () <= big . items () True As the code is not very common or intuitive, I'd add a comment to explain what you're doing.","title":"Check if a dictionary is a subset of another"},{"location":"coding/python/python_snippets/#when-to-use-isinstance-and-when-to-use-type","text":"isinstance takes into account inheritance, while type doesn't. So if we have the next code: class Shape : pass class Rectangle ( Shape ): def __init__ ( self , length , width ): self . length = length self . width = width self . area = length * width def get_area ( self ): return self . length * self . width class Square ( Rectangle ): def __init__ ( self , length ): Rectangle . __init__ ( self , length , length ) And we want to check if an object a = Square(5) is of type Rectangle , we could not use isinstance because it'll return True as it's a subclass of Rectangle : >>> isinstance ( a , Rectangle ) True Instead, use a comparison with type : >>> type ( a ) == Rectangle False","title":"When to use isinstance and when to use type"},{"location":"coding/python/python_snippets/#find-a-static-file-of-a-python-module","text":"Useful when you want to initialize a configuration file of a cli program when it's not present. Imagine you have a setup.py with the next contents: setup ( name = \"pynbox\" , packages = find_packages ( \"src\" ), package_dir = { \"\" : \"src\" }, package_data = { \"pynbox\" : [ \"py.typed\" , \"assets/config.yaml\" ]}, Then you could import the data with: import pkg_resources file_path = pkg_resources . resource_filename ( \"pynbox\" , \"assets/config.yaml\" ),","title":"Find a static file of a python module"},{"location":"coding/python/python_snippets/#delete-a-file","text":"import os os . remove ( 'demofile.txt' )","title":"Delete a file"},{"location":"coding/python/python_snippets/#measure-elapsed-time-between-lines-of-code","text":"import time start = time . time () print ( \"hello\" ) end = time . time () print ( end - start )","title":"Measure elapsed time between lines of code"},{"location":"coding/python/python_snippets/#create-combination-of-elements-in-groups-of-two","text":"Using the combinations function in Python's itertools module: >>> list ( itertools . combinations ( 'ABC' , 2 )) [( 'A' , 'B' ), ( 'A' , 'C' ), ( 'B' , 'C' )] If you want the permutations use itertools.permutations .","title":"Create combination of elements in groups of two"},{"location":"coding/python/python_snippets/#convert-html-to-readable-plaintext","text":"pip install html2text import html2text html = open ( \"foobar.html\" ) . read () print ( html2text . html2text ( html ))","title":"Convert html to readable plaintext"},{"location":"coding/python/python_snippets/#parse-a-datetime-from-a-string","text":"from dateutil import parser parser . parse ( \"Aug 28 1999 12:00AM\" ) # datetime.datetime(1999, 8, 28, 0, 0)","title":"Parse a datetime from a string"},{"location":"coding/python/python_snippets/#install-a-python-dependency-from-a-git-repository","text":"With pip you can : pip install git+git://github.com/path/to/repository@master If you want to hard code it in your setup.py , you need to: install_requires = [ 'some-pkg @ git+ssh://git@github.com/someorgname/pkg-repo-name@v1.1#egg=some-pkg' , ] But Pypi won't allow you to upload the package , as it will give you an error: HTTPError: 400 Bad Request from https://test.pypi.org/legacy/ Invalid value for requires_dist. Error: Can't have direct dependency: 'deepdiff @ git+git://github.com/lyz-code/deepdiff@master' It looks like this is a conscious decision on the PyPI side. Basically, they don't want pip to reach out to URLs outside their site when installing from PyPI. An ugly patch is to install the dependencies in a PostInstall custom script in the setup.py of your program: from setuptools.command.install import install from subprocess import getoutput # ignore: cannot subclass install, has type Any. And what would you do? class PostInstall ( install ): # type: ignore \"\"\"Install direct dependency. Pypi doesn't allow uploading packages with direct dependencies, so we need to install them manually. \"\"\" def run ( self ) -> None : \"\"\"Install dependencies.\"\"\" install . run ( self ) print ( getoutput ( \"pip install git+git://github.com/lyz-code/deepdiff@master\" )) setup ( cmdclass = { 'install' : PostInstall } ) It may not work! Last time I used this solution, when I added the library on a setup.py the direct dependencies weren't installed :S","title":"Install a python dependency from a git repository"},{"location":"coding/python/python_snippets/#check-directories-and-files","text":"def test_dir ( directory ): from os.path import exists from os import makedirs if not exists ( directory ): makedirs ( directory ) def test_file ( filepath , mode ): ''' Check if a file exist and is accessible. ''' def check_mode ( os_mode , mode ): if os . path . isfile ( filepath ) and os . access ( filepath , os_mode ): return else : raise IOError ( \"Can't access the file with mode \" + mode ) if mode is 'r' : check_mode ( os . R_OK , mode ) elif mode is 'w' : check_mode ( os . W_OK , mode ) elif mode is 'a' : check_mode ( os . R_OK , mode ) check_mode ( os . W_OK , mode )","title":"Check directories and files"},{"location":"coding/python/python_snippets/#remove-the-extension-of-a-file","text":"os . path . splitext ( \"/path/to/some/file.txt\" )[ 0 ]","title":"Remove the extension of a file"},{"location":"coding/python/python_snippets/#iterate-over-the-files-of-a-directory","text":"import os directory = '/path/to/directory' for entry in os . scandir ( directory ): if ( entry . path . endswith ( \".jpg\" ) or entry . path . endswith ( \".png\" )) and entry . is_file (): print ( entry . path )","title":"Iterate over the files of a directory"},{"location":"coding/python/python_snippets/#create-directory","text":"if not os . path . exists ( directory ): os . makedirs ( directory )","title":"Create directory"},{"location":"coding/python/python_snippets/#touch-a-file","text":"from pathlib import Path Path ( 'path/to/file.txt' ) . touch ()","title":"Touch a file"},{"location":"coding/python/python_snippets/#get-the-first-day-of-next-month","text":"current = datetime . datetime ( mydate . year , mydate . month , 1 ) next_month = datetime . datetime ( mydate . year + int ( mydate . month / 12 ), (( mydate . month % 12 ) + 1 ), 1 )","title":"Get the first day of next month"},{"location":"coding/python/python_snippets/#get-the-week-number-of-a-datetime","text":"datetime.datetime has a isocalendar() method, which returns a tuple containing the calendar week: >>> import datetime >>> datetime . datetime ( 2010 , 6 , 16 ) . isocalendar ()[ 1 ] 24 datetime.date.isocalendar() is an instance-method returning a tuple containing year, weeknumber and weekday in respective order for the given date instance.","title":"Get the week number of a datetime"},{"location":"coding/python/python_snippets/#get-the-monday-of-a-week-number","text":"A week number is not enough to generate a date; you need a day of the week as well. Add a default: import datetime d = \"2013-W26\" r = datetime . datetime . strptime ( d + '-1' , \"%Y-W%W-%w\" ) The -1 and -%w pattern tells the parser to pick the Monday in that week.","title":"Get the monday of a week number"},{"location":"coding/python/python_snippets/#get-the-month-name-from-a-number","text":"import calendar >> calendar . month_name [ 3 ] 'March'","title":"Get the month name from a number"},{"location":"coding/python/python_snippets/#get-ordinal-from-number","text":"def int_to_ordinal ( number : int ) -> str : '''Convert an integer into its ordinal representation. make_ordinal(0) => '0th' make_ordinal(3) => '3rd' make_ordinal(122) => '122nd' make_ordinal(213) => '213th' Args: number: Number to convert Returns: ordinal representation of the number ''' suffix = [ 'th' , 'st' , 'nd' , 'rd' , 'th' ][ min ( number % 10 , 4 )] if 11 <= ( number % 100 ) <= 13 : suffix = 'th' return f \" { number }{ suffix } \"","title":"Get ordinal from number"},{"location":"coding/python/python_snippets/#group-or-sort-a-list-of-dictionaries-or-objects-by-a-specific-key","text":"Python lists have a built-in list.sort() method that modifies the list in-place. There is also a sorted() built-in function that builds a new sorted list from an iterable.","title":"Group or sort a list of dictionaries or objects by a specific key"},{"location":"coding/python/python_snippets/#sorting-basics","text":"A simple ascending sort is very easy: just call the sorted() function. It returns a new sorted list: >>> sorted ([ 5 , 2 , 3 , 1 , 4 ]) [ 1 , 2 , 3 , 4 , 5 ]","title":"Sorting basics"},{"location":"coding/python/python_snippets/#key-functions","text":"Both list.sort() and sorted() have a key parameter to specify a function (or other callable) to be called on each list element prior to making comparisons. For example, here\u2019s a case-insensitive string comparison: >>> sorted ( \"This is a test string from Andrew\" . split (), key = str . lower ) [ 'a' , 'Andrew' , 'from' , 'is' , 'string' , 'test' , 'This' ] The value of the key parameter should be a function (or other callable) that takes a single argument and returns a key to use for sorting purposes. This technique is fast because the key function is called exactly once for each input record. A common pattern is to sort complex objects using some of the object\u2019s indices as keys. For example: >>> from operator import itemgetter >>> student_tuples = [ ( 'john' , 'A' , 15 ), ( 'jane' , 'B' , 12 ), ( 'dave' , 'B' , 10 ), ] >>> sorted ( student_tuples , key = itemgetter ( 2 )) # sort by age [( 'dave' , 'B' , 10 ), ( 'jane' , 'B' , 12 ), ( 'john' , 'A' , 15 )] The same technique works for objects with named attributes. For example: >>> from operator import attrgetter >>> class Student : def __init__ ( self , name , grade , age ): self . name = name self . grade = grade self . age = age def __repr__ ( self ): return repr (( self . name , self . grade , self . age )) >>> student_objects = [ Student ( 'john' , 'A' , 15 ), Student ( 'jane' , 'B' , 12 ), Student ( 'dave' , 'B' , 10 ), ] >>> sorted ( student_objects , key = attrgetter ( 'age' )) # sort by age [( 'dave' , 'B' , 10 ), ( 'jane' , 'B' , 12 ), ( 'john' , 'A' , 15 )] The operator module functions allow multiple levels of sorting. For example, to sort by grade then by age: >>> sorted ( student_tuples , key = itemgetter ( 1 , 2 )) [( 'john' , 'A' , 15 ), ( 'dave' , 'B' , 10 ), ( 'jane' , 'B' , 12 )] >>> sorted ( student_objects , key = attrgetter ( 'grade' , 'age' )) [( 'john' , 'A' , 15 ), ( 'dave' , 'B' , 10 ), ( 'jane' , 'B' , 12 )]","title":"Key functions"},{"location":"coding/python/python_snippets/#sorts-stability-and-complex-sorts","text":"Sorts are guaranteed to be stable. That means that when multiple records have the same key, their original order is preserved. >>> data = [( 'red' , 1 ), ( 'blue' , 1 ), ( 'red' , 2 ), ( 'blue' , 2 )] >>> sorted ( data , key = itemgetter ( 0 )) [( 'blue' , 1 ), ( 'blue' , 2 ), ( 'red' , 1 ), ( 'red' , 2 )] Notice how the two records for blue retain their original order so that ('blue', 1) is guaranteed to precede ('blue', 2) . This wonderful property lets you build complex sorts in a series of sorting steps. For example, to sort the student data by descending grade and then ascending age, do the age sort first and then sort again using grade: >>> s = sorted ( student_objects , key = attrgetter ( 'age' )) # sort on secondary key >>> sorted ( s , key = attrgetter ( 'grade' ), reverse = True ) # now sort on primary key, descending [( 'dave' , 'B' , 10 ), ( 'jane' , 'B' , 12 ), ( 'john' , 'A' , 15 )] This can be abstracted out into a wrapper function that can take a list and tuples of field and order to sort them on multiple passes. >>> def multisort ( xs , specs ): for key , reverse in reversed ( specs ): xs . sort ( key = attrgetter ( key ), reverse = reverse ) return xs >>> multisort ( list ( student_objects ), (( 'grade' , True ), ( 'age' , False ))) [( 'dave' , 'B' , 10 ), ( 'jane' , 'B' , 12 ), ( 'john' , 'A' , 15 )]","title":"Sorts stability and complex sorts"},{"location":"coding/python/python_snippets/#iterate-over-an-instance-objects-data-attributes-in-python","text":"@dataclass ( frozen = True ) class Search : center : str distance : str se = Search ( 'a' , 'b' ) for key , value in se . __dict__ . items (): print ( key , value )","title":"Iterate over an instance object's data attributes in Python"},{"location":"coding/python/python_snippets/#generate-ssh-key","text":"pip install cryptography from os import chmod from cryptography.hazmat.primitives import serialization from cryptography.hazmat.primitives.asymmetric import rsa from cryptography.hazmat.backends import default_backend as crypto_default_backend private_key = rsa . generate_private_key ( backend = crypto_default_backend (), public_exponent = 65537 , key_size = 4096 ) pem = private_key . private_bytes ( encoding = serialization . Encoding . PEM , format = serialization . PrivateFormat . TraditionalOpenSSL , encryption_algorithm = serialization . NoEncryption () ) with open ( \"/tmp/private.key\" , 'wb' ) as content_file : chmod ( \"/tmp/private.key\" , 0600 ) content_file . write ( pem ) public_key = ( private_key . public_key () . public_bytes ( encoding = serialization . Encoding . OpenSSH , format = serialization . PublicFormat . OpenSSH , ) + b ' user@email.org' ) with open ( \"/tmp/public.key\" , 'wb' ) as content_file : content_file . write ( public_key )","title":"Generate ssh key"},{"location":"coding/python/python_snippets/#make-multiline-code-look-clean","text":"If you need variables that contain multiline strings inside functions or methods you need to remove the indentation def test (): # end first line with \\ to avoid the empty line! s = ''' \\ hello world ''' Which is inconvenient as it breaks some editor source code folding and it's ugly for the eye. The solution is to use textwrap.dedent() import textwrap def test (): # end first line with \\ to avoid the empty line! s = ''' \\ hello world ''' print ( repr ( s )) # prints ' hello\\n world\\n ' print ( repr ( textwrap . dedent ( s ))) # prints 'hello\\n world\\n' If you forget to add the trailing \\ character of s = '''\\ or use s = '''hello , you're going to have a bad time with black .","title":"Make multiline code look clean"},{"location":"coding/python/python_snippets/#play-a-sound","text":"pip install playsound from playsound import playsound playsound ( 'path/to/file.wav' )","title":"Play a sound"},{"location":"coding/python/python_snippets/#deep-copy-a-dictionary","text":"import copy d = { ... } d2 = copy . deepcopy ( d )","title":"Deep copy a dictionary"},{"location":"coding/python/python_snippets/#find-the-root-directory-of-a-package","text":"pyprojroot finds the root working directory for your project as a pathlib object. You can now use the here function to pass in a relative path from the project root directory (no matter what working directory you are in the project), and you will get a full path to the specified file.","title":"Find the root directory of a package"},{"location":"coding/python/python_snippets/#installation","text":"pip install pyprojroot","title":"Installation"},{"location":"coding/python/python_snippets/#usage","text":"from pyprojroot import here here ()","title":"Usage"},{"location":"coding/python/python_snippets/#check-if-an-object-has-an-attribute","text":"if hasattr ( a , 'property' ): a . property","title":"Check if an object has an attribute"},{"location":"coding/python/python_snippets/#check-if-a-loop-ends-completely","text":"for loops can take an else block which is not run if the loop has ended with a break statement. for i in [ 1 , 2 , 3 ]: print ( i ) if i == 3 : break else : print ( \"for loop was not broken\" )","title":"Check if a loop ends completely"},{"location":"coding/python/python_snippets/#merge-two-lists","text":"z = x + y","title":"Merge two lists"},{"location":"coding/python/python_snippets/#merge-two-dictionaries","text":"z = { ** x , ** y }","title":"Merge two dictionaries"},{"location":"coding/python/python_snippets/#create-user-defined-exceptions","text":"Programs may name their own exceptions by creating a new exception class. Exceptions should typically be derived from the Exception class, either directly or indirectly. Exception classes are meant to be kept simple, only offering a number of attributes that allow information about the error to be extracted by handlers for the exception. When creating a module that can raise several distinct errors, a common practice is to create a base class for exceptions defined by that module, and subclass that to create specific exception classes for different error conditions: class Error ( Exception ): \"\"\"Base class for exceptions in this module.\"\"\" class ConceptNotFoundError ( Error ): \"\"\"Transactions with unmatched concept.\"\"\" def __init__ ( self , message : str , transactions : List [ Transaction ]) -> None : \"\"\"Initialize the exception.\"\"\" self . message = message self . transactions = transactions super () . __init__ ( self . message ) Most exceptions are defined with names that end in \u201cError\u201d, similar to the naming of the standard exceptions.","title":"Create user defined exceptions"},{"location":"coding/python/python_snippets/#import-a-module-or-its-objects-from-within-a-python-program","text":"import importlib module = importlib . import_module ( 'os' ) module_class = module . getcwd relative_module = importlib . import_module ( '.model' , package = 'mypackage' ) class_to_extract = 'MyModel' extracted_class = geattr ( relative_module , class_to_extract ) The first argument specifies what module to import in absolute or relative terms (e.g. either pkg.mod or ..mod ). If the name is specified in relative terms, then the package argument must be set to the name of the package which is to act as the anchor for resolving the package name (e.g. import_module('..mod', 'pkg.subpkg') will import pkg.mod ).","title":"Import a module or it's objects from within a python program"},{"location":"coding/python/python_snippets/#get-systems-timezone-and-use-it-in-datetime","text":"To obtain timezone information in the form of a datetime.tzinfo object, use dateutil.tz.tzlocal() : from dateutil import tz myTimeZone = tz . tzlocal () This object can be used in the tz parameter of datetime.datetime.now() : from datetime import datetime from dateutil import tz localisedDatetime = datetime . now ( tz = tz . tzlocal ())","title":"Get system's timezone and use it in datetime"},{"location":"coding/python/python_snippets/#capitalize-a-sentence","text":"To change the caps of the first letter of the first word of a sentence use: >> sentence = \"add funny Emojis\" >> sentence [ 0 ] . upper () + sentence [ 1 :] Add funny Emojis The .capitalize method transforms the rest of words to lowercase. The .title transforms all sentence words to capitalize.","title":"Capitalize a sentence"},{"location":"coding/python/python_snippets/#get-the-last-monday-datetime","text":"import datetime today = datetime . date . today () last_monday = today - datetime . timedelta ( days = today . weekday ())","title":"Get the last monday datetime"},{"location":"coding/python/python_snippets/#issues","text":"Pypi won't allow you to upload packages with direct dependencies : update the section above.","title":"Issues"},{"location":"coding/python/redis-py/","text":"Redis-py is The Python interface to the Redis key-value store. The library encapsulates an actual TCP connection to a Redis server and sends raw commands, as bytes serialized using the REdis Serialization Protocol (RESP) , to the server. It then takes the raw reply and parses it back into a Python object such as bytes, int, or even datetime.datetime. Installation \u2691 pip install redis Usage \u2691 import redis r = redis . Redis ( host = 'localhost' , port = 6379 , db = 0 , password = None , socket_timeout = None , ) The arguments specified above are the default ones, so it's the same as calling r = redis.Redis() . The db parameter is the database number. You can manage multiple databases in Redis at once, and each is identified by an integer. The max number of databases is 16 by default. Common pitfalls \u2691 Redis returned objects are bytes type, so you may need to convert it to string with r.get(\"Bahamas\").decode(\"utf-8\") . References \u2691 Real Python introduction to Redis-py Git Docs : Very technical and small.","title":"Redis-py"},{"location":"coding/python/redis-py/#installation","text":"pip install redis","title":"Installation"},{"location":"coding/python/redis-py/#usage","text":"import redis r = redis . Redis ( host = 'localhost' , port = 6379 , db = 0 , password = None , socket_timeout = None , ) The arguments specified above are the default ones, so it's the same as calling r = redis.Redis() . The db parameter is the database number. You can manage multiple databases in Redis at once, and each is identified by an integer. The max number of databases is 16 by default.","title":"Usage"},{"location":"coding/python/redis-py/#common-pitfalls","text":"Redis returned objects are bytes type, so you may need to convert it to string with r.get(\"Bahamas\").decode(\"utf-8\") .","title":"Common pitfalls"},{"location":"coding/python/redis-py/#references","text":"Real Python introduction to Redis-py Git Docs : Very technical and small.","title":"References"},{"location":"coding/python/requests_mock/","text":"The requests-mock library is a requests transport adapter that can be preloaded with responses that are returned if certain URIs are requested. This is particularly useful in unit tests where you want to return known responses from HTTP requests without making actual calls. Installation \u2691 pip install requests-mock Usage \u2691 Object initialization \u2691 Select one of the following ways to initialize the mock. As a pytest fixture \u2691 The ease of use with pytest it is awesome. requests-mock provides an external fixture registered with pytest such that it is usable simply by specifying it as a parameter. There is no need to import requests-mock it simply needs to be installed and specified as an argument in the test definition. import pytest import requests from requests_mock.mocker import Mocker def test_url ( requests_mock : Mocker ): requests_mock . get ( 'http://test.com' , text = 'data' ) assert 'data' == requests . get ( 'http://test.com' ) . text As a function decorator \u2691 >>> @requests_mock . Mocker () ... def test_function ( m ): ... m . get ( 'http://test.com' , text = 'resp' ) ... return requests . get ( 'http://test.com' ) . text ... >>> test_function () 'resp' As a context manager \u2691 >>> import requests >>> import requests_mock >>> with requests_mock . Mocker () as m : ... m . get ( 'http://test.com' , text = 'resp' ) ... requests . get ( 'http://test.com' ) . text ... 'resp' Mocking responses \u2691 Return a json \u2691 requests_mock . get ( ' {} /api/repos/owner/repository/builds' . format ( self . url ), json = { \"id\" : 882 , \"number\" : 209 , \"finished\" : 1591197904 , }, ) Add a header or a cookie to the response \u2691 requests_mock . post ( \"https://test.com\" , cookies = { \"Id\" : \"0\" }, headers = { \"id\" : \"0\" }, ) Multiple responses \u2691 Multiple responses can be provided to be returned in order by specifying the keyword parameters in a list. requests_mock . get ( 'mock://test.com/4' , [ { 'text' : 'resp1' , 'status_code' : 300 }, { 'text' : 'resp2' , 'status_code' : 200 } ] ) Get requests history \u2691 Called \u2691 The easiest way to test if a request hit the adapter is to simply check the called property or the call_count property. >>> import requests >>> import requests_mock >>> with requests_mock . mock () as m : ... m . get ( 'http://test.com, text=' resp ') ... resp = requests . get ( 'http://test.com' ) ... >>> m . called True >>> m . call_count 1 Requests history \u2691 The history of objects that passed through the mocker/adapter can also be retrieved. >>> history = m . request_history >>> len ( history ) 1 >>> history [ 0 ] . method 'GET' >>> history [ 0 ] . url 'http://test.com/' References \u2691 Docs Git","title":"Requests-mock"},{"location":"coding/python/requests_mock/#installation","text":"pip install requests-mock","title":"Installation"},{"location":"coding/python/requests_mock/#usage","text":"","title":"Usage"},{"location":"coding/python/requests_mock/#object-initialization","text":"Select one of the following ways to initialize the mock.","title":"Object initialization"},{"location":"coding/python/requests_mock/#as-a-pytest-fixture","text":"The ease of use with pytest it is awesome. requests-mock provides an external fixture registered with pytest such that it is usable simply by specifying it as a parameter. There is no need to import requests-mock it simply needs to be installed and specified as an argument in the test definition. import pytest import requests from requests_mock.mocker import Mocker def test_url ( requests_mock : Mocker ): requests_mock . get ( 'http://test.com' , text = 'data' ) assert 'data' == requests . get ( 'http://test.com' ) . text","title":"As a pytest fixture"},{"location":"coding/python/requests_mock/#as-a-function-decorator","text":">>> @requests_mock . Mocker () ... def test_function ( m ): ... m . get ( 'http://test.com' , text = 'resp' ) ... return requests . get ( 'http://test.com' ) . text ... >>> test_function () 'resp'","title":"As a function decorator"},{"location":"coding/python/requests_mock/#as-a-context-manager","text":">>> import requests >>> import requests_mock >>> with requests_mock . Mocker () as m : ... m . get ( 'http://test.com' , text = 'resp' ) ... requests . get ( 'http://test.com' ) . text ... 'resp'","title":"As a context manager"},{"location":"coding/python/requests_mock/#mocking-responses","text":"","title":"Mocking responses"},{"location":"coding/python/requests_mock/#return-a-json","text":"requests_mock . get ( ' {} /api/repos/owner/repository/builds' . format ( self . url ), json = { \"id\" : 882 , \"number\" : 209 , \"finished\" : 1591197904 , }, )","title":"Return a json"},{"location":"coding/python/requests_mock/#add-a-header-or-a-cookie-to-the-response","text":"requests_mock . post ( \"https://test.com\" , cookies = { \"Id\" : \"0\" }, headers = { \"id\" : \"0\" }, )","title":"Add a header or a cookie to the response"},{"location":"coding/python/requests_mock/#multiple-responses","text":"Multiple responses can be provided to be returned in order by specifying the keyword parameters in a list. requests_mock . get ( 'mock://test.com/4' , [ { 'text' : 'resp1' , 'status_code' : 300 }, { 'text' : 'resp2' , 'status_code' : 200 } ] )","title":"Multiple responses"},{"location":"coding/python/requests_mock/#get-requests-history","text":"","title":"Get requests history"},{"location":"coding/python/requests_mock/#called","text":"The easiest way to test if a request hit the adapter is to simply check the called property or the call_count property. >>> import requests >>> import requests_mock >>> with requests_mock . mock () as m : ... m . get ( 'http://test.com, text=' resp ') ... resp = requests . get ( 'http://test.com' ) ... >>> m . called True >>> m . call_count 1","title":"Called"},{"location":"coding/python/requests_mock/#requests-history","text":"The history of objects that passed through the mocker/adapter can also be retrieved. >>> history = m . request_history >>> len ( history ) 1 >>> history [ 0 ] . method 'GET' >>> history [ 0 ] . url 'http://test.com/'","title":"Requests history"},{"location":"coding/python/requests_mock/#references","text":"Docs Git","title":"References"},{"location":"coding/python/rq/","text":"RQ (Redis Queue) is a simple Python library for queueing jobs and processing them in the background with workers. It is backed by Redis and it is designed to have a low barrier to entry. Check arq Next time you are going to use this, check if arq is better. Getting started \u2691 Assuming that a Redis server is running, define the function you want to run: import requests def count_words_at_url ( url ): resp = requests . get ( url ) return len ( resp . text . split ()) The, create a RQ queue: from redis import Redis from rq import Queue q = Queue ( connection = Redis ()) And enqueue the function call: from my_module import count_words_at_url result = q . enqueue ( count_words_at_url , 'http://nvie.com' ) To start executing enqueued function calls in the background, start a worker from your project\u2019s directory: $ rq worker *** Listening for work on default Got count_words_at_url ( 'http://nvie.com' ) from default Job result = 818 *** Listening for work on default Install \u2691 pip install rq Reference \u2691 Homepage Git Docs","title":"Rq"},{"location":"coding/python/rq/#getting-started","text":"Assuming that a Redis server is running, define the function you want to run: import requests def count_words_at_url ( url ): resp = requests . get ( url ) return len ( resp . text . split ()) The, create a RQ queue: from redis import Redis from rq import Queue q = Queue ( connection = Redis ()) And enqueue the function call: from my_module import count_words_at_url result = q . enqueue ( count_words_at_url , 'http://nvie.com' ) To start executing enqueued function calls in the background, start a worker from your project\u2019s directory: $ rq worker *** Listening for work on default Got count_words_at_url ( 'http://nvie.com' ) from default Job result = 818 *** Listening for work on default","title":"Getting started"},{"location":"coding/python/rq/#install","text":"pip install rq","title":"Install"},{"location":"coding/python/rq/#reference","text":"Homepage Git Docs","title":"Reference"},{"location":"coding/python/ruamel_yaml/","text":"ruamel.yaml is a YAML 1.2 loader/dumper package for Python. It is a derivative of Kirill Simonov\u2019s PyYAML 3.11. It has the following enhancements: Comments. Block style and key ordering are kept, so you can diff the round-tripped source. Flow style sequences ( \u2018a: b, c, d\u2019). Anchor names that are hand-crafted (i.e. not of the form idNNN ). Merges in dictionaries are preserved. Installation \u2691 I suggest to use the ruyaml fork, as it's maintained by the community and versioned with git. pip install ruyaml Usage \u2691 Very similar to PyYAML. If invoked with YAML(typ='safe') either the load or the write of the data, the comments of the yaml will be lost. Load from file \u2691 from ruamel.yaml import YAML from ruamel.yaml.scanner import ScannerError try : with open ( os . path . expanduser ( file_path ), 'r' ) as f : try : data = YAML () . load ( f ) except ScannerError as e : log . error ( 'Error parsing yaml of configuration file ' ' {} : {} ' . format ( e . problem_mark , e . problem , ) ) sys . exit ( 1 ) except FileNotFoundError : log . error ( 'Error opening configuration file {} ' . format ( file_path ) ) sys . exit ( 1 ) Save to file \u2691 with open ( os . path . expanduser ( file_path ), 'w+' ) as f : yaml = YAML () yaml . default_flow_style = False yaml . dump ( data , f ) Save to a string \u2691 For some unknown reason, they don't want to output the result to a string, you need to mess up with streams. # Configure YAML formatter yaml = YAML () yaml . indent ( mapping = 2 , sequence = 4 , offset = 2 ) yaml . allow_duplicate_keys = True yaml . explicit_start = False # Return the output to a string string_stream = StringIO () yaml . dump ({ 'products' : [ 'item 1' , 'item 2' ]}, string_stream ) source_code = string_stream . getvalue () string_stream . close () I've opened an issue in the ruyaml fork to solve it. References \u2691 Docs Code","title":"Ruamel YAML"},{"location":"coding/python/ruamel_yaml/#installation","text":"I suggest to use the ruyaml fork, as it's maintained by the community and versioned with git. pip install ruyaml","title":"Installation"},{"location":"coding/python/ruamel_yaml/#usage","text":"Very similar to PyYAML. If invoked with YAML(typ='safe') either the load or the write of the data, the comments of the yaml will be lost.","title":"Usage"},{"location":"coding/python/ruamel_yaml/#load-from-file","text":"from ruamel.yaml import YAML from ruamel.yaml.scanner import ScannerError try : with open ( os . path . expanduser ( file_path ), 'r' ) as f : try : data = YAML () . load ( f ) except ScannerError as e : log . error ( 'Error parsing yaml of configuration file ' ' {} : {} ' . format ( e . problem_mark , e . problem , ) ) sys . exit ( 1 ) except FileNotFoundError : log . error ( 'Error opening configuration file {} ' . format ( file_path ) ) sys . exit ( 1 )","title":"Load from file"},{"location":"coding/python/ruamel_yaml/#save-to-file","text":"with open ( os . path . expanduser ( file_path ), 'w+' ) as f : yaml = YAML () yaml . default_flow_style = False yaml . dump ( data , f )","title":"Save to file"},{"location":"coding/python/ruamel_yaml/#save-to-a-string","text":"For some unknown reason, they don't want to output the result to a string, you need to mess up with streams. # Configure YAML formatter yaml = YAML () yaml . indent ( mapping = 2 , sequence = 4 , offset = 2 ) yaml . allow_duplicate_keys = True yaml . explicit_start = False # Return the output to a string string_stream = StringIO () yaml . dump ({ 'products' : [ 'item 1' , 'item 2' ]}, string_stream ) source_code = string_stream . getvalue () string_stream . close () I've opened an issue in the ruyaml fork to solve it.","title":"Save to a string"},{"location":"coding/python/ruamel_yaml/#references","text":"Docs Code","title":"References"},{"location":"coding/python/sh/","text":"sh is a full-fledged subprocess replacement so beautiful that makes you want to cry. It allows you to call any program as if it were a function: from sh import ifconfig print ( ifconfig ( \"wlan0\" )) Output: wlan0 Link encap : Ethernet HWaddr 00 : 00 : 00 : 00 : 00 : 00 inet addr : 192.168.1.100 Bcast : 192.168.1.255 Mask : 255.255.255.0 inet6 addr : ffff :: ffff : ffff : ffff : fff / 64 Scope : Link UP BROADCAST RUNNING MULTICAST MTU : 1500 Metric : 1 RX packets : 0 errors : 0 dropped : 0 overruns : 0 frame : 0 TX packets : 0 errors : 0 dropped : 0 overruns : 0 carrier : 0 collisions : 0 txqueuelen : 1000 RX bytes : 0 ( 0 GB ) TX bytes : 0 ( 0 GB ) Note that these aren't Python functions, these are running the binary commands on your system by dynamically resolving your $PATH, much like Bash does, and then wrapping the binary in a function. In this way, all the programs on your system are available to you from within Python. Installation \u2691 pip install sh Usage \u2691 Passing arguments \u2691 sh . ls ( \"-l\" , \"/tmp\" , color = \"never\" ) If the command gives you a syntax error (like pass ), you can use bash. sh . bash ( \"-c\" , \"pass\" ) Handling exceptions \u2691 Normal processes exit with exit code 0. You can access the program return code with RunningCommand.exit_code : output = ls ( \"/\" ) print ( output . exit_code ) # should be 0 If a process terminates, and the exit code is not 0, sh generates an exception dynamically. This lets you catch a specific return code, or catch all error return codes through the base class ErrorReturnCode : try : print ( ls ( \"/some/non-existant/folder\" )) except sh . ErrorReturnCode_2 : print ( \"folder doesn't exist!\" ) create_the_folder () except sh . ErrorReturnCode : print ( \"unknown error\" ) The exception object is an sh command object, which has, between other , the stderr and stdout bytes attributes with the errors. To show them use: except sh . ErrorReturnCode as error : print ( str ( error . stderr , 'utf8' )) Redirecting output \u2691 sh . ifconfig ( _out = \"/tmp/interfaces\" ) Interacting with programs that ask input from the user \u2691 sh allows you to interact with programs that asks for user input. The documentation is not clear on how to do it, but between the function callbacks documentation, and the example on how to enter an SSH password we can deduce how to do it. Imagine we've got a python script that asks the user to enter a username so it can save it in a file. File: /tmp/script.py answer = input ( \"Enter username: \" ) with open ( \"/tmp/user.txt\" , \"w+\" ) as f : f . write ( answer ) When we run it in the terminal we get prompted and answer with lyz : $: /tmp/script.py Enter username: lyz $: cat /tmp/user.txt lyz To achieve the same goal automatically with sh we'll need to use the function callbacks. They are functions we pass to the sh command through the _out argument. import sys import re aggregated = \"\" def interact ( char , stdin ): global aggregated sys . stdout . write ( char . encode ()) sys . stdout . flush () aggregated += char if re . search ( r \"Enter username: \" , aggregated , re . MULTILINE ): stdin . put ( \"lyz \\n \" ) sh . bash ( \"-c\" , \"/tmp/script.py\" , _out = interact , _out_bufsize = 0 ) In the example above we've created an interact function that will get called on each character of the stdout of the command. It will be called on each character because we passed the argument _out_bufsize=0 . Check the ssh password example to see why we need that. As it's run on each character, and we need to input the username once the program is expecting us to enter the input and not before, we need to keep track of all the printed characters through the global aggregated variable. Once the regular expression matches what we want, sh will inject the desired value. Remember to add the \\n at the end of the string you want to inject. If the output never matches the regular expression, you'll enter an endless loop, so you need to know before hand all the possible user input prompts. References \u2691 Docs Git","title":"sh"},{"location":"coding/python/sh/#installation","text":"pip install sh","title":"Installation"},{"location":"coding/python/sh/#usage","text":"","title":"Usage"},{"location":"coding/python/sh/#passing-arguments","text":"sh . ls ( \"-l\" , \"/tmp\" , color = \"never\" ) If the command gives you a syntax error (like pass ), you can use bash. sh . bash ( \"-c\" , \"pass\" )","title":"Passing arguments"},{"location":"coding/python/sh/#handling-exceptions","text":"Normal processes exit with exit code 0. You can access the program return code with RunningCommand.exit_code : output = ls ( \"/\" ) print ( output . exit_code ) # should be 0 If a process terminates, and the exit code is not 0, sh generates an exception dynamically. This lets you catch a specific return code, or catch all error return codes through the base class ErrorReturnCode : try : print ( ls ( \"/some/non-existant/folder\" )) except sh . ErrorReturnCode_2 : print ( \"folder doesn't exist!\" ) create_the_folder () except sh . ErrorReturnCode : print ( \"unknown error\" ) The exception object is an sh command object, which has, between other , the stderr and stdout bytes attributes with the errors. To show them use: except sh . ErrorReturnCode as error : print ( str ( error . stderr , 'utf8' ))","title":"Handling exceptions"},{"location":"coding/python/sh/#redirecting-output","text":"sh . ifconfig ( _out = \"/tmp/interfaces\" )","title":"Redirecting output"},{"location":"coding/python/sh/#interacting-with-programs-that-ask-input-from-the-user","text":"sh allows you to interact with programs that asks for user input. The documentation is not clear on how to do it, but between the function callbacks documentation, and the example on how to enter an SSH password we can deduce how to do it. Imagine we've got a python script that asks the user to enter a username so it can save it in a file. File: /tmp/script.py answer = input ( \"Enter username: \" ) with open ( \"/tmp/user.txt\" , \"w+\" ) as f : f . write ( answer ) When we run it in the terminal we get prompted and answer with lyz : $: /tmp/script.py Enter username: lyz $: cat /tmp/user.txt lyz To achieve the same goal automatically with sh we'll need to use the function callbacks. They are functions we pass to the sh command through the _out argument. import sys import re aggregated = \"\" def interact ( char , stdin ): global aggregated sys . stdout . write ( char . encode ()) sys . stdout . flush () aggregated += char if re . search ( r \"Enter username: \" , aggregated , re . MULTILINE ): stdin . put ( \"lyz \\n \" ) sh . bash ( \"-c\" , \"/tmp/script.py\" , _out = interact , _out_bufsize = 0 ) In the example above we've created an interact function that will get called on each character of the stdout of the command. It will be called on each character because we passed the argument _out_bufsize=0 . Check the ssh password example to see why we need that. As it's run on each character, and we need to input the username once the program is expecting us to enter the input and not before, we need to keep track of all the printed characters through the global aggregated variable. Once the regular expression matches what we want, sh will inject the desired value. Remember to add the \\n at the end of the string you want to inject. If the output never matches the regular expression, you'll enter an endless loop, so you need to know before hand all the possible user input prompts.","title":"Interacting with programs that ask input from the user"},{"location":"coding/python/sh/#references","text":"Docs Git","title":"References"},{"location":"coding/python/sqlalchemy/","text":"SQLAlchemy is the Python SQL toolkit and Object Relational Mapper that gives application developers the full power and flexibility of SQL. I discourage you to use an ORM to manage the interactions with the database. Check the alternative solutions . Creating an SQL Schema \u2691 First of all it's important to create a diagram with the database structure, it will help you in the design and it's a great improvement of the project's documentation. I usually use ondras wwwsqldesigner through his demo , as it's easy to use and it's possible to save the data in your repository in an xml file. I assume you've already set up your project to support sqlalchemy in your project. If not, do so before moving forward. Mapping styles \u2691 Modern SQLAlchemy features two distinct styles of mapper configuration. The \u201cClassical\u201d style is SQLAlchemy\u2019s original mapping API, whereas \u201cDeclarative\u201d is the richer and more succinct system that builds on top of \u201cClassical\u201d. The Classical, on the other hand, doesn't lock your models to the ORM, something that we avoid when using the repository pattern . If you aren't going to use the repository pattern, use the declarative way, otherwise use the classical one Creating Tables \u2691 If you simply want to create a table of association without any parameters, such as with a many to many relationship association table, use this type of object. Declarative type \u2691 class User ( Base ): \"\"\" Class to define the User model. \"\"\" __tablename__ = 'user' id = Column ( Integer , primary_key = True , doc = 'User ID' ) name = Column ( String , doc = 'User name' ) def __init__ ( self , id , name = None , ): self . id = id self . name = name There are different types of fields to add to a table: Boolean: is_true = Column(Boolean) . Datetime: created_date = Column(DateTime, doc='Date of creation') . Float: score = Column(Float) Integer: id = Column(Integer, primary_key=True, doc='Source ID') . String: title = Column(String) . Text: long_text = Column(Text) . To make sure that a field can't contain nulls set the nullable=False attribute in the definition of the Column . If you want the contents to be unique use unique=True . If you want to use the Mysql driver of SQLAlchemy make sure to specify the length of the colums, for example String(16) . For reference this are the common lengths: url: 2083 name: 64 (it occupies the same 2 and 255). email: 64 (it occupies the same 2 and 255). username: 64 (it occupies the same 2 and 255). Classical type \u2691 File: model.py class User (): def __init__ ( self , id , name = None ): self . id = id self . name = name File: orm.py from models import User from sqlalchemy import ( Column , MetaData , String , Table , Text , ) metadata = MetaData () user = Table ( \"user\" , metadata , Column ( \"id\" , String ( 64 ), primary_key = True ), Column ( \"name\" , String ( 64 )), ) def start_mappers (): mapper ( User , user ) Creating relationships \u2691 Joined table inheritance \u2691 In joined table inheritance, each class along a hierarchy of classes is represented by a distinct table. Querying for a particular subclass in the hierarchy will render as a SQL JOIN along all tables in its inheritance path. If the queried class is the base class, the default behavior is to include only the base table in a SELECT statement. In all cases, the ultimate class to instantiate for a given row is determined by a discriminator column or an expression that works against the base table. When a subclass is loaded only against a base table, resulting objects by default will have base attributes populated at first; attributes that are local to the subclass will lazy load when they are accessed. The base class in a joined inheritance hierarchy is configured with additional arguments that will refer to the polymorphic discriminator column as well as the identifier for the base class. Declarative \u2691 class Employee ( Base ): __tablename__ = 'employee' id = Column ( Integer , primary_key = True ) name = Column ( String ( 50 )) type = Column ( String ( 50 )) __mapper_args__ = { 'polymorphic_identity' : 'employee' , 'polymorphic_on' : type } class Engineer ( Employee ): __tablename__ = 'engineer' id = Column ( Integer , ForeignKey ( 'employee.id' ), primary_key = True ) engineer_name = Column ( String ( 30 )) __mapper_args__ = { 'polymorphic_identity' : 'engineer' , } class Manager ( Employee ): __tablename__ = 'manager' id = Column ( Integer , ForeignKey ( 'employee.id' ), primary_key = True ) manager_name = Column ( String ( 30 )) __mapper_args__ = { 'polymorphic_identity' : 'manager' , } Classical \u2691 File: model.py class Employee : def __init__ ( self , name ): self . name = name class Manager ( Employee ): def __init__ ( self , name , manager_data ): super () . __init__ ( name ) self . manager_data = manager_data class Engineer ( Employee ): def __init__ ( self , name , engineer_info ): super () . __init__ ( name ) self . engineer_info = engineer_info File: orm.py metadata = MetaData () employee = Table ( 'employee' , metadata , Column ( 'id' , Integer , primary_key = True ), Column ( 'name' , String ( 50 )), Column ( 'type' , String ( 20 )), Column ( 'manager_data' , String ( 50 )), Column ( 'engineer_info' , String ( 50 )) ) mapper ( Employee , employee , polymorphic_on = employee . c . type , polymorphic_identity = 'employee' , exclude_properties = { 'engineer_info' , 'manager_data' }) mapper ( Manager , inherits = Employee , polymorphic_identity = 'manager' , exclude_properties = { 'engineer_info' }) mapper ( Engineer , inherits = Employee , polymorphic_identity = 'engineer' , exclude_properties = { 'manager_data' }) One to many \u2691 from sqlalchemy.orm import relationship class User ( db . Model ): __tablename__ = 'user' id = Column ( Integer , primary_key = True ) posts = relationship ( 'Post' , back_populates = 'user' ) class Post ( db . Model ): id = Column ( Integer , primary_key = True ) body = Column ( String ( 140 )) user_id = Column ( Integer , ForeignKey ( 'user.id' )) user = relationship ( 'User' , back_populates = 'posts' ) In the tests of the Post class, only check that the user attribute is present. Factoryboy supports the creation of Dependent objects direct ForeignKey . Self referenced one to many \u2691 class Task ( Base ): __tablename__ = 'task' id = Column ( String , primary_key = True , doc = 'fulid of creation' ) parent_id = Column ( String , ForeignKey ( 'task.id' )) parent = relationship ( 'Task' , remote_side = [ id ], backref = 'children' ) Many to many \u2691 # Association tables source_has_category = Table ( 'source_has_category' , Base . metadata , Column ( 'source_id' , Integer , ForeignKey ( 'source.id' )), Column ( 'category_id' , Integer , ForeignKey ( 'category.id' )) ) # Tables class Category ( Base ): __tablename__ = 'category' id = Column ( String , primary_key = True ) contents = relationship ( 'Content' , back_populates = 'categories' , secondary = source_has_category , ) class Content ( Base ): __tablename__ = 'content' id = Column ( Integer , primary_key = True , doc = 'Content ID' ) categories = relationship ( 'Category' , back_populates = 'contents' , secondary = source_has_category , ) Self referenced many to many \u2691 Using the followers table as an association table. followers = db . Table ( 'followers' , Base . metadata , Column ( 'follower_id' , Integer , ForeignKey ( 'user.id' )), Column ( 'followed_id' , Integer , ForeignKey ( 'user.id' )), ) class User ( Base ): __tablename__ = 'user' id = Column ( Integer , primary_key = True ) followed = relationship ( 'User' , secondary = followers , primaryjoin = ( followers . c . follower_id == id ), secondaryjoin = ( followers . c . followed_id == id ), backref = db . backref ( 'followers' , lazy = 'dynamic' ), lazy = 'dynamic' , ) Links User instances to other User instances, so as a convention let's say that for a pair of users linked by this relationship, the left side user is following the right side user. The relationship definition is created as seen from the left side user with the name followed, because when this relationship is queried from the left side it will get the list of followed users (i.e those on the right side). User : Is the right side entity of the relationship. Since this is a self-referential relationship, The same class must be used on both sides. secondary : configures the association table that is used for this relationship. primaryjoin : Indicates the condition that links the left side entity (the follower user) with the association table. The join condition for the left side of the relationship is the user id matching the follower_id field of the association table. The followers.c.follower_id expression references the follower_id column of the association table. secondaryjoin : Indicates the condition that links the right side entity (the followed user) with the association table. This condition is similar to the one for primaryjoin . backref : Defines how this relationship will be accessed from the right side entity. From the left side, the relationship is named followed , so from the right side, the name followers represent all the left side users that are linked to the target user in the right side. The additional lazy argument indicates the execution mode for this query. A mode of dynamic sets up the query not to run until specifically requested. lazy : same as with backref , but this one applies to the left side query instead of the right side. Testing SQLAlchemy Code \u2691 The definition of the database can be tested, I usually use them to test that the attributes are loaded and that the factory objects work as expected. Several steps need to be set to make it work: Create the factory boy objects in tests/factories.py . Configure the tests to use a temporal sqlite database in the tests/conftest.py file with the following contents (changing {{ program_name }} ): from alembic.command import upgrade from alembic.config import Config from sqlalchemy.orm import sessionmaker import os import pytest import tempfile temp_ddbb = tempfile . mkstemp ()[ 1 ] os . environ [ '{{ program_name }} _DATABASE_URL' ] = 'sqlite:/// {} ' . format ( temp_ddbb ) # It needs to be after the environmental variable from {{ program_name }} . models import engine from tests import factories @pytest . fixture ( scope = 'module' ) def connection (): ''' Fixture to set up the connection to the temporal database, the path is stablished at conftest.py ''' # Create database connection connection = engine . connect () # Applies all alembic migrations. config = Config ( '{{ program_name }}/migrations/alembic.ini' ) upgrade ( config , 'head' ) # End of setUp yield connection # Start of tearDown connection . close () @pytest . fixture ( scope = 'function' ) def session ( connection ): ''' Fixture to set up the sqlalchemy session of the database. ''' # Begin a non-ORM transaction and bind session transaction = connection . begin () session = sessionmaker ()( bind = connection ) factories . UserFactory . _meta . sqlalchemy_session = session yield session # Close session and rollback transaction session . close () transaction . rollback () Define an abstract base test class BaseModelTest defined as following in the tests/unit/test_models.py file. from {{ program_name }} import models from tests import factories import pytest class BaseModelTest : \"\"\" Abstract base test class to refactor model tests. The Children classes must define the following attributes: self.model: The model object to test. self.dummy_instance: A factory object of the model to test. self.model_attributes: List of model attributes to test Public attributes: dummy_instance (Factory_boy object): Dummy instance of the model. \"\"\" @pytest . fixture ( autouse = True ) def base_setup ( self , session ): self . session = session def test_attributes_defined ( self ): for attribute in self . model_attributes : assert getattr ( self . model , attribute ) == \\ getattr ( self . dummy_instance , attribute ) @pytest . mark . usefixtures ( 'base_setup' ) class TestUser ( BaseModelTest ): @pytest . fixture ( autouse = True ) def setup ( self , session ): self . factory = factories . UserFactory self . dummy_instance = self . factory . create () self . model = models . User ( id = self . dummy_instance . id , name = self . dummy_instance . name , ) self . model_attributes = [ 'name' , 'id' , ] Then create the models table . Create an alembic revision Run pytest : python -m pytest . Exporting database to json \u2691 import json def dump_sqlalchemy ( output_connection_string , output_schema ): \"\"\" Returns the entire content of a database as lists of dicts\"\"\" engine = create_engine ( f ' { output_connection_string }{ output_schema } ' ) meta = MetaData () meta . reflect ( bind = engine ) # http://docs.sqlalchemy.org/en/rel_0_9/core/reflection.html result = {} for table in meta . sorted_tables : result [ table . name ] = [ dict ( row ) for row in engine . execute ( table . select ())] return json . dumps ( result ) Cloning an SQLAlchemy object \u2691 The following function: Copies all the non-primary-key columns from the input model to a new model instance. Allows definition of specific arguments. Leaves the original model object unmodified. def clone_model ( model , ** kwargs ): \"\"\"Clone an arbitrary sqlalchemy model object without its primary key values.\"\"\" table = model . __table__ non_primary_key_columns = [ column_name for column_name in table . __mapper__ . attrs .. keys () if column_name not in table . primary_key ] data = { column_name : getattr ( model , column_name ) for column_name in non_pk_columns } data . update ( kwargs ) return model . __class__ ( ** data ) References \u2691 Home Docs","title":"SQLAlchemy"},{"location":"coding/python/sqlalchemy/#creating-an-sql-schema","text":"First of all it's important to create a diagram with the database structure, it will help you in the design and it's a great improvement of the project's documentation. I usually use ondras wwwsqldesigner through his demo , as it's easy to use and it's possible to save the data in your repository in an xml file. I assume you've already set up your project to support sqlalchemy in your project. If not, do so before moving forward.","title":"Creating an SQL Schema"},{"location":"coding/python/sqlalchemy/#mapping-styles","text":"Modern SQLAlchemy features two distinct styles of mapper configuration. The \u201cClassical\u201d style is SQLAlchemy\u2019s original mapping API, whereas \u201cDeclarative\u201d is the richer and more succinct system that builds on top of \u201cClassical\u201d. The Classical, on the other hand, doesn't lock your models to the ORM, something that we avoid when using the repository pattern . If you aren't going to use the repository pattern, use the declarative way, otherwise use the classical one","title":"Mapping styles"},{"location":"coding/python/sqlalchemy/#creating-tables","text":"If you simply want to create a table of association without any parameters, such as with a many to many relationship association table, use this type of object.","title":"Creating Tables"},{"location":"coding/python/sqlalchemy/#declarative-type","text":"class User ( Base ): \"\"\" Class to define the User model. \"\"\" __tablename__ = 'user' id = Column ( Integer , primary_key = True , doc = 'User ID' ) name = Column ( String , doc = 'User name' ) def __init__ ( self , id , name = None , ): self . id = id self . name = name There are different types of fields to add to a table: Boolean: is_true = Column(Boolean) . Datetime: created_date = Column(DateTime, doc='Date of creation') . Float: score = Column(Float) Integer: id = Column(Integer, primary_key=True, doc='Source ID') . String: title = Column(String) . Text: long_text = Column(Text) . To make sure that a field can't contain nulls set the nullable=False attribute in the definition of the Column . If you want the contents to be unique use unique=True . If you want to use the Mysql driver of SQLAlchemy make sure to specify the length of the colums, for example String(16) . For reference this are the common lengths: url: 2083 name: 64 (it occupies the same 2 and 255). email: 64 (it occupies the same 2 and 255). username: 64 (it occupies the same 2 and 255).","title":"Declarative type"},{"location":"coding/python/sqlalchemy/#classical-type","text":"File: model.py class User (): def __init__ ( self , id , name = None ): self . id = id self . name = name File: orm.py from models import User from sqlalchemy import ( Column , MetaData , String , Table , Text , ) metadata = MetaData () user = Table ( \"user\" , metadata , Column ( \"id\" , String ( 64 ), primary_key = True ), Column ( \"name\" , String ( 64 )), ) def start_mappers (): mapper ( User , user )","title":"Classical type"},{"location":"coding/python/sqlalchemy/#creating-relationships","text":"","title":"Creating relationships"},{"location":"coding/python/sqlalchemy/#joined-table-inheritance","text":"In joined table inheritance, each class along a hierarchy of classes is represented by a distinct table. Querying for a particular subclass in the hierarchy will render as a SQL JOIN along all tables in its inheritance path. If the queried class is the base class, the default behavior is to include only the base table in a SELECT statement. In all cases, the ultimate class to instantiate for a given row is determined by a discriminator column or an expression that works against the base table. When a subclass is loaded only against a base table, resulting objects by default will have base attributes populated at first; attributes that are local to the subclass will lazy load when they are accessed. The base class in a joined inheritance hierarchy is configured with additional arguments that will refer to the polymorphic discriminator column as well as the identifier for the base class.","title":"Joined table inheritance"},{"location":"coding/python/sqlalchemy/#declarative","text":"class Employee ( Base ): __tablename__ = 'employee' id = Column ( Integer , primary_key = True ) name = Column ( String ( 50 )) type = Column ( String ( 50 )) __mapper_args__ = { 'polymorphic_identity' : 'employee' , 'polymorphic_on' : type } class Engineer ( Employee ): __tablename__ = 'engineer' id = Column ( Integer , ForeignKey ( 'employee.id' ), primary_key = True ) engineer_name = Column ( String ( 30 )) __mapper_args__ = { 'polymorphic_identity' : 'engineer' , } class Manager ( Employee ): __tablename__ = 'manager' id = Column ( Integer , ForeignKey ( 'employee.id' ), primary_key = True ) manager_name = Column ( String ( 30 )) __mapper_args__ = { 'polymorphic_identity' : 'manager' , }","title":"Declarative"},{"location":"coding/python/sqlalchemy/#classical","text":"File: model.py class Employee : def __init__ ( self , name ): self . name = name class Manager ( Employee ): def __init__ ( self , name , manager_data ): super () . __init__ ( name ) self . manager_data = manager_data class Engineer ( Employee ): def __init__ ( self , name , engineer_info ): super () . __init__ ( name ) self . engineer_info = engineer_info File: orm.py metadata = MetaData () employee = Table ( 'employee' , metadata , Column ( 'id' , Integer , primary_key = True ), Column ( 'name' , String ( 50 )), Column ( 'type' , String ( 20 )), Column ( 'manager_data' , String ( 50 )), Column ( 'engineer_info' , String ( 50 )) ) mapper ( Employee , employee , polymorphic_on = employee . c . type , polymorphic_identity = 'employee' , exclude_properties = { 'engineer_info' , 'manager_data' }) mapper ( Manager , inherits = Employee , polymorphic_identity = 'manager' , exclude_properties = { 'engineer_info' }) mapper ( Engineer , inherits = Employee , polymorphic_identity = 'engineer' , exclude_properties = { 'manager_data' })","title":"Classical"},{"location":"coding/python/sqlalchemy/#one-to-many","text":"from sqlalchemy.orm import relationship class User ( db . Model ): __tablename__ = 'user' id = Column ( Integer , primary_key = True ) posts = relationship ( 'Post' , back_populates = 'user' ) class Post ( db . Model ): id = Column ( Integer , primary_key = True ) body = Column ( String ( 140 )) user_id = Column ( Integer , ForeignKey ( 'user.id' )) user = relationship ( 'User' , back_populates = 'posts' ) In the tests of the Post class, only check that the user attribute is present. Factoryboy supports the creation of Dependent objects direct ForeignKey .","title":"One to many"},{"location":"coding/python/sqlalchemy/#self-referenced-one-to-many","text":"class Task ( Base ): __tablename__ = 'task' id = Column ( String , primary_key = True , doc = 'fulid of creation' ) parent_id = Column ( String , ForeignKey ( 'task.id' )) parent = relationship ( 'Task' , remote_side = [ id ], backref = 'children' )","title":"Self referenced one to many"},{"location":"coding/python/sqlalchemy/#many-to-many","text":"# Association tables source_has_category = Table ( 'source_has_category' , Base . metadata , Column ( 'source_id' , Integer , ForeignKey ( 'source.id' )), Column ( 'category_id' , Integer , ForeignKey ( 'category.id' )) ) # Tables class Category ( Base ): __tablename__ = 'category' id = Column ( String , primary_key = True ) contents = relationship ( 'Content' , back_populates = 'categories' , secondary = source_has_category , ) class Content ( Base ): __tablename__ = 'content' id = Column ( Integer , primary_key = True , doc = 'Content ID' ) categories = relationship ( 'Category' , back_populates = 'contents' , secondary = source_has_category , )","title":"Many to many"},{"location":"coding/python/sqlalchemy/#self-referenced-many-to-many","text":"Using the followers table as an association table. followers = db . Table ( 'followers' , Base . metadata , Column ( 'follower_id' , Integer , ForeignKey ( 'user.id' )), Column ( 'followed_id' , Integer , ForeignKey ( 'user.id' )), ) class User ( Base ): __tablename__ = 'user' id = Column ( Integer , primary_key = True ) followed = relationship ( 'User' , secondary = followers , primaryjoin = ( followers . c . follower_id == id ), secondaryjoin = ( followers . c . followed_id == id ), backref = db . backref ( 'followers' , lazy = 'dynamic' ), lazy = 'dynamic' , ) Links User instances to other User instances, so as a convention let's say that for a pair of users linked by this relationship, the left side user is following the right side user. The relationship definition is created as seen from the left side user with the name followed, because when this relationship is queried from the left side it will get the list of followed users (i.e those on the right side). User : Is the right side entity of the relationship. Since this is a self-referential relationship, The same class must be used on both sides. secondary : configures the association table that is used for this relationship. primaryjoin : Indicates the condition that links the left side entity (the follower user) with the association table. The join condition for the left side of the relationship is the user id matching the follower_id field of the association table. The followers.c.follower_id expression references the follower_id column of the association table. secondaryjoin : Indicates the condition that links the right side entity (the followed user) with the association table. This condition is similar to the one for primaryjoin . backref : Defines how this relationship will be accessed from the right side entity. From the left side, the relationship is named followed , so from the right side, the name followers represent all the left side users that are linked to the target user in the right side. The additional lazy argument indicates the execution mode for this query. A mode of dynamic sets up the query not to run until specifically requested. lazy : same as with backref , but this one applies to the left side query instead of the right side.","title":"Self referenced many to many"},{"location":"coding/python/sqlalchemy/#testing-sqlalchemy-code","text":"The definition of the database can be tested, I usually use them to test that the attributes are loaded and that the factory objects work as expected. Several steps need to be set to make it work: Create the factory boy objects in tests/factories.py . Configure the tests to use a temporal sqlite database in the tests/conftest.py file with the following contents (changing {{ program_name }} ): from alembic.command import upgrade from alembic.config import Config from sqlalchemy.orm import sessionmaker import os import pytest import tempfile temp_ddbb = tempfile . mkstemp ()[ 1 ] os . environ [ '{{ program_name }} _DATABASE_URL' ] = 'sqlite:/// {} ' . format ( temp_ddbb ) # It needs to be after the environmental variable from {{ program_name }} . models import engine from tests import factories @pytest . fixture ( scope = 'module' ) def connection (): ''' Fixture to set up the connection to the temporal database, the path is stablished at conftest.py ''' # Create database connection connection = engine . connect () # Applies all alembic migrations. config = Config ( '{{ program_name }}/migrations/alembic.ini' ) upgrade ( config , 'head' ) # End of setUp yield connection # Start of tearDown connection . close () @pytest . fixture ( scope = 'function' ) def session ( connection ): ''' Fixture to set up the sqlalchemy session of the database. ''' # Begin a non-ORM transaction and bind session transaction = connection . begin () session = sessionmaker ()( bind = connection ) factories . UserFactory . _meta . sqlalchemy_session = session yield session # Close session and rollback transaction session . close () transaction . rollback () Define an abstract base test class BaseModelTest defined as following in the tests/unit/test_models.py file. from {{ program_name }} import models from tests import factories import pytest class BaseModelTest : \"\"\" Abstract base test class to refactor model tests. The Children classes must define the following attributes: self.model: The model object to test. self.dummy_instance: A factory object of the model to test. self.model_attributes: List of model attributes to test Public attributes: dummy_instance (Factory_boy object): Dummy instance of the model. \"\"\" @pytest . fixture ( autouse = True ) def base_setup ( self , session ): self . session = session def test_attributes_defined ( self ): for attribute in self . model_attributes : assert getattr ( self . model , attribute ) == \\ getattr ( self . dummy_instance , attribute ) @pytest . mark . usefixtures ( 'base_setup' ) class TestUser ( BaseModelTest ): @pytest . fixture ( autouse = True ) def setup ( self , session ): self . factory = factories . UserFactory self . dummy_instance = self . factory . create () self . model = models . User ( id = self . dummy_instance . id , name = self . dummy_instance . name , ) self . model_attributes = [ 'name' , 'id' , ] Then create the models table . Create an alembic revision Run pytest : python -m pytest .","title":"Testing SQLAlchemy Code"},{"location":"coding/python/sqlalchemy/#exporting-database-to-json","text":"import json def dump_sqlalchemy ( output_connection_string , output_schema ): \"\"\" Returns the entire content of a database as lists of dicts\"\"\" engine = create_engine ( f ' { output_connection_string }{ output_schema } ' ) meta = MetaData () meta . reflect ( bind = engine ) # http://docs.sqlalchemy.org/en/rel_0_9/core/reflection.html result = {} for table in meta . sorted_tables : result [ table . name ] = [ dict ( row ) for row in engine . execute ( table . select ())] return json . dumps ( result )","title":"Exporting database to json"},{"location":"coding/python/sqlalchemy/#cloning-an-sqlalchemy-object","text":"The following function: Copies all the non-primary-key columns from the input model to a new model instance. Allows definition of specific arguments. Leaves the original model object unmodified. def clone_model ( model , ** kwargs ): \"\"\"Clone an arbitrary sqlalchemy model object without its primary key values.\"\"\" table = model . __table__ non_primary_key_columns = [ column_name for column_name in table . __mapper__ . attrs .. keys () if column_name not in table . primary_key ] data = { column_name : getattr ( model , column_name ) for column_name in non_pk_columns } data . update ( kwargs ) return model . __class__ ( ** data )","title":"Cloning an SQLAlchemy object"},{"location":"coding/python/sqlalchemy/#references","text":"Home Docs","title":"References"},{"location":"coding/python/tinydb/","text":"Tinydb is a document oriented database that stores data in a json file. It's the closest solution to a NoSQL SQLite solution that I've found. The advantages are that you can use a NoSQL database without installing a server. Tinydb is small, simple to use, well tested, optimized and extensible . On the other hand, if you are searching for advanced database features like more than one connection or high performance, you should consider using databases like SQLite or MongoDB. I think it's the perfect solution for initial versions of a program, when the database schema is variable and there is no need of high performance. Once the program is stabilized and the performance drops, you can change the storage provider to a production ready one. To make this change doable, I recommend implementing the repository pattern to decouple the storage layer from your application logic. Install \u2691 pip install tinydb Basic usage \u2691 TL;DR: Operation Cheatsheet Inserting data : db.insert(...) . Getting data : db.all() : Get all documents. iter(db) : Iterate over all the documents. db.search(query) : Get a list of documents matching the query. Updating : db.update(fields, query) : Update all documents matching the query to contain fields. Removing : db.remove(query) : Remove all documents matching the query. db.truncate() : Remove all documents. Querying : Query() : Create a new query object. Query().field == 2 : Match any document that has a key field with value == 2 (also possible: != , > , >= , < , <= ). First you need to setup the database: from tinydb import TinyDB , Query db = TinyDB ( 'db.json' ) TinyDB expects the data to be Python dictionaries: db . insert ({ 'type' : 'apple' , 'count' : 7 }) db . insert ({ 'type' : 'peach' , 'count' : 3 }) You can also iterate over stored documents: >>> for item in db : >>> print ( item ) { 'count' : 7 , 'type' : 'apple' } { 'count' : 3 , 'type' : 'peach' } You can search for specific documents: >>> Fruit = Query () >>> db . search ( Fruit . type == 'peach' ) [{ 'count' : 3 , 'type' : 'peach' }] >>> db . search ( Fruit . count > 5 ) [{ 'count' : 7 , 'type' : 'apple' }] You can update fields: >>> db . update ({ 'count' : 10 }, Fruit . type == 'apple' ) >>> db . all () [{ 'count' : 10 , 'type' : 'apple' }, { 'count' : 3 , 'type' : 'peach' }] And remove documents: >>> db . remove ( Fruit . count < 5 ) >>> db . all () [{ 'count' : 10 , 'type' : 'apple' }] Query construction \u2691 Match any document where a field called field exists: Query().field.exists() . Match any document with the whole field matching the regular expression: Query().field.matches(regex) . Match any document with a substring of the field matching the regular expression: Query().field.search(regex) . Match any document for which the function returns True : Query().field.test(func, *args) . If given a query, match all documents where all documents in the list field match the query. If given a list, matches all documents where all documents in the list field are a member of the given list: Query().field.all(query | list) . If given a query, match all documents where at least one document in the list field match the query. If given a list, matches all documents where at least one documents in the list field are a member of the given list: Query().field.any(query | list) . Match if the field is contained in the list: Query().field.one_of(list) . Logical operations on queries Match documents that don't match the query: ~ (query) . Match documents that match both queries: (query1) & (query2) . Match documents that match at least one of the queries: (query1) | (query2) . To retrieve the data from the database, you need to use Query objects in a similar way as you do with ORMs. from tinydb import Query User = Query () db . search ( User . name == 'John' ) db . search ( User . birthday . year == 1990 ) If the field is not a valid Python identifier use the following syntax: db . search ( User [ 'country-code' ] == 'foo' ) Advanced queries \u2691 TinyDB supports other ways to search in your data: Testing the existence of a field: db . search ( User . name . exists ()) Testing values against regular expressions: # Full item has to match the regex: db . search ( User . name . matches ( '[aZ]*' )) # Any part of the item has to match the regex: db . search ( User . name . search ( 'b+' )) Testing using custom tests: # Custom test: test_func = lambda s : s == 'John' db . search ( User . name . test ( test_func )) # Custom test with parameters: def test_func ( val , m , n ): return m <= val <= n db . search ( User . age . test ( test_func , 0 , 21 )) db . search ( User . age . test ( test_func , 21 , 99 )) Testing fields that contain lists with the any and all methods: Assuming we have a user object with a groups list like this: db . insert ({ 'name' : 'user1' , 'groups' : [ 'user' ]}) db . insert ({ 'name' : 'user2' , 'groups' : [ 'admin' , 'user' ]}) db . insert ({ 'name' : 'user3' , 'groups' : [ 'sudo' , 'user' ]}) You can use the following queries: # User's groups include at least one value from ['admin', 'sudo'] >>> db . search ( User . groups . any ([ 'admin' , 'sudo' ])) [{ 'name' : 'user2' , 'groups' : [ 'admin' , 'user' ]}, { 'name' : 'user3' , 'groups' : [ 'sudo' , 'user' ]}] # User's groups include all values from ['admin', 'user'] >>> db . search ( User . groups . all ([ 'admin' , 'user' ])) [{ 'name' : 'user2' , 'groups' : [ 'admin' , 'user' ]}] Testing nested queries: Assuming we have the following table: Group = Query () Permission = Query () groups = db . table ( 'groups' ) groups . insert ({ 'name' : 'user' , 'permissions' : [{ 'type' : 'read' }]}) groups . insert ({ 'name' : 'sudo' , 'permissions' : [{ 'type' : 'read' }, { 'type' : 'sudo' }]}) groups . insert ({ 'name' : 'admin' , 'permissions' : [{ 'type' : 'read' }, { 'type' : 'write' }, { 'type' : 'sudo' }]}) You can search this table using nested any / all queries: # Group has a permission with type 'read' >>> groups . search ( Group . permissions . any ( Permission . type == 'read' )) [{ 'name' : 'user' , 'permissions' : [{ 'type' : 'read' }]}, { 'name' : 'sudo' , 'permissions' : [{ 'type' : 'read' }, { 'type' : 'sudo' }]}, { 'name' : 'admin' , 'permissions' : [{ 'type' : 'read' }, { 'type' : 'write' }, { 'type' : 'sudo' }]}] # Group has ONLY permission 'read' >>> groups . search ( Group . permissions . all ( Permission . type == 'read' )) [{ 'name' : 'user' , 'permissions' : [{ 'type' : 'read' }]}] any tests if there is at least one document matching the query while all ensures all documents match the query. The opposite operation, checking if a list contains a single item, is also possible using one_of : >>> db . search ( User . name . one_of ([ 'jane' , 'john' ])) Query modifiers \u2691 TinyDB allows you to use logical operations to change and combine queries Negate a query: db.search(~ (User.name == 'John')) . Logical AND : db.search((User.name == 'John') & (User.age <= 30)) . Logical OR : db.search((User.name == 'John') | (User.name == 'Bob')) . Inserting more than one document \u2691 In case you want to insert more than one document, you can use db.insert_multiple(...) : >>> db . insert_multiple ([ { 'name' : 'John' , 'age' : 22 }, { 'name' : 'John' , 'age' : 37 }]) >>> db . insert_multiple ({ 'int' : 1 , 'value' : i } for i in range ( 2 )) Updating data \u2691 To update all the documents of the database, leave out the query argument: db . update ({ 'foo' : 'bar' }) When you pass a dictionary to db.update(fields, query) , you update a document by adding or overwriting its values. TinyDB also supports some common operations you can do on your data: delete(key) : Delete a key from the document. increment(key) : Increment the value of a key. decrement(key) : Decrement the value of a key. add(key, value) : Add value to the value of a key (also works for strings). subtract(key, value) : Subtract value from the value of a key. set(key, value) : Set key to value. >>> from tinydb.operations import delete >>> db . update ( delete ( 'key1' ), User . name == 'John' ) This will remove the key key1 from all matching documents. You also can write your own operations: >>> def your_operation ( your_arguments ): ... def transform ( doc ): ... # do something with the document ... # ... ... return transform ... >>> db . update ( your_operation ( arguments ), query ) Retrieving data \u2691 If you want to get one element use db.get(...) . Be warned, if more than one document match the query, a random will be returned. >>> db . get ( User . name == 'John' ) { 'name' : 'John' , 'age' : 22 } If you want to know if the database stores a document, use db.contains(...) . >>> db . contains ( User . name == 'John' ) True If you want to know the number of documents that match a query use db.count(...) . >>> db . count ( User . name == 'John' ) 2 Serializing custom data \u2691 TinyDB has a limited support to serialize common objects, they added support for custom serializers but it's not yet documented . Check the tinydb-serialization package to see how to implement your own. Serializing datetime objects \u2691 The tinydb-serialization package gives serialization objects for datetime objects. from tinydb import TinyDB from tinydb.storages import JSONStorage from tinydb_serialization import SerializationMiddleware from tinydb_serialization.serializers import DateTimeSerializer serialization = SerializationMiddleware ( JSONStorage ) serialization . register_serializer ( DateTimeSerializer (), 'TinyDate' ) db = TinyDB ( 'db.json' , storage = serialization ) Tables \u2691 TinyDB supports working with more than one table. To create and use a table, use db.table(name) . They behave as the TinyDB class. >>> table = db . table ( 'table_name' ) >>> table . insert ({ 'value' : True }) >>> table . all () [{ 'value' : True }] >>> for row in table : >>> print ( row ) { 'value' : True } To remove a table from a database, use: db . drop_table ( 'table_name' ) To remove all tables, use: db . drop_tables () To get a list with the names of all tables in your database: >>> db . tables () { '_default' , 'table_name' } Query caching \u2691 TinyDB caches query result for performance. That way re-running a query won't have to read the data from the storage as long as the database hasn't been modified. You can optimize the query cache size by passing the cache_size to the table(...) function: table = db . table ( 'table_name' , cache_size = 30 ) You can set cache_size to None to make the cache unlimited in size. Also, you can set cache_size to 0 to disable it. Storage types \u2691 TinyDB comes with two storage types: JSON and in-memory. By default TinyDB stores its data in JSON files so you have to specify the path where to store it: from tinydb import TinyDB , where db = TinyDB ( 'path/to/db.json' ) To use the in-memory storage, use: from tinydb.storages import MemoryStorage db = TinyDB ( storage = MemoryStorage ) All arguments except for the storage argument are forwarded to the underlying storage. For the JSON storage you can use this to pass additional keyword arguments to Python\u2019s json.dump(\u2026) method. For example, you can set it to create prettified JSON files like this: >>> db = TinyDB ( 'db.json' , sort_keys = True , indent = 4 , separators = ( ',' , ': ' )) References \u2691 Docs Git Issues Reference","title":"TinyDB"},{"location":"coding/python/tinydb/#install","text":"pip install tinydb","title":"Install"},{"location":"coding/python/tinydb/#basic-usage","text":"TL;DR: Operation Cheatsheet Inserting data : db.insert(...) . Getting data : db.all() : Get all documents. iter(db) : Iterate over all the documents. db.search(query) : Get a list of documents matching the query. Updating : db.update(fields, query) : Update all documents matching the query to contain fields. Removing : db.remove(query) : Remove all documents matching the query. db.truncate() : Remove all documents. Querying : Query() : Create a new query object. Query().field == 2 : Match any document that has a key field with value == 2 (also possible: != , > , >= , < , <= ). First you need to setup the database: from tinydb import TinyDB , Query db = TinyDB ( 'db.json' ) TinyDB expects the data to be Python dictionaries: db . insert ({ 'type' : 'apple' , 'count' : 7 }) db . insert ({ 'type' : 'peach' , 'count' : 3 }) You can also iterate over stored documents: >>> for item in db : >>> print ( item ) { 'count' : 7 , 'type' : 'apple' } { 'count' : 3 , 'type' : 'peach' } You can search for specific documents: >>> Fruit = Query () >>> db . search ( Fruit . type == 'peach' ) [{ 'count' : 3 , 'type' : 'peach' }] >>> db . search ( Fruit . count > 5 ) [{ 'count' : 7 , 'type' : 'apple' }] You can update fields: >>> db . update ({ 'count' : 10 }, Fruit . type == 'apple' ) >>> db . all () [{ 'count' : 10 , 'type' : 'apple' }, { 'count' : 3 , 'type' : 'peach' }] And remove documents: >>> db . remove ( Fruit . count < 5 ) >>> db . all () [{ 'count' : 10 , 'type' : 'apple' }]","title":"Basic usage"},{"location":"coding/python/tinydb/#query-construction","text":"Match any document where a field called field exists: Query().field.exists() . Match any document with the whole field matching the regular expression: Query().field.matches(regex) . Match any document with a substring of the field matching the regular expression: Query().field.search(regex) . Match any document for which the function returns True : Query().field.test(func, *args) . If given a query, match all documents where all documents in the list field match the query. If given a list, matches all documents where all documents in the list field are a member of the given list: Query().field.all(query | list) . If given a query, match all documents where at least one document in the list field match the query. If given a list, matches all documents where at least one documents in the list field are a member of the given list: Query().field.any(query | list) . Match if the field is contained in the list: Query().field.one_of(list) . Logical operations on queries Match documents that don't match the query: ~ (query) . Match documents that match both queries: (query1) & (query2) . Match documents that match at least one of the queries: (query1) | (query2) . To retrieve the data from the database, you need to use Query objects in a similar way as you do with ORMs. from tinydb import Query User = Query () db . search ( User . name == 'John' ) db . search ( User . birthday . year == 1990 ) If the field is not a valid Python identifier use the following syntax: db . search ( User [ 'country-code' ] == 'foo' )","title":"Query construction"},{"location":"coding/python/tinydb/#advanced-queries","text":"TinyDB supports other ways to search in your data: Testing the existence of a field: db . search ( User . name . exists ()) Testing values against regular expressions: # Full item has to match the regex: db . search ( User . name . matches ( '[aZ]*' )) # Any part of the item has to match the regex: db . search ( User . name . search ( 'b+' )) Testing using custom tests: # Custom test: test_func = lambda s : s == 'John' db . search ( User . name . test ( test_func )) # Custom test with parameters: def test_func ( val , m , n ): return m <= val <= n db . search ( User . age . test ( test_func , 0 , 21 )) db . search ( User . age . test ( test_func , 21 , 99 )) Testing fields that contain lists with the any and all methods: Assuming we have a user object with a groups list like this: db . insert ({ 'name' : 'user1' , 'groups' : [ 'user' ]}) db . insert ({ 'name' : 'user2' , 'groups' : [ 'admin' , 'user' ]}) db . insert ({ 'name' : 'user3' , 'groups' : [ 'sudo' , 'user' ]}) You can use the following queries: # User's groups include at least one value from ['admin', 'sudo'] >>> db . search ( User . groups . any ([ 'admin' , 'sudo' ])) [{ 'name' : 'user2' , 'groups' : [ 'admin' , 'user' ]}, { 'name' : 'user3' , 'groups' : [ 'sudo' , 'user' ]}] # User's groups include all values from ['admin', 'user'] >>> db . search ( User . groups . all ([ 'admin' , 'user' ])) [{ 'name' : 'user2' , 'groups' : [ 'admin' , 'user' ]}] Testing nested queries: Assuming we have the following table: Group = Query () Permission = Query () groups = db . table ( 'groups' ) groups . insert ({ 'name' : 'user' , 'permissions' : [{ 'type' : 'read' }]}) groups . insert ({ 'name' : 'sudo' , 'permissions' : [{ 'type' : 'read' }, { 'type' : 'sudo' }]}) groups . insert ({ 'name' : 'admin' , 'permissions' : [{ 'type' : 'read' }, { 'type' : 'write' }, { 'type' : 'sudo' }]}) You can search this table using nested any / all queries: # Group has a permission with type 'read' >>> groups . search ( Group . permissions . any ( Permission . type == 'read' )) [{ 'name' : 'user' , 'permissions' : [{ 'type' : 'read' }]}, { 'name' : 'sudo' , 'permissions' : [{ 'type' : 'read' }, { 'type' : 'sudo' }]}, { 'name' : 'admin' , 'permissions' : [{ 'type' : 'read' }, { 'type' : 'write' }, { 'type' : 'sudo' }]}] # Group has ONLY permission 'read' >>> groups . search ( Group . permissions . all ( Permission . type == 'read' )) [{ 'name' : 'user' , 'permissions' : [{ 'type' : 'read' }]}] any tests if there is at least one document matching the query while all ensures all documents match the query. The opposite operation, checking if a list contains a single item, is also possible using one_of : >>> db . search ( User . name . one_of ([ 'jane' , 'john' ]))","title":"Advanced queries"},{"location":"coding/python/tinydb/#query-modifiers","text":"TinyDB allows you to use logical operations to change and combine queries Negate a query: db.search(~ (User.name == 'John')) . Logical AND : db.search((User.name == 'John') & (User.age <= 30)) . Logical OR : db.search((User.name == 'John') | (User.name == 'Bob')) .","title":"Query modifiers"},{"location":"coding/python/tinydb/#inserting-more-than-one-document","text":"In case you want to insert more than one document, you can use db.insert_multiple(...) : >>> db . insert_multiple ([ { 'name' : 'John' , 'age' : 22 }, { 'name' : 'John' , 'age' : 37 }]) >>> db . insert_multiple ({ 'int' : 1 , 'value' : i } for i in range ( 2 ))","title":"Inserting more than one document"},{"location":"coding/python/tinydb/#updating-data","text":"To update all the documents of the database, leave out the query argument: db . update ({ 'foo' : 'bar' }) When you pass a dictionary to db.update(fields, query) , you update a document by adding or overwriting its values. TinyDB also supports some common operations you can do on your data: delete(key) : Delete a key from the document. increment(key) : Increment the value of a key. decrement(key) : Decrement the value of a key. add(key, value) : Add value to the value of a key (also works for strings). subtract(key, value) : Subtract value from the value of a key. set(key, value) : Set key to value. >>> from tinydb.operations import delete >>> db . update ( delete ( 'key1' ), User . name == 'John' ) This will remove the key key1 from all matching documents. You also can write your own operations: >>> def your_operation ( your_arguments ): ... def transform ( doc ): ... # do something with the document ... # ... ... return transform ... >>> db . update ( your_operation ( arguments ), query )","title":"Updating data"},{"location":"coding/python/tinydb/#retrieving-data","text":"If you want to get one element use db.get(...) . Be warned, if more than one document match the query, a random will be returned. >>> db . get ( User . name == 'John' ) { 'name' : 'John' , 'age' : 22 } If you want to know if the database stores a document, use db.contains(...) . >>> db . contains ( User . name == 'John' ) True If you want to know the number of documents that match a query use db.count(...) . >>> db . count ( User . name == 'John' ) 2","title":"Retrieving data"},{"location":"coding/python/tinydb/#serializing-custom-data","text":"TinyDB has a limited support to serialize common objects, they added support for custom serializers but it's not yet documented . Check the tinydb-serialization package to see how to implement your own.","title":"Serializing custom data"},{"location":"coding/python/tinydb/#serializing-datetime-objects","text":"The tinydb-serialization package gives serialization objects for datetime objects. from tinydb import TinyDB from tinydb.storages import JSONStorage from tinydb_serialization import SerializationMiddleware from tinydb_serialization.serializers import DateTimeSerializer serialization = SerializationMiddleware ( JSONStorage ) serialization . register_serializer ( DateTimeSerializer (), 'TinyDate' ) db = TinyDB ( 'db.json' , storage = serialization )","title":"Serializing datetime objects"},{"location":"coding/python/tinydb/#tables","text":"TinyDB supports working with more than one table. To create and use a table, use db.table(name) . They behave as the TinyDB class. >>> table = db . table ( 'table_name' ) >>> table . insert ({ 'value' : True }) >>> table . all () [{ 'value' : True }] >>> for row in table : >>> print ( row ) { 'value' : True } To remove a table from a database, use: db . drop_table ( 'table_name' ) To remove all tables, use: db . drop_tables () To get a list with the names of all tables in your database: >>> db . tables () { '_default' , 'table_name' }","title":"Tables"},{"location":"coding/python/tinydb/#query-caching","text":"TinyDB caches query result for performance. That way re-running a query won't have to read the data from the storage as long as the database hasn't been modified. You can optimize the query cache size by passing the cache_size to the table(...) function: table = db . table ( 'table_name' , cache_size = 30 ) You can set cache_size to None to make the cache unlimited in size. Also, you can set cache_size to 0 to disable it.","title":"Query caching"},{"location":"coding/python/tinydb/#storage-types","text":"TinyDB comes with two storage types: JSON and in-memory. By default TinyDB stores its data in JSON files so you have to specify the path where to store it: from tinydb import TinyDB , where db = TinyDB ( 'path/to/db.json' ) To use the in-memory storage, use: from tinydb.storages import MemoryStorage db = TinyDB ( storage = MemoryStorage ) All arguments except for the storage argument are forwarded to the underlying storage. For the JSON storage you can use this to pass additional keyword arguments to Python\u2019s json.dump(\u2026) method. For example, you can set it to create prettified JSON files like this: >>> db = TinyDB ( 'db.json' , sort_keys = True , indent = 4 , separators = ( ',' , ': ' ))","title":"Storage types"},{"location":"coding/python/tinydb/#references","text":"Docs Git Issues Reference","title":"References"},{"location":"coding/python/type_hints/","text":"Type hints are the Python native way to define the type of the objects in a program. Traditionally, the Python interpreter handles types in a flexible but implicit way. Recent versions of Python allow you to specify explicit type hints that different tools can use to help you develop your code more efficiently. TL;DR Use Type hints whenever unit tests are worth writing def headline ( text : str , align : bool = True ) -> str : if align : return f \" { text . title () } \\n { '-' * len ( text ) } \" else : return f \" { text . title () } \" . center ( 50 , \"o\" ) Type hints are not enforced on their own by python. So you won't catch an error if you try to run headline(\"use mypy\", align=\"center\") unless you use a static type checker like Mypy . Advantages and disadvantages \u2691 Advantages: Help catch certain errors if used with a static type checker. Help check your code . It's not trivial to use docstrings to do automatic checks. Help to reason about code: Knowing the parameters type makes it a lot easier to understand and maintain a code base. It can speed up the time required to catch up with a code snippet. Always remember that you read code a lot more often than you write it, so you should optimize for ease of reading. Help you build and maintain a cleaner architecture . The act of writing type hints force you to think about the types in your program. Helps refactoring: Type hints make it trivial to find where a given class is used when you're trying to refactor your code base. Improve IDEs and linters. Cons: Type hints take developer time and effort to add . Even though it probably pays off in spending less time debugging, you will spend more time entering code. Introduce a slight penalty in start-up time . If you need to use the typing module, the import time may be significant, even more in short scripts. Work best in modern Pythons. Follow these guidelines when deciding if you want to add types to your project: In libraries that will be used by others, they add a lot of value. In complex projects, type hints help you understand how types flow through your code and are highly recommended. If you are beginning to learn Python, don't use them yet. If you are writing throw-away scripts, don't use them. So, Use Type hints whenever unit tests are worth writing . Usage \u2691 Function annotations \u2691 def func ( arg : arg_type , optarg : arg_type = default ) -> return_type : ... For arguments the syntax is argument: annotation , while the return type is annotated using -> annotation . Note that the annotation must be a valid Python expression. When running the code, the special .__annotations__ attribute on the function stores the typing information. Variable annotations \u2691 Sometimes the type checker needs help in figuring out the types of variables as well. The syntax is similar: pi : float = 3.142 def circumference ( radius : float ) -> float : return 2 * pi * radius Composite types \u2691 If you need to hint other types than str , float and bool , you'll need to import the typing module. For example to define the hint types of list, dictionaries and tuples: >>> from typing import Dict , List , Tuple >>> names : List [ str ] = [ \"Guido\" , \"Jukka\" , \"Ivan\" ] >>> version : Tuple [ int , int , int ] = ( 3 , 7 , 1 ) >>> options : Dict [ str , bool ] = { \"centered\" : False , \"capitalize\" : True } If your function expects some kind of sequence but don't care whether it's a list or a tuple, use the typing.Sequence object. Dictionaries with different value types per key . \u2691 TypedDict declares a dictionary type that expects all of its instances to have a certain set of keys, where each key is associated with a value of a consistent type. This expectation is not checked at runtime but is only enforced by type checkers. TypedDict started life as an experimental Mypy feature to wrangle typing onto the heterogeneous, structure-oriented use of dictionaries. As of Python 3.8, it was adopted into the standard library. try : from typing import TypedDict # >=3.8 except ImportError : from mypy_extensions import TypedDict # <=3.7 Movie = TypedDict ( 'Movie' , { 'name' : str , 'year' : int }) A class-based type constructor is also available: class Movie ( TypedDict ): name : str year : int By default, all keys must be present in a TypedDict . It is possible to override this by specifying totality. Usage: class point2D ( TypedDict , total = False ): x : int y : int This means that a point2D TypedDict can have any of the keys omitted. A type checker is only expected to support a literal False or True as the value of the total argument. True is the default, and makes all items defined in the class body be required. Functions without return values \u2691 Some functions aren't meant to return anything. Use the -> None hint in these cases. def play ( player_name : str ) -> None : print ( f \" { player_name } plays\" ) ret_val = play ( \"Filip\" ) The annotation help catch the kinds of subtle bugs where you are trying to use a meaningless return value. If your function doesn't return any object, use the NoReturn type. from typing import NoReturn def black_hole () -> NoReturn : raise Exception ( \"There is no going back ...\" ) Note This is the first iteration of the synoptical reading of the full Real python article on type checking . Optional arguments \u2691 A common pattern is to use None as a default value for an argument. This is done either to avoid problems with mutable default values or to have a sentinel value flagging special behavior. This creates a challenge for type hinting as the argument may be of type string (for example) but it can also be None . We use the Optional type to address this case. from typing import Optional def player ( name : str , start : Optional [ str ] = None ) -> str : ... A similar way would be to use Union[None, str] . Type aliases \u2691 Type hints might become oblique when working with nested types. If it's the case, save them into a new variable, and use that instead. from typing import List , Tuple Card = Tuple [ str , str ] Deck = List [ Card ] def deal_hands ( deck : Deck ) -> Tuple [ Deck , Deck , Deck , Deck ]: \"\"\"Deal the cards in the deck into four hands\"\"\" return ( deck [ 0 :: 4 ], deck [ 1 :: 4 ], deck [ 2 :: 4 ], deck [ 3 :: 4 ]) Allow any subclass \u2691 Every class is also a valid type. Any instance of a subclass is also compatible with all superclasses \u2013 it follows that every value is compatible with the object type (and incidentally also the Any type, discussed below). Mypy analyzes the bodies of classes to determine which methods and attributes are available in instances. For example class A : def f ( self ) -> int : # Type of self inferred (A) return 2 class B ( A ): def f ( self ) -> int : return 3 def g ( self ) -> int : return 4 def foo ( a : A ) -> None : print ( a . f ()) # 3 a . g () # Error: \"A\" has no attribute \"g\" foo ( B ()) # OK (B is a subclass of A) Deduce returned value type from the arguments \u2691 The previous approach works if you don't need to use class objects that inherit from a given class. For example: class User : # Defines fields like name, email class BasicUser ( User ): def upgrade ( self ): \"\"\"Upgrade to Pro\"\"\" class ProUser ( User ): def pay ( self ): \"\"\"Pay bill\"\"\" def new_user ( user_class ) -> User : user = user_class () # (Here we could write the user object to a database) return user Where: ProUser doesn\u2019t inherit from BasicUser . new_user creates an instance of one of these classes if you pass it the right class object. The problem is that right now mypy doesn't know which subclass of User you're giving it, and will only accept the methods and attributes defined in the parent class User . buyer = new_user ( ProUser ) buyer . pay () # Rejected, not a method on User This can be solved using Type variables with upper bounds . UserType = TypeVar ( 'UserType' , bound = User ) def new_user ( user_class : Type [ UserType ]) -> UserType : # Same implementation as before We're creating a new type UserType that is linked to the class or subclasses of User . That way, mypy knows that the return value is an object created from the class given in the argument user_class . beginner = new_user ( BasicUser ) # Inferred type is BasicUser beginner . upgrade () # OK Keep in mind that the TypeVar is a Generic type , as such, they take one or more type parameters, similar to built-in types such as List[X] . That means that when you create type aliases, you'll need to give the type parameter. So: UserType = TypeVar ( \"UserType\" , bound = User ) UserTypes = List [ Type [ UserType ]] def new_users ( user_class : UserTypes ) -> UserType : # Type error! pass Will give a Missing type parameters for generic type \"UserTypes\" error. To solve it use: def new_users ( user_class : UserTypes [ UserType ]) -> UserType : # OK! pass Specify the type of the class in it's method and attributes \u2691 If you are using Python 3.10 or later, it just works. Python 3.7 introduces PEP 563: postponed evaluation of annotations. A module that uses the future statement from __future__ import annotations to store annotations as strings automatically: from __future__ import annotations class Position : def __add__ ( self , other : Position ) -> Position : ... But pyflakes will still complain, so I've used strings. from __future__ import annotations class Position : def __add__ ( self , other : 'Position' ) -> 'Position' : ... Type hints of Generators \u2691 from typing import Generator def generate () -> Generator [ int , None , None ]: Where the first argument of Generator is the type of the yielded value. Using mypy with an existing codebase \u2691 These steps will get you started with mypy on an existing codebase: Start small : Pick a subset of your codebase to run mypy on, without any annotations. You\u2019ll probably need to fix some mypy errors, either by inserting annotations requested by mypy or by adding # type: ignore comments to silence errors you don\u2019t want to fix now. Get a clean mypy build for some files, with some annotations. * Write a mypy runner script to ensure consistent results. Here are some steps you may want to do in the script: * Ensure that you install the correct version of mypy. * Specify mypy config file or command-line options. * Provide set of files to type check. You may want to configure the inclusion and exclusion filters for full control of the file list. * Run mypy in Continuous Integration to prevent type errors : Once you have a clean mypy run and a runner script for a part of your codebase, set up your Continuous Integration (CI) system to run mypy to ensure that developers won\u2019t introduce bad annotations. A small CI script could look something like this: python3 - m pip install mypy == 0.600 # Pinned version avoids surprises scripts / mypy # Runs with the correct options * Gradually annotate commonly imported modules: Most projects have some widely imported modules, such as utilities or model classes. It\u2019s a good idea to annotate these soon, since this allows code using these modules to be type checked more effectively. Since mypy supports gradual typing, it\u2019s okay to leave some of these modules unannotated. The more you annotate, the more useful mypy will be, but even a little annotation coverage is useful. * Write annotations as you change existing code and write new code: Now you are ready to include type annotations in your development workflows. Consider adding something like these in your code style conventions: Developers should add annotations for any new code. It\u2019s also encouraged to write annotations when you change existing code. If you need to ignore a linter error and a type error use first the type and then the linter. For example, # type: ignore # noqa: W0212 . Reveal the type of an expression \u2691 You can use reveal_type(expr) to ask mypy to display the inferred static type of an expression. This can be useful when you don't quite understand how mypy handles a particular piece of code. Example: reveal_type (( 1 , 'hello' )) # Revealed type is 'Tuple[builtins.int, builtins.str]' You can also use reveal_locals() at any line in a file to see the types of all local variables at once. Example: a = 1 b = 'one' reveal_locals () # Revealed local types are: # a: builtins.int # b: builtins.str reveal_type and reveal_locals are only understood by mypy and don't exist in Python. If you try to run your program, you\u2019ll have to remove any reveal_type and reveal_locals calls before you can run your code. Both are always available and you don't need to import them. Solve cyclic imports due to typing \u2691 You can use a conditional import that is only active in \"type hinting mode\", but doesn't interfere at run time. The typing.TYPE_CHECKING constant makes this easily possible. For example: # thing.py from typing import TYPE_CHECKING if TYPE_CHECKING : from connection import ApiConnection class Thing : def __init__ ( self , connection : 'ApiConnection' ): self . _conn = connection The code will now execute properly as there is no circular import issue anymore. Type hinting tools on the other hand should still be able to resolve the ApiConnection type hint in Thing.__init__ . Make your library compatible with mypy \u2691 PEP 561 notes three main ways to distribute type information. The first is a package that has only inline type annotations in the code itself. The second is a package that ships stub files with type information alongside the runtime code. The third method, also known as a \u201cstub only package\u201d is a package that ships type information for a package separately as stub files. If you would like to publish a library package to a package repository (e.g. PyPI) for either internal or external use in type checking, packages that supply type information via type comments or annotations in the code should put a py.typed file in their package directory. For example, with a directory structure as follows setup.py package_a/ __init__.py lib.py py.typed the setup.py might look like: from distutils.core import setup setup ( name = \"SuperPackageA\" , author = \"Me\" , version = \"0.1\" , package_data = { \"package_a\" : [ \"py.typed\" ]}, packages = [ \"package_a\" ] ) If you use setuptools, you must pass the option zip_safe=False to setup() , or mypy will not be able to find the installed package. Reference \u2691 Bernat gabor article on the state of type hints in python Real python article on type checking","title":"Type Hints"},{"location":"coding/python/type_hints/#advantages-and-disadvantages","text":"Advantages: Help catch certain errors if used with a static type checker. Help check your code . It's not trivial to use docstrings to do automatic checks. Help to reason about code: Knowing the parameters type makes it a lot easier to understand and maintain a code base. It can speed up the time required to catch up with a code snippet. Always remember that you read code a lot more often than you write it, so you should optimize for ease of reading. Help you build and maintain a cleaner architecture . The act of writing type hints force you to think about the types in your program. Helps refactoring: Type hints make it trivial to find where a given class is used when you're trying to refactor your code base. Improve IDEs and linters. Cons: Type hints take developer time and effort to add . Even though it probably pays off in spending less time debugging, you will spend more time entering code. Introduce a slight penalty in start-up time . If you need to use the typing module, the import time may be significant, even more in short scripts. Work best in modern Pythons. Follow these guidelines when deciding if you want to add types to your project: In libraries that will be used by others, they add a lot of value. In complex projects, type hints help you understand how types flow through your code and are highly recommended. If you are beginning to learn Python, don't use them yet. If you are writing throw-away scripts, don't use them. So, Use Type hints whenever unit tests are worth writing .","title":"Advantages and disadvantages"},{"location":"coding/python/type_hints/#usage","text":"","title":"Usage"},{"location":"coding/python/type_hints/#function-annotations","text":"def func ( arg : arg_type , optarg : arg_type = default ) -> return_type : ... For arguments the syntax is argument: annotation , while the return type is annotated using -> annotation . Note that the annotation must be a valid Python expression. When running the code, the special .__annotations__ attribute on the function stores the typing information.","title":"Function annotations"},{"location":"coding/python/type_hints/#variable-annotations","text":"Sometimes the type checker needs help in figuring out the types of variables as well. The syntax is similar: pi : float = 3.142 def circumference ( radius : float ) -> float : return 2 * pi * radius","title":"Variable annotations"},{"location":"coding/python/type_hints/#composite-types","text":"If you need to hint other types than str , float and bool , you'll need to import the typing module. For example to define the hint types of list, dictionaries and tuples: >>> from typing import Dict , List , Tuple >>> names : List [ str ] = [ \"Guido\" , \"Jukka\" , \"Ivan\" ] >>> version : Tuple [ int , int , int ] = ( 3 , 7 , 1 ) >>> options : Dict [ str , bool ] = { \"centered\" : False , \"capitalize\" : True } If your function expects some kind of sequence but don't care whether it's a list or a tuple, use the typing.Sequence object.","title":"Composite types"},{"location":"coding/python/type_hints/#dictionaries-with-different-value-types-per-key","text":"TypedDict declares a dictionary type that expects all of its instances to have a certain set of keys, where each key is associated with a value of a consistent type. This expectation is not checked at runtime but is only enforced by type checkers. TypedDict started life as an experimental Mypy feature to wrangle typing onto the heterogeneous, structure-oriented use of dictionaries. As of Python 3.8, it was adopted into the standard library. try : from typing import TypedDict # >=3.8 except ImportError : from mypy_extensions import TypedDict # <=3.7 Movie = TypedDict ( 'Movie' , { 'name' : str , 'year' : int }) A class-based type constructor is also available: class Movie ( TypedDict ): name : str year : int By default, all keys must be present in a TypedDict . It is possible to override this by specifying totality. Usage: class point2D ( TypedDict , total = False ): x : int y : int This means that a point2D TypedDict can have any of the keys omitted. A type checker is only expected to support a literal False or True as the value of the total argument. True is the default, and makes all items defined in the class body be required.","title":"Dictionaries with different value types per key."},{"location":"coding/python/type_hints/#functions-without-return-values","text":"Some functions aren't meant to return anything. Use the -> None hint in these cases. def play ( player_name : str ) -> None : print ( f \" { player_name } plays\" ) ret_val = play ( \"Filip\" ) The annotation help catch the kinds of subtle bugs where you are trying to use a meaningless return value. If your function doesn't return any object, use the NoReturn type. from typing import NoReturn def black_hole () -> NoReturn : raise Exception ( \"There is no going back ...\" ) Note This is the first iteration of the synoptical reading of the full Real python article on type checking .","title":"Functions without return values"},{"location":"coding/python/type_hints/#optional-arguments","text":"A common pattern is to use None as a default value for an argument. This is done either to avoid problems with mutable default values or to have a sentinel value flagging special behavior. This creates a challenge for type hinting as the argument may be of type string (for example) but it can also be None . We use the Optional type to address this case. from typing import Optional def player ( name : str , start : Optional [ str ] = None ) -> str : ... A similar way would be to use Union[None, str] .","title":"Optional arguments"},{"location":"coding/python/type_hints/#type-aliases","text":"Type hints might become oblique when working with nested types. If it's the case, save them into a new variable, and use that instead. from typing import List , Tuple Card = Tuple [ str , str ] Deck = List [ Card ] def deal_hands ( deck : Deck ) -> Tuple [ Deck , Deck , Deck , Deck ]: \"\"\"Deal the cards in the deck into four hands\"\"\" return ( deck [ 0 :: 4 ], deck [ 1 :: 4 ], deck [ 2 :: 4 ], deck [ 3 :: 4 ])","title":"Type aliases"},{"location":"coding/python/type_hints/#allow-any-subclass","text":"Every class is also a valid type. Any instance of a subclass is also compatible with all superclasses \u2013 it follows that every value is compatible with the object type (and incidentally also the Any type, discussed below). Mypy analyzes the bodies of classes to determine which methods and attributes are available in instances. For example class A : def f ( self ) -> int : # Type of self inferred (A) return 2 class B ( A ): def f ( self ) -> int : return 3 def g ( self ) -> int : return 4 def foo ( a : A ) -> None : print ( a . f ()) # 3 a . g () # Error: \"A\" has no attribute \"g\" foo ( B ()) # OK (B is a subclass of A)","title":"Allow any subclass"},{"location":"coding/python/type_hints/#deduce-returned-value-type-from-the-arguments","text":"The previous approach works if you don't need to use class objects that inherit from a given class. For example: class User : # Defines fields like name, email class BasicUser ( User ): def upgrade ( self ): \"\"\"Upgrade to Pro\"\"\" class ProUser ( User ): def pay ( self ): \"\"\"Pay bill\"\"\" def new_user ( user_class ) -> User : user = user_class () # (Here we could write the user object to a database) return user Where: ProUser doesn\u2019t inherit from BasicUser . new_user creates an instance of one of these classes if you pass it the right class object. The problem is that right now mypy doesn't know which subclass of User you're giving it, and will only accept the methods and attributes defined in the parent class User . buyer = new_user ( ProUser ) buyer . pay () # Rejected, not a method on User This can be solved using Type variables with upper bounds . UserType = TypeVar ( 'UserType' , bound = User ) def new_user ( user_class : Type [ UserType ]) -> UserType : # Same implementation as before We're creating a new type UserType that is linked to the class or subclasses of User . That way, mypy knows that the return value is an object created from the class given in the argument user_class . beginner = new_user ( BasicUser ) # Inferred type is BasicUser beginner . upgrade () # OK Keep in mind that the TypeVar is a Generic type , as such, they take one or more type parameters, similar to built-in types such as List[X] . That means that when you create type aliases, you'll need to give the type parameter. So: UserType = TypeVar ( \"UserType\" , bound = User ) UserTypes = List [ Type [ UserType ]] def new_users ( user_class : UserTypes ) -> UserType : # Type error! pass Will give a Missing type parameters for generic type \"UserTypes\" error. To solve it use: def new_users ( user_class : UserTypes [ UserType ]) -> UserType : # OK! pass","title":"Deduce returned value type from the arguments"},{"location":"coding/python/type_hints/#specify-the-type-of-the-class-in-its-method-and-attributes","text":"If you are using Python 3.10 or later, it just works. Python 3.7 introduces PEP 563: postponed evaluation of annotations. A module that uses the future statement from __future__ import annotations to store annotations as strings automatically: from __future__ import annotations class Position : def __add__ ( self , other : Position ) -> Position : ... But pyflakes will still complain, so I've used strings. from __future__ import annotations class Position : def __add__ ( self , other : 'Position' ) -> 'Position' : ...","title":"Specify the type of the class in it's method and attributes"},{"location":"coding/python/type_hints/#type-hints-of-generators","text":"from typing import Generator def generate () -> Generator [ int , None , None ]: Where the first argument of Generator is the type of the yielded value.","title":"Type hints of Generators"},{"location":"coding/python/type_hints/#using-mypy-with-an-existing-codebase","text":"These steps will get you started with mypy on an existing codebase: Start small : Pick a subset of your codebase to run mypy on, without any annotations. You\u2019ll probably need to fix some mypy errors, either by inserting annotations requested by mypy or by adding # type: ignore comments to silence errors you don\u2019t want to fix now. Get a clean mypy build for some files, with some annotations. * Write a mypy runner script to ensure consistent results. Here are some steps you may want to do in the script: * Ensure that you install the correct version of mypy. * Specify mypy config file or command-line options. * Provide set of files to type check. You may want to configure the inclusion and exclusion filters for full control of the file list. * Run mypy in Continuous Integration to prevent type errors : Once you have a clean mypy run and a runner script for a part of your codebase, set up your Continuous Integration (CI) system to run mypy to ensure that developers won\u2019t introduce bad annotations. A small CI script could look something like this: python3 - m pip install mypy == 0.600 # Pinned version avoids surprises scripts / mypy # Runs with the correct options * Gradually annotate commonly imported modules: Most projects have some widely imported modules, such as utilities or model classes. It\u2019s a good idea to annotate these soon, since this allows code using these modules to be type checked more effectively. Since mypy supports gradual typing, it\u2019s okay to leave some of these modules unannotated. The more you annotate, the more useful mypy will be, but even a little annotation coverage is useful. * Write annotations as you change existing code and write new code: Now you are ready to include type annotations in your development workflows. Consider adding something like these in your code style conventions: Developers should add annotations for any new code. It\u2019s also encouraged to write annotations when you change existing code. If you need to ignore a linter error and a type error use first the type and then the linter. For example, # type: ignore # noqa: W0212 .","title":"Using mypy with an existing codebase"},{"location":"coding/python/type_hints/#reveal-the-type-of-an-expression","text":"You can use reveal_type(expr) to ask mypy to display the inferred static type of an expression. This can be useful when you don't quite understand how mypy handles a particular piece of code. Example: reveal_type (( 1 , 'hello' )) # Revealed type is 'Tuple[builtins.int, builtins.str]' You can also use reveal_locals() at any line in a file to see the types of all local variables at once. Example: a = 1 b = 'one' reveal_locals () # Revealed local types are: # a: builtins.int # b: builtins.str reveal_type and reveal_locals are only understood by mypy and don't exist in Python. If you try to run your program, you\u2019ll have to remove any reveal_type and reveal_locals calls before you can run your code. Both are always available and you don't need to import them.","title":"Reveal the type of an expression"},{"location":"coding/python/type_hints/#solve-cyclic-imports-due-to-typing","text":"You can use a conditional import that is only active in \"type hinting mode\", but doesn't interfere at run time. The typing.TYPE_CHECKING constant makes this easily possible. For example: # thing.py from typing import TYPE_CHECKING if TYPE_CHECKING : from connection import ApiConnection class Thing : def __init__ ( self , connection : 'ApiConnection' ): self . _conn = connection The code will now execute properly as there is no circular import issue anymore. Type hinting tools on the other hand should still be able to resolve the ApiConnection type hint in Thing.__init__ .","title":"Solve cyclic imports due to typing"},{"location":"coding/python/type_hints/#make-your-library-compatible-with-mypy","text":"PEP 561 notes three main ways to distribute type information. The first is a package that has only inline type annotations in the code itself. The second is a package that ships stub files with type information alongside the runtime code. The third method, also known as a \u201cstub only package\u201d is a package that ships type information for a package separately as stub files. If you would like to publish a library package to a package repository (e.g. PyPI) for either internal or external use in type checking, packages that supply type information via type comments or annotations in the code should put a py.typed file in their package directory. For example, with a directory structure as follows setup.py package_a/ __init__.py lib.py py.typed the setup.py might look like: from distutils.core import setup setup ( name = \"SuperPackageA\" , author = \"Me\" , version = \"0.1\" , package_data = { \"package_a\" : [ \"py.typed\" ]}, packages = [ \"package_a\" ] ) If you use setuptools, you must pass the option zip_safe=False to setup() , or mypy will not be able to find the installed package.","title":"Make your library compatible with mypy"},{"location":"coding/python/type_hints/#reference","text":"Bernat gabor article on the state of type hints in python Real python article on type checking","title":"Reference"},{"location":"coding/python/yoyo/","text":"Yoyo is a database schema migration tool. Migrations are written as SQL files or Python scripts that define a list of migration steps. Installation \u2691 pip install yoyo-migrations Usage \u2691 Command line \u2691 Start a new migration: yoyo new ./migrations -m \"Add column to foo\" Apply migrations from directory migrations to a PostgreSQL database: yoyo apply --database postgresql://scott:tiger@localhost/db ./migrations Rollback migrations previously applied to a MySQL database: yoyo rollback --database mysql://scott:tiger@localhost/database ./migrations Reapply (ie rollback then apply again) migrations to a SQLite database at location /home/sheila/important.db: yoyo reapply --database sqlite:////home/sheila/important.db ./migrations List available migrations: yoyo list --database sqlite:////home/sheila/important.db ./migrations By default, yoyo-migrations starts in an interactive mode, prompting you for each migration file before applying it, making it easy to preview which migrations to apply and rollback. Connecting to the database \u2691 Database connections are specified using a URL. Examples: # SQLite: use 4 slashes for an absolute database path on unix like platforms database = sqlite : //// home / user / mydb . sqlite # SQLite: use 3 slashes for a relative path database = sqlite : /// mydb . sqlite # SQLite: absolute path on Windows. database = sqlite : /// c : \\ home \\ user \\ mydb . sqlite # MySQL: Network database connection database = mysql : // scott : tiger @localhost / mydatabase # MySQL: unix socket connection database = mysql : // scott : tiger @/ mydatabase ? unix_socket =/ tmp / mysql . sock # MySQL with the MySQLdb driver (instead of pymysql) database = mysql + mysqldb : // scott : tiger @localhost / mydatabase # MySQL with SSL/TLS enabled database = mysql + mysqldb : // scott : tiger @localhost / mydatabase ? ssl = yes & sslca =/ path / to / cert # PostgreSQL: database connection database = postgresql : // scott : tiger @localhost / mydatabase # PostgreSQL: unix socket connection database = postgresql : // scott : tiger @/ mydatabase # PostgreSQL: changing the schema (via set search_path) database = postgresql : // scott : tiger @/ mydatabase ? schema = some_schema You can specify your database username and password either as part of the database connection string on the command line (exposing your database password in the process list) or in a configuration file where other users may be able to read it. The -p or --prompt-password flag causes yoyo to prompt for a password, helping prevent your credentials from being leaked. Migration files \u2691 The migrations directory contains a series of migration scripts. Each migration script is a Python (.py) or SQL file (.sql). The name of each file without the extension is used as the migration\u2019s unique identifier. Migrations scripts are run in dependency then filename order. Each migration file is run in a single transaction where this is supported by the database. Yoyo creates tables in your target database to track which migrations have been applied. Migrations as Python scripts \u2691 A migration script written in Python has the following structure: # # file: migrations/0001_create_foo.py # from yoyo import step __depends__ = { \"0000.initial-schema\" } steps = [ step ( \"CREATE TABLE foo (id INT, bar VARCHAR(20), PRIMARY KEY (id))\" , \"DROP TABLE foo\" , ), step ( \"ALTER TABLE foo ADD COLUMN baz INT NOT NULL\" ) ] The step function may take up to 3 arguments: apply : an SQL query (or Python function, see below) to apply the migration step. rollback : (optional) an SQL query (or Python function) to rollback the migration step. ignore_errors : (optional, one of apply , rollback or all ) causes yoyo to ignore database errors in either the apply stage, rollback stage or both. Migrations may declare dependencies on other migrations via the __depends__ attribute: If you use the yoyo new command the __depends__ attribute will be auto populated for you. Migration steps as Python functions \u2691 If SQL is not flexible enough, you may supply a Python function as either or both of the apply or rollback arguments of step. Each function should take a database connection as its only argument: # # file: migrations/0001_create_foo.py # from yoyo import step def apply_step ( conn ): cursor = conn . cursor () cursor . execute ( # query to perform the migration ) def rollback_step ( conn ): cursor = conn . cursor () cursor . execute ( # query to undo the above ) steps = [ step ( apply_step , rollback_step ) ] Post-apply hook \u2691 It can be useful to have a script that is run after every successful migration. For example you could use this to update database permissions or re-create views. To do this, create a special migration file called post-apply.py . Configuration file \u2691 Yoyo looks for a configuration file named yoyo.ini in the current working directory or any ancestor directory. The configuration file may contain the following options: [DEFAULT] # List of migration source directories. \"%(here)s\" is expanded to the # full path of the directory containing this ini file. sources = %(here)s/migrations %(here)s/lib/module/migrations # Target database database = postgresql://scott:tiger@localhost/mydb # Verbosity level. Goes from 0 (least verbose) to 3 (most verbose) verbosity = 3 # Disable interactive features batch_mode = on # Editor to use when starting new migrations # \"{}\" is expanded to the filename of the new migration editor = /usr/local/bin/vim -f {} # An arbitrary command to run after a migration has been created # \"{}\" is expanded to the filename of the new migration post_create_command = hg add {} # A prefix to use for generated migration filenames prefix = myproject_ Calling Yoyo from Python code \u2691 The following example shows how to apply migrations from inside python code: from yoyo import read_migrations from yoyo import get_backend backend = get_backend ( 'postgres://myuser@localhost/mydatabase' ) migrations = read_migrations ( 'path/to/migrations' ) with backend . lock (): # Apply any outstanding migrations backend . apply_migrations ( backend . to_apply ( migrations )) # Rollback all migrations backend . rollback_migrations ( backend . to_rollback ( migrations )) References \u2691 Docs Source Issue Tracker Mailing list","title":"Yoyo"},{"location":"coding/python/yoyo/#installation","text":"pip install yoyo-migrations","title":"Installation"},{"location":"coding/python/yoyo/#usage","text":"","title":"Usage"},{"location":"coding/python/yoyo/#command-line","text":"Start a new migration: yoyo new ./migrations -m \"Add column to foo\" Apply migrations from directory migrations to a PostgreSQL database: yoyo apply --database postgresql://scott:tiger@localhost/db ./migrations Rollback migrations previously applied to a MySQL database: yoyo rollback --database mysql://scott:tiger@localhost/database ./migrations Reapply (ie rollback then apply again) migrations to a SQLite database at location /home/sheila/important.db: yoyo reapply --database sqlite:////home/sheila/important.db ./migrations List available migrations: yoyo list --database sqlite:////home/sheila/important.db ./migrations By default, yoyo-migrations starts in an interactive mode, prompting you for each migration file before applying it, making it easy to preview which migrations to apply and rollback.","title":"Command line"},{"location":"coding/python/yoyo/#connecting-to-the-database","text":"Database connections are specified using a URL. Examples: # SQLite: use 4 slashes for an absolute database path on unix like platforms database = sqlite : //// home / user / mydb . sqlite # SQLite: use 3 slashes for a relative path database = sqlite : /// mydb . sqlite # SQLite: absolute path on Windows. database = sqlite : /// c : \\ home \\ user \\ mydb . sqlite # MySQL: Network database connection database = mysql : // scott : tiger @localhost / mydatabase # MySQL: unix socket connection database = mysql : // scott : tiger @/ mydatabase ? unix_socket =/ tmp / mysql . sock # MySQL with the MySQLdb driver (instead of pymysql) database = mysql + mysqldb : // scott : tiger @localhost / mydatabase # MySQL with SSL/TLS enabled database = mysql + mysqldb : // scott : tiger @localhost / mydatabase ? ssl = yes & sslca =/ path / to / cert # PostgreSQL: database connection database = postgresql : // scott : tiger @localhost / mydatabase # PostgreSQL: unix socket connection database = postgresql : // scott : tiger @/ mydatabase # PostgreSQL: changing the schema (via set search_path) database = postgresql : // scott : tiger @/ mydatabase ? schema = some_schema You can specify your database username and password either as part of the database connection string on the command line (exposing your database password in the process list) or in a configuration file where other users may be able to read it. The -p or --prompt-password flag causes yoyo to prompt for a password, helping prevent your credentials from being leaked.","title":"Connecting to the database"},{"location":"coding/python/yoyo/#migration-files","text":"The migrations directory contains a series of migration scripts. Each migration script is a Python (.py) or SQL file (.sql). The name of each file without the extension is used as the migration\u2019s unique identifier. Migrations scripts are run in dependency then filename order. Each migration file is run in a single transaction where this is supported by the database. Yoyo creates tables in your target database to track which migrations have been applied.","title":"Migration files"},{"location":"coding/python/yoyo/#migrations-as-python-scripts","text":"A migration script written in Python has the following structure: # # file: migrations/0001_create_foo.py # from yoyo import step __depends__ = { \"0000.initial-schema\" } steps = [ step ( \"CREATE TABLE foo (id INT, bar VARCHAR(20), PRIMARY KEY (id))\" , \"DROP TABLE foo\" , ), step ( \"ALTER TABLE foo ADD COLUMN baz INT NOT NULL\" ) ] The step function may take up to 3 arguments: apply : an SQL query (or Python function, see below) to apply the migration step. rollback : (optional) an SQL query (or Python function) to rollback the migration step. ignore_errors : (optional, one of apply , rollback or all ) causes yoyo to ignore database errors in either the apply stage, rollback stage or both. Migrations may declare dependencies on other migrations via the __depends__ attribute: If you use the yoyo new command the __depends__ attribute will be auto populated for you.","title":"Migrations as Python scripts"},{"location":"coding/python/yoyo/#migration-steps-as-python-functions","text":"If SQL is not flexible enough, you may supply a Python function as either or both of the apply or rollback arguments of step. Each function should take a database connection as its only argument: # # file: migrations/0001_create_foo.py # from yoyo import step def apply_step ( conn ): cursor = conn . cursor () cursor . execute ( # query to perform the migration ) def rollback_step ( conn ): cursor = conn . cursor () cursor . execute ( # query to undo the above ) steps = [ step ( apply_step , rollback_step ) ]","title":"Migration steps as Python functions"},{"location":"coding/python/yoyo/#post-apply-hook","text":"It can be useful to have a script that is run after every successful migration. For example you could use this to update database permissions or re-create views. To do this, create a special migration file called post-apply.py .","title":"Post-apply hook"},{"location":"coding/python/yoyo/#configuration-file","text":"Yoyo looks for a configuration file named yoyo.ini in the current working directory or any ancestor directory. The configuration file may contain the following options: [DEFAULT] # List of migration source directories. \"%(here)s\" is expanded to the # full path of the directory containing this ini file. sources = %(here)s/migrations %(here)s/lib/module/migrations # Target database database = postgresql://scott:tiger@localhost/mydb # Verbosity level. Goes from 0 (least verbose) to 3 (most verbose) verbosity = 3 # Disable interactive features batch_mode = on # Editor to use when starting new migrations # \"{}\" is expanded to the filename of the new migration editor = /usr/local/bin/vim -f {} # An arbitrary command to run after a migration has been created # \"{}\" is expanded to the filename of the new migration post_create_command = hg add {} # A prefix to use for generated migration filenames prefix = myproject_","title":"Configuration file"},{"location":"coding/python/yoyo/#calling-yoyo-from-python-code","text":"The following example shows how to apply migrations from inside python code: from yoyo import read_migrations from yoyo import get_backend backend = get_backend ( 'postgres://myuser@localhost/mydatabase' ) migrations = read_migrations ( 'path/to/migrations' ) with backend . lock (): # Apply any outstanding migrations backend . apply_migrations ( backend . to_apply ( migrations )) # Rollback all migrations backend . rollback_migrations ( backend . to_rollback ( migrations ))","title":"Calling Yoyo from Python code"},{"location":"coding/python/yoyo/#references","text":"Docs Source Issue Tracker Mailing list","title":"References"},{"location":"coding/python/python_project_template/python_cli_template/","text":"Create the tests directories mkdir -p tests/unit touch tests/__init__.py touch tests/unit/__init__.py Create the program module structure mkdir {{ program_name }} Create the program setup.py file from setuptools import find_packages , setup # Get the version from drode/version.py without importing the package exec ( compile ( open ( '{{ program_name }}/version.py' ) . read (), '{{ program_name }}/version.py' , 'exec' )) setup ( name = '{{ program_name }}' , version = __version__ , # noqa: F821 description = '{{ program_description }}' , author = '{{ author }}' , author_email = '{{ author_email }}' , license = 'GPLv3' , long_description = open ( 'README.md' ) . read (), packages = find_packages ( exclude = ( 'tests' ,)), package_data = { '{{ program_name }}' : [ 'migrations/*' , 'migrations/versions/*' , ]}, entry_points = { 'console_scripts' : [ '{{ program_name }} = {{ program_name }}:main' ]}, install_requires = [ ] ) Remember to fill up the install_requirements with the dependencies that need to be installed at installation time. Create the {{ program_name }}/version.py file with the following contents: __version__ = '1.0.0' This ugly way of loading the __version__ was stolen from youtube-dl, it loads and executes the version.py without loading the whole module. Solutions like from {{ program_name }}.version import __version__ will fail as it tries to load the whole module. Defining it in the setup.py file doesn't work either if you need to load the version in your program code. from setup.py import __version__ will also fail. The only problem with this approach is that as the __version__ is not defined in the code it will raise a Flake8 error, therefore the # noqa: F821 in the setup.py code. Create the requirements.txt file. It should contain the install_requirements in addition to the testing requirements such as: pytest pytest-cov Configure SQLAlchemy for projects without flask","title":"Command-line Project Template"},{"location":"coding/python/python_project_template/python_docker/","text":"Docker is a popular way to distribute applications. Assuming that you've set all required dependencies in the setup.py , we're going to create an image with these properties: Run by an unprivileged user : Create an unprivileged user with permissions to run our program. Robust to vulnerabilities: Don't use Alpine as it's known to react slow to new vulnerabilities. Use a base of Debian instead. Smallest possible: Use Docker multi build step. Create a builder Docker that will run pip install and copies the required executables to the final image. FROM python:3.8-slim-buster as base FROM base as builder RUN python -m venv /opt/venv # Make sure we use the virtualenv: ENV PATH = \"/opt/venv/bin: $PATH \" COPY . /app WORKDIR /app RUN pip install . FROM base COPY --from = builder /opt/venv /opt/venv RUN useradd -m myapp WORKDIR /home/myapp # Copy the required directories for your program to work. COPY --from = builder /root/.local/share/myapp /home/myapp/.local/share/myapp COPY --from = builder /app/myapp /home/myapp/myapp RUN chown -R myapp:myapp /home/myapp/.local USER myapp ENV PATH = \"/opt/venv/bin: $PATH \" ENTRYPOINT [ \"/opt/venv/bin/myapp\" ] If we need to use it with MariaDB or with Redis, the easiest way is to use docker-compose . version : '3.8' services : myapp : image : myapp:latest restart : always links : - db depends_on : - db environment : - AIRSS_DATABASE_URL=mysql+pymysql://myapp:supersecurepassword@db/myapp db : image : mariadb:latest restart : always environment : - MYSQL_USER=myapp - MYSQL_PASSWORD=supersecurepassword - MYSQL_DATABASE=myapp - MYSQL_ALLOW_EMPTY_PASSWORD=yes ports : - 3306:3306 command : - '--character-set-server=utf8mb4' - '--collation-server=utf8mb4_unicode_ci' volumes : - /data/myapp/mariadb:/var/lib/mysql The depends_on flag is not enough to ensure that the database is up when our application tries to connect. So we need to use external programs like wait-for-it . To use it, change the earlier Dockerfile to match these lines: ... FROM base RUN apt-get update && apt-get install -y \\ wait-for-it \\ && rm -rf /var/lib/apt/lists/* ... ENTRYPOINT [ \"/home/myapp/entrypoint.sh\" ] Where entrypoint.sh is something like: #!/bin/bash # Wait for the database to be up if [[ -n $DATABASE_URL ]] ; then wait-for-it db:3306 fi # Execute database migrations /opt/venv/bin/myapp install # Enter in daemon mode /opt/venv/bin/myapp daemon Remember to add the permissions to run the script: chmod +x entrypoint.sh","title":"Configure Docker to host the application"},{"location":"coding/python/python_project_template/python_docs/","text":"It's important to create the documentation at the same time as you code your project, otherwise you won't ever do it or you'll die trying. Right now I use mkdocs with Github Pages for the documentation. Follow the steps under Installation to configure it. I've automated the creation of the mkdocs site in this cookiecutter template .","title":"Create the documentation repository"},{"location":"coding/python/python_project_template/python_flask_template/","text":"Flask is very flexible when it comes to define the project layout, as a result, there are several different approaches, which can be confusing if you're building your first application. Follow this template if you want an application that meets these requirements: Use SQLAlchemy as ORM. Use pytest as testing framework (instead of unittest). Sets a robust foundation for application growth. Set a clear defined project structure that can be used for frontend applications as well as backend APIs. Microservice friendly. I've crafted this template after studying the following projects: Miguel's Flask mega tutorial ( code ). Greb Obinna Flask-RESTPlus tutorial ( code ). Abhinav Suri Flask tutorial ( code ). Patrick's software blog project layout and pytest definition ( code ). Jaime Buelta Hands On Docker for Microservices with Python ( code ). Each has it's strengths and weaknesses: Project Alembic Pytest Complex app Friendly layout Strong points Miguel True False True False Has a book explaining the code Greb False False False False flask-restplus Abhinav True False True True flask-base Patrick False True False True pytest Jaime False True True False Microservices, CI, Kubernetes, logging, metrics I'm going to start with Abhinav base layout as it's the most clear and complete. Furthermore, it's based in flask-base , a simple Flask boilerplate app with SQLAlchemy, Redis, User Authentication, and more. Which can be used directly to start a frontend flask project. I won't use it for a backend API though. With that base layout, I'm going to take Patrick's pytest layout to configure the tests using pytest-flask , Greb flask-restplus code to create the API and Miguel's book to glue everything together. Finally, I'll follow Jaime's book to merge the different microservices into an integrated project. As well as defining the deployment process, CI definition, logging, metrics and integration with Kubernetes.","title":"Flask Project Template"},{"location":"coding/python/python_project_template/python_microservices_template/","text":"Follow this template if you want to build a project that meets these requirements: Based on Python Flask microservices. Easily expandable. Tested by unit, functional and integration tests through continuous integration pipelines. Deployed through uWSGI and Nginx dockers. Orchestrated through docker-compose or Kubernetes. Defining the project layout of a flask application is not easy , even less one with several","title":"Microservices Project Template"},{"location":"coding/python/python_project_template/python_sqlalchemy_mariadb/","text":"I discourage you to use an ORM to manage the interactions with the database. Check the alternative solutions . To use Mysql you'll need to first install (or add to your requirements) pymysql : pip install pymysql The url to connect to the database will be: 'mysql+pymysql:// {} : {} @ {} : {} / {} ' . format ( DB_USER , DB_PASS , DB_HOST , DB_PORT , DATABASE ) It's probable that you'll need to use UTF8 with multi byte , otherwise the addition of some strings into the database will fail. I've tried adding it to the database url without success. So I've modified the MariaDB Docker-compose section to use that character and collation set: services : db : image : mariadb:latest restart : always environment : - MYSQL_USER=xxxx - MYSQL_PASSWORD=xxxx - MYSQL_DATABASE=xxxx - MYSQL_ALLOW_EMPTY_PASSWORD=yes ports : - 3306:3306 command : - '--character-set-server=utf8mb4' - '--collation-server=utf8mb4_unicode_ci'","title":"Configure SQLAlchemy to use the MariaDB/Mysql backend"},{"location":"coding/python/python_project_template/python_sqlalchemy_without_flask/","text":"I discourage you to use an ORM to manage the interactions with the database. Check the alternative solutions . Install Alembic : pip install alembic It's important that the migration scripts are saved with the rest of the source code. Following Miguel Gringberg suggestion , we'll store them in the {{ program_name }}/migrations directory. Execute the following command to initialize the alembic repository. alembic init {{ program_name }} /migrations Create the basic models.py file under the project code. \"\"\" Module to store the models. Classes: Class_name: Class description. ... \"\"\" import os from sqlalchemy import \\ create_engine , \\ Column , \\ Integer from sqlalchemy.ext.declarative import declarative_base db_path = os . path . expanduser ( '{{ path_to_sqlite_file }}' ) engine = create_engine ( os . environ . get ( '{{ program_name }}_DATABASE_URL' ) or 'sqlite:///' + db_path ) Base = declarative_base ( bind = engine ) class User ( Base ): \"\"\" Class to define the User model. \"\"\" __tablename__ = 'user' id = Column ( Integer , primary_key = True , doc = 'User ID' ) Create the migrations/env.py file as specified in the alembic article . Create the first alembic revision. alembic \\ -c {{ program_name }} /migrations/alembic.ini \\ revision \\ --autogenerate \\ -m \"Initial schema\" Set up the testing environment for SQLAlchemy","title":"Configure SQLAlchemy for projects without flask"},{"location":"coding/react/react/","text":"React is a declarative, efficient, and flexible JavaScript library for building user interfaces. It lets you compose complex UIs from small and isolated pieces of code called \u201ccomponents\u201d. Set up a new project \u2691 Install Node.js Create the project baseline with Create React App . Using this tool avoids: Learning and configuring many build tools. Optimize your bundles. Worry about the incompatibility of versions between the underlying pieces. So you can focus on the development of your code. npx create-react-app my-app Delete all files in the src/ folder of the new project. cd my-app rm src/* Create the basic files index.css , index.js in the src directory. Run the server: npm start . Start a React + Flask project \u2691 Create the api directory. mkdir api Make the virtualenv. mkvirtualenv \\ --python = python3 \\ -a ~/projects/my-app \\ my-app Install flask. pip install flask python-dotenv Add a basic file to api/api.py . import time from flask import Flask app = Flask ( __name__ ) @app . route ( '/api/time' ) def get_current_time (): return { 'time' : time . time ()} Create the .flaskenv file. FLASK_APP = api/api.py FLASK_ENV = development Make sure everything is alright. flask run The basics \u2691 Components tell React what to show on the screen. When the data changes, React will efficiently update and re-render the components. class ShoppingList extends React . Component { render () { return ( < div className = \"shopping-list\" > < h1 > Shopping List for { this . props . name } < /h1> < ul > < li > Instagram < /li> < li > WhatsApp < /li> < li > Oculus < /li> < /ul> < /div> ); } } // Example usage: <ShoppingList name=\"Mark\" /> ShoppingList is a React component class , or React component type . A component takes in parameters, called props (short for \u201cproperties\u201d), and returns a hierarchy of views to display via the render method. The render method returns a description of what you want to see on the screen. React takes the description and displays the result. In particular, render returns a React element , which is a lightweight description of what to render. Most React developers use a special syntax called \u201cJSX\u201d which makes these structures easier to write. The syntax is transformed at build time to React.createElement('div'). The example above is equivalent to: return React . createElement ( 'div' , { className : 'shopping-list' }, React . createElement ( 'h1' , /* ... h1 children ... */ ), React . createElement ( 'ul' , /* ... ul children ... */ ) ); The ShoppingList component above only renders built-in DOM components like <div /> and <li /> . But it can compose and render custom React components too. For example, Use <ShoppingList /> to refer to the whole shopping list. Each React component is encapsulated and can operate independently; this allows the building of complex UIs from simple components. Pass data between components \u2691 Data is passed between components through the props method. class Square extends React . Component { render () { return ( < button className = \"square\" > { this . props . value } < /button> ); } } class Board extends React . Component { renderSquare ( i ) { return < Square value = { i } /> ; } ... } Use of the state \u2691 React components can have state by setting this.state in their constructors. this.state should be considered as private to a React component that it\u2019s defined in. Components need a constructor to initialize the state. In JavaScript classes, it's required to call super when defining the constructor of a subclass. All React component classes that have a constructor should start with a super(props) call. class Square extends React . Component { constructor ( props ){ super ( props ); this . state = { value : null , } } render () { ... } } Then use the this.setState method to set the value ... render () { return ( < button className = \"square\" onClick = {() => this . setState ({ value : 'X' })} > { this . state . value } < /button> ); } Share the state between parent and children \u2691 To collect data from multiple children, or to have two child components communicate with each other, you need to declare the shared state in their parent component instead. The parent component can pass the state back down to the children by using props; this keeps the child components in sync with each other and with the parent component. First define the parent state class Square extends React . Component { render () { return ( < button className = \"square\" onClick = {() => this . props . onClick ()} > { this . props . value } < /button> ); } } class Board extends React . Component { constructor ( props ) { super ( props ); this . state = { squares : Array ( 9 ). fill ( null ), }; } handleClick ( i ) { const squares = this . state . squares . slice (); squares [ i ] = 'X' ; this . setState ({ squares : squares }); } renderSquare ( i ) { return < Square value = { this . state . squares [ i ]} onClick = {() => this . handleClick ( i )} /> ; } ... } Since state is considered to be private to a component that defines it, it's not possible to update the parent\u2019s state directly from the children. Instead, A function needs to be passed down from the parent to the children, so the children can call that function when required. For example the onClick={() => this.handleClick(i)} in the example above. When a Square is clicked, the onClick function provided by the Board is called. Here\u2019s a review of how this is achieved: The onClick prop on the built-in DOM <button> component tells React to set up a click event listener. When the button is clicked, React will call the onClick event handler that is defined in Square \u2019s render() method. This event handler calls this.props.onClick() . The Square \u2019s onClick prop was specified by the Board . Since the Board passed onClick={() => this.handleClick(i)} to Square , the Square calls this.handleClick(i) when clicked. So now the state is stored in Board instead of the individual Square components. When the Board \u2019s state changes, the Square components re-render automatically. In React terms, the Square components are now controlled components. Handling data change \u2691 There are generally two approaches to changing data. The first one is to mutate the data by directly changing the it\u2019s values. The second is to replace the data with a new copy which has the desired changes. Data Change with Mutation. var player = { score : 1 , name : 'Jeff' }; player . score = 2 ; // Now player is {score: 2, name: 'Jeff'} Data Change without Mutation. var player = { score : 1 , name : 'Jeff' }; var newPlayer = Object . assign ({}, player , { score : 2 }); // Now player is unchanged, but newPlayer is {score: 2, name: 'Jeff'} // Or if you are using object spread syntax proposal, you can write: // var newPlayer = {...player, score: 2}; By not mutating directly, several benefits are gained: Complex features become simple: Immutability makes complex features much easier to implement. Detecting Changes: Detecting changes in mutable objects is difficult because they are modified directly. This detection requires the mutable object to be compared to previous copies of itself and the entire object tree to be traversed. Detecting changes in immutable objects is considerably easier. If the immutable object that is being referenced is different than the previous one, then the object has changed. Determining When to Re-Render in React: The main benefit of immutability is that it helps you build pure components in React. Immutable data can easily determine if changes have been made which helps to determine when a component requires re-rendering. Function components \u2691 Function components are a simpler way to write components that only contain a render method and don\u2019t have their own state. Instead of defining a class which extends React.Component , we can write a function that takes props as input and returns what should be rendered. Function components are less tedious to write than classes, and many components can be expressed this way. Instead of class Square extends React . Component { render () { return ( < button className = \"square\" onClick = {() => this . props . onClick ()} > { this . props . value } < /button> ); } } Use function Square ( props ) { return ( < button ClassName = \"square\" onClick = { props . onClick } > { props . value } < /button> ); } Miscellaneous \u2691 List rendering \u2691 When React renders a list, it stores some information about each rendered list item. Once a list is updated, React needs to determine what has changed. A key property needs to be specified for each list item to differentiate each list item from its siblings. So for: < li > Ben : 9 tasks left < /li> < li > Claudia : 8 tasks left < /li> < li > Alexa : 5 tasks left < /li> < li key = { user . id } > { user . name } : { user . taskCount } tasks left < /li> Could be used. When a list is re-rendered, React takes each list item\u2019s key and searches the previous list\u2019s items for a matching key. If the current list has a key that didn\u2019t exist before, React creates a component. If the current list is missing a key that existed in the previous list, React destroys the previous component. If two keys match, the corresponding component is moved. Keys tell React about the identity of each compone. Keys tell React about the identity of each component which allows React to maintain state between re-renders. If a component\u2019s key changes, the component will be destroyed and re-created with a new state. key is a special and reserved property in React. When an element is created, React extracts the key property and stores the key directly on the returned element. Even though key may look like it belongs in props , key cannot be referenced using this.props.key . React automatically uses key to decide which components to update. A component cannot inquire about its key . Links \u2691 React tutorial Awesome React Awesome React components Responsive React \u2691 Responsive react Responsive websites without css react-responsive library With Flask \u2691 How to create a react + flask project How to deploy a react + flask project","title":"React"},{"location":"coding/react/react/#set-up-a-new-project","text":"Install Node.js Create the project baseline with Create React App . Using this tool avoids: Learning and configuring many build tools. Optimize your bundles. Worry about the incompatibility of versions between the underlying pieces. So you can focus on the development of your code. npx create-react-app my-app Delete all files in the src/ folder of the new project. cd my-app rm src/* Create the basic files index.css , index.js in the src directory. Run the server: npm start .","title":"Set up a new project"},{"location":"coding/react/react/#start-a-react-flask-project","text":"Create the api directory. mkdir api Make the virtualenv. mkvirtualenv \\ --python = python3 \\ -a ~/projects/my-app \\ my-app Install flask. pip install flask python-dotenv Add a basic file to api/api.py . import time from flask import Flask app = Flask ( __name__ ) @app . route ( '/api/time' ) def get_current_time (): return { 'time' : time . time ()} Create the .flaskenv file. FLASK_APP = api/api.py FLASK_ENV = development Make sure everything is alright. flask run","title":"Start a React + Flask project"},{"location":"coding/react/react/#the-basics","text":"Components tell React what to show on the screen. When the data changes, React will efficiently update and re-render the components. class ShoppingList extends React . Component { render () { return ( < div className = \"shopping-list\" > < h1 > Shopping List for { this . props . name } < /h1> < ul > < li > Instagram < /li> < li > WhatsApp < /li> < li > Oculus < /li> < /ul> < /div> ); } } // Example usage: <ShoppingList name=\"Mark\" /> ShoppingList is a React component class , or React component type . A component takes in parameters, called props (short for \u201cproperties\u201d), and returns a hierarchy of views to display via the render method. The render method returns a description of what you want to see on the screen. React takes the description and displays the result. In particular, render returns a React element , which is a lightweight description of what to render. Most React developers use a special syntax called \u201cJSX\u201d which makes these structures easier to write. The syntax is transformed at build time to React.createElement('div'). The example above is equivalent to: return React . createElement ( 'div' , { className : 'shopping-list' }, React . createElement ( 'h1' , /* ... h1 children ... */ ), React . createElement ( 'ul' , /* ... ul children ... */ ) ); The ShoppingList component above only renders built-in DOM components like <div /> and <li /> . But it can compose and render custom React components too. For example, Use <ShoppingList /> to refer to the whole shopping list. Each React component is encapsulated and can operate independently; this allows the building of complex UIs from simple components.","title":"The basics"},{"location":"coding/react/react/#pass-data-between-components","text":"Data is passed between components through the props method. class Square extends React . Component { render () { return ( < button className = \"square\" > { this . props . value } < /button> ); } } class Board extends React . Component { renderSquare ( i ) { return < Square value = { i } /> ; } ... }","title":"Pass data between components"},{"location":"coding/react/react/#use-of-the-state","text":"React components can have state by setting this.state in their constructors. this.state should be considered as private to a React component that it\u2019s defined in. Components need a constructor to initialize the state. In JavaScript classes, it's required to call super when defining the constructor of a subclass. All React component classes that have a constructor should start with a super(props) call. class Square extends React . Component { constructor ( props ){ super ( props ); this . state = { value : null , } } render () { ... } } Then use the this.setState method to set the value ... render () { return ( < button className = \"square\" onClick = {() => this . setState ({ value : 'X' })} > { this . state . value } < /button> ); }","title":"Use of the state"},{"location":"coding/react/react/#share-the-state-between-parent-and-children","text":"To collect data from multiple children, or to have two child components communicate with each other, you need to declare the shared state in their parent component instead. The parent component can pass the state back down to the children by using props; this keeps the child components in sync with each other and with the parent component. First define the parent state class Square extends React . Component { render () { return ( < button className = \"square\" onClick = {() => this . props . onClick ()} > { this . props . value } < /button> ); } } class Board extends React . Component { constructor ( props ) { super ( props ); this . state = { squares : Array ( 9 ). fill ( null ), }; } handleClick ( i ) { const squares = this . state . squares . slice (); squares [ i ] = 'X' ; this . setState ({ squares : squares }); } renderSquare ( i ) { return < Square value = { this . state . squares [ i ]} onClick = {() => this . handleClick ( i )} /> ; } ... } Since state is considered to be private to a component that defines it, it's not possible to update the parent\u2019s state directly from the children. Instead, A function needs to be passed down from the parent to the children, so the children can call that function when required. For example the onClick={() => this.handleClick(i)} in the example above. When a Square is clicked, the onClick function provided by the Board is called. Here\u2019s a review of how this is achieved: The onClick prop on the built-in DOM <button> component tells React to set up a click event listener. When the button is clicked, React will call the onClick event handler that is defined in Square \u2019s render() method. This event handler calls this.props.onClick() . The Square \u2019s onClick prop was specified by the Board . Since the Board passed onClick={() => this.handleClick(i)} to Square , the Square calls this.handleClick(i) when clicked. So now the state is stored in Board instead of the individual Square components. When the Board \u2019s state changes, the Square components re-render automatically. In React terms, the Square components are now controlled components.","title":"Share the state between parent and children"},{"location":"coding/react/react/#handling-data-change","text":"There are generally two approaches to changing data. The first one is to mutate the data by directly changing the it\u2019s values. The second is to replace the data with a new copy which has the desired changes. Data Change with Mutation. var player = { score : 1 , name : 'Jeff' }; player . score = 2 ; // Now player is {score: 2, name: 'Jeff'} Data Change without Mutation. var player = { score : 1 , name : 'Jeff' }; var newPlayer = Object . assign ({}, player , { score : 2 }); // Now player is unchanged, but newPlayer is {score: 2, name: 'Jeff'} // Or if you are using object spread syntax proposal, you can write: // var newPlayer = {...player, score: 2}; By not mutating directly, several benefits are gained: Complex features become simple: Immutability makes complex features much easier to implement. Detecting Changes: Detecting changes in mutable objects is difficult because they are modified directly. This detection requires the mutable object to be compared to previous copies of itself and the entire object tree to be traversed. Detecting changes in immutable objects is considerably easier. If the immutable object that is being referenced is different than the previous one, then the object has changed. Determining When to Re-Render in React: The main benefit of immutability is that it helps you build pure components in React. Immutable data can easily determine if changes have been made which helps to determine when a component requires re-rendering.","title":"Handling data change"},{"location":"coding/react/react/#function-components","text":"Function components are a simpler way to write components that only contain a render method and don\u2019t have their own state. Instead of defining a class which extends React.Component , we can write a function that takes props as input and returns what should be rendered. Function components are less tedious to write than classes, and many components can be expressed this way. Instead of class Square extends React . Component { render () { return ( < button className = \"square\" onClick = {() => this . props . onClick ()} > { this . props . value } < /button> ); } } Use function Square ( props ) { return ( < button ClassName = \"square\" onClick = { props . onClick } > { props . value } < /button> ); }","title":"Function components"},{"location":"coding/react/react/#miscellaneous","text":"","title":"Miscellaneous"},{"location":"coding/react/react/#list-rendering","text":"When React renders a list, it stores some information about each rendered list item. Once a list is updated, React needs to determine what has changed. A key property needs to be specified for each list item to differentiate each list item from its siblings. So for: < li > Ben : 9 tasks left < /li> < li > Claudia : 8 tasks left < /li> < li > Alexa : 5 tasks left < /li> < li key = { user . id } > { user . name } : { user . taskCount } tasks left < /li> Could be used. When a list is re-rendered, React takes each list item\u2019s key and searches the previous list\u2019s items for a matching key. If the current list has a key that didn\u2019t exist before, React creates a component. If the current list is missing a key that existed in the previous list, React destroys the previous component. If two keys match, the corresponding component is moved. Keys tell React about the identity of each compone. Keys tell React about the identity of each component which allows React to maintain state between re-renders. If a component\u2019s key changes, the component will be destroyed and re-created with a new state. key is a special and reserved property in React. When an element is created, React extracts the key property and stores the key directly on the returned element. Even though key may look like it belongs in props , key cannot be referenced using this.props.key . React automatically uses key to decide which components to update. A component cannot inquire about its key .","title":"List rendering"},{"location":"coding/react/react/#links","text":"React tutorial Awesome React Awesome React components","title":"Links"},{"location":"coding/react/react/#responsive-react","text":"Responsive react Responsive websites without css react-responsive library","title":"Responsive React"},{"location":"coding/react/react/#with-flask","text":"How to create a react + flask project How to deploy a react + flask project","title":"With Flask"},{"location":"coding/sql/sql/","text":"SQL Data Types \u2691 String data types: VARCHAR(size) : A variable length string (can contain letters, numbers, and special characters). The size parameter specifies the maximum column length in characters - can be from 0 to 65535. TEXT(size) : Holds a string with a maximum length of 65,535 bytes. MEDIUMTEXT : Holds a string with a maximum length of 16,777,215 characters. LONGTEXT : Holds a string with a maximum length of 4,294,967,295 characters. ENUM(val1, val2, val3, ...) : A string object that can have only one value, chosen from a list of possible values. You can list up to 65535 values in an ENUM list. If a value is inserted that is not in the list, a blank value will be inserted. The values are sorted in the order you enter them. SET(val1, val2, val3, ...) : A string object that can have 0 or more values, chosen from a list of possible values. You can list up to 64 values in a SET list. Numeric data types: BOOL or BOOLEAN : Zero is considered as false, nonzero values are considered as true. TINYINT(size) : A very small integer. Signed range is from -128 to 127. Unsigned range is from 0 to 255. The size parameter specifies the maximum display width (which is 255). SMALLINT(size) : A small integer. Signed range is from -32768 to 32767. Unsigned range is from 0 to 65535. The size parameter specifies the maximum display width (which is 255). INT(size) : A medium integer. Signed range is from -2147483648 to 2147483647. Unsigned range is from 0 to 4294967295. The size parameter specifies the maximum display width (which is 255). FLOAT(p) : A floating point number. MySQL uses the p value to determine whether to use FLOAT or DOUBLE for the resulting data type. If p is from 0 to 24, the data type becomes FLOAT() . If p is from 25 to 53, the data type becomes DOUBLE() . Date and time data types: DATE : A date. Format: YYYY-MM-DD. The supported range is from '1000-01-01' to '9999-12-31'. DATETIME(fsp) : A date and time combination. Format: YYYY-MM-DD hh:mm:ss . The supported range is from 1000-01-01 00:00:00 to 9999-12-31 23:59:59 . Adding DEFAULT and ON UPDATE in the column definition to get automatic initialization and updating to the current date and time. Table relationships \u2691 One to One \u2691 A one-to-one relationship between two entities exists when a particular entity instance exists in one table, and it can have only one associated entity instance in another table. For example, a user can have only one address, and an address belongs to only one user. This sort of relationship is implemented by setting the PRIMARY KEY of the users table ( id ) as both the FOREIGN KEY and PRIMARY KEY of the addresses table. CREATE TABLE addresses ( user_id int , -- Both a primary and foreign key street varchar ( 30 ) NOT NULL , city varchar ( 30 ) NOT NULL , state varchar ( 30 ) NOT NULL , PRIMARY KEY ( user_id ), FOREIGN KEY ( user_id ) REFERENCES users ( id ) ON DELETE CASCADE ); The ON DELETE CASCADE clause of the FOREIGN_KEY definition instructs the database to delete the referencing row if the referenced row is deleted. There are alternatives to CASCADE such as SET NULL or SET DEFAULT which instead of deleting the referencing row will set a new value in the appropriate column for that row. One to many \u2691 A one-to-many relationship exists between two entities if an entity instance in one of the tables can be associated with multiple records (entity instances) in the other table. The opposite relationship does not exist; that is, each entity instance in the second table can only be associated with one entity instance in the first table. For example, a review belongs to only one book while a book has many reviews. CREATE TABLE books ( id serial , title varchar ( 100 ) NOT NULL , author varchar ( 100 ) NOT NULL , published_date timestamp NOT NULL , isbn char ( 12 ), PRIMARY KEY ( id ), UNIQUE ( isbn ) ); /* one to many: Book has many reviews */ CREATE TABLE reviews ( id serial , book_id integer NOT NULL , reviewer_name varchar ( 255 ), content varchar ( 255 ), rating integer , published_date timestamp DEFAULT CURRENT_TIMESTAMP , PRIMARY KEY ( id ), FOREIGN KEY ( book_id ) REFERENCES books ( id ) ON DELETE CASCADE ); Unlike our addresses table, the PRIMARY KEY and FOREIGN KEY reference different columns, id and book_id respectively. This means that the FOREIGN KEY column, book_id is not bound by the UNIQUE constraint of our PRIMARY KEY and so the same value from the id column of the books table can appear in this column more than once. In other words a book can have many reviews. Many to many \u2691 A many-to-many relationship exists between two entities if for one entity instance there may be multiple records in the other table, and vice versa. For example, a user can check out many books, and a book can be checked out by many users. In order to implement this sort of relationship we need to introduce a third, cross-reference, table. This table holds the relationship between the two entities, by having two FOREIGN KEY s, each of which references the PRIMARY KEY of one of the tables for which we want to create this relationship. We already have our books and users tables, so we just need to create the cross-reference table: checkouts . CREATE TABLE checkouts ( id serial , user_id int NOT NULL , book_id int NOT NULL , checkout_date timestamp , return_date timestamp , PRIMARY KEY ( id ), FOREIGN KEY ( user_id ) REFERENCES users ( id ) ON DELETE CASCADE , FOREIGN KEY ( book_id ) REFERENCES books ( id ) ON DELETE CASCADE ); Joins \u2691 A JOIN clause is used to combine rows from two or more tables, based on a related column between them. One to one join \u2691 SELECT users . id , addresses . street FROM users LEFT JOIN addresses ON users . id = addresses . user_id It will return one line. One to many join \u2691 SELECT books . id , reviews . rating FROM books LEFT JOIN reviews ON books . id = reviews . book_id It will return many lines. [Many to many \u2691 join]( https://lornajane.net/posts/2011/inner-vs-outer-joins-on-a-many-to-many-relationship ) SELECT users . id , books . id FROM users LEFT OUTER JOIN checkouts ON users . id == checkouts . user_id Left OUTER JOIN books ON checkouts . book_id == books . id","title":"SQL"},{"location":"coding/sql/sql/#sql-data-types","text":"String data types: VARCHAR(size) : A variable length string (can contain letters, numbers, and special characters). The size parameter specifies the maximum column length in characters - can be from 0 to 65535. TEXT(size) : Holds a string with a maximum length of 65,535 bytes. MEDIUMTEXT : Holds a string with a maximum length of 16,777,215 characters. LONGTEXT : Holds a string with a maximum length of 4,294,967,295 characters. ENUM(val1, val2, val3, ...) : A string object that can have only one value, chosen from a list of possible values. You can list up to 65535 values in an ENUM list. If a value is inserted that is not in the list, a blank value will be inserted. The values are sorted in the order you enter them. SET(val1, val2, val3, ...) : A string object that can have 0 or more values, chosen from a list of possible values. You can list up to 64 values in a SET list. Numeric data types: BOOL or BOOLEAN : Zero is considered as false, nonzero values are considered as true. TINYINT(size) : A very small integer. Signed range is from -128 to 127. Unsigned range is from 0 to 255. The size parameter specifies the maximum display width (which is 255). SMALLINT(size) : A small integer. Signed range is from -32768 to 32767. Unsigned range is from 0 to 65535. The size parameter specifies the maximum display width (which is 255). INT(size) : A medium integer. Signed range is from -2147483648 to 2147483647. Unsigned range is from 0 to 4294967295. The size parameter specifies the maximum display width (which is 255). FLOAT(p) : A floating point number. MySQL uses the p value to determine whether to use FLOAT or DOUBLE for the resulting data type. If p is from 0 to 24, the data type becomes FLOAT() . If p is from 25 to 53, the data type becomes DOUBLE() . Date and time data types: DATE : A date. Format: YYYY-MM-DD. The supported range is from '1000-01-01' to '9999-12-31'. DATETIME(fsp) : A date and time combination. Format: YYYY-MM-DD hh:mm:ss . The supported range is from 1000-01-01 00:00:00 to 9999-12-31 23:59:59 . Adding DEFAULT and ON UPDATE in the column definition to get automatic initialization and updating to the current date and time.","title":"SQL Data Types"},{"location":"coding/sql/sql/#table-relationships","text":"","title":"Table relationships"},{"location":"coding/sql/sql/#one-to-one","text":"A one-to-one relationship between two entities exists when a particular entity instance exists in one table, and it can have only one associated entity instance in another table. For example, a user can have only one address, and an address belongs to only one user. This sort of relationship is implemented by setting the PRIMARY KEY of the users table ( id ) as both the FOREIGN KEY and PRIMARY KEY of the addresses table. CREATE TABLE addresses ( user_id int , -- Both a primary and foreign key street varchar ( 30 ) NOT NULL , city varchar ( 30 ) NOT NULL , state varchar ( 30 ) NOT NULL , PRIMARY KEY ( user_id ), FOREIGN KEY ( user_id ) REFERENCES users ( id ) ON DELETE CASCADE ); The ON DELETE CASCADE clause of the FOREIGN_KEY definition instructs the database to delete the referencing row if the referenced row is deleted. There are alternatives to CASCADE such as SET NULL or SET DEFAULT which instead of deleting the referencing row will set a new value in the appropriate column for that row.","title":"One to One"},{"location":"coding/sql/sql/#one-to-many","text":"A one-to-many relationship exists between two entities if an entity instance in one of the tables can be associated with multiple records (entity instances) in the other table. The opposite relationship does not exist; that is, each entity instance in the second table can only be associated with one entity instance in the first table. For example, a review belongs to only one book while a book has many reviews. CREATE TABLE books ( id serial , title varchar ( 100 ) NOT NULL , author varchar ( 100 ) NOT NULL , published_date timestamp NOT NULL , isbn char ( 12 ), PRIMARY KEY ( id ), UNIQUE ( isbn ) ); /* one to many: Book has many reviews */ CREATE TABLE reviews ( id serial , book_id integer NOT NULL , reviewer_name varchar ( 255 ), content varchar ( 255 ), rating integer , published_date timestamp DEFAULT CURRENT_TIMESTAMP , PRIMARY KEY ( id ), FOREIGN KEY ( book_id ) REFERENCES books ( id ) ON DELETE CASCADE ); Unlike our addresses table, the PRIMARY KEY and FOREIGN KEY reference different columns, id and book_id respectively. This means that the FOREIGN KEY column, book_id is not bound by the UNIQUE constraint of our PRIMARY KEY and so the same value from the id column of the books table can appear in this column more than once. In other words a book can have many reviews.","title":"One to many"},{"location":"coding/sql/sql/#many-to-many","text":"A many-to-many relationship exists between two entities if for one entity instance there may be multiple records in the other table, and vice versa. For example, a user can check out many books, and a book can be checked out by many users. In order to implement this sort of relationship we need to introduce a third, cross-reference, table. This table holds the relationship between the two entities, by having two FOREIGN KEY s, each of which references the PRIMARY KEY of one of the tables for which we want to create this relationship. We already have our books and users tables, so we just need to create the cross-reference table: checkouts . CREATE TABLE checkouts ( id serial , user_id int NOT NULL , book_id int NOT NULL , checkout_date timestamp , return_date timestamp , PRIMARY KEY ( id ), FOREIGN KEY ( user_id ) REFERENCES users ( id ) ON DELETE CASCADE , FOREIGN KEY ( book_id ) REFERENCES books ( id ) ON DELETE CASCADE );","title":"Many to many"},{"location":"coding/sql/sql/#joins","text":"A JOIN clause is used to combine rows from two or more tables, based on a related column between them.","title":"Joins"},{"location":"coding/sql/sql/#one-to-one-join","text":"SELECT users . id , addresses . street FROM users LEFT JOIN addresses ON users . id = addresses . user_id It will return one line.","title":"One to one join"},{"location":"coding/sql/sql/#one-to-many-join","text":"SELECT books . id , reviews . rating FROM books LEFT JOIN reviews ON books . id = reviews . book_id It will return many lines.","title":"One to many join"},{"location":"coding/sql/sql/#many-to-many_1","text":"join]( https://lornajane.net/posts/2011/inner-vs-outer-joins-on-a-many-to-many-relationship ) SELECT users . id , books . id FROM users LEFT OUTER JOIN checkouts ON users . id == checkouts . user_id Left OUTER JOIN books ON checkouts . book_id == books . id","title":"[Many to many"},{"location":"coding/yaml/yaml/","text":"YAML (a recursive acronym for \"YAML Ain't Markup Language\") is a human-readable data-serialization language. It is commonly used for configuration files and in applications where data is being stored or transmitted. YAML targets many of the same communications applications as Extensible Markup Language (XML) but has a minimal syntax which intentionally differs from SGML. It uses both Python-style indentation to indicate nesting, and a more compact format that uses [...] for lists and {...} for maps making YAML 1.2 a superset of JSON. Break long lines \u2691 Use > most of the time: interior line breaks are stripped out, although you get one at the end: key : > Your long string here. Use | if you want those line breaks to be preserved as \\n (for instance, embedded markdown with paragraphs): key : | ### Heading * Bullet * Points Use >- or |- instead if you don't want a line break appended at the end. Use \"...\" if you need to split lines in the middle of words or want to literally type line breaks as \\n : key : \"Antidisestab\\ lishmentarianism.\\n\\nGet on it.\" YAML is crazy.","title":"YAML"},{"location":"coding/yaml/yaml/#break-long-lines","text":"Use > most of the time: interior line breaks are stripped out, although you get one at the end: key : > Your long string here. Use | if you want those line breaks to be preserved as \\n (for instance, embedded markdown with paragraphs): key : | ### Heading * Bullet * Points Use >- or |- instead if you don't want a line break appended at the end. Use \"...\" if you need to split lines in the middle of words or want to literally type line breaks as \\n : key : \"Antidisestab\\ lishmentarianism.\\n\\nGet on it.\" YAML is crazy.","title":"Break long lines"},{"location":"dancing/cutting_shapes_basics/","text":"Basic crossing \u2691 8-and : Left legs straight under our body, with the weight in the toes and the foot a little turned with the heel pointing to our right (7 o'clock), right leg lifted up straight to our 3 o'clock with toes pointing down. with knee up front. Weight lies on the left foot. 1 : Right foot crosses before the left one, toes touch the floor first in our longitudinal axes. The left foot pivots over the toes to end up pointing to our 11. Weight is evenly shared between the feet toes. 1-and : Mirrors 8-and . 2 : Mirrors 1 . 2-and : Equal to 8-and . Sources: 1 Wiggle feet \u2691 8-and : Both legs straight under our body, right leg lifted up with knee up front. Weight lies on the left foot. 1 : Right foot touches the floor behind the hips pointing to 10.5, left foot pivots over the toes to point to 1.5. Weight is even between feet. 1-and : Right foot pivots on the heel till 1.5. Left foot pivots on the toes till 10.5. 2 : Equal 1 . 2-and : Mirrors 1-and . 3 : Equal 1 . 3-and : Equal 1-and . 4 : Equal 1 . Sources: 1 References \u2691 Where to discover more tutorials \u2691 Elements tutorial playlist Anderson Jovani playlist","title":"Cutting Shapes"},{"location":"dancing/cutting_shapes_basics/#basic-crossing","text":"8-and : Left legs straight under our body, with the weight in the toes and the foot a little turned with the heel pointing to our right (7 o'clock), right leg lifted up straight to our 3 o'clock with toes pointing down. with knee up front. Weight lies on the left foot. 1 : Right foot crosses before the left one, toes touch the floor first in our longitudinal axes. The left foot pivots over the toes to end up pointing to our 11. Weight is evenly shared between the feet toes. 1-and : Mirrors 8-and . 2 : Mirrors 1 . 2-and : Equal to 8-and . Sources: 1","title":"Basic crossing"},{"location":"dancing/cutting_shapes_basics/#wiggle-feet","text":"8-and : Both legs straight under our body, right leg lifted up with knee up front. Weight lies on the left foot. 1 : Right foot touches the floor behind the hips pointing to 10.5, left foot pivots over the toes to point to 1.5. Weight is even between feet. 1-and : Right foot pivots on the heel till 1.5. Left foot pivots on the toes till 10.5. 2 : Equal 1 . 2-and : Mirrors 1-and . 3 : Equal 1 . 3-and : Equal 1-and . 4 : Equal 1 . Sources: 1","title":"Wiggle feet"},{"location":"dancing/cutting_shapes_basics/#references","text":"","title":"References"},{"location":"dancing/cutting_shapes_basics/#where-to-discover-more-tutorials","text":"Elements tutorial playlist Anderson Jovani playlist","title":"Where to discover more tutorials"},{"location":"dancing/rave_dances/","text":"Shuffle \u2691 Shuffle is a rave dance that developed in the 1980s. Typically performed to electronic music, the dance originated in the Melbourne rave scene, and was popular in the 1980s and 1990s. The dance moves involve a fast heel-and-toe movement or T-step, combined with a variation of the running man coupled with a matching arm action. It's an improvised dance and involves \"shuffling your feet inwards, then outwards, while thrusting your arms up and down, or side to side, in time with the beat\". It seems to be a minority underground dance without official competitions or schools. Given the music, it's origins and the energy it requires, most of the videos are uploaded by young people that reproduce the nasty gender roles that the society embeds in us. So expect to see over sexualized girls and over testosteroned guys, which is sad. Styles \u2691 Melbourne Shuffle \u2691 It's the original shuffle, which was the one that caught my eye several years back with Francis' video: It's \"simple and straightforward\", the base is the running man, with a few kicks and spins sprinkled in from time to time, so there is not much room for variety. It has a low skill floor and a moderate skill ceiling. Usually the dancers wear trousers with a wide end that makes a nice flying visual effect and a hoodie. The percent of read women Melbourne shuffle videos is low. Here is a list of other Melbourne shuffle videos: 1 , 2 , 3 , 4 The malasian and russian styles are, to my untrained eye, similar to the Melbourne, although they've got different T-step. Although in the source post they say that these styles are usually danced between 150 and 200 bpms, I've found that the videos range from 130 to 160 bpms with an average of 150 bpms. Cutting shapes \u2691 It's the most popular right now (at least for the number of videos I've found). The base steps are the Charleston or the heel variants of the running man, which looks less like running. The basic step is used less than in the other styles, filling most of the moves with heel-toe movements, criss-crossings, and spins, so there is little \"shuffling\" in cutting shapes. It has a low skill floor, but an absurdly high skill ceiling. There is a lot of room for variety and you can even mix it up by adding other dance styles to it, as it's very flexible. Usually the dancers wear street clothes and some use shoes with led lights. The percent of read women Cutting shapes shuffle videos is even with read males. Here is a list of other cutting shapes shuffle videos: 1 , 2 Cutting shapes songs are mostly around the 130 bpms. Jumpstyle \u2691 Jumpstyle is an electronic dance style and music genre popular in Western Europe, with existent scenes in Eastern Europe, Australia, and the Americas. It's music is an offspring of tech-trance, hardstyle, gabber and m\u00e1kina. Its tempo is usually between 140 and 150 BPM. Here are another Jumpstyle videos: 1 , 2 .","title":"Rave Dances"},{"location":"dancing/rave_dances/#shuffle","text":"Shuffle is a rave dance that developed in the 1980s. Typically performed to electronic music, the dance originated in the Melbourne rave scene, and was popular in the 1980s and 1990s. The dance moves involve a fast heel-and-toe movement or T-step, combined with a variation of the running man coupled with a matching arm action. It's an improvised dance and involves \"shuffling your feet inwards, then outwards, while thrusting your arms up and down, or side to side, in time with the beat\". It seems to be a minority underground dance without official competitions or schools. Given the music, it's origins and the energy it requires, most of the videos are uploaded by young people that reproduce the nasty gender roles that the society embeds in us. So expect to see over sexualized girls and over testosteroned guys, which is sad.","title":"Shuffle"},{"location":"dancing/rave_dances/#styles","text":"","title":"Styles"},{"location":"dancing/rave_dances/#melbourne-shuffle","text":"It's the original shuffle, which was the one that caught my eye several years back with Francis' video: It's \"simple and straightforward\", the base is the running man, with a few kicks and spins sprinkled in from time to time, so there is not much room for variety. It has a low skill floor and a moderate skill ceiling. Usually the dancers wear trousers with a wide end that makes a nice flying visual effect and a hoodie. The percent of read women Melbourne shuffle videos is low. Here is a list of other Melbourne shuffle videos: 1 , 2 , 3 , 4 The malasian and russian styles are, to my untrained eye, similar to the Melbourne, although they've got different T-step. Although in the source post they say that these styles are usually danced between 150 and 200 bpms, I've found that the videos range from 130 to 160 bpms with an average of 150 bpms.","title":"Melbourne Shuffle"},{"location":"dancing/rave_dances/#cutting-shapes","text":"It's the most popular right now (at least for the number of videos I've found). The base steps are the Charleston or the heel variants of the running man, which looks less like running. The basic step is used less than in the other styles, filling most of the moves with heel-toe movements, criss-crossings, and spins, so there is little \"shuffling\" in cutting shapes. It has a low skill floor, but an absurdly high skill ceiling. There is a lot of room for variety and you can even mix it up by adding other dance styles to it, as it's very flexible. Usually the dancers wear street clothes and some use shoes with led lights. The percent of read women Cutting shapes shuffle videos is even with read males. Here is a list of other cutting shapes shuffle videos: 1 , 2 Cutting shapes songs are mostly around the 130 bpms.","title":"Cutting shapes"},{"location":"dancing/rave_dances/#jumpstyle","text":"Jumpstyle is an electronic dance style and music genre popular in Western Europe, with existent scenes in Eastern Europe, Australia, and the Americas. It's music is an offspring of tech-trance, hardstyle, gabber and m\u00e1kina. Its tempo is usually between 140 and 150 BPM. Here are another Jumpstyle videos: 1 , 2 .","title":"Jumpstyle"},{"location":"dancing/shuffle_basics/","text":"Shuffle steps are usually split each tempo into two phases, songs usually follow a 4/4 structure. To differentiate the steps between tempos I'm going to use the tempo number they come from followed by an -and so the structure will be: 8-and , 1 , 1-and , 2 ... Where the 1 is the phrase starter. Most of the steps don't have names so I've put one that felt it fits. To specify the relative position of body parts, I'm using an imaginary clock, so 12 means upfront, 3 our right, 6 our back and 9 our left. The steps below can be danced with \"slow\" or quick tempos. It's best to learn with the slow ones as they build the muscle memory to be able to skip parts of it as you move to higher tempos. Dance routine \u2691 After several iterations I've come to this routine: Start dancing a bit to low tempo songs, I've got a playlist with increasing tempo songs, the aim is to start warming up and remember what I've learnt the last time. Once you're warmed up dance some songs to your current tempo level. Then put some songs a little above your tempo level and try to speed up your running man and T-Step . Improve this notes with the new discoveries. Analyze a video to extract some steps. Add one or two songs to the shuffle playlist. Prepend the song name with the bpms (it's better to measure them manually as automatic software tends to fail badly). Running man \u2691 The running man is the basic step of Melbourne shuffle, so make sure to master it. 8-and : Both legs straight under our body, right leg lifted up with knee up front. Weight lies on the left foot. 1 : Right foot kicks forward, left foot slides backward, in a way that each travel the same distance. Weight is evenly shared between the feet. 1-and : Right foot slides backwards, right foot goes up, knee to the front. Weight lies on the right foot. 2 : Mirrors 1 . 2-and : Mirrors 1-and and it's equal to 8-and . Once you feel comfortable with it, start going around the room doing running mans, so you get a grasp of doing it without being stationary. Sources: 1 , 2 The foot going forward can land in the floor flat, heel first or toes first. Quick tempo Running man \u2691 As the tempo speeds up, even if it seems there is no time to stop in the 1-and or 2-and they do manage to do so. To reach there, I'd put a song a little bit quicker than you are comfortable and: Get familiarity with the movement of the foot in the ground: Start with your basic running man stance left foot in front. When the 1 comes, try to reach the 1-and focusing to lift your right knee and stomp in the 2 . Rest a pair of beats and then try the other foot. Keep on doing it till you are comfortable doing this movement. Then try to do 2 complete running mans. Then dance :). For me the most difficult part is to bring back the non dominant leg backwards, to get used to the movement I slided backwards with that leg several times, then do three running mans starting with that leg, rest, and then do another three starting with the other one. T-Step \u2691 The T-Step is other basic step of Melbourne shuffle, used to do lateral movement. Learning the T-Step \u2691 The following instructions are a nice way to get the grasp of the movement, but we'll change them slightly to be able to link it with the other moves. 8-and : Both legs straight under our body, right leg lifted up with knee up front. Weight lies on the left foot, which looks at 1.5 although the rest of you look at 12. 1 : Right foot kicks sideways towards 3, try to leave the foot at 90 degrees with your leg and that it doesn't wobble, so it's a clean kick. The weight is on the left leg's toes and it pivotes over them till it points to 10.5. Waist ends up turned. 1-and : Right foot is retrieved till the left leg's knee, keeping the base parallel to the ground. Weight is on the left leg's heel and pivotes over it to reach the 1.5 position. Waist is straight now. 2 : Equal to 1 . 2-and : Equal to 1-and . 3 : Right foot goes front, left foot goes back to the running man stance. To move to the other side is the mirror. Keep the leg that holds you a bit bent so you don't harm your knee when twisting. The usual basic pattern of Melbourne shuffle is doing some running mans pointing at 10.5, then doing a side step (like the T-step) to the right, then another running man pointing to 1.5, finishing with another side step to the left. To join the T-Step with the running man, when you are in the 1-and make it the 8-and of the T-Step. Sources: 1 2 Francis T-Step \u2691 I've seen that Francis when finishes his T-Step with the other leg. So instead of moving the right leg in front and slide back the left (if we are going to the right), he: 1-and : Equal to normal T-Step. 2 : When you put your right foot down, shift your weight till it's evenly spread between feet. 2-and : Switch your weight to the right foot, lift the left while you keep on shifting your body weight to your 3. 3 : Left foot touches ground at your 1.5, slide the right back to the running man stance. What I like of this variation is that you conserve the energy, it makes total sense. What it doesn't is that he is dancing with sandals (\u00ac\u00ba-\u00b0)\u00ac . Try to move from side to side with 2 T-Steps in between using this variation. Quick tempo T-Step \u2691 To improve the speed of your T-Steps, I'd put a song a little bit quicker than you are comfortable and: Get familiarity with the movement of the foot in the ground: Start with your right knee up in position 8-and of the T-Step. First focus only to move the feet that you've got in the floor, so when the 1 comes, start pivoting the foot a pair of tempos. Put your right foot down and lift the left knee. Wait 3 or 4 tempos and when the 1 comes, start going to the other side. Once you are comfortable with the movement, repeat the same exercise but doing the kicks. Then instead of finishing with both feet under your body, finish in a running man stance. Then instead of starting with your knee up, start from the running man stance. Future steps \u2691 Sacco kicks at 14:50, 15:49, 17:08 Rocky kicks at 13:26 or 27:58. Rocky version at 1:19, 2:34, 7:29 Recce kicks at 15:56 (explanation) or 27:43 or 28:05. Reverse kick spin at 18:01 or 27:65. Rocky version at 1:18, 2:16, 2:55, 3:26, 3:59, 4:58, 6:24, 8:17, 8:35, 15:00, 15:55 Forward kick spin at 19:33. Rocky version at 2:20, 3:16, 3:47, 8:40, 8:49 Rocky side to side at 15:04, 16:08 Beats per minute progression \u2691 2020-09-07: First time to be able to dance a song at 118bpm to C2C - Happy . References \u2691 More videos to study \u2691 Rocky Shuffle compilation Francis Shuffle compilation Siera Shuffle compilation Bulldog Shuffle compilation","title":"Basics"},{"location":"dancing/shuffle_basics/#dance-routine","text":"After several iterations I've come to this routine: Start dancing a bit to low tempo songs, I've got a playlist with increasing tempo songs, the aim is to start warming up and remember what I've learnt the last time. Once you're warmed up dance some songs to your current tempo level. Then put some songs a little above your tempo level and try to speed up your running man and T-Step . Improve this notes with the new discoveries. Analyze a video to extract some steps. Add one or two songs to the shuffle playlist. Prepend the song name with the bpms (it's better to measure them manually as automatic software tends to fail badly).","title":"Dance routine"},{"location":"dancing/shuffle_basics/#running-man","text":"The running man is the basic step of Melbourne shuffle, so make sure to master it. 8-and : Both legs straight under our body, right leg lifted up with knee up front. Weight lies on the left foot. 1 : Right foot kicks forward, left foot slides backward, in a way that each travel the same distance. Weight is evenly shared between the feet. 1-and : Right foot slides backwards, right foot goes up, knee to the front. Weight lies on the right foot. 2 : Mirrors 1 . 2-and : Mirrors 1-and and it's equal to 8-and . Once you feel comfortable with it, start going around the room doing running mans, so you get a grasp of doing it without being stationary. Sources: 1 , 2 The foot going forward can land in the floor flat, heel first or toes first.","title":"Running man"},{"location":"dancing/shuffle_basics/#quick-tempo-running-man","text":"As the tempo speeds up, even if it seems there is no time to stop in the 1-and or 2-and they do manage to do so. To reach there, I'd put a song a little bit quicker than you are comfortable and: Get familiarity with the movement of the foot in the ground: Start with your basic running man stance left foot in front. When the 1 comes, try to reach the 1-and focusing to lift your right knee and stomp in the 2 . Rest a pair of beats and then try the other foot. Keep on doing it till you are comfortable doing this movement. Then try to do 2 complete running mans. Then dance :). For me the most difficult part is to bring back the non dominant leg backwards, to get used to the movement I slided backwards with that leg several times, then do three running mans starting with that leg, rest, and then do another three starting with the other one.","title":"Quick tempo Running man"},{"location":"dancing/shuffle_basics/#t-step","text":"The T-Step is other basic step of Melbourne shuffle, used to do lateral movement.","title":"T-Step"},{"location":"dancing/shuffle_basics/#learning-the-t-step","text":"The following instructions are a nice way to get the grasp of the movement, but we'll change them slightly to be able to link it with the other moves. 8-and : Both legs straight under our body, right leg lifted up with knee up front. Weight lies on the left foot, which looks at 1.5 although the rest of you look at 12. 1 : Right foot kicks sideways towards 3, try to leave the foot at 90 degrees with your leg and that it doesn't wobble, so it's a clean kick. The weight is on the left leg's toes and it pivotes over them till it points to 10.5. Waist ends up turned. 1-and : Right foot is retrieved till the left leg's knee, keeping the base parallel to the ground. Weight is on the left leg's heel and pivotes over it to reach the 1.5 position. Waist is straight now. 2 : Equal to 1 . 2-and : Equal to 1-and . 3 : Right foot goes front, left foot goes back to the running man stance. To move to the other side is the mirror. Keep the leg that holds you a bit bent so you don't harm your knee when twisting. The usual basic pattern of Melbourne shuffle is doing some running mans pointing at 10.5, then doing a side step (like the T-step) to the right, then another running man pointing to 1.5, finishing with another side step to the left. To join the T-Step with the running man, when you are in the 1-and make it the 8-and of the T-Step. Sources: 1 2","title":"Learning the T-Step"},{"location":"dancing/shuffle_basics/#francis-t-step","text":"I've seen that Francis when finishes his T-Step with the other leg. So instead of moving the right leg in front and slide back the left (if we are going to the right), he: 1-and : Equal to normal T-Step. 2 : When you put your right foot down, shift your weight till it's evenly spread between feet. 2-and : Switch your weight to the right foot, lift the left while you keep on shifting your body weight to your 3. 3 : Left foot touches ground at your 1.5, slide the right back to the running man stance. What I like of this variation is that you conserve the energy, it makes total sense. What it doesn't is that he is dancing with sandals (\u00ac\u00ba-\u00b0)\u00ac . Try to move from side to side with 2 T-Steps in between using this variation.","title":"Francis T-Step"},{"location":"dancing/shuffle_basics/#quick-tempo-t-step","text":"To improve the speed of your T-Steps, I'd put a song a little bit quicker than you are comfortable and: Get familiarity with the movement of the foot in the ground: Start with your right knee up in position 8-and of the T-Step. First focus only to move the feet that you've got in the floor, so when the 1 comes, start pivoting the foot a pair of tempos. Put your right foot down and lift the left knee. Wait 3 or 4 tempos and when the 1 comes, start going to the other side. Once you are comfortable with the movement, repeat the same exercise but doing the kicks. Then instead of finishing with both feet under your body, finish in a running man stance. Then instead of starting with your knee up, start from the running man stance.","title":"Quick tempo T-Step"},{"location":"dancing/shuffle_basics/#future-steps","text":"Sacco kicks at 14:50, 15:49, 17:08 Rocky kicks at 13:26 or 27:58. Rocky version at 1:19, 2:34, 7:29 Recce kicks at 15:56 (explanation) or 27:43 or 28:05. Reverse kick spin at 18:01 or 27:65. Rocky version at 1:18, 2:16, 2:55, 3:26, 3:59, 4:58, 6:24, 8:17, 8:35, 15:00, 15:55 Forward kick spin at 19:33. Rocky version at 2:20, 3:16, 3:47, 8:40, 8:49 Rocky side to side at 15:04, 16:08","title":"Future steps"},{"location":"dancing/shuffle_basics/#beats-per-minute-progression","text":"2020-09-07: First time to be able to dance a song at 118bpm to C2C - Happy .","title":"Beats per minute progression"},{"location":"dancing/shuffle_basics/#references","text":"","title":"References"},{"location":"dancing/shuffle_basics/#more-videos-to-study","text":"Rocky Shuffle compilation Francis Shuffle compilation Siera Shuffle compilation Bulldog Shuffle compilation","title":"More videos to study"},{"location":"dancing/shuffle_kicks/","text":"Kicking-Running man \u2691 8-and : Both legs straight under our body, right leg lifted up with knee up front. Weight lies on the left foot. 1 : Right foot kicks forward without touching the ground, it's important that the kick is clean, so swift till the end and stop suddenly. At the same time the left foot has slided back a little bit. 1-and : Right foot is retrieved, knee up and foot parallel to the ground, left foot keeps on sliding back. 2 : Right foot hits the ground, left foot keeps on sliding, weight is evenly shared between feet like the 1 of the running man. Sources: 1 Sacco kicks \u2691 Sacco kicks looks similar to the T-Step adding some side kicks. There are two variations, either kick first in and then out or the other way around. I'm more comfortable with the first and I think it gives an advantage when combining it with the running man. 8-and : Equal to the T-Step. 1 : The foot of the floor equal to the T-Step but the right foot kicks to your 10.5. 1-and : Equal to the T-Step. 2 : The foot of the floor equal to the T-Step but the right foot kicks to your 3. The advantage of the Sacco kicks is that as you are going to kick to your 10.5 in 1 , you can slide the left foot quite a lot to your back (from 8 to 1 ), therefore gliding further than with a simple T-Step (you only have half a tempo before you need to put all your weight in your left leg because you need to twist). Although the guy in the video slides the foot backwards when going to his left, try to do the twist of the T-Step. (The guy is crazy, shuffling barefoot over gravel soil (\u256f\u00b0\u25a1\u00b0)\u256f \u253b\u2501\u253b ). Source: 1","title":"Kicks"},{"location":"dancing/shuffle_kicks/#kicking-running-man","text":"8-and : Both legs straight under our body, right leg lifted up with knee up front. Weight lies on the left foot. 1 : Right foot kicks forward without touching the ground, it's important that the kick is clean, so swift till the end and stop suddenly. At the same time the left foot has slided back a little bit. 1-and : Right foot is retrieved, knee up and foot parallel to the ground, left foot keeps on sliding back. 2 : Right foot hits the ground, left foot keeps on sliding, weight is evenly shared between feet like the 1 of the running man. Sources: 1","title":"Kicking-Running man"},{"location":"dancing/shuffle_kicks/#sacco-kicks","text":"Sacco kicks looks similar to the T-Step adding some side kicks. There are two variations, either kick first in and then out or the other way around. I'm more comfortable with the first and I think it gives an advantage when combining it with the running man. 8-and : Equal to the T-Step. 1 : The foot of the floor equal to the T-Step but the right foot kicks to your 10.5. 1-and : Equal to the T-Step. 2 : The foot of the floor equal to the T-Step but the right foot kicks to your 3. The advantage of the Sacco kicks is that as you are going to kick to your 10.5 in 1 , you can slide the left foot quite a lot to your back (from 8 to 1 ), therefore gliding further than with a simple T-Step (you only have half a tempo before you need to put all your weight in your left leg because you need to twist). Although the guy in the video slides the foot backwards when going to his left, try to do the twist of the T-Step. (The guy is crazy, shuffling barefoot over gravel soil (\u256f\u00b0\u25a1\u00b0)\u256f \u253b\u2501\u253b ). Source: 1","title":"Sacco kicks"},{"location":"dancing/shuffle_spins/","text":"Tap spin \u2691 8 : Feet at the end of a running man, we generate a slight upper body twist clockwise to load the twist. 8-and : Left foot goes back to the body axes and the right foot goes to the body with knee up like a running man, with the difference that we release the twist energy and we start turning counterclockwise. Weight is on the left foot toes and it's knee is a little bit bent. 1 : Right foot kicks sideways towards 3 touching the ground but not putting any weight into it while we unbend the left knee taking the chance to keep the spin going. 1-and : Right foot goes back to the body and left knee is again a little bit bent waiting for the next kick. 2 : Equal to 1 2-and : Equal to 1-and 3 : Becomes the 1 of the running man. You can spin as long as you want, I've explained a two times spin, but if you want to go further repeat 1 and 1-and as long as you want. Sources: 1 Francis Spin \u2691 Awesome to start dancing. 8 : Both feet under your body, weight evenly spread between them. Give a small jump to start rotating clockwise (for example). 8-and : After 90 degrees of turn, your feet touch the ground, take the chance to give the big push to rotate the following 270. Bring your exterior arm to your shoulder level. 1 : Arrive to the running man stance with the left foot in front, but don't stop the rotation inertia. 1-and : Retrieve the left foot like any running man, but the foot points to your 5, right knee up. 2 : End your running man to your 5 with your right foot down. Add the final touch by lowering your external arm down. To get there : First practice the last part, the running man with a spin. The steps 1 to 2 described above until you are comfortable with it in both spinning directions. It's good to change the spin direction not to get dizzy. Do the full move until you are comfortable. Get it in time. Count in the song to start the movement in the 8 of the last time before a big drop. Source: 1","title":"Spins"},{"location":"dancing/shuffle_spins/#tap-spin","text":"8 : Feet at the end of a running man, we generate a slight upper body twist clockwise to load the twist. 8-and : Left foot goes back to the body axes and the right foot goes to the body with knee up like a running man, with the difference that we release the twist energy and we start turning counterclockwise. Weight is on the left foot toes and it's knee is a little bit bent. 1 : Right foot kicks sideways towards 3 touching the ground but not putting any weight into it while we unbend the left knee taking the chance to keep the spin going. 1-and : Right foot goes back to the body and left knee is again a little bit bent waiting for the next kick. 2 : Equal to 1 2-and : Equal to 1-and 3 : Becomes the 1 of the running man. You can spin as long as you want, I've explained a two times spin, but if you want to go further repeat 1 and 1-and as long as you want. Sources: 1","title":"Tap spin"},{"location":"dancing/shuffle_spins/#francis-spin","text":"Awesome to start dancing. 8 : Both feet under your body, weight evenly spread between them. Give a small jump to start rotating clockwise (for example). 8-and : After 90 degrees of turn, your feet touch the ground, take the chance to give the big push to rotate the following 270. Bring your exterior arm to your shoulder level. 1 : Arrive to the running man stance with the left foot in front, but don't stop the rotation inertia. 1-and : Retrieve the left foot like any running man, but the foot points to your 5, right knee up. 2 : End your running man to your 5 with your right foot down. Add the final touch by lowering your external arm down. To get there : First practice the last part, the running man with a spin. The steps 1 to 2 described above until you are comfortable with it in both spinning directions. It's good to change the spin direction not to get dizzy. Do the full move until you are comfortable. Get it in time. Count in the song to start the movement in the 8 of the last time before a big drop. Source: 1","title":"Francis Spin"},{"location":"data_analysis/recommender_systems/recommender_systems/","text":"A recommender system, or a recommendation system , is a subclass of information filtering system that seeks to predict the \"rating\" or \"preference\" a user would give to an item. The entity to which the recommendation is provided is referred to as the user, and the product being recommended is also referred to as an item. Therefore, recommendation analysis is often based on the previous interaction between users and items, because past interests and proclivities are often good indicators of future choices. These relations can be learned in a data-driven manner from the ratings matrix, and the resulting model is used to make predictions for target users.The larger the number of rated items that are available for a user, the easier it is to make robust predictions about the future behavior of the user. The problem may be formulated in two ways: Prediction version of problem: This approach aims to predict the rating value for a user-item combination. It is assumed that training data is available, indicating user preferences for items. For m users and n items, this corresponds to an incomplete m x n matrix, hwere the specified (or observed) values are used for training. The missing (or unobserved) values are predicted using this training model. This problem is also referred to as the matrix completion problem . Ranking version of problem: This approach aims to recommend the top- k items for a particular user, or determine the top- k users to target for a particular item. Being the first one more common. The problem is also referred to as the top-k recommendation problem . Goals of recommender systems \u2691 The common operational and technical goals of recommender systems are: Relevance : Recommend items that are relevant to the user at hand. Although it's the primary operational goal, it is not sufficient in isolation. Novelty : Recommend items that the user has not seen in the past. Serendipity : Recommend items that are outside the user's bubble, rather than simply something they did not know about before. For example, if a new Indian restaurant opens in a neighborhood, then the recommendation of that restaurant to a user who normally eats Indian food is novel but not necessarily serendipitous. On the other hand, when the same user is recommended Ethiopian food, and it was unknown to the user that such food might appeal to her, then the recommendation is serendipitous. Serendipity has the beneficial side effect of beginning new areas of interest. Increase recommendation diversity: Recommend items that aren't similar to increase the chances that the user likes at least one of these items. Basic Models of recommender systems \u2691 There are four types of basic models: Collaborative filtering : Use collaborative power of the ratings provided by multiple users to make recommendations. Content-based : Use the descriptive attributes of the user rated items to create a user-specific model that predicts the rating of unobserved items. Knowledge-based : Use the similarities between customer requirements and item descriptions. Hybrid systems : Combine the above to benefit from the mix of their strengths to perform more robustly. Collaborative Filtering Models \u2691 These models use the collaborative power of the ratings provided by multiple users to make recommendations. The main challenge is that the underlying ratings matrices are sparse. To solve it, unspecified ratings are guessed by analyzing the relations of high correlation across various users and items. There are two common types of methods. Memory-based methods : Also referred to as neighborhood-based collaborative filtering algorithms , predict ratings on the basis of their neighborhoods. These can be defined through two ways: User-based collaborative filtering : Ratings provided by like-minded users of a target user A are used in order to make the recommendations for A. Thus, the goal is to determine users who are similar to the target user A, and recommend ratings for the unobserved ratings of A by computing the averages of the ratings of this peer group. Item-based collaborative filtering : In order to make the rating predictions for target item B by user A, the first step is to determine a set S of items that are most similar to target item B. The ratings in the item set S, which are defined by A, are used to predict whether the user A will like item B. These systems are simple to implement and the resulting recommendations are often easy to explain. On the other hand, they do not work very well with sparse rating matrices. Model-based methods : Machine learning and data mining methods are used in the context of predictive models. In cases where the model is parameterized, the parameters of this model are learned within the context of an optimization framework. Some examples of such methods include decision trees, rule-based models, Bayesian methods and latent factor models. Many of these methods have a high level of coverage even for sparse ratings matrices. Types of ratings \u2691 The design of recommendation algorithms is influenced by the system used for tracking ratings. There are different types of ratings: interval-based : A discrete set of ordered numbers are used to quantify like or dislike. ordinal : A discrete set of ordered categorical values, such as agree or strongly agree, are used to achieve the same goal. binary : Only the like or dislike for the item can be specified. unary : Only liking of an item can be specified. Another categorization of rating systems is based in the way the feedback is retrieved: explicit ratings : Users actively give information on their preferences. implicit ratings : Users preferences are derived from their behavior. Such as visiting a link. Therefore, implicit ratings are usually unary. Content-Based Recommender Systems \u2691 In content-based recommender systems, descriptive attributes of the user rated items are used to create a user-specific model that predicts the rating of unobserved items. These systems have the following advantages: Works well for new items, when sufficient data is not available. If the user has rated items with similar attributes. Can make recommendations with only the data of one user. And the following disadvantages: As the community knowledge from similar users is not leveraged, it tends to reduce the diversity of the recommended items. It requires large number of rating user data to produce robust predictions without overfitting. Knowledge-based Recommender Systems \u2691 The recommendation process is performed based on the similarities between user requirements and item descriptions or the user requirements constrains. The process is facilitated with the use of knowledge bases, which contain data about rules and similarity functions to use during the retrieval process. Knowledge-based systems can be classified by the type of user interface: Constraint-based recommender systems : Users specify requirements on the item attributes or give information about their attributes. Then domain specific rules are used to select the items to recommend. Case-based recommender systems : Specific cases are selected by the user as targets or anchor points. Similarity metrics are defined in a domain specific way on the item attributes to retrieve similar items to these cases. These user interfaces can interact with the users through several ways: Conversational systems : User preferences are determined iteratively in the context of a feedback loop. It's useful if the item domain is complex. Search-based systems : User preferences are elicited by using a preset of questions. Navigation-based recommendation : Users specify a number of attribute changes to the item being recommended. Also known as critiquing recommender systems . The main difference between content-based systems and knowledge-based systems is that while the former learns from past user behavior, the latter does it from active user specification of their needs and interests. These systems have the following advantages: Works well for items with varied properties and/or few ratings. Such as in cold start scenarios, if it's difficult to capture the user interests with historical data or if the item is not often consumed. Allows the users to explicitly specify what they want, thus giving them a greater control over the recommendation process. Allows the user to iteratively change their specified requirements to reach the desired items. Can make recommendations with only the data of one user. And the following disadvantages: As the community knowledge from similar users is not leveraged, it tends to reduce the diversity of the recommended items. Domain-Specific recommender systems \u2691 Demographic recommender systems \u2691 In these systems the demographic information about the user is leveraged to learn classifiers that can map specific demographics to ratings. Although they do not usually provide the best results on a standalone basis, they enhance and increase robustness if used as a component of hybrid systems. Pitfalls to avoid \u2691 Pre-substitution of missing ratings is not recommended in explicit rating matrices as it leads to a significant amount of bias in the analysis. In unary ratings it's common to substitute the missing data by 0 as even though it adds some bias, it's not as great because it's assumed that the default behavior. Interesting resources \u2691 Content indexers \u2691 Open Library : Open, editable library catalog, building towards a web page for every book ever published. Data can be retrieved through their API or bulk downloaded . Rating Datasets \u2691 Books \u2691 Book-Crossing : 278,858 users providing 1,149,780 ratings (explicit / implicit) about 271,379 books. Movies \u2691 MovieLens : 27,000,000 ratings and 1,100,000 tag applications applied to 58,000 movies by 280,000 users. HetRec 2011 Movielens + IMDB/rotten Tomatoes : 86,000 ratings from 2113 users. Netflix prize dataset : 480,000 users doing 100 million ratings on 17,000 movies. Music \u2691 HetRec 2011 Last.FM : 92,800 artist listening records from 1892 users. Web \u2691 HetRec 2011 Delicious : 105,000 bookmarks from 1867 users. Miscelaneous \u2691 Wikilens : generalized collaborative recommender system that allowed its community to define item types (e.g. beer) and categories (e.g. microbrews, pale ales, stouts), and then rate and get recommendations for items. Past Projects \u2691 GroupLens: Pioneer recommender system to recommend Usenet news. BookLens : Books implementation of Grouplens. MovieLens : Movies implementation of Grouplens. References \u2691 Books \u2691 Recommender Systems by Chary C.Aggarwal . Recommender systems, an introduction by Dietmar Jannach, Markus Zanker, Alexander Felfernig and Gerhard Friedrich. Practical Recommender Systems by Kim Falk. Hands On recommendation systems in Python by Rounak Banik. Awesome recommender systems \u2691 Grahamjenson","title":"Recommender Systems"},{"location":"data_analysis/recommender_systems/recommender_systems/#goals-of-recommender-systems","text":"The common operational and technical goals of recommender systems are: Relevance : Recommend items that are relevant to the user at hand. Although it's the primary operational goal, it is not sufficient in isolation. Novelty : Recommend items that the user has not seen in the past. Serendipity : Recommend items that are outside the user's bubble, rather than simply something they did not know about before. For example, if a new Indian restaurant opens in a neighborhood, then the recommendation of that restaurant to a user who normally eats Indian food is novel but not necessarily serendipitous. On the other hand, when the same user is recommended Ethiopian food, and it was unknown to the user that such food might appeal to her, then the recommendation is serendipitous. Serendipity has the beneficial side effect of beginning new areas of interest. Increase recommendation diversity: Recommend items that aren't similar to increase the chances that the user likes at least one of these items.","title":"Goals of recommender systems"},{"location":"data_analysis/recommender_systems/recommender_systems/#basic-models-of-recommender-systems","text":"There are four types of basic models: Collaborative filtering : Use collaborative power of the ratings provided by multiple users to make recommendations. Content-based : Use the descriptive attributes of the user rated items to create a user-specific model that predicts the rating of unobserved items. Knowledge-based : Use the similarities between customer requirements and item descriptions. Hybrid systems : Combine the above to benefit from the mix of their strengths to perform more robustly.","title":"Basic Models of recommender systems"},{"location":"data_analysis/recommender_systems/recommender_systems/#collaborative-filtering-models","text":"These models use the collaborative power of the ratings provided by multiple users to make recommendations. The main challenge is that the underlying ratings matrices are sparse. To solve it, unspecified ratings are guessed by analyzing the relations of high correlation across various users and items. There are two common types of methods. Memory-based methods : Also referred to as neighborhood-based collaborative filtering algorithms , predict ratings on the basis of their neighborhoods. These can be defined through two ways: User-based collaborative filtering : Ratings provided by like-minded users of a target user A are used in order to make the recommendations for A. Thus, the goal is to determine users who are similar to the target user A, and recommend ratings for the unobserved ratings of A by computing the averages of the ratings of this peer group. Item-based collaborative filtering : In order to make the rating predictions for target item B by user A, the first step is to determine a set S of items that are most similar to target item B. The ratings in the item set S, which are defined by A, are used to predict whether the user A will like item B. These systems are simple to implement and the resulting recommendations are often easy to explain. On the other hand, they do not work very well with sparse rating matrices. Model-based methods : Machine learning and data mining methods are used in the context of predictive models. In cases where the model is parameterized, the parameters of this model are learned within the context of an optimization framework. Some examples of such methods include decision trees, rule-based models, Bayesian methods and latent factor models. Many of these methods have a high level of coverage even for sparse ratings matrices.","title":"Collaborative Filtering Models"},{"location":"data_analysis/recommender_systems/recommender_systems/#types-of-ratings","text":"The design of recommendation algorithms is influenced by the system used for tracking ratings. There are different types of ratings: interval-based : A discrete set of ordered numbers are used to quantify like or dislike. ordinal : A discrete set of ordered categorical values, such as agree or strongly agree, are used to achieve the same goal. binary : Only the like or dislike for the item can be specified. unary : Only liking of an item can be specified. Another categorization of rating systems is based in the way the feedback is retrieved: explicit ratings : Users actively give information on their preferences. implicit ratings : Users preferences are derived from their behavior. Such as visiting a link. Therefore, implicit ratings are usually unary.","title":"Types of ratings"},{"location":"data_analysis/recommender_systems/recommender_systems/#content-based-recommender-systems","text":"In content-based recommender systems, descriptive attributes of the user rated items are used to create a user-specific model that predicts the rating of unobserved items. These systems have the following advantages: Works well for new items, when sufficient data is not available. If the user has rated items with similar attributes. Can make recommendations with only the data of one user. And the following disadvantages: As the community knowledge from similar users is not leveraged, it tends to reduce the diversity of the recommended items. It requires large number of rating user data to produce robust predictions without overfitting.","title":"Content-Based Recommender Systems"},{"location":"data_analysis/recommender_systems/recommender_systems/#knowledge-based-recommender-systems","text":"The recommendation process is performed based on the similarities between user requirements and item descriptions or the user requirements constrains. The process is facilitated with the use of knowledge bases, which contain data about rules and similarity functions to use during the retrieval process. Knowledge-based systems can be classified by the type of user interface: Constraint-based recommender systems : Users specify requirements on the item attributes or give information about their attributes. Then domain specific rules are used to select the items to recommend. Case-based recommender systems : Specific cases are selected by the user as targets or anchor points. Similarity metrics are defined in a domain specific way on the item attributes to retrieve similar items to these cases. These user interfaces can interact with the users through several ways: Conversational systems : User preferences are determined iteratively in the context of a feedback loop. It's useful if the item domain is complex. Search-based systems : User preferences are elicited by using a preset of questions. Navigation-based recommendation : Users specify a number of attribute changes to the item being recommended. Also known as critiquing recommender systems . The main difference between content-based systems and knowledge-based systems is that while the former learns from past user behavior, the latter does it from active user specification of their needs and interests. These systems have the following advantages: Works well for items with varied properties and/or few ratings. Such as in cold start scenarios, if it's difficult to capture the user interests with historical data or if the item is not often consumed. Allows the users to explicitly specify what they want, thus giving them a greater control over the recommendation process. Allows the user to iteratively change their specified requirements to reach the desired items. Can make recommendations with only the data of one user. And the following disadvantages: As the community knowledge from similar users is not leveraged, it tends to reduce the diversity of the recommended items.","title":"Knowledge-based Recommender Systems"},{"location":"data_analysis/recommender_systems/recommender_systems/#domain-specific-recommender-systems","text":"","title":"Domain-Specific recommender systems"},{"location":"data_analysis/recommender_systems/recommender_systems/#demographic-recommender-systems","text":"In these systems the demographic information about the user is leveraged to learn classifiers that can map specific demographics to ratings. Although they do not usually provide the best results on a standalone basis, they enhance and increase robustness if used as a component of hybrid systems.","title":"Demographic recommender systems"},{"location":"data_analysis/recommender_systems/recommender_systems/#pitfalls-to-avoid","text":"Pre-substitution of missing ratings is not recommended in explicit rating matrices as it leads to a significant amount of bias in the analysis. In unary ratings it's common to substitute the missing data by 0 as even though it adds some bias, it's not as great because it's assumed that the default behavior.","title":"Pitfalls to avoid"},{"location":"data_analysis/recommender_systems/recommender_systems/#interesting-resources","text":"","title":"Interesting resources"},{"location":"data_analysis/recommender_systems/recommender_systems/#content-indexers","text":"Open Library : Open, editable library catalog, building towards a web page for every book ever published. Data can be retrieved through their API or bulk downloaded .","title":"Content indexers"},{"location":"data_analysis/recommender_systems/recommender_systems/#rating-datasets","text":"","title":"Rating Datasets"},{"location":"data_analysis/recommender_systems/recommender_systems/#books","text":"Book-Crossing : 278,858 users providing 1,149,780 ratings (explicit / implicit) about 271,379 books.","title":"Books"},{"location":"data_analysis/recommender_systems/recommender_systems/#movies","text":"MovieLens : 27,000,000 ratings and 1,100,000 tag applications applied to 58,000 movies by 280,000 users. HetRec 2011 Movielens + IMDB/rotten Tomatoes : 86,000 ratings from 2113 users. Netflix prize dataset : 480,000 users doing 100 million ratings on 17,000 movies.","title":"Movies"},{"location":"data_analysis/recommender_systems/recommender_systems/#music","text":"HetRec 2011 Last.FM : 92,800 artist listening records from 1892 users.","title":"Music"},{"location":"data_analysis/recommender_systems/recommender_systems/#web","text":"HetRec 2011 Delicious : 105,000 bookmarks from 1867 users.","title":"Web"},{"location":"data_analysis/recommender_systems/recommender_systems/#miscelaneous","text":"Wikilens : generalized collaborative recommender system that allowed its community to define item types (e.g. beer) and categories (e.g. microbrews, pale ales, stouts), and then rate and get recommendations for items.","title":"Miscelaneous"},{"location":"data_analysis/recommender_systems/recommender_systems/#past-projects","text":"GroupLens: Pioneer recommender system to recommend Usenet news. BookLens : Books implementation of Grouplens. MovieLens : Movies implementation of Grouplens.","title":"Past Projects"},{"location":"data_analysis/recommender_systems/recommender_systems/#references","text":"","title":"References"},{"location":"data_analysis/recommender_systems/recommender_systems/#books_1","text":"Recommender Systems by Chary C.Aggarwal . Recommender systems, an introduction by Dietmar Jannach, Markus Zanker, Alexander Felfernig and Gerhard Friedrich. Practical Recommender Systems by Kim Falk. Hands On recommendation systems in Python by Rounak Banik.","title":"Books"},{"location":"data_analysis/recommender_systems/recommender_systems/#awesome-recommender-systems","text":"Grahamjenson","title":"Awesome recommender systems"},{"location":"devops/alex/","text":"Alex helps you find gender favoring, polarizing, race related, religion inconsiderate, or other unequal phrasing in text. For example, when We\u2019ve confirmed his identity is given , alex will warn you and suggest using their instead of his . Give alex a spin on the Online demo . Installation \u2691 npm install alex --global You can use it with Vim through the ALE plugin . As of 2020-07-14, there is no pre-commit available. So I'm going to use it as a soft linter. References \u2691 Git","title":"Alex"},{"location":"devops/alex/#installation","text":"npm install alex --global You can use it with Vim through the ALE plugin . As of 2020-07-14, there is no pre-commit available. So I'm going to use it as a soft linter.","title":"Installation"},{"location":"devops/alex/#references","text":"Git","title":"References"},{"location":"devops/api_management/","text":"API management is the process of creating and publishing web application programming interfaces (APIs) under a service that: Enforces the usage of policies. Controls access. Collects and analyzes usage statistics. Reports on performance. Components \u2691 While solutions vary, components that provide the following functionality are typically found in API management products: Gateway : a server that acts as an API front-end, receives API requests, enforces throttling and security policies, passes requests to the back-end service and then passes the response back to the requester. A gateway often includes a transformation engine to orchestrate and modify the requests and responses on the fly. A gateway can also provide functionality such as collecting analytics data and providing caching. The gateway can provide functionality to support authentication, authorization, security, audit and regulatory compliance. Publishing tools : a collection of tools that API providers use to define APIs, for instance using the OpenAPI or RAML specifications, generate API documentation, manage access and usage policies for APIs, test and debug the execution of API, including security testing and automated generation of tests and test suites, deploy APIs into production, staging, and quality assurance environments, and coordinate the overall API lifecycle. Developer portal/API store : community site, typically branded by an API provider, that can encapsulate for API users in a single convenient source information and functionality including documentation, tutorials, sample code, software development kits, an interactive API console and sandbox to trial APIs, the ability to subscribe to the APIs and manage subscription keys such as OAuth2 Client ID and Client Secret, and obtain support from the API provider and user and community. Reporting and analytics : functionality to monitor API usage and load (overall hits, completed transactions, number of data objects returned, amount of compute time and other internal resources consumed, volume of data transferred). This can include real-time monitoring of the API with alerts being raised directly or via a higher-level network management system, for instance, if the load on an API has become too great, as well as functionality to analyze historical data, such as transaction logs, to detect usage trends. Functionality can also be provided to create synthetic transactions that can be used to test the performance and behavior of API endpoints. The information gathered by the reporting and analytics functionality can be used by the API provider to optimize the API offering within an organization's overall continuous improvement process and for defining software Service-Level Agreements for APIs. Monetization : functionality to support charging for access to commercial APIs. This functionality can include support for setting up pricing rules, based on usage, load and functionality, issuing invoices and collecting payments including multiple types of credit card payments.","title":"API Management"},{"location":"devops/api_management/#components","text":"While solutions vary, components that provide the following functionality are typically found in API management products: Gateway : a server that acts as an API front-end, receives API requests, enforces throttling and security policies, passes requests to the back-end service and then passes the response back to the requester. A gateway often includes a transformation engine to orchestrate and modify the requests and responses on the fly. A gateway can also provide functionality such as collecting analytics data and providing caching. The gateway can provide functionality to support authentication, authorization, security, audit and regulatory compliance. Publishing tools : a collection of tools that API providers use to define APIs, for instance using the OpenAPI or RAML specifications, generate API documentation, manage access and usage policies for APIs, test and debug the execution of API, including security testing and automated generation of tests and test suites, deploy APIs into production, staging, and quality assurance environments, and coordinate the overall API lifecycle. Developer portal/API store : community site, typically branded by an API provider, that can encapsulate for API users in a single convenient source information and functionality including documentation, tutorials, sample code, software development kits, an interactive API console and sandbox to trial APIs, the ability to subscribe to the APIs and manage subscription keys such as OAuth2 Client ID and Client Secret, and obtain support from the API provider and user and community. Reporting and analytics : functionality to monitor API usage and load (overall hits, completed transactions, number of data objects returned, amount of compute time and other internal resources consumed, volume of data transferred). This can include real-time monitoring of the API with alerts being raised directly or via a higher-level network management system, for instance, if the load on an API has become too great, as well as functionality to analyze historical data, such as transaction logs, to detect usage trends. Functionality can also be provided to create synthetic transactions that can be used to test the performance and behavior of API endpoints. The information gathered by the reporting and analytics functionality can be used by the API provider to optimize the API offering within an organization's overall continuous improvement process and for defining software Service-Level Agreements for APIs. Monetization : functionality to support charging for access to commercial APIs. This functionality can include support for setting up pricing rules, based on usage, load and functionality, issuing invoices and collecting payments including multiple types of credit card payments.","title":"Components"},{"location":"devops/bandit/","text":"Bandit finds common security issues in Python code. To do this, Bandit processes each file, builds an AST from it, and runs appropriate plugins against the AST nodes. Once Bandit has finished scanning all the files, it generates a report. You can use this cookiecutter template to create a python project with bandit already configured. Installation \u2691 pip install bandit Usage \u2691 Ignore an error. \u2691 Add the # nosec comment in the line. Configuration \u2691 You can run bandit through: Pre-commit: File: .pre-commit-config.yaml repos : - repo : https://github.com/Lucas-C/pre-commit-hooks-bandit rev : v1.0.4 hooks : - id : python-bandit-vulnerability-check bandit takes a lot of time to run, so it slows down too much the commiting, therefore it should be run only in the CI. Github Actions: Make sure to check that the correct python version is applied. File: .github/workflows/security.yml name : Security on : [ push , pull_request ] jobs : bandit : runs-on : ubuntu-latest steps : - name : Checkout uses : actions/checkout@v2 - uses : actions/setup-python@v2 with : python-version : 3.7 - name : Install dependencies run : pip install bandit - name : Execute bandit run : bandit -r project References \u2691 Docs","title":"Bandit"},{"location":"devops/bandit/#installation","text":"pip install bandit","title":"Installation"},{"location":"devops/bandit/#usage","text":"","title":"Usage"},{"location":"devops/bandit/#ignore-an-error","text":"Add the # nosec comment in the line.","title":"Ignore an error."},{"location":"devops/bandit/#configuration","text":"You can run bandit through: Pre-commit: File: .pre-commit-config.yaml repos : - repo : https://github.com/Lucas-C/pre-commit-hooks-bandit rev : v1.0.4 hooks : - id : python-bandit-vulnerability-check bandit takes a lot of time to run, so it slows down too much the commiting, therefore it should be run only in the CI. Github Actions: Make sure to check that the correct python version is applied. File: .github/workflows/security.yml name : Security on : [ push , pull_request ] jobs : bandit : runs-on : ubuntu-latest steps : - name : Checkout uses : actions/checkout@v2 - uses : actions/setup-python@v2 with : python-version : 3.7 - name : Install dependencies run : pip install bandit - name : Execute bandit run : bandit -r project","title":"Configuration"},{"location":"devops/bandit/#references","text":"Docs","title":"References"},{"location":"devops/black/","text":"Black is a style guide enforcement tool. You can use this cookiecutter template to create a python project with black already configured. Installation \u2691 pip install black Configuration \u2691 Its configuration is stored in pyproject.toml . File: pyproject.toml # Example configuration for Black. # NOTE: you have to use single-quoted strings in TOML for regular expressions. # It's the equivalent of r-strings in Python. Multiline strings are treated as # verbose regular expressions by Black. Use [ ] to denote a significant space # character. [tool.black] line-length = 88 target-version = ['py36', 'py37', 'py38'] include = '\\.pyi?$' exclude = ''' /( \\.eggs | \\.git | \\.hg | \\.mypy_cache | \\.tox | \\.venv | _build | buck-out | build | dist # The following are specific to Black, you probably don't want those. | blib2to3 | tests/data | profiling )/ ''' You can use it both with: The Vim plugin Pre-commit : File: .pre-commit-config.yaml repos : - repo : https://github.com/ambv/black rev : stable hooks : - id : black language_version : python3.7 Github Actions: File: .github/workflows/lint.yml --- name : Lint on : [ push , pull_request ] jobs : Black : runs-on : ubuntu-latest steps : - uses : actions/checkout@v2 - uses : actions/setup-python@v2 - name : Black uses : psf/black@stable Split long lines \u2691 If you want to split long lines, you need to use the --experimental-string-processing flag. I haven't found how to set that option in the config file. Disable the formatting of some lines \u2691 You can use the comments # fmt: off and # fmt: on References \u2691 Docs Git","title":"Black"},{"location":"devops/black/#installation","text":"pip install black","title":"Installation"},{"location":"devops/black/#configuration","text":"Its configuration is stored in pyproject.toml . File: pyproject.toml # Example configuration for Black. # NOTE: you have to use single-quoted strings in TOML for regular expressions. # It's the equivalent of r-strings in Python. Multiline strings are treated as # verbose regular expressions by Black. Use [ ] to denote a significant space # character. [tool.black] line-length = 88 target-version = ['py36', 'py37', 'py38'] include = '\\.pyi?$' exclude = ''' /( \\.eggs | \\.git | \\.hg | \\.mypy_cache | \\.tox | \\.venv | _build | buck-out | build | dist # The following are specific to Black, you probably don't want those. | blib2to3 | tests/data | profiling )/ ''' You can use it both with: The Vim plugin Pre-commit : File: .pre-commit-config.yaml repos : - repo : https://github.com/ambv/black rev : stable hooks : - id : black language_version : python3.7 Github Actions: File: .github/workflows/lint.yml --- name : Lint on : [ push , pull_request ] jobs : Black : runs-on : ubuntu-latest steps : - uses : actions/checkout@v2 - uses : actions/setup-python@v2 - name : Black uses : psf/black@stable","title":"Configuration"},{"location":"devops/black/#split-long-lines","text":"If you want to split long lines, you need to use the --experimental-string-processing flag. I haven't found how to set that option in the config file.","title":"Split long lines"},{"location":"devops/black/#disable-the-formatting-of-some-lines","text":"You can use the comments # fmt: off and # fmt: on","title":"Disable the formatting of some lines"},{"location":"devops/black/#references","text":"Docs Git","title":"References"},{"location":"devops/ci/","text":"Continuous Integration (CI) allows to automatically run processes on the code each time a commit is pushed. For example it can be used to run the tests, build the documentation, build a package or maintain dependencies updated. I've automated the configuration of CI/CD pipelines for python projects in this cookiecutter template . There are three non exclusive ways to run the tests: Integrate them in your editor, so it's executed each time you save the file. Through a pre-commit hook to make it easy for the collaborator to submit correctly formatted code. pre-commit is a framework for managing and maintaining multi-language pre-commit hooks. Through a CI server (like Drone or Github Actions) to ensure that the commited code meets the quality standards. Developers can bypass the pre-commit filter, so we need to set up the quality gate in an agnostic environment. Depending on the time the test takes to run and their different implementations, we'll choose from one to three of the choices above. Configuring pre-commit \u2691 To adopt pre-commit to our system we have to: Install pre-commit: pip3 install pre-commit and add it to the development requirements.txt . Define .pre-commit-config.yaml with the hooks you want to include (they don't plan to support pyproject.toml ). Execute pre-commit install to install git hooks in your .git/ directory. Execute pre-commit run --all-files to tests all the files. Usually pre-commit will only run on the changed files during git hooks. Static analysis checkers \u2691 Static analysis is the analysis of computer software that is performed without actually executing programs. Formatters \u2691 Formatters are tools that change your files to meet a linter requirements. Black : A python style guide formatter tool. Linters \u2691 Lint, or a linter , is a static code analysis tool used to flag programming errors, bugs, stylistic errors, and suspicious constructs. The term originates from a Unix utility that examined C language source code. alex to find gender favoring, polarizing, race related, religion inconsiderate, or other unequal phrasing in text. Flake8 : A python style guide checker tool. markdownlint : A linter for Markdown files. proselint : Is another linter for prose. Yamllint : A linter for YAML files. write-good is a naive linter for English prose. Type checkers \u2691 Type checkers are programs that the code is compliant with a defined type system which is a logical system comprising a set of rules that assigns a property called a type to the various constructs of a computer program, such as variables, expressions, functions or modules. The main purpose of a type system is to reduce possibilities for bugs by defining interfaces between different parts of the program, and then checking that the parts have been connected in a consistent way. Mypy : A static type checker for Python. Security vulnerability checkers \u2691 Tools that check potential vulnerabilities in the code. Bandit : Finds common security issues in Python code. Safety : Checks your installed dependencies for known security vulnerabilities. Other pre-commit tests \u2691 Pre-commit comes with several tests by default. These are the ones I've chosen. File: .pre-commit-config.yaml repos : - repo : https://github.com/pre-commit/pre-commit-hooks rev : v3.1.0 hooks : - id : trailing-whitespace - id : check-added-large-files - id : check-docstring-first - id : check-merge-conflict - id : end-of-file-fixer - id : detect-private-key Update package dependencies \u2691 Tools to automatically keep your dependencies updated. pip-tools Coverage reports \u2691 Coveralls is a service that monitors and writes statistics on the coverage of your repositories. To use them, you'll need to log in with your Github account and enable the repos you want to test. Save the secret in the repository configuration and add this step to your tests job. - name : Coveralls uses : coverallsapp/github-action@master with : github-token : ${{ secrets.COVERALLS_TOKEN }} Add the following badge to your README.md. Variables to substitute: repository_path : Github repository path, like lyz-code/pydo . [ ![Coverage Status ]( https://coveralls.io/repos/github/{{ repository_path }}/badge.svg?branch=master )](https://coveralls.io/github/{{ repository_path }}?branch=master) Troubleshooting \u2691 error: pathspec 'master' did not match any file(s) known to git \u2691 If you have this error while making a commit through a pipeline step, it may be the pre-commits stepping in. To fix it, remove all git hooks with rm -r .git/hooks .","title":"Continuous Integration"},{"location":"devops/ci/#configuring-pre-commit","text":"To adopt pre-commit to our system we have to: Install pre-commit: pip3 install pre-commit and add it to the development requirements.txt . Define .pre-commit-config.yaml with the hooks you want to include (they don't plan to support pyproject.toml ). Execute pre-commit install to install git hooks in your .git/ directory. Execute pre-commit run --all-files to tests all the files. Usually pre-commit will only run on the changed files during git hooks.","title":"Configuring pre-commit"},{"location":"devops/ci/#static-analysis-checkers","text":"Static analysis is the analysis of computer software that is performed without actually executing programs.","title":"Static analysis checkers"},{"location":"devops/ci/#formatters","text":"Formatters are tools that change your files to meet a linter requirements. Black : A python style guide formatter tool.","title":"Formatters"},{"location":"devops/ci/#linters","text":"Lint, or a linter , is a static code analysis tool used to flag programming errors, bugs, stylistic errors, and suspicious constructs. The term originates from a Unix utility that examined C language source code. alex to find gender favoring, polarizing, race related, religion inconsiderate, or other unequal phrasing in text. Flake8 : A python style guide checker tool. markdownlint : A linter for Markdown files. proselint : Is another linter for prose. Yamllint : A linter for YAML files. write-good is a naive linter for English prose.","title":"Linters"},{"location":"devops/ci/#type-checkers","text":"Type checkers are programs that the code is compliant with a defined type system which is a logical system comprising a set of rules that assigns a property called a type to the various constructs of a computer program, such as variables, expressions, functions or modules. The main purpose of a type system is to reduce possibilities for bugs by defining interfaces between different parts of the program, and then checking that the parts have been connected in a consistent way. Mypy : A static type checker for Python.","title":"Type checkers"},{"location":"devops/ci/#security-vulnerability-checkers","text":"Tools that check potential vulnerabilities in the code. Bandit : Finds common security issues in Python code. Safety : Checks your installed dependencies for known security vulnerabilities.","title":"Security vulnerability checkers"},{"location":"devops/ci/#other-pre-commit-tests","text":"Pre-commit comes with several tests by default. These are the ones I've chosen. File: .pre-commit-config.yaml repos : - repo : https://github.com/pre-commit/pre-commit-hooks rev : v3.1.0 hooks : - id : trailing-whitespace - id : check-added-large-files - id : check-docstring-first - id : check-merge-conflict - id : end-of-file-fixer - id : detect-private-key","title":"Other pre-commit tests"},{"location":"devops/ci/#update-package-dependencies","text":"Tools to automatically keep your dependencies updated. pip-tools","title":"Update package dependencies"},{"location":"devops/ci/#coverage-reports","text":"Coveralls is a service that monitors and writes statistics on the coverage of your repositories. To use them, you'll need to log in with your Github account and enable the repos you want to test. Save the secret in the repository configuration and add this step to your tests job. - name : Coveralls uses : coverallsapp/github-action@master with : github-token : ${{ secrets.COVERALLS_TOKEN }} Add the following badge to your README.md. Variables to substitute: repository_path : Github repository path, like lyz-code/pydo . [ ![Coverage Status ]( https://coveralls.io/repos/github/{{ repository_path }}/badge.svg?branch=master )](https://coveralls.io/github/{{ repository_path }}?branch=master)","title":"Coverage reports"},{"location":"devops/ci/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"devops/ci/#error-pathspec-master-did-not-match-any-files-known-to-git","text":"If you have this error while making a commit through a pipeline step, it may be the pre-commits stepping in. To fix it, remove all git hooks with rm -r .git/hooks .","title":"error: pathspec 'master' did not match any file(s) known to git"},{"location":"devops/devops/","text":"DevOps is a set of practices that combines software development (Dev) and information-technology operations (Ops) which aims to shorten the systems development life cycle and provide continuous delivery with high software quality. One of the most important goals of the DevOps initiative is to break the silos between the developers and the sysadmins, that lead to ill feelings and unproductivity. It's a relatively new concept , the main ideas emerged in the 1990s and the first conference was in 2009. That means that as of 2021 there is still a lot of debate of what people understand as DevOps. DevOps pitfalls \u2691 I've found that the DevOps word leads to some pitfalls that we should try to avoid. Getting lost in the label \u2691 Labels are a language tool used to speed up communication by describing someone or something in a word or short phrase. However, there are times when labels achieve the complete oposite, as it's the case with DevOps, where there are different views on what the label represents and usually one of the communication parties strongly feels they belong to the label while the other doesn't agree. These discussions can fall into an unproductive, agitated semantic debate where each part tries to convince each other. So instead of starting a twitter thread telling people why they aren't a DevOps team, we could invest those energies in creating resources that close the gap between both parties. Similarly, instead of starting an internal discussion of what do we understand as DevOps? , we could discuss how to improve our existent processes so that the team members feel more comfortable contributing to the application or infrastructure code. You need to do it all to be awarded the DevOps pin \u2691 I find specially harmful the idea that to be qualified as DevOps you need to develop and maintain the application at the same time as the infrastructure that holds it. To be able to do that in a typical company product you'll need to know (between another thousand more things): How to operate the cloud infrastructure where the project lives, which can be AWS , Google Cloud, Azure or/and baremetal servers. Deploy new resources in that infrastructure, which probably would mean knowing Terraform, Ansible, Docker or/and Kubernetes . How to integrate the new resources with the operations processes, for example: The monitoring system, so you'll need to know how to use Prometheus , Nagios, Zabbix or the existent solution. The continuous integration or delivery system, that you'll need to know how to maintain, so you have to know how it works and how is it built. The backup system. The log centralizer system. Infrastructure architecture to know what you need to deploy, how and where. To code efficiently in the language that the application is developed in, for example Java, Python, Rust, Go, PHP or Javascript, in a way that meets the quality requirements (code style, linters, coverage and documentation). Knowing how to test your code in that language. Software architecture to structure complex code projects in a maintainable way. The product you're developing to be able to suggest features and fixtures when the product owner or the stakeholders show their needs. How to make the application user friendly so anyone wants to use it. And don't forget that you also need to do that in a secure way, so you should also have to know about pentesting, static and dynamic security tools, common security vulnerabilities... And I could keep on going forever. Even if you could get there (and you won't), it wouldn't matter, because when you did, the technologies will have changed so much that you will already be outdated and would need to start over. It's the sickness of the fullstack developer. If you make job openings with this mindset you're going to end up with a team of cis white males in their thirties or forties that are used to earn 3 or 4 times the minimum salary. No other kind of people can reach that point and hold it in time. But bare with me a little longer, even if you make there. What happens when the project changes so that you need to: Change the programming language of your application. Change the cloud provider. Change the deployment system. Change the program architecture. Change the language framework. Or any other thousand of possible changes? You would need to be able to keep up with them. Noooo way. Luckily you are not alone. You are a cog in a team, that as a whole is able to overcome these changes. That is why we have developers, sysadmins, security, user experience, quality, scrum master and product owners. So each profile can design, create, investigate and learn it's best in their area of expertise. In the merge of all that personal knowledge is where the team thrives. DevOps then, as I understand it, is the philosophy where the developers and sysadmins try their best to break the barriers that separate them. That idea can be brought to earth by for example: Open discussions on how to: Improve the development workflow. Make developers or sysadmins life easier. Make it easier to use the sysadmin tools. Make it easier to understand the developers code. Formations on the technologies or architecture used by either side. A clear documentation that allows either side to catch up with new changes. Periodic meetings to update each other with the changes. Periodic meetings to release the tension that have appeared between them. Joined design sessions to decide how to solve problems. Learn path \u2691 DevOps has become a juicy work, if you want to become one, I think you first need to get the basic knowledge of each of them (developing and operating) before being able to unlock the benefits of the combination of both. You can try to learn both at the same time, but I think it can be a daunting task. To get the basic knowledge of the Ops side I would: Learn basic Linux administration, otherwise you'll be lost. Learn how to be comfortable searching for anything you don't know, most of your questions are already answered, and even the most senior people spent a great amount of time searching for solutions in the project's documentation, Github issues or Stackoverflow. When you start, navigating this knowledge sources is hard and consumes a lot of your life, but it will get easier with the time. Learn how to use Git. If you can, host your own Gitea, if not, use an existing service such as Gitlab or Github. Learn how to install and maintain services, (that is why I suggested hosting your own Gitea). If you don't know what to install, take a look at the awesome self-hosted list. Learn how to use Ansible, from now on try to deploy every machine with it. Build a small project inside AWS so you can get used to the most common services (VPC, EC2, S3, Route53, RDS), most of them have free-tier resources so you don't need to pay anything. You can try also with Google Cloud or Azure, but I recommend against it. Once you are comfortable with AWS, learn how to use Terraform. You could for example deploy the previous project. From now on only use Terraform to provision AWS resources. Get into the CI/CD world hosting your own Drone, if not, use Gitlab runners or Github Actions . To get the basic knowledge of the Dev side I would: Learn the basics of a programming language, for example Python. There are thousand sources there on how to do it, books, articles, videos, forums or courses, choose the one that suits you best. As with the Ops path, get comfortable with git and searching for things you don't know. As soon as you can, start doing small programming projects that make your life easier. Coding your stuff is what's going to make you internalize the learned concepts, by finding solutions to the blocks you encounter. Publish those projects into a public git server, don't be afraid if you code is good enough, it works for you, you did your best and you should be happy about it. That's all that matters. By doing so, you'll start collaborating to the open source world and it will probably force yourself to make your code better. Step into the TDD world, learn why, how and when to test your code. For those projects that you want to maintain, create CI/CD pipelines that enhance the quality of your code, by for example running your tests or some linters . Once you're comfortable, try to collaborate with existent projects (right now you may not now where to look for projects to collaborate, but when you reach this point, I promise you will).","title":"DevOps"},{"location":"devops/devops/#devops-pitfalls","text":"I've found that the DevOps word leads to some pitfalls that we should try to avoid.","title":"DevOps pitfalls"},{"location":"devops/devops/#getting-lost-in-the-label","text":"Labels are a language tool used to speed up communication by describing someone or something in a word or short phrase. However, there are times when labels achieve the complete oposite, as it's the case with DevOps, where there are different views on what the label represents and usually one of the communication parties strongly feels they belong to the label while the other doesn't agree. These discussions can fall into an unproductive, agitated semantic debate where each part tries to convince each other. So instead of starting a twitter thread telling people why they aren't a DevOps team, we could invest those energies in creating resources that close the gap between both parties. Similarly, instead of starting an internal discussion of what do we understand as DevOps? , we could discuss how to improve our existent processes so that the team members feel more comfortable contributing to the application or infrastructure code.","title":"Getting lost in the label"},{"location":"devops/devops/#you-need-to-do-it-all-to-be-awarded-the-devops-pin","text":"I find specially harmful the idea that to be qualified as DevOps you need to develop and maintain the application at the same time as the infrastructure that holds it. To be able to do that in a typical company product you'll need to know (between another thousand more things): How to operate the cloud infrastructure where the project lives, which can be AWS , Google Cloud, Azure or/and baremetal servers. Deploy new resources in that infrastructure, which probably would mean knowing Terraform, Ansible, Docker or/and Kubernetes . How to integrate the new resources with the operations processes, for example: The monitoring system, so you'll need to know how to use Prometheus , Nagios, Zabbix or the existent solution. The continuous integration or delivery system, that you'll need to know how to maintain, so you have to know how it works and how is it built. The backup system. The log centralizer system. Infrastructure architecture to know what you need to deploy, how and where. To code efficiently in the language that the application is developed in, for example Java, Python, Rust, Go, PHP or Javascript, in a way that meets the quality requirements (code style, linters, coverage and documentation). Knowing how to test your code in that language. Software architecture to structure complex code projects in a maintainable way. The product you're developing to be able to suggest features and fixtures when the product owner or the stakeholders show their needs. How to make the application user friendly so anyone wants to use it. And don't forget that you also need to do that in a secure way, so you should also have to know about pentesting, static and dynamic security tools, common security vulnerabilities... And I could keep on going forever. Even if you could get there (and you won't), it wouldn't matter, because when you did, the technologies will have changed so much that you will already be outdated and would need to start over. It's the sickness of the fullstack developer. If you make job openings with this mindset you're going to end up with a team of cis white males in their thirties or forties that are used to earn 3 or 4 times the minimum salary. No other kind of people can reach that point and hold it in time. But bare with me a little longer, even if you make there. What happens when the project changes so that you need to: Change the programming language of your application. Change the cloud provider. Change the deployment system. Change the program architecture. Change the language framework. Or any other thousand of possible changes? You would need to be able to keep up with them. Noooo way. Luckily you are not alone. You are a cog in a team, that as a whole is able to overcome these changes. That is why we have developers, sysadmins, security, user experience, quality, scrum master and product owners. So each profile can design, create, investigate and learn it's best in their area of expertise. In the merge of all that personal knowledge is where the team thrives. DevOps then, as I understand it, is the philosophy where the developers and sysadmins try their best to break the barriers that separate them. That idea can be brought to earth by for example: Open discussions on how to: Improve the development workflow. Make developers or sysadmins life easier. Make it easier to use the sysadmin tools. Make it easier to understand the developers code. Formations on the technologies or architecture used by either side. A clear documentation that allows either side to catch up with new changes. Periodic meetings to update each other with the changes. Periodic meetings to release the tension that have appeared between them. Joined design sessions to decide how to solve problems.","title":"You need to do it all to be awarded the DevOps pin"},{"location":"devops/devops/#learn-path","text":"DevOps has become a juicy work, if you want to become one, I think you first need to get the basic knowledge of each of them (developing and operating) before being able to unlock the benefits of the combination of both. You can try to learn both at the same time, but I think it can be a daunting task. To get the basic knowledge of the Ops side I would: Learn basic Linux administration, otherwise you'll be lost. Learn how to be comfortable searching for anything you don't know, most of your questions are already answered, and even the most senior people spent a great amount of time searching for solutions in the project's documentation, Github issues or Stackoverflow. When you start, navigating this knowledge sources is hard and consumes a lot of your life, but it will get easier with the time. Learn how to use Git. If you can, host your own Gitea, if not, use an existing service such as Gitlab or Github. Learn how to install and maintain services, (that is why I suggested hosting your own Gitea). If you don't know what to install, take a look at the awesome self-hosted list. Learn how to use Ansible, from now on try to deploy every machine with it. Build a small project inside AWS so you can get used to the most common services (VPC, EC2, S3, Route53, RDS), most of them have free-tier resources so you don't need to pay anything. You can try also with Google Cloud or Azure, but I recommend against it. Once you are comfortable with AWS, learn how to use Terraform. You could for example deploy the previous project. From now on only use Terraform to provision AWS resources. Get into the CI/CD world hosting your own Drone, if not, use Gitlab runners or Github Actions . To get the basic knowledge of the Dev side I would: Learn the basics of a programming language, for example Python. There are thousand sources there on how to do it, books, articles, videos, forums or courses, choose the one that suits you best. As with the Ops path, get comfortable with git and searching for things you don't know. As soon as you can, start doing small programming projects that make your life easier. Coding your stuff is what's going to make you internalize the learned concepts, by finding solutions to the blocks you encounter. Publish those projects into a public git server, don't be afraid if you code is good enough, it works for you, you did your best and you should be happy about it. That's all that matters. By doing so, you'll start collaborating to the open source world and it will probably force yourself to make your code better. Step into the TDD world, learn why, how and when to test your code. For those projects that you want to maintain, create CI/CD pipelines that enhance the quality of your code, by for example running your tests or some linters . Once you're comfortable, try to collaborate with existent projects (right now you may not now where to look for projects to collaborate, but when you reach this point, I promise you will).","title":"Learn path"},{"location":"devops/flake8/","text":"DEPRECATION: Use Flakehell instead Flake8 doesn't support pyproject.toml , which is becoming the standard, so I suggest using Flakehell instead. Flake8 is a style guide enforcement tool. Its configuration is stored in setup.cfg , tox.ini or .flake8 . File: .flake8 [flake8] # ignore = E203, E266, E501, W503, F403, F401 max-line-length = 88 # max-complexity = 18 # select = B,C,E,F,W,T4,B9 You can use it both with: Pre-commit: File: .pre-commit-config.yaml repos : - repo : https://gitlab.com/pycqa/flake8 rev : master hooks : - id : flake8 Github Actions: File: .github/workflows/lint.yml name : Lint on : [ push , pull_request ] jobs : Flake8 : runs-on : ubuntu-latest steps : - uses : actions/checkout@v2 - uses : actions/setup-python@v2 - name : Flake8 uses : cclauss/GitHub-Action-for-Flake8@v0.5.0 References \u2691 Docs","title":"Flake8"},{"location":"devops/flake8/#references","text":"Docs","title":"References"},{"location":"devops/flakehell/","text":"Flakehell is a Flake8 wrapper to make it cool. Some of it's features are: Lint md, rst, ipynb, and more . Shareable and remote configs . Legacy-friendly : ability to get report only about new errors. Caching for much better performance. Use only specified plugins , not everything installed. Make output beautiful . pyproject.toml support. Check that all required plugins are installed . Syntax highlighting in messages and code snippets . PyLint integration. Remove unused noqa . Powerful GitLab support . Codes management: Manage codes per plugin. Enable and disable plugins and codes by wildcard. Show codes for installed plugins . Show all messages and codes for a plugin . Allow codes intersection for different plugins. You can use this cookiecutter template to create a python project with flakehell already configured. Installation \u2691 pip install flakehell Configuration \u2691 FlakeHell can be configured in pyproject.toml . You can specify any Flake8 options and FlakeHell-specific parameters. Plugins \u2691 In pyproject.toml you can specify [tool.flakehell.plugins] table. It's a list of flake8 plugins and associated to them rules. Key can be exact plugin name or wildcard template. For example \"flake8-commas\" or \"flake8-*\" . FlakeHell will choose the longest match for every plugin if possible. In the previous example, flake8-commas will match to the first pattern, flake8-bandit and flake8-bugbear to the second, and pycodestyle will not match to any pattern. Value is a list of templates for error codes for this plugin. First symbol in every template must be + (include) or - (exclude). The latest matched pattern wins. For example, [\"+*\", \"-F*\", \"-E30?\", \"-E401\"] means \"Include everything except all checks that starts with F , check from E301 to E310 , and E401 \". Example: pyproject.toml [tool.flakehell] # optionally inherit from remote config (or local if you want) base = \"https://raw.githubusercontent.com/life4/flakehell/master/pyproject.toml\" # specify any flake8 options. For example, exclude \"example.py\": exclude = [\"example.py\"] # make output nice format = \"grouped\" # don't limit yourself max_line_length = 120 # show line of source code in output show_source = true # list of plugins and rules for them [tool.flakehell.plugins] # include everything in pyflakes except F401 pyflakes = [\"+*\", \"-F401\"] # enable only codes from S100 to S199 flake8-bandit = [\"-*\", \"+S1??\"] # enable everything that starts from `flake8-` \"flake8-*\" = [\"+*\"] # explicitly disable plugin flake8-docstrings = [\"-*\"] # disable some checks for tests [tool.flakehell.exceptions.\"tests/\"] pycodestyle = [\"-F401\"] # disable a check pyflakes = [\"-*\"] # disable a plugin # do not disable `pyflakes` for one file in tests [tool.flakehell.exceptions.\"tests/test_example.py\"] pyflakes = [\"+*\"] # enable a plugin Check a complete list of flake8 extensions. flake8-bugbear : Finding likely bugs and design problems in your program. Contains warnings that don't belong in pyflakes and pycodestyle. flake8-fixme : Check for FIXME, TODO and other temporary developer notes. flake8-debugger : Check for pdb or idbp imports and set traces. flake8-mutable : Checks for mutable default arguments anti-pattern. flake8-pytest : Check for uses of Django-style assert-statements in tests. So no more self.assertEqual(a, b) , but instead assert a == b . flake8-pytest-style : Checks common style issues or inconsistencies with pytest-based tests. flake8-simplify : Helps you to simplify code. flake8-variables-names : Helps to make more readable variables names. pep8-naming : Check your code against PEP 8 naming conventions. flake8-expression-complexity : Check expression complexity. flake8-use-fstring : Checks you're using f-strings. flake8-docstrings : adds an extension for the fantastic pydocstyle tool to Flake8 . flake8-markdown : lints GitHub-style Python code blocks in Markdown files using flake8. pylint is a Python static code analysis tool which looks for programming errors, helps enforcing a coding standard, sniffs for code smells and offers simple refactoring suggestions. dlint : Encourage best coding practices and helping ensure Python code is secure. flake8-aaa : Checks Python tests follow the Arrange-Act-Assert pattern . flake8-annotations-complexity : Report on too complex type annotations. flake8-annotations : Detects the absence of PEP 3107-style function annotations and PEP 484-style type comments. flake8-typing-imports : Checks that typing imports are properly guarded. flake8-comprehensions : Help you write better list/set/dict comprehensions. flake8-eradicate : find commented out (or so called \"dead\") code. Usage \u2691 When using FlakeHell, I frequently use the following commands: flakehell lint Runs the linter, similar to the flake8 command. flakehell plugins Lists all the plugins used, and their configuration status. flakehell missed Shows any plugins that are in the configuration but not installed properly. flakehell code S322 (or any other code) Shows the explanation for that specific warning code. flakehell yesqa Removes unused codes from # noqa and removes bare noqa that says \u201cignore everything on this line\u201d as is a bad practice. Integrations \u2691 Flakehell checks can be run in: In Vim though the ALE plugin . Through a pre-commit: - repo : https://github.com/life4/flakehell/ rev : master hooks : - name : Run flakehell static analysis tool id : flakehell In the CI: - name : Test linters run : make lint Assuming you're using a Makefile like the one in my cookiecutter-python-project . Issues \u2691 ImportError: cannot import name 'MergedConfigParser' from 'flake8.options.config' : upgrade the pre-commit of autoimport. 'Namespace' object has no attribute 'extended_default_ignore' error : Until it's fixed either use a version below or equal to 3.9.0, or add to your pyproject.toml : [tool.flakehell] extended_default_ignore = [] # add this Once it's fixed, remove the patch from the maintained projects. Troubleshooting \u2691 'Namespace' object has no attribute 'extended_default_ignore' \u2691 Add to your pyproject.toml : [tool.flakehell] extended_default_ignore = [] References \u2691 Git Docs Using Flake8 and pyproject.toml with FlakeHell article by Jonathan Bowman","title":"Flakehell"},{"location":"devops/flakehell/#installation","text":"pip install flakehell","title":"Installation"},{"location":"devops/flakehell/#configuration","text":"FlakeHell can be configured in pyproject.toml . You can specify any Flake8 options and FlakeHell-specific parameters.","title":"Configuration"},{"location":"devops/flakehell/#plugins","text":"In pyproject.toml you can specify [tool.flakehell.plugins] table. It's a list of flake8 plugins and associated to them rules. Key can be exact plugin name or wildcard template. For example \"flake8-commas\" or \"flake8-*\" . FlakeHell will choose the longest match for every plugin if possible. In the previous example, flake8-commas will match to the first pattern, flake8-bandit and flake8-bugbear to the second, and pycodestyle will not match to any pattern. Value is a list of templates for error codes for this plugin. First symbol in every template must be + (include) or - (exclude). The latest matched pattern wins. For example, [\"+*\", \"-F*\", \"-E30?\", \"-E401\"] means \"Include everything except all checks that starts with F , check from E301 to E310 , and E401 \". Example: pyproject.toml [tool.flakehell] # optionally inherit from remote config (or local if you want) base = \"https://raw.githubusercontent.com/life4/flakehell/master/pyproject.toml\" # specify any flake8 options. For example, exclude \"example.py\": exclude = [\"example.py\"] # make output nice format = \"grouped\" # don't limit yourself max_line_length = 120 # show line of source code in output show_source = true # list of plugins and rules for them [tool.flakehell.plugins] # include everything in pyflakes except F401 pyflakes = [\"+*\", \"-F401\"] # enable only codes from S100 to S199 flake8-bandit = [\"-*\", \"+S1??\"] # enable everything that starts from `flake8-` \"flake8-*\" = [\"+*\"] # explicitly disable plugin flake8-docstrings = [\"-*\"] # disable some checks for tests [tool.flakehell.exceptions.\"tests/\"] pycodestyle = [\"-F401\"] # disable a check pyflakes = [\"-*\"] # disable a plugin # do not disable `pyflakes` for one file in tests [tool.flakehell.exceptions.\"tests/test_example.py\"] pyflakes = [\"+*\"] # enable a plugin Check a complete list of flake8 extensions. flake8-bugbear : Finding likely bugs and design problems in your program. Contains warnings that don't belong in pyflakes and pycodestyle. flake8-fixme : Check for FIXME, TODO and other temporary developer notes. flake8-debugger : Check for pdb or idbp imports and set traces. flake8-mutable : Checks for mutable default arguments anti-pattern. flake8-pytest : Check for uses of Django-style assert-statements in tests. So no more self.assertEqual(a, b) , but instead assert a == b . flake8-pytest-style : Checks common style issues or inconsistencies with pytest-based tests. flake8-simplify : Helps you to simplify code. flake8-variables-names : Helps to make more readable variables names. pep8-naming : Check your code against PEP 8 naming conventions. flake8-expression-complexity : Check expression complexity. flake8-use-fstring : Checks you're using f-strings. flake8-docstrings : adds an extension for the fantastic pydocstyle tool to Flake8 . flake8-markdown : lints GitHub-style Python code blocks in Markdown files using flake8. pylint is a Python static code analysis tool which looks for programming errors, helps enforcing a coding standard, sniffs for code smells and offers simple refactoring suggestions. dlint : Encourage best coding practices and helping ensure Python code is secure. flake8-aaa : Checks Python tests follow the Arrange-Act-Assert pattern . flake8-annotations-complexity : Report on too complex type annotations. flake8-annotations : Detects the absence of PEP 3107-style function annotations and PEP 484-style type comments. flake8-typing-imports : Checks that typing imports are properly guarded. flake8-comprehensions : Help you write better list/set/dict comprehensions. flake8-eradicate : find commented out (or so called \"dead\") code.","title":"Plugins"},{"location":"devops/flakehell/#usage","text":"When using FlakeHell, I frequently use the following commands: flakehell lint Runs the linter, similar to the flake8 command. flakehell plugins Lists all the plugins used, and their configuration status. flakehell missed Shows any plugins that are in the configuration but not installed properly. flakehell code S322 (or any other code) Shows the explanation for that specific warning code. flakehell yesqa Removes unused codes from # noqa and removes bare noqa that says \u201cignore everything on this line\u201d as is a bad practice.","title":"Usage"},{"location":"devops/flakehell/#integrations","text":"Flakehell checks can be run in: In Vim though the ALE plugin . Through a pre-commit: - repo : https://github.com/life4/flakehell/ rev : master hooks : - name : Run flakehell static analysis tool id : flakehell In the CI: - name : Test linters run : make lint Assuming you're using a Makefile like the one in my cookiecutter-python-project .","title":"Integrations"},{"location":"devops/flakehell/#issues","text":"ImportError: cannot import name 'MergedConfigParser' from 'flake8.options.config' : upgrade the pre-commit of autoimport. 'Namespace' object has no attribute 'extended_default_ignore' error : Until it's fixed either use a version below or equal to 3.9.0, or add to your pyproject.toml : [tool.flakehell] extended_default_ignore = [] # add this Once it's fixed, remove the patch from the maintained projects.","title":"Issues"},{"location":"devops/flakehell/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"devops/flakehell/#namespace-object-has-no-attribute-extended_default_ignore","text":"Add to your pyproject.toml : [tool.flakehell] extended_default_ignore = []","title":"'Namespace' object has no attribute 'extended_default_ignore'"},{"location":"devops/flakehell/#references","text":"Git Docs Using Flake8 and pyproject.toml with FlakeHell article by Jonathan Bowman","title":"References"},{"location":"devops/helmfile/","text":"Helmfile is a declarative spec for deploying Helm charts. It lets you: Keep a directory of chart value files and maintain changes in version control. Apply CI/CD to configuration changes. Environmental chart promotion. Periodically sync to avoid skew in environments. To avoid upgrades for each iteration of helm, the helmfile executable delegates to helm - as a result, helm must be installed. All information is saved in the helmfile.yaml file. In case we need custom yamls, we'll use kustomize . Installation \u2691 Helmfile is not yet in the distribution package managers, so you'll need to install it manually. Gather the latest release number . wget {{ bin_url }} -O helmfile_linux_amd64 chmod +x helmfile_linux_amd64 mv helmfile_linux_amd64 ~/.local/bin/helmfile Usage \u2691 How to deploy a new chart \u2691 When we want to add a new chart, the workflow would be: Run helmfile deps && helmfile diff to check that your existing charts are updated, if they are not, run helmfile apply . Configure the release in helmfile.yaml specifying: name : Deployment name. namespace : K8s namespace to deploy. chart : Chart release. values : path pointing to the values file created above. Create a directory with the {{ chart_name }} . mkdir {{ chart_name }} Get a copy of the chart values inside that directory. helm inspect values {{ package_name }} > {{ chart_name }} /values.yaml Edit the values.yaml file according to the chart documentation. Be careful becase some charts specify the docker image version in the name. Comment out that line because upgrading the chart version without upgrading the image tag can break the service. Run helmfile deps to update the lock file. Run helmfile diff to check the changes. Run helmfile apply to apply the changes. Keep charts updated \u2691 Updating charts with helmfile is easy as long as you don't use environments, you run helmfile deps , then helmfile diff and finally helmfile apply . The tricky business comes when you want to use environments to reuse your helmfile code and don't repeat yourself. This is my suggested workflow, I've opened an issue to see if the developers agree with it: As of today, helmfile doesn't support lock files per environment , that means that the lock file needs to be shared by all of them. At a first sight this is a good idea, because it forces us to have the same versions of the charts in all the environments. The problem comes when you want to upgrade the charts of staging, test that they work and then apply the same changes in production. You'd start the process by running helmfile deps , which will read the helmfiles and update the lock file to the latest version. From this point on you need to be careful on executing the next steps in order so as not to break production. Tell your team that you're going to do the update operation, so that they don't try to run helmfile against any environment of the cluster. Run helmfile --environment=staging diff to review the changes to be introduced. To be able to see the differences of long diff files, you can filter it with egrep . helmfile diff | egrep -A20 -B20 \"^.{5}(\\-|\\+)\" It will show you all the changed lines with the 20 previous and next ones. * Once you agree on them, run helmfile --environment=staging apply to apply them. * Wait 20 minutes to see if the monitoring system or your fellow partners start yelling at you. * If something breaks up, try to fix it up, if you see it's going to delay you to the point that you're not going to be able to finish the upgrade in your working day, it's better to revert back to the working version of that chart and move on with the next steps. Keep in mind that since you run the apply to the last of the steps of this long process, the team is blocked by you. So prioritize to commit the next stable version to the version control repository. * Once you've checked that all the desired upgrades are working, run helmfile --environment=production diff . This review should be quick, as it should be the same as the staging one. * Now upgrade the production environment with helmfile --environment=production apply . * Wait another 20 minutes and check that everything is working. * Make a commit with the new lockfile and upload it to the version control repository. If you want the team to be involved in the review process, you can open a PR with the lock file updated with the WIP state, and upload the relevant diff of staging and production, let the discussion end and then run the apply on staging and then on production if everything goes well. Another ugly solution that I thought was to have a lockfile per environment, and let a Makefile manage them, for example, copying it to helmfile.lock before running any command. Uninstall charts \u2691 Helmfile still doesn't remove charts if you remove them from your helmfile.yaml . To remove them you have to either set installed: false in the release candidate and execute helmfile apply or delete the release definition from your helmfile and remove it using standard helm commands. Force the reinstallation of everything \u2691 If you manually changed the deployed resources and want to reset the cluster state to the helmfile one, use helmfile sync which will reinstall all the releases. Multi-environment project structure \u2691 helmfile can handle environments with many different project structures. Such as the next one: \u251c\u2500\u2500 README.md \u251c\u2500\u2500 helmfile.yaml \u251c\u2500\u2500 vars \u2502 \u251c\u2500\u2500 production_secrets.yaml \u2502 \u251c\u2500\u2500 production_values.yaml \u2502 \u251c\u2500\u2500 default_secrets.yaml \u2502 \u2514\u2500\u2500 default_values.yaml \u251c\u2500\u2500 charts \u2502 \u251c\u2500\u2500 local_defined_chart_1 \u2502 \u2514\u2500\u2500 local_defined_chart_2 \u251c\u2500\u2500 templates \u2502 \u251c\u2500\u2500 environments.yaml \u2502 \u2514\u2500\u2500 templates.yaml \u251c\u2500\u2500 base \u2502 \u251c\u2500\u2500 README.md \u2502 \u251c\u2500\u2500 helmfile.yaml \u2502 \u251c\u2500\u2500 helmfile.lock \u2502 \u251c\u2500\u2500 repos.yaml \u2502 \u251c\u2500\u2500 chart_1 \u2502 \u2502 \u251c\u2500\u2500 secrets.yaml \u2502 \u2502 \u251c\u2500\u2500 values.yaml \u2502 \u2502 \u251c\u2500\u2500 production_secrets.yaml \u2502 \u2502 \u251c\u2500\u2500 production_values.yaml \u2502 \u2502 \u251c\u2500\u2500 default_secrets.yaml \u2502 \u2502 \u2514\u2500\u2500 default_values.yaml \u2502 \u2514\u2500\u2500 chart_2 \u2502 \u251c\u2500\u2500 secrets.yaml \u2502 \u251c\u2500\u2500 values.yaml \u2502 \u251c\u2500\u2500 production_secrets.yaml \u2502 \u251c\u2500\u2500 production_values.yaml \u2502 \u251c\u2500\u2500 default_secrets.yaml \u2502 \u2514\u2500\u2500 default_values.yaml \u2514\u2500\u2500 service_1 \u251c\u2500\u2500 README.md \u251c\u2500\u2500 helmfile.yaml \u251c\u2500\u2500 helmfile.lock \u251c\u2500\u2500 repos.yaml \u251c\u2500\u2500 chart_1 \u2502 \u251c\u2500\u2500 secrets.yaml \u2502 \u251c\u2500\u2500 values.yaml \u2502 \u251c\u2500\u2500 production_secrets.yaml \u2502 \u251c\u2500\u2500 production_values.yaml \u2502 \u251c\u2500\u2500 default_secrets.yaml \u2502 \u2514\u2500\u2500 default_values.yaml \u2514\u2500\u2500 chart_2 \u251c\u2500\u2500 secrets.yaml \u251c\u2500\u2500 values.yaml \u251c\u2500\u2500 production_secrets.yaml \u251c\u2500\u2500 production_values.yaml \u251c\u2500\u2500 default_secrets.yaml \u2514\u2500\u2500 default_values.yaml Where: There is a general README.md that introduces the repository. Optionally there could be a helmfile.yaml file at the root with a glob pattern so that it's easy to run commands on all children helmfiles. helmfiles : - ./*/helmfile.yaml * There is a vars directory to store the variables and secrets shared by the charts that belong to different services. * There is a templates directory to store the helmfile code to reuse through templates and layering . * The project structure is defined by the services hosted in the Kubernetes cluster. Each service contains: A README.md to document the service implementation. A helmfile.yaml file to configure the service charts. A helmfile.lock to lock the versions of the service charts. A repos.yaml to define the repositories to fetch the charts from. One or more chart directories that contain the environment specific and shared chart values and secrets. There is a base service that manages all the charts required to keep the cluster running, such as the ingress, csi, cni or the cluster-autoscaler. Using helmfile environments \u2691 To customize the contents of a helmfile.yaml or values.yaml file per environment, add them under the environments key in the helmfile.yaml : environments : default : production : The environment name defaults to default , that is, helmfile sync implies the default environment. So it's a good idea to use staging as default to be more robust against human errors. If you want to specify a non-default environment, provide a --environment NAME flag to helmfile like helmfile --environment production sync . In the environments definition we'll load the values and secrets from the vars directory with the next snippet. environments : default : secrets : - ../vars/default_secrets.yaml values : - ../vars/default_values.yaml production : secrets : - ../vars/production_secrets.yaml values : - ../vars/production_values.yaml As this snippet is going to be repeated on every helmfile.yaml we'll use a state layering for it . To install a release only in one environment use: environments : default : production : --- releases : - name : newrelic-agent installed : {{ eq .Environment.Name \"production\" | toYaml }} # snip Using environment specific variables \u2691 Environment Values allows you to inject a set of values specific to the selected environment, into values.yaml templates or helmfile.yaml files. Use it to inject common values from the environment to multiple values files, to make your configuration DRY. Suppose you have three files helmfile.yaml, production.yaml and values.yaml.gotmpl File: helmfile.yaml environments : production : values : - production.yaml --- releases : - name : myapp values : - values.yaml.gotmpl File: production.yaml domain : prod.example.com File: values.yaml.gotmpl domain : {{ .Values | get \"domain\" \"dev.example.com\" }} Sadly you can't use templates in the secrets files , so you'll need to repeat the code. Loading the chart variables and secrets \u2691 For each chart definition in the helmfile.yaml we need to load it's secrets and values. We could use the next snippet: - name : chart_1 values : - ./chart_1/values.yaml - ./chart_1/{{ Environment.Name }}_values.yaml secrets : - ./chart_1/secrets.yaml - ./chart_1/{{ Environment.Name }}_secrets.yaml This assumes that the environment variable is set, as it's going to be shared by all the helmfiles.yaml you can add it to the vars files: File: vars/production_values.yaml environment : production File: vars/default_values.yaml environment : staging Instead of .Environment.Name , in theory you could have used .Vars | get \"environment\" , which could have prevented the variables and secrets of the default environment will need to be called default_values.yaml , and default_secrets.yaml , which is misleading. But you can't use .Values in the helmfile.yaml as it's not loaded when the file is parsed, and you get an error. A solution would be to layer the helmfile state files but I wasn't able to make it work. Avoiding code repetition \u2691 Besides environments, helmfile gives other useful tricks to prevent the illness of code repetition. Using release templates \u2691 For each chart in a helmfile.yaml we're going to repeat the values and secrets sections, to avoid it, we can use release templates: templates : default : &default # This prevents helmfile exiting when it encounters a missing file # Valid values are \"Error\", \"Warn\", \"Info\", \"Debug\". The default is \"Error\" # Use \"Debug\" to make missing files errors invisible at the default log level(--log-level=INFO) missingFileHandler : Warn values : - {{ ` {{ .Release.Name }} ` }} /values.yaml - {{ ` {{ .Release.Name }} ` }} /{{`{{ .Values | get \"environment\" }}`}}.yaml secrets : - config/{{`{{ .Release.Name }}`}}/secrets.yaml - config/{{`{{ .Release.Name }}`}}/{{`{{ .Values | get \"environment\" }}`}}-secrets.yaml releases : - name : chart_1 chart : stable/chart_1 << : *default - name : chart_2 chart : stable/chart_2 << : *default If you're not familiar with YAML anchors, &default names the block, then *default references it. The <<: syntax says to \"extend\" (merge) that reference into the current tree. The missingFileHandler: Warn field is necessary if you don't need all the values and secret files, but want to use the same definition for all charts. {{` {{ .Release.Name }} `}} is surrounded by {{` and }}` so as not to be executed on the loading time of helmfile.yaml . We need to defer it until each release is actually processed by the helmfile command, such as diff or apply . For more information see this issue . Layering the state \u2691 You may occasionally end up with many helmfiles that shares common parts like which repositories to use, and which release to be bundled by default. Use Layering to extract the common parts into a dedicated library helmfiles, so that each helmfile becomes DRY. Let's assume that your code looks like: File: helmfile.yaml bases : - environments.yaml releases : - name : metricbeat chart : stable/metricbeat - name : myapp chart : mychart File: environments.yaml environments : development : production : At run time, bases in your helmfile.yaml are evaluated to produce: --- # environments.yaml environments : development : production : --- # helmfile.yaml releases : - name : myapp chart : mychart - name : metricbeat chart : stable/metricbeat Finally the resulting YAML documents are merged in the order of occurrence, so that your helmfile.yaml becomes: environments : development : production : releases : - name : metricbeat chart : stable/metricbeat - name : myapp chart : mychart Using this concept, we can reuse the environments section as: File: vars/environments.yaml environments : default : secrets : - ../vars/staging-secrets.yaml values : - ../vars/staging-values.yaml production : secrets : - ../vars/production-secrets.yaml values : - ../vars/production-values.yaml And the default release templates as: File: templates/templates.yaml templates : default : &default values : - {{ ` {{ .Release.Name }} ` }} /values.yaml - {{ ` {{ .Release.Name }} ` }} /{{`{{ .Values | get \"environment\" }}`}}.yaml secrets : - config/{{`{{ .Release.Name }}`}}/secrets.yaml - config/{{`{{ .Release.Name }}`}}/{{`{{ .Values | get \"environment\" }}`}}-secrets.yaml So the service's helmfile.yaml turns out to be: bases : - ../templates/environments.yaml - ../templates/templates.yaml releases : - name : chart_1 chart : stable/chart_1 << : *default - name : chart_2 chart : stable/chart_2 << : *default Much shorter and simple. Managing dependencies \u2691 Helmfile support concurrency with the option --concurrency=N so we can take advantage of it and improve our deployment speed, but to ensure it works as expected we have to define the dependencies among charts. For example, if an application needs a database, it has to be deployed before hand. releases : - name : vpn-dashboard chart : incubator/raw needs : - monitoring/prometheus-operator - name : prometheus-operator namespace : monitoring chart : prometheus-community/kube-prometheus-stack Debugging helmfile \u2691 Error: \"release-name\" has no deployed releases \u2691 This may happen when you try to install a chart and it fails. The best solution until this issue is resolved is to use helm delete --purge {{ release-name }} and then apply again. Error: failed to download \"stable/metrics-server\" (hint: running helm repo update may help) \u2691 I had this issue if verify: true in the helmfile.yaml file. Comment it or set it to false. Links \u2691 Git","title":"Helmfile"},{"location":"devops/helmfile/#installation","text":"Helmfile is not yet in the distribution package managers, so you'll need to install it manually. Gather the latest release number . wget {{ bin_url }} -O helmfile_linux_amd64 chmod +x helmfile_linux_amd64 mv helmfile_linux_amd64 ~/.local/bin/helmfile","title":"Installation"},{"location":"devops/helmfile/#usage","text":"","title":"Usage"},{"location":"devops/helmfile/#how-to-deploy-a-new-chart","text":"When we want to add a new chart, the workflow would be: Run helmfile deps && helmfile diff to check that your existing charts are updated, if they are not, run helmfile apply . Configure the release in helmfile.yaml specifying: name : Deployment name. namespace : K8s namespace to deploy. chart : Chart release. values : path pointing to the values file created above. Create a directory with the {{ chart_name }} . mkdir {{ chart_name }} Get a copy of the chart values inside that directory. helm inspect values {{ package_name }} > {{ chart_name }} /values.yaml Edit the values.yaml file according to the chart documentation. Be careful becase some charts specify the docker image version in the name. Comment out that line because upgrading the chart version without upgrading the image tag can break the service. Run helmfile deps to update the lock file. Run helmfile diff to check the changes. Run helmfile apply to apply the changes.","title":"How to deploy a new chart"},{"location":"devops/helmfile/#keep-charts-updated","text":"Updating charts with helmfile is easy as long as you don't use environments, you run helmfile deps , then helmfile diff and finally helmfile apply . The tricky business comes when you want to use environments to reuse your helmfile code and don't repeat yourself. This is my suggested workflow, I've opened an issue to see if the developers agree with it: As of today, helmfile doesn't support lock files per environment , that means that the lock file needs to be shared by all of them. At a first sight this is a good idea, because it forces us to have the same versions of the charts in all the environments. The problem comes when you want to upgrade the charts of staging, test that they work and then apply the same changes in production. You'd start the process by running helmfile deps , which will read the helmfiles and update the lock file to the latest version. From this point on you need to be careful on executing the next steps in order so as not to break production. Tell your team that you're going to do the update operation, so that they don't try to run helmfile against any environment of the cluster. Run helmfile --environment=staging diff to review the changes to be introduced. To be able to see the differences of long diff files, you can filter it with egrep . helmfile diff | egrep -A20 -B20 \"^.{5}(\\-|\\+)\" It will show you all the changed lines with the 20 previous and next ones. * Once you agree on them, run helmfile --environment=staging apply to apply them. * Wait 20 minutes to see if the monitoring system or your fellow partners start yelling at you. * If something breaks up, try to fix it up, if you see it's going to delay you to the point that you're not going to be able to finish the upgrade in your working day, it's better to revert back to the working version of that chart and move on with the next steps. Keep in mind that since you run the apply to the last of the steps of this long process, the team is blocked by you. So prioritize to commit the next stable version to the version control repository. * Once you've checked that all the desired upgrades are working, run helmfile --environment=production diff . This review should be quick, as it should be the same as the staging one. * Now upgrade the production environment with helmfile --environment=production apply . * Wait another 20 minutes and check that everything is working. * Make a commit with the new lockfile and upload it to the version control repository. If you want the team to be involved in the review process, you can open a PR with the lock file updated with the WIP state, and upload the relevant diff of staging and production, let the discussion end and then run the apply on staging and then on production if everything goes well. Another ugly solution that I thought was to have a lockfile per environment, and let a Makefile manage them, for example, copying it to helmfile.lock before running any command.","title":"Keep charts updated"},{"location":"devops/helmfile/#uninstall-charts","text":"Helmfile still doesn't remove charts if you remove them from your helmfile.yaml . To remove them you have to either set installed: false in the release candidate and execute helmfile apply or delete the release definition from your helmfile and remove it using standard helm commands.","title":"Uninstall charts"},{"location":"devops/helmfile/#force-the-reinstallation-of-everything","text":"If you manually changed the deployed resources and want to reset the cluster state to the helmfile one, use helmfile sync which will reinstall all the releases.","title":"Force the reinstallation of everything"},{"location":"devops/helmfile/#multi-environment-project-structure","text":"helmfile can handle environments with many different project structures. Such as the next one: \u251c\u2500\u2500 README.md \u251c\u2500\u2500 helmfile.yaml \u251c\u2500\u2500 vars \u2502 \u251c\u2500\u2500 production_secrets.yaml \u2502 \u251c\u2500\u2500 production_values.yaml \u2502 \u251c\u2500\u2500 default_secrets.yaml \u2502 \u2514\u2500\u2500 default_values.yaml \u251c\u2500\u2500 charts \u2502 \u251c\u2500\u2500 local_defined_chart_1 \u2502 \u2514\u2500\u2500 local_defined_chart_2 \u251c\u2500\u2500 templates \u2502 \u251c\u2500\u2500 environments.yaml \u2502 \u2514\u2500\u2500 templates.yaml \u251c\u2500\u2500 base \u2502 \u251c\u2500\u2500 README.md \u2502 \u251c\u2500\u2500 helmfile.yaml \u2502 \u251c\u2500\u2500 helmfile.lock \u2502 \u251c\u2500\u2500 repos.yaml \u2502 \u251c\u2500\u2500 chart_1 \u2502 \u2502 \u251c\u2500\u2500 secrets.yaml \u2502 \u2502 \u251c\u2500\u2500 values.yaml \u2502 \u2502 \u251c\u2500\u2500 production_secrets.yaml \u2502 \u2502 \u251c\u2500\u2500 production_values.yaml \u2502 \u2502 \u251c\u2500\u2500 default_secrets.yaml \u2502 \u2502 \u2514\u2500\u2500 default_values.yaml \u2502 \u2514\u2500\u2500 chart_2 \u2502 \u251c\u2500\u2500 secrets.yaml \u2502 \u251c\u2500\u2500 values.yaml \u2502 \u251c\u2500\u2500 production_secrets.yaml \u2502 \u251c\u2500\u2500 production_values.yaml \u2502 \u251c\u2500\u2500 default_secrets.yaml \u2502 \u2514\u2500\u2500 default_values.yaml \u2514\u2500\u2500 service_1 \u251c\u2500\u2500 README.md \u251c\u2500\u2500 helmfile.yaml \u251c\u2500\u2500 helmfile.lock \u251c\u2500\u2500 repos.yaml \u251c\u2500\u2500 chart_1 \u2502 \u251c\u2500\u2500 secrets.yaml \u2502 \u251c\u2500\u2500 values.yaml \u2502 \u251c\u2500\u2500 production_secrets.yaml \u2502 \u251c\u2500\u2500 production_values.yaml \u2502 \u251c\u2500\u2500 default_secrets.yaml \u2502 \u2514\u2500\u2500 default_values.yaml \u2514\u2500\u2500 chart_2 \u251c\u2500\u2500 secrets.yaml \u251c\u2500\u2500 values.yaml \u251c\u2500\u2500 production_secrets.yaml \u251c\u2500\u2500 production_values.yaml \u251c\u2500\u2500 default_secrets.yaml \u2514\u2500\u2500 default_values.yaml Where: There is a general README.md that introduces the repository. Optionally there could be a helmfile.yaml file at the root with a glob pattern so that it's easy to run commands on all children helmfiles. helmfiles : - ./*/helmfile.yaml * There is a vars directory to store the variables and secrets shared by the charts that belong to different services. * There is a templates directory to store the helmfile code to reuse through templates and layering . * The project structure is defined by the services hosted in the Kubernetes cluster. Each service contains: A README.md to document the service implementation. A helmfile.yaml file to configure the service charts. A helmfile.lock to lock the versions of the service charts. A repos.yaml to define the repositories to fetch the charts from. One or more chart directories that contain the environment specific and shared chart values and secrets. There is a base service that manages all the charts required to keep the cluster running, such as the ingress, csi, cni or the cluster-autoscaler.","title":"Multi-environment project structure"},{"location":"devops/helmfile/#using-helmfile-environments","text":"To customize the contents of a helmfile.yaml or values.yaml file per environment, add them under the environments key in the helmfile.yaml : environments : default : production : The environment name defaults to default , that is, helmfile sync implies the default environment. So it's a good idea to use staging as default to be more robust against human errors. If you want to specify a non-default environment, provide a --environment NAME flag to helmfile like helmfile --environment production sync . In the environments definition we'll load the values and secrets from the vars directory with the next snippet. environments : default : secrets : - ../vars/default_secrets.yaml values : - ../vars/default_values.yaml production : secrets : - ../vars/production_secrets.yaml values : - ../vars/production_values.yaml As this snippet is going to be repeated on every helmfile.yaml we'll use a state layering for it . To install a release only in one environment use: environments : default : production : --- releases : - name : newrelic-agent installed : {{ eq .Environment.Name \"production\" | toYaml }} # snip","title":"Using helmfile environments"},{"location":"devops/helmfile/#using-environment-specific-variables","text":"Environment Values allows you to inject a set of values specific to the selected environment, into values.yaml templates or helmfile.yaml files. Use it to inject common values from the environment to multiple values files, to make your configuration DRY. Suppose you have three files helmfile.yaml, production.yaml and values.yaml.gotmpl File: helmfile.yaml environments : production : values : - production.yaml --- releases : - name : myapp values : - values.yaml.gotmpl File: production.yaml domain : prod.example.com File: values.yaml.gotmpl domain : {{ .Values | get \"domain\" \"dev.example.com\" }} Sadly you can't use templates in the secrets files , so you'll need to repeat the code.","title":"Using environment specific variables"},{"location":"devops/helmfile/#loading-the-chart-variables-and-secrets","text":"For each chart definition in the helmfile.yaml we need to load it's secrets and values. We could use the next snippet: - name : chart_1 values : - ./chart_1/values.yaml - ./chart_1/{{ Environment.Name }}_values.yaml secrets : - ./chart_1/secrets.yaml - ./chart_1/{{ Environment.Name }}_secrets.yaml This assumes that the environment variable is set, as it's going to be shared by all the helmfiles.yaml you can add it to the vars files: File: vars/production_values.yaml environment : production File: vars/default_values.yaml environment : staging Instead of .Environment.Name , in theory you could have used .Vars | get \"environment\" , which could have prevented the variables and secrets of the default environment will need to be called default_values.yaml , and default_secrets.yaml , which is misleading. But you can't use .Values in the helmfile.yaml as it's not loaded when the file is parsed, and you get an error. A solution would be to layer the helmfile state files but I wasn't able to make it work.","title":"Loading the chart variables and secrets"},{"location":"devops/helmfile/#avoiding-code-repetition","text":"Besides environments, helmfile gives other useful tricks to prevent the illness of code repetition.","title":"Avoiding code repetition"},{"location":"devops/helmfile/#using-release-templates","text":"For each chart in a helmfile.yaml we're going to repeat the values and secrets sections, to avoid it, we can use release templates: templates : default : &default # This prevents helmfile exiting when it encounters a missing file # Valid values are \"Error\", \"Warn\", \"Info\", \"Debug\". The default is \"Error\" # Use \"Debug\" to make missing files errors invisible at the default log level(--log-level=INFO) missingFileHandler : Warn values : - {{ ` {{ .Release.Name }} ` }} /values.yaml - {{ ` {{ .Release.Name }} ` }} /{{`{{ .Values | get \"environment\" }}`}}.yaml secrets : - config/{{`{{ .Release.Name }}`}}/secrets.yaml - config/{{`{{ .Release.Name }}`}}/{{`{{ .Values | get \"environment\" }}`}}-secrets.yaml releases : - name : chart_1 chart : stable/chart_1 << : *default - name : chart_2 chart : stable/chart_2 << : *default If you're not familiar with YAML anchors, &default names the block, then *default references it. The <<: syntax says to \"extend\" (merge) that reference into the current tree. The missingFileHandler: Warn field is necessary if you don't need all the values and secret files, but want to use the same definition for all charts. {{` {{ .Release.Name }} `}} is surrounded by {{` and }}` so as not to be executed on the loading time of helmfile.yaml . We need to defer it until each release is actually processed by the helmfile command, such as diff or apply . For more information see this issue .","title":"Using release templates"},{"location":"devops/helmfile/#layering-the-state","text":"You may occasionally end up with many helmfiles that shares common parts like which repositories to use, and which release to be bundled by default. Use Layering to extract the common parts into a dedicated library helmfiles, so that each helmfile becomes DRY. Let's assume that your code looks like: File: helmfile.yaml bases : - environments.yaml releases : - name : metricbeat chart : stable/metricbeat - name : myapp chart : mychart File: environments.yaml environments : development : production : At run time, bases in your helmfile.yaml are evaluated to produce: --- # environments.yaml environments : development : production : --- # helmfile.yaml releases : - name : myapp chart : mychart - name : metricbeat chart : stable/metricbeat Finally the resulting YAML documents are merged in the order of occurrence, so that your helmfile.yaml becomes: environments : development : production : releases : - name : metricbeat chart : stable/metricbeat - name : myapp chart : mychart Using this concept, we can reuse the environments section as: File: vars/environments.yaml environments : default : secrets : - ../vars/staging-secrets.yaml values : - ../vars/staging-values.yaml production : secrets : - ../vars/production-secrets.yaml values : - ../vars/production-values.yaml And the default release templates as: File: templates/templates.yaml templates : default : &default values : - {{ ` {{ .Release.Name }} ` }} /values.yaml - {{ ` {{ .Release.Name }} ` }} /{{`{{ .Values | get \"environment\" }}`}}.yaml secrets : - config/{{`{{ .Release.Name }}`}}/secrets.yaml - config/{{`{{ .Release.Name }}`}}/{{`{{ .Values | get \"environment\" }}`}}-secrets.yaml So the service's helmfile.yaml turns out to be: bases : - ../templates/environments.yaml - ../templates/templates.yaml releases : - name : chart_1 chart : stable/chart_1 << : *default - name : chart_2 chart : stable/chart_2 << : *default Much shorter and simple.","title":"Layering the state"},{"location":"devops/helmfile/#managing-dependencies","text":"Helmfile support concurrency with the option --concurrency=N so we can take advantage of it and improve our deployment speed, but to ensure it works as expected we have to define the dependencies among charts. For example, if an application needs a database, it has to be deployed before hand. releases : - name : vpn-dashboard chart : incubator/raw needs : - monitoring/prometheus-operator - name : prometheus-operator namespace : monitoring chart : prometheus-community/kube-prometheus-stack","title":"Managing dependencies"},{"location":"devops/helmfile/#debugging-helmfile","text":"","title":"Debugging helmfile"},{"location":"devops/helmfile/#error-release-name-has-no-deployed-releases","text":"This may happen when you try to install a chart and it fails. The best solution until this issue is resolved is to use helm delete --purge {{ release-name }} and then apply again.","title":"Error: \"release-name\" has no deployed releases"},{"location":"devops/helmfile/#error-failed-to-download-stablemetrics-server-hint-running-helm-repo-update-may-help","text":"I had this issue if verify: true in the helmfile.yaml file. Comment it or set it to false.","title":"Error: failed to download \"stable/metrics-server\" (hint: running helm repo update may help)"},{"location":"devops/helmfile/#links","text":"Git","title":"Links"},{"location":"devops/markdownlint/","text":"markdownlint-cli is a command line interface for the markdownlint Node.js style checker and lint tool for Markdown/CommonMark files. I've evaluated these other projects ( 1 , 2 , but their configuration is less user friendly and are less maintained. You can use this cookiecutter template to create a python project with markdownlint already configured. Installation \u2691 npm install -g markdownlint-cli Configuration \u2691 To configure your project , add a .markdownlint.json in your project root directory, or in any parent. I've opened an issue to see if they are going to support pyproject.toml to save the configuration. Check the styles examples . Go to the rules document if you ever need to check more information on a specific rule. You can use it both with: The Vim through the ALE plugin . Pre-commit : File: .pre-commit-config.yaml - repo : https://github.com/igorshubovych/markdownlint-cli rev : v0.23.2 hooks : - id : markdownlint Troubleshooting \u2691 Until the #2926 PR is merged you need to change the let l:pattern=.* file to make the linting work to: File: ~/.vim/bundle/ale/autoload/ale/handlers let l :pattern = ': \\?\\(\\d*\\):\\? \\(MD\\d\\{3}\\)\\(\\/\\)\\([A-Za-z0-9-\\/]\\+\\)\\(.*\\)$' References \u2691 Git","title":"Markdownlint"},{"location":"devops/markdownlint/#installation","text":"npm install -g markdownlint-cli","title":"Installation"},{"location":"devops/markdownlint/#configuration","text":"To configure your project , add a .markdownlint.json in your project root directory, or in any parent. I've opened an issue to see if they are going to support pyproject.toml to save the configuration. Check the styles examples . Go to the rules document if you ever need to check more information on a specific rule. You can use it both with: The Vim through the ALE plugin . Pre-commit : File: .pre-commit-config.yaml - repo : https://github.com/igorshubovych/markdownlint-cli rev : v0.23.2 hooks : - id : markdownlint","title":"Configuration"},{"location":"devops/markdownlint/#troubleshooting","text":"Until the #2926 PR is merged you need to change the let l:pattern=.* file to make the linting work to: File: ~/.vim/bundle/ale/autoload/ale/handlers let l :pattern = ': \\?\\(\\d*\\):\\? \\(MD\\d\\{3}\\)\\(\\/\\)\\([A-Za-z0-9-\\/]\\+\\)\\(.*\\)$'","title":"Troubleshooting"},{"location":"devops/markdownlint/#references","text":"Git","title":"References"},{"location":"devops/mypy/","text":"Mypy is an optional static type checker for Python that aims to combine the benefits of dynamic (or \"duck\") typing and static typing. Mypy combines the expressive power and convenience of Python with a powerful type system and compile-time type checking. You can use this cookiecutter template to create a python project with mypy already configured. Installation \u2691 pip install mypy Configuration \u2691 Mypy configuration is saved in the mypy.ini file, and they don't yet support pyproject.toml . File: mypy.ini [mypy] show_error_codes = True follow_imports = silent strict_optional = True warn_redundant_casts = True warn_unused_ignores = True disallow_any_generics = True check_untyped_defs = True no_implicit_reexport = True warn_unused_configs = True disallow_subclassing_any = True disallow_incomplete_defs = True disallow_untyped_decorators = True disallow_untyped_calls = True # for strict mypy: (this is the tricky one :-)) disallow_untyped_defs = True You can use it both with: Pre-commit : File: .pre-commit-config.yaml repos : - repo : https://github.com/pre-commit/mirrors-mypy rev : v0.782 hooks : - name : Run mypy static analysis tool id : mypy Github Actions: File: .github/workflows/lint.yml name : Lint on : [ push , pull_request ] jobs : Mypy : runs-on : ubuntu-latest name : Mypy steps : - uses : actions/checkout@v1 - name : Set up Python 3.7 uses : actions/setup-python@v1 with : python-version : 3.7 - name : Install Dependencies run : pip install mypy - name : mypy run : mypy Ignore one line \u2691 Add # type: ignore to the line you want to skip. Troubleshooting \u2691 Module X has no attribute Y \u2691 If you're importing objects from your own module, you need to tell mypy that those objects are available. To do so in the __init__.py of your module, list them under the __all__ variable . File: init .py from .model import Entity __all__ = [ \"Entity\" , ] [W0707: Consider explicitly re-raising using the 'from' \u2691 keyword]( https://blog.ram.rachum.com/post/621791438475296768/improving-python-exception-chaining-with ) The error can be raised by two cases. An exception was raised, we were handling it, and something went wrong in the process of handling it. An exception was raised, and we decided to replace it with a different exception that will make more sense to whoever called this code. try : self . connection , _ = self . sock . accept () except socket . timeout as error : raise IPCException ( 'The socket timed out' ) from error The error bit at the end tells Python: The IPCException that we\u2019re raising is just a friendlier version of the socket.timeout that we just caught. When we run that code and reach that exception, the traceback is going to look like this: Traceback (most recent call last): File \"foo.py\", line 19, in self.connection, _ = self.sock.accept() File \"foo.py\", line 7, in accept raise socket.timeout socket.timeout The above exception was the direct cause of the following exception: Traceback (most recent call last): File \"foo.py\", line 21, in raise IPCException('The socket timed out') from e IPCException: The socket timed out The The above exception was the direct cause of the following exception: part tells us that we are in the second case. If you were dealing with the first one, the message between the two tracebacks would be: During handling of the above exception, another exception occurred: Issues \u2691 Incompatible return value with TypeVar : search for 10003 in repository-pattern and fix the type: ignore . References \u2691 Docs Git Homepage","title":"Mypy"},{"location":"devops/mypy/#installation","text":"pip install mypy","title":"Installation"},{"location":"devops/mypy/#configuration","text":"Mypy configuration is saved in the mypy.ini file, and they don't yet support pyproject.toml . File: mypy.ini [mypy] show_error_codes = True follow_imports = silent strict_optional = True warn_redundant_casts = True warn_unused_ignores = True disallow_any_generics = True check_untyped_defs = True no_implicit_reexport = True warn_unused_configs = True disallow_subclassing_any = True disallow_incomplete_defs = True disallow_untyped_decorators = True disallow_untyped_calls = True # for strict mypy: (this is the tricky one :-)) disallow_untyped_defs = True You can use it both with: Pre-commit : File: .pre-commit-config.yaml repos : - repo : https://github.com/pre-commit/mirrors-mypy rev : v0.782 hooks : - name : Run mypy static analysis tool id : mypy Github Actions: File: .github/workflows/lint.yml name : Lint on : [ push , pull_request ] jobs : Mypy : runs-on : ubuntu-latest name : Mypy steps : - uses : actions/checkout@v1 - name : Set up Python 3.7 uses : actions/setup-python@v1 with : python-version : 3.7 - name : Install Dependencies run : pip install mypy - name : mypy run : mypy","title":"Configuration"},{"location":"devops/mypy/#ignore-one-line","text":"Add # type: ignore to the line you want to skip.","title":"Ignore one line"},{"location":"devops/mypy/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"devops/mypy/#module-x-has-no-attribute-y","text":"If you're importing objects from your own module, you need to tell mypy that those objects are available. To do so in the __init__.py of your module, list them under the __all__ variable . File: init .py from .model import Entity __all__ = [ \"Entity\" , ]","title":"Module X has no attribute Y"},{"location":"devops/mypy/#w0707-consider-explicitly-re-raising-using-the-from","text":"keyword]( https://blog.ram.rachum.com/post/621791438475296768/improving-python-exception-chaining-with ) The error can be raised by two cases. An exception was raised, we were handling it, and something went wrong in the process of handling it. An exception was raised, and we decided to replace it with a different exception that will make more sense to whoever called this code. try : self . connection , _ = self . sock . accept () except socket . timeout as error : raise IPCException ( 'The socket timed out' ) from error The error bit at the end tells Python: The IPCException that we\u2019re raising is just a friendlier version of the socket.timeout that we just caught. When we run that code and reach that exception, the traceback is going to look like this: Traceback (most recent call last): File \"foo.py\", line 19, in self.connection, _ = self.sock.accept() File \"foo.py\", line 7, in accept raise socket.timeout socket.timeout The above exception was the direct cause of the following exception: Traceback (most recent call last): File \"foo.py\", line 21, in raise IPCException('The socket timed out') from e IPCException: The socket timed out The The above exception was the direct cause of the following exception: part tells us that we are in the second case. If you were dealing with the first one, the message between the two tracebacks would be: During handling of the above exception, another exception occurred:","title":"[W0707: Consider explicitly re-raising using the 'from'"},{"location":"devops/mypy/#issues","text":"Incompatible return value with TypeVar : search for 10003 in repository-pattern and fix the type: ignore .","title":"Issues"},{"location":"devops/mypy/#references","text":"Docs Git Homepage","title":"References"},{"location":"devops/pip_tools/","text":"Pip-tools is a set of command line tools to help you keep your pip-based packages fresh, even when you've pinned them. For stability reasons it's a good idea to hardcode the dependencies versions. Furthermore, safety needs them to work properly. You can use this cookiecutter template to create a python project with pip-tools already configured. We've got three places where the dependencies are defined: setup.py should declare the loosest possible dependency versions that are still workable. Its job is to say what a particular package can work with. requirements.txt is a deployment manifest that defines an entire installation job, and shouldn't be thought of as tied to any one package. Its job is to declare an exhaustive list of all the necessary packages to make a deployment work. requirements-dev.txt Adds the dependencies required for the development of the program. Content of examples may be outdated An updated version of setup.py and requirements-dev.in can be found in the cookiecutter template . With pip-tools, the dependency management is trivial. Install the tool: pip install pip-tools Set the general dependencies in the setup.py install_requires . Generate the requirements.txt file: pip-compile -U --allow-unsafe ` The -U flag will try to upgrade the dependencies, and --allow-unsafe will let you manage the setuptools and pip dependencies. Add the additional testing dependencies in the requirements-dev.in file. File: requirements-dev.in -c requirements.txt pip-tools factory_boy pytest pytest-cov The -c line will make pip-compile look at that file for compatibility, but it won't duplicate those requirements in the requirements-dev.txt . Compile the development requirements requirements-dev.txt with pip-compile dev-requirements.in . If you have another requirements.txt for the mkdocs documentation, run pip-compile docs/requirements.txt . To sync the virtualenv libraries with the files , use sync : python - m piptools sync requirements . txt requirements - dev . txt To uninstall all pip packages use pip freeze | xargs pip uninstall -y Trigger hooks: Pre-commit: File: .pre-commit-config.yaml - repo : https://github.com/jazzband/pip-tools rev : 5.0.0 hooks : - name : Build requirements.txt id : pip-compile - name : Build dev-requirements.txt id : pip-compile args : [ 'dev-requirements.in' ] - name : Build mkdocs requirements.txt id : pip-compile args : [ 'docs/requirements.txt' ] pip-tools generates different results in the CI than in the development environment breaking the CI without an easy way to fix it. Therefore it should be run by the developers periodically. References \u2691 Git","title":"Pip-tools"},{"location":"devops/pip_tools/#references","text":"Git","title":"References"},{"location":"devops/proselint/","text":"Proselint is another linter for prose. Installation \u2691 pip install proselint Configuration \u2691 It can be configured through the ~/.config/proselint/config file, such as: { \"checks\" : { \"typography.diacritical_marks\" : false } } The Vim through the ALE plugin . Pre-commit : - repo : https://github.com/amperser/proselint/ rev : 0.10.2 hooks : - id : proselint exclude : LICENSE|requirements files : \\.(md|mdown|markdown)$ References \u2691 Git","title":"Proselint"},{"location":"devops/proselint/#installation","text":"pip install proselint","title":"Installation"},{"location":"devops/proselint/#configuration","text":"It can be configured through the ~/.config/proselint/config file, such as: { \"checks\" : { \"typography.diacritical_marks\" : false } } The Vim through the ALE plugin . Pre-commit : - repo : https://github.com/amperser/proselint/ rev : 0.10.2 hooks : - id : proselint exclude : LICENSE|requirements files : \\.(md|mdown|markdown)$","title":"Configuration"},{"location":"devops/proselint/#references","text":"Git","title":"References"},{"location":"devops/safety/","text":"Safety checks your installed dependencies for known security vulnerabilities. You can use this cookiecutter template to create a python project with safety already configured. Installation \u2691 pip install safety Configuration \u2691 Safety can be used through: Pre-commit: File: .pre-commit-config.yaml repos : - repo : https://github.com/Lucas-C/pre-commit-hooks-safety rev : v1.1.3 hooks : - id : python-safety-dependencies-check Github Actions: Make sure to check that the correct python version is applied. File: .github/workflows/security.yml name : Security on : [ push , pull_request ] jobs : Safety : runs-on : ubuntu-latest steps : - name : Checkout uses : actions/checkout@v2 - uses : actions/setup-python@v2 with : python-version : 3.7 - name : Install dependencies run : pip install safety - name : Execute safety run : safety check References \u2691 Git","title":"Safety"},{"location":"devops/safety/#installation","text":"pip install safety","title":"Installation"},{"location":"devops/safety/#configuration","text":"Safety can be used through: Pre-commit: File: .pre-commit-config.yaml repos : - repo : https://github.com/Lucas-C/pre-commit-hooks-safety rev : v1.1.3 hooks : - id : python-safety-dependencies-check Github Actions: Make sure to check that the correct python version is applied. File: .github/workflows/security.yml name : Security on : [ push , pull_request ] jobs : Safety : runs-on : ubuntu-latest steps : - name : Checkout uses : actions/checkout@v2 - uses : actions/setup-python@v2 with : python-version : 3.7 - name : Install dependencies run : pip install safety - name : Execute safety run : safety check","title":"Configuration"},{"location":"devops/safety/#references","text":"Git","title":"References"},{"location":"devops/write_good/","text":"write-good is a naive linter for English prose. Installation \u2691 npm install -g write-good There is no way to configure it through a configuration file, but it accepts command line arguments. The ALE vim implementation supports the specification of such flags with the ale_writegood_options variable: let g :ale_writegood_options = \"--no-passive\" Use write-good --help to see the available flags. As of 2020-07-14, there is no pre-commit available. So I'm going to use it as a soft linter. References \u2691 Git","title":"Write Good"},{"location":"devops/write_good/#installation","text":"npm install -g write-good There is no way to configure it through a configuration file, but it accepts command line arguments. The ALE vim implementation supports the specification of such flags with the ale_writegood_options variable: let g :ale_writegood_options = \"--no-passive\" Use write-good --help to see the available flags. As of 2020-07-14, there is no pre-commit available. So I'm going to use it as a soft linter.","title":"Installation"},{"location":"devops/write_good/#references","text":"Git","title":"References"},{"location":"devops/yamllint/","text":"Yamllint is a linter for YAML files. yamllint does not only check for syntax validity, but for weirdnesses like key repetition and cosmetic problems such as lines length, trailing spaces or indentation. You can use it both with: The Vim through the ALE plugin . Pre-commit : File: .pre-commit-config.yaml - repo : https://github.com/adrienverge/yamllint rev : v1.21.0 hooks : - id : yamllint References \u2691 Git Docs","title":"Yamllint"},{"location":"devops/yamllint/#references","text":"Git Docs","title":"References"},{"location":"devops/aws/aws/","text":"Amazon Web Services (AWS) is a subsidiary of Amazon that provides on-demand cloud computing platforms and APIs to individuals, companies, and governments, on a metered pay-as-you-go basis. In aggregate, these cloud computing web services provide a set of primitive abstract technical infrastructure and distributed computing building blocks and tools. Learn path \u2691 TBD","title":"AWS"},{"location":"devops/aws/aws/#learn-path","text":"TBD","title":"Learn path"},{"location":"devops/aws/eks/","text":"Amazon Elastic Kubernetes Service (Amazon EKS) is a managed service that makes it easy for you to run Kubernetes on AWS without needing to stand up or maintain your own Kubernetes control plane. Upgrade an EKS cluster \u2691 New Kubernetes versions introduce significant changes, so it's recommended that you test the behavior of your applications against a new Kubernetes version before performing the update on your production clusters. The update process consists of Amazon EKS launching new API server nodes with the updated Kubernetes version to replace the existing ones. Amazon EKS performs standard infrastructure and readiness health checks for network traffic on these new nodes to verify that they are working as expected. If any of these checks fail, Amazon EKS reverts the infrastructure deployment, and your cluster remains on the prior Kubernetes version. Running applications are not affected, and your cluster is never left in a non-deterministic or unrecoverable state. Amazon EKS regularly backs up all managed clusters, and mechanisms exist to recover clusters if necessary. We are constantly evaluating and improving our Kubernetes infrastructure management processes. To upgrade a cluster follow these steps: Upgrade all your charts to the latest version with helmfile . helmfile deps helmfile apply Check your current version and compare it with the one you want to upgrade. kubectl version --short kubectl get nodes Check the docs to see if the version you want to upgrade requires some special steps. If your worker nodes aren't at the same version as the cluster control plane upgrade them to the control plane version (never higher). Edit the cluster_version attribute of the eks terraform module and apply the changes (reviewing them first). terraform apply This is a long step (approximately 40 minutes) * Upgrade your charts again. References \u2691 Docs","title":"EKS"},{"location":"devops/aws/eks/#upgrade-an-eks-cluster","text":"New Kubernetes versions introduce significant changes, so it's recommended that you test the behavior of your applications against a new Kubernetes version before performing the update on your production clusters. The update process consists of Amazon EKS launching new API server nodes with the updated Kubernetes version to replace the existing ones. Amazon EKS performs standard infrastructure and readiness health checks for network traffic on these new nodes to verify that they are working as expected. If any of these checks fail, Amazon EKS reverts the infrastructure deployment, and your cluster remains on the prior Kubernetes version. Running applications are not affected, and your cluster is never left in a non-deterministic or unrecoverable state. Amazon EKS regularly backs up all managed clusters, and mechanisms exist to recover clusters if necessary. We are constantly evaluating and improving our Kubernetes infrastructure management processes. To upgrade a cluster follow these steps: Upgrade all your charts to the latest version with helmfile . helmfile deps helmfile apply Check your current version and compare it with the one you want to upgrade. kubectl version --short kubectl get nodes Check the docs to see if the version you want to upgrade requires some special steps. If your worker nodes aren't at the same version as the cluster control plane upgrade them to the control plane version (never higher). Edit the cluster_version attribute of the eks terraform module and apply the changes (reviewing them first). terraform apply This is a long step (approximately 40 minutes) * Upgrade your charts again.","title":"Upgrade an EKS cluster"},{"location":"devops/aws/eks/#references","text":"Docs","title":"References"},{"location":"devops/aws/s3/","text":"S3 is the secure, durable, and scalable object storage infrastructure of AWS. Often used for serving static website content or holding backups or data. Commands \u2691 Bucket management \u2691 List buckets \u2691 aws s3 ls Create bucket \u2691 aws s3api create-bucket \\ --bucket {{ bucket_name }} \\ --create-bucket-configuration LocationConstraint = us-east-1 Enable versioning \u2691 aws s3api put-bucket-versioning --bucket {{ bucket_name }} --versioning-configuration Status = Enabled Enable encryption \u2691 aws s3api put-bucket-encryption --bucket {{ bucket_name }} \\ --server-side-encryption-configuration = '{ \"Rules\": [ { \"ApplyServerSideEncryptionByDefault\": { \"SSEAlgorithm\": \"AES256\" } } ] }' Download bucket \u2691 aws s3 cp --recursive s3:// {{ bucket_name }} . Audit the S3 bucket policy \u2691 IFS = $( echo -en \"\\n\\b\" ) for bucket in ` aws s3 ls | awk '{ print $3 }' ` do echo \"Bucket $bucket :\" aws s3api get-bucket-acl --bucket \" $bucket \" done Add cache header to all items in a bucket \u2691 Log in to AWS Management Console. Go into S3 bucket. Select all files by route. Choose \"More\" from the menu. Select \"Change metadata\". In the \"Key\" field, select \"Cache-Control\" from the drop down menu max-age=604800Enter (7 days in seconds) for Value. Press \"Save\" button. Object management \u2691 Remove an object \u2691 aws s3 rm s3:// {{ bucket_name }} / {{ path_to_file }} Upload \u2691 Upload a local file with the cli \u2691 aws s3 cp {{ path_to_file }} s3:// {{ bucket_name }} / {{ upload_path }} Upload a file unauthenticated \u2691 curl --request PUT --upload-file test.txt https:// {{ bucket_name }} .s3.amazonaws.com/uploads/ Restore an object \u2691 First you need to get the version of the object aws s3api list-object-versions \\ --bucket {{ bucket_name }} \\ --prefix {{ bucket_path_to_file }} Fetch the VersionId and download the file aws s3api get-object \\ --bucket {{ bucket_name }} \\ --key {{ bucket_path_to_file }} \\ --version-id {{ versionid }} Once you have it, overwrite the same object in the same path aws s3 cp \\ {{ local_path_to_restored_file }} \\ s3:// {{ bucket_name }} / {{ upload_path }} Copy objects between buckets \u2691 aws s3 sync s3://SOURCE_BUCKET_NAME s3://NEW_BUCKET_NAME Troubleshooting \u2691 get_environ_proxies() missing 1 required positional argument: 'no_proxy' \u2691 sudo pip3 install --upgrade boto3 Links \u2691 User guide","title":"S3"},{"location":"devops/aws/s3/#commands","text":"","title":"Commands"},{"location":"devops/aws/s3/#bucket-management","text":"","title":"Bucket management"},{"location":"devops/aws/s3/#list-buckets","text":"aws s3 ls","title":"List buckets"},{"location":"devops/aws/s3/#create-bucket","text":"aws s3api create-bucket \\ --bucket {{ bucket_name }} \\ --create-bucket-configuration LocationConstraint = us-east-1","title":"Create bucket"},{"location":"devops/aws/s3/#enable-versioning","text":"aws s3api put-bucket-versioning --bucket {{ bucket_name }} --versioning-configuration Status = Enabled","title":"Enable versioning"},{"location":"devops/aws/s3/#enable-encryption","text":"aws s3api put-bucket-encryption --bucket {{ bucket_name }} \\ --server-side-encryption-configuration = '{ \"Rules\": [ { \"ApplyServerSideEncryptionByDefault\": { \"SSEAlgorithm\": \"AES256\" } } ] }'","title":"Enable encryption"},{"location":"devops/aws/s3/#download-bucket","text":"aws s3 cp --recursive s3:// {{ bucket_name }} .","title":"Download bucket"},{"location":"devops/aws/s3/#audit-the-s3-bucket-policy","text":"IFS = $( echo -en \"\\n\\b\" ) for bucket in ` aws s3 ls | awk '{ print $3 }' ` do echo \"Bucket $bucket :\" aws s3api get-bucket-acl --bucket \" $bucket \" done","title":"Audit the S3 bucket policy"},{"location":"devops/aws/s3/#add-cache-header-to-all-items-in-a-bucket","text":"Log in to AWS Management Console. Go into S3 bucket. Select all files by route. Choose \"More\" from the menu. Select \"Change metadata\". In the \"Key\" field, select \"Cache-Control\" from the drop down menu max-age=604800Enter (7 days in seconds) for Value. Press \"Save\" button.","title":"Add cache header to all items in a bucket"},{"location":"devops/aws/s3/#object-management","text":"","title":"Object management"},{"location":"devops/aws/s3/#remove-an-object","text":"aws s3 rm s3:// {{ bucket_name }} / {{ path_to_file }}","title":"Remove an object"},{"location":"devops/aws/s3/#upload","text":"","title":"Upload"},{"location":"devops/aws/s3/#upload-a-local-file-with-the-cli","text":"aws s3 cp {{ path_to_file }} s3:// {{ bucket_name }} / {{ upload_path }}","title":"Upload a local file with the cli"},{"location":"devops/aws/s3/#upload-a-file-unauthenticated","text":"curl --request PUT --upload-file test.txt https:// {{ bucket_name }} .s3.amazonaws.com/uploads/","title":"Upload a file unauthenticated"},{"location":"devops/aws/s3/#restore-an-object","text":"First you need to get the version of the object aws s3api list-object-versions \\ --bucket {{ bucket_name }} \\ --prefix {{ bucket_path_to_file }} Fetch the VersionId and download the file aws s3api get-object \\ --bucket {{ bucket_name }} \\ --key {{ bucket_path_to_file }} \\ --version-id {{ versionid }} Once you have it, overwrite the same object in the same path aws s3 cp \\ {{ local_path_to_restored_file }} \\ s3:// {{ bucket_name }} / {{ upload_path }}","title":"Restore an object"},{"location":"devops/aws/s3/#copy-objects-between-buckets","text":"aws s3 sync s3://SOURCE_BUCKET_NAME s3://NEW_BUCKET_NAME","title":"Copy objects between buckets"},{"location":"devops/aws/s3/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"devops/aws/s3/#get_environ_proxies-missing-1-required-positional-argument-no_proxy","text":"sudo pip3 install --upgrade boto3","title":"get_environ_proxies() missing 1 required positional argument: 'no_proxy'"},{"location":"devops/aws/s3/#links","text":"User guide","title":"Links"},{"location":"devops/aws/security_groups/","text":"Security groups are the AWS way of defining firewall rules between the resources. If not handled properly they can soon become hard to read, which can lead to an insecure infrastructure. It has helped me to use four types of security groups: Default security groups: Security groups created by AWS per VPC and region, they can't be deleted. Naming security groups: Used to identify an aws resource. They are usually referenced in other security groups. Ingress security groups: Used to define the rules of ingress traffic to the resource. Egress security groups: Used to define the rules of egress traffic to the resource. But what helped most has been using clinv while refactoring all the security groups. With clinv unused I got rid of all the security groups that weren't used by any AWS resource (beware of #16 , 17 , #18 and #19 ), then used the clinv unassigned security_groups to methodically decide if they were correct and add them to my inventory or if I needed to refactor them. Best practices \u2691 Follow a naming convention . Avoid as much as you can the use of CIDRs in the definition of security groups. Instead, use naming security groups as much as you can. This will probably mean that you'll need to create security rules for each service that is going to use the security group. It is cumbersome but from a security point of view we gain traceability. Follow the principle of least privileges. Open the least number of ports required for the service to work. Reuse existing security groups. If there is a security group for web servers that uses port 80, don't create the new service using port 8080. Remove all rules from the default security groups and don't use them. Don't define the rules in the aws_security_group terraform resource. Use aws_security_group_rules for each security group to avoid creation dependency loops. Add descriptions to each security group and security group rule. Avoid using port ranges in the security group rule definitions, as you probably won't need them. Naming convention \u2691 A naming convention is required once the number of security groups starts to grow, both to understand them and to be able to search in them. Note It is assumed that terraform is used to create the resources Default security groups \u2691 There are going to be two kinds of default security groups: VPC default security groups. Region default security groups. For the first one we'll use: resource \"aws_default_security_group\" \"{{ region_id }}_{{ vpc_friendly_identifier }}\" { vpc_id = \"{{ vpc_id }}\" } Where: region_id is the region identifier with underscores, for example us_east_1 vpc_friendly_identifier is a human understandable identifier, such as publicdmz . vpc_id is the VPC id such as vpc-xxxxxxxxxxxxxxxxx . For the second one: resource \"aws_default_security_group\" \"{{ region_id }}\" { provider = aws .{{ region_id }} } Where the provider must be configured in the `terraform_config.tf` file, for example : ```terraform provider \"aws\" { alias = \"us_west_2\" region = \"us-west-2\" } Naming security groups \u2691 For the naming security groups I've created an UltiSnips template. snippet naming \"naming security group rule\" b resource \"aws_security_group\" \"${1:resource_name}_${2:resource_type}\" { name = \"$1-$2\" description = \"Identify the $1 $2.\" vpc_id = data.terraform_remote_state.vpc.outputs.vpc_ $ { 3 : vpc } _id tags = { Name = \"$1 $2\" } } output \"$1_$2_id\" { value = aws_security_group . $ 1 _$ 2.id } $ 0 endsnippet Where: instance_name is a human friendly identifier of the resource that the security group is going to identify, for example gitea , ci or bastion . resource_type identifies the type of resource, such as instance for EC2, or load_balancer for ELBs. vpc : is a human friendly identifier of the vpc. It has to match the outputs of the vpc resources, as it's going to be also used in the definition of the vpc used by the security group. Once you've finished defining the security group, move the output resource to the outputs.tf file. Ingress security groups \u2691 For the ingress security groups I've created another UltiSnips template. snippet ingress \"ingress security group rule\" b resource \"aws_security_group\" \"ingress_${1:protocol}_from_${2:destination}_at_${3:vpc}\" { name = \"ingress-$1-from-$2-at-$3\" description = \"Allow the ingress of $1 traffic from the $2 instances at $3\" vpc_id = data.terraform_remote_state.vpc.outputs.vpc_ $ 3 _id tags = { Name = \"Ingress $1 from $2 at $3\" } } resource \"aws_security_group_rule\" \"ingress_$1_from_$2_at_$3\" { type = \"ingress\" from_port = $ { 4 : port } to_port = $ { 5 : $ 4 } protocol = \"${6:tcp}\" security_group_id = aws_security_group.ingress_ $ 1 _from_$ 2 _at_$ 3.id source_security_group_id = aws_security_group . $ 7.id description = \"Allow the ingress $1 traffic on port $4 from the $2 instances at $3.\" } output \"ingress_$1_from_$2_at_$3_id\" { value = aws_security_group.ingress_ $ 1 _from_$ 2 _at_$ 3.id } $ 0 endsnippet Where: protocol is a human friendly identifier of what kind of traffic the security group is going to allow, for example gitea , ssh , proxy or openvpn . destination identifies the resources that are going to use the security group, for example drone_instance , ldap_instance or everywhere . vpc : is a human friendly identifier of the vpc. It has to match the outputs of the vpc resources, as it's going to be also used in the definition of the vpc used by the security group. port is the port we are going to open. $6 we assume that the to_port is the same as from_port . $7 ID of the naming security group that will have access to the particular security group rule. If you need to use CIDRs for the rule definition, change that line for the following: cidr_blocks = [ \"{{ cidr }}\" ] Once you've finished defining the security group, move the output resource to the outputs.tf file. Egress security groups \u2691 For the egress security groups I've created another UltiSnips template. snippet egress \"egress security group rule\" b resource \"aws_security_group\" \"egress_${1:protocol}_to_${2:destination}_from_${3:vpc}\" { name = \"egress-$1-to-$2-from-$3\" description = \"Allow the egress of $1 traffic to the $2 instances at $3\" vpc_id = data.terraform_remote_state.vpc.outputs.vpc_ $ 3 _id tags = { Name = \"Egress $1 to $2 at $3\" } } resource \"aws_security_group_rule\" \"egress_$1_to_$2_from_$3\" { type = \"egress\" from_port = $ { 4 : port } to_port = $ { 5 : $ 4 } protocol = \"${6:tcp}\" security_group_id = aws_security_group.egress_ $ 1 _to_$ 2 _from_$ 3.id source_security_group_id = aws_security_group . $ 7.id description = \"Allow the egress of $1 traffic on port $4 to the $2 instances at $3.\" } output \"egress_$1_to_$2_from_$3_id\" { value = aws_security_group.egress_ $ 1 _to_$ 2 _from_$ 3.id } $ 0 endsnippet Where: protocol is a human friendly identifier of what kind of traffic the security group is going to allow, for example gitea , ssh , proxy or openvpn . destination identifies the resources that are going to be accessed by the security group, for example drone_instance , ldap_instance or everywhere . vpc : is a human friendly identifier of the vpc. It has to match the outputs of the vpc resources, as it's going to be also used in the definition of the vpc used by the security group. port is the port we are going to open. $6 we assume that the to_port is the same as from_port . $7 ID of the naming security group that will be accessed by the particular security group rule. If you need to use CIDRs for the rule definition, change that line for the following: cidr_blocks = [ \"{{ cidr }}\" ] Once you've finished defining the security group, move the output resource to the outputs.tf file. Instance security group definition \u2691 When defining the security groups in the aws_instance resources, define them in this order: Naming security groups. Ingress security groups. Egress security groups. For example resource \"aws_instance\" \"gitea_production\" { ami = ... availability_zone = ... subnet_id = ... vpc_security_group_ids = [ data.terraform_remote_state.security_groups.outputs.gitea_instance_id , data.terraform_remote_state.security_groups.outputs.ingress_http_from_gitea_loadbalancer_at_publicdmz_id , data.terraform_remote_state.security_groups.outputs.ingress_http_from_monitoring_at_privatedmz_id , data.terraform_remote_state.security_groups.outputs.ingress_administration_from_bastion_at_connectiondmz_id , data.terraform_remote_state.security_groups.outputs.egress_ldap_to_ldap_instance_from_publicdmz_id , data.terraform_remote_state.security_groups.outputs.egress_https_to_debian_repositories_from_publicdmz_id , ]","title":"Security groups workflow"},{"location":"devops/aws/security_groups/#best-practices","text":"Follow a naming convention . Avoid as much as you can the use of CIDRs in the definition of security groups. Instead, use naming security groups as much as you can. This will probably mean that you'll need to create security rules for each service that is going to use the security group. It is cumbersome but from a security point of view we gain traceability. Follow the principle of least privileges. Open the least number of ports required for the service to work. Reuse existing security groups. If there is a security group for web servers that uses port 80, don't create the new service using port 8080. Remove all rules from the default security groups and don't use them. Don't define the rules in the aws_security_group terraform resource. Use aws_security_group_rules for each security group to avoid creation dependency loops. Add descriptions to each security group and security group rule. Avoid using port ranges in the security group rule definitions, as you probably won't need them.","title":"Best practices"},{"location":"devops/aws/security_groups/#naming-convention","text":"A naming convention is required once the number of security groups starts to grow, both to understand them and to be able to search in them. Note It is assumed that terraform is used to create the resources","title":"Naming convention"},{"location":"devops/aws/security_groups/#default-security-groups","text":"There are going to be two kinds of default security groups: VPC default security groups. Region default security groups. For the first one we'll use: resource \"aws_default_security_group\" \"{{ region_id }}_{{ vpc_friendly_identifier }}\" { vpc_id = \"{{ vpc_id }}\" } Where: region_id is the region identifier with underscores, for example us_east_1 vpc_friendly_identifier is a human understandable identifier, such as publicdmz . vpc_id is the VPC id such as vpc-xxxxxxxxxxxxxxxxx . For the second one: resource \"aws_default_security_group\" \"{{ region_id }}\" { provider = aws .{{ region_id }} } Where the provider must be configured in the `terraform_config.tf` file, for example : ```terraform provider \"aws\" { alias = \"us_west_2\" region = \"us-west-2\" }","title":"Default security groups"},{"location":"devops/aws/security_groups/#naming-security-groups","text":"For the naming security groups I've created an UltiSnips template. snippet naming \"naming security group rule\" b resource \"aws_security_group\" \"${1:resource_name}_${2:resource_type}\" { name = \"$1-$2\" description = \"Identify the $1 $2.\" vpc_id = data.terraform_remote_state.vpc.outputs.vpc_ $ { 3 : vpc } _id tags = { Name = \"$1 $2\" } } output \"$1_$2_id\" { value = aws_security_group . $ 1 _$ 2.id } $ 0 endsnippet Where: instance_name is a human friendly identifier of the resource that the security group is going to identify, for example gitea , ci or bastion . resource_type identifies the type of resource, such as instance for EC2, or load_balancer for ELBs. vpc : is a human friendly identifier of the vpc. It has to match the outputs of the vpc resources, as it's going to be also used in the definition of the vpc used by the security group. Once you've finished defining the security group, move the output resource to the outputs.tf file.","title":"Naming security groups"},{"location":"devops/aws/security_groups/#ingress-security-groups","text":"For the ingress security groups I've created another UltiSnips template. snippet ingress \"ingress security group rule\" b resource \"aws_security_group\" \"ingress_${1:protocol}_from_${2:destination}_at_${3:vpc}\" { name = \"ingress-$1-from-$2-at-$3\" description = \"Allow the ingress of $1 traffic from the $2 instances at $3\" vpc_id = data.terraform_remote_state.vpc.outputs.vpc_ $ 3 _id tags = { Name = \"Ingress $1 from $2 at $3\" } } resource \"aws_security_group_rule\" \"ingress_$1_from_$2_at_$3\" { type = \"ingress\" from_port = $ { 4 : port } to_port = $ { 5 : $ 4 } protocol = \"${6:tcp}\" security_group_id = aws_security_group.ingress_ $ 1 _from_$ 2 _at_$ 3.id source_security_group_id = aws_security_group . $ 7.id description = \"Allow the ingress $1 traffic on port $4 from the $2 instances at $3.\" } output \"ingress_$1_from_$2_at_$3_id\" { value = aws_security_group.ingress_ $ 1 _from_$ 2 _at_$ 3.id } $ 0 endsnippet Where: protocol is a human friendly identifier of what kind of traffic the security group is going to allow, for example gitea , ssh , proxy or openvpn . destination identifies the resources that are going to use the security group, for example drone_instance , ldap_instance or everywhere . vpc : is a human friendly identifier of the vpc. It has to match the outputs of the vpc resources, as it's going to be also used in the definition of the vpc used by the security group. port is the port we are going to open. $6 we assume that the to_port is the same as from_port . $7 ID of the naming security group that will have access to the particular security group rule. If you need to use CIDRs for the rule definition, change that line for the following: cidr_blocks = [ \"{{ cidr }}\" ] Once you've finished defining the security group, move the output resource to the outputs.tf file.","title":"Ingress security groups"},{"location":"devops/aws/security_groups/#egress-security-groups","text":"For the egress security groups I've created another UltiSnips template. snippet egress \"egress security group rule\" b resource \"aws_security_group\" \"egress_${1:protocol}_to_${2:destination}_from_${3:vpc}\" { name = \"egress-$1-to-$2-from-$3\" description = \"Allow the egress of $1 traffic to the $2 instances at $3\" vpc_id = data.terraform_remote_state.vpc.outputs.vpc_ $ 3 _id tags = { Name = \"Egress $1 to $2 at $3\" } } resource \"aws_security_group_rule\" \"egress_$1_to_$2_from_$3\" { type = \"egress\" from_port = $ { 4 : port } to_port = $ { 5 : $ 4 } protocol = \"${6:tcp}\" security_group_id = aws_security_group.egress_ $ 1 _to_$ 2 _from_$ 3.id source_security_group_id = aws_security_group . $ 7.id description = \"Allow the egress of $1 traffic on port $4 to the $2 instances at $3.\" } output \"egress_$1_to_$2_from_$3_id\" { value = aws_security_group.egress_ $ 1 _to_$ 2 _from_$ 3.id } $ 0 endsnippet Where: protocol is a human friendly identifier of what kind of traffic the security group is going to allow, for example gitea , ssh , proxy or openvpn . destination identifies the resources that are going to be accessed by the security group, for example drone_instance , ldap_instance or everywhere . vpc : is a human friendly identifier of the vpc. It has to match the outputs of the vpc resources, as it's going to be also used in the definition of the vpc used by the security group. port is the port we are going to open. $6 we assume that the to_port is the same as from_port . $7 ID of the naming security group that will be accessed by the particular security group rule. If you need to use CIDRs for the rule definition, change that line for the following: cidr_blocks = [ \"{{ cidr }}\" ] Once you've finished defining the security group, move the output resource to the outputs.tf file.","title":"Egress security groups"},{"location":"devops/aws/security_groups/#instance-security-group-definition","text":"When defining the security groups in the aws_instance resources, define them in this order: Naming security groups. Ingress security groups. Egress security groups. For example resource \"aws_instance\" \"gitea_production\" { ami = ... availability_zone = ... subnet_id = ... vpc_security_group_ids = [ data.terraform_remote_state.security_groups.outputs.gitea_instance_id , data.terraform_remote_state.security_groups.outputs.ingress_http_from_gitea_loadbalancer_at_publicdmz_id , data.terraform_remote_state.security_groups.outputs.ingress_http_from_monitoring_at_privatedmz_id , data.terraform_remote_state.security_groups.outputs.ingress_administration_from_bastion_at_connectiondmz_id , data.terraform_remote_state.security_groups.outputs.egress_ldap_to_ldap_instance_from_publicdmz_id , data.terraform_remote_state.security_groups.outputs.egress_https_to_debian_repositories_from_publicdmz_id , ]","title":"Instance security group definition"},{"location":"devops/aws/iam/iam/","text":"AWS Identity and Access Management (IAM) is a web service that helps you securely control access to AWS resources for your users. You use IAM to control who can use your AWS resources (authentication) and what resources they can use and in what ways (authorization). Configurable AWS access controls: Grant access to AWS Management console, APIs Create individual users Manage permissions with groups Configure a strong password policy Enable Multi-Factor Authentication for privileged users Use IAM roles for EC2 instances Use IAM roles to share access Rotate security credentials regularly Restrict privileged access further with conditions Use your corporate directory system or a third party authentication Links \u2691 Docs","title":"IAM"},{"location":"devops/aws/iam/iam/#links","text":"Docs","title":"Links"},{"location":"devops/aws/iam/iam_commands/","text":"Information gathering \u2691 List roles \u2691 aws iam list-roles --query 'Roles[*].{RoleName: RoleName, RoleId: RoleId}' --output table List policies \u2691 aws iam list-policies --query 'Policies[*].{PolicyName: PolicyName, PolicyId: PolicyId}' --output table List attached policies \u2691 aws iam list-attached-role-policies --role-name {{ role_name }} Get role configuration \u2691 aws iam get-role --role-name {{ role_name }} Get role policies \u2691 aws iam list-role-policies --role-name {{ role_name }}","title":"IAM Commands"},{"location":"devops/aws/iam/iam_commands/#information-gathering","text":"","title":"Information gathering"},{"location":"devops/aws/iam/iam_commands/#list-roles","text":"aws iam list-roles --query 'Roles[*].{RoleName: RoleName, RoleId: RoleId}' --output table","title":"List roles"},{"location":"devops/aws/iam/iam_commands/#list-policies","text":"aws iam list-policies --query 'Policies[*].{PolicyName: PolicyName, PolicyId: PolicyId}' --output table","title":"List policies"},{"location":"devops/aws/iam/iam_commands/#list-attached-policies","text":"aws iam list-attached-role-policies --role-name {{ role_name }}","title":"List attached policies"},{"location":"devops/aws/iam/iam_commands/#get-role-configuration","text":"aws iam get-role --role-name {{ role_name }}","title":"Get role configuration"},{"location":"devops/aws/iam/iam_commands/#get-role-policies","text":"aws iam list-role-policies --role-name {{ role_name }}","title":"Get role policies"},{"location":"devops/aws/iam/iam_debug/","text":"MFADevice entity at the same path and name already exists \u2691 It happens when a user receives an error while creating her MFA authentication. To solve it: List the existing MFA physical or virtual devices aws iam list-mfa-devices aws iam list-virtual-mfa-devices Delete the conflictive one aws iam delete-virtual-mfa-device --serial-number {{ mfa_arn }}","title":"IAM Debugging"},{"location":"devops/aws/iam/iam_debug/#mfadevice-entity-at-the-same-path-and-name-already-exists","text":"It happens when a user receives an error while creating her MFA authentication. To solve it: List the existing MFA physical or virtual devices aws iam list-mfa-devices aws iam list-virtual-mfa-devices Delete the conflictive one aws iam delete-virtual-mfa-device --serial-number {{ mfa_arn }}","title":"MFADevice entity at the same path and name already exists"},{"location":"devops/helm/helm/","text":"Helm is the package manager for Kubernetes. Through charts it helps you define, install and upgrade even the most complex Kubernetes applications. The advantages of using helm over kubectl apply are the easiness of: Repeatable application installation. CI integration. Versioning and sharing. Charts are a group of Go templates of kubernetes yaml resource manifests, they are easy to create, version, share, and publish. Helm alone lacks some features, that are satisfied through some external programs: Helmfile is used to declaratively configure your charts, so they can be versioned through git. Helm-secrets is used to remove hardcoded credentials from values.yaml files. Helm has an open issue to integrate it into it's codebase. Helm-git is used to install helm charts directly from Git repositories. Links \u2691 Homepage Docs Git Chart hub Git charts repositories","title":"Helm"},{"location":"devops/helm/helm/#links","text":"Homepage Docs Git Chart hub Git charts repositories","title":"Links"},{"location":"devops/helm/helm_commands/","text":"Small cheatsheet on how to use the helm command. List charts \u2691 helm ls Get information of chart \u2691 helm inspect {{ package_name }} List all the available versions of a chart \u2691 helm search -l {{ package_name }} Download a chart \u2691 helm fetch {{ package_name }} Download and extract helm fetch --untar {{ package_name }} Search charts \u2691 helm search {{ package_name }} Operations you should do with helmfile \u2691 The following operations can be done with helm, but consider using helmfile instead. Install chart \u2691 Helm deploys all the pods, replication controllers and services. The pod will be in a pending state while the Docker Image is downloaded and until a Persistent Volume is available. Once complete it will transition into a running state. helm install {{ package_name }} Give it a name \u2691 helm install --name {{ release_name }} {{ package_name }} Give it a namespace \u2691 helm install --namespace {{ namespace }} {{ package_name }} Customize the chart before installing \u2691 helm inspect values {{ package_name }} > values.yml Edit the values.yml helm install -f values.yml {{ package_name }} Upgrade a release \u2691 If a new version of the chart is released or you want to change the configuration use helm upgrade -f {{ config_file }} {{ release_name }} {{ package_name }} Rollback an upgrade \u2691 First check the revisions helm history {{ release_name }} Then rollback helm rollback {{ release_name }} {{ revision }} Delete a release \u2691 helm delete --purge {{ release_name }} Working with repositories \u2691 List repositories \u2691 helm repo list Add repository \u2691 helm repo add {{ repo_name }} {{ repo_url }} Update repositories \u2691 helm repo update","title":"Helm Commands"},{"location":"devops/helm/helm_commands/#list-charts","text":"helm ls","title":"List charts"},{"location":"devops/helm/helm_commands/#get-information-of-chart","text":"helm inspect {{ package_name }}","title":"Get information of chart"},{"location":"devops/helm/helm_commands/#list-all-the-available-versions-of-a-chart","text":"helm search -l {{ package_name }}","title":"List all the available versions of a chart"},{"location":"devops/helm/helm_commands/#download-a-chart","text":"helm fetch {{ package_name }} Download and extract helm fetch --untar {{ package_name }}","title":"Download a chart"},{"location":"devops/helm/helm_commands/#search-charts","text":"helm search {{ package_name }}","title":"Search charts"},{"location":"devops/helm/helm_commands/#operations-you-should-do-with-helmfile","text":"The following operations can be done with helm, but consider using helmfile instead.","title":"Operations you should do with helmfile"},{"location":"devops/helm/helm_commands/#install-chart","text":"Helm deploys all the pods, replication controllers and services. The pod will be in a pending state while the Docker Image is downloaded and until a Persistent Volume is available. Once complete it will transition into a running state. helm install {{ package_name }}","title":"Install chart"},{"location":"devops/helm/helm_commands/#give-it-a-name","text":"helm install --name {{ release_name }} {{ package_name }}","title":"Give it a name"},{"location":"devops/helm/helm_commands/#give-it-a-namespace","text":"helm install --namespace {{ namespace }} {{ package_name }}","title":"Give it a namespace"},{"location":"devops/helm/helm_commands/#customize-the-chart-before-installing","text":"helm inspect values {{ package_name }} > values.yml Edit the values.yml helm install -f values.yml {{ package_name }}","title":"Customize the chart before installing"},{"location":"devops/helm/helm_commands/#upgrade-a-release","text":"If a new version of the chart is released or you want to change the configuration use helm upgrade -f {{ config_file }} {{ release_name }} {{ package_name }}","title":"Upgrade a release"},{"location":"devops/helm/helm_commands/#rollback-an-upgrade","text":"First check the revisions helm history {{ release_name }} Then rollback helm rollback {{ release_name }} {{ revision }}","title":"Rollback an upgrade"},{"location":"devops/helm/helm_commands/#delete-a-release","text":"helm delete --purge {{ release_name }}","title":"Delete a release"},{"location":"devops/helm/helm_commands/#working-with-repositories","text":"","title":"Working with repositories"},{"location":"devops/helm/helm_commands/#list-repositories","text":"helm repo list","title":"List repositories"},{"location":"devops/helm/helm_commands/#add-repository","text":"helm repo add {{ repo_name }} {{ repo_url }}","title":"Add repository"},{"location":"devops/helm/helm_commands/#update-repositories","text":"helm repo update","title":"Update repositories"},{"location":"devops/helm/helm_installation/","text":"There are two usable versions of Helm, v2 and v3, the latter is quite new so some of the things we need to install as of 2020-01-27 are not yet supported (Prometheus operator), so we are going to stick to the version 2. Helm has a client-server architecture, the server is installed in the Kubernetes cluster and the client is a Go executable installed in the user computer. Helm client \u2691 You'll first need to configure kubectl. The binary is still not available in the distribution package managers, so check the latest version in the releases of their git repository , and get the download link of the tar.gz . wget {{ url_to_tar_gz }} -O helm.tar.gz tar -xvf helm.tar.gz mv linux-amd64/helm ~/.local/bin/ We are going to use some plugins inside Helmfile , so install them with: helm plugin install https://github.com/futuresimple/helm-secrets helm plugin install https://github.com/databus23/helm-diff Finally initialize the environment helm init Now that you've got Helm installed, you'll probably want to install Helmfile .","title":"Helm Installation"},{"location":"devops/helm/helm_installation/#helm-client","text":"You'll first need to configure kubectl. The binary is still not available in the distribution package managers, so check the latest version in the releases of their git repository , and get the download link of the tar.gz . wget {{ url_to_tar_gz }} -O helm.tar.gz tar -xvf helm.tar.gz mv linux-amd64/helm ~/.local/bin/ We are going to use some plugins inside Helmfile , so install them with: helm plugin install https://github.com/futuresimple/helm-secrets helm plugin install https://github.com/databus23/helm-diff Finally initialize the environment helm init Now that you've got Helm installed, you'll probably want to install Helmfile .","title":"Helm client"},{"location":"devops/helm/helm_secrets/","text":"Helm-secrets is a helm plugin that manages secrets with Git workflow and stores them anywhere. It delegates the cryptographic operations to Mozilla's Sops tool, which supports PGP, AWS KMS and GCP KMS. The configuration is stored in .sops.yaml files. You can find in Mozilla's documentation a detailed configuration guide. For my use case, I'm only going to use a list of PGP keys, so the following contents should be in the .sops.yaml file at the project root directory. creation_rules : - pgp : >- {{ gpg_key_1 }}, {{ gpg_key_2}} Installation \u2691 Weirdly, helm plugin install https://github.com/jkroepke/helm-secrets --version v3.9.1 asks for your github user :S so I'd rather install it by hand. wget https://github.com/jkroepke/helm-secrets/releases/download/v3.9.1/helm-secrets.tar.gz tar xvzf helm-secrets.tar.gz -C \" $( helm env HELM_PLUGINS ) \" rm helm-secrets.tar.gz If you're going to use GPG as backend you need to install sops . It's in your distribution repositories, but probably not in the latest version, therefore I suggest you install the binary directly: Grab the latest release Download, chmod +x and move it somewhere in your $PATH . Prevent committing decrypted files to git \u2691 From the docs : If you like to secure situation when decrypted file is committed by mistake to git you can add your secrets.yaml.dec files to you charts project repository .gitignore. A second level of security is to add for example a .sopscommithook file inside your chart repository local commit hook. This will prevent committing decrypted files without sops metadata. .sopscommithook content example: #!/bin/sh for FILE in $(git diff-index HEAD --name-only | grep <your vars dir> | grep \"secrets.y\"); do if [ -f \"$FILE\" ] && ! grep -C10000 \"sops:\" $FILE | grep -q \"version:\"; then echo \"!!!!! $FILE\" 'File is not encrypted !!!!!' echo \"Run : helm secrets enc <file path>\" exit 1 fi done exit Usage \u2691 Encrypt secret files \u2691 Imagine you've got a values.yaml with the following information: grafana : enabled : true adminPassword : admin If you want to encrypt adminPassword , remove that line from the values.yaml and create a secrets.yaml file with: grafana : adminPassword : supersecretpassword And encrypt the file. helm secrets enc secrets.yaml If you use Helmfile , you'll need to add the secrets file to your helmfile.yaml. values : - values.yaml secrets : - secrets.yaml From that point on, helmfile will automatically decrypt the credentials. Edit secret files \u2691 helm secrets edit secrets.yaml Decrypt secret files \u2691 helm secrets dec secrets.yaml It will generate a secrets.yaml.dec file that it's not decrypted. Be careful not to add these files to git . Clean all the decrypted files \u2691 helm secrets clean . Add or remove keys \u2691 Until this helm-secrets issue has been solved, if you want to add or remove PGP keys from .sops.yaml , you need to execute sops updatekeys -y for each secrets.yaml file in the repository. Check sops documentation for more options. Links \u2691 Git","title":"Helm Secrets"},{"location":"devops/helm/helm_secrets/#installation","text":"Weirdly, helm plugin install https://github.com/jkroepke/helm-secrets --version v3.9.1 asks for your github user :S so I'd rather install it by hand. wget https://github.com/jkroepke/helm-secrets/releases/download/v3.9.1/helm-secrets.tar.gz tar xvzf helm-secrets.tar.gz -C \" $( helm env HELM_PLUGINS ) \" rm helm-secrets.tar.gz If you're going to use GPG as backend you need to install sops . It's in your distribution repositories, but probably not in the latest version, therefore I suggest you install the binary directly: Grab the latest release Download, chmod +x and move it somewhere in your $PATH .","title":"Installation"},{"location":"devops/helm/helm_secrets/#prevent-committing-decrypted-files-to-git","text":"From the docs : If you like to secure situation when decrypted file is committed by mistake to git you can add your secrets.yaml.dec files to you charts project repository .gitignore. A second level of security is to add for example a .sopscommithook file inside your chart repository local commit hook. This will prevent committing decrypted files without sops metadata. .sopscommithook content example: #!/bin/sh for FILE in $(git diff-index HEAD --name-only | grep <your vars dir> | grep \"secrets.y\"); do if [ -f \"$FILE\" ] && ! grep -C10000 \"sops:\" $FILE | grep -q \"version:\"; then echo \"!!!!! $FILE\" 'File is not encrypted !!!!!' echo \"Run : helm secrets enc <file path>\" exit 1 fi done exit","title":"Prevent committing decrypted files to git"},{"location":"devops/helm/helm_secrets/#usage","text":"","title":"Usage"},{"location":"devops/helm/helm_secrets/#encrypt-secret-files","text":"Imagine you've got a values.yaml with the following information: grafana : enabled : true adminPassword : admin If you want to encrypt adminPassword , remove that line from the values.yaml and create a secrets.yaml file with: grafana : adminPassword : supersecretpassword And encrypt the file. helm secrets enc secrets.yaml If you use Helmfile , you'll need to add the secrets file to your helmfile.yaml. values : - values.yaml secrets : - secrets.yaml From that point on, helmfile will automatically decrypt the credentials.","title":"Encrypt secret files"},{"location":"devops/helm/helm_secrets/#edit-secret-files","text":"helm secrets edit secrets.yaml","title":"Edit secret files"},{"location":"devops/helm/helm_secrets/#decrypt-secret-files","text":"helm secrets dec secrets.yaml It will generate a secrets.yaml.dec file that it's not decrypted. Be careful not to add these files to git .","title":"Decrypt secret files"},{"location":"devops/helm/helm_secrets/#clean-all-the-decrypted-files","text":"helm secrets clean .","title":"Clean all the decrypted files"},{"location":"devops/helm/helm_secrets/#add-or-remove-keys","text":"Until this helm-secrets issue has been solved, if you want to add or remove PGP keys from .sops.yaml , you need to execute sops updatekeys -y for each secrets.yaml file in the repository. Check sops documentation for more options.","title":"Add or remove keys"},{"location":"devops/helm/helm_secrets/#links","text":"Git","title":"Links"},{"location":"devops/kong/kong/","text":"Kong is a lua application API platform running in Nginx. Installation \u2691 Kong supports several platforms of which we'll use Kubernetes with the helm chart , as it gives the following advantages: Kong is configured dynamically and responds to the changes in your infrastructure. Kong is deployed onto Kubernetes with a Controller, which is responsible for configuring Kong. All of Kong\u2019s configuration is done using Kubernetes resources, stored in Kubernetes\u2019 data-store (etcd). Use the power of kubectl (or any custom tooling around kubectl) to configure Kong and get benefits of all Kubernetes, such as declarative configuration, cloud-provider agnostic deployments, RBAC, reconciliation of desired state, and elastic scalability. Kong is configured using a combination of Ingress Resource and Custom Resource Definitions(CRDs). DB-less by default, meaning Kong has the capability of running without a database and using only memory storage for entities. In the helmfile.yaml add the repository and the release: repositories : - name : kong url : https://charts.konghq.com releases : - name : kong namespace : api-manager chart : kong/kong values : - kong/values.yaml secrets : - kong/secrets.yaml While particularizing the values.yaml keep in mind that: If you don't want the ingress controller set up ingressController.enabled: false , and in proxy set service: ClusterIP and ingress.enabled: true . Kong can be run with or without a database. By default the chart installs it without database. If you deploy it without database and without the ingress controller, you have to provide a declarative configuration for Kong to run. It can be provided using an existing ConfigMap dblessConfig.configMap or the whole configuration can be put into the values.yaml file for deployment itself, under the dblessConfig.config parameter. Although kong supports it's own Kubernetes resources (CRD) for plugins and consumers , I've found now way of integrating them into the helm chart, therefore I'm going to specify everything in the dblessConfig.config . So the general kong configuration values.yaml would be: dblessConfig : config : _format_version : \"1.1\" services : - name : example.com url : https://api.example.com plugins : - name : key-auth - name : rate-limiting config : second : 10 hour : 1000 policy : local routes : - name : example paths : - /example And the secrets.yaml : consumers : - username : lyz keyauth_credentials : - key : vRQO6xfBbTY3KRvNV7TbeFUUW7kjBmPhIFcUUxvkm4 To test that everything works use curl -I https://api.example.com/example -H 'apikey: vRQO6xfBbTY3KRvNV7TbeFUUW7kjBmPhIFcUUxvkm4' To add the prometheus monitorization, enable the serviceMonitor.enabled: true and make sure you set the correct labels . There is a grafana official dashboard you can also use. Links \u2691 Homepage Docs","title":"Kong"},{"location":"devops/kong/kong/#installation","text":"Kong supports several platforms of which we'll use Kubernetes with the helm chart , as it gives the following advantages: Kong is configured dynamically and responds to the changes in your infrastructure. Kong is deployed onto Kubernetes with a Controller, which is responsible for configuring Kong. All of Kong\u2019s configuration is done using Kubernetes resources, stored in Kubernetes\u2019 data-store (etcd). Use the power of kubectl (or any custom tooling around kubectl) to configure Kong and get benefits of all Kubernetes, such as declarative configuration, cloud-provider agnostic deployments, RBAC, reconciliation of desired state, and elastic scalability. Kong is configured using a combination of Ingress Resource and Custom Resource Definitions(CRDs). DB-less by default, meaning Kong has the capability of running without a database and using only memory storage for entities. In the helmfile.yaml add the repository and the release: repositories : - name : kong url : https://charts.konghq.com releases : - name : kong namespace : api-manager chart : kong/kong values : - kong/values.yaml secrets : - kong/secrets.yaml While particularizing the values.yaml keep in mind that: If you don't want the ingress controller set up ingressController.enabled: false , and in proxy set service: ClusterIP and ingress.enabled: true . Kong can be run with or without a database. By default the chart installs it without database. If you deploy it without database and without the ingress controller, you have to provide a declarative configuration for Kong to run. It can be provided using an existing ConfigMap dblessConfig.configMap or the whole configuration can be put into the values.yaml file for deployment itself, under the dblessConfig.config parameter. Although kong supports it's own Kubernetes resources (CRD) for plugins and consumers , I've found now way of integrating them into the helm chart, therefore I'm going to specify everything in the dblessConfig.config . So the general kong configuration values.yaml would be: dblessConfig : config : _format_version : \"1.1\" services : - name : example.com url : https://api.example.com plugins : - name : key-auth - name : rate-limiting config : second : 10 hour : 1000 policy : local routes : - name : example paths : - /example And the secrets.yaml : consumers : - username : lyz keyauth_credentials : - key : vRQO6xfBbTY3KRvNV7TbeFUUW7kjBmPhIFcUUxvkm4 To test that everything works use curl -I https://api.example.com/example -H 'apikey: vRQO6xfBbTY3KRvNV7TbeFUUW7kjBmPhIFcUUxvkm4' To add the prometheus monitorization, enable the serviceMonitor.enabled: true and make sure you set the correct labels . There is a grafana official dashboard you can also use.","title":"Installation"},{"location":"devops/kong/kong/#links","text":"Homepage Docs","title":"Links"},{"location":"devops/kubectl/kubectl/","text":"Kubectl Definition Kubectl is a command line tool for controlling Kubernetes clusters. kubectl looks for a file named config in the $HOME/.kube directory. You can specify other kubeconfig files by setting the KUBECONFIG environment variable or by setting the --kubeconfig flag. Resource types and it's aliases \u2691 Resource Name Short Name clusters componentstatuses cs configmaps cm daemonsets ds deployments deploy endpoints ep event ev horizontalpodautoscalers hpa ingresses ing jobs limitranges limits namespaces ns networkpolicies netpol nodes no statefulsets sts persistentvolumeclaims pvc persistentvolumes pv pods po podsecuritypolicies psp podtemplates replicasets rs replicationcontrollers rc resourcequotas quota cronjob secrets serviceaccount sa services svc storageclasses thirdpartyresources Links \u2691 Overview . Cheatsheet . Kbenv : Virtualenv for kubectl.","title":"Kubectl"},{"location":"devops/kubectl/kubectl/#resource-types-and-its-aliases","text":"Resource Name Short Name clusters componentstatuses cs configmaps cm daemonsets ds deployments deploy endpoints ep event ev horizontalpodautoscalers hpa ingresses ing jobs limitranges limits namespaces ns networkpolicies netpol nodes no statefulsets sts persistentvolumeclaims pvc persistentvolumes pv pods po podsecuritypolicies psp podtemplates replicasets rs replicationcontrollers rc resourcequotas quota cronjob secrets serviceaccount sa services svc storageclasses thirdpartyresources","title":"Resource types and it's aliases"},{"location":"devops/kubectl/kubectl/#links","text":"Overview . Cheatsheet . Kbenv : Virtualenv for kubectl.","title":"Links"},{"location":"devops/kubectl/kubectl_commands/","text":"Configuration and context \u2691 Add a new cluster to your kubeconf that supports basic auth \u2691 kubectl config set-credentials {{ username }} / {{ cluster_dns }} --username ={{ username }} --password ={{ password }} Create new context \u2691 kubectl config set-context {{ context_name }} --user ={{ username }} --namespace ={{ namespace }} Get current context \u2691 kubectl config current-context List contexts \u2691 kubectl config get-contexts Switch context \u2691 kubectl config use-context {{ context_name }} Creating objects \u2691 Create Resource \u2691 kubectl create -f {{ resource.definition.yaml | dir.where.yamls.live | url.to.yaml }} --record Deleting resources \u2691 Delete the pod using the type and name specified in a file \u2691 kubectl delete -f {{ path_to_file }} Delete pods and services by name \u2691 kubectl delete pod,service {{ pod_names }} {{ service_names }} Delete pods and services by label \u2691 kubectl delete pod,services -l {{ label_name }}={{ label_value }} Delete all pods and services in namespace \u2691 kubectl -n {{ namespace_name }} delete po,svc --all Delete all evicted pods \u2691 while read i ; do kubectl delete pod \" $i \" ; done < < ( kubectl get pods | grep -i evicted | sed 's/ .*//g' ) Editing resources \u2691 Edit a service \u2691 kubectl edit svc/ {{ service_name }} Information gathering \u2691 Get credentials \u2691 Get credentials kubectl config view --minify Deployments \u2691 View status of deployments \u2691 kubectl get deployments Describe Deployments \u2691 kubectl describe deployment {{ deployment_name }} Get images of deployment \u2691 kubectl get pods --selector = app ={{ deployment_name }} -o json | \\ jq '.items[] | .metadata.name + \": \" + .spec.containers[0].image' Nodes \u2691 List all nodes \u2691 kubectl get nodes Check which nodes are ready \u2691 JSONPATH = '{range .items[*]}{@.metadata.name}:{range @.status.conditions[*]}{@.type}={@.status};{end}{end}' \\ && kubectl get nodes -o jsonpath = $JSONPATH | grep \"Ready=True\" External IPs of all nodes \u2691 kubectl get nodes -o jsonpath = '{.items[*].status.addresses[?(@.type==\"ExternalIP\")].address}' Get exposed ports of node \u2691 export NODE_PORT = $( kubectl get services/kubernetes-bootcamp -o go-template = '{{(index .spec.ports 0).nodePort}}' ) Pods \u2691 List all pods in the current namespace \u2691 kubectl get pods List all pods in all namespaces \u2691 kubectl get pods --all-namespaces List all pods of a selected namespace \u2691 kubectl get pods -n {{ namespace }} List with more detail \u2691 kubectl get pods -o wide Get pods of a selected deployment \u2691 kubectl get pods --selector = \"name={{ name }}\" Get pods of a given label \u2691 kubectl get pods -l {{ label_name }} Get pods by IP \u2691 kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE alpine-3835730047-ggn2v 1 /1 Running 0 5d 10 .22.19.69 ip-10-35-80-221.ec2.internal Sort pods by restart count \u2691 kubectl get pods --sort-by = '.status.containerStatuses[0].restartCount' Describe Pods \u2691 kubectl describe pods {{ pod_name }} Get name of pod \u2691 pod = $( kubectl get pod --selector ={{ selector_label }}={{ selector_value }} -o jsonpath ={ .items..metadata.name } ) List pods that belong to a particular RC \u2691 sel = ${ $( kubectl get rc my-rc --output = json | jq -j '.spec.selector | to_entries | .[] | \"\\(.key)=\\(.value),\"' ) %? } echo $( kubectl get pods --selector = $sel --output = jsonpath ={ .items..metadata.name } ) Services \u2691 List services in namespace \u2691 kubectl get services List services sorted by name kubectl get services --sort-by = .metadata.name Describe Services \u2691 kubectl describe services {{ service_name }} kubectl describe svc {{ service_name }} Replication controller \u2691 List all replication controller \u2691 kubectl get rc Secrets \u2691 View status of secrets \u2691 kubectl get secrets Namespaces \u2691 View namespaces \u2691 kubectl get namespaces Limits \u2691 kubectl get limitrange kubectl describe limitrange limits Jobs and cronjobs \u2691 Get cronjobs of a namespace \u2691 kubectl get cronjobs -n {{ namespace }} Get jobs of a namespace \u2691 kubectl get jobs -n {{ namespace }} You can then describe a specific job to get the pod it created. kubectl describe job -n {{ namespace }} {{ job_name }} And now you can see the evolution of the job with: kubectl logs -n {{ namespace }} {{ pod_name }} Interacting with nodes and cluster \u2691 Mark node as unschedulable \u2691 kubectl cordon {{ node_name }} Mark node as schedulable \u2691 kubectl uncordon {{ node_name }} Drain node in preparation for maintenance \u2691 kubectl drain {{ node_name }} Show metrics of all node \u2691 kubectl top node Show metrics of a node \u2691 kubectl top node {{ node_name }} Display addresses of the master and servies \u2691 kubectl cluster-info Dump current cluster state to stdout \u2691 kubectl cluster-info dump Dump current cluster state to directory \u2691 kubectl cluster-info dump --output-directory ={{ path_to_directory }} Interacting with pods \u2691 Dump logs of pod \u2691 kubectl logs {{ pod_name }} Dump logs of pod and specified container \u2691 kubectl logs {{ pod_name }} -c {{ container_name }} Stream logs of pod \u2691 kubectl logs -f {{ pod_name }} kubectl logs -f {{ pod_name }} -c {{ container_name }} Another option is to use the kubetail program. Attach to running container \u2691 kubectl attach {{ pod_name }} -i Get a shell of a running container \u2691 kubectl exec {{ pod_name }} -it bash Get a debian container inside kubernetes \u2691 kubectl run --generator = run-pod/v1 -i --tty debian --image = debian -- bash Get a root shell of a running container \u2691 Get the Node where the pod is and the docker ID kubectl describe pod {{ pod_name }} SSH into the node ssh {{ node }} Get into docker docker exec -it -u root {{ docker_id }} bash Forward port of pod to your local machine \u2691 kubectl port-forward {{ pod_name }} {{ pod_port }} : {{ local_port }} Expose port \u2691 kubectl expose {{ deployment_name }} --type = \"{{ expose_type }}\" --port {{ port_number }} Where expose_type is one of: ['NodePort', 'ClusterIP', 'LoadBalancer', 'externalName'] Run command on existing pod \u2691 kubectl exec {{ pod_name }} -- ls / kubectl exec {{ pod_name }} -c {{ container_name }} -- ls / Show metrics for a given pod and it's containers \u2691 kubectl top pod {{ pod_name }} --containers Extract file from pod \u2691 kubectl cp {{ container_id }} : {{ path_to_file }} {{ path_to_local_file }} Scaling resources \u2691 Scale a deployment with a specified size \u2691 kubectl scale deployment {{ deployment_name }} --replicas {{ replicas_number }} Scale a replicaset \u2691 kubectl scale --replicas ={{ replicas_number }} rs/ {{ replicaset_name }} Scale a resource specified in a file \u2691 kubectl scale --replicas ={{ replicas_number }} -f {{ path_to_yaml }} Updating resources \u2691 Namespaces \u2691 Temporary set the namespace for a request \u2691 kubectl -n {{ namespace_name }} {{ command_to_execute }} kubectl --namespace ={{ namespace_name }} {{ command_to_execute }} Permanently set the namespace for a request \u2691 kubectl config set-context $( kubectl config current-context ) --namespace ={{ namespace_name }} Deployment \u2691 Modify the image of a deployment \u2691 kubectl set image {{ deployment_name }} {{ label }} : {{ label_value }} for example kubectl set image deployment/nginx-deployment nginx = nginx:1.9.1 Or edit it by hand kubectl edit {{ deployment_name }} Get the status of the rolling update \u2691 kubectl rollout status {{ deployment_name }} Get the history of the deployment \u2691 kubectl rollout history deployment {{ deployment_name }} To get more details of a selected revision: kubectl rollout history deployment {{ deployment_name }} --revision ={{ revision_number }} Get back to a specified revision \u2691 To get to the last version kubectl rollout undo deployment {{ deployment_name }} To go to a specific version kubectl rollout undo {{ deployment_name }} --to-revision ={{ revision_number }} Pods \u2691 Rolling update of pods \u2691 Is prefered to use the deployment rollout kubectl rolling-update {{ pod_name }} -f {{ new_pod_definition_yaml }} Change the name of the resource and update the image \u2691 kubectl rolling-update {{ old_pod_name }} {{ new_pod_name }} --image = image: {{ new_pod_version }} Abort existing rollout in progress \u2691 kubectl rolling-update {{ old_pod_name }} {{ new_pod_name }} --rollback Force replace, delete and then re-create the resource \u2691 ** Will cause a service outage ** kubectl replace --force -f {{ new_pod_yaml }} Add a label \u2691 kubectl label pods {{ pod_name }} new-label ={{ new_label }} Autoscale a deployment \u2691 kubectl autoscale deployment {{ deployment_name }} --min ={{ min_instances }} --max ={{ max_instances }} [ --cpu-percent ={{ cpu_percent }}] Copy resources between namespaces \u2691 kubectl get rs,secrets -o json --namespace old | jq '.items[].metadata.namespace = \"new\"' | kubectl create-f - Formatting output \u2691 Print a table using a comma separated list of custom columns \u2691 -o = custom-columns = <spec> Print a table using the custom columns template in the file \u2691 -o = custom-columns-file = <filename> Output a JSON formatted API object \u2691 -o = json Print the fields defined in a jsonpath expression \u2691 -o = jsonpath = <template> Print the fields defined by the jsonpath expression in the file \u2691 -o = jsonpath-file = <filename> Print only the resource name and nothing else \u2691 -o = name Output in the plain-text format with any additional information, and for pods, the node name is included \u2691 -o = wide Output a YAML formatted API object \u2691 -o = yaml","title":"Kubectl Commands"},{"location":"devops/kubectl/kubectl_commands/#configuration-and-context","text":"","title":"Configuration and context"},{"location":"devops/kubectl/kubectl_commands/#add-a-new-cluster-to-your-kubeconf-that-supports-basic-auth","text":"kubectl config set-credentials {{ username }} / {{ cluster_dns }} --username ={{ username }} --password ={{ password }}","title":"Add a new cluster to your kubeconf that supports basic auth"},{"location":"devops/kubectl/kubectl_commands/#create-new-context","text":"kubectl config set-context {{ context_name }} --user ={{ username }} --namespace ={{ namespace }}","title":"Create new context"},{"location":"devops/kubectl/kubectl_commands/#get-current-context","text":"kubectl config current-context","title":"Get current context"},{"location":"devops/kubectl/kubectl_commands/#list-contexts","text":"kubectl config get-contexts","title":"List contexts"},{"location":"devops/kubectl/kubectl_commands/#switch-context","text":"kubectl config use-context {{ context_name }}","title":"Switch context"},{"location":"devops/kubectl/kubectl_commands/#creating-objects","text":"","title":"Creating objects"},{"location":"devops/kubectl/kubectl_commands/#create-resource","text":"kubectl create -f {{ resource.definition.yaml | dir.where.yamls.live | url.to.yaml }} --record","title":"Create Resource"},{"location":"devops/kubectl/kubectl_commands/#deleting-resources","text":"","title":"Deleting resources"},{"location":"devops/kubectl/kubectl_commands/#delete-the-pod-using-the-type-and-name-specified-in-a-file","text":"kubectl delete -f {{ path_to_file }}","title":"Delete the pod using the type and name specified in a file"},{"location":"devops/kubectl/kubectl_commands/#delete-pods-and-services-by-name","text":"kubectl delete pod,service {{ pod_names }} {{ service_names }}","title":"Delete pods and services by name"},{"location":"devops/kubectl/kubectl_commands/#delete-pods-and-services-by-label","text":"kubectl delete pod,services -l {{ label_name }}={{ label_value }}","title":"Delete pods and services by label"},{"location":"devops/kubectl/kubectl_commands/#delete-all-pods-and-services-in-namespace","text":"kubectl -n {{ namespace_name }} delete po,svc --all","title":"Delete all pods and services in namespace"},{"location":"devops/kubectl/kubectl_commands/#delete-all-evicted-pods","text":"while read i ; do kubectl delete pod \" $i \" ; done < < ( kubectl get pods | grep -i evicted | sed 's/ .*//g' )","title":"Delete all evicted pods"},{"location":"devops/kubectl/kubectl_commands/#editing-resources","text":"","title":"Editing resources"},{"location":"devops/kubectl/kubectl_commands/#edit-a-service","text":"kubectl edit svc/ {{ service_name }}","title":"Edit a service"},{"location":"devops/kubectl/kubectl_commands/#information-gathering","text":"","title":"Information gathering"},{"location":"devops/kubectl/kubectl_commands/#get-credentials","text":"Get credentials kubectl config view --minify","title":"Get credentials"},{"location":"devops/kubectl/kubectl_commands/#deployments","text":"","title":"Deployments"},{"location":"devops/kubectl/kubectl_commands/#view-status-of-deployments","text":"kubectl get deployments","title":"View status of deployments"},{"location":"devops/kubectl/kubectl_commands/#describe-deployments","text":"kubectl describe deployment {{ deployment_name }}","title":"Describe Deployments"},{"location":"devops/kubectl/kubectl_commands/#get-images-of-deployment","text":"kubectl get pods --selector = app ={{ deployment_name }} -o json | \\ jq '.items[] | .metadata.name + \": \" + .spec.containers[0].image'","title":"Get images of deployment"},{"location":"devops/kubectl/kubectl_commands/#nodes","text":"","title":"Nodes"},{"location":"devops/kubectl/kubectl_commands/#list-all-nodes","text":"kubectl get nodes","title":"List all nodes"},{"location":"devops/kubectl/kubectl_commands/#check-which-nodes-are-ready","text":"JSONPATH = '{range .items[*]}{@.metadata.name}:{range @.status.conditions[*]}{@.type}={@.status};{end}{end}' \\ && kubectl get nodes -o jsonpath = $JSONPATH | grep \"Ready=True\"","title":"Check which nodes are ready"},{"location":"devops/kubectl/kubectl_commands/#external-ips-of-all-nodes","text":"kubectl get nodes -o jsonpath = '{.items[*].status.addresses[?(@.type==\"ExternalIP\")].address}'","title":"External IPs of all nodes"},{"location":"devops/kubectl/kubectl_commands/#get-exposed-ports-of-node","text":"export NODE_PORT = $( kubectl get services/kubernetes-bootcamp -o go-template = '{{(index .spec.ports 0).nodePort}}' )","title":"Get exposed ports of node"},{"location":"devops/kubectl/kubectl_commands/#pods","text":"","title":"Pods"},{"location":"devops/kubectl/kubectl_commands/#list-all-pods-in-the-current-namespace","text":"kubectl get pods","title":"List all pods in the current namespace"},{"location":"devops/kubectl/kubectl_commands/#list-all-pods-in-all-namespaces","text":"kubectl get pods --all-namespaces","title":"List all pods in all namespaces"},{"location":"devops/kubectl/kubectl_commands/#list-all-pods-of-a-selected-namespace","text":"kubectl get pods -n {{ namespace }}","title":"List all pods of a selected namespace"},{"location":"devops/kubectl/kubectl_commands/#list-with-more-detail","text":"kubectl get pods -o wide","title":"List with more detail"},{"location":"devops/kubectl/kubectl_commands/#get-pods-of-a-selected-deployment","text":"kubectl get pods --selector = \"name={{ name }}\"","title":"Get pods of a selected deployment"},{"location":"devops/kubectl/kubectl_commands/#get-pods-of-a-given-label","text":"kubectl get pods -l {{ label_name }}","title":"Get pods of a given label"},{"location":"devops/kubectl/kubectl_commands/#get-pods-by-ip","text":"kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE alpine-3835730047-ggn2v 1 /1 Running 0 5d 10 .22.19.69 ip-10-35-80-221.ec2.internal","title":"Get pods by IP"},{"location":"devops/kubectl/kubectl_commands/#sort-pods-by-restart-count","text":"kubectl get pods --sort-by = '.status.containerStatuses[0].restartCount'","title":"Sort pods by restart count"},{"location":"devops/kubectl/kubectl_commands/#describe-pods","text":"kubectl describe pods {{ pod_name }}","title":"Describe Pods"},{"location":"devops/kubectl/kubectl_commands/#get-name-of-pod","text":"pod = $( kubectl get pod --selector ={{ selector_label }}={{ selector_value }} -o jsonpath ={ .items..metadata.name } )","title":"Get name of pod"},{"location":"devops/kubectl/kubectl_commands/#list-pods-that-belong-to-a-particular-rc","text":"sel = ${ $( kubectl get rc my-rc --output = json | jq -j '.spec.selector | to_entries | .[] | \"\\(.key)=\\(.value),\"' ) %? } echo $( kubectl get pods --selector = $sel --output = jsonpath ={ .items..metadata.name } )","title":"List pods that belong to a particular RC"},{"location":"devops/kubectl/kubectl_commands/#services","text":"","title":"Services"},{"location":"devops/kubectl/kubectl_commands/#list-services-in-namespace","text":"kubectl get services List services sorted by name kubectl get services --sort-by = .metadata.name","title":"List services in namespace"},{"location":"devops/kubectl/kubectl_commands/#describe-services","text":"kubectl describe services {{ service_name }} kubectl describe svc {{ service_name }}","title":"Describe Services"},{"location":"devops/kubectl/kubectl_commands/#replication-controller","text":"","title":"Replication controller"},{"location":"devops/kubectl/kubectl_commands/#list-all-replication-controller","text":"kubectl get rc","title":"List all replication controller"},{"location":"devops/kubectl/kubectl_commands/#secrets","text":"","title":"Secrets"},{"location":"devops/kubectl/kubectl_commands/#view-status-of-secrets","text":"kubectl get secrets","title":"View status of secrets"},{"location":"devops/kubectl/kubectl_commands/#namespaces","text":"","title":"Namespaces"},{"location":"devops/kubectl/kubectl_commands/#view-namespaces","text":"kubectl get namespaces","title":"View namespaces"},{"location":"devops/kubectl/kubectl_commands/#limits","text":"kubectl get limitrange kubectl describe limitrange limits","title":"Limits"},{"location":"devops/kubectl/kubectl_commands/#jobs-and-cronjobs","text":"","title":"Jobs and cronjobs"},{"location":"devops/kubectl/kubectl_commands/#get-cronjobs-of-a-namespace","text":"kubectl get cronjobs -n {{ namespace }}","title":"Get cronjobs of a namespace"},{"location":"devops/kubectl/kubectl_commands/#get-jobs-of-a-namespace","text":"kubectl get jobs -n {{ namespace }} You can then describe a specific job to get the pod it created. kubectl describe job -n {{ namespace }} {{ job_name }} And now you can see the evolution of the job with: kubectl logs -n {{ namespace }} {{ pod_name }}","title":"Get jobs of a namespace"},{"location":"devops/kubectl/kubectl_commands/#interacting-with-nodes-and-cluster","text":"","title":"Interacting with nodes and cluster"},{"location":"devops/kubectl/kubectl_commands/#mark-node-as-unschedulable","text":"kubectl cordon {{ node_name }}","title":"Mark node as unschedulable"},{"location":"devops/kubectl/kubectl_commands/#mark-node-as-schedulable","text":"kubectl uncordon {{ node_name }}","title":"Mark node as schedulable"},{"location":"devops/kubectl/kubectl_commands/#drain-node-in-preparation-for-maintenance","text":"kubectl drain {{ node_name }}","title":"Drain node in preparation for maintenance"},{"location":"devops/kubectl/kubectl_commands/#show-metrics-of-all-node","text":"kubectl top node","title":"Show metrics of all node"},{"location":"devops/kubectl/kubectl_commands/#show-metrics-of-a-node","text":"kubectl top node {{ node_name }}","title":"Show metrics of a node"},{"location":"devops/kubectl/kubectl_commands/#display-addresses-of-the-master-and-servies","text":"kubectl cluster-info","title":"Display addresses of the master and servies"},{"location":"devops/kubectl/kubectl_commands/#dump-current-cluster-state-to-stdout","text":"kubectl cluster-info dump","title":"Dump current cluster state to stdout"},{"location":"devops/kubectl/kubectl_commands/#dump-current-cluster-state-to-directory","text":"kubectl cluster-info dump --output-directory ={{ path_to_directory }}","title":"Dump current cluster state to directory"},{"location":"devops/kubectl/kubectl_commands/#interacting-with-pods","text":"","title":"Interacting with pods"},{"location":"devops/kubectl/kubectl_commands/#dump-logs-of-pod","text":"kubectl logs {{ pod_name }}","title":"Dump logs of pod"},{"location":"devops/kubectl/kubectl_commands/#dump-logs-of-pod-and-specified-container","text":"kubectl logs {{ pod_name }} -c {{ container_name }}","title":"Dump logs of pod and specified container"},{"location":"devops/kubectl/kubectl_commands/#stream-logs-of-pod","text":"kubectl logs -f {{ pod_name }} kubectl logs -f {{ pod_name }} -c {{ container_name }} Another option is to use the kubetail program.","title":"Stream logs of pod"},{"location":"devops/kubectl/kubectl_commands/#attach-to-running-container","text":"kubectl attach {{ pod_name }} -i","title":"Attach to running container"},{"location":"devops/kubectl/kubectl_commands/#get-a-shell-of-a-running-container","text":"kubectl exec {{ pod_name }} -it bash","title":"Get a shell of a running container"},{"location":"devops/kubectl/kubectl_commands/#get-a-debian-container-inside-kubernetes","text":"kubectl run --generator = run-pod/v1 -i --tty debian --image = debian -- bash","title":"Get a debian container inside kubernetes"},{"location":"devops/kubectl/kubectl_commands/#get-a-root-shell-of-a-running-container","text":"Get the Node where the pod is and the docker ID kubectl describe pod {{ pod_name }} SSH into the node ssh {{ node }} Get into docker docker exec -it -u root {{ docker_id }} bash","title":"Get a root shell of a running container"},{"location":"devops/kubectl/kubectl_commands/#forward-port-of-pod-to-your-local-machine","text":"kubectl port-forward {{ pod_name }} {{ pod_port }} : {{ local_port }}","title":"Forward port of pod to your local machine"},{"location":"devops/kubectl/kubectl_commands/#expose-port","text":"kubectl expose {{ deployment_name }} --type = \"{{ expose_type }}\" --port {{ port_number }} Where expose_type is one of: ['NodePort', 'ClusterIP', 'LoadBalancer', 'externalName']","title":"Expose port"},{"location":"devops/kubectl/kubectl_commands/#run-command-on-existing-pod","text":"kubectl exec {{ pod_name }} -- ls / kubectl exec {{ pod_name }} -c {{ container_name }} -- ls /","title":"Run command on existing pod"},{"location":"devops/kubectl/kubectl_commands/#show-metrics-for-a-given-pod-and-its-containers","text":"kubectl top pod {{ pod_name }} --containers","title":"Show metrics for a given pod and it's containers"},{"location":"devops/kubectl/kubectl_commands/#extract-file-from-pod","text":"kubectl cp {{ container_id }} : {{ path_to_file }} {{ path_to_local_file }}","title":"Extract file from pod"},{"location":"devops/kubectl/kubectl_commands/#scaling-resources","text":"","title":"Scaling resources"},{"location":"devops/kubectl/kubectl_commands/#scale-a-deployment-with-a-specified-size","text":"kubectl scale deployment {{ deployment_name }} --replicas {{ replicas_number }}","title":"Scale a deployment with a specified size"},{"location":"devops/kubectl/kubectl_commands/#scale-a-replicaset","text":"kubectl scale --replicas ={{ replicas_number }} rs/ {{ replicaset_name }}","title":"Scale a replicaset"},{"location":"devops/kubectl/kubectl_commands/#scale-a-resource-specified-in-a-file","text":"kubectl scale --replicas ={{ replicas_number }} -f {{ path_to_yaml }}","title":"Scale a resource specified in a file"},{"location":"devops/kubectl/kubectl_commands/#updating-resources","text":"","title":"Updating resources"},{"location":"devops/kubectl/kubectl_commands/#namespaces_1","text":"","title":"Namespaces"},{"location":"devops/kubectl/kubectl_commands/#temporary-set-the-namespace-for-a-request","text":"kubectl -n {{ namespace_name }} {{ command_to_execute }} kubectl --namespace ={{ namespace_name }} {{ command_to_execute }}","title":"Temporary set the namespace for a request"},{"location":"devops/kubectl/kubectl_commands/#permanently-set-the-namespace-for-a-request","text":"kubectl config set-context $( kubectl config current-context ) --namespace ={{ namespace_name }}","title":"Permanently set the namespace for a request"},{"location":"devops/kubectl/kubectl_commands/#deployment","text":"","title":"Deployment"},{"location":"devops/kubectl/kubectl_commands/#modify-the-image-of-a-deployment","text":"kubectl set image {{ deployment_name }} {{ label }} : {{ label_value }} for example kubectl set image deployment/nginx-deployment nginx = nginx:1.9.1 Or edit it by hand kubectl edit {{ deployment_name }}","title":"Modify the image of a deployment"},{"location":"devops/kubectl/kubectl_commands/#get-the-status-of-the-rolling-update","text":"kubectl rollout status {{ deployment_name }}","title":"Get the status of the rolling update"},{"location":"devops/kubectl/kubectl_commands/#get-the-history-of-the-deployment","text":"kubectl rollout history deployment {{ deployment_name }} To get more details of a selected revision: kubectl rollout history deployment {{ deployment_name }} --revision ={{ revision_number }}","title":"Get the history of the deployment"},{"location":"devops/kubectl/kubectl_commands/#get-back-to-a-specified-revision","text":"To get to the last version kubectl rollout undo deployment {{ deployment_name }} To go to a specific version kubectl rollout undo {{ deployment_name }} --to-revision ={{ revision_number }}","title":"Get back to a specified revision"},{"location":"devops/kubectl/kubectl_commands/#pods_1","text":"","title":"Pods"},{"location":"devops/kubectl/kubectl_commands/#rolling-update-of-pods","text":"Is prefered to use the deployment rollout kubectl rolling-update {{ pod_name }} -f {{ new_pod_definition_yaml }}","title":"Rolling update of pods"},{"location":"devops/kubectl/kubectl_commands/#change-the-name-of-the-resource-and-update-the-image","text":"kubectl rolling-update {{ old_pod_name }} {{ new_pod_name }} --image = image: {{ new_pod_version }}","title":"Change the name of the resource and update the image"},{"location":"devops/kubectl/kubectl_commands/#abort-existing-rollout-in-progress","text":"kubectl rolling-update {{ old_pod_name }} {{ new_pod_name }} --rollback","title":"Abort existing rollout in progress"},{"location":"devops/kubectl/kubectl_commands/#force-replace-delete-and-then-re-create-the-resource","text":"** Will cause a service outage ** kubectl replace --force -f {{ new_pod_yaml }}","title":"Force replace, delete and then re-create the resource"},{"location":"devops/kubectl/kubectl_commands/#add-a-label","text":"kubectl label pods {{ pod_name }} new-label ={{ new_label }}","title":"Add a label"},{"location":"devops/kubectl/kubectl_commands/#autoscale-a-deployment","text":"kubectl autoscale deployment {{ deployment_name }} --min ={{ min_instances }} --max ={{ max_instances }} [ --cpu-percent ={{ cpu_percent }}]","title":"Autoscale a deployment"},{"location":"devops/kubectl/kubectl_commands/#copy-resources-between-namespaces","text":"kubectl get rs,secrets -o json --namespace old | jq '.items[].metadata.namespace = \"new\"' | kubectl create-f -","title":"Copy resources between namespaces"},{"location":"devops/kubectl/kubectl_commands/#formatting-output","text":"","title":"Formatting output"},{"location":"devops/kubectl/kubectl_commands/#print-a-table-using-a-comma-separated-list-of-custom-columns","text":"-o = custom-columns = <spec>","title":"Print a table using a comma separated list of custom columns"},{"location":"devops/kubectl/kubectl_commands/#print-a-table-using-the-custom-columns-template-in-the-file","text":"-o = custom-columns-file = <filename>","title":"Print a table using the custom columns template in the  file"},{"location":"devops/kubectl/kubectl_commands/#output-a-json-formatted-api-object","text":"-o = json","title":"Output a JSON formatted API object"},{"location":"devops/kubectl/kubectl_commands/#print-the-fields-defined-in-a-jsonpath-expression","text":"-o = jsonpath = <template>","title":"Print the fields defined in a jsonpath expression"},{"location":"devops/kubectl/kubectl_commands/#print-the-fields-defined-by-the-jsonpath-expression-in-the-file","text":"-o = jsonpath-file = <filename>","title":"Print the fields defined by the jsonpath expression in the  file"},{"location":"devops/kubectl/kubectl_commands/#print-only-the-resource-name-and-nothing-else","text":"-o = name","title":"Print only the resource name and nothing else"},{"location":"devops/kubectl/kubectl_commands/#output-in-the-plain-text-format-with-any-additional-information-and-for-pods-the-node-name-is-included","text":"-o = wide","title":"Output in the plain-text format with any additional information, and for pods, the node name is included"},{"location":"devops/kubectl/kubectl_commands/#output-a-yaml-formatted-api-object","text":"-o = yaml","title":"Output a YAML formatted API object"},{"location":"devops/kubectl/kubectl_installation/","text":"Kubectl is not yet in the distribution package managers, so we'll need to install it manually. curl -LO \"https://storage.googleapis.com/kubernetes-release/release/ $( \\ curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt \\ ) /bin/linux/amd64/kubectl\" chmod +x kubectl mv kubectl ~/.local/bin/kubectl Configure kubectl \u2691 Set editor \u2691 # File ~/.bashrc KUBE_EDITOR = \"vim\" Set auto completion \u2691 # File ~/.bashrc source < ( kubectl completion bash ) Configure EKS cluster \u2691 To configure the access to an existing cluster, we'll let aws-cli create the required files: aws eks update-kubeconfig --name {{ cluster_name }}","title":"Kubectl Installation"},{"location":"devops/kubectl/kubectl_installation/#configure-kubectl","text":"","title":"Configure kubectl"},{"location":"devops/kubectl/kubectl_installation/#set-editor","text":"# File ~/.bashrc KUBE_EDITOR = \"vim\"","title":"Set editor"},{"location":"devops/kubectl/kubectl_installation/#set-auto-completion","text":"# File ~/.bashrc source < ( kubectl completion bash )","title":"Set auto completion"},{"location":"devops/kubectl/kubectl_installation/#configure-eks-cluster","text":"To configure the access to an existing cluster, we'll let aws-cli create the required files: aws eks update-kubeconfig --name {{ cluster_name }}","title":"Configure EKS cluster"},{"location":"devops/kubernetes/kubernetes/","text":"Kubernetes (commonly stylized as k8s) is an open-source container-orchestration system for automating application deployment, scaling, and management. Developed by Google in Go under the Apache 2.0 license, it was first released on June 7, 2014 reaching 1.0 by July 21, 2015. It works with a range of container tools, including Docker. Many cloud services offer a Kubernetes-based platform or infrastructure as a service ( PaaS or IaaS ) on which Kubernetes can be deployed as a platform-providing service. Many vendors also provide their own branded Kubernetes distributions. It has become the standard infrastructure to manage containers in production environments. Docker Swarm would be an alternative but it falls short in features compared with Kubernetes. These are some of the advantages of using Kubernetes: Widely used in production and actively developed. Ensure high availability of your services with autohealing and autoscaling. Easy, quickly and predictable deployment and promotion of applications. Seamless roll out of features. Optimize hardware use while guaranteeing resource isolation. Easiest way to build multi-cloud and baremetal environments. Several companies have used Kubernetes to release their own PaaS : OpenShift by Red Hat. Tectonic by CoreOS. Rancher labs by Rancher. Learn roadmap \u2691 K8s is huge, and growing at a pace that most mortals can't stay updated unless you work with it daily. This is how I learnt, but probably there are better resources now : Read containing container chaos kubernetes . Test the katacoda lab . Install Kubernetes in laptop with minikube . Read K8s concepts . Then K8s tasks . I didn't like the book Getting started with kubernetes I'd personally avoid the book Getting started with kubernetes , I didn't like it \u00af\\(\u00b0_o)/\u00af . Tools to test \u2691 Velero : To backup and migrate Kubernetes resources and persistent volumes. Popeye is a utility that scans live Kubernetes cluster and reports potential issues with deployed resources and configurations. It sanitizes your cluster based on what's deployed and not what's sitting on disk. By scanning your cluster, it detects misconfigurations and helps you to ensure that best practices are in place, thus preventing future headaches. It aims at reducing the cognitive overload one faces when operating a Kubernetes cluster in the wild. Furthermore, if your cluster employs a metric-server, it reports potential resources over/under allocations and attempts to warn you should your cluster run out of capacity. Popeye is a readonly tool, it does not alter any of your Kubernetes resources in any way! Stern allows you to tail multiple pods on Kubernetes and multiple containers within the pod. Each result is color coded for quicker debugging. The query is a regular expression so the pod name can easily be filtered and you don't need to specify the exact id (for instance omitting the deployment id). If a pod is deleted it gets removed from tail and if a new pod is added it automatically gets tailed. When a pod contains multiple containers Stern can tail all of them too without having to do this manually for each one. Simply specify the container flag to limit what containers to show. By default all containers are listened to. Fairwinds' Polaris keeps your clusters sailing smoothly. It runs a variety of checks to ensure that Kubernetes pods and controllers are configured using best practices, helping you avoid problems in the future. kube-hunter hunts for security weaknesses in Kubernetes clusters. The tool was developed to increase awareness and visibility for security issues in Kubernetes environments. References \u2691 Docs Awesome K8s Katacoda playground Comic Diving deeper \u2691 Architecture Resources Kubectl Additional Components Networking Helm Tools Reference \u2691 References API conventions","title":"Kubernetes"},{"location":"devops/kubernetes/kubernetes/#learn-roadmap","text":"K8s is huge, and growing at a pace that most mortals can't stay updated unless you work with it daily. This is how I learnt, but probably there are better resources now : Read containing container chaos kubernetes . Test the katacoda lab . Install Kubernetes in laptop with minikube . Read K8s concepts . Then K8s tasks . I didn't like the book Getting started with kubernetes I'd personally avoid the book Getting started with kubernetes , I didn't like it \u00af\\(\u00b0_o)/\u00af .","title":"Learn roadmap"},{"location":"devops/kubernetes/kubernetes/#tools-to-test","text":"Velero : To backup and migrate Kubernetes resources and persistent volumes. Popeye is a utility that scans live Kubernetes cluster and reports potential issues with deployed resources and configurations. It sanitizes your cluster based on what's deployed and not what's sitting on disk. By scanning your cluster, it detects misconfigurations and helps you to ensure that best practices are in place, thus preventing future headaches. It aims at reducing the cognitive overload one faces when operating a Kubernetes cluster in the wild. Furthermore, if your cluster employs a metric-server, it reports potential resources over/under allocations and attempts to warn you should your cluster run out of capacity. Popeye is a readonly tool, it does not alter any of your Kubernetes resources in any way! Stern allows you to tail multiple pods on Kubernetes and multiple containers within the pod. Each result is color coded for quicker debugging. The query is a regular expression so the pod name can easily be filtered and you don't need to specify the exact id (for instance omitting the deployment id). If a pod is deleted it gets removed from tail and if a new pod is added it automatically gets tailed. When a pod contains multiple containers Stern can tail all of them too without having to do this manually for each one. Simply specify the container flag to limit what containers to show. By default all containers are listened to. Fairwinds' Polaris keeps your clusters sailing smoothly. It runs a variety of checks to ensure that Kubernetes pods and controllers are configured using best practices, helping you avoid problems in the future. kube-hunter hunts for security weaknesses in Kubernetes clusters. The tool was developed to increase awareness and visibility for security issues in Kubernetes environments.","title":"Tools to test"},{"location":"devops/kubernetes/kubernetes/#references","text":"Docs Awesome K8s Katacoda playground Comic","title":"References"},{"location":"devops/kubernetes/kubernetes/#diving-deeper","text":"Architecture Resources Kubectl Additional Components Networking Helm Tools","title":"Diving deeper"},{"location":"devops/kubernetes/kubernetes/#reference","text":"References API conventions","title":"Reference"},{"location":"devops/kubernetes/kubernetes_annotations/","text":"Annotations are non-identifying metadata key/value pairs attached to objects, such as pods. Annotations are intended to give meaningful and relevant information to libraries and tools. Annotations, like labels, are key/value maps: \"annotations\" : { \"key1\" : \"value1\" , \"key2\" : \"value2\" } Here are some examples of information that could be recorded in annotations: Fields managed by a declarative configuration layer. Attaching these fields as annotations distinguishes them from default values set by clients or servers, and from auto generated fields and fields set by auto sizing or auto scaling systems. Build, release, or image information like timestamps, release IDs, git branch, PR numbers, image hashes, and registry address. Pointers to logging, monitoring, analytics, or audit repositories. Client library or tool information that can be used for debugging purposes, for example, name, version, and build information. User or tool/system provenance information, such as URLs of related objects from other ecosystem components. Lightweight rollout tool metadata: for example, config or checkpoints.","title":"Annotations"},{"location":"devops/kubernetes/kubernetes_architecture/","text":"Kubernetes is a combination of components distributed between two kind of nodes, Masters and Workers . Master Nodes \u2691 Master nodes or Kubernetes Control Plane are the controlling unit for the cluster. They manage scheduling of new containers, configure networking and provide health information. To do so it uses: kube-api-server exposes the Kubernetes control plane API validating and configuring data for the different API objects. It's used by all the components to interact between themselves. etcd is a \"Distributed reliable key-value store for the most critical data of a distributed system\". Kubernetes uses Etcd to store state about the cluster and service discovery between nodes. This state includes what nodes exist in the cluster, which nodes they are running on and what containers should be running. kube-scheduler watches for newly created pods with no assigned node, and selects a node for them to run on. Factors taken into account for scheduling decisions include individual and collective resource requirements, hardware/software/policy constraints, affinity and anti-affinity specifications, data locality, inter-workload interference and deadlines. kube-controller-manager runs the following controllers: Node Controller : Responsible for noticing and responding when nodes go down. Replication Controller : Responsible for maintaining the correct number of pods for every replication controller object in the system. Endpoints Controller : Populates the Endpoints object (that is, joins Services & Pods). Service Account & Token Controllers : Create default accounts and API access tokens for new namespaces. cloud-controller-manager runs controllers that interact with the underlying cloud providers. Node Controller : For checking the cloud provider to determine if a node has been deleted in the cloud after it stops responding. Route Controller : For setting up routes in the underlying cloud infrastructure. Service Controller : For creating, updating and deleting cloud provider load balancers. Volume Controller : For creating, attaching, and mounting volumes, and interacting with the cloud provider to orchestrate volumes. Worker Nodes \u2691 Worker nodes are the units that hold the application data of the cluster. There can be different types of nodes: CPU architecture, amount of resources (CPU, RAM, GPU) or cloud provider. Each node has the services necessary to run pods: Container Runtime : The software responsible for running containers (Docker, rkt, containerd, CRI-O). kubelet : The primary \u201cnode agent\u201d. It works in terms of a PodSpec (a YAML or JSON object that describes it). kubelet takes a set of PodSpecs from the masters kube-api-server and ensures that the containers described are running and healthy. kube-proxy is the network proxy that runs on each node. This reflects services as defined in the Kubernetes API on each node and can do simple TCP and UDP stream forwarding or round robin across a set of backends. kube-proxy maintains network rules on nodes. These network rules allow network communication to your Pods from network sessions inside or outside of your cluster. kube-proxy uses the operating system packet filtering layer if there is one and it's available. Otherwise, it will forward the traffic itself. kube-proxy operation modes \u2691 kube-proxy currently supports three different operation modes: User space : This mode gets its name because the service routing takes place in kube-proxy in the user process space instead of in the kernel network stack. It is not commonly used as it is slow and outdated. iptables : This mode uses Linux kernel-level Netfilter rules to configure all routing for Kubernetes Services. This mode is the default for kube-proxy on most platforms. When load balancing for multiple backend pods, it uses unweighted round-robin scheduling. IPVS (IP Virtual Server) : Built on the Netfilter framework, IPVS implements Layer-4 load balancing in the Linux kernel, supporting multiple load-balancing algorithms, including least connections and shortest expected delay. This kube-proxy mode became generally available in Kubernetes 1.11, but it requires the Linux kernel to have the IPVS modules loaded. It is also not as widely supported by various Kubernetes networking projects as the iptables mode. Kubectl \u2691 The kubectl is the command line client used to communicate with the Masters. Number of clusters \u2691 You can run a given set of workloads either on few large clusters (with many workloads in each cluster) or on many clusters (with few workloads in each cluster). Here's a table that summarizes the pros and cons of various approaches: Figure: Possibilities of number of clusters from learnk8s.io article Reference to the original article for a full read (it's really good!). I'm going to analyze only the Large shared cluster and Cluster per environment options, as they are the closest to my use case. One Large shared cluster \u2691 With this option, you run all your workloads in the same cluster. Kubernetes provides namespaces to logically separate portions of a cluster from each other, and in the above case, you could use a separate namespace for each application instance. Pros: Efficient resource usage : You need to have only one copy of all the resources that are needed to run and manage a Kubernetes cluster (master nodes, load balancers, Ingress controllers, authentication, logging, and monitoring). Cheap : As you avoid the duplication of resources, you require less hardware. Efficient administration : Administering a Kubernetes cluster requires: Upgrading the Kubernetes version Setting up a CI/CD pipeline Installing a CNI plugin Setting up the user authentication system Installing an admission controller If you have only a single cluster, you need to do all of this only once. If you have many clusters, then you need to apply everything multiple times, which probably requires you to develop some automated processes and tools for being able to do this consistently. Cons: Single point of failure : If you have only one cluster and if that cluster breaks, then all your workloads are down. There are many ways that something can go wrong: A Kubernetes upgrade produces unexpected side effects. An cluster-wide component (such as a CNI plugin) doesn't work as expected. An erroneous configuration is made to one of the cluster components. An outage occurs in the underlying infrastructure. A single incident like this can produce major damage across all your workloads if you have only a single shared cluster. No hard security isolation : If multiple apps run in the same Kubernetes cluster, this means that these apps share the hardware, network, and operating system on the nodes of the cluster. Concretely, two containers of two different apps running on the same node are technically two processes running on the same hardware and operating system kernel. Linux containers provide some form of isolation, but this isolation is not as strong as the one provided by, for example, virtual machines (VMs). Under the hood, a process in a container is still just a process running on the host's operating system. This may be an issue from a security point of view \u2014 it theoretically allows unrelated apps to interact with each other in undesired ways (intentionally and unintentionally). Furthermore, all the workloads in a Kubernetes cluster share certain cluster-wide services, such as DNS \u2014 this allows apps to discover the Services of other apps in the cluster. It's important to keep in mind that Kubernetes is designed for sharing, and not for isolation and security. No hard multi-tenancy : Given the many shared resources in a Kubernetes cluster, there are many ways that different apps can \"step on each other's toes\". For example, an app may monopolize a certain shared resource, such as the CPU or memory, and thus starve other apps running on the same node. Kubernetes provides various ways to control this behaviour, however it's not trivial to tweak these tools in exactly the right way, and they cannot prevent every unwanted side effect either. Many users : If you have only a single cluster, then many people in your organisation must have access to this cluster. The more people have access to a system, the higher the risk that they break something. Within the cluster, you can control who can do what with role-based access control (RBAC) \u2014 however, this still can't prevent that users break something within their area of authorisation. Cluster per environment \u2691 With this option you have a separate cluster for each environment. For example, you can have a dev , test , and prod cluster where you run all the application instances of a specific environment. Isolation of the *prod environment*: In general, this approach isolates all the environments from each other \u2014 but, in practice, this especially matters for the prod environment. The production versions of your app are now not affected by whatever happens in any of the other clusters and application environments. So, if some misconfiguration creates havoc in your dev cluster, the prod versions of your apps keep running as if nothing had happened. Cluster can be customised for an environment : You can optimise each cluster for its environment \u2014 for example, you can: * Install development and debugging tools in the dev cluster. * Install testing frameworks and tools in the test cluster. * Use more powerful hardware and network connections for the prod cluster. This may improve the efficiency of both the development and operation of your apps. * Lock down access to prod cluster : Nobody really needs to do work on the prod cluster, so you can make access to it very restrictive. Cons: More administration and resources : In comparison with the single cluster. Lack of isolation between apps : The main disadvantage of this approach is the missing hardware and resource isolation between apps. Unrelated apps share cluster resources, such as the operating system kernel, CPU, memory, and several other services. As already mentioned, this may be a security issue. App requirements are not localised : If an app has special requirements, then these requirements must be satisfied in all clusters. Conclusion \u2691 If you don't need environment isolation and are not afraid of upgrading the Kubernetes cluster, go for the single cluster, otherwise use a cluster per environment. References \u2691 Kubernetes components overview","title":"Architecture"},{"location":"devops/kubernetes/kubernetes_architecture/#master-nodes","text":"Master nodes or Kubernetes Control Plane are the controlling unit for the cluster. They manage scheduling of new containers, configure networking and provide health information. To do so it uses: kube-api-server exposes the Kubernetes control plane API validating and configuring data for the different API objects. It's used by all the components to interact between themselves. etcd is a \"Distributed reliable key-value store for the most critical data of a distributed system\". Kubernetes uses Etcd to store state about the cluster and service discovery between nodes. This state includes what nodes exist in the cluster, which nodes they are running on and what containers should be running. kube-scheduler watches for newly created pods with no assigned node, and selects a node for them to run on. Factors taken into account for scheduling decisions include individual and collective resource requirements, hardware/software/policy constraints, affinity and anti-affinity specifications, data locality, inter-workload interference and deadlines. kube-controller-manager runs the following controllers: Node Controller : Responsible for noticing and responding when nodes go down. Replication Controller : Responsible for maintaining the correct number of pods for every replication controller object in the system. Endpoints Controller : Populates the Endpoints object (that is, joins Services & Pods). Service Account & Token Controllers : Create default accounts and API access tokens for new namespaces. cloud-controller-manager runs controllers that interact with the underlying cloud providers. Node Controller : For checking the cloud provider to determine if a node has been deleted in the cloud after it stops responding. Route Controller : For setting up routes in the underlying cloud infrastructure. Service Controller : For creating, updating and deleting cloud provider load balancers. Volume Controller : For creating, attaching, and mounting volumes, and interacting with the cloud provider to orchestrate volumes.","title":"Master Nodes"},{"location":"devops/kubernetes/kubernetes_architecture/#worker-nodes","text":"Worker nodes are the units that hold the application data of the cluster. There can be different types of nodes: CPU architecture, amount of resources (CPU, RAM, GPU) or cloud provider. Each node has the services necessary to run pods: Container Runtime : The software responsible for running containers (Docker, rkt, containerd, CRI-O). kubelet : The primary \u201cnode agent\u201d. It works in terms of a PodSpec (a YAML or JSON object that describes it). kubelet takes a set of PodSpecs from the masters kube-api-server and ensures that the containers described are running and healthy. kube-proxy is the network proxy that runs on each node. This reflects services as defined in the Kubernetes API on each node and can do simple TCP and UDP stream forwarding or round robin across a set of backends. kube-proxy maintains network rules on nodes. These network rules allow network communication to your Pods from network sessions inside or outside of your cluster. kube-proxy uses the operating system packet filtering layer if there is one and it's available. Otherwise, it will forward the traffic itself.","title":"Worker Nodes"},{"location":"devops/kubernetes/kubernetes_architecture/#kube-proxy-operation-modes","text":"kube-proxy currently supports three different operation modes: User space : This mode gets its name because the service routing takes place in kube-proxy in the user process space instead of in the kernel network stack. It is not commonly used as it is slow and outdated. iptables : This mode uses Linux kernel-level Netfilter rules to configure all routing for Kubernetes Services. This mode is the default for kube-proxy on most platforms. When load balancing for multiple backend pods, it uses unweighted round-robin scheduling. IPVS (IP Virtual Server) : Built on the Netfilter framework, IPVS implements Layer-4 load balancing in the Linux kernel, supporting multiple load-balancing algorithms, including least connections and shortest expected delay. This kube-proxy mode became generally available in Kubernetes 1.11, but it requires the Linux kernel to have the IPVS modules loaded. It is also not as widely supported by various Kubernetes networking projects as the iptables mode.","title":"kube-proxy operation modes"},{"location":"devops/kubernetes/kubernetes_architecture/#kubectl","text":"The kubectl is the command line client used to communicate with the Masters.","title":"Kubectl"},{"location":"devops/kubernetes/kubernetes_architecture/#number-of-clusters","text":"You can run a given set of workloads either on few large clusters (with many workloads in each cluster) or on many clusters (with few workloads in each cluster). Here's a table that summarizes the pros and cons of various approaches: Figure: Possibilities of number of clusters from learnk8s.io article Reference to the original article for a full read (it's really good!). I'm going to analyze only the Large shared cluster and Cluster per environment options, as they are the closest to my use case.","title":"Number of clusters"},{"location":"devops/kubernetes/kubernetes_architecture/#one-large-shared-cluster","text":"With this option, you run all your workloads in the same cluster. Kubernetes provides namespaces to logically separate portions of a cluster from each other, and in the above case, you could use a separate namespace for each application instance. Pros: Efficient resource usage : You need to have only one copy of all the resources that are needed to run and manage a Kubernetes cluster (master nodes, load balancers, Ingress controllers, authentication, logging, and monitoring). Cheap : As you avoid the duplication of resources, you require less hardware. Efficient administration : Administering a Kubernetes cluster requires: Upgrading the Kubernetes version Setting up a CI/CD pipeline Installing a CNI plugin Setting up the user authentication system Installing an admission controller If you have only a single cluster, you need to do all of this only once. If you have many clusters, then you need to apply everything multiple times, which probably requires you to develop some automated processes and tools for being able to do this consistently. Cons: Single point of failure : If you have only one cluster and if that cluster breaks, then all your workloads are down. There are many ways that something can go wrong: A Kubernetes upgrade produces unexpected side effects. An cluster-wide component (such as a CNI plugin) doesn't work as expected. An erroneous configuration is made to one of the cluster components. An outage occurs in the underlying infrastructure. A single incident like this can produce major damage across all your workloads if you have only a single shared cluster. No hard security isolation : If multiple apps run in the same Kubernetes cluster, this means that these apps share the hardware, network, and operating system on the nodes of the cluster. Concretely, two containers of two different apps running on the same node are technically two processes running on the same hardware and operating system kernel. Linux containers provide some form of isolation, but this isolation is not as strong as the one provided by, for example, virtual machines (VMs). Under the hood, a process in a container is still just a process running on the host's operating system. This may be an issue from a security point of view \u2014 it theoretically allows unrelated apps to interact with each other in undesired ways (intentionally and unintentionally). Furthermore, all the workloads in a Kubernetes cluster share certain cluster-wide services, such as DNS \u2014 this allows apps to discover the Services of other apps in the cluster. It's important to keep in mind that Kubernetes is designed for sharing, and not for isolation and security. No hard multi-tenancy : Given the many shared resources in a Kubernetes cluster, there are many ways that different apps can \"step on each other's toes\". For example, an app may monopolize a certain shared resource, such as the CPU or memory, and thus starve other apps running on the same node. Kubernetes provides various ways to control this behaviour, however it's not trivial to tweak these tools in exactly the right way, and they cannot prevent every unwanted side effect either. Many users : If you have only a single cluster, then many people in your organisation must have access to this cluster. The more people have access to a system, the higher the risk that they break something. Within the cluster, you can control who can do what with role-based access control (RBAC) \u2014 however, this still can't prevent that users break something within their area of authorisation.","title":"One Large shared cluster"},{"location":"devops/kubernetes/kubernetes_architecture/#cluster-per-environment","text":"With this option you have a separate cluster for each environment. For example, you can have a dev , test , and prod cluster where you run all the application instances of a specific environment. Isolation of the *prod environment*: In general, this approach isolates all the environments from each other \u2014 but, in practice, this especially matters for the prod environment. The production versions of your app are now not affected by whatever happens in any of the other clusters and application environments. So, if some misconfiguration creates havoc in your dev cluster, the prod versions of your apps keep running as if nothing had happened. Cluster can be customised for an environment : You can optimise each cluster for its environment \u2014 for example, you can: * Install development and debugging tools in the dev cluster. * Install testing frameworks and tools in the test cluster. * Use more powerful hardware and network connections for the prod cluster. This may improve the efficiency of both the development and operation of your apps. * Lock down access to prod cluster : Nobody really needs to do work on the prod cluster, so you can make access to it very restrictive. Cons: More administration and resources : In comparison with the single cluster. Lack of isolation between apps : The main disadvantage of this approach is the missing hardware and resource isolation between apps. Unrelated apps share cluster resources, such as the operating system kernel, CPU, memory, and several other services. As already mentioned, this may be a security issue. App requirements are not localised : If an app has special requirements, then these requirements must be satisfied in all clusters.","title":"Cluster per environment"},{"location":"devops/kubernetes/kubernetes_architecture/#conclusion","text":"If you don't need environment isolation and are not afraid of upgrading the Kubernetes cluster, go for the single cluster, otherwise use a cluster per environment.","title":"Conclusion"},{"location":"devops/kubernetes/kubernetes_architecture/#references","text":"Kubernetes components overview","title":"References"},{"location":"devops/kubernetes/kubernetes_cluster_autoscaler/","text":"While Horizontal pod autoscaling allows a deployment to scale given the resources needed, they are limited to the kubernetes existing working nodes. To autoscale the number of working nodes we need the cluster autoscaler . For AWS, there are the Amazon guidelines to enable it . But I'd use the cluster-autoscaler helm chart.","title":"Cluster Autoscaler"},{"location":"devops/kubernetes/kubernetes_dashboard/","text":"Dashboard definition Dashboard is a web-based Kubernetes user interface. You can use Dashboard to deploy containerized applications to a Kubernetes cluster, troubleshoot your containerized application, and manage the cluster resources. You can use Dashboard to get an overview of applications running on your cluster, as well as for creating or modifying individual Kubernetes resources (such as Deployments, Jobs, DaemonSets, etc). For example, you can scale a Deployment, initiate a rolling update, restart a pod or deploy new applications using a deploy wizard. Dashboard also provides information on the state of Kubernetes resources in your cluster and on any errors that may have occurred. Deployment \u2691 The best way to install it is with the stable/kubernetes-dashboard chart with helmfile . Links \u2691 Git Documentation Kubernetes introduction to the dashboard Hasham Haider guide","title":"Dashboard"},{"location":"devops/kubernetes/kubernetes_dashboard/#deployment","text":"The best way to install it is with the stable/kubernetes-dashboard chart with helmfile .","title":"Deployment"},{"location":"devops/kubernetes/kubernetes_dashboard/#links","text":"Git Documentation Kubernetes introduction to the dashboard Hasham Haider guide","title":"Links"},{"location":"devops/kubernetes/kubernetes_deployments/","text":"The different types of deployments configure a ReplicaSet and a PodSchema for your application. Depending on the type of application we'll use one of the following types. Deployments \u2691 Deployments are the controller for stateless applications, therefore it favors availability over consistency. It provides availability by creating multiple copies of the same pod. Those pods are disposable\u200a\u2014\u200aif they become unhealthy, Deployment will just create new replacements. What\u2019s more, you can update Deployment at a controlled rate, without a service outage. When an incident like the AWS outage happens, your workloads will automatically recover. Pods created by Deployment won't have the same identity after being killed and recreated, and they don't have unique persistent storage either. Concrete examples: Nginx, Tomcat A typical use case is: Create a Deployment to bring up a Replica Set and Pods. Check the status of a Deployment to see if it succeeds or not. Later, update that Deployment to recreate the Pods (for example, to use a new image). Rollback to an earlier Deployment revision if the current Deployment isn't stable. Pause and resume a Deployment. Deployment example apiVersion : apps/v1beta1 kind : Deployment metadata : name : nginx-deployment spec : replicas : 3 template : metadata : labels : app : nginx spec : containers : - name : nginx image : nginx:1.7.9 ports : - containerPort : 80 StatefulSets \u2691 StatefulSets are the controller for stateful applications, therefore it favors consistency over availability. If your applications need to store data, like databases, cache, and message queues, each of your stateful pod will need a stronger notion of identity. Unlike Deployment , StatefulSets creates pods with stable, unique, and sticky identity and storage. You can deploy, scale, and delete pods in order. Which is safer, and makes it easier for you to reason about your stateful applications. Concrete examples: Zookeeper, MongoDB, MySQL The tricky part of the StatefulSets is that in multi node cluster on different regions in AWS, as the persistence is achieved with EBS volumes and they are fixed to a region. Your pod will always spawn in the same node. If there is a failure in that node, the pod will be marked as unschedulable and the service will be down. So as of 2020-03-02 is still not advised to host your databases inside kubernetes unless you have an operator . DaemonSet \u2691 DaemonSets are the controller for applications that need to be run in each node. It's useful for recollecting log or monitoring purposes. DaemonSet makes sure that every node runs a copy of a pod. If you add or remove nodes, pods will be created or removed on them automatically. If you just want to run the daemons on some of the nodes, use node labels to control it. Concrete examples: fluentd, linkerd Job \u2691 Jobs are the controller to run batch processing workloads. It creates multiple pods running in parallel to process independent but related work items. This can be the emails to be sent or frames to be rendered.","title":"Deployments"},{"location":"devops/kubernetes/kubernetes_deployments/#deployments","text":"Deployments are the controller for stateless applications, therefore it favors availability over consistency. It provides availability by creating multiple copies of the same pod. Those pods are disposable\u200a\u2014\u200aif they become unhealthy, Deployment will just create new replacements. What\u2019s more, you can update Deployment at a controlled rate, without a service outage. When an incident like the AWS outage happens, your workloads will automatically recover. Pods created by Deployment won't have the same identity after being killed and recreated, and they don't have unique persistent storage either. Concrete examples: Nginx, Tomcat A typical use case is: Create a Deployment to bring up a Replica Set and Pods. Check the status of a Deployment to see if it succeeds or not. Later, update that Deployment to recreate the Pods (for example, to use a new image). Rollback to an earlier Deployment revision if the current Deployment isn't stable. Pause and resume a Deployment. Deployment example apiVersion : apps/v1beta1 kind : Deployment metadata : name : nginx-deployment spec : replicas : 3 template : metadata : labels : app : nginx spec : containers : - name : nginx image : nginx:1.7.9 ports : - containerPort : 80","title":"Deployments"},{"location":"devops/kubernetes/kubernetes_deployments/#statefulsets","text":"StatefulSets are the controller for stateful applications, therefore it favors consistency over availability. If your applications need to store data, like databases, cache, and message queues, each of your stateful pod will need a stronger notion of identity. Unlike Deployment , StatefulSets creates pods with stable, unique, and sticky identity and storage. You can deploy, scale, and delete pods in order. Which is safer, and makes it easier for you to reason about your stateful applications. Concrete examples: Zookeeper, MongoDB, MySQL The tricky part of the StatefulSets is that in multi node cluster on different regions in AWS, as the persistence is achieved with EBS volumes and they are fixed to a region. Your pod will always spawn in the same node. If there is a failure in that node, the pod will be marked as unschedulable and the service will be down. So as of 2020-03-02 is still not advised to host your databases inside kubernetes unless you have an operator .","title":"StatefulSets"},{"location":"devops/kubernetes/kubernetes_deployments/#daemonset","text":"DaemonSets are the controller for applications that need to be run in each node. It's useful for recollecting log or monitoring purposes. DaemonSet makes sure that every node runs a copy of a pod. If you add or remove nodes, pods will be created or removed on them automatically. If you just want to run the daemons on some of the nodes, use node labels to control it. Concrete examples: fluentd, linkerd","title":"DaemonSet"},{"location":"devops/kubernetes/kubernetes_deployments/#job","text":"Jobs are the controller to run batch processing workloads. It creates multiple pods running in parallel to process independent but related work items. This can be the emails to be sent or frames to be rendered.","title":"Job"},{"location":"devops/kubernetes/kubernetes_external_dns/","text":"The external-dns resource allows the creation of DNS records from within kubernetes inside the definition of service and ingress resources. It currently supports the following providers: Provider Status Google Cloud DNS Stable AWS Route 53 Stable AWS Cloud Map Beta AzureDNS Beta CloudFlare Beta RcodeZero Alpha DigitalOcean Alpha DNSimple Alpha Infoblox Alpha Dyn Alpha OpenStack Designate Alpha PowerDNS Alpha CoreDNS Alpha Exoscale Alpha Oracle Cloud Infrastructure DNS Alpha Linode DNS Alpha RFC2136 Alpha NS1 Alpha TransIP Alpha VinylDNS Alpha RancherDNS Alpha Akamai FastDNS Alpha There are two reasons to enable it: If there is any change in the ingress or service load balancer endpoint, due to a deployment, the dns records are automatically changed. It's easier for developers to connect their applications. Deployment in AWS \u2691 To install it inside EKS, create the ExternalDNSEKSIAMPolicy . ExternalDNSEKSIAMPolicy { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Action\" : [ \"route53:ChangeResourceRecordSets\" ], \"Resource\" : [ \"arn:aws:route53:::hostedzone/*\" ] }, { \"Effect\" : \"Allow\" , \"Action\" : [ \"route53:ListHostedZones\" , \"route53:ListResourceRecordSets\" ], \"Resource\" : [ \"*\" ] } ] } and the associated eks-external-dns role that will be attached to the pod service account. When defining iam_role resources that are going to be used inside EKS, you need to use the EKS OIDC provider as trusted entity, so instead of using the default empty role policy: { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Action\" : \"sts:AssumeRole\" , \"Effect\" : \"Allow\" , \"Principal\" : { \"Service\" : \"ec2.amazonaws.com\" } } ] } We'll use, as stated in the Creating an IAM Role and Policy for Service Account and Specifying an IAM Role for your Service Account documents: { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Action\" : \"sts:AssumeRoleWithWebIdentity\" , \"Condition\" : { \"StringEquals\" : { \"oidc.eks.us-east-1.amazonaws.com/id/{{ cluster_fingerprint }}:sub\" : \"system:serviceaccount:{{ service_account_namespace }}:{{ service_account_name }}\" } }, \"Effect\" : \"Allow\" , \"Principal\" : { \"Federated\" : \"arn:aws:iam::{{ aws_account_id }}:oidc-provider/oidc.eks.{{ aws_zone }}.amazonaws.com/id/{{ cluster_fingerprint }}\" } } ] } Then particularize the external-dns helm chart. There are two ways of attaching the IAM role to external-dns , using the asumeRoleArn attribute on the aws values.yaml key or under the rbac serviceAccountAnnotations . I've tried the first but it gave an error because the cluster IAM role didn't have permissions to execute the assume role on that specific role. The second worked flawlessly. For more information visit the official external-dns aws documentation .","title":"External DNS"},{"location":"devops/kubernetes/kubernetes_external_dns/#deployment-in-aws","text":"To install it inside EKS, create the ExternalDNSEKSIAMPolicy . ExternalDNSEKSIAMPolicy { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Action\" : [ \"route53:ChangeResourceRecordSets\" ], \"Resource\" : [ \"arn:aws:route53:::hostedzone/*\" ] }, { \"Effect\" : \"Allow\" , \"Action\" : [ \"route53:ListHostedZones\" , \"route53:ListResourceRecordSets\" ], \"Resource\" : [ \"*\" ] } ] } and the associated eks-external-dns role that will be attached to the pod service account. When defining iam_role resources that are going to be used inside EKS, you need to use the EKS OIDC provider as trusted entity, so instead of using the default empty role policy: { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Action\" : \"sts:AssumeRole\" , \"Effect\" : \"Allow\" , \"Principal\" : { \"Service\" : \"ec2.amazonaws.com\" } } ] } We'll use, as stated in the Creating an IAM Role and Policy for Service Account and Specifying an IAM Role for your Service Account documents: { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Action\" : \"sts:AssumeRoleWithWebIdentity\" , \"Condition\" : { \"StringEquals\" : { \"oidc.eks.us-east-1.amazonaws.com/id/{{ cluster_fingerprint }}:sub\" : \"system:serviceaccount:{{ service_account_namespace }}:{{ service_account_name }}\" } }, \"Effect\" : \"Allow\" , \"Principal\" : { \"Federated\" : \"arn:aws:iam::{{ aws_account_id }}:oidc-provider/oidc.eks.{{ aws_zone }}.amazonaws.com/id/{{ cluster_fingerprint }}\" } } ] } Then particularize the external-dns helm chart. There are two ways of attaching the IAM role to external-dns , using the asumeRoleArn attribute on the aws values.yaml key or under the rbac serviceAccountAnnotations . I've tried the first but it gave an error because the cluster IAM role didn't have permissions to execute the assume role on that specific role. The second worked flawlessly. For more information visit the official external-dns aws documentation .","title":"Deployment in AWS"},{"location":"devops/kubernetes/kubernetes_hpa/","text":"With Horizontal pod autoscaling , Kubernetes automatically scales the number of pods in a deployment or replication controller based on observed CPU utilization or on some other application provided metrics. The Horizontal Pod Autoscaler is implemented as a Kubernetes API resource and a controller. The resource determines the behavior of the controller. The controller periodically adjusts the number of replicas in a replication controller or deployment to match the observed average CPU utilization to the target specified by user. To make it work, the definition of pod resource consumption needs to be specified.","title":"Horizontal Pod Autoscaling"},{"location":"devops/kubernetes/kubernetes_ingress/","text":"An Ingress is An API object that manages external access to the services in a cluster, typically HTTP. Ingress provide a centralized way to: Load balancing. SSL termination. Dynamic service discovery. Traffic routing. Authentication. Traffic distribution: canary deployments, A/B testing, mirroring/shadowing. Graphical user interface. JWT validation. WAF and DDOS protection. Requests tracing. An Ingress controller is responsible for fulfilling the Ingress, usually with a load balancer, though it may also configure your edge router or additional frontends to help handle the traffic. An Ingress does not expose arbitrary ports or protocols. Exposing services other than HTTP and HTTPS to the internet typically uses a service of type NodePort or LoadBalancer .","title":"Ingress"},{"location":"devops/kubernetes/kubernetes_ingress_controller/","text":"Ingress controllers monitor the cluster events for the creation or modification of Ingress resources, modifying accordingly the underlying load balancers. They are not part of the master kube-controller-manager , so you'll need to install them manually. There are different Ingress controllers, such as AWS ALB, Nginx, HAProxy or Traefik, using one or other depends on your needs. Almost all controllers are open sourced and support dynamic service discovery, SSL termination or WebSockets. But they differ in: Supported protocols : HTTP, HTTPS, gRPC, HTTP/2.0, TCP (with SNI) or UDP. Underlying software : NGINX, Traefik, HAProxy or Envoy. Traffic routing : host and path, regular expression support. Namespace limitations : supported or not. Upstream probes : active checks, passive checks, retries, circuit breakers, custom health checks... Load balancing algorithms : round-robin, sticky sessions, rdp-cookie... Authentication : Basic, digest, Oauth, external auth, SSL certificate... Traffic distribution : canary deployments, A/B testing, mirroring/shadowing. Paid subscription : extended functionality or technical support. Graphical user interface : JWT validation : Customization of configuration : Basic DDOS protection mechanisms : rate limit, traffic filtering. WAF : Requests tracing : monitor, trace and debug requests via OpenTracing or other options. Both ITNext and Flant provide good ingress controller comparisons, a synoptical resume of both articles follows. Kubernetes Ingress controller \u2691 The \u201cofficial\u201d Kubernetes controller. Not to be confused with the one offered by the NGINX company. Developed by the community, it's based on the Nginx web server with a set of Lua plugins to implement extra features. Thanks to the popularity of NGINX and minimal modifications over it when using as a controller, it can be the simplest and most straightforward option for an average engineer dealing with K8s. Traefik \u2691 Originally, this proxy was created for the routing of requests for microservices and their dynamic environment, hence many of its useful features: Continuous update of configuration (no restarts) . Support for multiple load balancing algorithms. Web UI. Metrics export. Support for various protocols. REST API. Canary releases. Let\u2019s Encrypt certificates support. TCP/SSL with SNI. Traffic mirroring/shadowing. The main disadvantage is that in order to organize the high availability of the controller you have to install and connect its own KV-storage. In 2019, the same developers have developed Maesh . Another service mesh solution built on top of Traefik. HAProxy \u2691 HAProxy is well known as a proxy server and load balancer. As part of the Kubernetes cluster, it offers: * \u201csoft\u201d configuration update (without traffic loss) * DNS-based service discovery * Dynamic configuration through API. * Full customization of a config-files template (via replacing a ConfigMap). * Using Spring Boot functions. * Great number of supported balancing algorithms. In general, developers put emphasis on high speed, optimization, and efficiency in consumed resources. It\u2019s worth mentioning a lot of new features have appeared in a recent (June\u201919) v2.0 release, and even more (including OpenTracing support) is expected with upcoming v2.1. Istio Ingress \u2691 Istio is a comprehensive service mesh solution. It can manage not just all incoming outside traffic (as an Ingress controller) but control all traffic inside the cluster as well. Under the hood, Istio uses Envoy as a sidecar-proxy for each service. In essence, it is a large processor that can do almost anything. Its central idea is maximum control, extensibility, security, and transparency. With Istio Ingress, you can fine tune traffic routing, access authorization between services, balancing, monitoring, canary releases and much more. \u201c Back to microservices with Istio \u201d is a great intro to learn about Istio. ALB Ingress controller \u2691 The AWS ALB Ingress Controller satisfies Kubernetes ingress resources by provisioning Application Load Balancers . It's advantages are: AWS managed loadbalancer. Authentication with OIDC or Cognito. AWS WAF support. Natively redirect HTTP to HTTPS. Supports fixed response without forwarding to the application.. It has also the potential advantage of using IP traffic mode. ALB support two types of traffic: instance mode : Ingress traffic starts from the ALB and reaches the NodePort opened for your service. Traffic is then routed to the container Pods within the cluster. The number of hops for the packet to reach its destination in this mode is always two. IP mode : Ingress traffic starts from the ALB and reaches the container Pods within cluster directly. In order to use this mode, the networking plugin for the K8s cluster must use a secondary IP address on ENI as pod IP, aka AWS CNI plugin for K8s. The number of hops for the packet to reach its destination is always one. The IP mode gives the following advantages: The load balancer can be pod location-aware: reduce the chance to route traffic to an irrelevant node and then rely on kube-proxy and network agent. The number of hops for the packet to reach its destination is always one No extra overlay network comparing to using Network plugins (Calico, Flannel) directly int he cloud (AWS). It also has it's disadvantages: Even though AWS guides you on it's deployment , after two months of AWS Support cases, I wasn't able to deploy it using terraform and helm . You can't reuse existing ALBs instead of creating new ALB per ingress . Therefore ingress: false needs to be specified in the service helm chart and manually edit the ALB ingress helm chart to add each new service. ALB ingress deployment \u2691 This section is a defunct work in progress. I wasn't able to make it work, but it can be useful if you want to deploy it yourself. If you success, please make a PR . I've used the AWS Guide , in conjunction with the AWS general ALB controller documentation and the AWS general ALB documentation to define the properties of the incubator helm chart . Before that you need to an IAM policy ALBIngressControllerIAMPolicy to give the required permissions to manage the certificates, ALB creation and WAF integration. That IAM policy needs to be attached to the eks-alb-ingress-controller IAM role. You also had to create an IAM OIDC provider, which are entities in IAM that describe an external identity provider (IdP) service that supports the OpenID Connect (OIDC) standard, such as Google or Github. You use an IAM OIDC identity provider when you want to establish trust between an OIDC compatible IdP and your AWS account. This is useful when creating a mobile app or web application that requires access to AWS resources, but you don't want to create custom sign in code or manage your own user identities. The IAM OIDC provider is going to be used by the AWS EKS pod identity admission controller to attach IAM roles to specific service accounts. This will prevent the attachment of the IAM role to the whole node group. So instead of allowing every pod inside the worker groups to create the ALB, only the ones attached to the eks-alb-ingress-controller service account will be able to do so. Links \u2691 ITNext ingress controller comparison Flant ingress controller comparison","title":"Ingress Controller"},{"location":"devops/kubernetes/kubernetes_ingress_controller/#kubernetes-ingress-controller","text":"The \u201cofficial\u201d Kubernetes controller. Not to be confused with the one offered by the NGINX company. Developed by the community, it's based on the Nginx web server with a set of Lua plugins to implement extra features. Thanks to the popularity of NGINX and minimal modifications over it when using as a controller, it can be the simplest and most straightforward option for an average engineer dealing with K8s.","title":"Kubernetes Ingress controller"},{"location":"devops/kubernetes/kubernetes_ingress_controller/#traefik","text":"Originally, this proxy was created for the routing of requests for microservices and their dynamic environment, hence many of its useful features: Continuous update of configuration (no restarts) . Support for multiple load balancing algorithms. Web UI. Metrics export. Support for various protocols. REST API. Canary releases. Let\u2019s Encrypt certificates support. TCP/SSL with SNI. Traffic mirroring/shadowing. The main disadvantage is that in order to organize the high availability of the controller you have to install and connect its own KV-storage. In 2019, the same developers have developed Maesh . Another service mesh solution built on top of Traefik.","title":"Traefik"},{"location":"devops/kubernetes/kubernetes_ingress_controller/#haproxy","text":"HAProxy is well known as a proxy server and load balancer. As part of the Kubernetes cluster, it offers: * \u201csoft\u201d configuration update (without traffic loss) * DNS-based service discovery * Dynamic configuration through API. * Full customization of a config-files template (via replacing a ConfigMap). * Using Spring Boot functions. * Great number of supported balancing algorithms. In general, developers put emphasis on high speed, optimization, and efficiency in consumed resources. It\u2019s worth mentioning a lot of new features have appeared in a recent (June\u201919) v2.0 release, and even more (including OpenTracing support) is expected with upcoming v2.1.","title":"HAProxy"},{"location":"devops/kubernetes/kubernetes_ingress_controller/#istio-ingress","text":"Istio is a comprehensive service mesh solution. It can manage not just all incoming outside traffic (as an Ingress controller) but control all traffic inside the cluster as well. Under the hood, Istio uses Envoy as a sidecar-proxy for each service. In essence, it is a large processor that can do almost anything. Its central idea is maximum control, extensibility, security, and transparency. With Istio Ingress, you can fine tune traffic routing, access authorization between services, balancing, monitoring, canary releases and much more. \u201c Back to microservices with Istio \u201d is a great intro to learn about Istio.","title":"Istio Ingress"},{"location":"devops/kubernetes/kubernetes_ingress_controller/#alb-ingress-controller","text":"The AWS ALB Ingress Controller satisfies Kubernetes ingress resources by provisioning Application Load Balancers . It's advantages are: AWS managed loadbalancer. Authentication with OIDC or Cognito. AWS WAF support. Natively redirect HTTP to HTTPS. Supports fixed response without forwarding to the application.. It has also the potential advantage of using IP traffic mode. ALB support two types of traffic: instance mode : Ingress traffic starts from the ALB and reaches the NodePort opened for your service. Traffic is then routed to the container Pods within the cluster. The number of hops for the packet to reach its destination in this mode is always two. IP mode : Ingress traffic starts from the ALB and reaches the container Pods within cluster directly. In order to use this mode, the networking plugin for the K8s cluster must use a secondary IP address on ENI as pod IP, aka AWS CNI plugin for K8s. The number of hops for the packet to reach its destination is always one. The IP mode gives the following advantages: The load balancer can be pod location-aware: reduce the chance to route traffic to an irrelevant node and then rely on kube-proxy and network agent. The number of hops for the packet to reach its destination is always one No extra overlay network comparing to using Network plugins (Calico, Flannel) directly int he cloud (AWS). It also has it's disadvantages: Even though AWS guides you on it's deployment , after two months of AWS Support cases, I wasn't able to deploy it using terraform and helm . You can't reuse existing ALBs instead of creating new ALB per ingress . Therefore ingress: false needs to be specified in the service helm chart and manually edit the ALB ingress helm chart to add each new service.","title":"ALB Ingress controller"},{"location":"devops/kubernetes/kubernetes_ingress_controller/#alb-ingress-deployment","text":"This section is a defunct work in progress. I wasn't able to make it work, but it can be useful if you want to deploy it yourself. If you success, please make a PR . I've used the AWS Guide , in conjunction with the AWS general ALB controller documentation and the AWS general ALB documentation to define the properties of the incubator helm chart . Before that you need to an IAM policy ALBIngressControllerIAMPolicy to give the required permissions to manage the certificates, ALB creation and WAF integration. That IAM policy needs to be attached to the eks-alb-ingress-controller IAM role. You also had to create an IAM OIDC provider, which are entities in IAM that describe an external identity provider (IdP) service that supports the OpenID Connect (OIDC) standard, such as Google or Github. You use an IAM OIDC identity provider when you want to establish trust between an OIDC compatible IdP and your AWS account. This is useful when creating a mobile app or web application that requires access to AWS resources, but you don't want to create custom sign in code or manage your own user identities. The IAM OIDC provider is going to be used by the AWS EKS pod identity admission controller to attach IAM roles to specific service accounts. This will prevent the attachment of the IAM role to the whole node group. So instead of allowing every pod inside the worker groups to create the ALB, only the ones attached to the eks-alb-ingress-controller service account will be able to do so.","title":"ALB ingress deployment"},{"location":"devops/kubernetes/kubernetes_ingress_controller/#links","text":"ITNext ingress controller comparison Flant ingress controller comparison","title":"Links"},{"location":"devops/kubernetes/kubernetes_jobs/","text":"Kubernetes jobs creates one or more Pods and ensures that a specified number of them successfully terminate. As pods successfully complete, the Job tracks the successful completions. When a specified number of successful completions is reached, the task (ie, Job) is complete. Deleting a Job will clean up the Pods it created. Cronjobs creates Jobs on a repeating schedule. This example CronJob manifest prints the current time and a hello message every minute: apiVersion : batch/v1beta1 kind : CronJob metadata : name : hello spec : schedule : \"*/1 * * * *\" jobTemplate : spec : template : spec : containers : - name : hello image : busybox args : - /bin/sh - -c - date; echo Hello from the Kubernetes cluster restartPolicy : OnFailure To deploy cronjobs you can use the bambash helm chart . Check the kubectl commands to interact with jobs . Debugging job logs \u2691 To obtain the logs of a completed or failed job, you need to: Locate the cronjob you want to debug: kubectl get cronjobs -n cronjobs . Locate the associated job: kubectl get jobs -n cronjobs . Locate the associated pod: kubectl get pods -n cronjobs . If the pod still exists, you can execute kubectl logs -n cronjobs {{ pod_name }} . If the pod doesn't exist anymore, you need to search the pod in your log centralizer solution. Rerunning failed jobs \u2691 If you have a job that has failed after the 6 default retries, it will show up in your monitorization forever, to fix it, you can manually trigger the job yourself with: kubectl get job \"your-job\" -o json \\ | jq 'del(.spec.selector)' \\ | jq 'del(.spec.template.metadata.labels)' \\ | kubectl replace --force -f - Monitorization of cronjobs \u2691 Alerting of traditional Unix cronjobs meant sending an email if the job failed. Most job scheduling systems that have followed have provided the same experience, Kubernetes does not. One approach to alerting jobs is to use the Prometheus push gateway, allowing us to push richer metrics than the success/failure status. This approach has it\u2019s downsides; we have to update the code for our jobs, we also have to explicitly configure a push gateway location and update it if it changes (a burden alleviated by the pull based metrics for long lived workloads). You can use tools such as Sentry, but it will also require changes to the jobs. Jobs are powerful things allowing us to implement several different workflows, the combination of options can be overwhelming compared to a traditional Unix cron job. This variety makes it difficult to establish one simple rule for alerting failed jobs. Things get easier if we restrict ourselves to a subset of possible options. We will focus on non-concurrent jobs. The relationship between cronjobs and jobs makes the task ahead difficult. To make our life easier we will put one requirement on the jobs we create, they will have to include a label that associates them with the original cronjob. Below we present an example of our ideal cronjob (which matches what the helm chart deploys): apiVersion : batch/v1beta1 kind : CronJob metadata : name : our-task spec : schedule : \"*/5 * * * *\" successfulJobsHistoryLimit : 3 concurrencyPolicy : Forbid jobTemplate : metadata : labels : cron : our-task # <-- match created jobs with the cronjob spec : backoffLimit : 3 template : metadata : labels : cronjob : our-task spec : containers : - name : our-task command : - /user/bin/false image : alpine restartPolicy : Never Building our alert \u2691 We are also going to need some metrics to get us started. K8s does not provide us any by default, but fortunately kube-state-metrics is installed with the Prometheus operator chart, so we have the following metrics: kube_cronjob_labels{ cronjob=\"our-task\", namespace=\"default\"} 1 kube_job_created{ job=\"our-task-1520165700\", namespace=\"default\"} 1.520165707e+09 kube_job_failed{ condition=\"false\", job=\"our-task-1520165700\", namespace=\"default\"} 0 kube_job_failed{ condition=\"true\", job=\"our-task-1520165700\", namespace=\"default\"} 1 kube_job_labels{ job=\"our-task-1520165700\", label_cron=\"our-task\", namespace=\"default\"} 1 This shows the primary set of metrics we will be using to construct our alert. What is not shown above is the status of the cronjob. The big challenge with K8s cronjob alerting is that cronjobs themselves do not possess any status information, beyond the last time the cronjob created a job. The status information only exists on the job that the cronjob creates. In order to determine if our cronjob is failing, our first order of business is to find which jobs we should be looking at. A K8s cronjob creates new job objects and keeps a number of them around to help us debug the runs of our jobs. We have to be determine which job corresponds to the last run of our cronjob. If we have added the cron label to the jobs as above, we can find the last run time of the jobs for a given cronjob as follows: max ( kube_job_status_start_time * ON ( job_name ) GROUP_RIGHT () kube_job_labels { label_cron != \"\"} ) BY ( job_name , label_cron ) This query demonstrates an important technique when working with kube-state-metrics . For each API object it exported data on, it exports a time series including all the labels for that object. These time series have a value of 1. As such we can join the set of labels for an object onto the metrics about that object by multiplication. Depending on how your Prometheus instance is configured, the value of the job label on your metrics will likely be kube-state-metrics . kube-state-metrics adds a job label itself with the name of the job object. Prometheus resolves this collision of label names by including the raw metric\u2019s label as an job_name label. Since we are querying the start time of jobs, and there should only ever be one job with a given name. You may wonder why we need the max aggregation. Manually plugging the query into Prometheus may convince you that it is unnecessary. Consider though that you may have multiple instances of kube-state-metrics running for redundancy. Using max ensures our query is valid even if we have multiple instances of kube-state-metrics running. Issues of duplicate metrics are common when constructing production recording rules and alerts. We can find the start time of the most recent job for a given cronjob by finding the maximum of all job start times as follows: max ( kube_job_status_start_time * ON ( job_name ) GROUP_RIGHT () kube_job_labels { label_cron != \"\"} ) BY ( label_cron ) The only difference between this and the previous query is in the labels used for the aggregation. Now that we have the start time of each job, and the start time of the most recent job, we can do a simple equality match to find the start time of the most recent job for a given cronjob. We will create a metric for this: - record : job_cronjob : kube_job_status_start_time : max expr : | sum without ( label_cron , job_name ) ( label_replace ( label_replace ( max ( kube_job_status_start_time * ON ( job_name ) GROUP_RIGHT () kube_job_labels { label_cron != \"\"} ) BY ( job_name , label_cron ) == ON ( label_cron ) GROUP_LEFT () max ( kube_job_status_start_time * ON ( job_name ) GROUP_RIGHT () kube_job_labels { label_cron != \"\"} ) BY ( label_cron ) , \" job \", \" $1 \", \" job_name \", \" (.+) \" ) , \" cronjob \", \" $1 \", \" label_cron \", \" (.+) \" ) ) We have also taken the opportunity to adjust the labels to be a little more aesthetically pleasing. By copying job_name to job , label_cron to cronjob and removing job_name and label_cron . Now that we have the most recently started job for a given cronjob, we can find which, if any, have failed attempts: - record : job_cronjob : kube_job_status_failed : sum expr : | sum without ( label_cron , job_name ) ( clamp_max ( job_cronjob : kube_job_status_start_time : max , 1 ) * ON ( job ) GROUP_LEFT () label_replace ( label_replace ( ( kube_job_status_failed != 0 and kube_job_status_succeeded == 0 ) , \" job \", \" $1 \", \" job_name \", \" (.+) \" ) , \" cronjob \", \" $1 \", \" label_cron \", \" (.+) \" ) ) The initial clamp_max clause is used to transform our start times metric into a set of time series to perform label matching to filter another set of metrics. Multiplication by 1 (or addition of 0), is a useful means of filter and merging time series and it is well worth taking the time to understand the technique. We get those cronjobs that have a failed job and no successful ones with the query: ( kube_job_status_failed != 0 and kube_job_status_succeeded == 0 ) The kube_job_status_succeeded == 0 it's important, otherwise once a job has a failed instance, it doesn't matter if there's a posterior one that succeeded, we're going to keep on receiving the alert that it failed. We adjust the labels on the previous query to match our start time metric so ensure the labels have the same meaning as those on our job_cronjob:kube_job_status_start_time:max metric. The label matching on the multiplication will then perform our filtering. We now have a metric containing the set of most recently failed jobs, labeled by their parent cronjob, so we can now construct the alert: - alert : CronJobStatusFailed expr : job_cronjob : kube_job_status_failed : sum > 0 for : 1m annotations : description : ' {{ $labels.cronjob }} last run has failed {{ $value }} times. ' We use the kube_cronjob_labels here to merge in labels from the original cronjob.","title":"Jobs"},{"location":"devops/kubernetes/kubernetes_jobs/#debugging-job-logs","text":"To obtain the logs of a completed or failed job, you need to: Locate the cronjob you want to debug: kubectl get cronjobs -n cronjobs . Locate the associated job: kubectl get jobs -n cronjobs . Locate the associated pod: kubectl get pods -n cronjobs . If the pod still exists, you can execute kubectl logs -n cronjobs {{ pod_name }} . If the pod doesn't exist anymore, you need to search the pod in your log centralizer solution.","title":"Debugging job logs"},{"location":"devops/kubernetes/kubernetes_jobs/#rerunning-failed-jobs","text":"If you have a job that has failed after the 6 default retries, it will show up in your monitorization forever, to fix it, you can manually trigger the job yourself with: kubectl get job \"your-job\" -o json \\ | jq 'del(.spec.selector)' \\ | jq 'del(.spec.template.metadata.labels)' \\ | kubectl replace --force -f -","title":"Rerunning failed jobs"},{"location":"devops/kubernetes/kubernetes_jobs/#monitorization-of-cronjobs","text":"Alerting of traditional Unix cronjobs meant sending an email if the job failed. Most job scheduling systems that have followed have provided the same experience, Kubernetes does not. One approach to alerting jobs is to use the Prometheus push gateway, allowing us to push richer metrics than the success/failure status. This approach has it\u2019s downsides; we have to update the code for our jobs, we also have to explicitly configure a push gateway location and update it if it changes (a burden alleviated by the pull based metrics for long lived workloads). You can use tools such as Sentry, but it will also require changes to the jobs. Jobs are powerful things allowing us to implement several different workflows, the combination of options can be overwhelming compared to a traditional Unix cron job. This variety makes it difficult to establish one simple rule for alerting failed jobs. Things get easier if we restrict ourselves to a subset of possible options. We will focus on non-concurrent jobs. The relationship between cronjobs and jobs makes the task ahead difficult. To make our life easier we will put one requirement on the jobs we create, they will have to include a label that associates them with the original cronjob. Below we present an example of our ideal cronjob (which matches what the helm chart deploys): apiVersion : batch/v1beta1 kind : CronJob metadata : name : our-task spec : schedule : \"*/5 * * * *\" successfulJobsHistoryLimit : 3 concurrencyPolicy : Forbid jobTemplate : metadata : labels : cron : our-task # <-- match created jobs with the cronjob spec : backoffLimit : 3 template : metadata : labels : cronjob : our-task spec : containers : - name : our-task command : - /user/bin/false image : alpine restartPolicy : Never","title":"Monitorization of cronjobs"},{"location":"devops/kubernetes/kubernetes_jobs/#building-our-alert","text":"We are also going to need some metrics to get us started. K8s does not provide us any by default, but fortunately kube-state-metrics is installed with the Prometheus operator chart, so we have the following metrics: kube_cronjob_labels{ cronjob=\"our-task\", namespace=\"default\"} 1 kube_job_created{ job=\"our-task-1520165700\", namespace=\"default\"} 1.520165707e+09 kube_job_failed{ condition=\"false\", job=\"our-task-1520165700\", namespace=\"default\"} 0 kube_job_failed{ condition=\"true\", job=\"our-task-1520165700\", namespace=\"default\"} 1 kube_job_labels{ job=\"our-task-1520165700\", label_cron=\"our-task\", namespace=\"default\"} 1 This shows the primary set of metrics we will be using to construct our alert. What is not shown above is the status of the cronjob. The big challenge with K8s cronjob alerting is that cronjobs themselves do not possess any status information, beyond the last time the cronjob created a job. The status information only exists on the job that the cronjob creates. In order to determine if our cronjob is failing, our first order of business is to find which jobs we should be looking at. A K8s cronjob creates new job objects and keeps a number of them around to help us debug the runs of our jobs. We have to be determine which job corresponds to the last run of our cronjob. If we have added the cron label to the jobs as above, we can find the last run time of the jobs for a given cronjob as follows: max ( kube_job_status_start_time * ON ( job_name ) GROUP_RIGHT () kube_job_labels { label_cron != \"\"} ) BY ( job_name , label_cron ) This query demonstrates an important technique when working with kube-state-metrics . For each API object it exported data on, it exports a time series including all the labels for that object. These time series have a value of 1. As such we can join the set of labels for an object onto the metrics about that object by multiplication. Depending on how your Prometheus instance is configured, the value of the job label on your metrics will likely be kube-state-metrics . kube-state-metrics adds a job label itself with the name of the job object. Prometheus resolves this collision of label names by including the raw metric\u2019s label as an job_name label. Since we are querying the start time of jobs, and there should only ever be one job with a given name. You may wonder why we need the max aggregation. Manually plugging the query into Prometheus may convince you that it is unnecessary. Consider though that you may have multiple instances of kube-state-metrics running for redundancy. Using max ensures our query is valid even if we have multiple instances of kube-state-metrics running. Issues of duplicate metrics are common when constructing production recording rules and alerts. We can find the start time of the most recent job for a given cronjob by finding the maximum of all job start times as follows: max ( kube_job_status_start_time * ON ( job_name ) GROUP_RIGHT () kube_job_labels { label_cron != \"\"} ) BY ( label_cron ) The only difference between this and the previous query is in the labels used for the aggregation. Now that we have the start time of each job, and the start time of the most recent job, we can do a simple equality match to find the start time of the most recent job for a given cronjob. We will create a metric for this: - record : job_cronjob : kube_job_status_start_time : max expr : | sum without ( label_cron , job_name ) ( label_replace ( label_replace ( max ( kube_job_status_start_time * ON ( job_name ) GROUP_RIGHT () kube_job_labels { label_cron != \"\"} ) BY ( job_name , label_cron ) == ON ( label_cron ) GROUP_LEFT () max ( kube_job_status_start_time * ON ( job_name ) GROUP_RIGHT () kube_job_labels { label_cron != \"\"} ) BY ( label_cron ) , \" job \", \" $1 \", \" job_name \", \" (.+) \" ) , \" cronjob \", \" $1 \", \" label_cron \", \" (.+) \" ) ) We have also taken the opportunity to adjust the labels to be a little more aesthetically pleasing. By copying job_name to job , label_cron to cronjob and removing job_name and label_cron . Now that we have the most recently started job for a given cronjob, we can find which, if any, have failed attempts: - record : job_cronjob : kube_job_status_failed : sum expr : | sum without ( label_cron , job_name ) ( clamp_max ( job_cronjob : kube_job_status_start_time : max , 1 ) * ON ( job ) GROUP_LEFT () label_replace ( label_replace ( ( kube_job_status_failed != 0 and kube_job_status_succeeded == 0 ) , \" job \", \" $1 \", \" job_name \", \" (.+) \" ) , \" cronjob \", \" $1 \", \" label_cron \", \" (.+) \" ) ) The initial clamp_max clause is used to transform our start times metric into a set of time series to perform label matching to filter another set of metrics. Multiplication by 1 (or addition of 0), is a useful means of filter and merging time series and it is well worth taking the time to understand the technique. We get those cronjobs that have a failed job and no successful ones with the query: ( kube_job_status_failed != 0 and kube_job_status_succeeded == 0 ) The kube_job_status_succeeded == 0 it's important, otherwise once a job has a failed instance, it doesn't matter if there's a posterior one that succeeded, we're going to keep on receiving the alert that it failed. We adjust the labels on the previous query to match our start time metric so ensure the labels have the same meaning as those on our job_cronjob:kube_job_status_start_time:max metric. The label matching on the multiplication will then perform our filtering. We now have a metric containing the set of most recently failed jobs, labeled by their parent cronjob, so we can now construct the alert: - alert : CronJobStatusFailed expr : job_cronjob : kube_job_status_failed : sum > 0 for : 1m annotations : description : ' {{ $labels.cronjob }} last run has failed {{ $value }} times. ' We use the kube_cronjob_labels here to merge in labels from the original cronjob.","title":"Building our alert"},{"location":"devops/kubernetes/kubernetes_labels/","text":"Labels are identifying metadata key/value pairs attached to objects, such as pods. Labels are intended to give meaningful and relevant information to users, but which do not directly imply semantics to the core system. \"labels\" : { \"key1\" : \"value1\" , \"key2\" : \"value2\" }","title":"Labels"},{"location":"devops/kubernetes/kubernetes_metric_server/","text":"The metrics server monitors the resource consumption inside the cluster. It populates the information in kubectl top nodes to get the node status and gives the information to automatically autoscale deployments with Horizontal pod autoscaling . To install it, you can use the metrics-server helm chart. To test that the horizontal pod autoscaling is working, follow the AWS EKS guide .","title":"Metrics Server"},{"location":"devops/kubernetes/kubernetes_namespaces/","text":"Namespaces are virtual clusters backed by the same physical cluster. It's the first level of isolation between applications. When to Use Multiple Namespaces \u2691 Namespaces are intended for use in environments with many users spread across multiple teams, or projects. For clusters with a few to tens of users, you should not need to create or think about namespaces at all. Start using namespaces when you need the features they provide. Namespaces provide a scope for names. Names of resources need to be unique within a namespace, but not across namespaces. Namespaces are a way to divide cluster resources between multiple uses (via resource quota). It is not necessary to use multiple namespaces just to separate slightly different resources, such as different versions of the same software: use labels to distinguish resources within the same namespace.","title":"Namespaces"},{"location":"devops/kubernetes/kubernetes_namespaces/#when-to-use-multiple-namespaces","text":"Namespaces are intended for use in environments with many users spread across multiple teams, or projects. For clusters with a few to tens of users, you should not need to create or think about namespaces at all. Start using namespaces when you need the features they provide. Namespaces provide a scope for names. Names of resources need to be unique within a namespace, but not across namespaces. Namespaces are a way to divide cluster resources between multiple uses (via resource quota). It is not necessary to use multiple namespaces just to separate slightly different resources, such as different versions of the same software: use labels to distinguish resources within the same namespace.","title":"When to Use Multiple Namespaces"},{"location":"devops/kubernetes/kubernetes_networking/","text":"Container networking is the mechanism through which containers can optionally connect to other containers, the host, and outside networks like the internet. If you want to get a quickly grasp on how k8s networking works, I suggest you to read StackRox's Kubernetes networking demystified article . CNI comparison \u2691 Container networking is the mechanism through which containers can optionally connect to other containers, the host, and outside networks like the internet. There are different container networking plugins you can use for your cluster. To ensure that you choose the best one for your use case, I've made a summary based on the following resources: Rancher k8s CNI comparison . ITnext k8s CNI comparison . Mark Ramm-Christensen AWS CNI analysis . TL;DR \u2691 When using EKS, if you have no networking experience and understand and accept their disadvantages I'd use the AWS VPC CNI as it's installed by default. Nevertheless, the pod limit per node and the vendor locking makes interesting to migrate in the future to Calico. To make the transition smoother, you can enable Calico Network Policies with the AWS VPC CNI and get used to them before fully migrating to Calico. Calico seems to be the best solution when you need a greater control of the networking inside k8s, as it supports security features (NetworkPolicies or encryption), that can be improved even more with the integration with Istio. It's known to be easy to debug and has commercial support. If you aren't using EKS, you could evaluate to first use Canal as it uses Flannel simple network overlay but with Calico's Network Policies. If you do this, you could first focus in getting used to Network Policies and then dive further into the network configuration with Calico. But a deeper analysis should be done to assess if the compared difficulty justifies the need of this step. I don't recommend to use Flannel alone even though it's simple as you'll probably need security features such as NetworkPolicies, although they say, it's the best insecure solution for low resource or several architecture nodes. I wouldn't use Weave either unless you need encryption throughout all the internal network and multicast. Flannel \u2691 Flannel , a project developed by the CoreOS, is perhaps the most straightforward and popular CNI plugin available. It is one of the most mature examples of networking fabric for container orchestration systems, intended to allow for better inter-container and inter-host networking. As the CNI concept took off, a CNI plugin for Flannel was an early entry. Flannel is relatively easy to install and configure. It is packaged as a single binary called flanneld and can be installed by default by many common Kubernetes cluster deployment tools and in many Kubernetes distributions. Flannel can use the Kubernetes cluster\u2019s existing etcd cluster to store its state information using the API to avoid having to provision a dedicated data store. Flannel configures a layer 3 IPv4 overlay network. A large internal network is created that spans across every node within the cluster. Within this overlay network, each node is given a subnet to allocate IP addresses internally. As pods are provisioned, the Docker bridge interface on each node allocates an address for each new container. Pods within the same host can communicate using the Docker bridge, while pods on different hosts will have their traffic encapsulated in UDP packets by flanneld for routing to the appropriate destination. Flannel has several different types of backends available for encapsulation and routing. The default and recommended approach is to use VXLAN, as it offers both good performance and is less manual intervention than other options. Overall, Flannel is a good choice for most users. From an administrative perspective, it offers a simple networking model that sets up an environment that\u2019s suitable for most use cases when you only need the basics. In general, it\u2019s a safe bet to start out with Flannel until you need something that it cannot provide or you need security features, such as NetworkPolicies or encryption. It's also recommended if you have low resource nodes in your cluster (only a few GB of RAM, a few cores). Moreover, it is compatible with a very large number of architectures (amd64, arm, arm64, etc.). It is the only one, along with Cilium, that is able to correctly auto-detect your MTU, so you don\u2019t have to configure anything to let it work. Calico \u2691 Calico , is another popular networking option in the Kubernetes ecosystem. While Flannel is positioned as the simple choice, Calico is best known for its performance, flexibility, and power. Calico takes a more holistic view of networking, concerning itself not only with providing network connectivity between hosts and pods, but also with network security and administration. On a freshly provisioned Kubernetes cluster that meets the system requirements, Calico can be deployed quickly by applying a single manifest file. If you are interested in Calico\u2019s optional network policy capabilities, you can enable them by applying an additional manifest to your cluster. Unlike Flannel, Calico does not use an overlay network. Instead, Calico configures a layer 3 network that uses the BGP routing protocol to route packets between hosts. This means that packets do not need to be wrapped in an extra layer of encapsulation when moving between hosts. The BGP routing mechanism can direct packets natively without an extra step of wrapping traffic in an additional layer of traffic. Besides the performance that this offers, one side effect of this is that it allows for more conventional troubleshooting when network problems arise. While encapsulated solutions using technologies like VXLAN work well, the process manipulates packets in a way that can make tracing difficult. With Calico, the standard debugging tools have access to the same information they would in simple environments, making it easier for a wider range of developers and administrators to understand behavior. In addition to networking connectivity, Calico is well known for its advanced network features. Network policy is one of its most sought after capabilities. In addition, Calico can also integrate with Istio , a service mesh, to interpret and enforce policy for workloads within the cluster both at the service mesh layer and the network infrastructure layer. This means that you can configure powerful rules describing how pods should be able to send and accept traffic, improving security and control over your networking environment. Project Calico is a good choice for environments that support its requirements and when performance and features like network policy are important. Additionally, Calico offers commercial support if you\u2019re seeking a support contract or want to keep that option open for the future. In general, it\u2019s a good choice for when you want to be able to control your network instead of just configuring it once and forgetting about it. If you are looking on how to install Calico, you can start with Calico guide for clusters with less than 50 nodes or if you use EKS with AWS guide . Canal \u2691 Canal is an interesting option for quite a few reasons. First of all, Canal was the name for a project that sought to integrate the networking layer provided by flannel with the networking policy capabilities of Calico. As the contributors worked through the details however, it became apparent that a full integration was not necessarily needed if work was done on both projects to ensure standardization and flexibility. As a result, the official project became somewhat defunct, but the intended ability to deploy the two technology together was achieved. For this reason, it\u2019s still sometimes easiest to refer to the combination as \u201cCanal\u201d even if the project no longer exists. Because Canal is a combination of Flannel and Calico, its benefits are also at the intersection of these two technologies. The networking layer is the simple overlay provided by Flannel that works across many different deployment environments without much additional configuration. The network policy capabilities layered on top supplement the base network with Calico\u2019s powerful networking rule evaluation to provide additional security and control. After ensuring that the cluster fulfills the necessary system requirements, Canal can be deployed by applying two manifests, making it no more difficult to configure than either of the projects on their own. Canal is a good way for teams to start to experiment and gain experience with network policy before they\u2019re ready to experiment with changing their actual networking. In general, Canal is a good choice if you like the networking model that Flannel provides but find some of Calico\u2019s features enticing. The ability define network policy rules is a huge advantage from a security perspective and is, in many ways, Calico\u2019s killer feature. Being able to apply that technology onto a familiar networking layer means that you can get a more capable environment without having to go through much of a transition. Weave Net \u2691 Weave Net by Weaveworks is a CNI-capable networking option for Kubernetes that offers a different paradigm than the others we\u2019ve discussed so far. Weave creates a mesh overlay network between each of the nodes in the cluster, allowing for flexible routing between participants. This, coupled with a few other unique features, allows Weave to intelligently route in situations that might otherwise cause problems. To create its network, Weave relies on a routing component installed on each host in the network. These routers then exchange topology information to maintain an up-to-date view of the available network landscape. When looking to send traffic to a pod located on a different node, the weave router makes an automatic decision whether to send it via \u201cfast datapath\u201d or to fall back on the \u201csleeve\u201d packet forwarding method. Fast datapath is an approach that relies on the kernel\u2019s native Open vSwitch datapath module to forward packets to the appropriate pod without moving in and out of userspace multiple times. The Weave router updates the Open vSwitch configuration to ensure that the kernel layer has accurate information about how to route incoming packets. In contrast, sleeve mode is available as a backup when the networking topology isn\u2019t suitable for fast datapath routing. It is a slower encapsulation mode that can route packets in instances where fast datapath does not have the necessary routing information or connectivity. As traffic flows through the routers, they learn which peers are associated with which MAC addresses, allowing them to route more intelligently with fewer hops for subsequent traffic. This same mechanism helps each node self-correct when a network change alters the available routes. Like Calico, Weave also provides network policy capabilities for your cluster. This is automatically installed and configured when you set up Weave, so no additional configuration is necessary beyond adding your network rules. One thing that Weave provides that the other options do not is easy encryption for the entire network. While it adds quite a bit of network overhead, Weave can be configured to automatically encrypt all routed traffic by using NaCl encryption for sleeve traffic and, since it needs to encrypt VXLAN traffic in the kernel, IPsec ESP for fast datapath traffic. Another advantage of Weave is that it's the only CNI that supports multicast. In case you need it. Weave is a great option for those looking for feature rich encrypted networking without adding a large amount of complexity or management at the expense of worse overall performance. It is relatively easy to set up, offers many built in and automatically configured features, and can provide routing in scenarios where other solutions might fail. The mesh topography does put a limit on the size of the network that can be reasonably accommodated, but for most users, this won\u2019t be a problem. Additionally, Weave offers paid support for organizations that prefer to be able to have someone to contact for help and troubleshooting. AWS CNI \u2691 AWS developed their own CNI that uses Elastic Network Interfaces for pod networking. It's the default CNI if you use EKS. Advantages of the AWS CNI \u2691 Amazon native networking provides a number of significant advantages over some of the more traditional overlay network solutions for Kubernetes: Raw AWS network performance. Integration of tools familiar to AWS developers and admins, like AWS VPC flow logs and Security Groups \u2014 allowing users with existing VPC networks and networking best practices to carry those over directly to Kubernetes. The ability to enforce network policy decisions at the Kubernetes layer if you install Calico. If your team has significant experience with AWS networking, and/or your application is sensitive to network performance all of this makes VPC CNI very attractive. Disadvantages of the AWS CNI \u2691 On the other hand, there are a couple of limitations that may be significant to you. There are three primary reasons why you might instead choose an overlay network. Makes the multi-cloud k8s advantage more difficult. the CNI limits the number of pods that can be scheduled on each k8s node according to the number of IP Addresses available to each EC2 instance type so that each pod can be allocated an IP. Doesn't support encryption on the network. Multicast requirements. It eats up the number of IP Addresses available within your VPC unless you give it an alternate subnet. VPC CNI Pod Density Limitations \u2691 First, the VPC CNI plugin is designed to use/abuse ENI interfaces to get each pod in your cluster its own IP address from Amazon directly. This means that you will be network limited in the number of pods that you can run on any given worker node in the cluster. The primary IP for each ENI is used for cluster communication purposes. New pods are assigned to one of the secondary IPs for that ENI. VPC CNI has a custom DaemonSet that manages the assignment of IPs to pods. Because ENI and IP allocation requests can take time, this l-ipam daemon creates a warm pool of ENIs and IPs on each node and uses one of the available IPs for each new pod as it is assigned. This yields the following formula for maximum pod density for any given instance: ENIs * (IPs_per_ENI - 1) Each instance type in Amazon has unique limitations in the number of ENIs and IPs per ENI allowed. For example, an m5.large worker node allows 25 pod IP addresses per node at an approximate cost of $2.66/month per pod. Stepping up to an m5.xlarge allows for a theoretical maximum of 55 pods, for a monthly cost of $2.62, making the m5.large the more cost effective node choice by a small amount for clusters bound by IP address limitations. And if that set of calculations is not enough, there are a few other factors to consider. Kubernetes clusters generally run a set of services on each node. VPC CNI itself uses an l-ipam DaemonSet, and if you want Kubernetes network policies, calico requires another. Furthermore, production clusters generally also have DaemonSets for metrics collection, log aggregation, and other cluster-wide services. Each of these uses an IP address per node. So now the formula is: (ENIs * (IPs_per_ENI - 1) - 1 * DaemonSets This makes some of the cheaper instances on Amazon completely unusable because there are no IP addresses left for application pods. On the other hand, Kubernetes itself has a supported limit of 100 pods per node, making some of the larger instances with lots of available addresses less attractive. However, the pod per node limit IS configurable, and in my experience, this limit can be increased without much increase in Kubernetes overhead. Weave people made a quick Google sheet with each of the Amazon instance types, and the maximum pod densities for each based on the VPC CNI network plugin restrictions: A 100 pod/node limit setting in Kubernetes, A default of 4 DaemonSets (2 for AWS networking, 1 for log aggregation, and 1 for metric collection), A simple cost calculation for per-pod pricing. This sheet is not intended to provide a definitive answer on pod economics for AWS VPC based Kubernetes clusters. There are a number of important caveats to the use of this sheet: CPU and memory requirements will often dictate lower pod density than the theoretical maximum here. Beyond DaemonSets, Kubernetes System pods, and other \u201csystem level\u201d operational tools used in your cluster will consume Pod IP\u2019s and limit the number of application pods that you can run. Each instance type also has network performance limitations which may impact performance often far before theoretical pod limits are reached. Cloud Portability \u2691 Hybrid cloud, disaster recovery, and other requirements often push users away from custom cloud vendor solutions and towards open solutions. However, in this case the level of lock-in is quite low. The CNI layer provides a level of abstraction on top of the underlying network, and it is possible to deploy the same workloads using the same deployment configurations with different CNI backends. Which from my point of view, seems a bad and ugly idea. Links \u2691 StackRox Kubernetes networking demystified article . Writing your own simple CNI plug in .","title":"Networking"},{"location":"devops/kubernetes/kubernetes_networking/#cni-comparison","text":"Container networking is the mechanism through which containers can optionally connect to other containers, the host, and outside networks like the internet. There are different container networking plugins you can use for your cluster. To ensure that you choose the best one for your use case, I've made a summary based on the following resources: Rancher k8s CNI comparison . ITnext k8s CNI comparison . Mark Ramm-Christensen AWS CNI analysis .","title":"CNI comparison"},{"location":"devops/kubernetes/kubernetes_networking/#tldr","text":"When using EKS, if you have no networking experience and understand and accept their disadvantages I'd use the AWS VPC CNI as it's installed by default. Nevertheless, the pod limit per node and the vendor locking makes interesting to migrate in the future to Calico. To make the transition smoother, you can enable Calico Network Policies with the AWS VPC CNI and get used to them before fully migrating to Calico. Calico seems to be the best solution when you need a greater control of the networking inside k8s, as it supports security features (NetworkPolicies or encryption), that can be improved even more with the integration with Istio. It's known to be easy to debug and has commercial support. If you aren't using EKS, you could evaluate to first use Canal as it uses Flannel simple network overlay but with Calico's Network Policies. If you do this, you could first focus in getting used to Network Policies and then dive further into the network configuration with Calico. But a deeper analysis should be done to assess if the compared difficulty justifies the need of this step. I don't recommend to use Flannel alone even though it's simple as you'll probably need security features such as NetworkPolicies, although they say, it's the best insecure solution for low resource or several architecture nodes. I wouldn't use Weave either unless you need encryption throughout all the internal network and multicast.","title":"TL;DR"},{"location":"devops/kubernetes/kubernetes_networking/#flannel","text":"Flannel , a project developed by the CoreOS, is perhaps the most straightforward and popular CNI plugin available. It is one of the most mature examples of networking fabric for container orchestration systems, intended to allow for better inter-container and inter-host networking. As the CNI concept took off, a CNI plugin for Flannel was an early entry. Flannel is relatively easy to install and configure. It is packaged as a single binary called flanneld and can be installed by default by many common Kubernetes cluster deployment tools and in many Kubernetes distributions. Flannel can use the Kubernetes cluster\u2019s existing etcd cluster to store its state information using the API to avoid having to provision a dedicated data store. Flannel configures a layer 3 IPv4 overlay network. A large internal network is created that spans across every node within the cluster. Within this overlay network, each node is given a subnet to allocate IP addresses internally. As pods are provisioned, the Docker bridge interface on each node allocates an address for each new container. Pods within the same host can communicate using the Docker bridge, while pods on different hosts will have their traffic encapsulated in UDP packets by flanneld for routing to the appropriate destination. Flannel has several different types of backends available for encapsulation and routing. The default and recommended approach is to use VXLAN, as it offers both good performance and is less manual intervention than other options. Overall, Flannel is a good choice for most users. From an administrative perspective, it offers a simple networking model that sets up an environment that\u2019s suitable for most use cases when you only need the basics. In general, it\u2019s a safe bet to start out with Flannel until you need something that it cannot provide or you need security features, such as NetworkPolicies or encryption. It's also recommended if you have low resource nodes in your cluster (only a few GB of RAM, a few cores). Moreover, it is compatible with a very large number of architectures (amd64, arm, arm64, etc.). It is the only one, along with Cilium, that is able to correctly auto-detect your MTU, so you don\u2019t have to configure anything to let it work.","title":"Flannel"},{"location":"devops/kubernetes/kubernetes_networking/#calico","text":"Calico , is another popular networking option in the Kubernetes ecosystem. While Flannel is positioned as the simple choice, Calico is best known for its performance, flexibility, and power. Calico takes a more holistic view of networking, concerning itself not only with providing network connectivity between hosts and pods, but also with network security and administration. On a freshly provisioned Kubernetes cluster that meets the system requirements, Calico can be deployed quickly by applying a single manifest file. If you are interested in Calico\u2019s optional network policy capabilities, you can enable them by applying an additional manifest to your cluster. Unlike Flannel, Calico does not use an overlay network. Instead, Calico configures a layer 3 network that uses the BGP routing protocol to route packets between hosts. This means that packets do not need to be wrapped in an extra layer of encapsulation when moving between hosts. The BGP routing mechanism can direct packets natively without an extra step of wrapping traffic in an additional layer of traffic. Besides the performance that this offers, one side effect of this is that it allows for more conventional troubleshooting when network problems arise. While encapsulated solutions using technologies like VXLAN work well, the process manipulates packets in a way that can make tracing difficult. With Calico, the standard debugging tools have access to the same information they would in simple environments, making it easier for a wider range of developers and administrators to understand behavior. In addition to networking connectivity, Calico is well known for its advanced network features. Network policy is one of its most sought after capabilities. In addition, Calico can also integrate with Istio , a service mesh, to interpret and enforce policy for workloads within the cluster both at the service mesh layer and the network infrastructure layer. This means that you can configure powerful rules describing how pods should be able to send and accept traffic, improving security and control over your networking environment. Project Calico is a good choice for environments that support its requirements and when performance and features like network policy are important. Additionally, Calico offers commercial support if you\u2019re seeking a support contract or want to keep that option open for the future. In general, it\u2019s a good choice for when you want to be able to control your network instead of just configuring it once and forgetting about it. If you are looking on how to install Calico, you can start with Calico guide for clusters with less than 50 nodes or if you use EKS with AWS guide .","title":"Calico"},{"location":"devops/kubernetes/kubernetes_networking/#canal","text":"Canal is an interesting option for quite a few reasons. First of all, Canal was the name for a project that sought to integrate the networking layer provided by flannel with the networking policy capabilities of Calico. As the contributors worked through the details however, it became apparent that a full integration was not necessarily needed if work was done on both projects to ensure standardization and flexibility. As a result, the official project became somewhat defunct, but the intended ability to deploy the two technology together was achieved. For this reason, it\u2019s still sometimes easiest to refer to the combination as \u201cCanal\u201d even if the project no longer exists. Because Canal is a combination of Flannel and Calico, its benefits are also at the intersection of these two technologies. The networking layer is the simple overlay provided by Flannel that works across many different deployment environments without much additional configuration. The network policy capabilities layered on top supplement the base network with Calico\u2019s powerful networking rule evaluation to provide additional security and control. After ensuring that the cluster fulfills the necessary system requirements, Canal can be deployed by applying two manifests, making it no more difficult to configure than either of the projects on their own. Canal is a good way for teams to start to experiment and gain experience with network policy before they\u2019re ready to experiment with changing their actual networking. In general, Canal is a good choice if you like the networking model that Flannel provides but find some of Calico\u2019s features enticing. The ability define network policy rules is a huge advantage from a security perspective and is, in many ways, Calico\u2019s killer feature. Being able to apply that technology onto a familiar networking layer means that you can get a more capable environment without having to go through much of a transition.","title":"Canal"},{"location":"devops/kubernetes/kubernetes_networking/#weave-net","text":"Weave Net by Weaveworks is a CNI-capable networking option for Kubernetes that offers a different paradigm than the others we\u2019ve discussed so far. Weave creates a mesh overlay network between each of the nodes in the cluster, allowing for flexible routing between participants. This, coupled with a few other unique features, allows Weave to intelligently route in situations that might otherwise cause problems. To create its network, Weave relies on a routing component installed on each host in the network. These routers then exchange topology information to maintain an up-to-date view of the available network landscape. When looking to send traffic to a pod located on a different node, the weave router makes an automatic decision whether to send it via \u201cfast datapath\u201d or to fall back on the \u201csleeve\u201d packet forwarding method. Fast datapath is an approach that relies on the kernel\u2019s native Open vSwitch datapath module to forward packets to the appropriate pod without moving in and out of userspace multiple times. The Weave router updates the Open vSwitch configuration to ensure that the kernel layer has accurate information about how to route incoming packets. In contrast, sleeve mode is available as a backup when the networking topology isn\u2019t suitable for fast datapath routing. It is a slower encapsulation mode that can route packets in instances where fast datapath does not have the necessary routing information or connectivity. As traffic flows through the routers, they learn which peers are associated with which MAC addresses, allowing them to route more intelligently with fewer hops for subsequent traffic. This same mechanism helps each node self-correct when a network change alters the available routes. Like Calico, Weave also provides network policy capabilities for your cluster. This is automatically installed and configured when you set up Weave, so no additional configuration is necessary beyond adding your network rules. One thing that Weave provides that the other options do not is easy encryption for the entire network. While it adds quite a bit of network overhead, Weave can be configured to automatically encrypt all routed traffic by using NaCl encryption for sleeve traffic and, since it needs to encrypt VXLAN traffic in the kernel, IPsec ESP for fast datapath traffic. Another advantage of Weave is that it's the only CNI that supports multicast. In case you need it. Weave is a great option for those looking for feature rich encrypted networking without adding a large amount of complexity or management at the expense of worse overall performance. It is relatively easy to set up, offers many built in and automatically configured features, and can provide routing in scenarios where other solutions might fail. The mesh topography does put a limit on the size of the network that can be reasonably accommodated, but for most users, this won\u2019t be a problem. Additionally, Weave offers paid support for organizations that prefer to be able to have someone to contact for help and troubleshooting.","title":"Weave Net"},{"location":"devops/kubernetes/kubernetes_networking/#aws-cni","text":"AWS developed their own CNI that uses Elastic Network Interfaces for pod networking. It's the default CNI if you use EKS.","title":"AWS CNI"},{"location":"devops/kubernetes/kubernetes_networking/#advantages-of-the-aws-cni","text":"Amazon native networking provides a number of significant advantages over some of the more traditional overlay network solutions for Kubernetes: Raw AWS network performance. Integration of tools familiar to AWS developers and admins, like AWS VPC flow logs and Security Groups \u2014 allowing users with existing VPC networks and networking best practices to carry those over directly to Kubernetes. The ability to enforce network policy decisions at the Kubernetes layer if you install Calico. If your team has significant experience with AWS networking, and/or your application is sensitive to network performance all of this makes VPC CNI very attractive.","title":"Advantages of the AWS CNI"},{"location":"devops/kubernetes/kubernetes_networking/#disadvantages-of-the-aws-cni","text":"On the other hand, there are a couple of limitations that may be significant to you. There are three primary reasons why you might instead choose an overlay network. Makes the multi-cloud k8s advantage more difficult. the CNI limits the number of pods that can be scheduled on each k8s node according to the number of IP Addresses available to each EC2 instance type so that each pod can be allocated an IP. Doesn't support encryption on the network. Multicast requirements. It eats up the number of IP Addresses available within your VPC unless you give it an alternate subnet.","title":"Disadvantages of the AWS CNI"},{"location":"devops/kubernetes/kubernetes_networking/#vpc-cni-pod-density-limitations","text":"First, the VPC CNI plugin is designed to use/abuse ENI interfaces to get each pod in your cluster its own IP address from Amazon directly. This means that you will be network limited in the number of pods that you can run on any given worker node in the cluster. The primary IP for each ENI is used for cluster communication purposes. New pods are assigned to one of the secondary IPs for that ENI. VPC CNI has a custom DaemonSet that manages the assignment of IPs to pods. Because ENI and IP allocation requests can take time, this l-ipam daemon creates a warm pool of ENIs and IPs on each node and uses one of the available IPs for each new pod as it is assigned. This yields the following formula for maximum pod density for any given instance: ENIs * (IPs_per_ENI - 1) Each instance type in Amazon has unique limitations in the number of ENIs and IPs per ENI allowed. For example, an m5.large worker node allows 25 pod IP addresses per node at an approximate cost of $2.66/month per pod. Stepping up to an m5.xlarge allows for a theoretical maximum of 55 pods, for a monthly cost of $2.62, making the m5.large the more cost effective node choice by a small amount for clusters bound by IP address limitations. And if that set of calculations is not enough, there are a few other factors to consider. Kubernetes clusters generally run a set of services on each node. VPC CNI itself uses an l-ipam DaemonSet, and if you want Kubernetes network policies, calico requires another. Furthermore, production clusters generally also have DaemonSets for metrics collection, log aggregation, and other cluster-wide services. Each of these uses an IP address per node. So now the formula is: (ENIs * (IPs_per_ENI - 1) - 1 * DaemonSets This makes some of the cheaper instances on Amazon completely unusable because there are no IP addresses left for application pods. On the other hand, Kubernetes itself has a supported limit of 100 pods per node, making some of the larger instances with lots of available addresses less attractive. However, the pod per node limit IS configurable, and in my experience, this limit can be increased without much increase in Kubernetes overhead. Weave people made a quick Google sheet with each of the Amazon instance types, and the maximum pod densities for each based on the VPC CNI network plugin restrictions: A 100 pod/node limit setting in Kubernetes, A default of 4 DaemonSets (2 for AWS networking, 1 for log aggregation, and 1 for metric collection), A simple cost calculation for per-pod pricing. This sheet is not intended to provide a definitive answer on pod economics for AWS VPC based Kubernetes clusters. There are a number of important caveats to the use of this sheet: CPU and memory requirements will often dictate lower pod density than the theoretical maximum here. Beyond DaemonSets, Kubernetes System pods, and other \u201csystem level\u201d operational tools used in your cluster will consume Pod IP\u2019s and limit the number of application pods that you can run. Each instance type also has network performance limitations which may impact performance often far before theoretical pod limits are reached.","title":"VPC CNI Pod Density Limitations"},{"location":"devops/kubernetes/kubernetes_networking/#cloud-portability","text":"Hybrid cloud, disaster recovery, and other requirements often push users away from custom cloud vendor solutions and towards open solutions. However, in this case the level of lock-in is quite low. The CNI layer provides a level of abstraction on top of the underlying network, and it is possible to deploy the same workloads using the same deployment configurations with different CNI backends. Which from my point of view, seems a bad and ugly idea.","title":"Cloud Portability"},{"location":"devops/kubernetes/kubernetes_networking/#links","text":"StackRox Kubernetes networking demystified article . Writing your own simple CNI plug in .","title":"Links"},{"location":"devops/kubernetes/kubernetes_operators/","text":"Operators are Kubernetes specific applications (pods) that configure, manage and optimize other Kubernetes deployments automatically. A Kubernetes Operator might be able to: Install and provide sane initial configuration and sizing for your deployment, according to the specs of your Kubernetes cluster. Perform live reloading of deployments and pods to accommodate for any user requested parameter modification (hot config reloading). Safe coordination of application upgrades. Automatically scale up or down according to performance metrics. Service discovery via native Kubernetes APIs Application TLS certificate configuration Disaster recovery. Perform backups to offsite storage, integrity checks or any other maintenance task. How do they work? \u2691 An Operator encodes this domain knowledge and extends the Kubernetes API through the third party resources mechanism, enabling users to create, configure, and manage applications. Like Kubernetes's built-in resources, an Operator doesn't manage just a single instance of the application, but multiple instances across the cluster. Operators build upon two central Kubernetes concepts: Resources and Controllers. As an example, the built-in ReplicaSet resource lets users set a desired number number of Pods to run, and controllers inside Kubernetes ensure the desired state set in the ReplicaSet resource remains true by creating or removing running Pods. There are many fundamental controllers and resources in Kubernetes that work in this manner, including Services, Deployments, and Daemon Sets. An Operator builds upon the basic Kubernetes resource and controller concepts and adds a set of knowledge or configuration that allows the Operator to execute common application tasks. For example, when scaling an etcd cluster manually, a user has to perform a number of steps: create a DNS name for the new etcd member, launch the new etcd instance, and then use the etcd administrative tools (etcdctl member add) to tell the existing cluster about this new member. Instead with the etcd Operator a user can simply increase the etcd cluster size field by 1. Links \u2691 CoreOS introduction to Operators Sysdig Prometheus Operator guide part 3","title":"Operators"},{"location":"devops/kubernetes/kubernetes_operators/#how-do-they-work","text":"An Operator encodes this domain knowledge and extends the Kubernetes API through the third party resources mechanism, enabling users to create, configure, and manage applications. Like Kubernetes's built-in resources, an Operator doesn't manage just a single instance of the application, but multiple instances across the cluster. Operators build upon two central Kubernetes concepts: Resources and Controllers. As an example, the built-in ReplicaSet resource lets users set a desired number number of Pods to run, and controllers inside Kubernetes ensure the desired state set in the ReplicaSet resource remains true by creating or removing running Pods. There are many fundamental controllers and resources in Kubernetes that work in this manner, including Services, Deployments, and Daemon Sets. An Operator builds upon the basic Kubernetes resource and controller concepts and adds a set of knowledge or configuration that allows the Operator to execute common application tasks. For example, when scaling an etcd cluster manually, a user has to perform a number of steps: create a DNS name for the new etcd member, launch the new etcd instance, and then use the etcd administrative tools (etcdctl member add) to tell the existing cluster about this new member. Instead with the etcd Operator a user can simply increase the etcd cluster size field by 1.","title":"How do they work?"},{"location":"devops/kubernetes/kubernetes_operators/#links","text":"CoreOS introduction to Operators Sysdig Prometheus Operator guide part 3","title":"Links"},{"location":"devops/kubernetes/kubernetes_pods/","text":"Pods are the basic building block of Kubernetes, the smallest and simplest unit in the object model that you create or deploy. A Pod represents a running process on your cluster. A Pod represents a unit of deployment. It encapsulates: An application container (or, in some cases, multiple tightly coupled containers). Storage resources. A unique network IP. Options that govern how the container(s) should run.","title":"Pods"},{"location":"devops/kubernetes/kubernetes_replicasets/","text":"ReplicaSet maintains a stable set of replica Pods running at any given time. As such, it is often used to guarantee the availability of a specified number of identical Pods. You'll probably never manually use these resources, as they are defined inside the deployments . The older version of this resource are the Replication controllers .","title":"ReplicaSets"},{"location":"devops/kubernetes/kubernetes_services/","text":"A Service defines a policy to access a logical set of Pods using a reliable endpoint. Users and other programs can access pods running on your cluster seamlessly. Therefore allowing a loose coupling between dependent Pods. When a request arrives the endpoint, the kube-proxy pod of the node forwards the request to the Pods that match the service LabelSelector. Services can be exposed in different ways by specifying a type in the ServiceSpec: ClusterIP (default): Exposes the Service on an internal IP in the cluster. This type makes the Service only reachable from within the cluster. NodePort : Exposes the Service on the same port of each selected Node in the cluster using NAT to the outside. LoadBalancer : Creates an external load balancer in the current cloud and assigns a fixed, external IP to the Service. To create an internal ELB of AWs add to the annotations: annotations : service.beta.kubernetes.io/aws-load-balancer-internal : 0.0.0.0/0 ExternalName : Exposes the Service using an arbitrary name by returning a CNAME record with the name. No proxy is used. If no RBAC or NetworkPolicies are applied, you can call a service of another namespace with the following nomenclature. curl {{ service_name }} . {{ service_namespace }} .svc.cluster.local","title":"Services"},{"location":"devops/kubernetes/kubernetes_storage_driver/","text":"Storage drivers are pods that through the Container Storage Interface or CSI provide an interface to use external storage services from within Kubernetes. Amazon EBS CSI storage driver \u2691 Allows Kubernetes clusters to manage the lifecycle of Amazon EBS volumes for persistent volumes with the awsElasticBlockStore volume type . To install it, you first need to attach the Amazon_EBS_CSI_Driver IAM policy to the worker nodes. Then you can use the aws-ebs-csi-driver helm chart. To test it worked follow the steps under To deploy a sample application and verify that the CSI driver is working .","title":"Storage Driver"},{"location":"devops/kubernetes/kubernetes_storage_driver/#amazon-ebs-csi-storage-driver","text":"Allows Kubernetes clusters to manage the lifecycle of Amazon EBS volumes for persistent volumes with the awsElasticBlockStore volume type . To install it, you first need to attach the Amazon_EBS_CSI_Driver IAM policy to the worker nodes. Then you can use the aws-ebs-csi-driver helm chart. To test it worked follow the steps under To deploy a sample application and verify that the CSI driver is working .","title":"Amazon EBS CSI storage driver"},{"location":"devops/kubernetes/kubernetes_tools/","text":"There are several tools built to enhance the operation, installation and use of Kubernetes. Tried \u2691 K3s : Recommended small kubernetes, like hyperkube. To try \u2691 crossplane : Crossplane is an open source multicloud control plane. It introduces workload and resource abstractions on-top of existing managed services that enables a high degree of workload portability across cloud providers. A single crossplane enables the provisioning and full-lifecycle management of services and infrastructure across a wide range of providers, offerings, vendors, regions, and clusters. Crossplane offers a universal API for cloud computing, a workload scheduler, and a set of smart controllers that can automate work across clouds. razee : A multi-cluster continuous delivery tool for Kubernetes Automate the rollout process of Kubernetes resources across multiple clusters, environments, and cloud providers, and gain insight into what applications and versions run in your cluster. kube-ops-view : it shows how are the ops on the nodes. kubediff : a tool for Kubernetes to show differences between running state and version controlled configuration. ksniff : A kubectl plugin that utilize tcpdump and Wireshark to start a remote capture on any pod in your Kubernetes cluster. kubeview : Visualize dependencies kubernetes.","title":"Tools"},{"location":"devops/kubernetes/kubernetes_tools/#tried","text":"K3s : Recommended small kubernetes, like hyperkube.","title":"Tried"},{"location":"devops/kubernetes/kubernetes_tools/#to-try","text":"crossplane : Crossplane is an open source multicloud control plane. It introduces workload and resource abstractions on-top of existing managed services that enables a high degree of workload portability across cloud providers. A single crossplane enables the provisioning and full-lifecycle management of services and infrastructure across a wide range of providers, offerings, vendors, regions, and clusters. Crossplane offers a universal API for cloud computing, a workload scheduler, and a set of smart controllers that can automate work across clouds. razee : A multi-cluster continuous delivery tool for Kubernetes Automate the rollout process of Kubernetes resources across multiple clusters, environments, and cloud providers, and gain insight into what applications and versions run in your cluster. kube-ops-view : it shows how are the ops on the nodes. kubediff : a tool for Kubernetes to show differences between running state and version controlled configuration. ksniff : A kubectl plugin that utilize tcpdump and Wireshark to start a remote capture on any pod in your Kubernetes cluster. kubeview : Visualize dependencies kubernetes.","title":"To try"},{"location":"devops/kubernetes/kubernetes_vertical_pod_autoscaler/","text":"Kubernetes knows the amount of resources a pod needs to operate through some metadata specified in the deployment. Generally this values change and manually maintaining all the resources requested and limits is a nightmare. The Vertical pod autoscaler does data analysis on the pod metrics to automatically adjust these values. Nevertheless it's still not suggested to use it in conjunction with the horizontal pod autoscaler , so we'll need to watch out for future improvements.","title":"Vertical Pod Autoscaler"},{"location":"devops/kubernetes/kubernetes_volumes/","text":"On disk files in a Container are ephemeral by default, which presents the following issues: When a Container crashes, kubelet will restart it, but the files will be lost. When running Containers together in a Pod it is often necessary to share files between those Containers. The Kubernetes Volume abstraction solves both of these problems with several types . configMap \u2691 The configMap resource provides a way to inject configuration data into Pods. The data stored in a ConfigMap object can be referenced in a volume of type configMap and then consumed by containerized applications running in a Pod. emptyDir \u2691 An emptyDir volume is first created when a Pod is assigned to a Node, and exists as long as that Pod is running on that node. As the name says, it is initially empty. Containers in the Pod can all read and write the same files in the emptyDir volume. When a Pod is removed from a node for any reason, the data in the emptyDir is deleted forever. hostPath \u2691 A hostPath volume mounts a file or directory from the host node\u2019s filesystem into your Pod. This is not something that most Pods will need, but it offers a powerful escape hatch for some applications. For example, some uses for a hostPath are: Running a Container that needs access to Docker internals; use a hostPath of /var/lib/docker . Running cAdvisor in a Container; use a hostPath of /sys . secret \u2691 A secret volume is used to pass sensitive information, such as passwords, to Pods. You can store secrets in the Kubernetes API and mount them as files for use by Pods without coupling to Kubernetes directly. secret volumes are backed by tmpfs (a RAM-backed filesystem) so they are never written to non-volatile storage. awsElasticBlockStore \u2691 An awsElasticBlockStore volume mounts an Amazon Web Services (AWS) EBS Volume into your Pod. Unlike emptyDir , which is erased when a Pod is removed, the contents of an EBS volume are preserved and the volume is merely unmounted. This means that an EBS volume can be pre-populated with data, and that data can be \u201chanded off\u201d between Pods. There are some restrictions when using an awsElasticBlockStore volume: The nodes on which Pods are running must be AWS EC2 instances. Those instances need to be in the same region and availability-zone as the EBS volume. EBS only supports a single EC2 instance mounting a volume. nfs \u2691 An nfs volume allows an existing NFS (Network File System) share to be mounted into your Pod. Unlike emptyDir, which is erased when a Pod is removed, the contents of an nfs volume are preserved and the volume is merely unmounted. This means that an NFS volume can be pre-populated with data, and that data can be \u201chanded off\u201d between Pods. NFS can be mounted by multiple writers simultaneously. local \u2691 A local volume represents a mounted local storage device such as a disk, partition or directory. Local volumes can only be used as a statically created PersistentVolume. Dynamic provisioning is not supported yet. Compared to hostPath volumes, local volumes can be used in a durable and portable manner without manually scheduling Pods to nodes, as the system is aware of the volume\u2019s node constraints by looking at the node affinity on the PersistentVolume. However, local volumes are still subject to the availability of the underlying node and are not suitable for all applications. If a node becomes unhealthy, then the local volume will also become inaccessible, and a Pod using it will not be able to run. Applications using local volumes must be able to tolerate this reduced availability, as well as potential data loss, depending on the durability characteristics of the underlying disk. Others \u2691 glusterfs cephfs","title":"Volumes"},{"location":"devops/kubernetes/kubernetes_volumes/#configmap","text":"The configMap resource provides a way to inject configuration data into Pods. The data stored in a ConfigMap object can be referenced in a volume of type configMap and then consumed by containerized applications running in a Pod.","title":"configMap"},{"location":"devops/kubernetes/kubernetes_volumes/#emptydir","text":"An emptyDir volume is first created when a Pod is assigned to a Node, and exists as long as that Pod is running on that node. As the name says, it is initially empty. Containers in the Pod can all read and write the same files in the emptyDir volume. When a Pod is removed from a node for any reason, the data in the emptyDir is deleted forever.","title":"emptyDir"},{"location":"devops/kubernetes/kubernetes_volumes/#hostpath","text":"A hostPath volume mounts a file or directory from the host node\u2019s filesystem into your Pod. This is not something that most Pods will need, but it offers a powerful escape hatch for some applications. For example, some uses for a hostPath are: Running a Container that needs access to Docker internals; use a hostPath of /var/lib/docker . Running cAdvisor in a Container; use a hostPath of /sys .","title":"hostPath"},{"location":"devops/kubernetes/kubernetes_volumes/#secret","text":"A secret volume is used to pass sensitive information, such as passwords, to Pods. You can store secrets in the Kubernetes API and mount them as files for use by Pods without coupling to Kubernetes directly. secret volumes are backed by tmpfs (a RAM-backed filesystem) so they are never written to non-volatile storage.","title":"secret"},{"location":"devops/kubernetes/kubernetes_volumes/#awselasticblockstore","text":"An awsElasticBlockStore volume mounts an Amazon Web Services (AWS) EBS Volume into your Pod. Unlike emptyDir , which is erased when a Pod is removed, the contents of an EBS volume are preserved and the volume is merely unmounted. This means that an EBS volume can be pre-populated with data, and that data can be \u201chanded off\u201d between Pods. There are some restrictions when using an awsElasticBlockStore volume: The nodes on which Pods are running must be AWS EC2 instances. Those instances need to be in the same region and availability-zone as the EBS volume. EBS only supports a single EC2 instance mounting a volume.","title":"awsElasticBlockStore"},{"location":"devops/kubernetes/kubernetes_volumes/#nfs","text":"An nfs volume allows an existing NFS (Network File System) share to be mounted into your Pod. Unlike emptyDir, which is erased when a Pod is removed, the contents of an nfs volume are preserved and the volume is merely unmounted. This means that an NFS volume can be pre-populated with data, and that data can be \u201chanded off\u201d between Pods. NFS can be mounted by multiple writers simultaneously.","title":"nfs"},{"location":"devops/kubernetes/kubernetes_volumes/#local","text":"A local volume represents a mounted local storage device such as a disk, partition or directory. Local volumes can only be used as a statically created PersistentVolume. Dynamic provisioning is not supported yet. Compared to hostPath volumes, local volumes can be used in a durable and portable manner without manually scheduling Pods to nodes, as the system is aware of the volume\u2019s node constraints by looking at the node affinity on the PersistentVolume. However, local volumes are still subject to the availability of the underlying node and are not suitable for all applications. If a node becomes unhealthy, then the local volume will also become inaccessible, and a Pod using it will not be able to run. Applications using local volumes must be able to tolerate this reduced availability, as well as potential data loss, depending on the durability characteristics of the underlying disk.","title":"local"},{"location":"devops/kubernetes/kubernetes_volumes/#others","text":"glusterfs cephfs","title":"Others"},{"location":"devops/prometheus/alertmanager/","text":"The Alertmanager handles alerts sent by client applications such as the Prometheus server. It takes care of deduplicating, grouping, and routing them to the correct receiver integrations such as email, PagerDuty, or OpsGenie. It also takes care of silencing and inhibition of alerts. It is configured through the alertmanager.config key of the values.yaml of the helm chart. As stated in the configuration file , it has four main keys (as templates is handled in alertmanager.config.templateFiles ): global : SMTP and API main configuration, it will be inherited by the other elements. route : Route tree definition. receivers : Notification integrations configuration. inhibit_rules : Alert inhibition configuration. Route \u2691 A route block defines a node in a routing tree and its children. Its optional configuration parameters are inherited from its parent node if not set. Every alert enters the routing tree at the configured top-level route, which must match all alerts (i.e. not have any configured matchers). It then traverses the child nodes. If continue is set to false, it stops after the first matching child. If continue is true on a matching node, the alert will continue matching against subsequent siblings. If an alert does not match any children of a node (no matching child nodes, or none exist), the alert is handled based on the configuration parameters of the current node. Receivers \u2691 Notification receivers are the named configurations of one or more notification integrations. Email notifications \u2691 To configure email notifications, set up the following in your config : config : global : smtp_from : {{ from_email_address }} smtp_smarthost : {{ smtp_server_endpoint }} :{{ smtp_server_port }} smtp_auth_username : {{ smpt_authentication_username }} smtp_auth_password : {{ smpt_authentication_password }} receivers : - name : 'email' email_configs : - to : {{ receiver_email }} send_resolved : true If you need to set smtp_auth_username and smtp_auth_password you should value using helm secrets . send_resolved , set to False by default, defines whether or not to notify about resolved alerts. Rocketchat Notifications \u2691 Go to pavel-kazhavets AlertmanagerRocketChat repo for the updated rules. In RocketChat: Login as admin user and go to: Administration => Integrations => New Integration => Incoming WebHook. Set \"Enabled\" and \"Script Enabled\" to \"True\". Set all channel, icons, etc. as you need. Paste contents of the official AlertmanagerIntegrations.js or my version into Script field. AlertmanagerIntegrations.js class Script { process_incoming_request ({ request }) { console . log ( request . content ); var alertColor = \"warning\" ; if ( request . content . status == \"resolved\" ) { alertColor = \"good\" ; } else if ( request . content . status == \"firing\" ) { alertColor = \"danger\" ; } let finFields = []; for ( i = 0 ; i < request . content . alerts . length ; i ++ ) { var endVal = request . content . alerts [ i ]; var elem = { title : \"alertname: \" + endVal . labels . alertname , value : \"*instance:* \" + endVal . labels . instance , short : false }; finFields . push ( elem ); if ( !! endVal . annotations . summary ) { finFields . push ({ title : \"summary\" , value : endVal . annotations . summary }); } if ( !! endVal . annotations . severity ) { finFields . push ({ title : \"severity\" , value : endVal . labels . severity }); } if ( !! endVal . annotations . grafana ) { finFields . push ({ title : \"grafana\" , value : endVal . annotations . grafana }); } if ( !! endVal . annotations . prometheus ) { finFields . push ({ title : \"prometheus\" , value : endVal . annotations . prometheus }); } if ( !! endVal . annotations . message ) { finFields . push ({ title : \"message\" , value : endVal . annotations . message }); } if ( !! endVal . annotations . description ) { finFields . push ({ title : \"description\" , value : endVal . annotations . description }); } } return { content : { username : \"Prometheus Alert\" , attachments : [{ color : alertColor , title_link : request . content . externalURL , title : \"Prometheus notification\" , fields : finFields }] } }; return { error : { success : false } }; } } Create Integration. The field Webhook URL will appear in the Integration configuration. In Alertmanager: Create new receiver or modify config of existing one. You'll need to add webhooks_config to it. Small example: route : repeat_interval : 30m group_interval : 30m receiver : 'rocketchat' receivers : - name : 'rocketchat' webhook_configs : - send_resolved : false url : '${WEBHOOK_URL}' Reload/restart alertmanager. In order to test the webhook you can use the following curl (replace {{ webhook-url }} ): curl -X POST -H 'Content-Type: application/json' --data ' { \"text\": \"Example message\", \"attachments\": [ { \"title\": \"Rocket.Chat\", \"title_link\": \"https://rocket.chat\", \"text\": \"Rocket.Chat, the best open source chat\", \"image_url\": \"https://rocket.cha t/images/mockup.png\", \"color\": \"#764FA5\" } ], \"status\": \"firing\", \"alerts\": [ { \"labels\": { \"alertname\": \"high_load\", \"severity\": \"major\", \"instance\": \"node-exporter:9100\" }, \"annotations\": { \"message\": \"node-exporter:9100 of job xxxx is under high load.\", \"summary\": \"node-exporter:9100 under high load.\" } } ] } ' {{ webhook-url }} Inhibit rules \u2691 Inhibit rules define which alerts triggered by Prometheus shouldn't be forwarded to the notification integrations. For example the Watchdog alert is meant to test that everything works as expected, but is not meant to be used by the users. Similarly, if you are using EKS, you'll probably have an KubeVersionMismatch , because Kubernetes allows a certain version skew between their components. So the alert is more strict than the Kubernetes policy. To disable both alerts, set a match rule in config.inhibit_rules : config : inhibit_rules : - target_match : alertname : Watchdog - target_match : alertname : KubeVersionMismatch Alert rules \u2691 Alert rules are a special kind of Prometheus Rules that trigger alerts based on PromQL expressions. People have gathered several examples under Awesome prometheus alert rules Alerts must be configured in the Prometheus operator helm chart, under the additionalPrometheusRulesMap . For example: additionalPrometheusRulesMap : - groups : - name : alert-rules rules : - alert : BlackboxProbeFailed expr : probe_success == 0 for : 5m labels : severity : error annotations : summary : \"Blackbox probe failed (instance {{ $labels.target }})\" description : \"Probe failed\\n VALUE = {{ $value }}\\n LABELS: {{ $labels }}\" Other examples of rules are: Blackbox Exporter rules Links \u2691 Awesome prometheus alert rules","title":"AlertManager"},{"location":"devops/prometheus/alertmanager/#route","text":"A route block defines a node in a routing tree and its children. Its optional configuration parameters are inherited from its parent node if not set. Every alert enters the routing tree at the configured top-level route, which must match all alerts (i.e. not have any configured matchers). It then traverses the child nodes. If continue is set to false, it stops after the first matching child. If continue is true on a matching node, the alert will continue matching against subsequent siblings. If an alert does not match any children of a node (no matching child nodes, or none exist), the alert is handled based on the configuration parameters of the current node.","title":"Route"},{"location":"devops/prometheus/alertmanager/#receivers","text":"Notification receivers are the named configurations of one or more notification integrations.","title":"Receivers"},{"location":"devops/prometheus/alertmanager/#email-notifications","text":"To configure email notifications, set up the following in your config : config : global : smtp_from : {{ from_email_address }} smtp_smarthost : {{ smtp_server_endpoint }} :{{ smtp_server_port }} smtp_auth_username : {{ smpt_authentication_username }} smtp_auth_password : {{ smpt_authentication_password }} receivers : - name : 'email' email_configs : - to : {{ receiver_email }} send_resolved : true If you need to set smtp_auth_username and smtp_auth_password you should value using helm secrets . send_resolved , set to False by default, defines whether or not to notify about resolved alerts.","title":"Email notifications"},{"location":"devops/prometheus/alertmanager/#rocketchat-notifications","text":"Go to pavel-kazhavets AlertmanagerRocketChat repo for the updated rules. In RocketChat: Login as admin user and go to: Administration => Integrations => New Integration => Incoming WebHook. Set \"Enabled\" and \"Script Enabled\" to \"True\". Set all channel, icons, etc. as you need. Paste contents of the official AlertmanagerIntegrations.js or my version into Script field. AlertmanagerIntegrations.js class Script { process_incoming_request ({ request }) { console . log ( request . content ); var alertColor = \"warning\" ; if ( request . content . status == \"resolved\" ) { alertColor = \"good\" ; } else if ( request . content . status == \"firing\" ) { alertColor = \"danger\" ; } let finFields = []; for ( i = 0 ; i < request . content . alerts . length ; i ++ ) { var endVal = request . content . alerts [ i ]; var elem = { title : \"alertname: \" + endVal . labels . alertname , value : \"*instance:* \" + endVal . labels . instance , short : false }; finFields . push ( elem ); if ( !! endVal . annotations . summary ) { finFields . push ({ title : \"summary\" , value : endVal . annotations . summary }); } if ( !! endVal . annotations . severity ) { finFields . push ({ title : \"severity\" , value : endVal . labels . severity }); } if ( !! endVal . annotations . grafana ) { finFields . push ({ title : \"grafana\" , value : endVal . annotations . grafana }); } if ( !! endVal . annotations . prometheus ) { finFields . push ({ title : \"prometheus\" , value : endVal . annotations . prometheus }); } if ( !! endVal . annotations . message ) { finFields . push ({ title : \"message\" , value : endVal . annotations . message }); } if ( !! endVal . annotations . description ) { finFields . push ({ title : \"description\" , value : endVal . annotations . description }); } } return { content : { username : \"Prometheus Alert\" , attachments : [{ color : alertColor , title_link : request . content . externalURL , title : \"Prometheus notification\" , fields : finFields }] } }; return { error : { success : false } }; } } Create Integration. The field Webhook URL will appear in the Integration configuration. In Alertmanager: Create new receiver or modify config of existing one. You'll need to add webhooks_config to it. Small example: route : repeat_interval : 30m group_interval : 30m receiver : 'rocketchat' receivers : - name : 'rocketchat' webhook_configs : - send_resolved : false url : '${WEBHOOK_URL}' Reload/restart alertmanager. In order to test the webhook you can use the following curl (replace {{ webhook-url }} ): curl -X POST -H 'Content-Type: application/json' --data ' { \"text\": \"Example message\", \"attachments\": [ { \"title\": \"Rocket.Chat\", \"title_link\": \"https://rocket.chat\", \"text\": \"Rocket.Chat, the best open source chat\", \"image_url\": \"https://rocket.cha t/images/mockup.png\", \"color\": \"#764FA5\" } ], \"status\": \"firing\", \"alerts\": [ { \"labels\": { \"alertname\": \"high_load\", \"severity\": \"major\", \"instance\": \"node-exporter:9100\" }, \"annotations\": { \"message\": \"node-exporter:9100 of job xxxx is under high load.\", \"summary\": \"node-exporter:9100 under high load.\" } } ] } ' {{ webhook-url }}","title":"Rocketchat Notifications"},{"location":"devops/prometheus/alertmanager/#inhibit-rules","text":"Inhibit rules define which alerts triggered by Prometheus shouldn't be forwarded to the notification integrations. For example the Watchdog alert is meant to test that everything works as expected, but is not meant to be used by the users. Similarly, if you are using EKS, you'll probably have an KubeVersionMismatch , because Kubernetes allows a certain version skew between their components. So the alert is more strict than the Kubernetes policy. To disable both alerts, set a match rule in config.inhibit_rules : config : inhibit_rules : - target_match : alertname : Watchdog - target_match : alertname : KubeVersionMismatch","title":"Inhibit rules"},{"location":"devops/prometheus/alertmanager/#alert-rules","text":"Alert rules are a special kind of Prometheus Rules that trigger alerts based on PromQL expressions. People have gathered several examples under Awesome prometheus alert rules Alerts must be configured in the Prometheus operator helm chart, under the additionalPrometheusRulesMap . For example: additionalPrometheusRulesMap : - groups : - name : alert-rules rules : - alert : BlackboxProbeFailed expr : probe_success == 0 for : 5m labels : severity : error annotations : summary : \"Blackbox probe failed (instance {{ $labels.target }})\" description : \"Probe failed\\n VALUE = {{ $value }}\\n LABELS: {{ $labels }}\" Other examples of rules are: Blackbox Exporter rules","title":"Alert rules"},{"location":"devops/prometheus/alertmanager/#links","text":"Awesome prometheus alert rules","title":"Links"},{"location":"devops/prometheus/blackbox_exporter/","text":"The blackbox exporter allows blackbox probing of endpoints over HTTP, HTTPS, DNS, TCP and ICMP. It can be used to test: Website accessibility . Both for availability and security purposes. Website loading time . DNS response times to diagnose network latency issues. SSL certificates expiration . ICMP requests to gather network health information . Security protections such as if and endpoint stops being protected by VPN, WAF or SSL client certificate. Unauthorized read or write S3 buckets . When running, the Blackbox exporter is going to expose a HTTP endpoint that can be used in order to monitor targets over the network. By default, the Blackbox exporter exposes the /probe endpoint that is used to retrieve those metrics. The blackbox exporter is configured with a YAML configuration file made of modules . Installation \u2691 To install the exporter we'll use helmfile to install the stable/prometheus-blackbox-exporter chart . Add the following lines to your helmfile.yaml . - name : prometheus-blackbox-exporter namespace : monitoring chart : stable/prometheus-blackbox-exporter values : - prometheus-blackbox-exporter/values.yaml Edit the chart values. mkdir prometheus-blackbox-exporter helm inspect values stable/prometheus-blackbox-exporter > prometheus-blackbox-exporter/values.yaml vi prometheus-blackbox-exporter/values.yaml Make sure to enable the serviceMonitor in the values and target at least one page: serviceMonitor : enabled : true # Default values that will be used for all ServiceMonitors created by `targets` defaults : labels : release : prometheus-operator interval : 30s scrapeTimeout : 30s module : http_2xx targets : - name : lyz-code.github.io/blue-book url : https://lyz-code.github.io/blue-book The label release: prometheus-operator must be the one your prometheus instance is searching for . If you want to use the icmp probe, make sure to allow allowIcmp: true . If you want to probe endpoints protected behind client SSL certificates, until this chart issue is solved, you need to create them manually as the Prometheus blackbox exporter helm chart doesn't yet create the required secrets. kubectl create secret generic monitor-certificates \\ --from-file = monitor.crt.pem \\ --from-file = monitor.key.pem \\ -n monitoring Where monitor.crt.pem and monitor.key.pem are the SSL certificate and key for the monitor account. I've found two grafana dashboards for the blackbox exporter. 7587 didn't work straight out of the box while 5345 did. Taking as reference the grafana helm chart values, add the following yaml under the grafana key in the prometheus-operator values.yaml . grafana : enabled : true defaultDashboardsEnabled : true dashboardProviders : dashboardproviders.yaml : apiVersion : 1 providers : - name : 'default' orgId : 1 folder : '' type : file disableDeletion : false editable : true options : path : /var/lib/grafana/dashboards/default dashboards : default : blackbox-exporter : # Ref: https://grafana.com/dashboards/5345 gnetId : 5345 revision : 3 datasource : Prometheus And install. helmfile diff helmfile apply Blackbox exporter probes \u2691 Modules define how blackbox exporter is going to query the endpoint, therefore one needs to be created for each request type under the config.modules section of the chart. The modules are then used in the targets section for the desired endpoints. targets : - name : lyz-code.github.io/blue-book url : https://lyz-code.github.io/blue-book module : https_2xx HTTP endpoint working correctly \u2691 http_2xx : prober : http timeout : 5s http : valid_http_versions : [ \"HTTP/1.1\" , \"HTTP/2\" ] valid_status_codes : [ 200 ] no_follow_redirects : false preferred_ip_protocol : \"ip4\" HTTPS endpoint working correctly \u2691 https_2xx : prober : http timeout : 5s http : method : GET fail_if_ssl : false fail_if_not_ssl : true valid_http_versions : [ \"HTTP/1.1\" , \"HTTP/2\" ] valid_status_codes : [ 200 ] no_follow_redirects : false preferred_ip_protocol : \"ip4\" HTTPS endpoint behind client SSL certificate \u2691 https_client_2xx : prober : http timeout : 5s http : method : GET fail_if_ssl : false fail_if_not_ssl : true valid_http_versions : [ \"HTTP/1.1\" , \"HTTP/2\" ] valid_status_codes : [ 200 ] no_follow_redirects : false preferred_ip_protocol : \"ip4\" tls_config : cert_file : /etc/secrets/monitor.crt.pem key_file : /etc/secrets/monitor.key.pem Where the secrets have been created throughout the installation. HTTPS endpoint with an specific error \u2691 If you don't want to configure the authentication for example for an API, you can fetch the expected error. https_client_api : prober : http timeout : 5s http : method : GET fail_if_ssl : false fail_if_not_ssl : true valid_http_versions : [ \"HTTP/1.1\" , \"HTTP/2\" ] valid_status_codes : [ 404 ] no_follow_redirects : false preferred_ip_protocol : \"ip4\" fail_if_body_not_matches_regexp : - '.*ERROR route not.*' HTTP endpoint returning an error \u2691 http_4xx : prober : http timeout : 5s http : method : HEAD valid_status_codes : [ 404 , 403 ] valid_http_versions : [ \"HTTP/1.1\" , \"HTTP/2\" ] no_follow_redirects : false HTTPS endpoint through an HTTP proxy \u2691 https_external_2xx : prober : http timeout : 5s http : method : GET fail_if_ssl : false fail_if_not_ssl : true valid_http_versions : [ \"HTTP/1.0\" , \"HTTP/1.1\" , \"HTTP/2\" ] valid_status_codes : [ 200 ] no_follow_redirects : false proxy_url : \"http://{{ proxy_url }}:{{ proxy_port }}\" preferred_ip_protocol : \"ip4\" HTTPS endpoint with basic auth \u2691 https_basic_auth_2xx : prober : http timeout : 5s http : method : GET fail_if_ssl : false fail_if_not_ssl : true valid_http_versions : - HTTP/1.1 - HTTP/2 valid_status_codes : - 200 no_follow_redirects : false preferred_ip_protocol : ip4 basic_auth : username : {{ username }} password : {{ password }} HTTPs endpoint with API key \u2691 https_api_2xx : prober : http timeout : 5s http : method : GET fail_if_ssl : false fail_if_not_ssl : true valid_http_versions : - HTTP/1.1 - HTTP/2 valid_status_codes : - 200 no_follow_redirects : false preferred_ip_protocol : ip4 headers : apikey : {{ api_key }} HTTPS Put file \u2691 Test if the probe can upload a file. https_put_file_2xx : prober : http timeout : 5s http : method : PUT body : hi fail_if_ssl : false fail_if_not_ssl : true valid_http_versions : [ \"HTTP/1.1\" , \"HTTP/2\" ] valid_status_codes : [ 200 ] no_follow_redirects : false preferred_ip_protocol : \"ip4\" Check open port \u2691 tcp_connect : prober : tcp The port is specified when using the module. - name : lyz-code.github.io url : lyz-code.github.io:389 module : tcp_connect Ping to the resource \u2691 Test if the target is alive. It's useful When you don't know what port to check or if it uses UDP. ping : prober : icmp timeout : 5s icmp : preferred_ip_protocol : \"ip4\" Blackbox exporter alerts \u2691 Now that we've got the metrics, we can define the alert rules . Most have been tweaked from the Awesome prometheus alert rules collection. To make security tests Availability alerts \u2691 The most basic probes, test if the service is up and returning. Blackbox probe failed \u2691 Blackbox probe failed. - alert : BlackboxProbeFailed expr : probe_success == 0 for : 5m labels : severity : error annotations : summary : \"Blackbox probe failed (instance {{ $labels.target }})\" message : \"Probe failed\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url }}&var-targets={{ $labels.target }}\" prometheus : \"{{ prometheus_url }}/graph?g0.expr=probe_success+%3D%3D+0&g0.tab=1\" If you use the security alerts , use the following expr: instead expr : probe_success{target!~\".*-fail-.*$\"} == 0 Blackbox probe HTTP failure \u2691 HTTP status code is not 200-399. - alert : BlackboxProbeHttpFailure expr : probe_http_status_code <= 199 OR probe_http_status_code >= 400 for : 5m labels : severity : error annotations : summary : \"Blackbox probe HTTP failure (instance {{ $labels.target }})\" message : \"HTTP status code is not 200-399\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url }}&var-targets={{ $labels.target }}\" prometheus : \"{{ prometheus_url }}/graph?g0.expr=probe_ssl_earliest_cert_expiry+-+time%28%29+%3C+86400+%2A+30&g0.tab=1\" Performance alerts \u2691 Blackbox slow probe \u2691 Blackbox probe took more than 1s to complete. - alert : BlackboxSlowProbe expr : avg_over_time(probe_duration_seconds[1m]) > 1 for : 5m labels : severity : warning annotations : summary : \"Blackbox slow probe (target {{ $labels.target }})\" message : \"Blackbox probe took more than 1s to complete\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url }}&var-targets={{ $labels.target }}\" prometheus : \"{{ prometheus_url }}/graph?g0.expr=avg_over_time%28probe_duration_seconds%5B1m%5D%29+%3E+1&g0.tab=1\" If you use the security alerts , use the following expr: instead expr : avg_over_time(probe_duration_seconds{,target!~\".*-fail-.*\"}[1m]) > 1 Blackbox probe slow HTTP \u2691 HTTP request took more than 1s. - alert : BlackboxProbeSlowHttp expr : avg_over_time(probe_http_duration_seconds[1m]) > 1 for : 5m labels : severity : warning annotations : summary : \"Blackbox probe slow HTTP (instance {{ $labels.target }})\" message : \"HTTP request took more than 1s\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url }}&var-targets={{ $labels.target }}\" prometheus : \"{{ prometheus_url }}/graph?g0.expr=avg_over_time%28probe_http_duration_seconds%5B1m%5D%29+%3E+1&g0.tab=1\" If you use the security alerts , use the following expr: instead expr : avg_over_time(probe_http_duration_seconds{,target!~\".*-fail-.*\"}[1m]) > 1 Blackbox probe slow ping \u2691 Blackbox ping took more than 1s. - alert : BlackboxProbeSlowPing expr : avg_over_time(probe_icmp_duration_seconds[1m]) > 1 for : 5m labels : severity : warning annotations : summary : \"Blackbox probe slow ping (instance {{ $labels.target }})\" message : \"Blackbox ping took more than 1s\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url }}&var-targets={{ $labels.target }}\" prometheus : \"{{ prometheus_url }}/graph?g0.expr=avg_over_time%28probe_icmp_duration_seconds%5B1m%5D%29+%3E+1&g0.tab=1\" SSL certificate alerts \u2691 Blackbox SSL certificate will expire in a month \u2691 SSL certificate expires in 30 days. - alert : BlackboxSslCertificateWillExpireSoon expr : probe_ssl_earliest_cert_expiry - time() < 86400 * 30 for : 5m labels : severity : warning annotations : summary : \"Blackbox SSL certificate will expire soon (instance {{ $labels.target }})\" message : \"SSL certificate expires in 30 days\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url }}&var-targets={{ $labels.target }}\" prometheus : \"{{ prometheus_url }}/graph?g0.expr=probe_ssl_earliest_cert_expiry+-+time%28%29+%3C+86400+%2A+30&g0.tab=1\" Blackbox SSL certificate will expire in a few days \u2691 SSL certificate expires in 3 days. - alert : BlackboxSslCertificateWillExpireSoon expr : probe_ssl_earliest_cert_expiry - time() < 86400 * 3 for : 5m labels : severity : error annotations : summary : \"Blackbox SSL certificate will expire soon (instance {{ $labels.target }})\" message : \"SSL certificate expires in 3 days\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url }}&var-targets={{ $labels.target }}\" prometheus : \"{{ prometheus_url }}/graph?g0.expr=probe_ssl_earliest_cert_expiry+-+time%28%29+%3C+86400+%2A+3&g0.tab=1\" Blackbox SSL certificate expired \u2691 SSL certificate has expired already. - alert : BlackboxSslCertificateExpired expr : probe_ssl_earliest_cert_expiry - time() <= 0 for : 5m labels : severity : error annotations : summary : \"Blackbox SSL certificate expired (instance {{ $labels.target }})\" message : \"SSL certificate has expired already\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url }}&var-targets={{ $labels.target }}\" prometheus : \"{{ prometheus_url }}/graph?g0.expr=probe_ssl_earliest_cert_expiry+-+time%28%29+%3C%3D+0&g0.tab=1\" Security alerts \u2691 To define the security alerts, I've found easier to create a probe with the action I want to prevent and make sure that the probe fails. This probes contain the -fail- key in the target name, followed by the test it's performing. This convention allows the concatenation of tests. For example, when testing if and endpoint is accessible without basic auth and without vpn we'd use: - name : protected.endpoint.org-fail-without-ssl-and-without-credentials url : protected.endpoint.org module : https_external_2xx Test endpoints protected with network policies \u2691 Assuming that the blackbox exporter is in the internal network and that there is an http proxy on the external network we want to test. Create a working probe with the https_external_2xx module containing the -fail-without-vpn key in the target name. - alert : BlackboxVPNProtectionRemoved expr : probe_success{target=~\".*-fail-.*without-vpn.*\"} == 1 for : 5m labels : severity : error annotations : summary : \"VPN protection was removed from (instance {{ $labels.target }})\" message : \"Successful probe to the endpoint from outside the internal network\" grafana : \"{{ grafana_url }}&var-targets={{ $labels.target }}\" prometheus : \"{{ prometheus_url }}/graph?g0.expr=probe_ssl_earliest_cert_expiry+-+time%28%29+%3C%3D+0&g0.tab=1\" Test endpoints protected with SSL client certificate \u2691 Create a working probe with a module without the SSL client certificate configured, such as https_2xx and set the -fail-without-ssl key in the target name. - alert : BlackboxClientSSLProtectionRemoved expr : probe_success{target=~\".*-fail-.*without-ssl.*\"} == 1 for : 5m labels : severity : error annotations : summary : \"SSL client certificate protection was removed from (instance {{ $labels.target }})\" message : \"Successful probe to the endpoint without SSL certificate\" grafana : \"{{ grafana_url }}&var-targets={{ $labels.target }}\" prometheus : \"{{ prometheus_url }}/graph?g0.expr=probe_ssl_earliest_cert_expiry+-+time%28%29+%3C%3D+0&g0.tab=1\" Test endpoints protected with credentials. \u2691 Create a working probe with a module without the basic auth credentials configured, such as https_2xx and set the -fail-without-credentials key in the target name. - alert : BlackboxCredentialsProtectionRemoved expr : probe_success{target=~\".*-fail-.*without-credentials.*\"} == 1 for : 5m labels : severity : error annotations : summary : \"Credentials protection was removed from (instance {{ $labels.target }})\" message : \"Successful probe to the endpoint without credentials\" grafana : \"{{ grafana_url }}&var-targets={{ $labels.target }}\" prometheus : \"{{ prometheus_url }}/graph?g0.expr=probe_ssl_earliest_cert_expiry+-+time%28%29+%3C%3D+0&g0.tab=1\" Test endpoints protected with WAF. \u2691 Create a working probe with a module bypassing the WAF, for example directly attacking the service and set the -fail-without-waf key in the target name. - alert : BlackboxWAFProtectionRemoved expr : probe_success{target=~\".*-fail-.*without-waf.*\"} == 1 for : 5m labels : severity : error annotations : summary : \"WAF protection was removed from (instance {{ $labels.target }})\" message : \"Successful probe to the haproxy endpoint from the internal network (bypassed the WAF)\" grafana : \"{{ grafana_url }}&var-targets={{ $labels.target }}\" prometheus : \"{{ prometheus_url }}/graph?g0.expr=probe_ssl_earliest_cert_expiry+-+time%28%29+%3C%3D+0&g0.tab=1\" Unauthorized read of S3 buckets \u2691 Create a working probe to an existent private object in an S3 bucket and set the -fail-read-object key in the target name. - alert : BlackboxS3BucketWrongReadPermissions expr : probe_success{target=~\".*-fail-.*read-object.*\"} == 1 for : 5m labels : severity : error annotations : summary : \"Wrong read permissions on S3 bucket (instance {{ $labels.target }})\" message : \"Successful read of a private object with an unauthenticated user\" grafana : \"{{ grafana_url }}&var-targets={{ $labels.target }}\" prometheus : \"{{ prometheus_url }}/graph?g0.expr=probe_ssl_earliest_cert_expiry+-+time%28%29+%3C%3D+0&g0.tab=1\" Unauthorized write of S3 buckets \u2691 Create a working probe using the https_put_file_2xx module to try to create a file in an S3 bucket and set the -fail-write-object key in the target name. - alert : BlackboxS3BucketWrongWritePermissions expr : probe_success{target=~\".*-fail-.*write-object.*\"} == 1 for : 5m labels : severity : error annotations : summary : \"Wrong write permissions on S3 bucket (instance {{ $labels.target }})\" message : \"Successful write of a private object with an unauthenticated user\" grafana : \"{{ grafana_url }}&var-targets={{ $labels.target }}\" prometheus : \"{{ prometheus_url }}/graph?g0.expr=probe_ssl_earliest_cert_expiry+-+time%28%29+%3C%3D+0&g0.tab=1\" Monitoring external access to internal services \u2691 There are two possible solutions to simulate traffic from outside your infrastructure to the internal services. Both require the installation of an agent outside of your internal infrastructure, it can be: An HTTP proxy. A blackbox exporter instance. Using the proxy you have following advantages: It's really easy to set up a transparent http proxy . All probe configuration goes in the same blackbox exporter instance values.yaml . With the following disadvantages: When using an external http proxy, the probe runs the DNS resolution locally . Therefore if the record doesn't exist in the local server the probe will fail, even if the proxy DNS resolver has the correct record. The ugly workaround I've implemented is to create a \"fake\" DNS record in my internal DNS server so the probe sees it exist. * There is no way to do tcp or ping probes to simulate external traffic. * The latency between the blackbox exporter and the proxy is added to all the external probes. While using an external blackbox exporter gives the following advantages: Traffic is completely external to the infrastructure, so the proxy disadvantages would be solved. And the following disadvantages: Simulation of external traffic in AWS could be done by spawning the blackbox exporter instance in another region, but as there is no way of using EKS worker nodes in different regions, there is no way of managing the exporter from within Kubernetes. This means: The loose of the advantages of the Prometheus operator , so we have to write the configuration manually. Configuration can't be managed with Helm , so two solutions should be used to manage the monitorization (Ansible could be used). Even if it's possible to host the second external blackbox exporter within Kubernetes, two independent Helm charts are needed, with the consequent configuration management burden. In conclusion, when using a Kubernetes cluster that allows the creation of worker nodes outside the main infrastructure, or if several non HTTP/HTTPS endpoints need to be probed with the tcp or ping modules, install an external blackbox exporter instance. Otherwise install an HTTP proxy and assume that you can only simulate external HTTP/HTTPS traffic. Troubleshooting \u2691 To get more debugging information of the blackbox probes, add &debug=true to the probe url, for example http://localhost:9115/probe?module=http_2xx&target=https://www.prometheus.io/&debug=true . Service monitors are not being created \u2691 When running helmfile apply several times to update the resources, some are not being correctly created. Until the bug is solved, a workaround is to remove the chart release helm delete --purge prometeus-blackbox-exporter and running helmfile apply again. probe_success == 0 when using an http proxy \u2691 Even when using an external http proxy, the probe runs the DNS resolution locally. Therefore if the record doesn't exist in the local server the probe will fail, even if the proxy DNS resolver has the correct record. The ugly workaround I've implemented is to create a \"fake\" DNS record in my internal DNS server so the probe sees it exist. Links \u2691 Git . Blackbox exporter modules configuration . Devconnected introduction to blackbox exporter .","title":"Blackbox Exporter"},{"location":"devops/prometheus/blackbox_exporter/#installation","text":"To install the exporter we'll use helmfile to install the stable/prometheus-blackbox-exporter chart . Add the following lines to your helmfile.yaml . - name : prometheus-blackbox-exporter namespace : monitoring chart : stable/prometheus-blackbox-exporter values : - prometheus-blackbox-exporter/values.yaml Edit the chart values. mkdir prometheus-blackbox-exporter helm inspect values stable/prometheus-blackbox-exporter > prometheus-blackbox-exporter/values.yaml vi prometheus-blackbox-exporter/values.yaml Make sure to enable the serviceMonitor in the values and target at least one page: serviceMonitor : enabled : true # Default values that will be used for all ServiceMonitors created by `targets` defaults : labels : release : prometheus-operator interval : 30s scrapeTimeout : 30s module : http_2xx targets : - name : lyz-code.github.io/blue-book url : https://lyz-code.github.io/blue-book The label release: prometheus-operator must be the one your prometheus instance is searching for . If you want to use the icmp probe, make sure to allow allowIcmp: true . If you want to probe endpoints protected behind client SSL certificates, until this chart issue is solved, you need to create them manually as the Prometheus blackbox exporter helm chart doesn't yet create the required secrets. kubectl create secret generic monitor-certificates \\ --from-file = monitor.crt.pem \\ --from-file = monitor.key.pem \\ -n monitoring Where monitor.crt.pem and monitor.key.pem are the SSL certificate and key for the monitor account. I've found two grafana dashboards for the blackbox exporter. 7587 didn't work straight out of the box while 5345 did. Taking as reference the grafana helm chart values, add the following yaml under the grafana key in the prometheus-operator values.yaml . grafana : enabled : true defaultDashboardsEnabled : true dashboardProviders : dashboardproviders.yaml : apiVersion : 1 providers : - name : 'default' orgId : 1 folder : '' type : file disableDeletion : false editable : true options : path : /var/lib/grafana/dashboards/default dashboards : default : blackbox-exporter : # Ref: https://grafana.com/dashboards/5345 gnetId : 5345 revision : 3 datasource : Prometheus And install. helmfile diff helmfile apply","title":"Installation"},{"location":"devops/prometheus/blackbox_exporter/#blackbox-exporter-probes","text":"Modules define how blackbox exporter is going to query the endpoint, therefore one needs to be created for each request type under the config.modules section of the chart. The modules are then used in the targets section for the desired endpoints. targets : - name : lyz-code.github.io/blue-book url : https://lyz-code.github.io/blue-book module : https_2xx","title":"Blackbox exporter probes"},{"location":"devops/prometheus/blackbox_exporter/#http-endpoint-working-correctly","text":"http_2xx : prober : http timeout : 5s http : valid_http_versions : [ \"HTTP/1.1\" , \"HTTP/2\" ] valid_status_codes : [ 200 ] no_follow_redirects : false preferred_ip_protocol : \"ip4\"","title":"HTTP endpoint working correctly"},{"location":"devops/prometheus/blackbox_exporter/#https-endpoint-working-correctly","text":"https_2xx : prober : http timeout : 5s http : method : GET fail_if_ssl : false fail_if_not_ssl : true valid_http_versions : [ \"HTTP/1.1\" , \"HTTP/2\" ] valid_status_codes : [ 200 ] no_follow_redirects : false preferred_ip_protocol : \"ip4\"","title":"HTTPS endpoint working correctly"},{"location":"devops/prometheus/blackbox_exporter/#https-endpoint-behind-client-ssl-certificate","text":"https_client_2xx : prober : http timeout : 5s http : method : GET fail_if_ssl : false fail_if_not_ssl : true valid_http_versions : [ \"HTTP/1.1\" , \"HTTP/2\" ] valid_status_codes : [ 200 ] no_follow_redirects : false preferred_ip_protocol : \"ip4\" tls_config : cert_file : /etc/secrets/monitor.crt.pem key_file : /etc/secrets/monitor.key.pem Where the secrets have been created throughout the installation.","title":"HTTPS endpoint behind client SSL certificate"},{"location":"devops/prometheus/blackbox_exporter/#https-endpoint-with-an-specific-error","text":"If you don't want to configure the authentication for example for an API, you can fetch the expected error. https_client_api : prober : http timeout : 5s http : method : GET fail_if_ssl : false fail_if_not_ssl : true valid_http_versions : [ \"HTTP/1.1\" , \"HTTP/2\" ] valid_status_codes : [ 404 ] no_follow_redirects : false preferred_ip_protocol : \"ip4\" fail_if_body_not_matches_regexp : - '.*ERROR route not.*'","title":"HTTPS endpoint with an specific error"},{"location":"devops/prometheus/blackbox_exporter/#http-endpoint-returning-an-error","text":"http_4xx : prober : http timeout : 5s http : method : HEAD valid_status_codes : [ 404 , 403 ] valid_http_versions : [ \"HTTP/1.1\" , \"HTTP/2\" ] no_follow_redirects : false","title":"HTTP endpoint returning an error"},{"location":"devops/prometheus/blackbox_exporter/#https-endpoint-through-an-http-proxy","text":"https_external_2xx : prober : http timeout : 5s http : method : GET fail_if_ssl : false fail_if_not_ssl : true valid_http_versions : [ \"HTTP/1.0\" , \"HTTP/1.1\" , \"HTTP/2\" ] valid_status_codes : [ 200 ] no_follow_redirects : false proxy_url : \"http://{{ proxy_url }}:{{ proxy_port }}\" preferred_ip_protocol : \"ip4\"","title":"HTTPS endpoint through an HTTP proxy"},{"location":"devops/prometheus/blackbox_exporter/#https-endpoint-with-basic-auth","text":"https_basic_auth_2xx : prober : http timeout : 5s http : method : GET fail_if_ssl : false fail_if_not_ssl : true valid_http_versions : - HTTP/1.1 - HTTP/2 valid_status_codes : - 200 no_follow_redirects : false preferred_ip_protocol : ip4 basic_auth : username : {{ username }} password : {{ password }}","title":"HTTPS endpoint with basic auth"},{"location":"devops/prometheus/blackbox_exporter/#https-endpoint-with-api-key","text":"https_api_2xx : prober : http timeout : 5s http : method : GET fail_if_ssl : false fail_if_not_ssl : true valid_http_versions : - HTTP/1.1 - HTTP/2 valid_status_codes : - 200 no_follow_redirects : false preferred_ip_protocol : ip4 headers : apikey : {{ api_key }}","title":"HTTPs endpoint with API key"},{"location":"devops/prometheus/blackbox_exporter/#https-put-file","text":"Test if the probe can upload a file. https_put_file_2xx : prober : http timeout : 5s http : method : PUT body : hi fail_if_ssl : false fail_if_not_ssl : true valid_http_versions : [ \"HTTP/1.1\" , \"HTTP/2\" ] valid_status_codes : [ 200 ] no_follow_redirects : false preferred_ip_protocol : \"ip4\"","title":"HTTPS Put file"},{"location":"devops/prometheus/blackbox_exporter/#check-open-port","text":"tcp_connect : prober : tcp The port is specified when using the module. - name : lyz-code.github.io url : lyz-code.github.io:389 module : tcp_connect","title":"Check open port"},{"location":"devops/prometheus/blackbox_exporter/#ping-to-the-resource","text":"Test if the target is alive. It's useful When you don't know what port to check or if it uses UDP. ping : prober : icmp timeout : 5s icmp : preferred_ip_protocol : \"ip4\"","title":"Ping to the resource"},{"location":"devops/prometheus/blackbox_exporter/#blackbox-exporter-alerts","text":"Now that we've got the metrics, we can define the alert rules . Most have been tweaked from the Awesome prometheus alert rules collection. To make security tests","title":"Blackbox exporter alerts"},{"location":"devops/prometheus/blackbox_exporter/#availability-alerts","text":"The most basic probes, test if the service is up and returning.","title":"Availability alerts"},{"location":"devops/prometheus/blackbox_exporter/#blackbox-probe-failed","text":"Blackbox probe failed. - alert : BlackboxProbeFailed expr : probe_success == 0 for : 5m labels : severity : error annotations : summary : \"Blackbox probe failed (instance {{ $labels.target }})\" message : \"Probe failed\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url }}&var-targets={{ $labels.target }}\" prometheus : \"{{ prometheus_url }}/graph?g0.expr=probe_success+%3D%3D+0&g0.tab=1\" If you use the security alerts , use the following expr: instead expr : probe_success{target!~\".*-fail-.*$\"} == 0","title":"Blackbox probe failed"},{"location":"devops/prometheus/blackbox_exporter/#blackbox-probe-http-failure","text":"HTTP status code is not 200-399. - alert : BlackboxProbeHttpFailure expr : probe_http_status_code <= 199 OR probe_http_status_code >= 400 for : 5m labels : severity : error annotations : summary : \"Blackbox probe HTTP failure (instance {{ $labels.target }})\" message : \"HTTP status code is not 200-399\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url }}&var-targets={{ $labels.target }}\" prometheus : \"{{ prometheus_url }}/graph?g0.expr=probe_ssl_earliest_cert_expiry+-+time%28%29+%3C+86400+%2A+30&g0.tab=1\"","title":"Blackbox probe HTTP failure"},{"location":"devops/prometheus/blackbox_exporter/#performance-alerts","text":"","title":"Performance alerts"},{"location":"devops/prometheus/blackbox_exporter/#blackbox-slow-probe","text":"Blackbox probe took more than 1s to complete. - alert : BlackboxSlowProbe expr : avg_over_time(probe_duration_seconds[1m]) > 1 for : 5m labels : severity : warning annotations : summary : \"Blackbox slow probe (target {{ $labels.target }})\" message : \"Blackbox probe took more than 1s to complete\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url }}&var-targets={{ $labels.target }}\" prometheus : \"{{ prometheus_url }}/graph?g0.expr=avg_over_time%28probe_duration_seconds%5B1m%5D%29+%3E+1&g0.tab=1\" If you use the security alerts , use the following expr: instead expr : avg_over_time(probe_duration_seconds{,target!~\".*-fail-.*\"}[1m]) > 1","title":"Blackbox slow probe"},{"location":"devops/prometheus/blackbox_exporter/#blackbox-probe-slow-http","text":"HTTP request took more than 1s. - alert : BlackboxProbeSlowHttp expr : avg_over_time(probe_http_duration_seconds[1m]) > 1 for : 5m labels : severity : warning annotations : summary : \"Blackbox probe slow HTTP (instance {{ $labels.target }})\" message : \"HTTP request took more than 1s\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url }}&var-targets={{ $labels.target }}\" prometheus : \"{{ prometheus_url }}/graph?g0.expr=avg_over_time%28probe_http_duration_seconds%5B1m%5D%29+%3E+1&g0.tab=1\" If you use the security alerts , use the following expr: instead expr : avg_over_time(probe_http_duration_seconds{,target!~\".*-fail-.*\"}[1m]) > 1","title":"Blackbox probe slow HTTP"},{"location":"devops/prometheus/blackbox_exporter/#blackbox-probe-slow-ping","text":"Blackbox ping took more than 1s. - alert : BlackboxProbeSlowPing expr : avg_over_time(probe_icmp_duration_seconds[1m]) > 1 for : 5m labels : severity : warning annotations : summary : \"Blackbox probe slow ping (instance {{ $labels.target }})\" message : \"Blackbox ping took more than 1s\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url }}&var-targets={{ $labels.target }}\" prometheus : \"{{ prometheus_url }}/graph?g0.expr=avg_over_time%28probe_icmp_duration_seconds%5B1m%5D%29+%3E+1&g0.tab=1\"","title":"Blackbox probe slow ping"},{"location":"devops/prometheus/blackbox_exporter/#ssl-certificate-alerts","text":"","title":"SSL certificate alerts"},{"location":"devops/prometheus/blackbox_exporter/#blackbox-ssl-certificate-will-expire-in-a-month","text":"SSL certificate expires in 30 days. - alert : BlackboxSslCertificateWillExpireSoon expr : probe_ssl_earliest_cert_expiry - time() < 86400 * 30 for : 5m labels : severity : warning annotations : summary : \"Blackbox SSL certificate will expire soon (instance {{ $labels.target }})\" message : \"SSL certificate expires in 30 days\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url }}&var-targets={{ $labels.target }}\" prometheus : \"{{ prometheus_url }}/graph?g0.expr=probe_ssl_earliest_cert_expiry+-+time%28%29+%3C+86400+%2A+30&g0.tab=1\"","title":"Blackbox SSL certificate will expire in a month"},{"location":"devops/prometheus/blackbox_exporter/#blackbox-ssl-certificate-will-expire-in-a-few-days","text":"SSL certificate expires in 3 days. - alert : BlackboxSslCertificateWillExpireSoon expr : probe_ssl_earliest_cert_expiry - time() < 86400 * 3 for : 5m labels : severity : error annotations : summary : \"Blackbox SSL certificate will expire soon (instance {{ $labels.target }})\" message : \"SSL certificate expires in 3 days\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url }}&var-targets={{ $labels.target }}\" prometheus : \"{{ prometheus_url }}/graph?g0.expr=probe_ssl_earliest_cert_expiry+-+time%28%29+%3C+86400+%2A+3&g0.tab=1\"","title":"Blackbox SSL certificate will expire in a few days"},{"location":"devops/prometheus/blackbox_exporter/#blackbox-ssl-certificate-expired","text":"SSL certificate has expired already. - alert : BlackboxSslCertificateExpired expr : probe_ssl_earliest_cert_expiry - time() <= 0 for : 5m labels : severity : error annotations : summary : \"Blackbox SSL certificate expired (instance {{ $labels.target }})\" message : \"SSL certificate has expired already\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url }}&var-targets={{ $labels.target }}\" prometheus : \"{{ prometheus_url }}/graph?g0.expr=probe_ssl_earliest_cert_expiry+-+time%28%29+%3C%3D+0&g0.tab=1\"","title":"Blackbox SSL certificate expired"},{"location":"devops/prometheus/blackbox_exporter/#security-alerts","text":"To define the security alerts, I've found easier to create a probe with the action I want to prevent and make sure that the probe fails. This probes contain the -fail- key in the target name, followed by the test it's performing. This convention allows the concatenation of tests. For example, when testing if and endpoint is accessible without basic auth and without vpn we'd use: - name : protected.endpoint.org-fail-without-ssl-and-without-credentials url : protected.endpoint.org module : https_external_2xx","title":"Security alerts"},{"location":"devops/prometheus/blackbox_exporter/#test-endpoints-protected-with-network-policies","text":"Assuming that the blackbox exporter is in the internal network and that there is an http proxy on the external network we want to test. Create a working probe with the https_external_2xx module containing the -fail-without-vpn key in the target name. - alert : BlackboxVPNProtectionRemoved expr : probe_success{target=~\".*-fail-.*without-vpn.*\"} == 1 for : 5m labels : severity : error annotations : summary : \"VPN protection was removed from (instance {{ $labels.target }})\" message : \"Successful probe to the endpoint from outside the internal network\" grafana : \"{{ grafana_url }}&var-targets={{ $labels.target }}\" prometheus : \"{{ prometheus_url }}/graph?g0.expr=probe_ssl_earliest_cert_expiry+-+time%28%29+%3C%3D+0&g0.tab=1\"","title":"Test endpoints protected with network policies"},{"location":"devops/prometheus/blackbox_exporter/#test-endpoints-protected-with-ssl-client-certificate","text":"Create a working probe with a module without the SSL client certificate configured, such as https_2xx and set the -fail-without-ssl key in the target name. - alert : BlackboxClientSSLProtectionRemoved expr : probe_success{target=~\".*-fail-.*without-ssl.*\"} == 1 for : 5m labels : severity : error annotations : summary : \"SSL client certificate protection was removed from (instance {{ $labels.target }})\" message : \"Successful probe to the endpoint without SSL certificate\" grafana : \"{{ grafana_url }}&var-targets={{ $labels.target }}\" prometheus : \"{{ prometheus_url }}/graph?g0.expr=probe_ssl_earliest_cert_expiry+-+time%28%29+%3C%3D+0&g0.tab=1\"","title":"Test endpoints protected with SSL client certificate"},{"location":"devops/prometheus/blackbox_exporter/#test-endpoints-protected-with-credentials","text":"Create a working probe with a module without the basic auth credentials configured, such as https_2xx and set the -fail-without-credentials key in the target name. - alert : BlackboxCredentialsProtectionRemoved expr : probe_success{target=~\".*-fail-.*without-credentials.*\"} == 1 for : 5m labels : severity : error annotations : summary : \"Credentials protection was removed from (instance {{ $labels.target }})\" message : \"Successful probe to the endpoint without credentials\" grafana : \"{{ grafana_url }}&var-targets={{ $labels.target }}\" prometheus : \"{{ prometheus_url }}/graph?g0.expr=probe_ssl_earliest_cert_expiry+-+time%28%29+%3C%3D+0&g0.tab=1\"","title":"Test endpoints protected with credentials."},{"location":"devops/prometheus/blackbox_exporter/#test-endpoints-protected-with-waf","text":"Create a working probe with a module bypassing the WAF, for example directly attacking the service and set the -fail-without-waf key in the target name. - alert : BlackboxWAFProtectionRemoved expr : probe_success{target=~\".*-fail-.*without-waf.*\"} == 1 for : 5m labels : severity : error annotations : summary : \"WAF protection was removed from (instance {{ $labels.target }})\" message : \"Successful probe to the haproxy endpoint from the internal network (bypassed the WAF)\" grafana : \"{{ grafana_url }}&var-targets={{ $labels.target }}\" prometheus : \"{{ prometheus_url }}/graph?g0.expr=probe_ssl_earliest_cert_expiry+-+time%28%29+%3C%3D+0&g0.tab=1\"","title":"Test endpoints protected with WAF."},{"location":"devops/prometheus/blackbox_exporter/#unauthorized-read-of-s3-buckets","text":"Create a working probe to an existent private object in an S3 bucket and set the -fail-read-object key in the target name. - alert : BlackboxS3BucketWrongReadPermissions expr : probe_success{target=~\".*-fail-.*read-object.*\"} == 1 for : 5m labels : severity : error annotations : summary : \"Wrong read permissions on S3 bucket (instance {{ $labels.target }})\" message : \"Successful read of a private object with an unauthenticated user\" grafana : \"{{ grafana_url }}&var-targets={{ $labels.target }}\" prometheus : \"{{ prometheus_url }}/graph?g0.expr=probe_ssl_earliest_cert_expiry+-+time%28%29+%3C%3D+0&g0.tab=1\"","title":"Unauthorized read of S3 buckets"},{"location":"devops/prometheus/blackbox_exporter/#unauthorized-write-of-s3-buckets","text":"Create a working probe using the https_put_file_2xx module to try to create a file in an S3 bucket and set the -fail-write-object key in the target name. - alert : BlackboxS3BucketWrongWritePermissions expr : probe_success{target=~\".*-fail-.*write-object.*\"} == 1 for : 5m labels : severity : error annotations : summary : \"Wrong write permissions on S3 bucket (instance {{ $labels.target }})\" message : \"Successful write of a private object with an unauthenticated user\" grafana : \"{{ grafana_url }}&var-targets={{ $labels.target }}\" prometheus : \"{{ prometheus_url }}/graph?g0.expr=probe_ssl_earliest_cert_expiry+-+time%28%29+%3C%3D+0&g0.tab=1\"","title":"Unauthorized write of S3 buckets"},{"location":"devops/prometheus/blackbox_exporter/#monitoring-external-access-to-internal-services","text":"There are two possible solutions to simulate traffic from outside your infrastructure to the internal services. Both require the installation of an agent outside of your internal infrastructure, it can be: An HTTP proxy. A blackbox exporter instance. Using the proxy you have following advantages: It's really easy to set up a transparent http proxy . All probe configuration goes in the same blackbox exporter instance values.yaml . With the following disadvantages: When using an external http proxy, the probe runs the DNS resolution locally . Therefore if the record doesn't exist in the local server the probe will fail, even if the proxy DNS resolver has the correct record. The ugly workaround I've implemented is to create a \"fake\" DNS record in my internal DNS server so the probe sees it exist. * There is no way to do tcp or ping probes to simulate external traffic. * The latency between the blackbox exporter and the proxy is added to all the external probes. While using an external blackbox exporter gives the following advantages: Traffic is completely external to the infrastructure, so the proxy disadvantages would be solved. And the following disadvantages: Simulation of external traffic in AWS could be done by spawning the blackbox exporter instance in another region, but as there is no way of using EKS worker nodes in different regions, there is no way of managing the exporter from within Kubernetes. This means: The loose of the advantages of the Prometheus operator , so we have to write the configuration manually. Configuration can't be managed with Helm , so two solutions should be used to manage the monitorization (Ansible could be used). Even if it's possible to host the second external blackbox exporter within Kubernetes, two independent Helm charts are needed, with the consequent configuration management burden. In conclusion, when using a Kubernetes cluster that allows the creation of worker nodes outside the main infrastructure, or if several non HTTP/HTTPS endpoints need to be probed with the tcp or ping modules, install an external blackbox exporter instance. Otherwise install an HTTP proxy and assume that you can only simulate external HTTP/HTTPS traffic.","title":"Monitoring external access to internal services"},{"location":"devops/prometheus/blackbox_exporter/#troubleshooting","text":"To get more debugging information of the blackbox probes, add &debug=true to the probe url, for example http://localhost:9115/probe?module=http_2xx&target=https://www.prometheus.io/&debug=true .","title":"Troubleshooting"},{"location":"devops/prometheus/blackbox_exporter/#service-monitors-are-not-being-created","text":"When running helmfile apply several times to update the resources, some are not being correctly created. Until the bug is solved, a workaround is to remove the chart release helm delete --purge prometeus-blackbox-exporter and running helmfile apply again.","title":"Service monitors are not being created"},{"location":"devops/prometheus/blackbox_exporter/#probe_success-0-when-using-an-http-proxy","text":"Even when using an external http proxy, the probe runs the DNS resolution locally. Therefore if the record doesn't exist in the local server the probe will fail, even if the proxy DNS resolver has the correct record. The ugly workaround I've implemented is to create a \"fake\" DNS record in my internal DNS server so the probe sees it exist.","title":"probe_success == 0 when using an http proxy"},{"location":"devops/prometheus/blackbox_exporter/#links","text":"Git . Blackbox exporter modules configuration . Devconnected introduction to blackbox exporter .","title":"Links"},{"location":"devops/prometheus/instance_sizing_analysis/","text":"Once we gather the instance metrics with the Node exporter , we can do statistical analysis on the evolution of time to detect the instances that are undersized or oversized. RAM analysis \u2691 Instance RAM percent usage metric can be calculated with the following Prometheus rule : - record : instance_path:node_memory_MemAvailable_percent expr : (1 - node_memory_MemAvailable_bytes/node_memory_MemTotal_bytes ) * 100 The average , standard deviation and the standard score of the last two weeks would be: - record : instance_path:node_memory_MemAvailable_percent:avg_over_time_2w expr : avg_over_time(instance_path:node_memory_MemAvailable_percent[2w]) - record : instance_path:node_memory_MemAvailable_percent:stddev_over_time_2w expr : stddev_over_time(instance_path:node_memory_MemAvailable_percent[2w]) - record : instance_path:node_memory_MemAvailable_percent:z_score expr : > ( instance_path:node_memory_MemAvailable_percent - instance_path:node_memory_MemAvailable_percent:avg_over_time_2w ) / instance_path:node_memory_MemAvailable_percent:stddev_over_time_2w With that data we can define that an instance is oversized if the average plus the standard deviation is less than 60% and undersized if its greater than 90%. With the average we take into account the nominal RAM consumption, and with the standard deviation we take into account the spikes. Tweak this rule to your use case The criteria of undersized and oversized is just a first approximation I'm going to use. You can use it as a base criteria, but don't go through with it blindly. See the disclaimer below for more information. # RAM - record : instance_path:wrong_resource_size expr : > instance_path:node_memory_MemAvailable_percent:avg_plus_stddev_over_time_2w < 60 labels : type : EC2 metric : RAM problem : oversized - record : instance_path:wrong_resource_size expr : > instance_path:node_memory_MemAvailable_percent:avg_plus_stddev_over_time_2w > 90 labels : type : EC2 metric : RAM problem : undersized Where avg_plus_stddev_over_time_2w is: - record : instance_path:node_memory_MemAvailable_percent:avg_plus_stddev_over_time_2w expr : > instance_path:node_memory_MemAvailable_percent:avg_over_time_2w + instance_path:node_memory_MemAvailable_percent:stddev_over_time_2w CPU analysis \u2691 Instance CPU percent usage metric can be calculated with the following Prometheus rule : - record : instance_path:node_cpu_percent:rate1m expr : > (1 - (avg by(instance) (rate(node_cpu_seconds_total{mode=\"idle\"}[1m])))) * 100 The node_cpu_seconds_total doesn't give us the percent of usage, that is why we need to do the average of the rate of the last minute. The average , standard deviation , the standard score and the undersize or oversize criteria is similar to the RAM case, so I'm adding it folded for reference only. CPU usage rules # --------------------------------------- # -- Resource consumption calculations -- # --------------------------------------- # CPU - record : instance_path:node_cpu_percent:rate1m expr : > (1 - (avg by(instance) (rate(node_cpu_seconds_total{mode=\"idle\"}[1m])))) * 100 - record : instance_path:node_cpu_percent:rate1m:avg_over_time_2w expr : avg_over_time(instance_path:node_cpu_percent:rate1m[2w]) - record : instance_path:node_cpu_percent:rate1m:stddev_over_time_2w expr : stddev_over_time(instance_path:node_cpu_percent:rate1m[2w]) - record : instance_path:node_cpu_percent:rate1m:avg_plus_stddev_over_time_2w expr : > instance_path:node_cpu_percent:rate1m:avg_over_time_2w + instance_path:node_cpu_percent:rate1m:stddev_over_time_2w - record : instance_path:node_cpu_percent:rate1m:z_score expr : > ( instance_path:node_cpu_percent:rate1m - instance_path:node_cpu_percent:rate1m:avg_over_time_2w ) / instance_path:node_cpu_percent:rate1m:stddev_over_time_2w # ---------------------------------- # -- Resource sizing calculations -- # ---------------------------------- # CPU - record : instance_path:wrong_resource_size expr : instance_path:node_cpu_percent:rate1m:avg_plus_stddev_over_time_2w < 60 labels : type : EC2 metric : CPU problem : oversized - record : instance_path:wrong_resource_size expr : instance_path:node_cpu_percent:rate1m:avg_plus_stddev_over_time_2w > 80 labels : type : EC2 metric : CPU problem : undersized Network analysis \u2691 We can deduce the network usage from the node_network_receive_bytes_total and node_network_transmit_bytes_total metrics. For example for the transmit, the Gigabits per second transmitted can be calculated with the following Prometheus rule : - record : instance_path:node_network_transmit_gigabits_per_second:rate5m expr : > increase( node_network_transmit_bytes_total{device=~\"(eth0|ens.*)\"}[1m] ) * 7.450580596923828 * 10^-9 / 60 Where we: Filter the traffic only to the external network interfaces node_network_transmit_bytes_total{device=~\"(eth0|ens.*)\"} . Those are the ones used by AWS , but you'll need to tweak that for your case. Convert the increase of Kilobytes per minute [1m] to Gigabits per second by multiplying it by 7.450580596923828 * 10^-9 / 60 . The average , standard deviation , the standard score and the undersize or oversize criteria is similar to the RAM case, so I'm adding it folded for reference only. Network usage rules # --------------------------------------- # -- Resource consumption calculations -- # --------------------------------------- # NetworkReceive - record : instance_path:node_network_receive_gigabits_per_second:rate1m expr : > increase( node_network_receive_bytes_total{device=~\"(eth0|ens.*)\"}[1m] ) * 7.450580596923828 * 10^-9 / 60 - record : instance_path:node_network_receive_gigabits_per_second:rate1m:avg_over_time_2w expr : > avg_over_time( instance_path:node_network_receive_gigabits_per_second:rate1m[2w] ) - record : instance_path:node_network_receive_gigabits_per_second:rate1m:stddev_over_time_2w expr : > stddev_over_time( instance_path:node_network_receive_gigabits_per_second:rate1m[2w] ) - record : instance_path:node_network_receive_gigabits_per_second:rate1m:avg_plus_stddev_over_time_2w expr : > instance_path:node_network_receive_gigabits_per_second:rate1m:avg_over_time_2w + instance_path:node_network_receive_gigabits_per_second:rate1m:stddev_over_time_2w - record : instance_path:node_network_receive_gigabits_per_second:rate1m:z_score expr : > ( instance_path:node_network_receive_gigabits_per_second:rate1m - instance_path:node_network_receive_gigabits_per_second:rate1m:avg_over_time_2w ) / instance_path:node_network_receive_gigabits_per_second:rate1m:stddev_over_time_2w # NetworkTransmit - record : instance_path:node_network_transmit_gigabits_per_second:rate1m expr : > increase( node_network_transmit_bytes_total{device=~\"(eth0|ens.*)\"}[1m] ) * 7.450580596923828 * 10^-9 / 60 - record : instance_path:node_network_transmit_gigabits_per_second:rate1m:avg_over_time_2w expr : > avg_over_time( instance_path:node_network_transmit_gigabits_per_second:rate1m[2w] ) - record : instance_path:node_network_transmit_gigabits_per_second:rate1m:stddev_over_time_2w expr : > stddev_over_time( instance_path:node_network_transmit_gigabits_per_second:rate1m[2w] ) - record : instance_path:node_network_transmit_gigabits_per_second:rate1m:avg_plus_stddev_over_time_2w expr : > instance_path:node_network_transmit_gigabits_per_second:rate1m:avg_over_time_2w + instance_path:node_network_transmit_gigabits_per_second:rate1m:stddev_over_time_2w - record : instance_path:node_network_transmit_gigabits_per_second:rate1m:z_score expr : > ( instance_path:node_network_transmit_gigabits_per_second:rate1m - instance_path:node_network_transmit_gigabits_per_second:rate1m:avg_over_time_2w ) / instance_path:node_network_transmit_gigabits_per_second:rate1m:stddev_over_time_2w # ---------------------------------- # -- Resource sizing calculations -- # ---------------------------------- # NetworkReceive - record : instance_path:wrong_resource_size expr : > instance_path:node_network_receive_gigabits_per_second:rate1m:avg_plus_stddev_over_time_2w < 0.5 labels : type : EC2 metric : NetworkReceive problem : oversized - record : instance_path:wrong_resource_size expr : > instance_path:node_network_receive_gigabits_per_second:rate1m:avg_plus_stddev_over_time_2w > 3 labels : type : EC2 metric : NetworkReceive problem : undersized # NetworkTransmit - record : instance_path:wrong_resource_size expr : > instance_path:node_network_transmit_gigabits_per_second:rate1m:avg_plus_stddev_over_time_2w < 0.5 labels : type : EC2 metric : NetworkTransmit problem : oversized - record : instance_path:wrong_resource_size expr : > instance_path:node_network_transmit_gigabits_per_second:rate1m:avg_plus_stddev_over_time_2w > 3 labels : type : EC2 metric : NetworkTransmit problem : undersized The difference with network is that we don't have a percent of the total instance bandwidth, In my case, my instances support from 0.5 to 5 Gbps which is more than I need, so most of my instances are marked as oversized with the < 0.5 rule. I will manually study the ones that go over 3 Gbps. The correct way to do it, is to tag the baseline, burst or/and maximum network performance by instance type. In the AWS case, the data can be extracted using the AWS docs or external benchmarks . Once you know the network performance per instance type, you can use relabeling in the Node exporter service monitor to add a label like max_network_performance and use it later in the rules. If you do follow this path, please contact me or do a pull request so I can test your solution. Overall analysis \u2691 Now that we have all the analysis under the metric instance_path:wrong_resource_size with labels, we can aggregate them to see the number of rules each instance is breaking with the following rule: # Mark the number of oversize rules matched by each instance - record : instance_path:wrong_instance_size expr : count by (instance) (sum by (metric, instance) (instance_path:wrong_resource_size)) By executing sort_desc(instance_path:wrong_instance_size) in the Prometheus web application, we'll be able to see such instances. instance_path:wrong_instance_size{instance=\"frontend-production:192.168.1.2\"} 4 instance_path:wrong_instance_size{instance=\"backend-production-instance:172.30.0.195\"} 2 ... To see the detail of what rules is our instance breaking we can use something like instance_path:wrong_resource_size{instance =~'frontend.*'} instance_path:wrong_resource_size{instance=\"fronted-production:192.168.1.2\",instance_type=\"c4.2xlarge\",job=\"node-exporter\",metric=\"RAM\",problem=\"oversized\",type=\"EC2\"} 5.126602454544287 instance_path:wrong_resource_size{instance=\"fronted-production:192.168.1.2\",metric=\"CPU\",problem=\"oversized\",type=\"EC2\"} 0.815639209497615 instance_path:wrong_resource_size{device=\"ens3\",instance=\"fronted-production:192.168.1.2\",instance_type=\"c4.2xlarge\",job=\"node-exporter\",metric=\"NetworkReceive\",problem=\"oversized\",type=\"EC2\"} 0.02973250128744766 instance_path:wrong_resource_size{device=\"ens3\",instance=\"fronted-production:192.168.1.2\",instance_type=\"c4.2xlarge\",job=\"node-exporter\",metric=\"NetworkTransmit\",problem=\"oversized\",type=\"EC2\"} 0.01586461503849804 Here we see that the frontend-production is a c4.2xlarge instance that is consuming an average plus standard deviation of CPU of 0.81%, RAM 5.12%, NetworkTransmit 0.015Gbps and NetworkReceive 0.029Gbps, which results in an oversized alert on all four metrics. If you want to see the evolution over the time, instead of Console click on Graph under the text box where you have entered the query. With this information, we can decide which is the correct instance for each application. Once all instances are migrated to their ideal size, we can add alerts on these metrics so we can have a continuous analysis of our instances. Once I've done it, I'll add the alerts here. Disclaimer \u2691 We haven't tested this rules yet in production to resize our infrastructure (will do soon), so use all the information in this document cautiously. What I can expect to fail is that the assumption of average plus a standard deviation criteria can not be enough, maybe I need to increase the resolution of the standard deviation so it can be more sensible to the spikes, or we need to use a safety factor of 2 or 3. We'll see :) Read throughly the Gitlab post on anomaly detection using Prometheus , it's awesome and it may give you insights on why this approach is not working with you, as well as other algorithms that for example take into account the seasonality of the metrics. In particular it's interesting to analyze your resources z-score evolution over time, if all values fall in the +4 to -4 range, you can statistically assert that your metric similarly follows the normal distribution, and can assume that any value of z_score above 3 is an anomaly. If your results return with a range of +20 to -20 , the tail is too long and your results will be skewed. To test it you can use the following queries to test the RAM behaviour, adapt them for the rest of the resources: # Minimum z_score value sort_desc ( abs (( min_over_time ( instance_path : node_memory_MemAvailable_percent [ 1w ] ) - instance_path : node_memory_MemAvailable_percent : avg_over_time_2w ) / instance_path : node_memory_MemAvailable_percent : stddev_over_time_2w )) # Maximum z_score value sort_desc ( abs (( max_over_time ( instance_path : node_memory_MemAvailable_percent [ 1w ] ) - instance_path : node_memory_MemAvailable_percent : avg_over_time_2w ) / instance_path : node_memory_MemAvailable_percent : stddev_over_time_2w )) For a less exhaustive but more graphical analysis, execute instance_path:node_memory_MemAvailable_percent:z_score in Graph mode. In my case the RAM is in the +-5 interval, with some peaks of 20, but after reviewing instance_path:node_memory_MemAvailable_percent:avg_plus_stddev_over_time_2w in those periods, I feel it's still safe to use the assumption. Same criteria applies to instance_path:node_cpu_percent:rate1m:z_score , instance_path:node_network_receive_gigabits_per_second:rate1m:z_score , and instance_path:node_network_transmit_gigabits_per_second:rate1m:z_score , metrics. References \u2691 Gitlab post on anomaly detection using Prometheus .","title":"Instance sizing analysis"},{"location":"devops/prometheus/instance_sizing_analysis/#ram-analysis","text":"Instance RAM percent usage metric can be calculated with the following Prometheus rule : - record : instance_path:node_memory_MemAvailable_percent expr : (1 - node_memory_MemAvailable_bytes/node_memory_MemTotal_bytes ) * 100 The average , standard deviation and the standard score of the last two weeks would be: - record : instance_path:node_memory_MemAvailable_percent:avg_over_time_2w expr : avg_over_time(instance_path:node_memory_MemAvailable_percent[2w]) - record : instance_path:node_memory_MemAvailable_percent:stddev_over_time_2w expr : stddev_over_time(instance_path:node_memory_MemAvailable_percent[2w]) - record : instance_path:node_memory_MemAvailable_percent:z_score expr : > ( instance_path:node_memory_MemAvailable_percent - instance_path:node_memory_MemAvailable_percent:avg_over_time_2w ) / instance_path:node_memory_MemAvailable_percent:stddev_over_time_2w With that data we can define that an instance is oversized if the average plus the standard deviation is less than 60% and undersized if its greater than 90%. With the average we take into account the nominal RAM consumption, and with the standard deviation we take into account the spikes. Tweak this rule to your use case The criteria of undersized and oversized is just a first approximation I'm going to use. You can use it as a base criteria, but don't go through with it blindly. See the disclaimer below for more information. # RAM - record : instance_path:wrong_resource_size expr : > instance_path:node_memory_MemAvailable_percent:avg_plus_stddev_over_time_2w < 60 labels : type : EC2 metric : RAM problem : oversized - record : instance_path:wrong_resource_size expr : > instance_path:node_memory_MemAvailable_percent:avg_plus_stddev_over_time_2w > 90 labels : type : EC2 metric : RAM problem : undersized Where avg_plus_stddev_over_time_2w is: - record : instance_path:node_memory_MemAvailable_percent:avg_plus_stddev_over_time_2w expr : > instance_path:node_memory_MemAvailable_percent:avg_over_time_2w + instance_path:node_memory_MemAvailable_percent:stddev_over_time_2w","title":"RAM analysis"},{"location":"devops/prometheus/instance_sizing_analysis/#cpu-analysis","text":"Instance CPU percent usage metric can be calculated with the following Prometheus rule : - record : instance_path:node_cpu_percent:rate1m expr : > (1 - (avg by(instance) (rate(node_cpu_seconds_total{mode=\"idle\"}[1m])))) * 100 The node_cpu_seconds_total doesn't give us the percent of usage, that is why we need to do the average of the rate of the last minute. The average , standard deviation , the standard score and the undersize or oversize criteria is similar to the RAM case, so I'm adding it folded for reference only. CPU usage rules # --------------------------------------- # -- Resource consumption calculations -- # --------------------------------------- # CPU - record : instance_path:node_cpu_percent:rate1m expr : > (1 - (avg by(instance) (rate(node_cpu_seconds_total{mode=\"idle\"}[1m])))) * 100 - record : instance_path:node_cpu_percent:rate1m:avg_over_time_2w expr : avg_over_time(instance_path:node_cpu_percent:rate1m[2w]) - record : instance_path:node_cpu_percent:rate1m:stddev_over_time_2w expr : stddev_over_time(instance_path:node_cpu_percent:rate1m[2w]) - record : instance_path:node_cpu_percent:rate1m:avg_plus_stddev_over_time_2w expr : > instance_path:node_cpu_percent:rate1m:avg_over_time_2w + instance_path:node_cpu_percent:rate1m:stddev_over_time_2w - record : instance_path:node_cpu_percent:rate1m:z_score expr : > ( instance_path:node_cpu_percent:rate1m - instance_path:node_cpu_percent:rate1m:avg_over_time_2w ) / instance_path:node_cpu_percent:rate1m:stddev_over_time_2w # ---------------------------------- # -- Resource sizing calculations -- # ---------------------------------- # CPU - record : instance_path:wrong_resource_size expr : instance_path:node_cpu_percent:rate1m:avg_plus_stddev_over_time_2w < 60 labels : type : EC2 metric : CPU problem : oversized - record : instance_path:wrong_resource_size expr : instance_path:node_cpu_percent:rate1m:avg_plus_stddev_over_time_2w > 80 labels : type : EC2 metric : CPU problem : undersized","title":"CPU analysis"},{"location":"devops/prometheus/instance_sizing_analysis/#network-analysis","text":"We can deduce the network usage from the node_network_receive_bytes_total and node_network_transmit_bytes_total metrics. For example for the transmit, the Gigabits per second transmitted can be calculated with the following Prometheus rule : - record : instance_path:node_network_transmit_gigabits_per_second:rate5m expr : > increase( node_network_transmit_bytes_total{device=~\"(eth0|ens.*)\"}[1m] ) * 7.450580596923828 * 10^-9 / 60 Where we: Filter the traffic only to the external network interfaces node_network_transmit_bytes_total{device=~\"(eth0|ens.*)\"} . Those are the ones used by AWS , but you'll need to tweak that for your case. Convert the increase of Kilobytes per minute [1m] to Gigabits per second by multiplying it by 7.450580596923828 * 10^-9 / 60 . The average , standard deviation , the standard score and the undersize or oversize criteria is similar to the RAM case, so I'm adding it folded for reference only. Network usage rules # --------------------------------------- # -- Resource consumption calculations -- # --------------------------------------- # NetworkReceive - record : instance_path:node_network_receive_gigabits_per_second:rate1m expr : > increase( node_network_receive_bytes_total{device=~\"(eth0|ens.*)\"}[1m] ) * 7.450580596923828 * 10^-9 / 60 - record : instance_path:node_network_receive_gigabits_per_second:rate1m:avg_over_time_2w expr : > avg_over_time( instance_path:node_network_receive_gigabits_per_second:rate1m[2w] ) - record : instance_path:node_network_receive_gigabits_per_second:rate1m:stddev_over_time_2w expr : > stddev_over_time( instance_path:node_network_receive_gigabits_per_second:rate1m[2w] ) - record : instance_path:node_network_receive_gigabits_per_second:rate1m:avg_plus_stddev_over_time_2w expr : > instance_path:node_network_receive_gigabits_per_second:rate1m:avg_over_time_2w + instance_path:node_network_receive_gigabits_per_second:rate1m:stddev_over_time_2w - record : instance_path:node_network_receive_gigabits_per_second:rate1m:z_score expr : > ( instance_path:node_network_receive_gigabits_per_second:rate1m - instance_path:node_network_receive_gigabits_per_second:rate1m:avg_over_time_2w ) / instance_path:node_network_receive_gigabits_per_second:rate1m:stddev_over_time_2w # NetworkTransmit - record : instance_path:node_network_transmit_gigabits_per_second:rate1m expr : > increase( node_network_transmit_bytes_total{device=~\"(eth0|ens.*)\"}[1m] ) * 7.450580596923828 * 10^-9 / 60 - record : instance_path:node_network_transmit_gigabits_per_second:rate1m:avg_over_time_2w expr : > avg_over_time( instance_path:node_network_transmit_gigabits_per_second:rate1m[2w] ) - record : instance_path:node_network_transmit_gigabits_per_second:rate1m:stddev_over_time_2w expr : > stddev_over_time( instance_path:node_network_transmit_gigabits_per_second:rate1m[2w] ) - record : instance_path:node_network_transmit_gigabits_per_second:rate1m:avg_plus_stddev_over_time_2w expr : > instance_path:node_network_transmit_gigabits_per_second:rate1m:avg_over_time_2w + instance_path:node_network_transmit_gigabits_per_second:rate1m:stddev_over_time_2w - record : instance_path:node_network_transmit_gigabits_per_second:rate1m:z_score expr : > ( instance_path:node_network_transmit_gigabits_per_second:rate1m - instance_path:node_network_transmit_gigabits_per_second:rate1m:avg_over_time_2w ) / instance_path:node_network_transmit_gigabits_per_second:rate1m:stddev_over_time_2w # ---------------------------------- # -- Resource sizing calculations -- # ---------------------------------- # NetworkReceive - record : instance_path:wrong_resource_size expr : > instance_path:node_network_receive_gigabits_per_second:rate1m:avg_plus_stddev_over_time_2w < 0.5 labels : type : EC2 metric : NetworkReceive problem : oversized - record : instance_path:wrong_resource_size expr : > instance_path:node_network_receive_gigabits_per_second:rate1m:avg_plus_stddev_over_time_2w > 3 labels : type : EC2 metric : NetworkReceive problem : undersized # NetworkTransmit - record : instance_path:wrong_resource_size expr : > instance_path:node_network_transmit_gigabits_per_second:rate1m:avg_plus_stddev_over_time_2w < 0.5 labels : type : EC2 metric : NetworkTransmit problem : oversized - record : instance_path:wrong_resource_size expr : > instance_path:node_network_transmit_gigabits_per_second:rate1m:avg_plus_stddev_over_time_2w > 3 labels : type : EC2 metric : NetworkTransmit problem : undersized The difference with network is that we don't have a percent of the total instance bandwidth, In my case, my instances support from 0.5 to 5 Gbps which is more than I need, so most of my instances are marked as oversized with the < 0.5 rule. I will manually study the ones that go over 3 Gbps. The correct way to do it, is to tag the baseline, burst or/and maximum network performance by instance type. In the AWS case, the data can be extracted using the AWS docs or external benchmarks . Once you know the network performance per instance type, you can use relabeling in the Node exporter service monitor to add a label like max_network_performance and use it later in the rules. If you do follow this path, please contact me or do a pull request so I can test your solution.","title":"Network analysis"},{"location":"devops/prometheus/instance_sizing_analysis/#overall-analysis","text":"Now that we have all the analysis under the metric instance_path:wrong_resource_size with labels, we can aggregate them to see the number of rules each instance is breaking with the following rule: # Mark the number of oversize rules matched by each instance - record : instance_path:wrong_instance_size expr : count by (instance) (sum by (metric, instance) (instance_path:wrong_resource_size)) By executing sort_desc(instance_path:wrong_instance_size) in the Prometheus web application, we'll be able to see such instances. instance_path:wrong_instance_size{instance=\"frontend-production:192.168.1.2\"} 4 instance_path:wrong_instance_size{instance=\"backend-production-instance:172.30.0.195\"} 2 ... To see the detail of what rules is our instance breaking we can use something like instance_path:wrong_resource_size{instance =~'frontend.*'} instance_path:wrong_resource_size{instance=\"fronted-production:192.168.1.2\",instance_type=\"c4.2xlarge\",job=\"node-exporter\",metric=\"RAM\",problem=\"oversized\",type=\"EC2\"} 5.126602454544287 instance_path:wrong_resource_size{instance=\"fronted-production:192.168.1.2\",metric=\"CPU\",problem=\"oversized\",type=\"EC2\"} 0.815639209497615 instance_path:wrong_resource_size{device=\"ens3\",instance=\"fronted-production:192.168.1.2\",instance_type=\"c4.2xlarge\",job=\"node-exporter\",metric=\"NetworkReceive\",problem=\"oversized\",type=\"EC2\"} 0.02973250128744766 instance_path:wrong_resource_size{device=\"ens3\",instance=\"fronted-production:192.168.1.2\",instance_type=\"c4.2xlarge\",job=\"node-exporter\",metric=\"NetworkTransmit\",problem=\"oversized\",type=\"EC2\"} 0.01586461503849804 Here we see that the frontend-production is a c4.2xlarge instance that is consuming an average plus standard deviation of CPU of 0.81%, RAM 5.12%, NetworkTransmit 0.015Gbps and NetworkReceive 0.029Gbps, which results in an oversized alert on all four metrics. If you want to see the evolution over the time, instead of Console click on Graph under the text box where you have entered the query. With this information, we can decide which is the correct instance for each application. Once all instances are migrated to their ideal size, we can add alerts on these metrics so we can have a continuous analysis of our instances. Once I've done it, I'll add the alerts here.","title":"Overall analysis"},{"location":"devops/prometheus/instance_sizing_analysis/#disclaimer","text":"We haven't tested this rules yet in production to resize our infrastructure (will do soon), so use all the information in this document cautiously. What I can expect to fail is that the assumption of average plus a standard deviation criteria can not be enough, maybe I need to increase the resolution of the standard deviation so it can be more sensible to the spikes, or we need to use a safety factor of 2 or 3. We'll see :) Read throughly the Gitlab post on anomaly detection using Prometheus , it's awesome and it may give you insights on why this approach is not working with you, as well as other algorithms that for example take into account the seasonality of the metrics. In particular it's interesting to analyze your resources z-score evolution over time, if all values fall in the +4 to -4 range, you can statistically assert that your metric similarly follows the normal distribution, and can assume that any value of z_score above 3 is an anomaly. If your results return with a range of +20 to -20 , the tail is too long and your results will be skewed. To test it you can use the following queries to test the RAM behaviour, adapt them for the rest of the resources: # Minimum z_score value sort_desc ( abs (( min_over_time ( instance_path : node_memory_MemAvailable_percent [ 1w ] ) - instance_path : node_memory_MemAvailable_percent : avg_over_time_2w ) / instance_path : node_memory_MemAvailable_percent : stddev_over_time_2w )) # Maximum z_score value sort_desc ( abs (( max_over_time ( instance_path : node_memory_MemAvailable_percent [ 1w ] ) - instance_path : node_memory_MemAvailable_percent : avg_over_time_2w ) / instance_path : node_memory_MemAvailable_percent : stddev_over_time_2w )) For a less exhaustive but more graphical analysis, execute instance_path:node_memory_MemAvailable_percent:z_score in Graph mode. In my case the RAM is in the +-5 interval, with some peaks of 20, but after reviewing instance_path:node_memory_MemAvailable_percent:avg_plus_stddev_over_time_2w in those periods, I feel it's still safe to use the assumption. Same criteria applies to instance_path:node_cpu_percent:rate1m:z_score , instance_path:node_network_receive_gigabits_per_second:rate1m:z_score , and instance_path:node_network_transmit_gigabits_per_second:rate1m:z_score , metrics.","title":"Disclaimer"},{"location":"devops/prometheus/instance_sizing_analysis/#references","text":"Gitlab post on anomaly detection using Prometheus .","title":"References"},{"location":"devops/prometheus/node_exporter/","text":"Node Exporter is a Prometheus exporter for hardware and OS metrics exposed by *NIX kernels, written in Go with pluggable metric collectors. Install \u2691 To install in kubernetes nodes, use this chart . Elsewhere use this ansible role . If you use node exporter agents outside kubernetes, you need to configure a prometheus service discovery to scrap the information from them. To auto discover EC2 instances use the ec2_sd_config configuration. It can be added in the helm chart values.yaml under the key prometheus.prometheusSpec.additionalScrapeConfigs - job_name : node_exporter ec2_sd_configs : - region : us-east-1 port : 9100 refresh_interval : 1m relabel_configs : - source_labels : [ '__meta_ec2_tag_Name' , '__meta_ec2_private_ip' ] separator : ':' target_label : instance - source_labels : - __meta_ec2_instance_type target_label : instance_type The relabel_configs part will substitute the instance label of each target from {{ instance_ip }}:9100 to {{ instance_name }}:{{ instance_ip }} . If the worker nodes already have an IAM role with the ec2:DescribeInstances permission there is no need to specify the role_arn or access_keys and secret_key . If you have stopped instances, the node exporter will raise an alert because it won't be able to scrape the metrics from them. To only fetch data from running instances add a filter: ec2_sd_configs : - region : us-east-1 filters : - name : instance-state-name values : - running To monitor only the instances of a list of VPCs use this filter: ec2_sd_configs : - region : us-east-1 filters : - name : vpc-id values : - vpc-xxxxxxxxxxxxxxxxx - vpc-yyyyyyyyyyyyyyyyy By default, prometheus will try to scrape the private instance ip. To use the public one you need to relabel it with the following snippet: ec2_sd_configs : - region : us-east-1 relabel_configs : - source_labels : [ '__meta_ec2_public_ip' ] regex : ^(.*)$ target_label : __address__ replacement : ${1}:9100 I'm using the 11074 grafana dashboards for the blackbox exporter, which worked straight out of the box. Taking as reference the grafana helm chart values, add the following yaml under the grafana key in the prometheus-operator values.yaml . grafana : enabled : true defaultDashboardsEnabled : true dashboardProviders : dashboardproviders.yaml : apiVersion : 1 providers : - name : 'default' orgId : 1 folder : '' type : file disableDeletion : false editable : true options : path : /var/lib/grafana/dashboards/default dashboards : default : node_exporter : # Ref: https://grafana.com/dashboards/11074 gnetId : 11074 revision : 4 datasource : Prometheus And install. helmfile diff helmfile apply Node exporter size analysis \u2691 Once the instance metrics are being ingested, we can do a periodic analysis to deduce which instances are undersized or oversized. Node exporter alerts \u2691 Now that we've got the metrics, we can define the alert rules . Most have been tweaked from the Awesome prometheus alert rules collection. Host out of memory \u2691 Node memory is filling up ( < 10% left). - alert : HostOutOfMemory expr : node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes * 100 < 10 for : 5m labels : severity : warning annotations : summary : \"Host out of memory (instance {{ $labels.instance }})\" message : \"Node memory is filling up (< 10% left)\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\" Host memory under memory pressure \u2691 The node is under heavy memory pressure. High rate of major page faults. - alert : HostMemoryUnderMemoryPressure expr : rate(node_vmstat_pgmajfault[1m]) > 1000 for : 5m labels : severity : warning annotations : summary : \"Host memory under memory pressure (instance {{ $labels.instance }})\" message : \"The node is under heavy memory pressure. High rate of major page faults.\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\" Host unusual network throughput in \u2691 Host network interfaces are probably receiving too much data (> 100 MB/s) - alert : HostUnusualNetworkThroughputIn expr : sum by (instance) (irate(node_network_receive_bytes_total[2m])) / 1024 / 1024 > 100 for : 5m labels : severity : warning annotations : summary : \"Host unusual network throughput in (instance {{ $labels.instance }})\" message : \"Host network interfaces are probably receiving too much data (> 100 MB/s)\\n VALUE = {{ $value }}.\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\" Host unusual network throughput out \u2691 Host network interfaces are probably sending too much data (> 100 MB/s) - alert : HostUnusualNetworkThroughputOut expr : sum by (instance) (irate(node_network_transmit_bytes_total[2m])) / 1024 / 1024 > 100 for : 5m labels : severity : warning annotations : summary : \"Host unusual network throughput out (instance {{ $labels.instance }})\" message : \"Host network interfaces are probably sending too much data (> 100 MB/s)\\n VALUE = {{ $value }}.\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\" Host unusual disk read rate \u2691 Disk is probably reading too much data (> 50 MB/s) - alert : HostUnusualDiskReadRate expr : sum by (instance) (irate(node_disk_read_bytes_total[2m])) / 1024 / 1024 > 50 for : 5m labels : severity : warning annotations : summary : \"Host unusual disk read rate (instance {{ $labels.instance }})\" message : \"Disk is probably reading too much data (> 50 MB/s)\\n VALUE = {{ $value }}.\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\" Host unusual disk write rate \u2691 Disk is probably writing too much data (> 50 MB/s) - alert : HostUnusualDiskWriteRate expr : sum by (instance) (irate(node_disk_written_bytes_total[2m])) / 1024 / 1024 > 50 for : 5m labels : severity : warning annotations : summary : \"Host unusual disk write rate (instance {{ $labels.instance }})\" message : \"Disk is probably writing too much data (> 50 MB/s)\\n VALUE = {{ $value }}.\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\" Host out of disk space \u2691 Disk is worryingly almost full ( < 10% left ). - alert : HostOutOfDiskSpace expr : (node_filesystem_avail_bytes{fstype!~\"tmpfs\"} * 100) / node_filesystem_size_bytes{fstype!~\"tmpfs\"} < 10 for : 5m labels : severity : critical annotations : summary : \"Host out of disk space (instance {{ $labels.instance }})\" message : \"Host disk is almost full (< 10% left)\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\" Disk is almost full ( < 20% left ) - alert : HostReachingOutOfDiskSpace expr : (node_filesystem_avail_bytes{fstype!~\"tmpfs\"} * 100) / node_filesystem_size_bytes{fstype!~\"tmpfs\"} < 20 for : 5m labels : severity : warning annotations : summary : \"Host reaching out of disk space (instance {{ $labels.instance }})\" message : \"Host disk is almost full (< 20% left)\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\" Host disk will fill in 4 hours \u2691 Disk will fill in 4 hours at current write rate - alert : HostDiskWillFillIn4Hours expr : predict_linear(node_filesystem_free_bytes{fstype!~\"tmpfs\"}[1h], 4 * 3600) < 0 for : 5m labels : severity : critical annotations : summary : \"Host disk will fill in 4 hours (instance {{ $labels.instance }})\" message : \"Disk will fill in 4 hours at current write rate\\n VALUE = {{ $value }}.\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\" Host out of inodes \u2691 Disk is almost running out of available inodes ( < 10% left ). - alert : HostOutOfInodes expr : node_filesystem_files_free{fstype!~\"tmpfs\"} / node_filesystem_files{fstype!~\"tmpfs\"} * 100 < 10 for : 5m labels : severity : warning annotations : summary : \"Host out of inodes (instance {{ $labels.instance }})\" message : \"Disk is almost running out of available inodes (< 10% left)\\n VALUE = {{ $value }}.\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\" Host unusual disk read latency \u2691 Disk latency is growing (read operations > 100ms). - alert : HostUnusualDiskReadLatency expr : rate(node_disk_read_time_seconds_total[1m]) / rate(node_disk_reads_completed_total[1m]) > 100 for : 5m labels : severity : warning annotations : summary : \"Host unusual disk read latency (instance {{ $labels.instance }})\" message : \"Disk latency is growing (read operations > 100ms)\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\" Host unusual disk write latency \u2691 Disk latency is growing (write operations > 100ms) - alert : HostUnusualDiskWriteLatency expr : rate(node_disk_write_time_seconds_total[1m]) / rate(node_disk_writes_completed_total[1m]) > 100 for : 5m labels : severity : warning annotations : summary : \"Host unusual disk write latency (instance {{ $labels.instance }})\" message : \"Disk latency is growing (write operations > 100ms)\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\" Host high CPU load \u2691 CPU load is > 80% - alert : HostHighCpuLoad expr : 100 - (avg by(instance) (irate(node_cpu_seconds_total{mode=\"idle\"}[5m])) * 100) > 80 for : 5m labels : severity : warning annotations : summary : \"Host high CPU load (instance {{ $labels.instance }})\" message : \"CPU load is > 80%\\n VALUE = {{ $value }}.\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\" Host context switching \u2691 Context switching is growing on node (> 1000 / s) # 1000 context switches is an arbitrary number. # Alert threshold depends on nature of application. # Please read: https://github.com/samber/awesome-prometheus-alerts/issues/58 - alert : HostContextSwitching expr : (rate(node_context_switches_total[5m])) / (count without(cpu, mode) (node_cpu_seconds_total{mode=\"idle\"})) > 1000 for : 5m labels : severity : warning annotations : summary : \"Host context switching (instance {{ $labels.instance }})\" message : \"Context switching is growing on node (> 1000 / s)\\n VALUE = {{ $value }}.\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\" Host swap is filling up \u2691 Swap is filling up (>80%) - alert : HostSwapIsFillingUp expr : (1 - (node_memory_SwapFree_bytes / node_memory_SwapTotal_bytes)) * 100 > 80 for : 5m labels : severity : warning annotations : summary : \"Host swap is filling up (instance {{ $labels.instance }})\" message : \"Swap is filling up (>80%)\\n VALUE = {{ $value }}.\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\" Host SystemD service crashed \u2691 SystemD service crashed - alert : HostSystemdServiceCrashed expr : node_systemd_unit_state{state=\"failed\"} == 1 for : 5m labels : severity : warning annotations : summary : \"Host SystemD service crashed (instance {{ $labels.instance }})\" message : \"SystemD service crashed\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\" Host physical component too hot \u2691 Physical hardware component too hot - alert : HostPhysicalComponentTooHot expr : node_hwmon_temp_celsius > 75 for : 5m labels : severity : warning annotations : summary : \"Host physical component too hot (instance {{ $labels.instance }})\" message : \"Physical hardware component too hot\\n VALUE = {{ $value }}.\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\" Host node overtemperature alarm \u2691 Physical node temperature alarm triggered - alert : HostNodeOvertemperatureAlarm expr : node_hwmon_temp_alarm == 1 for : 5m labels : severity : critical annotations : summary : \"Host node overtemperature alarm (instance {{ $labels.instance }})\" message : \"Physical node temperature alarm triggered\\n VALUE = {{ $value }}.\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\" Host RAID array got inactive \u2691 RAID array {{ $labels.device }} is in degraded state due to one or more disks failures. Number of spare drives is insufficient to fix issue automatically. - alert : HostRaidArrayGotInactive expr : node_md_state{state=\"inactive\"} > 0 for : 5m labels : severity : critical annotations : summary : \"Host RAID array got inactive (instance {{ $labels.instance }})\" message : \"RAID array {{ $labels.device }} is in degraded state due to one or more disks failures. Number of spare drives is insufficient to fix issue automatically.\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\" Host RAID disk failure \u2691 At least one device in RAID array on {{ $labels.instance }} failed. Array {{ $labels.md_device }} needs attention and possibly a disk swap. - alert : HostRaidDiskFailure expr : node_md_disks{state=\"fail\"} > 0 for : 5m labels : severity : warning annotations : summary : \"Host RAID disk failure (instance {{ $labels.instance }})\" message : \"At least one device in RAID array on {{ $labels.instance }} failed. Array {{ $labels.md_device }} needs attention and possibly a disk swap\\n VALUE = {{ $value }}.\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\" Host kernel version deviations \u2691 Different kernel versions are running. - alert : HostKernelVersionDeviations expr : count(sum(label_replace(node_uname_info, \"kernel\", \"$1\", \"release\", \"([0-9]+.[0-9]+.[0-9]+).*\")) by (kernel)) > 1 for : 5m labels : severity : warning annotations : summary : \"Host kernel version deviations (instance {{ $labels.instance }})\" message : \"Different kernel versions are running\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\" Host OOM kill detected \u2691 OOM kill detected - alert : HostOomKillDetected expr : increase(node_vmstat_oom_kill[5m]) > 0 for : 5m labels : severity : warning annotations : summary : \"Host OOM kill detected (instance {{ $labels.instance }})\" message : \"OOM kill detected\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\" Host Network Receive Errors \u2691 {{ $labels.instance }} interface {{ $labels.device }} has encountered {{ printf \"%.0f\" $value }} receive errors in the last five minutes. - alert : HostNetworkReceiveErrors expr : increase(node_network_receive_errs_total[5m]) > 0 for : 5m labels : severity : warning annotations : summary : \"Host Network Receive Errors (instance {{ $labels.instance }})\" message : \"{{ $labels.instance }} interface {{ $labels.device }} has encountered {{ printf '%.0f' $value }} receive errors in the last five minutes.\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\" Host Network Transmit Errors \u2691 {{ $labels.instance }} interface {{ $labels.device }} has encountered {{ printf \"%.0f\" $value }} transmit errors in the last five minutes. - alert : HostNetworkTransmitErrors expr : increase(node_network_transmit_errs_total[5m]) > 0 for : 5m labels : severity : warning annotations : summary : \"Host Network Transmit Errors (instance {{ $labels.instance }})\" message : \"{{ $labels.instance }} interface {{ $labels.device }} has encountered {{ printf '%.0f' $value }} transmit errors in the last five minutes.\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\" References \u2691 Git Prometheus node exporter guide Node exporter alerts","title":"Node Exporter"},{"location":"devops/prometheus/node_exporter/#install","text":"To install in kubernetes nodes, use this chart . Elsewhere use this ansible role . If you use node exporter agents outside kubernetes, you need to configure a prometheus service discovery to scrap the information from them. To auto discover EC2 instances use the ec2_sd_config configuration. It can be added in the helm chart values.yaml under the key prometheus.prometheusSpec.additionalScrapeConfigs - job_name : node_exporter ec2_sd_configs : - region : us-east-1 port : 9100 refresh_interval : 1m relabel_configs : - source_labels : [ '__meta_ec2_tag_Name' , '__meta_ec2_private_ip' ] separator : ':' target_label : instance - source_labels : - __meta_ec2_instance_type target_label : instance_type The relabel_configs part will substitute the instance label of each target from {{ instance_ip }}:9100 to {{ instance_name }}:{{ instance_ip }} . If the worker nodes already have an IAM role with the ec2:DescribeInstances permission there is no need to specify the role_arn or access_keys and secret_key . If you have stopped instances, the node exporter will raise an alert because it won't be able to scrape the metrics from them. To only fetch data from running instances add a filter: ec2_sd_configs : - region : us-east-1 filters : - name : instance-state-name values : - running To monitor only the instances of a list of VPCs use this filter: ec2_sd_configs : - region : us-east-1 filters : - name : vpc-id values : - vpc-xxxxxxxxxxxxxxxxx - vpc-yyyyyyyyyyyyyyyyy By default, prometheus will try to scrape the private instance ip. To use the public one you need to relabel it with the following snippet: ec2_sd_configs : - region : us-east-1 relabel_configs : - source_labels : [ '__meta_ec2_public_ip' ] regex : ^(.*)$ target_label : __address__ replacement : ${1}:9100 I'm using the 11074 grafana dashboards for the blackbox exporter, which worked straight out of the box. Taking as reference the grafana helm chart values, add the following yaml under the grafana key in the prometheus-operator values.yaml . grafana : enabled : true defaultDashboardsEnabled : true dashboardProviders : dashboardproviders.yaml : apiVersion : 1 providers : - name : 'default' orgId : 1 folder : '' type : file disableDeletion : false editable : true options : path : /var/lib/grafana/dashboards/default dashboards : default : node_exporter : # Ref: https://grafana.com/dashboards/11074 gnetId : 11074 revision : 4 datasource : Prometheus And install. helmfile diff helmfile apply","title":"Install"},{"location":"devops/prometheus/node_exporter/#node-exporter-size-analysis","text":"Once the instance metrics are being ingested, we can do a periodic analysis to deduce which instances are undersized or oversized.","title":"Node exporter size analysis"},{"location":"devops/prometheus/node_exporter/#node-exporter-alerts","text":"Now that we've got the metrics, we can define the alert rules . Most have been tweaked from the Awesome prometheus alert rules collection.","title":"Node exporter alerts"},{"location":"devops/prometheus/node_exporter/#host-out-of-memory","text":"Node memory is filling up ( < 10% left). - alert : HostOutOfMemory expr : node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes * 100 < 10 for : 5m labels : severity : warning annotations : summary : \"Host out of memory (instance {{ $labels.instance }})\" message : \"Node memory is filling up (< 10% left)\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\"","title":"Host out of memory"},{"location":"devops/prometheus/node_exporter/#host-memory-under-memory-pressure","text":"The node is under heavy memory pressure. High rate of major page faults. - alert : HostMemoryUnderMemoryPressure expr : rate(node_vmstat_pgmajfault[1m]) > 1000 for : 5m labels : severity : warning annotations : summary : \"Host memory under memory pressure (instance {{ $labels.instance }})\" message : \"The node is under heavy memory pressure. High rate of major page faults.\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\"","title":"Host memory under memory pressure"},{"location":"devops/prometheus/node_exporter/#host-unusual-network-throughput-in","text":"Host network interfaces are probably receiving too much data (> 100 MB/s) - alert : HostUnusualNetworkThroughputIn expr : sum by (instance) (irate(node_network_receive_bytes_total[2m])) / 1024 / 1024 > 100 for : 5m labels : severity : warning annotations : summary : \"Host unusual network throughput in (instance {{ $labels.instance }})\" message : \"Host network interfaces are probably receiving too much data (> 100 MB/s)\\n VALUE = {{ $value }}.\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\"","title":"Host unusual network throughput in"},{"location":"devops/prometheus/node_exporter/#host-unusual-network-throughput-out","text":"Host network interfaces are probably sending too much data (> 100 MB/s) - alert : HostUnusualNetworkThroughputOut expr : sum by (instance) (irate(node_network_transmit_bytes_total[2m])) / 1024 / 1024 > 100 for : 5m labels : severity : warning annotations : summary : \"Host unusual network throughput out (instance {{ $labels.instance }})\" message : \"Host network interfaces are probably sending too much data (> 100 MB/s)\\n VALUE = {{ $value }}.\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\"","title":"Host unusual network throughput out"},{"location":"devops/prometheus/node_exporter/#host-unusual-disk-read-rate","text":"Disk is probably reading too much data (> 50 MB/s) - alert : HostUnusualDiskReadRate expr : sum by (instance) (irate(node_disk_read_bytes_total[2m])) / 1024 / 1024 > 50 for : 5m labels : severity : warning annotations : summary : \"Host unusual disk read rate (instance {{ $labels.instance }})\" message : \"Disk is probably reading too much data (> 50 MB/s)\\n VALUE = {{ $value }}.\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\"","title":"Host unusual disk read rate"},{"location":"devops/prometheus/node_exporter/#host-unusual-disk-write-rate","text":"Disk is probably writing too much data (> 50 MB/s) - alert : HostUnusualDiskWriteRate expr : sum by (instance) (irate(node_disk_written_bytes_total[2m])) / 1024 / 1024 > 50 for : 5m labels : severity : warning annotations : summary : \"Host unusual disk write rate (instance {{ $labels.instance }})\" message : \"Disk is probably writing too much data (> 50 MB/s)\\n VALUE = {{ $value }}.\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\"","title":"Host unusual disk write rate"},{"location":"devops/prometheus/node_exporter/#host-out-of-disk-space","text":"Disk is worryingly almost full ( < 10% left ). - alert : HostOutOfDiskSpace expr : (node_filesystem_avail_bytes{fstype!~\"tmpfs\"} * 100) / node_filesystem_size_bytes{fstype!~\"tmpfs\"} < 10 for : 5m labels : severity : critical annotations : summary : \"Host out of disk space (instance {{ $labels.instance }})\" message : \"Host disk is almost full (< 10% left)\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\" Disk is almost full ( < 20% left ) - alert : HostReachingOutOfDiskSpace expr : (node_filesystem_avail_bytes{fstype!~\"tmpfs\"} * 100) / node_filesystem_size_bytes{fstype!~\"tmpfs\"} < 20 for : 5m labels : severity : warning annotations : summary : \"Host reaching out of disk space (instance {{ $labels.instance }})\" message : \"Host disk is almost full (< 20% left)\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\"","title":"Host out of disk space"},{"location":"devops/prometheus/node_exporter/#host-disk-will-fill-in-4-hours","text":"Disk will fill in 4 hours at current write rate - alert : HostDiskWillFillIn4Hours expr : predict_linear(node_filesystem_free_bytes{fstype!~\"tmpfs\"}[1h], 4 * 3600) < 0 for : 5m labels : severity : critical annotations : summary : \"Host disk will fill in 4 hours (instance {{ $labels.instance }})\" message : \"Disk will fill in 4 hours at current write rate\\n VALUE = {{ $value }}.\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\"","title":"Host disk will fill in 4 hours"},{"location":"devops/prometheus/node_exporter/#host-out-of-inodes","text":"Disk is almost running out of available inodes ( < 10% left ). - alert : HostOutOfInodes expr : node_filesystem_files_free{fstype!~\"tmpfs\"} / node_filesystem_files{fstype!~\"tmpfs\"} * 100 < 10 for : 5m labels : severity : warning annotations : summary : \"Host out of inodes (instance {{ $labels.instance }})\" message : \"Disk is almost running out of available inodes (< 10% left)\\n VALUE = {{ $value }}.\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\"","title":"Host out of inodes"},{"location":"devops/prometheus/node_exporter/#host-unusual-disk-read-latency","text":"Disk latency is growing (read operations > 100ms). - alert : HostUnusualDiskReadLatency expr : rate(node_disk_read_time_seconds_total[1m]) / rate(node_disk_reads_completed_total[1m]) > 100 for : 5m labels : severity : warning annotations : summary : \"Host unusual disk read latency (instance {{ $labels.instance }})\" message : \"Disk latency is growing (read operations > 100ms)\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\"","title":"Host unusual disk read latency"},{"location":"devops/prometheus/node_exporter/#host-unusual-disk-write-latency","text":"Disk latency is growing (write operations > 100ms) - alert : HostUnusualDiskWriteLatency expr : rate(node_disk_write_time_seconds_total[1m]) / rate(node_disk_writes_completed_total[1m]) > 100 for : 5m labels : severity : warning annotations : summary : \"Host unusual disk write latency (instance {{ $labels.instance }})\" message : \"Disk latency is growing (write operations > 100ms)\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\"","title":"Host unusual disk write latency"},{"location":"devops/prometheus/node_exporter/#host-high-cpu-load","text":"CPU load is > 80% - alert : HostHighCpuLoad expr : 100 - (avg by(instance) (irate(node_cpu_seconds_total{mode=\"idle\"}[5m])) * 100) > 80 for : 5m labels : severity : warning annotations : summary : \"Host high CPU load (instance {{ $labels.instance }})\" message : \"CPU load is > 80%\\n VALUE = {{ $value }}.\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\"","title":"Host high CPU load"},{"location":"devops/prometheus/node_exporter/#host-context-switching","text":"Context switching is growing on node (> 1000 / s) # 1000 context switches is an arbitrary number. # Alert threshold depends on nature of application. # Please read: https://github.com/samber/awesome-prometheus-alerts/issues/58 - alert : HostContextSwitching expr : (rate(node_context_switches_total[5m])) / (count without(cpu, mode) (node_cpu_seconds_total{mode=\"idle\"})) > 1000 for : 5m labels : severity : warning annotations : summary : \"Host context switching (instance {{ $labels.instance }})\" message : \"Context switching is growing on node (> 1000 / s)\\n VALUE = {{ $value }}.\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\"","title":"Host context switching"},{"location":"devops/prometheus/node_exporter/#host-swap-is-filling-up","text":"Swap is filling up (>80%) - alert : HostSwapIsFillingUp expr : (1 - (node_memory_SwapFree_bytes / node_memory_SwapTotal_bytes)) * 100 > 80 for : 5m labels : severity : warning annotations : summary : \"Host swap is filling up (instance {{ $labels.instance }})\" message : \"Swap is filling up (>80%)\\n VALUE = {{ $value }}.\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\"","title":"Host swap is filling up"},{"location":"devops/prometheus/node_exporter/#host-systemd-service-crashed","text":"SystemD service crashed - alert : HostSystemdServiceCrashed expr : node_systemd_unit_state{state=\"failed\"} == 1 for : 5m labels : severity : warning annotations : summary : \"Host SystemD service crashed (instance {{ $labels.instance }})\" message : \"SystemD service crashed\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\"","title":"Host SystemD service crashed"},{"location":"devops/prometheus/node_exporter/#host-physical-component-too-hot","text":"Physical hardware component too hot - alert : HostPhysicalComponentTooHot expr : node_hwmon_temp_celsius > 75 for : 5m labels : severity : warning annotations : summary : \"Host physical component too hot (instance {{ $labels.instance }})\" message : \"Physical hardware component too hot\\n VALUE = {{ $value }}.\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\"","title":"Host physical component too hot"},{"location":"devops/prometheus/node_exporter/#host-node-overtemperature-alarm","text":"Physical node temperature alarm triggered - alert : HostNodeOvertemperatureAlarm expr : node_hwmon_temp_alarm == 1 for : 5m labels : severity : critical annotations : summary : \"Host node overtemperature alarm (instance {{ $labels.instance }})\" message : \"Physical node temperature alarm triggered\\n VALUE = {{ $value }}.\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\"","title":"Host node overtemperature alarm"},{"location":"devops/prometheus/node_exporter/#host-raid-array-got-inactive","text":"RAID array {{ $labels.device }} is in degraded state due to one or more disks failures. Number of spare drives is insufficient to fix issue automatically. - alert : HostRaidArrayGotInactive expr : node_md_state{state=\"inactive\"} > 0 for : 5m labels : severity : critical annotations : summary : \"Host RAID array got inactive (instance {{ $labels.instance }})\" message : \"RAID array {{ $labels.device }} is in degraded state due to one or more disks failures. Number of spare drives is insufficient to fix issue automatically.\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\"","title":"Host RAID array got inactive"},{"location":"devops/prometheus/node_exporter/#host-raid-disk-failure","text":"At least one device in RAID array on {{ $labels.instance }} failed. Array {{ $labels.md_device }} needs attention and possibly a disk swap. - alert : HostRaidDiskFailure expr : node_md_disks{state=\"fail\"} > 0 for : 5m labels : severity : warning annotations : summary : \"Host RAID disk failure (instance {{ $labels.instance }})\" message : \"At least one device in RAID array on {{ $labels.instance }} failed. Array {{ $labels.md_device }} needs attention and possibly a disk swap\\n VALUE = {{ $value }}.\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\"","title":"Host RAID disk failure"},{"location":"devops/prometheus/node_exporter/#host-kernel-version-deviations","text":"Different kernel versions are running. - alert : HostKernelVersionDeviations expr : count(sum(label_replace(node_uname_info, \"kernel\", \"$1\", \"release\", \"([0-9]+.[0-9]+.[0-9]+).*\")) by (kernel)) > 1 for : 5m labels : severity : warning annotations : summary : \"Host kernel version deviations (instance {{ $labels.instance }})\" message : \"Different kernel versions are running\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\"","title":"Host kernel version deviations"},{"location":"devops/prometheus/node_exporter/#host-oom-kill-detected","text":"OOM kill detected - alert : HostOomKillDetected expr : increase(node_vmstat_oom_kill[5m]) > 0 for : 5m labels : severity : warning annotations : summary : \"Host OOM kill detected (instance {{ $labels.instance }})\" message : \"OOM kill detected\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\"","title":"Host OOM kill detected"},{"location":"devops/prometheus/node_exporter/#host-network-receive-errors","text":"{{ $labels.instance }} interface {{ $labels.device }} has encountered {{ printf \"%.0f\" $value }} receive errors in the last five minutes. - alert : HostNetworkReceiveErrors expr : increase(node_network_receive_errs_total[5m]) > 0 for : 5m labels : severity : warning annotations : summary : \"Host Network Receive Errors (instance {{ $labels.instance }})\" message : \"{{ $labels.instance }} interface {{ $labels.device }} has encountered {{ printf '%.0f' $value }} receive errors in the last five minutes.\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\"","title":"Host Network Receive Errors"},{"location":"devops/prometheus/node_exporter/#host-network-transmit-errors","text":"{{ $labels.instance }} interface {{ $labels.device }} has encountered {{ printf \"%.0f\" $value }} transmit errors in the last five minutes. - alert : HostNetworkTransmitErrors expr : increase(node_network_transmit_errs_total[5m]) > 0 for : 5m labels : severity : warning annotations : summary : \"Host Network Transmit Errors (instance {{ $labels.instance }})\" message : \"{{ $labels.instance }} interface {{ $labels.device }} has encountered {{ printf '%.0f' $value }} transmit errors in the last five minutes.\\n VALUE = {{ $value }}\" grafana : \"{{ grafana_url}}?var-job=node_exporter&var-hostname=All&var-node={{ $labels.instance }}\"","title":"Host Network Transmit Errors"},{"location":"devops/prometheus/node_exporter/#references","text":"Git Prometheus node exporter guide Node exporter alerts","title":"References"},{"location":"devops/prometheus/prometheus/","text":"Prometheus is a free software application used for event monitoring and alerting. It records real-time metrics in a time series database (allowing for high dimensionality) built using a HTTP pull model, with flexible queries and real-time alerting. The project is written in Go and licensed under the Apache 2 License, with source code available on GitHub, and is a graduated project of the Cloud Native Computing Foundation, along with Kubernetes and Envoy. A quick overview of Prometheus would be, as stated in the coreos article : At the core of the Prometheus monitoring system is the main server, which ingests samples from monitoring targets. A target is any application that exposes metrics according to the open specification understood by Prometheus. Since Prometheus pulls data, rather than expecting targets to actively push stats into the monitoring system, it supports a variety of service discovery integrations, like that with Kubernetes, to immediately adapt to changes in the set of targets. The second core component is the Alertmanager, implementing the idea of time series based alerting. It intelligently removes duplicate alerts sent by Prometheus servers, groups the alerts into informative notifications, and dispatches them to a variety of integrations, like those with PagerDuty and Slack. It also handles silencing of selected alerts and advanced routing configurations for notifications. There are several additional Prometheus components, such as client libraries for different programming languages, and a growing number of exporters. Exporters are small programs that provide Prometheus compatible metrics from systems that are not natively instrumented. Go to the Prometheus architecture post for more details. We are living a shift to the DevOps culture, containers and Kubernetes. So nowadays: Developers need to integrate app and business related metrics as an organic part of the infrastructure. So monitoring needs to be democratized, made more accessible and cover additional layers of the stack. Container based infrastructures are changing how we monitor the resources. Now we have a huge number of volatile software entities, services, virtual network addresses, exposed metrics that suddenly appear or vanish. Traditional monitoring tools are not designed to handle this. These reasons pushed Soundcloud to build a new monitoring system that had the following features Multi-dimensional data model : The model is based on key-value pairs, similar to how Kubernetes itself organizes infrastructure metadata using labels. It allows for flexible and accurate time series data, powering its Prometheus query language. Accessible format and protocols : Exposing prometheus metrics is a pretty straightforward task. Metrics are human readable, are in a self-explanatory format, and are published using a standard HTTP transport. You can check that the metrics are correctly exposed just using your web browser. Service discovery : The Prometheus server is in charge of periodically scraping the targets, so that applications and services don\u2019t need to worry about emitting data (metrics are pulled, not pushed). These Prometheus servers have several methods to auto-discover scrape targets, some of them can be configured to filter and match container metadata, making it an excellent fit for ephemeral Kubernetes workloads. Modular and highly available components : Metric collection, alerting, graphical visualization, etc, are performed by different composable services. All these services are designed to support redundancy and sharding. Pull based metrics : Most monitoring systems are pushing metrics to a centralized collection platform. Prometheus flips this model on it's head with the following advantages: No need to install custom software in the physical servers or containers. Doesn't require applications to use CPU cycles pushing metrics. Handles service failure/unavailability gracefully. If a target goes down, Prometheus can record it was unable to retrieve data. You can use the Pushgateway if pulling metrics is not feasible. Installation \u2691 There are several ways to install prometheus , but I'd recommend using the Kubernetes or Docker Prometheus operator . Exposing your metrics \u2691 Prometheus defines a very nice text-based format for its metrics: # HELP prometheus_engine_query_duration_seconds Query timings # TYPE prometheus_engine_query_duration_seconds summary prometheus_engine_query_duration_seconds{slice=\"inner_eval\",quantile=\"0.5\"} 7.0442e-05 prometheus_engine_query_duration_seconds{slice=\"inner_eval\",quantile=\"0.9\"} 0.0084092 prometheus_engine_query_duration_seconds{slice=\"inner_eval\",quantile=\"0.99\"} 0.389403939 The data is relatively human readable and we even have TYPE and HELP decorators to increase the readability. To expose application metrics to the Prometheus server, use one of the client libraries and follow the suggested naming and units conventions for metrics . Metric types \u2691 There are these metric types: Counter : A simple monotonically incrementing type; basically use this for situations where you want to know \u201chow many times has x happened\u201d. Gauge : A representation of a metric that can go both up and down. Think of a speedometer in a car, this type provides a snapshot of \u201cwhat is the current value of x now\u201d. Histogram : It represents observed metrics sharded into distinct buckets. Think of this as a mechanism to track \u201chow long something took\u201d or \u201chow big something was\u201d. Summary : Similar to a histogram, except the bins are converted into an aggregate immediately. Using labels \u2691 Prometheus metrics support the concept of Labels to provide extra dimensions to your data. By using Labels efficiently we can essentially provide more insights into our data whilst having to manage less actual metrics. Prometheus rules \u2691 Prometheus supports two types of rules which may be configured and then evaluated at regular intervals: recording rules and alerting rules. Recording rules allow you to precompute frequently needed or computationally expensive expressions and save their result as a new set of time series. Querying the precomputed result will then often be much faster than executing the original expression every time it is needed. A simple example rules file would be: groups : - name : example rules : - record : job:http_inprogress_requests:sum expr : sum by (job) (http_inprogress_requests) Regarding naming and aggregation conventions , Recording rules should be of the general form level:metric:operations . level represents the aggregation level and labels of the rule output. metric is the metric name and should be unchanged other than stripping _total off counters when using rate() or irate() . operations is a list of operations (splitted by : ) that were applied to the metric, newest operation first. If you want to add extra labels to the calculated rule use the labels tag like the following example: groups : - name : example rules : - record : instance_path:wrong_resource_size expr : > instance_path:node_memory_MemAvailable_percent:avg_plus_stddev_over_time_2w < 60 labels : type : EC2 metric : RAM problem : oversized Finding a metric \u2691 You can use {__name__=~\".*deploy.*\"} to find the metrics that have deploy somewhere in the name. Links \u2691 Homepage . Docs . Awesome Prometheus . Prometheus rules best practices and configuration . Diving deeper \u2691 Architecture Prometheus Operator Prometheus Installation Blackbox Exporter Node Exporter Prometheus Troubleshooting Introduction posts \u2691 Soundcloud introduction . Sysdig guide . Prometheus monitoring solutions comparison . ITNEXT overview Books \u2691 Prometheus Up & Running . Monitoring With Prometheus .","title":"Prometheus"},{"location":"devops/prometheus/prometheus/#installation","text":"There are several ways to install prometheus , but I'd recommend using the Kubernetes or Docker Prometheus operator .","title":"Installation"},{"location":"devops/prometheus/prometheus/#exposing-your-metrics","text":"Prometheus defines a very nice text-based format for its metrics: # HELP prometheus_engine_query_duration_seconds Query timings # TYPE prometheus_engine_query_duration_seconds summary prometheus_engine_query_duration_seconds{slice=\"inner_eval\",quantile=\"0.5\"} 7.0442e-05 prometheus_engine_query_duration_seconds{slice=\"inner_eval\",quantile=\"0.9\"} 0.0084092 prometheus_engine_query_duration_seconds{slice=\"inner_eval\",quantile=\"0.99\"} 0.389403939 The data is relatively human readable and we even have TYPE and HELP decorators to increase the readability. To expose application metrics to the Prometheus server, use one of the client libraries and follow the suggested naming and units conventions for metrics .","title":"Exposing your metrics"},{"location":"devops/prometheus/prometheus/#metric-types","text":"There are these metric types: Counter : A simple monotonically incrementing type; basically use this for situations where you want to know \u201chow many times has x happened\u201d. Gauge : A representation of a metric that can go both up and down. Think of a speedometer in a car, this type provides a snapshot of \u201cwhat is the current value of x now\u201d. Histogram : It represents observed metrics sharded into distinct buckets. Think of this as a mechanism to track \u201chow long something took\u201d or \u201chow big something was\u201d. Summary : Similar to a histogram, except the bins are converted into an aggregate immediately.","title":"Metric types"},{"location":"devops/prometheus/prometheus/#using-labels","text":"Prometheus metrics support the concept of Labels to provide extra dimensions to your data. By using Labels efficiently we can essentially provide more insights into our data whilst having to manage less actual metrics.","title":"Using labels"},{"location":"devops/prometheus/prometheus/#prometheus-rules","text":"Prometheus supports two types of rules which may be configured and then evaluated at regular intervals: recording rules and alerting rules. Recording rules allow you to precompute frequently needed or computationally expensive expressions and save their result as a new set of time series. Querying the precomputed result will then often be much faster than executing the original expression every time it is needed. A simple example rules file would be: groups : - name : example rules : - record : job:http_inprogress_requests:sum expr : sum by (job) (http_inprogress_requests) Regarding naming and aggregation conventions , Recording rules should be of the general form level:metric:operations . level represents the aggregation level and labels of the rule output. metric is the metric name and should be unchanged other than stripping _total off counters when using rate() or irate() . operations is a list of operations (splitted by : ) that were applied to the metric, newest operation first. If you want to add extra labels to the calculated rule use the labels tag like the following example: groups : - name : example rules : - record : instance_path:wrong_resource_size expr : > instance_path:node_memory_MemAvailable_percent:avg_plus_stddev_over_time_2w < 60 labels : type : EC2 metric : RAM problem : oversized","title":"Prometheus rules"},{"location":"devops/prometheus/prometheus/#finding-a-metric","text":"You can use {__name__=~\".*deploy.*\"} to find the metrics that have deploy somewhere in the name.","title":"Finding a metric"},{"location":"devops/prometheus/prometheus/#links","text":"Homepage . Docs . Awesome Prometheus . Prometheus rules best practices and configuration .","title":"Links"},{"location":"devops/prometheus/prometheus/#diving-deeper","text":"Architecture Prometheus Operator Prometheus Installation Blackbox Exporter Node Exporter Prometheus Troubleshooting","title":"Diving deeper"},{"location":"devops/prometheus/prometheus/#introduction-posts","text":"Soundcloud introduction . Sysdig guide . Prometheus monitoring solutions comparison . ITNEXT overview","title":"Introduction posts"},{"location":"devops/prometheus/prometheus/#books","text":"Prometheus Up & Running . Monitoring With Prometheus .","title":"Books"},{"location":"devops/prometheus/prometheus_architecture/","text":"Prometheus Server \u2691 Prometheus servers have the following assignments: Periodically scrape and store metrics from instrumented jobs, either directly or via an intermediary push gateway for short-lived jobs. Run rules over scraped data to either record new timeseries from existing data or generate alerts. Discovers new targets from the Service discovery. Push alerts to the Alertmanager. Executes PromQL queries. Prometheus Targets \u2691 Prometheus Targets define how does prometheus extract the metrics from the different sources. If the services expose the metrics themselves such as Kubernetes , Prometheus fetch them directly. On the other cases, exporters are used. Exporters are modules that extract information and translate it into the Prometheus format, which the server can then ingest. There are several exporters available, for example: Hardware: Node/system HTTP: HAProxy , NGINX , Apache . APIs: Github , Docker Hub . Other monitoring systems: Cloudwatch . Databases: MySQL , Elasticsearch . Messaging systems: RabbitMQ , Kafka . Miscellaneous: Blackbox , JMX . Pushgateway \u2691 In case the nodes are not exposing an endpoint from which the Prometheus server can collect the metrics, the Prometheus ecosystem has a push gateway. This gateway API is useful for one-off jobs that run, capture the data, transform that data into the Prometheus data format and then push that data into the Prometheus server. Service discovery \u2691 Prometheus is designed to require very little configuration when first setup, and was designed from the ground up to run in dynamic environments such Kubernetes. It therefore performs automatic discovery of services running to try and make a \u201cbest guess\u201d of what it should be monitoring. Alertmanager \u2691 The Alertmanager handles alerts sent by client applications such as the Prometheus server. It takes care of deduplicating, grouping, and routing them to the correct receiver integrations such as email, PagerDuty, or OpsGenie. It also takes care of silencing and inhibition of alerts. Data visualization and export \u2691 There are several ways to visualize or export data from Prometheus. Prometheus web UI \u2691 Prometheus comes with its own user interface that you can use to: Run PromQL queries. Check the Alertmanager rules. Check the configuration. Check the Targets. Check the service discovery. Grafana \u2691 Grafana is the best way to visually analyze the evolution of the metrics throughout time. API clients \u2691 Prometheus also exposes an API, so in case you are interested in writing your own clients, you can do that too. Links \u2691 Prometheus Overview Open Source for U architecture overview","title":"Architecture"},{"location":"devops/prometheus/prometheus_architecture/#prometheus-server","text":"Prometheus servers have the following assignments: Periodically scrape and store metrics from instrumented jobs, either directly or via an intermediary push gateway for short-lived jobs. Run rules over scraped data to either record new timeseries from existing data or generate alerts. Discovers new targets from the Service discovery. Push alerts to the Alertmanager. Executes PromQL queries.","title":"Prometheus Server"},{"location":"devops/prometheus/prometheus_architecture/#prometheus-targets","text":"Prometheus Targets define how does prometheus extract the metrics from the different sources. If the services expose the metrics themselves such as Kubernetes , Prometheus fetch them directly. On the other cases, exporters are used. Exporters are modules that extract information and translate it into the Prometheus format, which the server can then ingest. There are several exporters available, for example: Hardware: Node/system HTTP: HAProxy , NGINX , Apache . APIs: Github , Docker Hub . Other monitoring systems: Cloudwatch . Databases: MySQL , Elasticsearch . Messaging systems: RabbitMQ , Kafka . Miscellaneous: Blackbox , JMX .","title":"Prometheus Targets"},{"location":"devops/prometheus/prometheus_architecture/#pushgateway","text":"In case the nodes are not exposing an endpoint from which the Prometheus server can collect the metrics, the Prometheus ecosystem has a push gateway. This gateway API is useful for one-off jobs that run, capture the data, transform that data into the Prometheus data format and then push that data into the Prometheus server.","title":"Pushgateway"},{"location":"devops/prometheus/prometheus_architecture/#service-discovery","text":"Prometheus is designed to require very little configuration when first setup, and was designed from the ground up to run in dynamic environments such Kubernetes. It therefore performs automatic discovery of services running to try and make a \u201cbest guess\u201d of what it should be monitoring.","title":"Service discovery"},{"location":"devops/prometheus/prometheus_architecture/#alertmanager","text":"The Alertmanager handles alerts sent by client applications such as the Prometheus server. It takes care of deduplicating, grouping, and routing them to the correct receiver integrations such as email, PagerDuty, or OpsGenie. It also takes care of silencing and inhibition of alerts.","title":"Alertmanager"},{"location":"devops/prometheus/prometheus_architecture/#data-visualization-and-export","text":"There are several ways to visualize or export data from Prometheus.","title":"Data visualization and export"},{"location":"devops/prometheus/prometheus_architecture/#prometheus-web-ui","text":"Prometheus comes with its own user interface that you can use to: Run PromQL queries. Check the Alertmanager rules. Check the configuration. Check the Targets. Check the service discovery.","title":"Prometheus web UI"},{"location":"devops/prometheus/prometheus_architecture/#grafana","text":"Grafana is the best way to visually analyze the evolution of the metrics throughout time.","title":"Grafana"},{"location":"devops/prometheus/prometheus_architecture/#api-clients","text":"Prometheus also exposes an API, so in case you are interested in writing your own clients, you can do that too.","title":"API clients"},{"location":"devops/prometheus/prometheus_architecture/#links","text":"Prometheus Overview Open Source for U architecture overview","title":"Links"},{"location":"devops/prometheus/prometheus_installation/","text":"Kubernetes \u2691 Helm 2 is not supported anymore. Later versions of the chart return an Error: apiVersion 'v2' is not valid. The value must be \"v1\" when using helm 2. Diving deeper , it seems that from 11.1.7 support for helm 2 was dropped. To install the operator we'll use helmfile to install the stable/prometheus-operator chart . Add the following lines to your helmfile.yaml . - name : prometheus-operator namespace : monitoring chart : stable/prometheus-operator values : - prometheus-operator/values.yaml Edit the chart values. mkdir prometheus-operator helm inspect values stable/prometheus-operator > prometheus-operator/values.yaml vi prometheus-operator/values.yaml I've implemented the following changes: If you are using a managed solution like EKS, the provider will hide kube-scheduler and kube-controller-manager so those metrics will fail. Therefore you need to disable: defaultRules.rules.kubeScheduler: false . kubeScheduler.enabled: false . kubeControllerManager.enabled: false . Enabled the ingress of alertmanager , grafana and prometheus . Set up the storage of alertmanager and prometheus with storageClassName: gp2 (for AWS). Change additionalPrometheusRules to additionalPrometheusRulesMap as the former is going to be deprecated in future releases. For private clusters, disable the admission webhook . prometheusOperator.admissionWebhooks.enabled=false prometheusOperator.admissionWebhooks.patch.enabled=false prometheusOperator.tlsProxy.enabled=false And install. helmfile diff helmfile apply Once it's installed you can check everything is working by accessing the grafana dashboard. First of all get the pod name (we'll asume you've used the monitoring namespace). kubectl get pods -n monitoring | grep grafana Then set up the proxies kubectl port-forward {{ grafana_pod }} -n monitoring 3000 :3000 kubectl port-forward -n monitoring \\ prometheus-prometheus-operator-prometheus-0 9090 :9090 To access grafana, go to http://localhost:3000 through your browser and at the top left, click on Home and select any dashboard. To access prometheus, go to http://localhost:9090 . If you're using the EKS helm chart, you'll need to manually edit the kube-proxy-config configmap until this bug has been solved. Edit the 127.0.0.1 value to 0.0.0.0 for the key metricsBindAddress in kubectl -n kube-system edit cm kube-proxy-config And restart the DaemonSet: kubectl rollout restart -n kube-system daemonset.apps/kube-proxy Upgrading notes \u2691 10.x -> 11.1.7 \u2691 If you have a private cluster in EKS, you are not able to use the admission webhooks as the nodes are not able to reach the master. Between those versions, something changed and you need to disable tls too with: prometheusOperator : tls : enabled : false admissionWebhooks : enabled : false If you run helmfile apply without that flag, the deployment gets tainted, and you may need to edit the deployment to remove the tls-secret volume. Docker \u2691 To install it outside Kubernetes, use the cloudalchemy ansible role for host installations or the prom/prometheus docker with the following command: /usr/bin/docker run --rm \\ --name prometheus \\ -v /data/prometheus:/etc/prometheus \\ prom/prometheus:latest \\ --storage.tsdb.retention.time = 30d \\ --config.file = /etc/prometheus/prometheus.yml \\ With a basic prometheus configuration: File: /data/prometheus/prometheus.yml ```yaml \u2691 http://prometheus.io/docs/operating/configuration/ \u2691 global: evaluation_interval: 1m scrape_interval: 1m scrape_timeout: 10s external_labels: environment: helm rule_files: - /etc/prometheus/rules/*.rules scrape_configs: - job_name: prometheus metrics_path: /metrics static_configs: - targets: - prometheus:9090 ``` And some basic rules: File: /data/prometheus/rules/ groups : - name : ansible managed alert rules rules : - alert : Watchdog annotations : description : |- This is an alert meant to ensure that the entire alerting pipeline is functional. This alert is always firing, therefore it should always be firing in Alertmanager and always fire against a receiver. There are integrations with various notification mechanisms that send a notification when this alert is not firing. For example the \"DeadMansSnitch\" integration in PagerDuty. summary : Ensure entire alerting pipeline is functional expr : vector(1) for : 10m labels : severity : warning - alert : InstanceDown annotations : description : '{{ $labels.instance }} of job {{ $labels.job }} has been down for more than 5 minutes.' summary : Instance {{ $labels.instance }} down expr : up == 0 for : 5m labels : severity : critical - alert : RebootRequired annotations : description : '{{ $labels.instance }} requires a reboot.' summary : Instance {{ $labels.instance }} - reboot required expr : node_reboot_required > 0 labels : severity : warning - alert : NodeFilesystemSpaceFillingUp annotations : description : Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ printf \"%.2f\" $value }}% available space left and is filling up. summary : Filesystem is predicted to run out of space within the next 24 hours. expr : |- ( node_filesystem_avail_bytes{job=\"node\",fstype!=\"\"} / node_filesystem_size_bytes{job=\"node\",fstype!=\"\"} * 100 < 40 and predict_linear(node_filesystem_avail_bytes{job=\"node\",fstype!=\"\"}[6h], 24*60*60) < 0 and node_filesystem_readonly{job=\"node\",fstype!=\"\"} == 0 ) for : 1h labels : severity : warning - alert : NodeFilesystemSpaceFillingUp annotations : description : Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ printf \"%.2f\" $value }}% available space left and is filling up fast. summary : Filesystem is predicted to run out of space within the next 4 hours. expr : |- ( node_filesystem_avail_bytes{job=\"node\",fstype!=\"\"} / node_filesystem_size_bytes{job=\"node\",fstype!=\"\"} * 100 < 20 and predict_linear(node_filesystem_avail_bytes{job=\"node\",fstype!=\"\"}[6h], 4*60*60) < 0 and node_filesystem_readonly{job=\"node\",fstype!=\"\"} == 0 ) for : 1h labels : severity : critical - alert : NodeFilesystemAlmostOutOfSpace annotations : description : Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ printf \"%.2f\" $value }}% available space left. summary : Filesystem has less than 5% space left. expr : |- ( node_filesystem_avail_bytes{job=\"node\",fstype!=\"\"} / node_filesystem_size_bytes{job=\"node\",fstype!=\"\"} * 100 < 5 and node_filesystem_readonly{job=\"node\",fstype!=\"\"} == 0 ) for : 1h labels : severity : warning - alert : NodeFilesystemAlmostOutOfSpace annotations : description : Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ printf \"%.2f\" $value }}% available space left. summary : Filesystem has less than 3% space left. expr : |- ( node_filesystem_avail_bytes{job=\"node\",fstype!=\"\"} / node_filesystem_size_bytes{job=\"node\",fstype!=\"\"} * 100 < 3 and node_filesystem_readonly{job=\"node\",fstype!=\"\"} == 0 ) for : 1h labels : severity : critical - alert : NodeFilesystemFilesFillingUp annotations : description : Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ printf \"%.2f\" $value }}% available inodes left and is filling up. summary : Filesystem is predicted to run out of inodes within the next 24 hours. expr : |- ( node_filesystem_files_free{job=\"node\",fstype!=\"\"} / node_filesystem_files{job=\"node\",fstype!=\"\"} * 100 < 40 and predict_linear(node_filesystem_files_free{job=\"node\",fstype!=\"\"}[6h], 24*60*60) < 0 and node_filesystem_readonly{job=\"node\",fstype!=\"\"} == 0 ) for : 1h labels : severity : warning - alert : NodeFilesystemFilesFillingUp annotations : description : Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ printf \"%.2f\" $value }}% available inodes left and is filling up fast. summary : Filesystem is predicted to run out of inodes within the next 4 hours. expr : |- ( node_filesystem_files_free{job=\"node\",fstype!=\"\"} / node_filesystem_files{job=\"node\",fstype!=\"\"} * 100 < 20 and predict_linear(node_filesystem_files_free{job=\"node\",fstype!=\"\"}[6h], 4*60*60) < 0 and node_filesystem_readonly{job=\"node\",fstype!=\"\"} == 0 ) for : 1h labels : severity : critical - alert : NodeFilesystemAlmostOutOfFiles annotations : description : Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ printf \"%.2f\" $value }}% available inodes left. summary : Filesystem has less than 5% inodes left. expr : |- ( node_filesystem_files_free{job=\"node\",fstype!=\"\"} / node_filesystem_files{job=\"node\",fstype!=\"\"} * 100 < 5 and node_filesystem_readonly{job=\"node\",fstype!=\"\"} == 0 ) for : 1h labels : severity : warning - alert : NodeFilesystemAlmostOutOfFiles annotations : description : Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ printf \"%.2f\" $value }}% available inodes left. summary : Filesystem has less than 3% inodes left. expr : |- ( node_filesystem_files_free{job=\"node\",fstype!=\"\"} / node_filesystem_files{job=\"node\",fstype!=\"\"} * 100 < 3 and node_filesystem_readonly{job=\"node\",fstype!=\"\"} == 0 ) for : 1h labels : severity : critical - alert : NodeNetworkReceiveErrs annotations : description : '{{ $labels.instance }} interface {{ $labels.device }} has encountered {{ printf \"%.0f\" $value }} receive errors in the last two minutes.' summary : Network interface is reporting many receive errors. expr : |- increase(node_network_receive_errs_total[2m]) > 10 for : 1h labels : severity : warning - alert : NodeNetworkTransmitErrs annotations : description : '{{ $labels.instance }} interface {{ $labels.device }} has encountered {{ printf \"%.0f\" $value }} transmit errors in the last two minutes.' summary : Network interface is reporting many transmit errors. expr : |- increase(node_network_transmit_errs_total[2m]) > 10 for : 1h labels : severity : warning - alert : NodeHighNumberConntrackEntriesUsed annotations : description : '{{ $value | humanizePercentage }} of conntrack entries are used' summary : Number of conntrack are getting close to the limit expr : |- (node_nf_conntrack_entries / node_nf_conntrack_entries_limit) > 0.75 labels : severity : warning - alert : NodeClockSkewDetected annotations : message : Clock on {{ $labels.instance }} is out of sync by more than 300s. Ensure NTP is configured correctly on this host. summary : Clock skew detected. expr : |- ( node_timex_offset_seconds > 0.05 and deriv(node_timex_offset_seconds[5m]) >= 0 ) or ( node_timex_offset_seconds < -0.05 and deriv(node_timex_offset_seconds[5m]) <= 0 ) for : 10m labels : severity : warning - alert : NodeClockNotSynchronising annotations : message : Clock on {{ $labels.instance }} is not synchronising. Ensure NTP is configured on this host. summary : Clock not synchronising. expr : |- min_over_time(node_timex_sync_status[5m]) == 0 for : 10m labels : severity : warning Next steps \u2691 Configure the alertmanager alerts . Configure the Blackbox Exporter . Configure the grafana dashboards. Issues \u2691 Error: apiVersion 'v2' is not valid. The value must be \"v1\" : Update the warning above and update the clusters.","title":"Prometheus Install"},{"location":"devops/prometheus/prometheus_installation/#kubernetes","text":"Helm 2 is not supported anymore. Later versions of the chart return an Error: apiVersion 'v2' is not valid. The value must be \"v1\" when using helm 2. Diving deeper , it seems that from 11.1.7 support for helm 2 was dropped. To install the operator we'll use helmfile to install the stable/prometheus-operator chart . Add the following lines to your helmfile.yaml . - name : prometheus-operator namespace : monitoring chart : stable/prometheus-operator values : - prometheus-operator/values.yaml Edit the chart values. mkdir prometheus-operator helm inspect values stable/prometheus-operator > prometheus-operator/values.yaml vi prometheus-operator/values.yaml I've implemented the following changes: If you are using a managed solution like EKS, the provider will hide kube-scheduler and kube-controller-manager so those metrics will fail. Therefore you need to disable: defaultRules.rules.kubeScheduler: false . kubeScheduler.enabled: false . kubeControllerManager.enabled: false . Enabled the ingress of alertmanager , grafana and prometheus . Set up the storage of alertmanager and prometheus with storageClassName: gp2 (for AWS). Change additionalPrometheusRules to additionalPrometheusRulesMap as the former is going to be deprecated in future releases. For private clusters, disable the admission webhook . prometheusOperator.admissionWebhooks.enabled=false prometheusOperator.admissionWebhooks.patch.enabled=false prometheusOperator.tlsProxy.enabled=false And install. helmfile diff helmfile apply Once it's installed you can check everything is working by accessing the grafana dashboard. First of all get the pod name (we'll asume you've used the monitoring namespace). kubectl get pods -n monitoring | grep grafana Then set up the proxies kubectl port-forward {{ grafana_pod }} -n monitoring 3000 :3000 kubectl port-forward -n monitoring \\ prometheus-prometheus-operator-prometheus-0 9090 :9090 To access grafana, go to http://localhost:3000 through your browser and at the top left, click on Home and select any dashboard. To access prometheus, go to http://localhost:9090 . If you're using the EKS helm chart, you'll need to manually edit the kube-proxy-config configmap until this bug has been solved. Edit the 127.0.0.1 value to 0.0.0.0 for the key metricsBindAddress in kubectl -n kube-system edit cm kube-proxy-config And restart the DaemonSet: kubectl rollout restart -n kube-system daemonset.apps/kube-proxy","title":"Kubernetes"},{"location":"devops/prometheus/prometheus_installation/#upgrading-notes","text":"","title":"Upgrading notes"},{"location":"devops/prometheus/prometheus_installation/#10x-1117","text":"If you have a private cluster in EKS, you are not able to use the admission webhooks as the nodes are not able to reach the master. Between those versions, something changed and you need to disable tls too with: prometheusOperator : tls : enabled : false admissionWebhooks : enabled : false If you run helmfile apply without that flag, the deployment gets tainted, and you may need to edit the deployment to remove the tls-secret volume.","title":"10.x -&gt; 11.1.7"},{"location":"devops/prometheus/prometheus_installation/#docker","text":"To install it outside Kubernetes, use the cloudalchemy ansible role for host installations or the prom/prometheus docker with the following command: /usr/bin/docker run --rm \\ --name prometheus \\ -v /data/prometheus:/etc/prometheus \\ prom/prometheus:latest \\ --storage.tsdb.retention.time = 30d \\ --config.file = /etc/prometheus/prometheus.yml \\ With a basic prometheus configuration: File: /data/prometheus/prometheus.yml","title":"Docker"},{"location":"devops/prometheus/prometheus_installation/#yaml","text":"","title":"```yaml"},{"location":"devops/prometheus/prometheus_installation/#httpprometheusiodocsoperatingconfiguration","text":"global: evaluation_interval: 1m scrape_interval: 1m scrape_timeout: 10s external_labels: environment: helm rule_files: - /etc/prometheus/rules/*.rules scrape_configs: - job_name: prometheus metrics_path: /metrics static_configs: - targets: - prometheus:9090 ``` And some basic rules: File: /data/prometheus/rules/ groups : - name : ansible managed alert rules rules : - alert : Watchdog annotations : description : |- This is an alert meant to ensure that the entire alerting pipeline is functional. This alert is always firing, therefore it should always be firing in Alertmanager and always fire against a receiver. There are integrations with various notification mechanisms that send a notification when this alert is not firing. For example the \"DeadMansSnitch\" integration in PagerDuty. summary : Ensure entire alerting pipeline is functional expr : vector(1) for : 10m labels : severity : warning - alert : InstanceDown annotations : description : '{{ $labels.instance }} of job {{ $labels.job }} has been down for more than 5 minutes.' summary : Instance {{ $labels.instance }} down expr : up == 0 for : 5m labels : severity : critical - alert : RebootRequired annotations : description : '{{ $labels.instance }} requires a reboot.' summary : Instance {{ $labels.instance }} - reboot required expr : node_reboot_required > 0 labels : severity : warning - alert : NodeFilesystemSpaceFillingUp annotations : description : Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ printf \"%.2f\" $value }}% available space left and is filling up. summary : Filesystem is predicted to run out of space within the next 24 hours. expr : |- ( node_filesystem_avail_bytes{job=\"node\",fstype!=\"\"} / node_filesystem_size_bytes{job=\"node\",fstype!=\"\"} * 100 < 40 and predict_linear(node_filesystem_avail_bytes{job=\"node\",fstype!=\"\"}[6h], 24*60*60) < 0 and node_filesystem_readonly{job=\"node\",fstype!=\"\"} == 0 ) for : 1h labels : severity : warning - alert : NodeFilesystemSpaceFillingUp annotations : description : Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ printf \"%.2f\" $value }}% available space left and is filling up fast. summary : Filesystem is predicted to run out of space within the next 4 hours. expr : |- ( node_filesystem_avail_bytes{job=\"node\",fstype!=\"\"} / node_filesystem_size_bytes{job=\"node\",fstype!=\"\"} * 100 < 20 and predict_linear(node_filesystem_avail_bytes{job=\"node\",fstype!=\"\"}[6h], 4*60*60) < 0 and node_filesystem_readonly{job=\"node\",fstype!=\"\"} == 0 ) for : 1h labels : severity : critical - alert : NodeFilesystemAlmostOutOfSpace annotations : description : Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ printf \"%.2f\" $value }}% available space left. summary : Filesystem has less than 5% space left. expr : |- ( node_filesystem_avail_bytes{job=\"node\",fstype!=\"\"} / node_filesystem_size_bytes{job=\"node\",fstype!=\"\"} * 100 < 5 and node_filesystem_readonly{job=\"node\",fstype!=\"\"} == 0 ) for : 1h labels : severity : warning - alert : NodeFilesystemAlmostOutOfSpace annotations : description : Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ printf \"%.2f\" $value }}% available space left. summary : Filesystem has less than 3% space left. expr : |- ( node_filesystem_avail_bytes{job=\"node\",fstype!=\"\"} / node_filesystem_size_bytes{job=\"node\",fstype!=\"\"} * 100 < 3 and node_filesystem_readonly{job=\"node\",fstype!=\"\"} == 0 ) for : 1h labels : severity : critical - alert : NodeFilesystemFilesFillingUp annotations : description : Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ printf \"%.2f\" $value }}% available inodes left and is filling up. summary : Filesystem is predicted to run out of inodes within the next 24 hours. expr : |- ( node_filesystem_files_free{job=\"node\",fstype!=\"\"} / node_filesystem_files{job=\"node\",fstype!=\"\"} * 100 < 40 and predict_linear(node_filesystem_files_free{job=\"node\",fstype!=\"\"}[6h], 24*60*60) < 0 and node_filesystem_readonly{job=\"node\",fstype!=\"\"} == 0 ) for : 1h labels : severity : warning - alert : NodeFilesystemFilesFillingUp annotations : description : Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ printf \"%.2f\" $value }}% available inodes left and is filling up fast. summary : Filesystem is predicted to run out of inodes within the next 4 hours. expr : |- ( node_filesystem_files_free{job=\"node\",fstype!=\"\"} / node_filesystem_files{job=\"node\",fstype!=\"\"} * 100 < 20 and predict_linear(node_filesystem_files_free{job=\"node\",fstype!=\"\"}[6h], 4*60*60) < 0 and node_filesystem_readonly{job=\"node\",fstype!=\"\"} == 0 ) for : 1h labels : severity : critical - alert : NodeFilesystemAlmostOutOfFiles annotations : description : Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ printf \"%.2f\" $value }}% available inodes left. summary : Filesystem has less than 5% inodes left. expr : |- ( node_filesystem_files_free{job=\"node\",fstype!=\"\"} / node_filesystem_files{job=\"node\",fstype!=\"\"} * 100 < 5 and node_filesystem_readonly{job=\"node\",fstype!=\"\"} == 0 ) for : 1h labels : severity : warning - alert : NodeFilesystemAlmostOutOfFiles annotations : description : Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ printf \"%.2f\" $value }}% available inodes left. summary : Filesystem has less than 3% inodes left. expr : |- ( node_filesystem_files_free{job=\"node\",fstype!=\"\"} / node_filesystem_files{job=\"node\",fstype!=\"\"} * 100 < 3 and node_filesystem_readonly{job=\"node\",fstype!=\"\"} == 0 ) for : 1h labels : severity : critical - alert : NodeNetworkReceiveErrs annotations : description : '{{ $labels.instance }} interface {{ $labels.device }} has encountered {{ printf \"%.0f\" $value }} receive errors in the last two minutes.' summary : Network interface is reporting many receive errors. expr : |- increase(node_network_receive_errs_total[2m]) > 10 for : 1h labels : severity : warning - alert : NodeNetworkTransmitErrs annotations : description : '{{ $labels.instance }} interface {{ $labels.device }} has encountered {{ printf \"%.0f\" $value }} transmit errors in the last two minutes.' summary : Network interface is reporting many transmit errors. expr : |- increase(node_network_transmit_errs_total[2m]) > 10 for : 1h labels : severity : warning - alert : NodeHighNumberConntrackEntriesUsed annotations : description : '{{ $value | humanizePercentage }} of conntrack entries are used' summary : Number of conntrack are getting close to the limit expr : |- (node_nf_conntrack_entries / node_nf_conntrack_entries_limit) > 0.75 labels : severity : warning - alert : NodeClockSkewDetected annotations : message : Clock on {{ $labels.instance }} is out of sync by more than 300s. Ensure NTP is configured correctly on this host. summary : Clock skew detected. expr : |- ( node_timex_offset_seconds > 0.05 and deriv(node_timex_offset_seconds[5m]) >= 0 ) or ( node_timex_offset_seconds < -0.05 and deriv(node_timex_offset_seconds[5m]) <= 0 ) for : 10m labels : severity : warning - alert : NodeClockNotSynchronising annotations : message : Clock on {{ $labels.instance }} is not synchronising. Ensure NTP is configured on this host. summary : Clock not synchronising. expr : |- min_over_time(node_timex_sync_status[5m]) == 0 for : 10m labels : severity : warning","title":"http://prometheus.io/docs/operating/configuration/"},{"location":"devops/prometheus/prometheus_installation/#next-steps","text":"Configure the alertmanager alerts . Configure the Blackbox Exporter . Configure the grafana dashboards.","title":"Next steps"},{"location":"devops/prometheus/prometheus_installation/#issues","text":"Error: apiVersion 'v2' is not valid. The value must be \"v1\" : Update the warning above and update the clusters.","title":"Issues"},{"location":"devops/prometheus/prometheus_operator/","text":"Prometheus has it's own kubernetes operator , which makes it simple to install with helm, and enables users to configure and manage instances of Prometheus using simple declarative configuration that will, in response, create, configure, and manage Prometheus monitoring instances. Once installed the Prometheus Operator provides the following features: Create/Destroy : Easily launch a Prometheus instance for your Kubernetes namespace, a specific application or team easily using the Operator. Simple Configuration : Configure the fundamentals of Prometheus like versions, persistence, retention policies, and replicas from a native Kubernetes resource. Target Services via Labels : Automatically generate monitoring target configurations based on familiar Kubernetes label queries; no need to learn a Prometheus specific configuration language. How it works \u2691 The core idea of the Operator is to decouple deployment of Prometheus instances from the configuration of which entities they are monitoring. The Operator acts on the following custom resource definitions (CRDs): Prometheus : Defines the desired Prometheus deployment. The Operator ensures at all times that a deployment matching the resource definition is running. This entails aspects like the data retention time, persistent volume claims, number of replicas, the Prometheus version, and Alertmanager instances to send alerts to. ServiceMonitor : Specifies how metrics can be retrieved from a set of services exposing them in a common way. The Operator configures the Prometheus instance to monitor all services covered by included ServiceMonitors and keeps this configuration synchronized with any changes happening in the cluster. PrometheusRule : Defines a desired Prometheus rule file, which can be loaded by a Prometheus instance containing Prometheus alerting and recording rules. Alertmanager : Defines a desired Alertmanager deployment. The Operator ensures at all times that a deployment matching the resource definition is running. Links \u2691 Homepage CoreOS Prometheus operator presentation Sysdig Prometheus operator guide part 3","title":"Prometheus Operator"},{"location":"devops/prometheus/prometheus_operator/#how-it-works","text":"The core idea of the Operator is to decouple deployment of Prometheus instances from the configuration of which entities they are monitoring. The Operator acts on the following custom resource definitions (CRDs): Prometheus : Defines the desired Prometheus deployment. The Operator ensures at all times that a deployment matching the resource definition is running. This entails aspects like the data retention time, persistent volume claims, number of replicas, the Prometheus version, and Alertmanager instances to send alerts to. ServiceMonitor : Specifies how metrics can be retrieved from a set of services exposing them in a common way. The Operator configures the Prometheus instance to monitor all services covered by included ServiceMonitors and keeps this configuration synchronized with any changes happening in the cluster. PrometheusRule : Defines a desired Prometheus rule file, which can be loaded by a Prometheus instance containing Prometheus alerting and recording rules. Alertmanager : Defines a desired Alertmanager deployment. The Operator ensures at all times that a deployment matching the resource definition is running.","title":"How it works"},{"location":"devops/prometheus/prometheus_operator/#links","text":"Homepage CoreOS Prometheus operator presentation Sysdig Prometheus operator guide part 3","title":"Links"},{"location":"devops/prometheus/prometheus_troubleshooting/","text":"Solutions for problems with Prometheus. Service monitor not being recognized \u2691 Probably the service monitor labels aren't properly configured. Each prometheus monitors it's own targets, to see how you need to label your resources, describe the prometheus instance and search for Service Monitor Selector . kubectl get prometheus -n monitoring kubectl describe prometheus prometheus-operator-prometheus -n monitoring The last one will return something like: Service Monitor Selector : Match Labels : Release : prometheus-operator Which means you need to label your service monitors with release: prometheus-operator , be careful if you use Release: prometheus-operator it won't work. Failed calling webhook prometheusrulemutate.monitoring.coreos.com \u2691 Error: UPGRADE FAILED: failed to create resource: Internal error occurred: failed calling webhook \"prometheusrulemutate.monitoring.coreos.com\": Post https://prometheus-operator-operator.monitoring.svc:443/admission-prometheusrules/mutate?timeout=30s: no endpoints available for service \"prometheus-operator-operator\" Since version 0.30 of the operator, there is an admission webhook to prevent malformed rules from being added to the cluster. Without this validation, creating an invalid resource will cause Prometheus not to load it. If the container then restarts, it will go into a crashloop. For the webhook to work, the control plane needs to be able to access the webhook service. That means the addition of a firewall rule in EKS and GKE deployments. People have succeeded with GKE , but people struggling with EKS have decided to disable the webhook. To disable it, the following options have to be set: prometheusOperator.admissionWebhooks.enabled=false prometheusOperator.admissionWebhooks.patch.enabled=false prometheusOperator.tlsProxy.enabled=false If you have deployed your release with the webhook enabled, you also need to remove all the resources that match the following: kubectl get validatingwebhookconfigurations.admissionregistration.k8s.io kubectl get MutatingWebhookConfiguration Before executing helmfile apply again.","title":"Prometheus Troubleshooting"},{"location":"devops/prometheus/prometheus_troubleshooting/#service-monitor-not-being-recognized","text":"Probably the service monitor labels aren't properly configured. Each prometheus monitors it's own targets, to see how you need to label your resources, describe the prometheus instance and search for Service Monitor Selector . kubectl get prometheus -n monitoring kubectl describe prometheus prometheus-operator-prometheus -n monitoring The last one will return something like: Service Monitor Selector : Match Labels : Release : prometheus-operator Which means you need to label your service monitors with release: prometheus-operator , be careful if you use Release: prometheus-operator it won't work.","title":"Service monitor not being recognized"},{"location":"devops/prometheus/prometheus_troubleshooting/#failed-calling-webhook-prometheusrulemutatemonitoringcoreoscom","text":"Error: UPGRADE FAILED: failed to create resource: Internal error occurred: failed calling webhook \"prometheusrulemutate.monitoring.coreos.com\": Post https://prometheus-operator-operator.monitoring.svc:443/admission-prometheusrules/mutate?timeout=30s: no endpoints available for service \"prometheus-operator-operator\" Since version 0.30 of the operator, there is an admission webhook to prevent malformed rules from being added to the cluster. Without this validation, creating an invalid resource will cause Prometheus not to load it. If the container then restarts, it will go into a crashloop. For the webhook to work, the control plane needs to be able to access the webhook service. That means the addition of a firewall rule in EKS and GKE deployments. People have succeeded with GKE , but people struggling with EKS have decided to disable the webhook. To disable it, the following options have to be set: prometheusOperator.admissionWebhooks.enabled=false prometheusOperator.admissionWebhooks.patch.enabled=false prometheusOperator.tlsProxy.enabled=false If you have deployed your release with the webhook enabled, you also need to remove all the resources that match the following: kubectl get validatingwebhookconfigurations.admissionregistration.k8s.io kubectl get MutatingWebhookConfiguration Before executing helmfile apply again.","title":"Failed calling webhook prometheusrulemutate.monitoring.coreos.com"},{"location":"drawing/drawing/","text":"It's really difficult to choose where to start if you want to learn how to draw from scratch as there are too many resources. I'm starting with Drawabox , a free course based on series of practical exercises to teach the basics. I'd probably go to Ctrl+Paint once I'm done. The basics \u2691 Changing the mindset \u2691 Start your drawing path with the following guidelines to make your progression smoother: Focus on the experience of drawing instead of the result. Understand that doing something badly does not define who you are. So tackle the I can't draw that feeling with I can't draw that well . If you're afraid the thing you want to draw is out of your reach, draw it anyway. At least half of the time spent drawing must be devoted to drawing purely for its own sake. If you don't have much time, alternate the purpose of your sessions. Don't over control your hand with your brain to try to be absolutely precise and accurate. Doing so will result in numerous course corrections making your strokes wobbly, stiff and erratic. Furthermore, spending all the focus resources in precision, will result in a lack to solve the other problems involved. Once muscle memory is gained, the strokes will be cleaner. Draw exactly what you see, while you see it . Don't trust you memory, as it will simplify things without you noticing it. Draw from your shoulder . We are used to pivot on the wrist as it makes stiff and accurate linework, suitable for writing. But falls apart when making smooth and consistent strokes. So use the wrist when drawing stiff but precise marks in areas of detail or texture. There are plenty of cases where the elbow will work fine, but using it will get you in the habit of taking the path of least resistance . So try to use the shoulder. This means driving the motion from the muscles that control that joint. As it has a considerable range of motion, you should be able to move your arm with minimal adjustment from your elbow. If you catch yourself having fallen back to drawing from the elbow, do the following exercise: Draw pivoting from your wrist while locking the rest of the joints, to get used to what that feels like. Then lock it and move to the elbow. Finally lock the elbow and go for the shoulder. Drawing at it's simplest level is the act of putting marks on a page in order or communicate or convey something. Marks should: Flow continuously : When making a line between two points, do it with a single continuous stroke even if you miss the end. Flow smoothly : Draw with a confident and persistent pace (enough to keep your brain from interfering and attempting to course correct as you go). Again we favor flow over accuracy , so expect to make your lines less accurate. Maintain a consistent trayectory : Split lines into derivable strokes. Otherwise, you'll make mindless zigzags. Drawing skills \u2691 The course focuses on these psychological skills and concepts : Confidence : The willingness to push forwards without hesitation once your preparations are complete. Control : The ability to decide ahead of time what kind of mark you wish to puto down on the page, and to execute as intended. Patience : The path is hard. Spatial Reasoning : To be able to understand the things we draw as being three dimensional forms that exist in and relate to one another within the three dimensional world. Construction : The ability to look at a complex object and break it down into simple components that can be drawn individually and combined to reconstruct our complex object on a page. Visual Communication : The ability to take a concept, idea, or amount of information, and to convey it clearly and directly to an audience using visual means. Ghosting \u2691 Ghosting lines is a technique to break the mark making process into a series of steps that allows us to draw with confidence while also improving the accuracy of our results. It also forces us to think and consider our intentions before each and every mark we put down. Planning : Lay out the terms of the line you want to draw, paint a dot for the start and another for the end. Rotating the page : Find the most comfortable angle of approach for the line you've planned. Usually it's roughly a 45 degree angle fom left to right (if you're right handed). Ghosting : Go through the motion of drawing your line, over and over, in one direction, without actually touching the page, so as to build muscle memory. Execution : Once you feel comfortable with the motion, without missing a beat or breaking the rhythm of repetition, lower your pen to the page and go through the motion one more time. Drawabox course guidelines \u2691 When doing the course exercises, try to: Read only the instruction pages that are assigned to each exercise. Don't redo the exercises until you achieve perfection, even when you don't feel satisfied with your results. Accept this now, and it will save you a lot of grief and wasted time in the future. Your only focus should be on following the instructions to the best of your current ability. Read and follow the instructions carefully to ensure you understand them. Each time you finish an exercise incorporate into a pool from which take two or three at the beginning of each session to do for 10 or 15 minutes. Tools to use \u2691 For the Drawabox course, you need a fineliner , also called felt tips or technical pens . The author recommends Staedtler Pigments Liners and Faber Castell PITT Artist Pens (their sizing is different, F is the equivalent to 0.5). When using it, make sure you're not applying too much pressure, as it will damage the tip and reduce the flow of ink. For the paper, use the regular printer one. Basic Shapes \u2691 Ellipses \u2691 Ellipses are the next basic shape we're going to study (after the lines). They are extremely important and notoriously annoying to draw. Important because we're going to be using ellipses in 2D space to represent circles that exist in 3D space. How our circle (in 3D space) is rotated relative to the viewer will determine how wide our ellipse is going to be drawn. The degree shift (or angle) is measured between the minor axis and the line joining the viewer to the center of the circle. It will therefore be 90 degrees when it's facing directly to us, and 0 when we only see a line. Keep in mind that the degree shift will change if we move the circle around even if we don't rotate it. The minor axis is the line that passes across the ellipse's narrowest span and through it's center. This axis will split the ellipse into two equal symmetrical halves. An interesting property of the minor axis is that it coincides with and aligns with the normal vector of the circle. This means that we've got two ways of establishing the orientation of our circle in 3D space, as we draw it on our 2D page. The degree (width) of the ellipse controlling the circle's rotation The minor axis of the ellipse controlling the orientation of the circle's normal vector. If we need our circle to be oriented in a specific way in a more complicated scene, we will actually be able to start out with a normal vector line, then use it as the minor axis for our ellipse. Taking it further, the minor axis can be used to build cylindrical forms because any section of a cylinder shares the same normal vector. Drawing ellipses \u2691 To draw ellipses follow the next steps: Decide on the ellipse degree, position and boundaries. Ghost the ellipse Draw around the ellipse two or three times before lifting your pen. When you try to hit your ellipse in a single round, it's usually going to come out uneven and wobbly (due to drawing too slowly and carefully) or extremely loose (due to simply not having built up the muscle memory to nail an ellipse). Drawing through your ellipses gives your arm the chance to familiarize itself with what's being asked of it in that first pass, and then firm it up in the second. It also helps you maintain the confidence needed to achieve a smooth, even shape, without totally losing control. Check the ellipse exercises to practice your skills. Links \u2691 Drawabox Ctrl+Paint Dive deeper \u2691 Pool of drawing exercises","title":"Drawing"},{"location":"drawing/drawing/#the-basics","text":"","title":"The basics"},{"location":"drawing/drawing/#changing-the-mindset","text":"Start your drawing path with the following guidelines to make your progression smoother: Focus on the experience of drawing instead of the result. Understand that doing something badly does not define who you are. So tackle the I can't draw that feeling with I can't draw that well . If you're afraid the thing you want to draw is out of your reach, draw it anyway. At least half of the time spent drawing must be devoted to drawing purely for its own sake. If you don't have much time, alternate the purpose of your sessions. Don't over control your hand with your brain to try to be absolutely precise and accurate. Doing so will result in numerous course corrections making your strokes wobbly, stiff and erratic. Furthermore, spending all the focus resources in precision, will result in a lack to solve the other problems involved. Once muscle memory is gained, the strokes will be cleaner. Draw exactly what you see, while you see it . Don't trust you memory, as it will simplify things without you noticing it. Draw from your shoulder . We are used to pivot on the wrist as it makes stiff and accurate linework, suitable for writing. But falls apart when making smooth and consistent strokes. So use the wrist when drawing stiff but precise marks in areas of detail or texture. There are plenty of cases where the elbow will work fine, but using it will get you in the habit of taking the path of least resistance . So try to use the shoulder. This means driving the motion from the muscles that control that joint. As it has a considerable range of motion, you should be able to move your arm with minimal adjustment from your elbow. If you catch yourself having fallen back to drawing from the elbow, do the following exercise: Draw pivoting from your wrist while locking the rest of the joints, to get used to what that feels like. Then lock it and move to the elbow. Finally lock the elbow and go for the shoulder. Drawing at it's simplest level is the act of putting marks on a page in order or communicate or convey something. Marks should: Flow continuously : When making a line between two points, do it with a single continuous stroke even if you miss the end. Flow smoothly : Draw with a confident and persistent pace (enough to keep your brain from interfering and attempting to course correct as you go). Again we favor flow over accuracy , so expect to make your lines less accurate. Maintain a consistent trayectory : Split lines into derivable strokes. Otherwise, you'll make mindless zigzags.","title":"Changing the mindset"},{"location":"drawing/drawing/#drawing-skills","text":"The course focuses on these psychological skills and concepts : Confidence : The willingness to push forwards without hesitation once your preparations are complete. Control : The ability to decide ahead of time what kind of mark you wish to puto down on the page, and to execute as intended. Patience : The path is hard. Spatial Reasoning : To be able to understand the things we draw as being three dimensional forms that exist in and relate to one another within the three dimensional world. Construction : The ability to look at a complex object and break it down into simple components that can be drawn individually and combined to reconstruct our complex object on a page. Visual Communication : The ability to take a concept, idea, or amount of information, and to convey it clearly and directly to an audience using visual means.","title":"Drawing skills"},{"location":"drawing/drawing/#ghosting","text":"Ghosting lines is a technique to break the mark making process into a series of steps that allows us to draw with confidence while also improving the accuracy of our results. It also forces us to think and consider our intentions before each and every mark we put down. Planning : Lay out the terms of the line you want to draw, paint a dot for the start and another for the end. Rotating the page : Find the most comfortable angle of approach for the line you've planned. Usually it's roughly a 45 degree angle fom left to right (if you're right handed). Ghosting : Go through the motion of drawing your line, over and over, in one direction, without actually touching the page, so as to build muscle memory. Execution : Once you feel comfortable with the motion, without missing a beat or breaking the rhythm of repetition, lower your pen to the page and go through the motion one more time.","title":"Ghosting"},{"location":"drawing/drawing/#drawabox-course-guidelines","text":"When doing the course exercises, try to: Read only the instruction pages that are assigned to each exercise. Don't redo the exercises until you achieve perfection, even when you don't feel satisfied with your results. Accept this now, and it will save you a lot of grief and wasted time in the future. Your only focus should be on following the instructions to the best of your current ability. Read and follow the instructions carefully to ensure you understand them. Each time you finish an exercise incorporate into a pool from which take two or three at the beginning of each session to do for 10 or 15 minutes.","title":"Drawabox course guidelines"},{"location":"drawing/drawing/#tools-to-use","text":"For the Drawabox course, you need a fineliner , also called felt tips or technical pens . The author recommends Staedtler Pigments Liners and Faber Castell PITT Artist Pens (their sizing is different, F is the equivalent to 0.5). When using it, make sure you're not applying too much pressure, as it will damage the tip and reduce the flow of ink. For the paper, use the regular printer one.","title":"Tools to use"},{"location":"drawing/drawing/#basic-shapes","text":"","title":"Basic Shapes"},{"location":"drawing/drawing/#ellipses","text":"Ellipses are the next basic shape we're going to study (after the lines). They are extremely important and notoriously annoying to draw. Important because we're going to be using ellipses in 2D space to represent circles that exist in 3D space. How our circle (in 3D space) is rotated relative to the viewer will determine how wide our ellipse is going to be drawn. The degree shift (or angle) is measured between the minor axis and the line joining the viewer to the center of the circle. It will therefore be 90 degrees when it's facing directly to us, and 0 when we only see a line. Keep in mind that the degree shift will change if we move the circle around even if we don't rotate it. The minor axis is the line that passes across the ellipse's narrowest span and through it's center. This axis will split the ellipse into two equal symmetrical halves. An interesting property of the minor axis is that it coincides with and aligns with the normal vector of the circle. This means that we've got two ways of establishing the orientation of our circle in 3D space, as we draw it on our 2D page. The degree (width) of the ellipse controlling the circle's rotation The minor axis of the ellipse controlling the orientation of the circle's normal vector. If we need our circle to be oriented in a specific way in a more complicated scene, we will actually be able to start out with a normal vector line, then use it as the minor axis for our ellipse. Taking it further, the minor axis can be used to build cylindrical forms because any section of a cylinder shares the same normal vector.","title":"Ellipses"},{"location":"drawing/drawing/#drawing-ellipses","text":"To draw ellipses follow the next steps: Decide on the ellipse degree, position and boundaries. Ghost the ellipse Draw around the ellipse two or three times before lifting your pen. When you try to hit your ellipse in a single round, it's usually going to come out uneven and wobbly (due to drawing too slowly and carefully) or extremely loose (due to simply not having built up the muscle memory to nail an ellipse). Drawing through your ellipses gives your arm the chance to familiarize itself with what's being asked of it in that first pass, and then firm it up in the second. It also helps you maintain the confidence needed to achieve a smooth, even shape, without totally losing control. Check the ellipse exercises to practice your skills.","title":"Drawing ellipses"},{"location":"drawing/drawing/#links","text":"Drawabox Ctrl+Paint","title":"Links"},{"location":"drawing/drawing/#dive-deeper","text":"Pool of drawing exercises","title":"Dive deeper"},{"location":"drawing/exercise_pool/","text":"Set of exercises to maintain the fundamental skills required for drawing. Before doing a drawing session, spend 10-20 minutes doing one or several of these exercises. Lines \u2691 Superimposed lines : for different increasing lengths (4cm, 8cm, half the width and full width), draw a line with a ruler and repeat the stroke freehand eight times. Also try some arcing lines, and even some waves. Make sure you fray only at the end. Example: Ghosted lines : Fill up a page with straight lines following the ghosting method . Special things to avoid: Wobbly lines Arcing lines There are several levels of success with this exercise: Level 1 : Line is smooth and consistent without any visible wobbling, but doesn't quite pass through A or B, due to not following the right trajectory. It's a straight shot, but misses the mark a bit. Level 2 : Like level 1 and maintains the correct trajectory. It does however either fall short or overshot one or both points. Level 3 : Like level 2 and also starts at right at one point and ends exactly at the other. Example: Ghosted planes : Fill up a page with planes using the ghosting method . Start with 4 points, join them, then fill in the two diagonals, and then make a cross through the center of the X. Special things to avoid: Wobbly lines Arcing lines Example: As you repeat the exercise, you can start to envision these planes as being three dimensional rectilinear surfaces. The third and fourth steps, where we construct the diagonals and the cross can be treated as being a subdivision of the plane. The cross will require some estimation to find the center of each edge in space. Ellipses \u2691 Tables of ellipses \u2691 This exercise is meant to get you used to drawing ellipses, in a variety of sizes, orientations and degrees. It also sets out a clear space each ellipse is meant to occupy, giving us a means to assess whether or not an ellipse was successful, or if there were visible mistakes (where it went outside of its allotted space, or ended up falling short). Practicing against set criteria, with a way to judge success/failure is an important element of learning. There's nothing wrong with failure - it's an opportunity to learn. Having a clearly defined task allows us to analyze those failures and make the most of them. Start off by taking your piece of paper and dividing it into a table with two columns and a bunch of rows. For this one, you draw a circle starting from the far left of the box. Then, draw another beside it. Keep repeating it until you fill in the whole box. Strive to make your circles touch the top and bottom of the box, as well as the line to the left of it. Next, same idea, but with ellipses. Within the same section, you should aim to draw ellipses of the same degree. You can also play with the angle of the ellipse, and this should also be consistent within the same section. This one's a little different. Draw a wave through the section, dividing it into irregular pockets of space. Then fill these spaces with circles or ellipses, trying to keep them touching the bounds of the section as well as the curve. Everything should fit in there snugly, and nothing should be floating around. Things to remember \u2691 Draw confidently : use the ghosting method and \"draw through\" your ellipses two full times before lifting your pen to achieve a smooth, even shape Plan your ellipse : Set out a goal for the ellipse you're about to draw. Draw from the shoulder . As you get better, your ellipses will tighten up - the gaps between your successive passes will shrink and eventually your ellipses will appear much cleaner.","title":"Exercise Pool"},{"location":"drawing/exercise_pool/#lines","text":"Superimposed lines : for different increasing lengths (4cm, 8cm, half the width and full width), draw a line with a ruler and repeat the stroke freehand eight times. Also try some arcing lines, and even some waves. Make sure you fray only at the end. Example: Ghosted lines : Fill up a page with straight lines following the ghosting method . Special things to avoid: Wobbly lines Arcing lines There are several levels of success with this exercise: Level 1 : Line is smooth and consistent without any visible wobbling, but doesn't quite pass through A or B, due to not following the right trajectory. It's a straight shot, but misses the mark a bit. Level 2 : Like level 1 and maintains the correct trajectory. It does however either fall short or overshot one or both points. Level 3 : Like level 2 and also starts at right at one point and ends exactly at the other. Example: Ghosted planes : Fill up a page with planes using the ghosting method . Start with 4 points, join them, then fill in the two diagonals, and then make a cross through the center of the X. Special things to avoid: Wobbly lines Arcing lines Example: As you repeat the exercise, you can start to envision these planes as being three dimensional rectilinear surfaces. The third and fourth steps, where we construct the diagonals and the cross can be treated as being a subdivision of the plane. The cross will require some estimation to find the center of each edge in space.","title":"Lines"},{"location":"drawing/exercise_pool/#ellipses","text":"","title":"Ellipses"},{"location":"drawing/exercise_pool/#tables-of-ellipses","text":"This exercise is meant to get you used to drawing ellipses, in a variety of sizes, orientations and degrees. It also sets out a clear space each ellipse is meant to occupy, giving us a means to assess whether or not an ellipse was successful, or if there were visible mistakes (where it went outside of its allotted space, or ended up falling short). Practicing against set criteria, with a way to judge success/failure is an important element of learning. There's nothing wrong with failure - it's an opportunity to learn. Having a clearly defined task allows us to analyze those failures and make the most of them. Start off by taking your piece of paper and dividing it into a table with two columns and a bunch of rows. For this one, you draw a circle starting from the far left of the box. Then, draw another beside it. Keep repeating it until you fill in the whole box. Strive to make your circles touch the top and bottom of the box, as well as the line to the left of it. Next, same idea, but with ellipses. Within the same section, you should aim to draw ellipses of the same degree. You can also play with the angle of the ellipse, and this should also be consistent within the same section. This one's a little different. Draw a wave through the section, dividing it into irregular pockets of space. Then fill these spaces with circles or ellipses, trying to keep them touching the bounds of the section as well as the curve. Everything should fit in there snugly, and nothing should be floating around.","title":"Tables of ellipses"},{"location":"drawing/exercise_pool/#things-to-remember","text":"Draw confidently : use the ghosting method and \"draw through\" your ellipses two full times before lifting your pen to achieve a smooth, even shape Plan your ellipse : Set out a goal for the ellipse you're about to draw. Draw from the shoulder . As you get better, your ellipses will tighten up - the gaps between your successive passes will shrink and eventually your ellipses will appear much cleaner.","title":"Things to remember"},{"location":"feminism/privileges/","text":"This page has been gathered from my white cis male brain with myself and fellow privileged readers as target group. What are the privileges \u2691 Privileges are a group of special structural benefits, social advantages, that a group holds over another. So they are elements that should be removed from our lives. Although our individualist culture pushes us to think that the privileges are the result of our own individual efforts, they mainly come from facts that are structural, so they are out of our reach. Such as our genre, skin colour, ethnic, sexuality or overall economic status. Most of the things we consider privileges are rights that we should universalize. This confusion makes us mix them with the ones that should be eliminated because they reinforce the hierarchies and give the privileged license to keep on feeding the inequality relationships. To differentiate between privileges and rights, we can answer the question: Is it desirable to remove that privilege to the privileged? If the answer is yes, it is a privilege and we should fight to remove it. If the answer is no, then we should fight for universalize the right. What can we do to fight the privileges? \u2691 As white cis males we can: Make the different access to the basic security and dignity rights visible. Raise awareness on yourself and others on this topic between our fellow privileged environment. It's more effective to address and permeate a privileged person from a privileged position. Unstructured ideas \u2691 It is really difficult to notice a privilege if you've always liven with it. Go to the Michael Kimmel book for more information. The limits and barriers that slow us down is what is usually seen, unlike the facilities we have to run faster. The individual solutions usually fail to solve social issues. Unfinished list of male privileges \u2691 Male positions are more valued by social structures, even when we are wrong. There are several abilities that are taken for granted. We can neglect the care work. We are more hireables as stated by the Jennifer-John effect . We escalate more in the organizational hierarchy due to informal fraternity networks. We can use violence when we want. Unfinished list of rights to universalize \u2691 Science in general is performed by white male cis for the white male cis humans. Therefore there is a medical gender gap or facial recognition algorithms struggle to recognize black faces . Resources \u2691 Racial equity tools resources on white privilege Articles \u2691 \u00bfExisten los privilegios masculinos? How to talk to your family, friends about racism and white privilege Peggy McIntosh articles How to talk to your white friends and family about privilege Books \u2691 Privilege through the looking glass by Patricia Leavy Privilege A reader by Michael Kimmel Essays \u2691 White Privilege: Unpacking the Invisible Knapsack by Peggy McIntosh","title":"Privileges"},{"location":"feminism/privileges/#what-are-the-privileges","text":"Privileges are a group of special structural benefits, social advantages, that a group holds over another. So they are elements that should be removed from our lives. Although our individualist culture pushes us to think that the privileges are the result of our own individual efforts, they mainly come from facts that are structural, so they are out of our reach. Such as our genre, skin colour, ethnic, sexuality or overall economic status. Most of the things we consider privileges are rights that we should universalize. This confusion makes us mix them with the ones that should be eliminated because they reinforce the hierarchies and give the privileged license to keep on feeding the inequality relationships. To differentiate between privileges and rights, we can answer the question: Is it desirable to remove that privilege to the privileged? If the answer is yes, it is a privilege and we should fight to remove it. If the answer is no, then we should fight for universalize the right.","title":"What are the privileges"},{"location":"feminism/privileges/#what-can-we-do-to-fight-the-privileges","text":"As white cis males we can: Make the different access to the basic security and dignity rights visible. Raise awareness on yourself and others on this topic between our fellow privileged environment. It's more effective to address and permeate a privileged person from a privileged position.","title":"What can we do to fight the privileges?"},{"location":"feminism/privileges/#unstructured-ideas","text":"It is really difficult to notice a privilege if you've always liven with it. Go to the Michael Kimmel book for more information. The limits and barriers that slow us down is what is usually seen, unlike the facilities we have to run faster. The individual solutions usually fail to solve social issues.","title":"Unstructured ideas"},{"location":"feminism/privileges/#unfinished-list-of-male-privileges","text":"Male positions are more valued by social structures, even when we are wrong. There are several abilities that are taken for granted. We can neglect the care work. We are more hireables as stated by the Jennifer-John effect . We escalate more in the organizational hierarchy due to informal fraternity networks. We can use violence when we want.","title":"Unfinished list of male privileges"},{"location":"feminism/privileges/#unfinished-list-of-rights-to-universalize","text":"Science in general is performed by white male cis for the white male cis humans. Therefore there is a medical gender gap or facial recognition algorithms struggle to recognize black faces .","title":"Unfinished list of rights to universalize"},{"location":"feminism/privileges/#resources","text":"Racial equity tools resources on white privilege","title":"Resources"},{"location":"feminism/privileges/#articles","text":"\u00bfExisten los privilegios masculinos? How to talk to your family, friends about racism and white privilege Peggy McIntosh articles How to talk to your white friends and family about privilege","title":"Articles"},{"location":"feminism/privileges/#books","text":"Privilege through the looking glass by Patricia Leavy Privilege A reader by Michael Kimmel","title":"Books"},{"location":"feminism/privileges/#essays","text":"White Privilege: Unpacking the Invisible Knapsack by Peggy McIntosh","title":"Essays"},{"location":"linux/brew/","text":"Complementary package manager to manage the programs that aren't in the Debian repositories. Usage \u2691 TBC References \u2691 Homebrew formula for a Go app","title":"brew"},{"location":"linux/brew/#usage","text":"TBC","title":"Usage"},{"location":"linux/brew/#references","text":"Homebrew formula for a Go app","title":"References"},{"location":"linux/cookiecutter/","text":"Cookiecutter is a command-line utility that creates projects from cookiecutters (project templates). Install \u2691 pip install cookiecutter Use \u2691 DEPRECATION: use cruft instead You may want to use cruft to generate your templates instead, as it will help you maintain the project with the template updates. Something that it's not easy with cookiecutter alone cookiecutter {{ path_or_url_to_cookiecutter_template }} User config \u2691 If you use Cookiecutter a lot, you\u2019ll find it useful to have a user config file. By default Cookiecutter tries to retrieve settings from a .cookiecutterrc file in your home directory. Example user config: default_context : full_name : \"Audrey Roy\" email : \"audreyr@example.com\" github_username : \"audreyr\" cookiecutters_dir : \"/home/audreyr/my-custom-cookiecutters-dir/\" replay_dir : \"/home/audreyr/my-custom-replay-dir/\" abbreviations : python : https://github.com/audreyr/cookiecutter-pypackage.git gh : https://github.com/{0}.git bb : https://bitbucket.org/{0} Possible settings are: default_context A list of key/value pairs that you want injected as context whenever you generate a project with Cookiecutter. These values are treated like the defaults in cookiecutter.json , upon generation of any project. cookiecutters_dir Directory where your cookiecutters are cloned to when you use Cookiecutter with a repo argument. replay_dir Directory where Cookiecutter dumps context data to, which you can fetch later on when using the replay feature. abbreviations A list of abbreviations for cookiecutters. Abbreviations can be simple aliases for a repo name, or can be used as a prefix, in the form abbr:suffix . Any suffix will be inserted into the expansion in place of the text {0} , using standard Python string formatting. With the above aliases, you could use the cookiecutter-pypackage template simply by saying cookiecutter python . Write your own cookietemplates \u2691 Create files or directories with conditions \u2691 For files use a filename like '{{ \".vault_pass.sh\" if cookiecutter.vault_pass_entry != \"None\" else \"\" }}' . For directories I haven't yet found a nice way to do it (as the above will fail), check the issue or the hooks documentation for more information. File: post_gen_project.py import os import sys REMOVE_PATHS = [ '{ % i f cookiecutter.packaging != \"pip\" %} requirements.txt { % e ndif %}' , '{ % i f cookiecutter.packaging != \"poetry\" %} poetry.lock { % e ndif %}' , ] for path in REMOVE_PATHS : path = path . strip () if path and os . path . exists ( path ): if os . path . isdir ( path ): os . rmdir ( path ) else : os . unlink ( path ) Add some text to a file if a condition is met \u2691 Use jinja2 conditionals. Note the - at the end of the conditional opening, play with {%- ... -%} and {% ... %} for different results on line appending. { % if cookiecutter.install_docker == 'yes' -% } - src : git+ssh://mywebpage.org/ansible-roles/docker.git version : 1.0.3 { % - else -% } - src : git+ssh://mywebpage.org/ansible-roles/other-role.git version : 1.0.2 { % - endif % } Initialize git repository on the created cookiecutter \u2691 Added the following to the post generation hooks. File: hooks/post_gen_project.py import subprocess subprocess . call ([ 'git' , 'init' ]) subprocess . call ([ 'git' , 'add' , '*' ]) subprocess . call ([ 'git' , 'commit' , '-m' , 'Initial commit' ]) Prevent cookiecutter from processing some files \u2691 By default cookiecutter will try to process every file as a Jinja template. This behaviour produces wrong results if you have Jinja templates that are meant to be taken as literal. Starting with cookiecutter 1.1, you can tell cookiecutter to only copy some files without interpreting them as Jinja templates . Add a _copy_without_render key in the cookiecutter config file ( cookiecutter.json ). It takes a list of regular expressions. If a filename matches the regular expressions it will be copied and not processed as a Jinja template. { \"project_slug\" : \"sample\" , \"_copy_without_render\" : [ \"*.js\" , \"not_rendered_dir/*\" , \"rendered_dir/not_rendered_file.ini\" ] } Prevent additional whitespaces when jinja condition is not met. \u2691 Jinja2 has a whitespace control that can be used to manage the whitelines existent between the Jinja blocks. The problem comes when a condition is not met in an if block, in that case, Jinja adds a whitespace which will break most linters. This is the solution I've found out that works as expected. ### Multienvironment This playbook has support for the following environments: {% if cookiecutter.production_environment == \"True\" -%} * Production {% endif %} {%- if cookiecutter.staging_environment == \"True\" -%} * Staging {% endif %} {%- if cookiecutter.development_environment == \"True\" -%} * Development {% endif %} ### Tags Testing your own cookiecutter templates \u2691 The pytest-cookies plugin comes with a cookies fixture which is a wrapper for the cookiecutter API for generating projects. It helps you verify that your template is working as expected and takes care of cleaning up after running the tests. Install \u2691 pip install pytest-cookies Usage \u2691 @pytest.fixture def context(): return { \"playbook_name\": \"My Test Playbook\", } The cookies.bake() method generates a new project from your template based on the default values specified in cookiecutter.json: def test_bake_project ( cookies ): result = cookies . bake ( extra_context = { 'repo_name' : 'hello world' }) assert result . exit_code == 0 assert result . exception is None assert result . project . basename == 'hello world' assert result . project . isdir () It accepts the extra_context keyword argument that is passed to cookiecutter. The given dictionary will override the default values of the template context, allowing you to test arbitrary user input data. The cookiecutter-django has a nice test file using this fixture. Mocking the contents of the cookiecutter hooks \u2691 Sometimes it's interesting to add interactions with external services in the cookiecutter hooks, for example to activate a CI pipeline. If you want to test the cookiecutter template you need to mock those external interactions. But it's difficult to mock the contents of the hooks because their contents aren't run by the cookies.bake() code. Instead it delegates in cookiecutter to run them, which opens a subprocess to run them , so the mocks don't work. The alternative is setting an environmental variable in your tests to skip those steps: File: tests/conftest.py import os os . environ [ \"COOKIECUTTER_TESTING\" ] = \"true\" File: hooks/pre_gen_project.py def main (): # ... pre_hook content ... if __name__ == \"__main__\" : if os . environ . get ( \"COOKIECUTTER_TESTING\" ) != \"true\" : main () If you want to test the content of main , you can now mock each of the external interactions. But you'll face the problem that these files are jinja2 templates of python files, so it's tricky to test them, due to syntax errors. Debug failing template generation \u2691 Sometimes the generation of the templates will fail in the tests, I've found that the easier way to debug why is to inspect the result object of the result = cookies.bake() statement with pdb. It has an exception method with lineno argument and source . With that information I've been able to locate the failing line. It also has a filename attribute but it doesn't seem to work for me. References \u2691 Git Docs","title":"cookiecutter"},{"location":"linux/cookiecutter/#install","text":"pip install cookiecutter","title":"Install"},{"location":"linux/cookiecutter/#use","text":"DEPRECATION: use cruft instead You may want to use cruft to generate your templates instead, as it will help you maintain the project with the template updates. Something that it's not easy with cookiecutter alone cookiecutter {{ path_or_url_to_cookiecutter_template }}","title":"Use"},{"location":"linux/cookiecutter/#user-config","text":"If you use Cookiecutter a lot, you\u2019ll find it useful to have a user config file. By default Cookiecutter tries to retrieve settings from a .cookiecutterrc file in your home directory. Example user config: default_context : full_name : \"Audrey Roy\" email : \"audreyr@example.com\" github_username : \"audreyr\" cookiecutters_dir : \"/home/audreyr/my-custom-cookiecutters-dir/\" replay_dir : \"/home/audreyr/my-custom-replay-dir/\" abbreviations : python : https://github.com/audreyr/cookiecutter-pypackage.git gh : https://github.com/{0}.git bb : https://bitbucket.org/{0} Possible settings are: default_context A list of key/value pairs that you want injected as context whenever you generate a project with Cookiecutter. These values are treated like the defaults in cookiecutter.json , upon generation of any project. cookiecutters_dir Directory where your cookiecutters are cloned to when you use Cookiecutter with a repo argument. replay_dir Directory where Cookiecutter dumps context data to, which you can fetch later on when using the replay feature. abbreviations A list of abbreviations for cookiecutters. Abbreviations can be simple aliases for a repo name, or can be used as a prefix, in the form abbr:suffix . Any suffix will be inserted into the expansion in place of the text {0} , using standard Python string formatting. With the above aliases, you could use the cookiecutter-pypackage template simply by saying cookiecutter python .","title":"User config"},{"location":"linux/cookiecutter/#write-your-own-cookietemplates","text":"","title":"Write your own cookietemplates"},{"location":"linux/cookiecutter/#create-files-or-directories-with-conditions","text":"For files use a filename like '{{ \".vault_pass.sh\" if cookiecutter.vault_pass_entry != \"None\" else \"\" }}' . For directories I haven't yet found a nice way to do it (as the above will fail), check the issue or the hooks documentation for more information. File: post_gen_project.py import os import sys REMOVE_PATHS = [ '{ % i f cookiecutter.packaging != \"pip\" %} requirements.txt { % e ndif %}' , '{ % i f cookiecutter.packaging != \"poetry\" %} poetry.lock { % e ndif %}' , ] for path in REMOVE_PATHS : path = path . strip () if path and os . path . exists ( path ): if os . path . isdir ( path ): os . rmdir ( path ) else : os . unlink ( path )","title":"Create files or directories with conditions"},{"location":"linux/cookiecutter/#add-some-text-to-a-file-if-a-condition-is-met","text":"Use jinja2 conditionals. Note the - at the end of the conditional opening, play with {%- ... -%} and {% ... %} for different results on line appending. { % if cookiecutter.install_docker == 'yes' -% } - src : git+ssh://mywebpage.org/ansible-roles/docker.git version : 1.0.3 { % - else -% } - src : git+ssh://mywebpage.org/ansible-roles/other-role.git version : 1.0.2 { % - endif % }","title":"Add some text to a file if a condition is met"},{"location":"linux/cookiecutter/#initialize-git-repository-on-the-created-cookiecutter","text":"Added the following to the post generation hooks. File: hooks/post_gen_project.py import subprocess subprocess . call ([ 'git' , 'init' ]) subprocess . call ([ 'git' , 'add' , '*' ]) subprocess . call ([ 'git' , 'commit' , '-m' , 'Initial commit' ])","title":"Initialize git repository on the created cookiecutter"},{"location":"linux/cookiecutter/#prevent-cookiecutter-from-processing-some-files","text":"By default cookiecutter will try to process every file as a Jinja template. This behaviour produces wrong results if you have Jinja templates that are meant to be taken as literal. Starting with cookiecutter 1.1, you can tell cookiecutter to only copy some files without interpreting them as Jinja templates . Add a _copy_without_render key in the cookiecutter config file ( cookiecutter.json ). It takes a list of regular expressions. If a filename matches the regular expressions it will be copied and not processed as a Jinja template. { \"project_slug\" : \"sample\" , \"_copy_without_render\" : [ \"*.js\" , \"not_rendered_dir/*\" , \"rendered_dir/not_rendered_file.ini\" ] }","title":"Prevent cookiecutter from processing some files"},{"location":"linux/cookiecutter/#prevent-additional-whitespaces-when-jinja-condition-is-not-met","text":"Jinja2 has a whitespace control that can be used to manage the whitelines existent between the Jinja blocks. The problem comes when a condition is not met in an if block, in that case, Jinja adds a whitespace which will break most linters. This is the solution I've found out that works as expected. ### Multienvironment This playbook has support for the following environments: {% if cookiecutter.production_environment == \"True\" -%} * Production {% endif %} {%- if cookiecutter.staging_environment == \"True\" -%} * Staging {% endif %} {%- if cookiecutter.development_environment == \"True\" -%} * Development {% endif %} ### Tags","title":"Prevent additional whitespaces when jinja condition is not met."},{"location":"linux/cookiecutter/#testing-your-own-cookiecutter-templates","text":"The pytest-cookies plugin comes with a cookies fixture which is a wrapper for the cookiecutter API for generating projects. It helps you verify that your template is working as expected and takes care of cleaning up after running the tests.","title":"Testing your own cookiecutter templates"},{"location":"linux/cookiecutter/#install_1","text":"pip install pytest-cookies","title":"Install"},{"location":"linux/cookiecutter/#usage","text":"@pytest.fixture def context(): return { \"playbook_name\": \"My Test Playbook\", } The cookies.bake() method generates a new project from your template based on the default values specified in cookiecutter.json: def test_bake_project ( cookies ): result = cookies . bake ( extra_context = { 'repo_name' : 'hello world' }) assert result . exit_code == 0 assert result . exception is None assert result . project . basename == 'hello world' assert result . project . isdir () It accepts the extra_context keyword argument that is passed to cookiecutter. The given dictionary will override the default values of the template context, allowing you to test arbitrary user input data. The cookiecutter-django has a nice test file using this fixture.","title":"Usage"},{"location":"linux/cookiecutter/#mocking-the-contents-of-the-cookiecutter-hooks","text":"Sometimes it's interesting to add interactions with external services in the cookiecutter hooks, for example to activate a CI pipeline. If you want to test the cookiecutter template you need to mock those external interactions. But it's difficult to mock the contents of the hooks because their contents aren't run by the cookies.bake() code. Instead it delegates in cookiecutter to run them, which opens a subprocess to run them , so the mocks don't work. The alternative is setting an environmental variable in your tests to skip those steps: File: tests/conftest.py import os os . environ [ \"COOKIECUTTER_TESTING\" ] = \"true\" File: hooks/pre_gen_project.py def main (): # ... pre_hook content ... if __name__ == \"__main__\" : if os . environ . get ( \"COOKIECUTTER_TESTING\" ) != \"true\" : main () If you want to test the content of main , you can now mock each of the external interactions. But you'll face the problem that these files are jinja2 templates of python files, so it's tricky to test them, due to syntax errors.","title":"Mocking the contents of the cookiecutter hooks"},{"location":"linux/cookiecutter/#debug-failing-template-generation","text":"Sometimes the generation of the templates will fail in the tests, I've found that the easier way to debug why is to inspect the result object of the result = cookies.bake() statement with pdb. It has an exception method with lineno argument and source . With that information I've been able to locate the failing line. It also has a filename attribute but it doesn't seem to work for me.","title":"Debug failing template generation"},{"location":"linux/cookiecutter/#references","text":"Git Docs","title":"References"},{"location":"linux/cruft/","text":"cruft allows you to maintain all the necessary boilerplate for packaging and building projects separate from the code you intentionally write. Fully compatible with existing Cookiecutter templates. Many project template utilities exist that automate the copying and pasting of code to create new projects. This seems great! However, once created, most leave you with that copy-and-pasted code to manage through the life of your project. Key Features \u2691 Cookiecutter Compatible cruft utilizes Cookiecutter as its template expansion engine. Meaning it retains full compatibility with all existing Cookiecutter templates. Template Validation cruft can quickly validate whether or not a project is using the latest version of a template using cruft check . This check can easily be added to CI pipelines to ensure your projects stay in-sync. Automatic Template Updates cruft automates the process of updating code to match the latest version of a template, making it easy to utilize template improvements across many projects. Installation \u2691 pip install cruft Usage \u2691 Creating a New Project \u2691 To create a new project using cruft run cruft create PROJECT_URL from the command line. cruft will then ask you any necessary questions to create your new project. It will use your answers to expand the provided template, and then return the directory it placed the expanded project. Behind the scenes, cruft uses Cookiecutter to do the project expansion. The only difference in the resulting output is a .cruft.json file that contains the git hash of the template used as well as the parameters specified. Updating a Project \u2691 To update an existing project, that was created using cruft, run cruft update in the root of the project. If there are any updates, cruft will have you review them before applying. If you accept the changes cruft will apply them to your project and update the .cruft.json file for you. Sometimes certain files just aren't good fits for updating. Such as test cases or __init__ files. You can tell cruft to always skip updating these files on a project by project basis by added them to a skip section within your .cruft.json file: { \"template\" : \"https://github.com/timothycrosley/cookiecutter-python\" , \"commit\" : \"8a65a360d51250221193ed0ec5ed292e72b32b0b\" , \"skip\" : [ \"cruft/__init__.py\" , \"tests\" ], ... } Or, if you have toml installed, you can add skip files directly to a tool.cruft section of your pyproject.toml file: [tool.cruft] skip = [\"cruft/__init__.py\", \"tests\"] Checking a Project \u2691 Checking to see if a project is missing a template update is as easy as running cruft check . If the project is out-of-date an error and exit code 1 will be returned. cruft check can be added to CI pipelines to ensure projects don't unintentionally drift. Linking an Existing Project \u2691 Have an existing project that you created from a template in the past using Cookiecutter directly? You can link it to the template that was used to create it using: cruft link TEMPLATE_REPOSITORY . You can then specify the last commit of the template the project has been updated to be consistent with, or accept the default of using the latest commit from the template. Compute the diff \u2691 With time, your boilerplate may end up being very different from the actual cookiecutter template. Cruft allows you to quickly see what changed in your local project compared to the template. It is as easy as running cruft diff . If any local file differs from the template, the diff will appear in your terminal in a similar fashion to git diff . The cruft diff command optionally accepts an --exit-code flag that will make cruft exit with a non-0 code should any diff is found. You can combine this flag with the skip section of your .cruft.json to make stricter CI checks that ensures any improvement to the template is always submitted upstream. References \u2691 Docs","title":"cruft"},{"location":"linux/cruft/#key-features","text":"Cookiecutter Compatible cruft utilizes Cookiecutter as its template expansion engine. Meaning it retains full compatibility with all existing Cookiecutter templates. Template Validation cruft can quickly validate whether or not a project is using the latest version of a template using cruft check . This check can easily be added to CI pipelines to ensure your projects stay in-sync. Automatic Template Updates cruft automates the process of updating code to match the latest version of a template, making it easy to utilize template improvements across many projects.","title":"Key Features"},{"location":"linux/cruft/#installation","text":"pip install cruft","title":"Installation"},{"location":"linux/cruft/#usage","text":"","title":"Usage"},{"location":"linux/cruft/#creating-a-new-project","text":"To create a new project using cruft run cruft create PROJECT_URL from the command line. cruft will then ask you any necessary questions to create your new project. It will use your answers to expand the provided template, and then return the directory it placed the expanded project. Behind the scenes, cruft uses Cookiecutter to do the project expansion. The only difference in the resulting output is a .cruft.json file that contains the git hash of the template used as well as the parameters specified.","title":"Creating a New Project"},{"location":"linux/cruft/#updating-a-project","text":"To update an existing project, that was created using cruft, run cruft update in the root of the project. If there are any updates, cruft will have you review them before applying. If you accept the changes cruft will apply them to your project and update the .cruft.json file for you. Sometimes certain files just aren't good fits for updating. Such as test cases or __init__ files. You can tell cruft to always skip updating these files on a project by project basis by added them to a skip section within your .cruft.json file: { \"template\" : \"https://github.com/timothycrosley/cookiecutter-python\" , \"commit\" : \"8a65a360d51250221193ed0ec5ed292e72b32b0b\" , \"skip\" : [ \"cruft/__init__.py\" , \"tests\" ], ... } Or, if you have toml installed, you can add skip files directly to a tool.cruft section of your pyproject.toml file: [tool.cruft] skip = [\"cruft/__init__.py\", \"tests\"]","title":"Updating a Project"},{"location":"linux/cruft/#checking-a-project","text":"Checking to see if a project is missing a template update is as easy as running cruft check . If the project is out-of-date an error and exit code 1 will be returned. cruft check can be added to CI pipelines to ensure projects don't unintentionally drift.","title":"Checking a Project"},{"location":"linux/cruft/#linking-an-existing-project","text":"Have an existing project that you created from a template in the past using Cookiecutter directly? You can link it to the template that was used to create it using: cruft link TEMPLATE_REPOSITORY . You can then specify the last commit of the template the project has been updated to be consistent with, or accept the default of using the latest commit from the template.","title":"Linking an Existing Project"},{"location":"linux/cruft/#compute-the-diff","text":"With time, your boilerplate may end up being very different from the actual cookiecutter template. Cruft allows you to quickly see what changed in your local project compared to the template. It is as easy as running cruft diff . If any local file differs from the template, the diff will appear in your terminal in a similar fashion to git diff . The cruft diff command optionally accepts an --exit-code flag that will make cruft exit with a non-0 code should any diff is found. You can combine this flag with the skip section of your .cruft.json to make stricter CI checks that ensures any improvement to the template is always submitted upstream.","title":"Compute the diff"},{"location":"linux/cruft/#references","text":"Docs","title":"References"},{"location":"linux/elasticsearch/","text":"Backup \u2691 It's better to use the curator tool Create snapshot \u2691 curl {{ url }} /_snapshot/ {{ backup_path }} / {{ snapshot_name }} ?wait_for_completion = true Create snapshot of selected indices \u2691 curl {{ url }} /_snapshot/ {{ backup_path }} / {{ snapshot_name }} ?wait_for_completion = true curl -XPUT 'localhost:9200/_snapshot/my_backup/snapshot_1?pretty' -H 'Content-Type: application/json' -d ' { \"indices\": \"index_1,index_2\", \"ignore_unavailable\": true, \"include_global_state\": false } ' List all backups \u2691 Check for my-snapshot-repo curl {{ url }} /_snapshot/ {{ backup_path }} /*?pretty Restore backup \u2691 First you need to close the selected indices curl -X POST {{ url }} / {{ indice_name }} /_close Then restore curl {{ url }} /_snapshot/ {{ backup_path }} / {{ snapshot_name }} /_restore?wait_for_completion = true If you want to restore only one index, use: curl -X POST \"{{ url }}/_snapshot/{{ backup_path }}/{{ snapshot_name }}/_restore?pretty\" -H 'Content-Type: application/json' -d ' { \"indices\": \"{{ index_to_restore }}\", }' Delete snapshot \u2691 curl -XDELETE {{ url }} /_snapshot/ {{ backup_path }} / {{ snapshot_name }} Delete snapshots older than X \u2691 File: curator.yml client : hosts : - 'a data node' port : 9200 url_prefix : use_ssl : False certificate : client_cert : client_key : ssl_no_validate : False http_auth : timeout : 30 master_only : False logging : loglevel : INFO logfile : D:\\CuratorLogs\\logs.txt logformat : default blacklist : [ 'elasticsearch' , 'urllib3' ] File: delete_old_snapshots.yml actions : 1 : action : delete_snapshots description : >- Delete snapshots from the selected repository older than 100 days (based on creation_date), for everything but 'citydirectory-' prefixed snapshots. options : repository : 'dcs-elastic-snapshot' disable_action : False filters : - filtertype : pattern kind : prefix value : citydirectory- exclude : True - filtertype : age source : creation_date direction : older unit : days unit_count : 100 Information gathering \u2691 Get status of cluster \u2691 curl {{ url }} /_cluster/health?pretty curl {{ url }} /_cat/nodes?v curl {{ url }} /_cat/indices?v curl {{ url }} /_cat/shards If you've got red status, use the following command to choose the first unassigned shard that it finds and explains why it cannot be allocated to a node. curl {{ url }} /_cluster/allocation/explain?v Get settings \u2691 curl {{ url }} /_settings Get space left \u2691 curl {{ url }} /_nodes/stats/fs?pretty List plugins \u2691 curl {{ url }} /_nodes/plugins?pretty Upload \u2691 Single data upload \u2691 curl -XPOST '{{ url }}/{{ path_to_table }}' -d '{{ json_input }}' where json_input can be { \"field\" : \"value\" } Bulk upload of data \u2691 curl -H 'Content-Type: application/x-ndjson' -XPOST \\ '{{ url }}/{{ path_to_table }}/_bulk?pretty' --data-binary @ {{ json_file }} Delete \u2691 Delete data \u2691 curl -XDELETE {{ url }} / {{ path_to_ddbb }} Reindex an index \u2691 If you encountered errors while reindexing source_index to destination_index it can be because the cluster hit a timeout on the scroll locks. As a work around, you can increase the timeout period to a reasonable value and then reindex. The default AWS values are search context of 5 minutes, socket timeout of 30 seconds, and batch size of 1,000. First clear the cache of the index with: curl -X POST https://elastic.url/destination_index/_cache/clear If the index is big, they suggest to disable replicas in your destination index by setting number_of_replicas to 0 and re-enable them once the reindex process is complete. To get the current state use: curl https://elastic.url/destination_index/_settings Then disable the replicas with: curl -X PUT \\ https://elastic.url/destination_index \\ -H 'Content-Type: application/json' \\ -d ' { \"settings\" : { \"refresh_interval\" : -1, \"number_of_replicas\" : 0 }} Now you can reindex the index with: curl -X POST \\ https://elastic.url/_reindex?wait_for_completion = false \\& timeout = 10m \\& scroll = 10h \\& pretty = true \\ -H 'Content-Type: application/json' \\ -d '{\"source\": { \"remote\": { \"host\": \"https://elastic.url:443\", \"socket_timeout\": \"60m\" }, \"index\": \"source_index\" }, \"dest\": {\"index\": \"destination_index\"}}' And check the evolution of the task with: curl 'https://elastic.url/_tasks?detailed=true&actions=*reindex&group_by=parents&pretty=true' The output is quite verbose, so I use vimdiff to see the differences between instant states. If you see there are no tasks running, check the indices status to see if the reindex ended well. curl https://elastic.url/_cat/indices After the reindex process is complete, you can reset your desired replica count and remove the refresh interval setting. KNN \u2691 KNN sizing \u2691 Typically, in an Elasticsearch cluster, a certain portion of RAM is set aside for the JVM heap. The k-NN plugin allocates graphs to a portion of the remaining RAM. This portion\u2019s size is determined by the circuit_breaker_limit cluster setting. By default, the circuit breaker limit is set at 50%. The memory required for graphs is estimated to be 1.1 * (4 * dimension + 8 * M) bytes/vector. To get the dimension and m use the /index elasticsearch endpoint. To get the number of vectors, use /index/_count . The number of vectors is the same as the number of documents. As an example, assume that we have 1 Million vectors with a dimension of 256 and M of 16, and the memory required can be estimated as: 1.1 * (4 *256 + 8 * 16) * 1,000,000 ~= 1.26 GB Remember that having a replica will double the total number of vectors. I've seen some queries work with indices that required 120% of the available memory for the KNN. A good way to see if it fits, is warming up the knn vectors . If the process returns a timeout, you probably don't have enough memory. KNN warmup \u2691 The Hierarchical Navigable Small World (HNSW) graphs that are used to perform an approximate k-Nearest Neighbor (k-NN) search are stored as .hnsw files with other Apache Lucene segment files. In order for you to perform a search on these graphs using the k-NN plugin, these files need to be loaded into native memory. If the plugin has not loaded the graphs into native memory, it loads them when it receives a search request. This loading time can cause high latency during initial queries. To avoid this situation, users often run random queries during a warmup period. After this warmup period, the graphs are loaded into native memory and their production workloads can begin. This loading process is indirect and requires extra effort. As an alternative, you can avoid this latency issue by running the k-NN plugin warmup API operation on whatever indices you\u2019re interested in searching. This operation loads all the graphs for all of the shards (primaries and replicas) of all the indices specified in the request into native memory. After the process finishes, you can start searching against the indices with no initial latency penalties. The warmup API operation is idempotent, so if a segment\u2019s graphs are already loaded into memory, this operation has no impact on those graphs. It only loads graphs that aren\u2019t currently in memory. This request performs a warmup on three indices: GET /_opendistro/_knn/warmup/index1,index2,index3?pretty { \"_shards\" : { \"total\" : 6, \"successful\" : 6, \"failed\" : 0 } } total indicates how many shards the k-NN plugin attempted to warm up. The response also includes the number of shards the plugin succeeded and failed to warm up. The call does not return until the warmup operation is complete or the request times out. If the request times out, the operation still continues on the cluster. To monitor the warmup operation, use the Elasticsearch _tasks API: GET /_tasks Troubleshooting \u2691 Deal with the AWS service timeout \u2691 AWS' Elasticsearch service is exposed behind a load balancer that returns a timeout after 300 seconds. If the query you're sending takes longer you won't be able to retrieve the information. You can consider using Asynchronous search which requires Elasticsearch 7.10 or later. Asynchronous search lets you run search requests that run in the background. You can monitor the progress of these searches and get back partial results as they become available. After the search finishes, you can save the results to examine at a later time. If the query you're running is a KNN one, you can try: Using the knn warmup api before running initial queries. Scaling up the instances: Amazon ES uses half of an instance's RAM for the Java heap (up to a heap size of 32 GiB). By default, KNN uses up to 50% of the remaining half, so an instance type with 64 GiB of RAM can accommodate 16 GiB of graphs (64 * 0.5 * 0.5). Performance can suffer if graph memory usage exceeds this value. In a less recommended approach, you can make more percentage of memory available for KNN operations. Open Distro for Elasticsearch lets you modify all KNN settings using the _cluster/settings API. On Amazon ES, you can change all settings except knn.memory.circuit_breaker.enabled and knn.circuit_breaker.triggered . You can change the circuit breaker settings as: PUT /_cluster/settings { \"persistent\" : { \"knn.memory.circuit_breaker.limit\" : \"<value%>\" } } You could also do performance tuning your KNN request . Fix Circuit breakers triggers \u2691 The elasticsearch_exporter has a elasticsearch_breakers_tripped metric, which counts then number of Circuit Breakers triggered of the different kinds. The Grafana dashboard paints a count of all the triggers with a big red number, which may scare you at first. Lets first understand what are Circuit Breakers. Elasticsearch is built with Java and as such depends on the JVM heap for many operations and caching purposes. By default in AWS, each data node is assigned half the RAM to be used for heap for ES. In Elasticsearch the default Garbage Collector is Concurrent-Mark and Sweep (CMS). When the JVM Memory Pressure reaches 75%, this collector pauses some threads and attempts to reclaim some heap space. High heap usage occurs when the garbage collection process cannot keep up. An indicator of high heap usage is when the garbage collection is incapable of reducing the heap usage to around 30%. When a request reaches the ES nodes, circuit breakers estimate the amount of memory needed to load the required data. The cluster then compares the estimated size with the configured heap size limit. If the estimated size of your data is greater than the available heap size, the query is terminated. As a result, a CircuitBreakerException is thrown to prevent overloading the node. In essence, these breakers are present to prevent a request overloading a data node and consuming more heap space than that node can provide at that time. If these breakers weren't present, then the request will use up all the heap that the node can provide and this node will then restart due to OOM. Lets assume a data node has 16GB heap configured, When the parent circuit breaker is tripped, then a similar error is thrown: \"error\" : { \"root_cause\" : [ { \"type\" : \"circuit_breaking_exception\" , \"reason\" : \"[parent] Data too large, data for [<HTTP_request>] would be [16355096754/15.2gb], which is larger than the limit of [16213167308/15gb], real usage: [15283269136/14.2gb], new bytes reserved: [1071827618/1022.1mb]\" , } ] } The parent circuit breaker (a circuit breaker type) is responsible for the overall memory usage of your Elasticsearch cluster. When a parent circuit breaker exception occurs, the total memory used across all circuit breakers has exceeded the set limit. A parent breaker throws an exception when the cluster exceeds 95% of 16 GB, which is 15.2 GB of heap (in above example). A circuit breaking exception is generally caused by high JVM. When the JVM Memory Pressure is high, it indicates that a large portion of one or more data nodes configured heap is currently being used heavily, and as such, the frequency of the circuit breakers being tripped increases as there is not enough heap available at the time to process concurrent smaller or larger requests. It is worth noting that that the error can also be thrown by a certain request that would just consume all the available heap on a certain data node at the time such as an intensive search query. If you see numerous spikes to the high 90%, with occasionally spikes to 100%, it's not uncommon for the parent circuit breaker to be tripped in response to requests. To troubleshoot circuit breakers, you'll then have to address the High JVM issues, which can be caused by: Increase in the number of requests to the cluster. Check the IndexRate and SearchRate metrics in to determine your current load. Aggregation, wildcards, and using wide time ranges in your queries. Unbalanced shard allocation across nodes or too many shards in a cluster. Index mapping explosions. Using the fielddata data structure to query data. Fielddata can consume a large amount of heap space, and remains in the heap for the lifetime of a segment. As a result, JVM memory pressure remains high on the cluster when fielddata is used. Here's what happens as JVM memory pressure increases in AWS: At 75%: Amazon ES triggers the Concurrent Mark Sweep (CMS) garbage collector. The CMS collector runs alongside other processes to keep pauses and disruptions to a minimum. The garbage collection is a CPU-intensive process. If JVM memory pressure stays at this percentage for a few minutes, then you could encounter ClusterBlockException, JVM OutOfMemoryError, or other cluster performance issues. Above 75%: If the CMS collector fails to reclaim enough memory and usage remains above 75%, Amazon ES triggers a different garbage collection algorithm. This algorithm tries to free up memory and prevent a JVM OutOfMemoryError (OOM) exception by slowing or stopping processes. Above 92% for 30 minutes: Amazon ES blocks all write operations. Around 95%: Amazon ES kills processes that try to allocate memory. If a critical process is killed, one or more cluster nodes might fail. At 100%: Amazon ES JVM is configured to exit and eventually restarts on OutOfMemory (OOM). To resolve high JVM memory pressure, try the following tips: Reduce incoming traffic to your cluster, especially if you have a heavy workload. Consider scaling the cluster to obtain more JVM memory to support your workload. As mentioned above each data node gets half the RAM allocated to be used as Heap. Consider scaling to a data node type with more RAM and hence more Available Heap. Thereby increasing the parent circuit breaker limit. If cluster scaling isn't possible, try reducing the number of shards by deleting old or unused indices. Because shard metadata is stored in memory, reducing the number of shards can reduce overall memory usage. Enable slow logs to identify faulty requests. Note: Before enabling configuration changes, verify that JVM memory pressure is below 85%. This way, you can avoid additional overhead to existing resources. Optimize search and indexing requests, and choose the correct number of shards. Disable and avoid using fielddata. By default, fielddata is set to \"false\" on a text field unless it's explicitly defined as otherwise in index mappings. Field data is a potentially a huge consumer of JVM Heap space. This build up of field data occurs when aggregations are run on fields that are of type text . More on how you can periodically clear field data below. Change your index mapping type to a keyword , using reindex API. You can use the keyword type as an alternative for performing aggregations and sorting on text fields. As mentioned in above point, by aggregating on keyword type instead of text , no field data has to be built on demand and hence won't consume precious heap space. Look into the commonly aggregated fields in index mappings and ensure they are not of type text . If they are, you can consider changing them to keyword . You will have to create a new index with the desired mapping and then use the Reindex API to transfer over the documents from the source index to the new index. Once Re-index has completed then you can delete the old index. Avoid aggregating on text fields to prevent increases in field data. When you use more field data, more heap space is consumed. Use the cluster stats API operation to check your field data. Clear the fielddata cache with the following API call: POST /index_name/_cache/clear?fielddata=true (index-level cache) POST */_cache/clear?fielddata=true (cluster-level cache) Generally speaking, if you notice your workload (search rate and index rate) remaining consistent during these high spikes and non of the above optimizations can be applied or if they have already been applied and the JVM is still high during these workload times, then it is an indication that the cluster needs to be scaled in terms of JVM resources to cope with this workload. You can't reset the 'tripped' count. This is a Node level metric and thus will be reset to 0 when the Elasticsearch Service is restarted on that Node. Since in AWS it's a managed service, unfortunately you will not have access to the underlaying EC2 instance to restart the ES Process. However the ES Process can be restarted on your end (on all nodes) in the following ways: Initiate a Configuration Change that causes a blue/green deployment : When you initiate a configuration change, a subsequent blue/green deployment process is launched in which we launch a new fleet that matches the desired configuration. The old fleet continues to run and serve requests. Simultaneously, data in the form of shards are then migrated from the old fleet to the new fleet. Once all this data has been migrated the old fleet is terminated and the new one takes over. During this process ES is restarted on the Nodes. Ensure that CPU Utilization and JVM Memory Pressure are below the recommended 80% thresholds to prevent any issues with this process as it uses clusters resources to initiate and complete. You can scale the EBS Volumes attached to the data nodes by an arbitrary amount such as 1GB, wait for the blue/green to complete and then scale it back. Wait for a new service software release and update the service software of the Cluster. This will also cause a blue/green and hence ES process will be restarted on the nodes. Recover from yellow state \u2691 A yellow cluster represents that some of the replica shards in the cluster are unassigned. I can see that around 14 replica shards are unassigned. You can confirm the state of the cluster with the following commands curl <domain-endpoint>_cluster/health?pretty curl -X GET <domain-endpoint>/_cat/shards | grep UNASSIGNED curl -X GET <domain-endpoint>/_cat/indices | grep yellow If you have metrics of the JVMMemoryPressure of the nodes, check if the memory of a node reached 100% around the time the cluster reached yellow state. One can generally confirm the reason for a cluster going yellow by looking at the output of the following API call: curl -X GET <domain-endpoint>/_cluster/allocation/explain | jq If it shows a CircuitBreakerException , it confirms that a spike in the JVM metric caused the node to go down. Check the Fix Circuit breaker triggers section above to see how to solve that case. Reallocate unassigned shards \u2691 Elasticsearch makes 5 attempts to assign the shard but if it fails to be assigned after 5 attempts, the shards will remain unassigned. There is a solution to this issue in order to bring the cluster to green state. You can disable the replicas on the failing index and then enable replicas back. Disable Replica curl -X PUT \"<ES_endpoint>/<index_name>/_settings\" -H 'Content-Type: application/json' -d ' { \"index\" : { \"number_of_replicas\" : 0 } }' * Enable the Replica back: curl -X PUT \"<ES_endpoint>/<index_name>/_settings\" -H 'Content-Type: application/json' -d ' { \"index\" : { \"number_of_replicas\" : 1 } }' Please note that it will take some time for the shards to be completely assigned and hence you might see intermittent cluster status as YELLOW.","title":"elasticsearch"},{"location":"linux/elasticsearch/#backup","text":"It's better to use the curator tool","title":"Backup"},{"location":"linux/elasticsearch/#create-snapshot","text":"curl {{ url }} /_snapshot/ {{ backup_path }} / {{ snapshot_name }} ?wait_for_completion = true","title":"Create snapshot"},{"location":"linux/elasticsearch/#create-snapshot-of-selected-indices","text":"curl {{ url }} /_snapshot/ {{ backup_path }} / {{ snapshot_name }} ?wait_for_completion = true curl -XPUT 'localhost:9200/_snapshot/my_backup/snapshot_1?pretty' -H 'Content-Type: application/json' -d ' { \"indices\": \"index_1,index_2\", \"ignore_unavailable\": true, \"include_global_state\": false } '","title":"Create snapshot of selected indices"},{"location":"linux/elasticsearch/#list-all-backups","text":"Check for my-snapshot-repo curl {{ url }} /_snapshot/ {{ backup_path }} /*?pretty","title":"List all backups"},{"location":"linux/elasticsearch/#restore-backup","text":"First you need to close the selected indices curl -X POST {{ url }} / {{ indice_name }} /_close Then restore curl {{ url }} /_snapshot/ {{ backup_path }} / {{ snapshot_name }} /_restore?wait_for_completion = true If you want to restore only one index, use: curl -X POST \"{{ url }}/_snapshot/{{ backup_path }}/{{ snapshot_name }}/_restore?pretty\" -H 'Content-Type: application/json' -d ' { \"indices\": \"{{ index_to_restore }}\", }'","title":"Restore backup"},{"location":"linux/elasticsearch/#delete-snapshot","text":"curl -XDELETE {{ url }} /_snapshot/ {{ backup_path }} / {{ snapshot_name }}","title":"Delete snapshot"},{"location":"linux/elasticsearch/#delete-snapshots-older-than-x","text":"File: curator.yml client : hosts : - 'a data node' port : 9200 url_prefix : use_ssl : False certificate : client_cert : client_key : ssl_no_validate : False http_auth : timeout : 30 master_only : False logging : loglevel : INFO logfile : D:\\CuratorLogs\\logs.txt logformat : default blacklist : [ 'elasticsearch' , 'urllib3' ] File: delete_old_snapshots.yml actions : 1 : action : delete_snapshots description : >- Delete snapshots from the selected repository older than 100 days (based on creation_date), for everything but 'citydirectory-' prefixed snapshots. options : repository : 'dcs-elastic-snapshot' disable_action : False filters : - filtertype : pattern kind : prefix value : citydirectory- exclude : True - filtertype : age source : creation_date direction : older unit : days unit_count : 100","title":"Delete snapshots older than X"},{"location":"linux/elasticsearch/#information-gathering","text":"","title":"Information gathering"},{"location":"linux/elasticsearch/#get-status-of-cluster","text":"curl {{ url }} /_cluster/health?pretty curl {{ url }} /_cat/nodes?v curl {{ url }} /_cat/indices?v curl {{ url }} /_cat/shards If you've got red status, use the following command to choose the first unassigned shard that it finds and explains why it cannot be allocated to a node. curl {{ url }} /_cluster/allocation/explain?v","title":"Get status of cluster"},{"location":"linux/elasticsearch/#get-settings","text":"curl {{ url }} /_settings","title":"Get settings"},{"location":"linux/elasticsearch/#get-space-left","text":"curl {{ url }} /_nodes/stats/fs?pretty","title":"Get space left"},{"location":"linux/elasticsearch/#list-plugins","text":"curl {{ url }} /_nodes/plugins?pretty","title":"List plugins"},{"location":"linux/elasticsearch/#upload","text":"","title":"Upload"},{"location":"linux/elasticsearch/#single-data-upload","text":"curl -XPOST '{{ url }}/{{ path_to_table }}' -d '{{ json_input }}' where json_input can be { \"field\" : \"value\" }","title":"Single data upload"},{"location":"linux/elasticsearch/#bulk-upload-of-data","text":"curl -H 'Content-Type: application/x-ndjson' -XPOST \\ '{{ url }}/{{ path_to_table }}/_bulk?pretty' --data-binary @ {{ json_file }}","title":"Bulk upload of data"},{"location":"linux/elasticsearch/#delete","text":"","title":"Delete"},{"location":"linux/elasticsearch/#delete-data","text":"curl -XDELETE {{ url }} / {{ path_to_ddbb }}","title":"Delete data"},{"location":"linux/elasticsearch/#reindex-an-index","text":"If you encountered errors while reindexing source_index to destination_index it can be because the cluster hit a timeout on the scroll locks. As a work around, you can increase the timeout period to a reasonable value and then reindex. The default AWS values are search context of 5 minutes, socket timeout of 30 seconds, and batch size of 1,000. First clear the cache of the index with: curl -X POST https://elastic.url/destination_index/_cache/clear If the index is big, they suggest to disable replicas in your destination index by setting number_of_replicas to 0 and re-enable them once the reindex process is complete. To get the current state use: curl https://elastic.url/destination_index/_settings Then disable the replicas with: curl -X PUT \\ https://elastic.url/destination_index \\ -H 'Content-Type: application/json' \\ -d ' { \"settings\" : { \"refresh_interval\" : -1, \"number_of_replicas\" : 0 }} Now you can reindex the index with: curl -X POST \\ https://elastic.url/_reindex?wait_for_completion = false \\& timeout = 10m \\& scroll = 10h \\& pretty = true \\ -H 'Content-Type: application/json' \\ -d '{\"source\": { \"remote\": { \"host\": \"https://elastic.url:443\", \"socket_timeout\": \"60m\" }, \"index\": \"source_index\" }, \"dest\": {\"index\": \"destination_index\"}}' And check the evolution of the task with: curl 'https://elastic.url/_tasks?detailed=true&actions=*reindex&group_by=parents&pretty=true' The output is quite verbose, so I use vimdiff to see the differences between instant states. If you see there are no tasks running, check the indices status to see if the reindex ended well. curl https://elastic.url/_cat/indices After the reindex process is complete, you can reset your desired replica count and remove the refresh interval setting.","title":"Reindex an index"},{"location":"linux/elasticsearch/#knn","text":"","title":"KNN"},{"location":"linux/elasticsearch/#knn-sizing","text":"Typically, in an Elasticsearch cluster, a certain portion of RAM is set aside for the JVM heap. The k-NN plugin allocates graphs to a portion of the remaining RAM. This portion\u2019s size is determined by the circuit_breaker_limit cluster setting. By default, the circuit breaker limit is set at 50%. The memory required for graphs is estimated to be 1.1 * (4 * dimension + 8 * M) bytes/vector. To get the dimension and m use the /index elasticsearch endpoint. To get the number of vectors, use /index/_count . The number of vectors is the same as the number of documents. As an example, assume that we have 1 Million vectors with a dimension of 256 and M of 16, and the memory required can be estimated as: 1.1 * (4 *256 + 8 * 16) * 1,000,000 ~= 1.26 GB Remember that having a replica will double the total number of vectors. I've seen some queries work with indices that required 120% of the available memory for the KNN. A good way to see if it fits, is warming up the knn vectors . If the process returns a timeout, you probably don't have enough memory.","title":"KNN sizing"},{"location":"linux/elasticsearch/#knn-warmup","text":"The Hierarchical Navigable Small World (HNSW) graphs that are used to perform an approximate k-Nearest Neighbor (k-NN) search are stored as .hnsw files with other Apache Lucene segment files. In order for you to perform a search on these graphs using the k-NN plugin, these files need to be loaded into native memory. If the plugin has not loaded the graphs into native memory, it loads them when it receives a search request. This loading time can cause high latency during initial queries. To avoid this situation, users often run random queries during a warmup period. After this warmup period, the graphs are loaded into native memory and their production workloads can begin. This loading process is indirect and requires extra effort. As an alternative, you can avoid this latency issue by running the k-NN plugin warmup API operation on whatever indices you\u2019re interested in searching. This operation loads all the graphs for all of the shards (primaries and replicas) of all the indices specified in the request into native memory. After the process finishes, you can start searching against the indices with no initial latency penalties. The warmup API operation is idempotent, so if a segment\u2019s graphs are already loaded into memory, this operation has no impact on those graphs. It only loads graphs that aren\u2019t currently in memory. This request performs a warmup on three indices: GET /_opendistro/_knn/warmup/index1,index2,index3?pretty { \"_shards\" : { \"total\" : 6, \"successful\" : 6, \"failed\" : 0 } } total indicates how many shards the k-NN plugin attempted to warm up. The response also includes the number of shards the plugin succeeded and failed to warm up. The call does not return until the warmup operation is complete or the request times out. If the request times out, the operation still continues on the cluster. To monitor the warmup operation, use the Elasticsearch _tasks API: GET /_tasks","title":"KNN warmup"},{"location":"linux/elasticsearch/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"linux/elasticsearch/#deal-with-the-aws-service-timeout","text":"AWS' Elasticsearch service is exposed behind a load balancer that returns a timeout after 300 seconds. If the query you're sending takes longer you won't be able to retrieve the information. You can consider using Asynchronous search which requires Elasticsearch 7.10 or later. Asynchronous search lets you run search requests that run in the background. You can monitor the progress of these searches and get back partial results as they become available. After the search finishes, you can save the results to examine at a later time. If the query you're running is a KNN one, you can try: Using the knn warmup api before running initial queries. Scaling up the instances: Amazon ES uses half of an instance's RAM for the Java heap (up to a heap size of 32 GiB). By default, KNN uses up to 50% of the remaining half, so an instance type with 64 GiB of RAM can accommodate 16 GiB of graphs (64 * 0.5 * 0.5). Performance can suffer if graph memory usage exceeds this value. In a less recommended approach, you can make more percentage of memory available for KNN operations. Open Distro for Elasticsearch lets you modify all KNN settings using the _cluster/settings API. On Amazon ES, you can change all settings except knn.memory.circuit_breaker.enabled and knn.circuit_breaker.triggered . You can change the circuit breaker settings as: PUT /_cluster/settings { \"persistent\" : { \"knn.memory.circuit_breaker.limit\" : \"<value%>\" } } You could also do performance tuning your KNN request .","title":"Deal with the AWS service timeout"},{"location":"linux/elasticsearch/#fix-circuit-breakers-triggers","text":"The elasticsearch_exporter has a elasticsearch_breakers_tripped metric, which counts then number of Circuit Breakers triggered of the different kinds. The Grafana dashboard paints a count of all the triggers with a big red number, which may scare you at first. Lets first understand what are Circuit Breakers. Elasticsearch is built with Java and as such depends on the JVM heap for many operations and caching purposes. By default in AWS, each data node is assigned half the RAM to be used for heap for ES. In Elasticsearch the default Garbage Collector is Concurrent-Mark and Sweep (CMS). When the JVM Memory Pressure reaches 75%, this collector pauses some threads and attempts to reclaim some heap space. High heap usage occurs when the garbage collection process cannot keep up. An indicator of high heap usage is when the garbage collection is incapable of reducing the heap usage to around 30%. When a request reaches the ES nodes, circuit breakers estimate the amount of memory needed to load the required data. The cluster then compares the estimated size with the configured heap size limit. If the estimated size of your data is greater than the available heap size, the query is terminated. As a result, a CircuitBreakerException is thrown to prevent overloading the node. In essence, these breakers are present to prevent a request overloading a data node and consuming more heap space than that node can provide at that time. If these breakers weren't present, then the request will use up all the heap that the node can provide and this node will then restart due to OOM. Lets assume a data node has 16GB heap configured, When the parent circuit breaker is tripped, then a similar error is thrown: \"error\" : { \"root_cause\" : [ { \"type\" : \"circuit_breaking_exception\" , \"reason\" : \"[parent] Data too large, data for [<HTTP_request>] would be [16355096754/15.2gb], which is larger than the limit of [16213167308/15gb], real usage: [15283269136/14.2gb], new bytes reserved: [1071827618/1022.1mb]\" , } ] } The parent circuit breaker (a circuit breaker type) is responsible for the overall memory usage of your Elasticsearch cluster. When a parent circuit breaker exception occurs, the total memory used across all circuit breakers has exceeded the set limit. A parent breaker throws an exception when the cluster exceeds 95% of 16 GB, which is 15.2 GB of heap (in above example). A circuit breaking exception is generally caused by high JVM. When the JVM Memory Pressure is high, it indicates that a large portion of one or more data nodes configured heap is currently being used heavily, and as such, the frequency of the circuit breakers being tripped increases as there is not enough heap available at the time to process concurrent smaller or larger requests. It is worth noting that that the error can also be thrown by a certain request that would just consume all the available heap on a certain data node at the time such as an intensive search query. If you see numerous spikes to the high 90%, with occasionally spikes to 100%, it's not uncommon for the parent circuit breaker to be tripped in response to requests. To troubleshoot circuit breakers, you'll then have to address the High JVM issues, which can be caused by: Increase in the number of requests to the cluster. Check the IndexRate and SearchRate metrics in to determine your current load. Aggregation, wildcards, and using wide time ranges in your queries. Unbalanced shard allocation across nodes or too many shards in a cluster. Index mapping explosions. Using the fielddata data structure to query data. Fielddata can consume a large amount of heap space, and remains in the heap for the lifetime of a segment. As a result, JVM memory pressure remains high on the cluster when fielddata is used. Here's what happens as JVM memory pressure increases in AWS: At 75%: Amazon ES triggers the Concurrent Mark Sweep (CMS) garbage collector. The CMS collector runs alongside other processes to keep pauses and disruptions to a minimum. The garbage collection is a CPU-intensive process. If JVM memory pressure stays at this percentage for a few minutes, then you could encounter ClusterBlockException, JVM OutOfMemoryError, or other cluster performance issues. Above 75%: If the CMS collector fails to reclaim enough memory and usage remains above 75%, Amazon ES triggers a different garbage collection algorithm. This algorithm tries to free up memory and prevent a JVM OutOfMemoryError (OOM) exception by slowing or stopping processes. Above 92% for 30 minutes: Amazon ES blocks all write operations. Around 95%: Amazon ES kills processes that try to allocate memory. If a critical process is killed, one or more cluster nodes might fail. At 100%: Amazon ES JVM is configured to exit and eventually restarts on OutOfMemory (OOM). To resolve high JVM memory pressure, try the following tips: Reduce incoming traffic to your cluster, especially if you have a heavy workload. Consider scaling the cluster to obtain more JVM memory to support your workload. As mentioned above each data node gets half the RAM allocated to be used as Heap. Consider scaling to a data node type with more RAM and hence more Available Heap. Thereby increasing the parent circuit breaker limit. If cluster scaling isn't possible, try reducing the number of shards by deleting old or unused indices. Because shard metadata is stored in memory, reducing the number of shards can reduce overall memory usage. Enable slow logs to identify faulty requests. Note: Before enabling configuration changes, verify that JVM memory pressure is below 85%. This way, you can avoid additional overhead to existing resources. Optimize search and indexing requests, and choose the correct number of shards. Disable and avoid using fielddata. By default, fielddata is set to \"false\" on a text field unless it's explicitly defined as otherwise in index mappings. Field data is a potentially a huge consumer of JVM Heap space. This build up of field data occurs when aggregations are run on fields that are of type text . More on how you can periodically clear field data below. Change your index mapping type to a keyword , using reindex API. You can use the keyword type as an alternative for performing aggregations and sorting on text fields. As mentioned in above point, by aggregating on keyword type instead of text , no field data has to be built on demand and hence won't consume precious heap space. Look into the commonly aggregated fields in index mappings and ensure they are not of type text . If they are, you can consider changing them to keyword . You will have to create a new index with the desired mapping and then use the Reindex API to transfer over the documents from the source index to the new index. Once Re-index has completed then you can delete the old index. Avoid aggregating on text fields to prevent increases in field data. When you use more field data, more heap space is consumed. Use the cluster stats API operation to check your field data. Clear the fielddata cache with the following API call: POST /index_name/_cache/clear?fielddata=true (index-level cache) POST */_cache/clear?fielddata=true (cluster-level cache) Generally speaking, if you notice your workload (search rate and index rate) remaining consistent during these high spikes and non of the above optimizations can be applied or if they have already been applied and the JVM is still high during these workload times, then it is an indication that the cluster needs to be scaled in terms of JVM resources to cope with this workload. You can't reset the 'tripped' count. This is a Node level metric and thus will be reset to 0 when the Elasticsearch Service is restarted on that Node. Since in AWS it's a managed service, unfortunately you will not have access to the underlaying EC2 instance to restart the ES Process. However the ES Process can be restarted on your end (on all nodes) in the following ways: Initiate a Configuration Change that causes a blue/green deployment : When you initiate a configuration change, a subsequent blue/green deployment process is launched in which we launch a new fleet that matches the desired configuration. The old fleet continues to run and serve requests. Simultaneously, data in the form of shards are then migrated from the old fleet to the new fleet. Once all this data has been migrated the old fleet is terminated and the new one takes over. During this process ES is restarted on the Nodes. Ensure that CPU Utilization and JVM Memory Pressure are below the recommended 80% thresholds to prevent any issues with this process as it uses clusters resources to initiate and complete. You can scale the EBS Volumes attached to the data nodes by an arbitrary amount such as 1GB, wait for the blue/green to complete and then scale it back. Wait for a new service software release and update the service software of the Cluster. This will also cause a blue/green and hence ES process will be restarted on the nodes.","title":"Fix Circuit breakers triggers"},{"location":"linux/elasticsearch/#recover-from-yellow-state","text":"A yellow cluster represents that some of the replica shards in the cluster are unassigned. I can see that around 14 replica shards are unassigned. You can confirm the state of the cluster with the following commands curl <domain-endpoint>_cluster/health?pretty curl -X GET <domain-endpoint>/_cat/shards | grep UNASSIGNED curl -X GET <domain-endpoint>/_cat/indices | grep yellow If you have metrics of the JVMMemoryPressure of the nodes, check if the memory of a node reached 100% around the time the cluster reached yellow state. One can generally confirm the reason for a cluster going yellow by looking at the output of the following API call: curl -X GET <domain-endpoint>/_cluster/allocation/explain | jq If it shows a CircuitBreakerException , it confirms that a spike in the JVM metric caused the node to go down. Check the Fix Circuit breaker triggers section above to see how to solve that case.","title":"Recover from yellow state"},{"location":"linux/elasticsearch/#reallocate-unassigned-shards","text":"Elasticsearch makes 5 attempts to assign the shard but if it fails to be assigned after 5 attempts, the shards will remain unassigned. There is a solution to this issue in order to bring the cluster to green state. You can disable the replicas on the failing index and then enable replicas back. Disable Replica curl -X PUT \"<ES_endpoint>/<index_name>/_settings\" -H 'Content-Type: application/json' -d ' { \"index\" : { \"number_of_replicas\" : 0 } }' * Enable the Replica back: curl -X PUT \"<ES_endpoint>/<index_name>/_settings\" -H 'Content-Type: application/json' -d ' { \"index\" : { \"number_of_replicas\" : 1 } }' Please note that it will take some time for the shards to be completely assigned and hence you might see intermittent cluster status as YELLOW.","title":"Reallocate unassigned shards"},{"location":"linux/fail2ban/","text":"Usage \u2691 Unban IP \u2691 fail2ban-client set {{ jail }} unbanip {{ ip }} Where jail can be ssh .","title":"fail2ban"},{"location":"linux/fail2ban/#usage","text":"","title":"Usage"},{"location":"linux/fail2ban/#unban-ip","text":"fail2ban-client set {{ jail }} unbanip {{ ip }} Where jail can be ssh .","title":"Unban IP"},{"location":"linux/google_chrome/","text":"Although I hate it, there are web pages that don't work on Firefox or Chromium. In those cases I install google-chrome and uninstall as soon as I don't need to use that service. Installation \u2691 Debian \u2691 wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb sudo apt install ./google-chrome-stable_current_amd64.deb","title":"google chrome"},{"location":"linux/google_chrome/#installation","text":"","title":"Installation"},{"location":"linux/google_chrome/#debian","text":"wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb sudo apt install ./google-chrome-stable_current_amd64.deb","title":"Debian"},{"location":"linux/haproxy/","text":"HAProxy is free, open source software that provides a high availability load balancer and proxy server for TCP and HTTP-based applications that spreads requests across multiple servers. It is written in C and has a reputation for being fast and efficient (in terms of processor and memory usage). Use HAProxy as a reverse proxy \u2691 reverse proxy is a type of proxy server that retrieves resources on behalf of a client from one or more servers. These resources are then returned to the client, appearing as if they originated from the server itself. Unlike a forward proxy, which is an intermediary for its associated clients to contact any server, a reverse proxy is an intermediary for its associated servers to be contacted by any client. In other words, a proxy is associated with the client(s), while a reverse proxy is associated with the server(s); a reverse proxy is usually an internal-facing proxy used as a 'front-end' to control and protect access to a server on a private network. It can be done at Web server level (Nginx, Apache, ...) or at load balancer level. This HAProxy post shows how to translate Apache's proxy pass directives to the HAProxy configuration. frontend ft_global acl host_dom.com req.hdr(Host) dom.com acl path_mirror_foo path -m beg /mirror/foo/ use_backend bk_myapp if host_dom.com path_mirror_foo backend bk_myapp [...] # external URL => internal URL # http://dom.com/mirror/foo/bar => http://bk.dom.com/bar # ProxyPass /mirror/foo/ http://bk.dom.com/bar http-request set-header Host bk.dom.com reqirep ^([^ :]*)\\ /mirror/foo/(.*) \\1\\ /\\2 # ProxyPassReverse /mirror/foo/ http://bk.dom.com/bar # Note: we turn the urls into absolute in the mean time acl hdr_location res.hdr(Location) -m found rspirep ^Location:\\ (https?://bk.dom.com(:[0-9]+)?)?(/.*) Location:\\ /mirror/foo3 if hdr_location # ProxyPassReverseCookieDomain bk.dom.com dom.com acl hdr_set_cookie_dom res.hdr(Set-cookie) -m sub Domain= bk.dom.com rspirep ^(Set-Cookie:.*)\\ Domain=bk.dom.com(.*) \\1\\ Domain=dom.com\\2 if hdr_set_cookie_dom # ProxyPassReverseCookieDomain / /mirror/foo/ acl hdr_set_cookie_path res.hdr(Set-cookie) -m sub Path= rspirep ^(Set-Cookie:.*)\\ Path=(.*) \\1\\ Path=/mirror/foo2 if hdr_set_cookie_path Other useful examples can be retrieved from drmalex07 or ferdinandosimonetti gists. References \u2691 Guidelines for HAProxy termination in AWS","title":"HAProxy"},{"location":"linux/haproxy/#use-haproxy-as-a-reverse-proxy","text":"reverse proxy is a type of proxy server that retrieves resources on behalf of a client from one or more servers. These resources are then returned to the client, appearing as if they originated from the server itself. Unlike a forward proxy, which is an intermediary for its associated clients to contact any server, a reverse proxy is an intermediary for its associated servers to be contacted by any client. In other words, a proxy is associated with the client(s), while a reverse proxy is associated with the server(s); a reverse proxy is usually an internal-facing proxy used as a 'front-end' to control and protect access to a server on a private network. It can be done at Web server level (Nginx, Apache, ...) or at load balancer level. This HAProxy post shows how to translate Apache's proxy pass directives to the HAProxy configuration. frontend ft_global acl host_dom.com req.hdr(Host) dom.com acl path_mirror_foo path -m beg /mirror/foo/ use_backend bk_myapp if host_dom.com path_mirror_foo backend bk_myapp [...] # external URL => internal URL # http://dom.com/mirror/foo/bar => http://bk.dom.com/bar # ProxyPass /mirror/foo/ http://bk.dom.com/bar http-request set-header Host bk.dom.com reqirep ^([^ :]*)\\ /mirror/foo/(.*) \\1\\ /\\2 # ProxyPassReverse /mirror/foo/ http://bk.dom.com/bar # Note: we turn the urls into absolute in the mean time acl hdr_location res.hdr(Location) -m found rspirep ^Location:\\ (https?://bk.dom.com(:[0-9]+)?)?(/.*) Location:\\ /mirror/foo3 if hdr_location # ProxyPassReverseCookieDomain bk.dom.com dom.com acl hdr_set_cookie_dom res.hdr(Set-cookie) -m sub Domain= bk.dom.com rspirep ^(Set-Cookie:.*)\\ Domain=bk.dom.com(.*) \\1\\ Domain=dom.com\\2 if hdr_set_cookie_dom # ProxyPassReverseCookieDomain / /mirror/foo/ acl hdr_set_cookie_path res.hdr(Set-cookie) -m sub Path= rspirep ^(Set-Cookie:.*)\\ Path=(.*) \\1\\ Path=/mirror/foo2 if hdr_set_cookie_path Other useful examples can be retrieved from drmalex07 or ferdinandosimonetti gists.","title":"Use HAProxy as a reverse proxy"},{"location":"linux/haproxy/#references","text":"Guidelines for HAProxy termination in AWS","title":"References"},{"location":"linux/hard_drive_health/","text":"Hard drives die, so we must be ready for that to happen. There are several solutions, such as using RAID to minimize the impact of a disk loss, but even then, we should monitor the bad sectors to see when are our disks dying. S.M.A.R.T (Self-Monitoring, Analysis and Reporting Technology; often written as SMART) is a monitoring system included in computer hard disk drives (HDDs), solid-state drives (SSDs), and eMMC drives. Its primary function is to detect and report various indicators of drive reliability with the intent of anticipating imminent hardware failures. Between all the SMART attributes, some that define define the health status of the hard drive, such as: Reallocated Sectors Count : Count of reallocated sectors. The raw value represents a count of the bad sectors that have been found and remapped. Thus, the higher the attribute value, the more sectors the drive has had to reallocate. This value is primarily used as a metric of the life expectancy of the drive; a drive which has had any reallocations at all is significantly more likely to fail in the immediate months. Spin Retry Count : Count of retry of spin start attempts. This attribute stores a total count of the spin start attempts to reach the fully operational speed (under the condition that the first attempt was unsuccessful). An increase of this attribute value is a sign of problems in the hard disk mechanical subsystem. Reallocate Event Count : Count of remap operations. The raw value of this attribute shows the total count of attempts to transfer data from reallocated sectors to a spare area. Both successful and unsuccessful attempts are counted. Current Pending Sector Count : Count of \"unstable\" sectors (waiting to be remapped, because of unrecoverable read errors). If an unstable sector is subsequently read successfully, the sector is remapped and this value is decreased. Read errors on a sector will not remap the sector immediately (since the correct value cannot be read and so the value to remap is not known, and also it might become readable later); instead, the drive firmware remembers that the sector needs to be remapped, and will remap it the next time it's written. However, some drives will not immediately remap such sectors when written; instead the drive will first attempt to write to the problem sector and if the write operation is successful then the sector will be marked good (in this case, the \"Reallocation Event Count\" (0xC4) will not be increased). This is a serious shortcoming, for if such a drive contains marginal sectors that consistently fail only after some time has passed following a successful write operation, then the drive will never remap these problem sectors. * Offline Uncorrectable Sector Count : The total count of uncorrectable errors when reading/writing a sector. A rise in the value of this attribute indicates defects of the disk surface and/or problems in the mechanical subsystem. Check the warranty status \u2691 If your drive is still under warranty from the manufacturer you may consider RMA\u2019ing the drive (initiating a warranty return process). Seagate Warranty Check Western Digital (WD) Warranty Check HGST Warranty Check Toshiba Warranty Check Wipe all the disk \u2691 Sometimes the CurrentPendingSector doesn't get reallocated, if you don't mind about the data in the disk, you can wipe it all with: dd if = /dev/zero of = /dev/ {{ disk_id }} bs = 4096 status = progress Troubleshooting \u2691 SMART error (CurrentPendingSector) detected on host \u2691 As stated above, this means that at some point, the drive was unable to successfully read the data from X different sectors, and hence have flagged them for possible reallocation. The sector will be marked as reallocated if a subsequent write fails. If the write succeeds, it is removed from current pending sectors and assumed to be OK. Start with a long self test with smartctl . Assuming the disk to test is /dev/sdd : smartctl -t long /dev/sdd The command will respond with an estimate of how long it thinks the test will take to complete. (But this assumes that no errors will be found!) To check progress use: smartctl -A /dev/sdd | grep remaining # or smartctl -c /dev/sdd | grep remaining Don't check too often because it can abort the test with some drives. If you receive an empty output, examine the reported status with: smartctl -l selftest /dev/sdd You will see something like this: === START OF READ SMART DATA SECTION === SMART Self-test log structure revision number 1 Num Test_Description Status Remaining LifeTime ( hours ) LBA_of_first_error # 1 Extended offline Completed: read failure 20% 1596 44724966 So take that 'LBA' of 44724966 and multiply by (512/4096) which is the equivalent of 'divide by 8' 44724966 / 8 = 5590620 .75 The sector to test then is 5590620 . If it is in the middle of a file, overwritting it will corrupt the file. If you are not cool with that, check the following posts to check if that sector belongs to a file: Smartmontools and fixing Unreadable Disk Sectors . Smartmontools Bad Block how to Archlinux Identify damaged files page Archlinux badblocks page If you don't care to corrupt the file, use the following command to 'zero-out' the sector: dd if = /dev/zero of = /dev/sda conv = sync bs = 4096 count = 1 seek = 5590620 1 +0 records in 1 +0 records out sync Now retry the smartctl -t short (or smartctl -t long if short fails) and see if the test is able to finish the test without errors: === START OF READ SMART DATA SECTION === SMART Self-test log structure revision number 1 Num Test_Description Status Remaining LifeTime(hours) LBA_of_first_error # 1 Short offline Completed without error 00% 11699 - # 2 Extended offline Completed: read failure 90% 11680 65344288 # 3 Extended offline Completed: read failure 90% 11675 65344288 If reading errors remain, repeat the steps above until they don't or skip to the bad block analysis step . Current_Pending_Sector should be 0 now and the drive will probably be fine. As long as Reallocated_Sector_Ct is zero, you should be fine. Even a few reallocated sectors seems OK, but if that count starts to increment frequently, then that is a danger sign. To regularly keep a close eye on the counters use smartd to schedule daily tests. If Current_Pending_Sector is still not 0 , we need to do a deeper analysis on the bad blocks . Bad block analysis \u2691 The SMART long test gives no guarantee to find every error . To find them, we're going to use the badblocks tool instead. There is read-only mode (default) which is the least accurate. There is the destructive write-mode (-w option) which is the most accurate but takes longer and will (obviously) destroy all data on the drive, thus making it quite useless for matching sectors up to files. There is finally the non-destructive read-write mode which is probably as accurate as the destructive mode, with the only real downside that it is probably the slowest. However, if a drive is known to be failing then read-only mode is probably still the safest. Links \u2691 S.M.A.R.T Wikipedia article . linux-hardware SMART disk probes . Bad blocks \u2691 Smartmontools and fixing Unreadable Disk Sectors . Smartmontools Bad Block how to Archlinux Identify damaged files page Archlinux badblocks page Hard drive geek guide on reducing the current pending sector count . Hiddencode guide on how to check bad sectors Hiddencode guide on how to fix bad sectors","title":"Hard Drive Health"},{"location":"linux/hard_drive_health/#check-the-warranty-status","text":"If your drive is still under warranty from the manufacturer you may consider RMA\u2019ing the drive (initiating a warranty return process). Seagate Warranty Check Western Digital (WD) Warranty Check HGST Warranty Check Toshiba Warranty Check","title":"Check the warranty status"},{"location":"linux/hard_drive_health/#wipe-all-the-disk","text":"Sometimes the CurrentPendingSector doesn't get reallocated, if you don't mind about the data in the disk, you can wipe it all with: dd if = /dev/zero of = /dev/ {{ disk_id }} bs = 4096 status = progress","title":"Wipe all the disk"},{"location":"linux/hard_drive_health/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"linux/hard_drive_health/#smart-error-currentpendingsector-detected-on-host","text":"As stated above, this means that at some point, the drive was unable to successfully read the data from X different sectors, and hence have flagged them for possible reallocation. The sector will be marked as reallocated if a subsequent write fails. If the write succeeds, it is removed from current pending sectors and assumed to be OK. Start with a long self test with smartctl . Assuming the disk to test is /dev/sdd : smartctl -t long /dev/sdd The command will respond with an estimate of how long it thinks the test will take to complete. (But this assumes that no errors will be found!) To check progress use: smartctl -A /dev/sdd | grep remaining # or smartctl -c /dev/sdd | grep remaining Don't check too often because it can abort the test with some drives. If you receive an empty output, examine the reported status with: smartctl -l selftest /dev/sdd You will see something like this: === START OF READ SMART DATA SECTION === SMART Self-test log structure revision number 1 Num Test_Description Status Remaining LifeTime ( hours ) LBA_of_first_error # 1 Extended offline Completed: read failure 20% 1596 44724966 So take that 'LBA' of 44724966 and multiply by (512/4096) which is the equivalent of 'divide by 8' 44724966 / 8 = 5590620 .75 The sector to test then is 5590620 . If it is in the middle of a file, overwritting it will corrupt the file. If you are not cool with that, check the following posts to check if that sector belongs to a file: Smartmontools and fixing Unreadable Disk Sectors . Smartmontools Bad Block how to Archlinux Identify damaged files page Archlinux badblocks page If you don't care to corrupt the file, use the following command to 'zero-out' the sector: dd if = /dev/zero of = /dev/sda conv = sync bs = 4096 count = 1 seek = 5590620 1 +0 records in 1 +0 records out sync Now retry the smartctl -t short (or smartctl -t long if short fails) and see if the test is able to finish the test without errors: === START OF READ SMART DATA SECTION === SMART Self-test log structure revision number 1 Num Test_Description Status Remaining LifeTime(hours) LBA_of_first_error # 1 Short offline Completed without error 00% 11699 - # 2 Extended offline Completed: read failure 90% 11680 65344288 # 3 Extended offline Completed: read failure 90% 11675 65344288 If reading errors remain, repeat the steps above until they don't or skip to the bad block analysis step . Current_Pending_Sector should be 0 now and the drive will probably be fine. As long as Reallocated_Sector_Ct is zero, you should be fine. Even a few reallocated sectors seems OK, but if that count starts to increment frequently, then that is a danger sign. To regularly keep a close eye on the counters use smartd to schedule daily tests. If Current_Pending_Sector is still not 0 , we need to do a deeper analysis on the bad blocks .","title":"SMART error (CurrentPendingSector) detected on host"},{"location":"linux/hard_drive_health/#bad-block-analysis","text":"The SMART long test gives no guarantee to find every error . To find them, we're going to use the badblocks tool instead. There is read-only mode (default) which is the least accurate. There is the destructive write-mode (-w option) which is the most accurate but takes longer and will (obviously) destroy all data on the drive, thus making it quite useless for matching sectors up to files. There is finally the non-destructive read-write mode which is probably as accurate as the destructive mode, with the only real downside that it is probably the slowest. However, if a drive is known to be failing then read-only mode is probably still the safest.","title":"Bad block analysis"},{"location":"linux/hard_drive_health/#links","text":"S.M.A.R.T Wikipedia article . linux-hardware SMART disk probes .","title":"Links"},{"location":"linux/hard_drive_health/#bad-blocks","text":"Smartmontools and fixing Unreadable Disk Sectors . Smartmontools Bad Block how to Archlinux Identify damaged files page Archlinux badblocks page Hard drive geek guide on reducing the current pending sector count . Hiddencode guide on how to check bad sectors Hiddencode guide on how to fix bad sectors","title":"Bad blocks"},{"location":"linux/hypothesis/","text":"Hypothesis is an open-source software project that aims to collect comments about statements made in any web-accessible content, and filter and rank those comments to assess each statement's credibility. It offers an online web application where registered users share highlights and annotations over any webpage. As of 2020-06-11, although the service can be self-hosted, it's not yet easy to do so. Install \u2691 Client \u2691 If you're using Chrome or any derivative there is an official extension. Unfortunately if you use Firefox the extension is still being developed #310 although an unofficial release works just fine . Alternatively you can use the Hypothesis bookmarklet . The only problem is that both the extensions and the bookmarklet only works for the official service. In theory you can tweak the extension build process to use your custom settings . Though there is yet no documentation on this topic. I've thought of opening them a bug regarding this issue, but their github issues are only for bug reports, they use a google group to track the feature requests, I don't have an easy way to post there, so if you follow this path, please contact me . Server \u2691 The infrastructure can be deployed with Docker-compose. version : '3' services : postgres : image : postgres:11.5-alpine ports : - 5432 # - '5432:5432' elasticsearch : image : hypothesis/elasticsearch:latest ports : - 9200 #- '9200:9200' environment : - discovery.type=single-node rabbit : image : rabbitmq:3.6-management-alpine ports : - 5672 - 15672 #- '5672:5672' #- '15672:15672' web : image : hypothesis/hypothesis:latest environment : - APP_URL=http://localhost:5000 - AUTHORITY=localhost - BROKER_URL=amqp://guest:guest@rabbit:5672// - CLIENT_OAUTH_ID - CLIENT_URL=http://localhost:3001/hypothesis - DATABASE_URL=postgresql://postgres@postgres/postgres - ELASTICSEARCH_URL=http://elasticsearch:9200 - NEW_RELIC_APP_NAME=h (dev) - NEW_RELIC_LICENSE_KEY - SECRET_KEY=notasecret ports : - '5000:5000' depends_on : - postgres - elasticsearch - rabbit docker-compose up Initialize the database and create the admin user. docker-compose exec web /bin/sh hypothesis init hypothesis user add hypothesis user admin <username> The service is available at http://localhost:5000 . To check the latest developments of the Docker compose deployment follow the issue #4899 . They also provide the tools they use to deploy the production service into AWS. References \u2691 Homepage FAQ Bug tracker Feature request tracker Server deployment open issues \u2691 Self-hosting Docker compose Create admin user when using Docker compose Steps required to run both h and serve the client from internal server How to deploy h on VM","title":"hypothesis"},{"location":"linux/hypothesis/#install","text":"","title":"Install"},{"location":"linux/hypothesis/#client","text":"If you're using Chrome or any derivative there is an official extension. Unfortunately if you use Firefox the extension is still being developed #310 although an unofficial release works just fine . Alternatively you can use the Hypothesis bookmarklet . The only problem is that both the extensions and the bookmarklet only works for the official service. In theory you can tweak the extension build process to use your custom settings . Though there is yet no documentation on this topic. I've thought of opening them a bug regarding this issue, but their github issues are only for bug reports, they use a google group to track the feature requests, I don't have an easy way to post there, so if you follow this path, please contact me .","title":"Client"},{"location":"linux/hypothesis/#server","text":"The infrastructure can be deployed with Docker-compose. version : '3' services : postgres : image : postgres:11.5-alpine ports : - 5432 # - '5432:5432' elasticsearch : image : hypothesis/elasticsearch:latest ports : - 9200 #- '9200:9200' environment : - discovery.type=single-node rabbit : image : rabbitmq:3.6-management-alpine ports : - 5672 - 15672 #- '5672:5672' #- '15672:15672' web : image : hypothesis/hypothesis:latest environment : - APP_URL=http://localhost:5000 - AUTHORITY=localhost - BROKER_URL=amqp://guest:guest@rabbit:5672// - CLIENT_OAUTH_ID - CLIENT_URL=http://localhost:3001/hypothesis - DATABASE_URL=postgresql://postgres@postgres/postgres - ELASTICSEARCH_URL=http://elasticsearch:9200 - NEW_RELIC_APP_NAME=h (dev) - NEW_RELIC_LICENSE_KEY - SECRET_KEY=notasecret ports : - '5000:5000' depends_on : - postgres - elasticsearch - rabbit docker-compose up Initialize the database and create the admin user. docker-compose exec web /bin/sh hypothesis init hypothesis user add hypothesis user admin <username> The service is available at http://localhost:5000 . To check the latest developments of the Docker compose deployment follow the issue #4899 . They also provide the tools they use to deploy the production service into AWS.","title":"Server"},{"location":"linux/hypothesis/#references","text":"Homepage FAQ Bug tracker Feature request tracker","title":"References"},{"location":"linux/hypothesis/#server-deployment-open-issues","text":"Self-hosting Docker compose Create admin user when using Docker compose Steps required to run both h and serve the client from internal server How to deploy h on VM","title":"Server deployment open issues"},{"location":"linux/mkdocs/","text":"MkDocs is a fast, simple and downright gorgeous static site generator that's geared towards building project documentation. Documentation source files are written in Markdown, and configured with a single YAML configuration file. I've automated the creation of the mkdocs site in this cookiecutter template . Installation \u2691 Install the basic packages. pip install \\ mkdocs \\ mkdocs-material \\ mkdocs-autolink-plugin \\ mkdocs-minify-plugin \\ pymdown-extensions \\ mkdocs-git-revision-date-localized-plugin Create the docs repository. mkdocs new docs Although there are several themes , I usually use the material one. I won't dive into the different options, just show a working template of the mkdocs.yaml file. site_name : {{ site_name }} site_author : {{ your_name }} site_url : {{ site_url }} nav : - Introduction : 'index.md' - Basic Usage : 'basic_usage.md' - Configuration : 'configuration.md' - Update : 'update.md' - Advanced Usage : - Projects : \"projects.md\" - Tags : \"tags.md\" plugins : - search - autolinks - git-revision-date-localized : type : timeago - minify : minify_html : true markdown_extensions : - admonition - meta - toc : permalink : true baselevel : 2 - pymdownx.arithmatex - pymdownx.betterem : smart_enable : all - pymdownx.caret - pymdownx.critic - pymdownx.details - pymdownx.emoji : emoji_generator : !!python/name:pymdownx.emoji.to_svg - pymdownx.inlinehilite - pymdownx.magiclink - pymdownx.mark - pymdownx.smartsymbols - pymdownx.superfences - pymdownx.tasklist : custom_checkbox : true - pymdownx.tilde theme : name : material custom_dir : \"theme\" logo : \"images/logo.png\" palette : primary : 'blue grey' accent : 'light blue' extra_css : - 'stylesheets/extra.css' - 'stylesheets/links.css' repo_name : {{ repository_name }} # for example: 'lyz-code/pydo' repo_url : {{ repository_url }} # for example: 'https://github.com/lyz-code/pydo' Configure your logo by saving it into docs/images/logo.png . I like to show a small image above each link so you know where is it pointing to. To do so add the content of this directory to theme . and these files under docs/stylesheets . Initialize the git repository and create the first commit. Start the server to see everything is alright. mkdocs serve Material theme customizations \u2691 Color palette toggle \u2691 Since 7.1.0, you can have a light-dark mode on the site using a toggle in the upper bar. To enable it add to your mkdocs.yml : theme : palette : # Light mode - media : \"(prefers-color-scheme: light)\" scheme : default primary : blue grey accent : light blue toggle : icon : material/toggle-switch-off-outline name : Switch to dark mode # Dark mode - media : \"(prefers-color-scheme: dark)\" scheme : slate primary : blue grey accent : light blue toggle : icon : material/toggle-switch name : Switch to light mode Changing your desired colors for each mode Back to top button \u2691 Since 7.1.0, a back-to-top button can be shown when the user, after scrolling down, starts to scroll up again. It's rendered in the lower right corner of the viewport. Add the following lines to mkdocs.yml: theme : features : - navigation.top Add a github pages hook. \u2691 Save your requirements.txt . pip freeze > requirements.txt Create the .github/workflows/gh-pages.yml file with the following contents. name : Github pages on : push : branches : - master jobs : deploy : runs-on : ubuntu-18.04 steps : - uses : actions/checkout@v2 with : # Number of commits to fetch. 0 indicates all history. # Default: 1 fetch-depth : 0 - name : Setup Python uses : actions/setup-python@v1 with : python-version : '3.7' architecture : 'x64' - name : Cache dependencies uses : actions/cache@v1 with : path : ~/.cache/pip key : ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }} restore-keys : | ${{ runner.os }}-pip- - name : Install dependencies run : | python3 -m pip install --upgrade pip python3 -m pip install -r ./requirements.txt - run : | cd docs mkdocs build - name : Deploy uses : peaceiris/actions-gh-pages@v3 with : deploy_key : ${{ secrets.ACTIONS_DEPLOY_KEY }} publish_dir : ./docs/site Create an SSH deploy key Activate GitHub Pages repository configuration with gh-pages branch . Make a new commit and push to check it's working. Create MermaidJS diagrams \u2691 Even though the Material theme supports mermaid diagrams it's only giving it for the paid users. The funding needs to reach 5000$ so it's released to the general public. The alternative is to use the mkdocs-mermaid2-plugin plugin, which can't be used with mkdocs-minify-plugin and doesn't adapt to dark mode. To install it : Download the package: pip install mkdocs-mermaid2-plugin . Enable the plugin in mkdocs.yml . plugins : # Not compatible with mermaid2 # - minify: # minify_html: true - mermaid2 : arguments : securityLevel : 'loose' markdown_extensions : - pymdownx.superfences : # make exceptions to highlighting of code: custom_fences : - name : mermaid class : mermaid format : !!python/name:mermaid2.fence_mermaid Check the MermaidJS article to see how to create the diagrams. Plugin development \u2691 Like MkDocs, plugins must be written in Python. It is expected that each plugin would be distributed as a separate Python module. At a minimum, a MkDocs Plugin must consist of a BasePlugin subclass and an entry point which points to it. The BasePlugin class is meant to have on_<event_name> methods that run actions on the MkDocs defined events . The same object is called at the different events, so you can save objects from one event to the other in the object attributes. Keep in mind that the order of execution of the plugins follows the ordering of the list of the mkdocs.yml file where they are defined. Interesting objects \u2691 Files \u2691 mkdocs.structure.files.Files contains a list of File objects under the ._files attribute and allows you to append files to the collection. As well as extracting the different file types: documentation_pages : Iterable of markdown page file objects. static_pages : Iterable of static page file objects. media_files : Iterable of all files that are not documentation or static pages. javascript_files : Iterable of javascript files. css_files : Iterable of css files. It is initialized with a list of File objects. File \u2691 mkdocs.structure.files.File objects points to the source and destination locations of a file. It has the following interesting attributes: name : Name of the file without the extension. src_path or abs_src_path : Relative or absolute path to the original path, for example the markdown file. dest_path or abs_dest_path : Relative or absolute path to the destination path, for example the html file generated from the markdown one. url : Url where the file is going to be exposed. It is initialized with the arguments: path : Must be a path that exists relative to src_dir . src_dir : Absolute path on the local file system to the directory where the docs are. dest_dir : Absolute path on the local file system to the directory where the site is going to be built. use_directory_urls : If False , a Markdown file is mapped to an HTML file of the same name (the file extension is changed to .html ). If True, a Markdown file is mapped to an HTML index file ( index.html ) nested in a directory using the \"name\" of the file in path . The use_directory_urls argument has no effect on non-Markdown files. By default MkDocs uses True . Navigation \u2691 mkdocs.structure.nav.Navigation objects hold the information to build the navigation of the site. It has the following interesting attributes: items : Nested List with full navigation of Sections, SectionPages, Pages, and Links. pages : Flat List of subset of Pages in nav, in order. The Navigation object has no __eq__ method, so when testing, instead of trying to build a similar Navigation object and compare them, you need to assert that the contents of the object are what you expect. Page \u2691 mkdocs.structure.pages.Page models each page of the site. To initialize it you need the title , the File object of the page, and the MkDocs config object. Section \u2691 mkdocs.structure.nav.Section object models a section of the navigation of a MkDocs site. To initialize it you need the title of the section and the children which are the elements that belong to the section. If you don't yet know the children, pass an empty list [] . SectionPage \u2691 mkdocs_section_index.SectionPage , part of the mkdocs-section-index plugin, models Section objects that have an associated Page , allowing you to have nav sections that when clicked, load the Page and not only opens the menu for the children elements. To initialize it you need the title of the section, the File object of the page, , the MkDocs config object, and the children which are the elements that belong to the section. If you don't yet know the children, pass an empty list [] . Events \u2691 on_config \u2691 The config event is the first event called on build and is run immediately after the user configuration is loaded and validated. Any alterations to the config should be made here. Parameters: config : global configuration object Returns: global configuration object on_files \u2691 The files event is called after the files collection is populated from the docs_dir . Use this event to add, remove, or alter files in the collection. Note that Page objects have not yet been associated with the file objects in the collection. Use Page Events to manipulate page specific data. Parameters: files : global files collection config : global configuration object Returns: global files collection on_nav \u2691 The nav event is called after the site navigation is created and can be used to alter the site navigation. Read the following section if you want to add new files . Parameters: nav : global navigation object . config : global configuration object. files : global files collection . Returns: global navigation object Adding new files \u2691 TL;DR: Add them in the on_config event. To add new files to the repository you will need two phases: Create the markdown article pages. Add them to the navigation. My first idea as a MkDocs user, and newborn plugin developer was to add the navigation items to the nav key in the config object, as it's more easy to add items to a dictionary I'm used to work with than to dive into the code and understand how MkDocs creates the navigation. As I understood from the docs, the files should be created in the on_files event. the problem with this approach is that the only event that allows you to change the config is the on_config event, which is before the on_files one, so you can't build the navigation this way after you've created the files. Next idea was to add the items in the on_nav event, that means creating yourself the Section , Pages , SectionPages or Link objects and append them to the nav.items . The problem is that MkDocs initializes and processes the Navigation object in the get_navigation function. If you want to add items with a plugin in the on_nav event, you need to manually run all the post processing functions such as building the pages attribute, by running the _get_by_type , _add_previous_and_next_links or _add_parent_links yourself. Additionally, when building the site you'll get the The following pages exist in the docs directory, but are not included in the \"nav\" configuration error, because that check is done before all plugins change the navigation in the on_nav object. The last approach is to build the files and tweak the navigation in the on_config event. This approach has the next advantages: You need less knowledge of how MkDocs works. You don't need to create the File or Files objects. You don't need to create the Page , Section , SectionPage objects. More robust as you rely on existent MkDocs functionality. Testing \u2691 I haven't found any official documentation on how to test MkDocs plugins, in the issues they suggest you look at how they test it in the search plugin . I've looked at other plugins such as mkdocs_blog and used the next way to test mkdocs-newsletter . I see the plugin definition as an entrypoint to the functionality of our program, that's why I feel the definition should be in src/mkdocs_newsletter/entrypoints/mkdocs_plugin.py . As any entrypoint, the best way to test them are in end-to-end tests. You need to have a working test site in tests/assets/test_data , with it's mkdocs.yml file that loads your plugin and some fake articles. To prepare the test we can define the next fixture that prepares the building of the site: File: tests/conftest.py import os import shutil from mkdocs import config from mkdocs.config.base import Config from py._path.local import LocalPath @pytest . fixture ( name = \"config\" ) def config_ ( tmpdir : LocalPath ) -> Config : \"\"\"Load the mkdocs configuration.\"\"\" repo_path = tmpdir / \"test_data\" shutil . copytree ( \"tests/assets/test_data\" , repo_path ) mkdocs_config = config . load_config ( os . path . join ( repo_path , \"mkdocs.yml\" ) ) mkdocs_config [ \"site_dir\" ] = os . path . join ( repo_path , \"site\" ) return mkdocs_config It does the next steps: Copy the fake MkDocs site to a temporal directory Prepare the MkDocs Config object to build the site. Now we can use it in the e2e tests: File: tests/e2e/test_plugin.py def test_plugin_builds_newsletters ( full_repo : Repo , config : Config ) -> None : build . build ( config ) # act newsletter_path = f \" { full_repo . working_dir } /site/newsletter/2021_02/index.html\" with open ( newsletter_path , \"r\" ) as newsletter_file : newsletter = newsletter_file . read () assert \"<title>February of 2021 - The Blue Book</title>\" in newsletter That test is meant to ensure that our plugin works with the MkDocs ecosystem, so the assertions should be done against the created html files. If your functionality can't be covered by the happy path of the end-to-end test, it's better to create unit tests to make sure that they work as you want. You can see a full example here . Issues \u2691 Once they are closed: MermaidJS material theme support. Once it's released we need to: Migrate the projects that are using mkdocs-mermaid2-plugin Update the mermaidjs docs. References \u2691 Git Homepage . Material theme configuration guide Plugin development \u2691 User guide List of events Plugin testing example","title":"mkdocs"},{"location":"linux/mkdocs/#installation","text":"Install the basic packages. pip install \\ mkdocs \\ mkdocs-material \\ mkdocs-autolink-plugin \\ mkdocs-minify-plugin \\ pymdown-extensions \\ mkdocs-git-revision-date-localized-plugin Create the docs repository. mkdocs new docs Although there are several themes , I usually use the material one. I won't dive into the different options, just show a working template of the mkdocs.yaml file. site_name : {{ site_name }} site_author : {{ your_name }} site_url : {{ site_url }} nav : - Introduction : 'index.md' - Basic Usage : 'basic_usage.md' - Configuration : 'configuration.md' - Update : 'update.md' - Advanced Usage : - Projects : \"projects.md\" - Tags : \"tags.md\" plugins : - search - autolinks - git-revision-date-localized : type : timeago - minify : minify_html : true markdown_extensions : - admonition - meta - toc : permalink : true baselevel : 2 - pymdownx.arithmatex - pymdownx.betterem : smart_enable : all - pymdownx.caret - pymdownx.critic - pymdownx.details - pymdownx.emoji : emoji_generator : !!python/name:pymdownx.emoji.to_svg - pymdownx.inlinehilite - pymdownx.magiclink - pymdownx.mark - pymdownx.smartsymbols - pymdownx.superfences - pymdownx.tasklist : custom_checkbox : true - pymdownx.tilde theme : name : material custom_dir : \"theme\" logo : \"images/logo.png\" palette : primary : 'blue grey' accent : 'light blue' extra_css : - 'stylesheets/extra.css' - 'stylesheets/links.css' repo_name : {{ repository_name }} # for example: 'lyz-code/pydo' repo_url : {{ repository_url }} # for example: 'https://github.com/lyz-code/pydo' Configure your logo by saving it into docs/images/logo.png . I like to show a small image above each link so you know where is it pointing to. To do so add the content of this directory to theme . and these files under docs/stylesheets . Initialize the git repository and create the first commit. Start the server to see everything is alright. mkdocs serve","title":"Installation"},{"location":"linux/mkdocs/#material-theme-customizations","text":"","title":"Material theme customizations"},{"location":"linux/mkdocs/#color-palette-toggle","text":"Since 7.1.0, you can have a light-dark mode on the site using a toggle in the upper bar. To enable it add to your mkdocs.yml : theme : palette : # Light mode - media : \"(prefers-color-scheme: light)\" scheme : default primary : blue grey accent : light blue toggle : icon : material/toggle-switch-off-outline name : Switch to dark mode # Dark mode - media : \"(prefers-color-scheme: dark)\" scheme : slate primary : blue grey accent : light blue toggle : icon : material/toggle-switch name : Switch to light mode Changing your desired colors for each mode","title":"Color palette toggle"},{"location":"linux/mkdocs/#back-to-top-button","text":"Since 7.1.0, a back-to-top button can be shown when the user, after scrolling down, starts to scroll up again. It's rendered in the lower right corner of the viewport. Add the following lines to mkdocs.yml: theme : features : - navigation.top","title":"Back to top button"},{"location":"linux/mkdocs/#add-a-github-pages-hook","text":"Save your requirements.txt . pip freeze > requirements.txt Create the .github/workflows/gh-pages.yml file with the following contents. name : Github pages on : push : branches : - master jobs : deploy : runs-on : ubuntu-18.04 steps : - uses : actions/checkout@v2 with : # Number of commits to fetch. 0 indicates all history. # Default: 1 fetch-depth : 0 - name : Setup Python uses : actions/setup-python@v1 with : python-version : '3.7' architecture : 'x64' - name : Cache dependencies uses : actions/cache@v1 with : path : ~/.cache/pip key : ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }} restore-keys : | ${{ runner.os }}-pip- - name : Install dependencies run : | python3 -m pip install --upgrade pip python3 -m pip install -r ./requirements.txt - run : | cd docs mkdocs build - name : Deploy uses : peaceiris/actions-gh-pages@v3 with : deploy_key : ${{ secrets.ACTIONS_DEPLOY_KEY }} publish_dir : ./docs/site Create an SSH deploy key Activate GitHub Pages repository configuration with gh-pages branch . Make a new commit and push to check it's working.","title":"Add a github pages hook."},{"location":"linux/mkdocs/#create-mermaidjs-diagrams","text":"Even though the Material theme supports mermaid diagrams it's only giving it for the paid users. The funding needs to reach 5000$ so it's released to the general public. The alternative is to use the mkdocs-mermaid2-plugin plugin, which can't be used with mkdocs-minify-plugin and doesn't adapt to dark mode. To install it : Download the package: pip install mkdocs-mermaid2-plugin . Enable the plugin in mkdocs.yml . plugins : # Not compatible with mermaid2 # - minify: # minify_html: true - mermaid2 : arguments : securityLevel : 'loose' markdown_extensions : - pymdownx.superfences : # make exceptions to highlighting of code: custom_fences : - name : mermaid class : mermaid format : !!python/name:mermaid2.fence_mermaid Check the MermaidJS article to see how to create the diagrams.","title":"Create MermaidJS diagrams"},{"location":"linux/mkdocs/#plugin-development","text":"Like MkDocs, plugins must be written in Python. It is expected that each plugin would be distributed as a separate Python module. At a minimum, a MkDocs Plugin must consist of a BasePlugin subclass and an entry point which points to it. The BasePlugin class is meant to have on_<event_name> methods that run actions on the MkDocs defined events . The same object is called at the different events, so you can save objects from one event to the other in the object attributes. Keep in mind that the order of execution of the plugins follows the ordering of the list of the mkdocs.yml file where they are defined.","title":"Plugin development"},{"location":"linux/mkdocs/#interesting-objects","text":"","title":"Interesting objects"},{"location":"linux/mkdocs/#files","text":"mkdocs.structure.files.Files contains a list of File objects under the ._files attribute and allows you to append files to the collection. As well as extracting the different file types: documentation_pages : Iterable of markdown page file objects. static_pages : Iterable of static page file objects. media_files : Iterable of all files that are not documentation or static pages. javascript_files : Iterable of javascript files. css_files : Iterable of css files. It is initialized with a list of File objects.","title":"Files"},{"location":"linux/mkdocs/#file","text":"mkdocs.structure.files.File objects points to the source and destination locations of a file. It has the following interesting attributes: name : Name of the file without the extension. src_path or abs_src_path : Relative or absolute path to the original path, for example the markdown file. dest_path or abs_dest_path : Relative or absolute path to the destination path, for example the html file generated from the markdown one. url : Url where the file is going to be exposed. It is initialized with the arguments: path : Must be a path that exists relative to src_dir . src_dir : Absolute path on the local file system to the directory where the docs are. dest_dir : Absolute path on the local file system to the directory where the site is going to be built. use_directory_urls : If False , a Markdown file is mapped to an HTML file of the same name (the file extension is changed to .html ). If True, a Markdown file is mapped to an HTML index file ( index.html ) nested in a directory using the \"name\" of the file in path . The use_directory_urls argument has no effect on non-Markdown files. By default MkDocs uses True .","title":"File"},{"location":"linux/mkdocs/#navigation","text":"mkdocs.structure.nav.Navigation objects hold the information to build the navigation of the site. It has the following interesting attributes: items : Nested List with full navigation of Sections, SectionPages, Pages, and Links. pages : Flat List of subset of Pages in nav, in order. The Navigation object has no __eq__ method, so when testing, instead of trying to build a similar Navigation object and compare them, you need to assert that the contents of the object are what you expect.","title":"Navigation"},{"location":"linux/mkdocs/#page","text":"mkdocs.structure.pages.Page models each page of the site. To initialize it you need the title , the File object of the page, and the MkDocs config object.","title":"Page"},{"location":"linux/mkdocs/#section","text":"mkdocs.structure.nav.Section object models a section of the navigation of a MkDocs site. To initialize it you need the title of the section and the children which are the elements that belong to the section. If you don't yet know the children, pass an empty list [] .","title":"Section"},{"location":"linux/mkdocs/#sectionpage","text":"mkdocs_section_index.SectionPage , part of the mkdocs-section-index plugin, models Section objects that have an associated Page , allowing you to have nav sections that when clicked, load the Page and not only opens the menu for the children elements. To initialize it you need the title of the section, the File object of the page, , the MkDocs config object, and the children which are the elements that belong to the section. If you don't yet know the children, pass an empty list [] .","title":"SectionPage"},{"location":"linux/mkdocs/#events","text":"","title":"Events"},{"location":"linux/mkdocs/#on_config","text":"The config event is the first event called on build and is run immediately after the user configuration is loaded and validated. Any alterations to the config should be made here. Parameters: config : global configuration object Returns: global configuration object","title":"on_config"},{"location":"linux/mkdocs/#on_files","text":"The files event is called after the files collection is populated from the docs_dir . Use this event to add, remove, or alter files in the collection. Note that Page objects have not yet been associated with the file objects in the collection. Use Page Events to manipulate page specific data. Parameters: files : global files collection config : global configuration object Returns: global files collection","title":"on_files"},{"location":"linux/mkdocs/#on_nav","text":"The nav event is called after the site navigation is created and can be used to alter the site navigation. Read the following section if you want to add new files . Parameters: nav : global navigation object . config : global configuration object. files : global files collection . Returns: global navigation object","title":"on_nav"},{"location":"linux/mkdocs/#adding-new-files","text":"TL;DR: Add them in the on_config event. To add new files to the repository you will need two phases: Create the markdown article pages. Add them to the navigation. My first idea as a MkDocs user, and newborn plugin developer was to add the navigation items to the nav key in the config object, as it's more easy to add items to a dictionary I'm used to work with than to dive into the code and understand how MkDocs creates the navigation. As I understood from the docs, the files should be created in the on_files event. the problem with this approach is that the only event that allows you to change the config is the on_config event, which is before the on_files one, so you can't build the navigation this way after you've created the files. Next idea was to add the items in the on_nav event, that means creating yourself the Section , Pages , SectionPages or Link objects and append them to the nav.items . The problem is that MkDocs initializes and processes the Navigation object in the get_navigation function. If you want to add items with a plugin in the on_nav event, you need to manually run all the post processing functions such as building the pages attribute, by running the _get_by_type , _add_previous_and_next_links or _add_parent_links yourself. Additionally, when building the site you'll get the The following pages exist in the docs directory, but are not included in the \"nav\" configuration error, because that check is done before all plugins change the navigation in the on_nav object. The last approach is to build the files and tweak the navigation in the on_config event. This approach has the next advantages: You need less knowledge of how MkDocs works. You don't need to create the File or Files objects. You don't need to create the Page , Section , SectionPage objects. More robust as you rely on existent MkDocs functionality.","title":"Adding new files"},{"location":"linux/mkdocs/#testing","text":"I haven't found any official documentation on how to test MkDocs plugins, in the issues they suggest you look at how they test it in the search plugin . I've looked at other plugins such as mkdocs_blog and used the next way to test mkdocs-newsletter . I see the plugin definition as an entrypoint to the functionality of our program, that's why I feel the definition should be in src/mkdocs_newsletter/entrypoints/mkdocs_plugin.py . As any entrypoint, the best way to test them are in end-to-end tests. You need to have a working test site in tests/assets/test_data , with it's mkdocs.yml file that loads your plugin and some fake articles. To prepare the test we can define the next fixture that prepares the building of the site: File: tests/conftest.py import os import shutil from mkdocs import config from mkdocs.config.base import Config from py._path.local import LocalPath @pytest . fixture ( name = \"config\" ) def config_ ( tmpdir : LocalPath ) -> Config : \"\"\"Load the mkdocs configuration.\"\"\" repo_path = tmpdir / \"test_data\" shutil . copytree ( \"tests/assets/test_data\" , repo_path ) mkdocs_config = config . load_config ( os . path . join ( repo_path , \"mkdocs.yml\" ) ) mkdocs_config [ \"site_dir\" ] = os . path . join ( repo_path , \"site\" ) return mkdocs_config It does the next steps: Copy the fake MkDocs site to a temporal directory Prepare the MkDocs Config object to build the site. Now we can use it in the e2e tests: File: tests/e2e/test_plugin.py def test_plugin_builds_newsletters ( full_repo : Repo , config : Config ) -> None : build . build ( config ) # act newsletter_path = f \" { full_repo . working_dir } /site/newsletter/2021_02/index.html\" with open ( newsletter_path , \"r\" ) as newsletter_file : newsletter = newsletter_file . read () assert \"<title>February of 2021 - The Blue Book</title>\" in newsletter That test is meant to ensure that our plugin works with the MkDocs ecosystem, so the assertions should be done against the created html files. If your functionality can't be covered by the happy path of the end-to-end test, it's better to create unit tests to make sure that they work as you want. You can see a full example here .","title":"Testing"},{"location":"linux/mkdocs/#issues","text":"Once they are closed: MermaidJS material theme support. Once it's released we need to: Migrate the projects that are using mkdocs-mermaid2-plugin Update the mermaidjs docs.","title":"Issues"},{"location":"linux/mkdocs/#references","text":"Git Homepage . Material theme configuration guide","title":"References"},{"location":"linux/mkdocs/#plugin-development_1","text":"User guide List of events Plugin testing example","title":"Plugin development"},{"location":"linux/monica/","text":"Monica is an open-source web application to organize the interactions with your loved ones. They call it a PRM, or Personal Relationship Management. Think of it as a CRM (a popular tool used by sales teams in the corporate world) for your friends or family. Monica allows people to keep track of everything that's important about their friends and family. Like the activities done with them. When you last called someone. What you talked about. It will help you remember the name and the age of the kids. It can also remind you to call someone you haven't talked to in a while. They have pricing plans for their hosted service, but the self-hosted solution has all the features. It also has a nice API to interact with. Install \u2691 They provide a very throughout documented Docker installation . If you just want to test it, use this docker compose File: docker-compose.yml version: \"3.4\" services: app: image: monicahq/monicahq depends_on: - db ports: - 8080:80 environment: # generate with `pwgen -s 32 1` for instance: - APP_KEY=DoKMvhGu795QcMBP1I5sw8uk85MMAPS9 - DB_HOST=db volumes: - data:/var/www/monica/storage restart: always db: image: mysql:5.7 environment: - MYSQL_RANDOM_ROOT_PASSWORD=true - MYSQL_DATABASE=monica - MYSQL_USER=homestead - MYSQL_PASSWORD=secret volumes: - mysql:/var/lib/mysql restart: always volumes: data: name: data mysql: name: mysql Once you install your own, you may want to: Change the APP_KEY Change the database credentials. In the application docker are loaded as DB_USERNAME , DB_HOST and DB_PASSWORD . Set up the environment and the application url with APP_ENV=production and APP_URL . Set up the email configuration MAIL_MAILER : smtp MAIL_HOST : smtp.service.com # ex: smtp.sendgrid.net MAIL_PORT : 587 # is using tls, as you should MAIL_USERNAME : my_service_username # ex: apikey MAIL_PASSWORD : my_service_password # ex: SG.Psuoc6NZTrGHAF9fdsgsdgsbvjQ.JuxNWVYmJ8LE0 MAIL_ENCRYPTION : tls MAIL_FROM_ADDRESS : no-reply@xxx.com # ex: email you want the email to be FROM MAIL_FROM_NAME : Monica # ex: name of the sender Here is an example of all the possible configurations. They also share other configuration examples where you can take ideas of alternate setups. If you don't want to use docker, check the other installation documentation . References \u2691 Homepage Git Docs Blog","title":"monica"},{"location":"linux/monica/#install","text":"They provide a very throughout documented Docker installation . If you just want to test it, use this docker compose File: docker-compose.yml version: \"3.4\" services: app: image: monicahq/monicahq depends_on: - db ports: - 8080:80 environment: # generate with `pwgen -s 32 1` for instance: - APP_KEY=DoKMvhGu795QcMBP1I5sw8uk85MMAPS9 - DB_HOST=db volumes: - data:/var/www/monica/storage restart: always db: image: mysql:5.7 environment: - MYSQL_RANDOM_ROOT_PASSWORD=true - MYSQL_DATABASE=monica - MYSQL_USER=homestead - MYSQL_PASSWORD=secret volumes: - mysql:/var/lib/mysql restart: always volumes: data: name: data mysql: name: mysql Once you install your own, you may want to: Change the APP_KEY Change the database credentials. In the application docker are loaded as DB_USERNAME , DB_HOST and DB_PASSWORD . Set up the environment and the application url with APP_ENV=production and APP_URL . Set up the email configuration MAIL_MAILER : smtp MAIL_HOST : smtp.service.com # ex: smtp.sendgrid.net MAIL_PORT : 587 # is using tls, as you should MAIL_USERNAME : my_service_username # ex: apikey MAIL_PASSWORD : my_service_password # ex: SG.Psuoc6NZTrGHAF9fdsgsdgsbvjQ.JuxNWVYmJ8LE0 MAIL_ENCRYPTION : tls MAIL_FROM_ADDRESS : no-reply@xxx.com # ex: email you want the email to be FROM MAIL_FROM_NAME : Monica # ex: name of the sender Here is an example of all the possible configurations. They also share other configuration examples where you can take ideas of alternate setups. If you don't want to use docker, check the other installation documentation .","title":"Install"},{"location":"linux/monica/#references","text":"Homepage Git Docs Blog","title":"References"},{"location":"linux/nodejs/","text":"Node.js is a JavaScript runtime built on Chrome's V8 JavaScript engine. Install \u2691 The debian base repositories are really outdated, so add the NodeSource repository curl -sL https://deb.nodesource.com/setup_12.x | sudo bash - sudo apt-get update sudo apt-get install nodejs npm nodejs --version Links \u2691 Home","title":"nodejs"},{"location":"linux/nodejs/#install","text":"The debian base repositories are really outdated, so add the NodeSource repository curl -sL https://deb.nodesource.com/setup_12.x | sudo bash - sudo apt-get update sudo apt-get install nodejs npm nodejs --version","title":"Install"},{"location":"linux/nodejs/#links","text":"Home","title":"Links"},{"location":"linux/rm/","text":"rm definition In computing, rm (short for remove) is a basic command on Unix and Unix-like operating systems used to remove objects such as computer files, directories and symbolic links from file systems and also special files such as device nodes, pipes and sockets Debugging \u2691 Cannot remove file: \u201cStructure needs cleaning\u201d \u2691 From Victoria Stuart and Depressed Daniel answer You first need to: Umount the partition. Do a sector level backup of your disk. If your filesystem is ext4 run: fsck.ext4 {{ device }} Accept all suggested fixes. Mount again the partition.","title":"rm"},{"location":"linux/rm/#debugging","text":"","title":"Debugging"},{"location":"linux/rm/#cannot-remove-file-structure-needs-cleaning","text":"From Victoria Stuart and Depressed Daniel answer You first need to: Umount the partition. Do a sector level backup of your disk. If your filesystem is ext4 run: fsck.ext4 {{ device }} Accept all suggested fixes. Mount again the partition.","title":"Cannot remove file: \u201cStructure needs cleaning\u201d"},{"location":"linux/syncthing/","text":"Syncthing is a continuous file synchronization program. It synchronizes files between two or more computers in real time, safely protected from prying eyes. Your data is your data alone and you deserve to choose where it is stored, whether it is shared with some third party, and how it's transmitted over the internet. Installation \u2691 Debian or Ubuntu \u2691 # Add the release PGP keys: curl -s https://syncthing.net/release-key.txt | sudo apt-key add - # Add the \"stable\" channel to your APT sources: echo \"deb https://apt.syncthing.net/ syncthing stable\" | sudo tee /etc/apt/sources.list.d/syncthing.list # Update and install syncthing: sudo apt-get update sudo apt-get install syncthing Docker \u2691 Use Linuxserver Docker Configuration \u2691 If you're only going to use syncthing in an internal network, or you're going to fix the IPs of the devices you can disable the Global Discovery and Relaying connections so that you don't leak the existence of your services to the syncthing servers. Troubleshooting \u2691 Syncthing over Tor \u2691 There are many posts on this topic ( 1 , 2 ) but I wasn't able to connect two clients through Tor. Here are the steps I took in case anyone is interested. If you make it work, please contact me. Suggest to use a relay , go to relays.syncthing.net to see the public ones. You need to add the required servers to the Sync Protocol Listen Address field, under Actions and Settings . The syntax is: relay://<host name|IP>[:port]/?id=<relay device ID> The only way I've found to get the relay device ID is setting a fake one, and getting the correct one from the logs of syncthing. It will say that the fingerprint ( what you put ) doesn't match ( actual fingerprint ) . Steps \u2691 Configure the client: export all_proxy = socks5://127.0.0.1:9058 export ALL_PROXY_NO_FALLBACK = 1 syncthing --home /tmp/syncthing_1 Allow the connection to the local server: sudo iptables -I OUTPUT -o lo -p tcp --dport 8384 -j ACCEPT If you're using Tails and Tor Browser, you'll need to set the about:config setting network.proxy.allow_hijacking_localhost to false . Otherwise you won't be able to access the user interface. Issues \u2691 Wifi run condition needs location to be turned on : update and check that you no longer need it. Links \u2691 Home Getting Started","title":"Syncthing"},{"location":"linux/syncthing/#installation","text":"","title":"Installation"},{"location":"linux/syncthing/#debian-or-ubuntu","text":"# Add the release PGP keys: curl -s https://syncthing.net/release-key.txt | sudo apt-key add - # Add the \"stable\" channel to your APT sources: echo \"deb https://apt.syncthing.net/ syncthing stable\" | sudo tee /etc/apt/sources.list.d/syncthing.list # Update and install syncthing: sudo apt-get update sudo apt-get install syncthing","title":"Debian or Ubuntu"},{"location":"linux/syncthing/#docker","text":"Use Linuxserver Docker","title":"Docker"},{"location":"linux/syncthing/#configuration","text":"If you're only going to use syncthing in an internal network, or you're going to fix the IPs of the devices you can disable the Global Discovery and Relaying connections so that you don't leak the existence of your services to the syncthing servers.","title":"Configuration"},{"location":"linux/syncthing/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"linux/syncthing/#syncthing-over-tor","text":"There are many posts on this topic ( 1 , 2 ) but I wasn't able to connect two clients through Tor. Here are the steps I took in case anyone is interested. If you make it work, please contact me. Suggest to use a relay , go to relays.syncthing.net to see the public ones. You need to add the required servers to the Sync Protocol Listen Address field, under Actions and Settings . The syntax is: relay://<host name|IP>[:port]/?id=<relay device ID> The only way I've found to get the relay device ID is setting a fake one, and getting the correct one from the logs of syncthing. It will say that the fingerprint ( what you put ) doesn't match ( actual fingerprint ) .","title":"Syncthing over Tor"},{"location":"linux/syncthing/#steps","text":"Configure the client: export all_proxy = socks5://127.0.0.1:9058 export ALL_PROXY_NO_FALLBACK = 1 syncthing --home /tmp/syncthing_1 Allow the connection to the local server: sudo iptables -I OUTPUT -o lo -p tcp --dport 8384 -j ACCEPT If you're using Tails and Tor Browser, you'll need to set the about:config setting network.proxy.allow_hijacking_localhost to false . Otherwise you won't be able to access the user interface.","title":"Steps"},{"location":"linux/syncthing/#issues","text":"Wifi run condition needs location to be turned on : update and check that you no longer need it.","title":"Issues"},{"location":"linux/syncthing/#links","text":"Home Getting Started","title":"Links"},{"location":"linux/wireguard/","text":"Wireguard is an simple yet fast and modern VPN that utilizes state-of-the-art cryptography. It aims to be faster, simpler, leaner, and more useful than IPsec, while avoiding the massive headache. It intends to be considerably more performant than OpenVPN. WireGuard is a general purpose VPN for running on embedded interfaces and super computers alike. Initially released for the Linux kernel, it's now cross-platform (Windows, macOS, BSD, iOS, Android) and widely deployable. Although it's under heavy development, it already might be the most secure, easiest to use, and simplest VPN solution in the industry. Features: Simple and easy to use: WireGuard aims to be as easy to configure and deploy as SSH. A VPN connection is made by exchanging public keys \u2013 exactly like exchanging SSH keys \u2013 and all the rest is transparently handled by WireGuard. It's even capable of roaming between IP addresses, like Mosh. There is no need to manage connections, worry about state, manage daemons, or worry about what's under the hood. WireGuard presents a basic yet powerful interface. Cryptographically Sound: WireGuard uses state-of-the-art cryptography, such as the Noise protocol framework, Curve25519, ChaCha20, Poly1305, BLAKE2, SipHash24, HKDF, and secure trusted constructions. It makes conservative and reasonable choices and has been reviewed by cryptographers. Minimal Attack Surface: WireGuard is designed with ease-of-implementation and simplicity in mind. It's meant to be implemented in very few lines of code, and auditable for security vulnerabilities. Compared to behemoths like *Swan/IPsec or OpenVPN/OpenSSL, in which auditing the gigantic codebases is an overwhelming task even for large teams of security experts, WireGuard is meant to be comprehensively reviewable by single individuals. High Performance: A combination of extremely high-speed cryptographic primitives and the fact that WireGuard lives inside the Linux kernel means that secure networking can be very high-speed. It is suitable for both small embedded devices like smartphones and fully loaded backbone routers. Well Defined & Thoroughly Considered: WireGuard is the result of a lengthy and thoroughly considered academic process, resulting in the technical whitepaper, an academic research paper which clearly defines the protocol and the intense considerations that went into each decision. Plus it's created by the same guy as pass , which uses Gentoo, I like this guy. Conceptual Overview \u2691 WireGuard securely encapsulates IP packets over UDP. You add a WireGuard interface, configure it with your private key and your peers' public keys, and then you send packets across it. All issues of key distribution and pushed configurations are out of scope of WireGuard; these are issues much better left for other layers. It mimics the model of SSH and Mosh; both parties have each other's public keys, and then they're simply able to begin exchanging packets through the interface. Simple Network Interface \u2691 WireGuard works by adding network interfaces, called wg0 (or wg1, wg2, wg3, etc). This network interface can then be configured normally using the ordinary networking utilities. The specific WireGuard aspects of the interface are configured using the wg tool. This interface acts as a tunnel interface. WireGuard associates tunnel IP addresses with public keys and remote endpoints. When the interface sends a packet to a peer, it does the following: This packet is meant for 192.168.30.8. Which peer is that? Let me look... Okay, it's for peer ABCDEFGH. (Or if it's not for any configured peer, drop the packet.) Encrypt entire IP packet using peer ABCDEFGH's public key. What is the remote endpoint of peer ABCDEFGH? Let me look... Okay, the endpoint is UDP port 53133 on host 216.58.211.110. Send encrypted bytes from step 2 over the Internet to 216.58.211.110:53133 using UDP. When the interface receives a packet, this happens: I just got a packet from UDP port 7361 on host 98.139.183.24. Let's decrypt it! It decrypted and authenticated properly for peer LMNOPQRS. Okay, let's remember that peer LMNOPQRS's most recent Internet endpoint is 98.139.183.24:7361 using UDP. Once decrypted, the plain-text packet is from 192.168.43.89. Is peer LMNOPQRS allowed to be sending us packets as 192.168.43.89? If so, accept the packet on the interface. If not, drop it. Behind the scenes there is much happening to provide proper privacy, authenticity, and perfect forward secrecy, using state-of-the-art cryptography. Cryptokey Routing \u2691 At the heart of WireGuard is a concept called Cryptokey Routing, which works by associating public keys with a list of tunnel IP addresses that are allowed inside the tunnel. Each network interface has a private key and a list of peers. Each peer has a public key. For example, a server computer might have this configuration: [Interface] PrivateKey = yAnz5TF+lXXJte14tji3zlMNq+hd2rYUIgJBgB3fBmk= ListenPort = 51820 [Peer] PublicKey = xTIBA5rboUvnH4htodjb6e697QjLERt1NAB4mZqp8Dg= AllowedIPs = 10.192.122.3/32, 10.192.124.1/24 [Peer] PublicKey = TrMvSoP4jYQlY6RIzBgbssQqY3vxI2Pi+y71lOWWXX0= AllowedIPs = 10.192.122.4/32, 192.168.0.0/16 [Peer] PublicKey = gN65BkIKy1eCE9pP1wdc8ROUtkHLF2PfAqYdyYBz6EA= AllowedIPs = 10.10.10.230/32 And a client computer might have this simpler configuration: [Interface] PrivateKey = gI6EdUSYvn8ugXOt8QQD6Yc+JyiZxIhp3GInSWRfWGE= ListenPort = 21841 [Peer] PublicKey = HIgo9xNzJMWLKASShiTqIybxZ0U3wGLiUeJ1PKf8ykw= Endpoint = 192.95.5.69:51820 AllowedIPs = 0.0.0.0/0 In the server configuration, each peer (a client) will be able to send packets to the network interface with a source IP matching his corresponding list of allowed IPs. For example, when a packet is received by the server from peer gN65BkIK..., after being decrypted and authenticated, if its source IP is 10.10.10.230, then it's allowed onto the interface; otherwise it's dropped. In the server configuration, when the network interface wants to send a packet to a peer (a client), it looks at that packet's destination IP and compares it to each peer's list of allowed IPs to see which peer to send it to. For example, if the network interface is asked to send a packet with a destination IP of 10.10.10.230, it will encrypt it using the public key of peer gN65BkIK..., and then send it to that peer's most recent Internet endpoint. In the client configuration, its single peer (the server) will be able to send packets to the network interface with any source IP (since 0.0.0.0/0 is a wildcard). For example, when a packet is received from peer HIgo9xNz..., if it decrypts and authenticates correctly, with any source IP, then it's allowed onto the interface; otherwise it's dropped. In the client configuration, when the network interface wants to send a packet to its single peer (the server), it will encrypt packets for the single peer with any destination IP address (since 0.0.0.0/0 is a wildcard). For example, if the network interface is asked to send a packet with any destination IP, it will encrypt it using the public key of the single peer HIgo9xNz..., and then send it to the single peer's most recent Internet endpoint. In other words, when sending packets, the list of allowed IPs behaves as a sort of routing table, and when receiving packets, the list of allowed IPs behaves as a sort of access control list. This is what we call a Cryptokey Routing Table: the simple association of public keys and allowed IPs. Because all packets sent on the WireGuard interface are encrypted and authenticated, and because there is such a tight coupling between the identity of a peer and the allowed IP address of a peer, system administrators do not need complicated firewall extensions, such as in the case of IPsec, but rather they can simply match on \"is it from this IP? on this interface?\", and be assured that it is a secure and authentic packet. This greatly simplifies network management and access control, and provides a great deal more assurance that your iptables rules are actually doing what you intended for them to do. Built-in Roaming \u2691 The client configuration contains an initial endpoint of its single peer (the server), so that it knows where to send encrypted data before it has received encrypted data. The server configuration doesn't have any initial endpoints of its peers (the clients). This is because the server discovers the endpoint of its peers by examining from where correctly authenticated data originates. If the server itself changes its own endpoint, and sends data to the clients, the clients will discover the new server endpoint and update the configuration just the same. Both client and server send encrypted data to the most recent IP endpoint for which they authentically decrypted data. Thus, there is full IP roaming on both ends.","title":"Wireguard"},{"location":"linux/wireguard/#conceptual-overview","text":"WireGuard securely encapsulates IP packets over UDP. You add a WireGuard interface, configure it with your private key and your peers' public keys, and then you send packets across it. All issues of key distribution and pushed configurations are out of scope of WireGuard; these are issues much better left for other layers. It mimics the model of SSH and Mosh; both parties have each other's public keys, and then they're simply able to begin exchanging packets through the interface.","title":"Conceptual Overview"},{"location":"linux/wireguard/#simple-network-interface","text":"WireGuard works by adding network interfaces, called wg0 (or wg1, wg2, wg3, etc). This network interface can then be configured normally using the ordinary networking utilities. The specific WireGuard aspects of the interface are configured using the wg tool. This interface acts as a tunnel interface. WireGuard associates tunnel IP addresses with public keys and remote endpoints. When the interface sends a packet to a peer, it does the following: This packet is meant for 192.168.30.8. Which peer is that? Let me look... Okay, it's for peer ABCDEFGH. (Or if it's not for any configured peer, drop the packet.) Encrypt entire IP packet using peer ABCDEFGH's public key. What is the remote endpoint of peer ABCDEFGH? Let me look... Okay, the endpoint is UDP port 53133 on host 216.58.211.110. Send encrypted bytes from step 2 over the Internet to 216.58.211.110:53133 using UDP. When the interface receives a packet, this happens: I just got a packet from UDP port 7361 on host 98.139.183.24. Let's decrypt it! It decrypted and authenticated properly for peer LMNOPQRS. Okay, let's remember that peer LMNOPQRS's most recent Internet endpoint is 98.139.183.24:7361 using UDP. Once decrypted, the plain-text packet is from 192.168.43.89. Is peer LMNOPQRS allowed to be sending us packets as 192.168.43.89? If so, accept the packet on the interface. If not, drop it. Behind the scenes there is much happening to provide proper privacy, authenticity, and perfect forward secrecy, using state-of-the-art cryptography.","title":"Simple Network Interface"},{"location":"linux/wireguard/#cryptokey-routing","text":"At the heart of WireGuard is a concept called Cryptokey Routing, which works by associating public keys with a list of tunnel IP addresses that are allowed inside the tunnel. Each network interface has a private key and a list of peers. Each peer has a public key. For example, a server computer might have this configuration: [Interface] PrivateKey = yAnz5TF+lXXJte14tji3zlMNq+hd2rYUIgJBgB3fBmk= ListenPort = 51820 [Peer] PublicKey = xTIBA5rboUvnH4htodjb6e697QjLERt1NAB4mZqp8Dg= AllowedIPs = 10.192.122.3/32, 10.192.124.1/24 [Peer] PublicKey = TrMvSoP4jYQlY6RIzBgbssQqY3vxI2Pi+y71lOWWXX0= AllowedIPs = 10.192.122.4/32, 192.168.0.0/16 [Peer] PublicKey = gN65BkIKy1eCE9pP1wdc8ROUtkHLF2PfAqYdyYBz6EA= AllowedIPs = 10.10.10.230/32 And a client computer might have this simpler configuration: [Interface] PrivateKey = gI6EdUSYvn8ugXOt8QQD6Yc+JyiZxIhp3GInSWRfWGE= ListenPort = 21841 [Peer] PublicKey = HIgo9xNzJMWLKASShiTqIybxZ0U3wGLiUeJ1PKf8ykw= Endpoint = 192.95.5.69:51820 AllowedIPs = 0.0.0.0/0 In the server configuration, each peer (a client) will be able to send packets to the network interface with a source IP matching his corresponding list of allowed IPs. For example, when a packet is received by the server from peer gN65BkIK..., after being decrypted and authenticated, if its source IP is 10.10.10.230, then it's allowed onto the interface; otherwise it's dropped. In the server configuration, when the network interface wants to send a packet to a peer (a client), it looks at that packet's destination IP and compares it to each peer's list of allowed IPs to see which peer to send it to. For example, if the network interface is asked to send a packet with a destination IP of 10.10.10.230, it will encrypt it using the public key of peer gN65BkIK..., and then send it to that peer's most recent Internet endpoint. In the client configuration, its single peer (the server) will be able to send packets to the network interface with any source IP (since 0.0.0.0/0 is a wildcard). For example, when a packet is received from peer HIgo9xNz..., if it decrypts and authenticates correctly, with any source IP, then it's allowed onto the interface; otherwise it's dropped. In the client configuration, when the network interface wants to send a packet to its single peer (the server), it will encrypt packets for the single peer with any destination IP address (since 0.0.0.0/0 is a wildcard). For example, if the network interface is asked to send a packet with any destination IP, it will encrypt it using the public key of the single peer HIgo9xNz..., and then send it to the single peer's most recent Internet endpoint. In other words, when sending packets, the list of allowed IPs behaves as a sort of routing table, and when receiving packets, the list of allowed IPs behaves as a sort of access control list. This is what we call a Cryptokey Routing Table: the simple association of public keys and allowed IPs. Because all packets sent on the WireGuard interface are encrypted and authenticated, and because there is such a tight coupling between the identity of a peer and the allowed IP address of a peer, system administrators do not need complicated firewall extensions, such as in the case of IPsec, but rather they can simply match on \"is it from this IP? on this interface?\", and be assured that it is a secure and authentic packet. This greatly simplifies network management and access control, and provides a great deal more assurance that your iptables rules are actually doing what you intended for them to do.","title":"Cryptokey Routing"},{"location":"linux/wireguard/#built-in-roaming","text":"The client configuration contains an initial endpoint of its single peer (the server), so that it knows where to send encrypted data before it has received encrypted data. The server configuration doesn't have any initial endpoints of its peers (the clients). This is because the server discovers the endpoint of its peers by examining from where correctly authenticated data originates. If the server itself changes its own endpoint, and sends data to the clients, the clients will discover the new server endpoint and update the configuration just the same. Both client and server send encrypted data to the most recent IP endpoint for which they authentically decrypted data. Thus, there is full IP roaming on both ends.","title":"Built-in Roaming"},{"location":"linux/zfs/","text":"ZFS combines a file system with a volume manager. Usage \u2691 Mount a pool as readonly \u2691 zpool import -o readonly = on {{ pool_name }} Mount a ZFS snapshot in a directory as readonly \u2691 mount -t zfs {{ pool_name }} / {{ snapshot_name }} {{ mount_path }} -o ro List volumes \u2691 zpool list List snapshots \u2691 zfs list -t snapshot Get read and write stats from pool \u2691 zpool iostat {{ pool_name }} {{ refresh_time_in_seconds }}","title":"ZFS"},{"location":"linux/zfs/#usage","text":"","title":"Usage"},{"location":"linux/zfs/#mount-a-pool-as-readonly","text":"zpool import -o readonly = on {{ pool_name }}","title":"Mount a pool as readonly"},{"location":"linux/zfs/#mount-a-zfs-snapshot-in-a-directory-as-readonly","text":"mount -t zfs {{ pool_name }} / {{ snapshot_name }} {{ mount_path }} -o ro","title":"Mount a ZFS snapshot in a directory as readonly"},{"location":"linux/zfs/#list-volumes","text":"zpool list","title":"List volumes"},{"location":"linux/zfs/#list-snapshots","text":"zfs list -t snapshot","title":"List snapshots"},{"location":"linux/zfs/#get-read-and-write-stats-from-pool","text":"zpool iostat {{ pool_name }} {{ refresh_time_in_seconds }}","title":"Get read and write stats from pool"},{"location":"linux/zip/","text":"zip is an UNIX command line tool to package and compress files. Usage \u2691 Create a zip file \u2691 zip -r {{ zip_file }} {{ files_to_save }} Split files to a specific size \u2691 zip -s {{ size }} -r {{ destination_zip }} {{ files }} Where {{ size }} can be 950m Compress with password \u2691 zip -er {{ zip_file }} {{ files_to_save }} Read files to compress from a file \u2691 cat {{ files_to_compress.txt }} | zip -er {{ destination_zip }} -@ Uncompress a zip file \u2691 unzip {{ zip_file }}","title":"zip"},{"location":"linux/zip/#usage","text":"","title":"Usage"},{"location":"linux/zip/#create-a-zip-file","text":"zip -r {{ zip_file }} {{ files_to_save }}","title":"Create a zip file"},{"location":"linux/zip/#split-files-to-a-specific-size","text":"zip -s {{ size }} -r {{ destination_zip }} {{ files }} Where {{ size }} can be 950m","title":"Split files to a specific size"},{"location":"linux/zip/#compress-with-password","text":"zip -er {{ zip_file }} {{ files_to_save }}","title":"Compress with password"},{"location":"linux/zip/#read-files-to-compress-from-a-file","text":"cat {{ files_to_compress.txt }} | zip -er {{ destination_zip }} -@","title":"Read files to compress from a file"},{"location":"linux/zip/#uncompress-a-zip-file","text":"unzip {{ zip_file }}","title":"Uncompress a zip file"},{"location":"linux/luks/luks/","text":"LUKS definition The Linux Unified Key Setup (LUKS) is a disk encryption specification created by Clemens Fruhwirth in 2004 and was originally intended for Linux. While most disk encryption software implements different, incompatible, and undocumented formats, LUKS implements a platform-independent standard on-disk format for use in various tools. This not only facilitates compatibility and interoperability among different programs, but also assures that they all implement password management in a secure and documented manner. The reference implementation for LUKS operates on Linux and is based on an enhanced version of cryptsetup, using dm-crypt as the disk encryption backend. LUKS is designed to conform to the TKS1 secure key setup scheme. LUKS Commands \u2691 We use the cryptsetup command to interact with LUKS partitions. Header management \u2691 Get the disk header \u2691 cryptsetup luksDump /dev/sda3 Backup header \u2691 cryptsetup luksHeaderBackup --header-backup-file {{ file }} {{ device }} Key management \u2691 Add a key \u2691 cryptsetup luksAddKey --key-slot 1 {{ luks_device }} Change a key \u2691 cryptsetup luksChangeKey {{ luks_device }} -s 0 Test if you remember the key \u2691 Try to add a new key and cancel the process cryptsetup luksAddKey --key-slot 3 {{ luks_device }} Delete some keys \u2691 cryptsetup luksDump {{ device }} cryptsetup luksKillSlot {{ device }} {{ slot_number }} Delete all keys \u2691 cryptsetup luksErase {{ device }} Encrypt hard drive \u2691 Configure LUKS partition cryptsetup -y -v luksFormat /dev/sdg Open the container cryptsetup luksOpen /dev/sdg crypt Fill it with zeros pv -tpreb /dev/zero | dd of = /dev/mapper/crypt bs = 128M Make filesystem mkfs.ext4 /dev/mapper/crypt LUKS debugging \u2691 Resource busy \u2691 Umount the lv first lvscan lvchange -a n {{ partition_name }} Then close the luks device cryptsetup luksClose {{ device_name }}","title":"LUKS"},{"location":"linux/luks/luks/#luks-commands","text":"We use the cryptsetup command to interact with LUKS partitions.","title":"LUKS Commands"},{"location":"linux/luks/luks/#header-management","text":"","title":"Header management"},{"location":"linux/luks/luks/#get-the-disk-header","text":"cryptsetup luksDump /dev/sda3","title":"Get the disk header"},{"location":"linux/luks/luks/#backup-header","text":"cryptsetup luksHeaderBackup --header-backup-file {{ file }} {{ device }}","title":"Backup header"},{"location":"linux/luks/luks/#key-management","text":"","title":"Key management"},{"location":"linux/luks/luks/#add-a-key","text":"cryptsetup luksAddKey --key-slot 1 {{ luks_device }}","title":"Add a key"},{"location":"linux/luks/luks/#change-a-key","text":"cryptsetup luksChangeKey {{ luks_device }} -s 0","title":"Change a key"},{"location":"linux/luks/luks/#test-if-you-remember-the-key","text":"Try to add a new key and cancel the process cryptsetup luksAddKey --key-slot 3 {{ luks_device }}","title":"Test if you remember the key"},{"location":"linux/luks/luks/#delete-some-keys","text":"cryptsetup luksDump {{ device }} cryptsetup luksKillSlot {{ device }} {{ slot_number }}","title":"Delete some keys"},{"location":"linux/luks/luks/#delete-all-keys","text":"cryptsetup luksErase {{ device }}","title":"Delete all keys"},{"location":"linux/luks/luks/#encrypt-hard-drive","text":"Configure LUKS partition cryptsetup -y -v luksFormat /dev/sdg Open the container cryptsetup luksOpen /dev/sdg crypt Fill it with zeros pv -tpreb /dev/zero | dd of = /dev/mapper/crypt bs = 128M Make filesystem mkfs.ext4 /dev/mapper/crypt","title":"Encrypt hard drive"},{"location":"linux/luks/luks/#luks-debugging","text":"","title":"LUKS debugging"},{"location":"linux/luks/luks/#resource-busy","text":"Umount the lv first lvscan lvchange -a n {{ partition_name }} Then close the luks device cryptsetup luksClose {{ device_name }}","title":"Resource busy"},{"location":"linux/vim/vim_plugins/","text":"Black \u2691 To install Black you first need python3-venv . sudo apt-get install python3-venv Add the plugin and configure it so vim runs it each time you save. File ~/.vimrc Plugin 'psf/black' \" Black autocmd BufWritePre *.py execute ':Black' A configuration issue exists for neovim. If you encounter the error AttributeError: module 'black' has no attribute 'find_pyproject_toml' , do the following: cd ~/.vim/bundle/black git checkout 19 .10b0 As the default line length is 88 (ugly number by the way), we need to change the indent, python-mode configuration as well \"\" python indent autocmd BufNewFile,BufRead *.py setlocal foldmethod=indent tabstop=4 softtabstop=4 shiftwidth=4 textwidth=88 smarttab expandtab \" python-mode let g:pymode_options_max_line_length = 88 let g:pymode_lint_options_pep8 = {'max_line_length': g:pymode_options_max_line_length} ALE \u2691 ALE (Asynchronous Lint Engine) is a plugin providing linting (syntax checking and semantic errors) in NeoVim 0.2.0+ and Vim 8 while you edit your text files, and acts as a Vim Language Server Protocol client. ALE makes use of NeoVim and Vim 8 job control functions and timers to run linters on the contents of text buffers and return errors as text changes in Vim. This allows for displaying warnings and errors in files before they are saved back to a filesystem. In other words, this plugin allows you to lint while you type. ALE offers support for fixing code with command line tools in a non-blocking manner with the :ALEFix feature, supporting tools in many languages , like prettier, eslint, autopep8, and more. Installation \u2691 Install with Vundle: Plugin 'dense-analysis/ale' Configuration \u2691 let g :ale_sign_error = '\u2718' let g :ale_sign_warning = '\u26a0' highlight ALEErrorSign ctermbg = NONE ctermfg = red highlight ALEWarningSign ctermbg = NONE ctermfg = yellow let g :ale_linters_explicit = 1 let g :ale_lint_on_text_changed = 'normal' \" let g:ale_lint_on_text_changed = 'never' let g :ale_lint_on_enter = 0 let g :ale_lint_on_save = 1 let g :ale_fix_on_save = 1 let g :ale_linters = { \\ 'markdown' : [ 'markdownlint' , 'writegood' , 'alex' , 'proselint' ] , \\ 'json' : [ 'jsonlint' ] , \\ 'python' : [ 'flake8' , 'mypy' , 'pylint' , 'alex' ] , \\ 'yaml' : [ 'yamllint' , 'alex' ] , \\ '*' : [ 'alex' , 'writegood' ] , \\} let g :ale_fixers = { \\ '*' : [ 'remove_trailing_lines' , 'trim_whitespace' ] , \\ 'json' : [ 'jq' ] , \\ 'python' : [ 'isort' ] , \\ 'terraform' : [ 'terraform' ] , \\} inoremap < leader > e < esc > :ALENext < cr > nnoremap < leader > e :ALENext < cr > inoremap < leader > p < esc > :ALEPrevious < cr > nnoremap < leader > p :ALEPrevious < cr > Where: let g:ale_linters_explicit : Prevent ALE load only the selected linters. use <leader>e and <leader>p to navigate through the warnings. If you feel that it's too heavy, use ale_lint_on_enter or increase the ale_lint_delay . Use :ALEInfo to see the ALE configuration for the specific buffer. Flakehell \u2691 Flakehell is not supported yet . Until that issue is closed we need the following configuration: let g:ale_python_flake8_executable = flake8helled let g:ale_python_flake8_use_global = 1 Toggle fixers on save \u2691 There are cases when you don't want to run the fixers in your code. Ale doesn't have an option to do it , but zArubaru showed how to do it. If you add to your configuration command! ALEToggleFixer execute \"let g:ale_fix_on_save = get(g:, 'ale_fix_on_save', 0) ? 0 : 1\" You can then use :ALEToggleFixer to activate an deactivate them. vim-easymotion \u2691 EasyMotion provides a much simpler way to use some motions in vim. It takes the <number> out of <number>w or <number>f{char} by highlighting all possible choices and allowing you to press one key to jump directly to the target. When one of the available motions is triggered, all visible text preceding or following the cursor is faded, and motion targets are highlighted. Installation \u2691 Add to Vundle Plugin 'easymotion/vim-easymotion' The configuration can be quite complex, but I'm starting with the basics: \" Easymotion let g :EasyMotion_do_mapping = 0 \" Disable default mappings let g :EasyMotion_keys = 'asdfghjkl' \" Jump to anywhere you want with minimal keystrokes, with just one key binding. \" `s{char}{label}` nmap s < Plug >( easymotion - overwin - f ) \" JK motions: Line motions map < Leader > j < Plug >( easymotion - j ) map < Leader > k < Plug >( easymotion - k ) It's awesome to move between windows with s . Vim Fugitive \u2691 Add portions of file to the index \u2691 To stage only part of the file to a commit, open it and launch :Gdiff . With diffput and diffobtain Vim's functionality you move to the index file (the one in the left) the changes you want to stage. Prepare environment to write the commit message \u2691 When I write the commit message I like to review what changes I'm commiting. To do it I find useful to close all windows and do a vertical split with the changes so I can write the commit message in one of the window while I scroll down in the other. As the changes are usually no the at the top of the file, I scroll the window of the right to the first change and then switch back to the left one in insert mode to start writing. I've also made some movement remappings: jj , kk , <C-d> and <C-u> in insert mode will insert normal mode and go to the window in the right to continue seeing the changes. i , a , o , O : if you are in the changes window it will go to the commit message window in insert mode. Once I've made the commit I want to only retain one buffer. Add the following snippet to do just that: \" Open commit message buffer in fullscreen with a vertical split, and close it with \" leader q au BufNewFile,BufRead *COMMIT_EDITMSG call CommitMessage() function! RestoreBindings() inoremap jj <esc>j inoremap kk <esc>k inoremap <C-d> <C-d> inoremap <C-u> <C-u> nnoremap i i nnoremap a a nnoremap o o nnoremap O O endfunction function! CommitMessage() \" Remap the saving mappings \" Close buffer when saving inoremap <silent> <leader>q <esc>:w<cr> \\| :only<cr> \\| :call RestoreBindings()<cr> \\|:Sayonara<CR> nnoremap <silent> <leader>q <esc>:w<cr> \\| :only<cr> \\| :call RestoreBindings()<cr> \\|:Sayonara<CR> inoremap jj <esc>:wincmd l<cr>j inoremap kk <esc>:wincmd l<cr>k inoremap <C-d> <esc>:wincmd l<cr><C-d> inoremap <C-u> <esc>:wincmd l<cr><C-u> nnoremap i :wincmd h<cr>i nnoremap a :wincmd h<cr>a nnoremap o :wincmd h<cr>o nnoremap O :wincmd h<cr>O \" Remove bad habits inoremap jk <nop> inoremap ZZ <nop> nnoremap ZZ <nop> \" Close all other windows only \" Create a vertical split vsplit \" Go to the right split wincmd l \" Go to the first change execute \"normal! /^diff\\<cr>8j\" \" Clear the search highlights nohl \" Go back to the left split wincmd h \" Enter insert mode execute \"startinsert\" endfunction I'm assuming that you save with <leader>w and that you're using Sayonara to close your buffers. Git push sets the upstream by default \u2691 Add to your config: nnoremap < leader > gp :Git - c push.default = current push < CR > If you want to see the output of the push command , use :copen after the successful push. Vim-test \u2691 A Vim wrapper for running tests on different granularities. Currently the following testing frameworks are supported: Language Frameworks Identifiers C# .NET dotnettest Clojure Fireplace.vim fireplacetest Crystal Crystal crystalspec Elixir ESpec, ExUnit espec , exunit Erlang CommonTest commontest Go Ginkgo, Go ginkgo , gotest Java Maven maventest JavaScript Intern, Jasmine, Jest, Karma, Lab, Mocha, TAP, intern , jasmine , jest , karma , lab , mocha , tap Lua Busted busted PHP Behat, Codeception, Kahlan, Peridot, PHPUnit, PHPSpec behat , codeception , kahlan , peridot , phpunit , phpspec Perl Prove prove Python Django, Nose, Nose2, PyTest, PyUnit djangotest , djangonose nose , nose2 , pytest , pyunit Racket RackUnit rackunit Ruby Cucumber, [M], [Minitest][minitest], Rails, RSpec cucumber , m , minitest , rails , rspec Rust Cargo cargotest Shell Bats bats VimScript Vader.vim, VSpec vader , vspec Features \u2691 Zero dependencies Zero configuration required (it Does the Right Thing\u2122, see Philosophy ) Wide range of test runners which are automagically detected Polyfills for nearest tests (by constructing regexes ) Wide range of execution environments (\" strategies \") Fully customized CLI options configuration Extendable with new runners and strategies Test.vim consists of a core which provides an abstraction over running any kind of tests from the command-line. Concrete test runners are then simply plugged in, so they all work in the same unified way. Issues \u2691 Vim-Abolish \u2691 Error adding elipsis instead of three dots : Pope said that it's not possible :(. References \u2691 ALE supported tools","title":"Vim Plugins"},{"location":"linux/vim/vim_plugins/#black","text":"To install Black you first need python3-venv . sudo apt-get install python3-venv Add the plugin and configure it so vim runs it each time you save. File ~/.vimrc Plugin 'psf/black' \" Black autocmd BufWritePre *.py execute ':Black' A configuration issue exists for neovim. If you encounter the error AttributeError: module 'black' has no attribute 'find_pyproject_toml' , do the following: cd ~/.vim/bundle/black git checkout 19 .10b0 As the default line length is 88 (ugly number by the way), we need to change the indent, python-mode configuration as well \"\" python indent autocmd BufNewFile,BufRead *.py setlocal foldmethod=indent tabstop=4 softtabstop=4 shiftwidth=4 textwidth=88 smarttab expandtab \" python-mode let g:pymode_options_max_line_length = 88 let g:pymode_lint_options_pep8 = {'max_line_length': g:pymode_options_max_line_length}","title":"Black"},{"location":"linux/vim/vim_plugins/#ale","text":"ALE (Asynchronous Lint Engine) is a plugin providing linting (syntax checking and semantic errors) in NeoVim 0.2.0+ and Vim 8 while you edit your text files, and acts as a Vim Language Server Protocol client. ALE makes use of NeoVim and Vim 8 job control functions and timers to run linters on the contents of text buffers and return errors as text changes in Vim. This allows for displaying warnings and errors in files before they are saved back to a filesystem. In other words, this plugin allows you to lint while you type. ALE offers support for fixing code with command line tools in a non-blocking manner with the :ALEFix feature, supporting tools in many languages , like prettier, eslint, autopep8, and more.","title":"ALE"},{"location":"linux/vim/vim_plugins/#installation","text":"Install with Vundle: Plugin 'dense-analysis/ale'","title":"Installation"},{"location":"linux/vim/vim_plugins/#configuration","text":"let g :ale_sign_error = '\u2718' let g :ale_sign_warning = '\u26a0' highlight ALEErrorSign ctermbg = NONE ctermfg = red highlight ALEWarningSign ctermbg = NONE ctermfg = yellow let g :ale_linters_explicit = 1 let g :ale_lint_on_text_changed = 'normal' \" let g:ale_lint_on_text_changed = 'never' let g :ale_lint_on_enter = 0 let g :ale_lint_on_save = 1 let g :ale_fix_on_save = 1 let g :ale_linters = { \\ 'markdown' : [ 'markdownlint' , 'writegood' , 'alex' , 'proselint' ] , \\ 'json' : [ 'jsonlint' ] , \\ 'python' : [ 'flake8' , 'mypy' , 'pylint' , 'alex' ] , \\ 'yaml' : [ 'yamllint' , 'alex' ] , \\ '*' : [ 'alex' , 'writegood' ] , \\} let g :ale_fixers = { \\ '*' : [ 'remove_trailing_lines' , 'trim_whitespace' ] , \\ 'json' : [ 'jq' ] , \\ 'python' : [ 'isort' ] , \\ 'terraform' : [ 'terraform' ] , \\} inoremap < leader > e < esc > :ALENext < cr > nnoremap < leader > e :ALENext < cr > inoremap < leader > p < esc > :ALEPrevious < cr > nnoremap < leader > p :ALEPrevious < cr > Where: let g:ale_linters_explicit : Prevent ALE load only the selected linters. use <leader>e and <leader>p to navigate through the warnings. If you feel that it's too heavy, use ale_lint_on_enter or increase the ale_lint_delay . Use :ALEInfo to see the ALE configuration for the specific buffer.","title":"Configuration"},{"location":"linux/vim/vim_plugins/#flakehell","text":"Flakehell is not supported yet . Until that issue is closed we need the following configuration: let g:ale_python_flake8_executable = flake8helled let g:ale_python_flake8_use_global = 1","title":"Flakehell"},{"location":"linux/vim/vim_plugins/#toggle-fixers-on-save","text":"There are cases when you don't want to run the fixers in your code. Ale doesn't have an option to do it , but zArubaru showed how to do it. If you add to your configuration command! ALEToggleFixer execute \"let g:ale_fix_on_save = get(g:, 'ale_fix_on_save', 0) ? 0 : 1\" You can then use :ALEToggleFixer to activate an deactivate them.","title":"Toggle fixers on save"},{"location":"linux/vim/vim_plugins/#vim-easymotion","text":"EasyMotion provides a much simpler way to use some motions in vim. It takes the <number> out of <number>w or <number>f{char} by highlighting all possible choices and allowing you to press one key to jump directly to the target. When one of the available motions is triggered, all visible text preceding or following the cursor is faded, and motion targets are highlighted.","title":"vim-easymotion"},{"location":"linux/vim/vim_plugins/#installation_1","text":"Add to Vundle Plugin 'easymotion/vim-easymotion' The configuration can be quite complex, but I'm starting with the basics: \" Easymotion let g :EasyMotion_do_mapping = 0 \" Disable default mappings let g :EasyMotion_keys = 'asdfghjkl' \" Jump to anywhere you want with minimal keystrokes, with just one key binding. \" `s{char}{label}` nmap s < Plug >( easymotion - overwin - f ) \" JK motions: Line motions map < Leader > j < Plug >( easymotion - j ) map < Leader > k < Plug >( easymotion - k ) It's awesome to move between windows with s .","title":"Installation"},{"location":"linux/vim/vim_plugins/#vim-fugitive","text":"","title":"Vim Fugitive"},{"location":"linux/vim/vim_plugins/#add-portions-of-file-to-the-index","text":"To stage only part of the file to a commit, open it and launch :Gdiff . With diffput and diffobtain Vim's functionality you move to the index file (the one in the left) the changes you want to stage.","title":"Add portions of file to the index"},{"location":"linux/vim/vim_plugins/#prepare-environment-to-write-the-commit-message","text":"When I write the commit message I like to review what changes I'm commiting. To do it I find useful to close all windows and do a vertical split with the changes so I can write the commit message in one of the window while I scroll down in the other. As the changes are usually no the at the top of the file, I scroll the window of the right to the first change and then switch back to the left one in insert mode to start writing. I've also made some movement remappings: jj , kk , <C-d> and <C-u> in insert mode will insert normal mode and go to the window in the right to continue seeing the changes. i , a , o , O : if you are in the changes window it will go to the commit message window in insert mode. Once I've made the commit I want to only retain one buffer. Add the following snippet to do just that: \" Open commit message buffer in fullscreen with a vertical split, and close it with \" leader q au BufNewFile,BufRead *COMMIT_EDITMSG call CommitMessage() function! RestoreBindings() inoremap jj <esc>j inoremap kk <esc>k inoremap <C-d> <C-d> inoremap <C-u> <C-u> nnoremap i i nnoremap a a nnoremap o o nnoremap O O endfunction function! CommitMessage() \" Remap the saving mappings \" Close buffer when saving inoremap <silent> <leader>q <esc>:w<cr> \\| :only<cr> \\| :call RestoreBindings()<cr> \\|:Sayonara<CR> nnoremap <silent> <leader>q <esc>:w<cr> \\| :only<cr> \\| :call RestoreBindings()<cr> \\|:Sayonara<CR> inoremap jj <esc>:wincmd l<cr>j inoremap kk <esc>:wincmd l<cr>k inoremap <C-d> <esc>:wincmd l<cr><C-d> inoremap <C-u> <esc>:wincmd l<cr><C-u> nnoremap i :wincmd h<cr>i nnoremap a :wincmd h<cr>a nnoremap o :wincmd h<cr>o nnoremap O :wincmd h<cr>O \" Remove bad habits inoremap jk <nop> inoremap ZZ <nop> nnoremap ZZ <nop> \" Close all other windows only \" Create a vertical split vsplit \" Go to the right split wincmd l \" Go to the first change execute \"normal! /^diff\\<cr>8j\" \" Clear the search highlights nohl \" Go back to the left split wincmd h \" Enter insert mode execute \"startinsert\" endfunction I'm assuming that you save with <leader>w and that you're using Sayonara to close your buffers.","title":"Prepare environment to write the commit message"},{"location":"linux/vim/vim_plugins/#git-push-sets-the-upstream-by-default","text":"Add to your config: nnoremap < leader > gp :Git - c push.default = current push < CR > If you want to see the output of the push command , use :copen after the successful push.","title":"Git push sets the upstream by default"},{"location":"linux/vim/vim_plugins/#vim-test","text":"A Vim wrapper for running tests on different granularities. Currently the following testing frameworks are supported: Language Frameworks Identifiers C# .NET dotnettest Clojure Fireplace.vim fireplacetest Crystal Crystal crystalspec Elixir ESpec, ExUnit espec , exunit Erlang CommonTest commontest Go Ginkgo, Go ginkgo , gotest Java Maven maventest JavaScript Intern, Jasmine, Jest, Karma, Lab, Mocha, TAP, intern , jasmine , jest , karma , lab , mocha , tap Lua Busted busted PHP Behat, Codeception, Kahlan, Peridot, PHPUnit, PHPSpec behat , codeception , kahlan , peridot , phpunit , phpspec Perl Prove prove Python Django, Nose, Nose2, PyTest, PyUnit djangotest , djangonose nose , nose2 , pytest , pyunit Racket RackUnit rackunit Ruby Cucumber, [M], [Minitest][minitest], Rails, RSpec cucumber , m , minitest , rails , rspec Rust Cargo cargotest Shell Bats bats VimScript Vader.vim, VSpec vader , vspec","title":"Vim-test"},{"location":"linux/vim/vim_plugins/#features","text":"Zero dependencies Zero configuration required (it Does the Right Thing\u2122, see Philosophy ) Wide range of test runners which are automagically detected Polyfills for nearest tests (by constructing regexes ) Wide range of execution environments (\" strategies \") Fully customized CLI options configuration Extendable with new runners and strategies Test.vim consists of a core which provides an abstraction over running any kind of tests from the command-line. Concrete test runners are then simply plugged in, so they all work in the same unified way.","title":"Features"},{"location":"linux/vim/vim_plugins/#issues","text":"","title":"Issues"},{"location":"linux/vim/vim_plugins/#vim-abolish","text":"Error adding elipsis instead of three dots : Pope said that it's not possible :(.","title":"Vim-Abolish"},{"location":"linux/vim/vim_plugins/#references","text":"ALE supported tools","title":"References"},{"location":"newsletter/0_newsletter_index/","text":"If you want to follow the meaningful changes of this site, you can either: Subscribe to the RSS feed Browse the Newsletter section RSS feed \u2691 You can choose how often you want to see the site updates: Daily Weekly Monthly Yearly Newsletter section \u2691 We aggregate the changes by year, month, week and day. You can navigate this section to see the latest changes. Credits \u2691 The newsletters and the RSS feeds are automatically created from the message commits of the repository thanks to the mkdocs-newsletter plugin.","title":"Newsletters"},{"location":"newsletter/0_newsletter_index/#rss-feed","text":"You can choose how often you want to see the site updates: Daily Weekly Monthly Yearly","title":"RSS feed"},{"location":"newsletter/0_newsletter_index/#newsletter-section","text":"We aggregate the changes by year, month, week and day. You can navigate this section to see the latest changes.","title":"Newsletter section"},{"location":"newsletter/0_newsletter_index/#credits","text":"The newsletters and the RSS feeds are automatically created from the message commits of the repository thanks to the mkdocs-newsletter plugin.","title":"Credits"},{"location":"newsletter/2020/","text":"Meta \u2691 Projects \u2691 New: Update information on active projects. DevOps \u2691 Infrastructure as Code \u2691 Helm \u2691 New: Introduce helm-git to install charts directly from git repositories. Helmfile \u2691 New: Tell how to make long diffs usable. Infrastructure Solutions \u2691 Kubernetes \u2691 New: Explain jobs, cronjobs and how to monitor them with prometheus. New: Explain how to debug cronjob logs. Kubectl \u2691 New: Add kubectl command cheatsheet. Continuous Integration \u2691 Reorganization: Split CI documents into their own pages. Black \u2691 New: Explain how to reveal the type of an expression. Improvement: Add git link. Correction: Configure black to process long lines. Use the --experimental-string-procesing flag to process long lines. New: Introduce the alex linter. Alex helps you find gender favoring, polarizing, race related, religion inconsiderate, or other unequal phrasing in text. New: Explain how to skip one line. New: Explain how to fix the Module X has no attribute Y. New: Explain how to prevent the formatter on some lines. Yamlfix \u2691 New: Introduce flakehell python linter. Flakehell is a Flake8 wrapper to make it cool. Improvement: Annotate the slowness of the bandit tests in pre-commit. New: Add yamlfix formatter. Markdownlint \u2691 New: Introduce the markdownlint linter. markdownlint is A linter for Markdown files. Proselint \u2691 New: Introduce proselint linter. Proselint is another linter for prose. feat(write-good) introduce the write-good linter write-good is a naive linter for English prose. Automating Processes \u2691 cookiecutter \u2691 New: Add cookiecutter template testing guidelines. New: Explain how to remove unwanted directories. New: Explain how to use a default configuration for all your templates. Improvement: Add references on why is not easy to update cookiecutter templates. New: Explain how to debug failing cookiecutter tests. cruft \u2691 New: Introduce cruft tool to manage cookiecutter templates. cruft allows you to maintain all the necessary boilerplate for packaging and building projects separate from the code you intentionally write. Fully compatible with existing Cookiecutter templates. Monitoring \u2691 Prometheus \u2691 New: Explain how to find a metric name. New: Add statistical analysis on instance sizes using prometheus metrics. Explain how to configure prometheus to automatically check if your instances are of the correct size and which are your bottlenecks. New: Explain how to install with docker. Reorganization: Move the installation of docker to prometheus install. Node Exporter \u2691 New: Filter out stopped instances. Coding \u2691 Python \u2691 New: Add cookiecutter documentation. New: Add docker construction for a python project. New: Add commit guidelines with commitizen. Correction: Type hints of subclasses of abstract classes. New: Introduce the python docstrings. New: Add python snippets article. Includes the generation of OpenSSH keys with the cryptography library Improvement: Correct typo in the generation of ssh keys. Correction: Add email to the generated ssh snippet. New: Explain how to make multiline code look clean. Use textwrap.dedent() to define variables that require multiline strings New: Explain how to play a file inside python. New: Explain how to save a python object to a string using ruamel parser. New: Explain how to do a deep copy of a dictionary. New: Explain how to solve the R0201 pylint error. New: Do an initial analysis on Python profiling. New: Introduce some ideas on optimization of python code. Alembic \u2691 Correction: Explain how to use alembic from a python scripts and not. Click \u2691 New: Explain how to setup and test a click application. New: Introduce click arguments. New: Explain how to accept options from environmental variables and how to handle contexts. Correction: Add note on capturing stderr and stdout with caplog instead of click methods. New: Introduce the boolean options and variadic arguments. New: Explain how to use a default command to a click group. New: Improve arguments documentation. Add example on how to use variadic arguments. Explain how to use File and Path click arguments New: Explain how to set the allowable values for an argument. New: Explain how to hide a command from the --help output. Dash \u2691 New: Introduce dash and dash-leaflet. Explain how to initiate dash and how to create a map with dash-leaflet New: Explain how to interact with programs that ask for user input. Type Hints \u2691 New: Improve the definition of objects with multiple types with TypeVar. New: Explain how to use the TypedDict instead of Dict. Useful if the different keys have different types Code Styling \u2691 New: Introduce the deepdiff library. New: Explain how to fix the Pylint R0201 error. New: Explain why we can safely ignore W1203. FactoryBoy \u2691 Correction: Add warning that generating your own attributes doesn't work anymore. Faker \u2691 New: Explain how to populate the faker fixture with random seeds. Correction: Improve the way of generating random seed. Folium \u2691 New: Explain how to use folium, change tileset and load data. Correction: Change the order of the layers. Openstreetmaps is more clear than the IGN, so the waypoints are better seen Pytest \u2691 New: Explain what fixtures are. New: Tell how to use a fixture more than once in a function. New: Document the capsys, caplog and tmpdir builtin fixtures. New: Add freezegun fixture. New: Explain how to save the fixtures into a separate file. New: Explain the different parametrization options. Introduce the awesome pytest-cases. New: Explain how to use marks to group the tests. New: Explain how to test error raising with pytest. Improvement: Solve W0621 Redefining name %r from outer scope (line %s) error. New: Explain how to change the log level with the caplog. Correction: Add link to the unpack_fixture section. Correction: Explain how to better use with pytest.raises snippets. To capture the message in a cleaner way mkdocstrings \u2691 New: Introduce the mkdocstrings library. mkdocstrings is a library to automatically generate mkdocs pages from the code docstrings. Passpy \u2691 New: Introduce the passpy library. passpy a platform independent library and cli that is compatible with ZX2C4's pass . Pydantic \u2691 New: Introduce pydantic with it's models and types. New: Explain how validators work. New: Explain how to export the models. New: Explain how to validate functions and use the mypy plugin. New: Explain how to initialize empty iterables on attributes. New: Explain how to solve the E0611 error. Correction: Explain how to solve the E0611 error in code lines. Correction: Correct pylint R0201 on pydantic models. Pypika \u2691 New: Introduce pypika. New: Explain how to select, filter and delete data. sqlite3 \u2691 New: Introduce the sqlite python library. New: Explain how to get the columns of a sqlite3 query. Talkey \u2691 New: Introduce text to speech python library. Yoyo \u2691 New: Introduce yoyo database migration tool. New: Explain how to do yoyo table relationships. JSON \u2691 New: Add json linters and fixers. SQL \u2691 New: Introduce the sql data types. SQLite \u2691 New: Introduce sqlite and it's upsert feature. Software Architecture \u2691 Domain Driven Design \u2691 New: Explain how to inject fake dependencies into e2e tests with click. Repository Pattern \u2691 New: Warn about the definition of attributes created by the ORMs. Life Management \u2691 Strategy \u2691 New: Introduce strategy document. Differentiate between strategic planning and strategic thinking Health \u2691 Sleep \u2691 New: Explain the benefits of sleep, the consequences of lack of sleep and the physiological effects of sleep including the circadian rhythm and what is melatonin. New: Explain sleep pressure, caffeine and the relationship with the circadian rhythm. New: Explain the independence between circadian and sleep pressure. Operative Systems \u2691 Linux \u2691 HAProxy \u2691 New: Introduce haproxy and how to do a reverse proxy with it. mkdocs \u2691 Correction: Correct meditation navigation element. monica \u2691 New: Add monica installation. Vim \u2691 New: Add ale language server processor Vim plugin. New: Add only part of files to the index to stage. New: Create ALEToggleFixer command to enable/disable fixers. New: Make fugitive commit workflow more user friendly. Correction: Add movement mappings for the commit message window. New: Explain how to use YouCompleteMe to complete prose. Correction: Explain how to limit the autosuggestion results to one when writing prose. New: Explain how to search synonyms inside vim. Correction: Improve the environment to write commits with more bindings and restoring bindings once you close the message. New: Explain how to manage python foldings. ZFS \u2691 New: Introduce zfs and some basic commands. Arts \u2691 Writing \u2691 New: Add there is/are avoidance pattern. New: Explain the guidelines and tools I use for writing. Meditation \u2691 New: Define meditation and it's types. Video Gaming \u2691 The Battle for Wesnoth \u2691 New: Explain how to play the loyalist civilization. Other \u2691 New: Add sh awesome library. Correction: Fix broken links. New: Add factoryboy factory usage. New: Add the xy problem. New: Add tinydb documentation. New: Add prevent cookiecutter from processing some files docs. New: Add first cutting shapes steps. Correction: Improve the changelog generation with commitizen. Correction: Remove unwanted gifs on rave dances. New: Add kicking-running man and tap spin. New: Prevent additional whitespaces when jinja condition is not met. Correction: Correct the running man. New: Add how to keep historical data on database table changes. New: List all process swap space usage. New: Introduce the click python library. New: Add more steps. Quick tempo running man Quick tempo T-Step Francis T-Step Sacco kicks New: Add Francis spin and first version of dance routine. Corrected how to reach the quick tempo running man. New: More guidelines on how to speed up the running man. And refactor in different files New: Add times for next steps to learn. New: Added ash, birch and beech description. New: Introduce wireguard. New: Explain how to debug elasticsearch yellow state. Correction: Update python ci docs. New: Explain how to solve cyclic imports when using type hints. New: Evaluate the different solutions to programmatically interact with databases. Correction: Remove xkcd image. Reorganization: Extract type hints to it's own file. New: Bump material version to 6.0.2. Also take the chance to move images directory to img Correction: Correct repository pattern images path. Correction: Correct image paths. Correction: Add note on the flexibility of query builders. Correction: Remove type hints from python code styling. New: Explain the use of Generic typing. Correction: Correct domain driven design image path. Correction: Remove broken links. New: Explain how to make your python packages mypy compliant. Correction: Explain how to remove all pip packages from a virtualenv. Correction: Discourage the use of pip-tools in the CI. Correction: Explain how to get started. Correction: References between articles. New: Explain how to fix W0707 mypy error. New: Introduce wesnoth, and the northerners and rebels civilizations. New: Explain how to use Wake on Lan. New: Explain how to use TypeVar to specify children class. New: Explain how to solve W0106 in list comprehensions. New: Explain how to solve SIM105.","title":"2020"},{"location":"newsletter/2020/#meta","text":"","title":"Meta"},{"location":"newsletter/2020/#projects","text":"New: Update information on active projects.","title":"Projects"},{"location":"newsletter/2020/#devops","text":"","title":"DevOps"},{"location":"newsletter/2020/#infrastructure-as-code","text":"","title":"Infrastructure as Code"},{"location":"newsletter/2020/#helm","text":"New: Introduce helm-git to install charts directly from git repositories.","title":"Helm"},{"location":"newsletter/2020/#helmfile","text":"New: Tell how to make long diffs usable.","title":"Helmfile"},{"location":"newsletter/2020/#infrastructure-solutions","text":"","title":"Infrastructure Solutions"},{"location":"newsletter/2020/#kubernetes","text":"New: Explain jobs, cronjobs and how to monitor them with prometheus. New: Explain how to debug cronjob logs.","title":"Kubernetes"},{"location":"newsletter/2020/#kubectl","text":"New: Add kubectl command cheatsheet.","title":"Kubectl"},{"location":"newsletter/2020/#continuous-integration","text":"Reorganization: Split CI documents into their own pages.","title":"Continuous Integration"},{"location":"newsletter/2020/#black","text":"New: Explain how to reveal the type of an expression. Improvement: Add git link. Correction: Configure black to process long lines. Use the --experimental-string-procesing flag to process long lines. New: Introduce the alex linter. Alex helps you find gender favoring, polarizing, race related, religion inconsiderate, or other unequal phrasing in text. New: Explain how to skip one line. New: Explain how to fix the Module X has no attribute Y. New: Explain how to prevent the formatter on some lines.","title":"Black"},{"location":"newsletter/2020/#yamlfix","text":"New: Introduce flakehell python linter. Flakehell is a Flake8 wrapper to make it cool. Improvement: Annotate the slowness of the bandit tests in pre-commit. New: Add yamlfix formatter.","title":"Yamlfix"},{"location":"newsletter/2020/#markdownlint","text":"New: Introduce the markdownlint linter. markdownlint is A linter for Markdown files.","title":"Markdownlint"},{"location":"newsletter/2020/#proselint","text":"New: Introduce proselint linter. Proselint is another linter for prose. feat(write-good) introduce the write-good linter write-good is a naive linter for English prose.","title":"Proselint"},{"location":"newsletter/2020/#automating-processes","text":"","title":"Automating Processes"},{"location":"newsletter/2020/#cookiecutter","text":"New: Add cookiecutter template testing guidelines. New: Explain how to remove unwanted directories. New: Explain how to use a default configuration for all your templates. Improvement: Add references on why is not easy to update cookiecutter templates. New: Explain how to debug failing cookiecutter tests.","title":"cookiecutter"},{"location":"newsletter/2020/#cruft","text":"New: Introduce cruft tool to manage cookiecutter templates. cruft allows you to maintain all the necessary boilerplate for packaging and building projects separate from the code you intentionally write. Fully compatible with existing Cookiecutter templates.","title":"cruft"},{"location":"newsletter/2020/#monitoring","text":"","title":"Monitoring"},{"location":"newsletter/2020/#prometheus","text":"New: Explain how to find a metric name. New: Add statistical analysis on instance sizes using prometheus metrics. Explain how to configure prometheus to automatically check if your instances are of the correct size and which are your bottlenecks. New: Explain how to install with docker. Reorganization: Move the installation of docker to prometheus install.","title":"Prometheus"},{"location":"newsletter/2020/#node-exporter","text":"New: Filter out stopped instances.","title":"Node Exporter"},{"location":"newsletter/2020/#coding","text":"","title":"Coding"},{"location":"newsletter/2020/#python","text":"New: Add cookiecutter documentation. New: Add docker construction for a python project. New: Add commit guidelines with commitizen. Correction: Type hints of subclasses of abstract classes. New: Introduce the python docstrings. New: Add python snippets article. Includes the generation of OpenSSH keys with the cryptography library Improvement: Correct typo in the generation of ssh keys. Correction: Add email to the generated ssh snippet. New: Explain how to make multiline code look clean. Use textwrap.dedent() to define variables that require multiline strings New: Explain how to play a file inside python. New: Explain how to save a python object to a string using ruamel parser. New: Explain how to do a deep copy of a dictionary. New: Explain how to solve the R0201 pylint error. New: Do an initial analysis on Python profiling. New: Introduce some ideas on optimization of python code.","title":"Python"},{"location":"newsletter/2020/#alembic","text":"Correction: Explain how to use alembic from a python scripts and not.","title":"Alembic"},{"location":"newsletter/2020/#click","text":"New: Explain how to setup and test a click application. New: Introduce click arguments. New: Explain how to accept options from environmental variables and how to handle contexts. Correction: Add note on capturing stderr and stdout with caplog instead of click methods. New: Introduce the boolean options and variadic arguments. New: Explain how to use a default command to a click group. New: Improve arguments documentation. Add example on how to use variadic arguments. Explain how to use File and Path click arguments New: Explain how to set the allowable values for an argument. New: Explain how to hide a command from the --help output.","title":"Click"},{"location":"newsletter/2020/#dash","text":"New: Introduce dash and dash-leaflet. Explain how to initiate dash and how to create a map with dash-leaflet New: Explain how to interact with programs that ask for user input.","title":"Dash"},{"location":"newsletter/2020/#type-hints","text":"New: Improve the definition of objects with multiple types with TypeVar. New: Explain how to use the TypedDict instead of Dict. Useful if the different keys have different types","title":"Type Hints"},{"location":"newsletter/2020/#code-styling","text":"New: Introduce the deepdiff library. New: Explain how to fix the Pylint R0201 error. New: Explain why we can safely ignore W1203.","title":"Code Styling"},{"location":"newsletter/2020/#factoryboy","text":"Correction: Add warning that generating your own attributes doesn't work anymore.","title":"FactoryBoy"},{"location":"newsletter/2020/#faker","text":"New: Explain how to populate the faker fixture with random seeds. Correction: Improve the way of generating random seed.","title":"Faker"},{"location":"newsletter/2020/#folium","text":"New: Explain how to use folium, change tileset and load data. Correction: Change the order of the layers. Openstreetmaps is more clear than the IGN, so the waypoints are better seen","title":"Folium"},{"location":"newsletter/2020/#pytest","text":"New: Explain what fixtures are. New: Tell how to use a fixture more than once in a function. New: Document the capsys, caplog and tmpdir builtin fixtures. New: Add freezegun fixture. New: Explain how to save the fixtures into a separate file. New: Explain the different parametrization options. Introduce the awesome pytest-cases. New: Explain how to use marks to group the tests. New: Explain how to test error raising with pytest. Improvement: Solve W0621 Redefining name %r from outer scope (line %s) error. New: Explain how to change the log level with the caplog. Correction: Add link to the unpack_fixture section. Correction: Explain how to better use with pytest.raises snippets. To capture the message in a cleaner way","title":"Pytest"},{"location":"newsletter/2020/#mkdocstrings","text":"New: Introduce the mkdocstrings library. mkdocstrings is a library to automatically generate mkdocs pages from the code docstrings.","title":"mkdocstrings"},{"location":"newsletter/2020/#passpy","text":"New: Introduce the passpy library. passpy a platform independent library and cli that is compatible with ZX2C4's pass .","title":"Passpy"},{"location":"newsletter/2020/#pydantic","text":"New: Introduce pydantic with it's models and types. New: Explain how validators work. New: Explain how to export the models. New: Explain how to validate functions and use the mypy plugin. New: Explain how to initialize empty iterables on attributes. New: Explain how to solve the E0611 error. Correction: Explain how to solve the E0611 error in code lines. Correction: Correct pylint R0201 on pydantic models.","title":"Pydantic"},{"location":"newsletter/2020/#pypika","text":"New: Introduce pypika. New: Explain how to select, filter and delete data.","title":"Pypika"},{"location":"newsletter/2020/#sqlite3","text":"New: Introduce the sqlite python library. New: Explain how to get the columns of a sqlite3 query.","title":"sqlite3"},{"location":"newsletter/2020/#talkey","text":"New: Introduce text to speech python library.","title":"Talkey"},{"location":"newsletter/2020/#yoyo","text":"New: Introduce yoyo database migration tool. New: Explain how to do yoyo table relationships.","title":"Yoyo"},{"location":"newsletter/2020/#json","text":"New: Add json linters and fixers.","title":"JSON"},{"location":"newsletter/2020/#sql","text":"New: Introduce the sql data types.","title":"SQL"},{"location":"newsletter/2020/#sqlite","text":"New: Introduce sqlite and it's upsert feature.","title":"SQLite"},{"location":"newsletter/2020/#software-architecture","text":"","title":"Software Architecture"},{"location":"newsletter/2020/#domain-driven-design","text":"New: Explain how to inject fake dependencies into e2e tests with click.","title":"Domain Driven Design"},{"location":"newsletter/2020/#repository-pattern","text":"New: Warn about the definition of attributes created by the ORMs.","title":"Repository Pattern"},{"location":"newsletter/2020/#life-management","text":"","title":"Life Management"},{"location":"newsletter/2020/#strategy","text":"New: Introduce strategy document. Differentiate between strategic planning and strategic thinking","title":"Strategy"},{"location":"newsletter/2020/#health","text":"","title":"Health"},{"location":"newsletter/2020/#sleep","text":"New: Explain the benefits of sleep, the consequences of lack of sleep and the physiological effects of sleep including the circadian rhythm and what is melatonin. New: Explain sleep pressure, caffeine and the relationship with the circadian rhythm. New: Explain the independence between circadian and sleep pressure.","title":"Sleep"},{"location":"newsletter/2020/#operative-systems","text":"","title":"Operative Systems"},{"location":"newsletter/2020/#linux","text":"","title":"Linux"},{"location":"newsletter/2020/#haproxy","text":"New: Introduce haproxy and how to do a reverse proxy with it.","title":"HAProxy"},{"location":"newsletter/2020/#mkdocs","text":"Correction: Correct meditation navigation element.","title":"mkdocs"},{"location":"newsletter/2020/#monica","text":"New: Add monica installation.","title":"monica"},{"location":"newsletter/2020/#vim","text":"New: Add ale language server processor Vim plugin. New: Add only part of files to the index to stage. New: Create ALEToggleFixer command to enable/disable fixers. New: Make fugitive commit workflow more user friendly. Correction: Add movement mappings for the commit message window. New: Explain how to use YouCompleteMe to complete prose. Correction: Explain how to limit the autosuggestion results to one when writing prose. New: Explain how to search synonyms inside vim. Correction: Improve the environment to write commits with more bindings and restoring bindings once you close the message. New: Explain how to manage python foldings.","title":"Vim"},{"location":"newsletter/2020/#zfs","text":"New: Introduce zfs and some basic commands.","title":"ZFS"},{"location":"newsletter/2020/#arts","text":"","title":"Arts"},{"location":"newsletter/2020/#writing","text":"New: Add there is/are avoidance pattern. New: Explain the guidelines and tools I use for writing.","title":"Writing"},{"location":"newsletter/2020/#meditation","text":"New: Define meditation and it's types.","title":"Meditation"},{"location":"newsletter/2020/#video-gaming","text":"","title":"Video Gaming"},{"location":"newsletter/2020/#the-battle-for-wesnoth","text":"New: Explain how to play the loyalist civilization.","title":"The Battle for Wesnoth"},{"location":"newsletter/2020/#other","text":"New: Add sh awesome library. Correction: Fix broken links. New: Add factoryboy factory usage. New: Add the xy problem. New: Add tinydb documentation. New: Add prevent cookiecutter from processing some files docs. New: Add first cutting shapes steps. Correction: Improve the changelog generation with commitizen. Correction: Remove unwanted gifs on rave dances. New: Add kicking-running man and tap spin. New: Prevent additional whitespaces when jinja condition is not met. Correction: Correct the running man. New: Add how to keep historical data on database table changes. New: List all process swap space usage. New: Introduce the click python library. New: Add more steps. Quick tempo running man Quick tempo T-Step Francis T-Step Sacco kicks New: Add Francis spin and first version of dance routine. Corrected how to reach the quick tempo running man. New: More guidelines on how to speed up the running man. And refactor in different files New: Add times for next steps to learn. New: Added ash, birch and beech description. New: Introduce wireguard. New: Explain how to debug elasticsearch yellow state. Correction: Update python ci docs. New: Explain how to solve cyclic imports when using type hints. New: Evaluate the different solutions to programmatically interact with databases. Correction: Remove xkcd image. Reorganization: Extract type hints to it's own file. New: Bump material version to 6.0.2. Also take the chance to move images directory to img Correction: Correct repository pattern images path. Correction: Correct image paths. Correction: Add note on the flexibility of query builders. Correction: Remove type hints from python code styling. New: Explain the use of Generic typing. Correction: Correct domain driven design image path. Correction: Remove broken links. New: Explain how to make your python packages mypy compliant. Correction: Explain how to remove all pip packages from a virtualenv. Correction: Discourage the use of pip-tools in the CI. Correction: Explain how to get started. Correction: References between articles. New: Explain how to fix W0707 mypy error. New: Introduce wesnoth, and the northerners and rebels civilizations. New: Explain how to use Wake on Lan. New: Explain how to use TypeVar to specify children class. New: Explain how to solve W0106 in list comprehensions. New: Explain how to solve SIM105.","title":"Other"},{"location":"newsletter/2020_07/","text":"DevOps \u2691 Automating Processes \u2691 cookiecutter \u2691 New: Add cookiecutter template testing guidelines. Coding \u2691 Python \u2691 New: Add cookiecutter documentation. New: Add docker construction for a python project. New: Add commit guidelines with commitizen. Correction: Type hints of subclasses of abstract classes. JSON \u2691 New: Add json linters and fixers. Operative Systems \u2691 Linux \u2691 Vim \u2691 New: Add ale language server processor Vim plugin. Arts \u2691 Writing \u2691 New: Add there is/are avoidance pattern. Other \u2691 New: Add sh awesome library. Correction: Fix broken links. New: Add factoryboy factory usage. New: Add the xy problem. New: Add tinydb documentation. New: Add prevent cookiecutter from processing some files docs.","title":"July of 2020"},{"location":"newsletter/2020_07/#devops","text":"","title":"DevOps"},{"location":"newsletter/2020_07/#automating-processes","text":"","title":"Automating Processes"},{"location":"newsletter/2020_07/#cookiecutter","text":"New: Add cookiecutter template testing guidelines.","title":"cookiecutter"},{"location":"newsletter/2020_07/#coding","text":"","title":"Coding"},{"location":"newsletter/2020_07/#python","text":"New: Add cookiecutter documentation. New: Add docker construction for a python project. New: Add commit guidelines with commitizen. Correction: Type hints of subclasses of abstract classes.","title":"Python"},{"location":"newsletter/2020_07/#json","text":"New: Add json linters and fixers.","title":"JSON"},{"location":"newsletter/2020_07/#operative-systems","text":"","title":"Operative Systems"},{"location":"newsletter/2020_07/#linux","text":"","title":"Linux"},{"location":"newsletter/2020_07/#vim","text":"New: Add ale language server processor Vim plugin.","title":"Vim"},{"location":"newsletter/2020_07/#arts","text":"","title":"Arts"},{"location":"newsletter/2020_07/#writing","text":"New: Add there is/are avoidance pattern.","title":"Writing"},{"location":"newsletter/2020_07/#other","text":"New: Add sh awesome library. Correction: Fix broken links. New: Add factoryboy factory usage. New: Add the xy problem. New: Add tinydb documentation. New: Add prevent cookiecutter from processing some files docs.","title":"Other"},{"location":"newsletter/2020_07_14/","text":"DevOps \u2691 Automating Processes \u2691 cookiecutter \u2691 New: Add cookiecutter template testing guidelines. Coding \u2691 Python \u2691 New: Add cookiecutter documentation. New: Add docker construction for a python project. New: Add commit guidelines with commitizen. Other \u2691 New: Add sh awesome library.","title":"14th July 2020"},{"location":"newsletter/2020_07_14/#devops","text":"","title":"DevOps"},{"location":"newsletter/2020_07_14/#automating-processes","text":"","title":"Automating Processes"},{"location":"newsletter/2020_07_14/#cookiecutter","text":"New: Add cookiecutter template testing guidelines.","title":"cookiecutter"},{"location":"newsletter/2020_07_14/#coding","text":"","title":"Coding"},{"location":"newsletter/2020_07_14/#python","text":"New: Add cookiecutter documentation. New: Add docker construction for a python project. New: Add commit guidelines with commitizen.","title":"Python"},{"location":"newsletter/2020_07_14/#other","text":"New: Add sh awesome library.","title":"Other"},{"location":"newsletter/2020_07_15/","text":"Coding \u2691 JSON \u2691 New: Add json linters and fixers. Operative Systems \u2691 Linux \u2691 Vim \u2691 New: Add ale language server processor Vim plugin. Arts \u2691 Writing \u2691 New: Add there is/are avoidance pattern. Other \u2691 Correction: Fix broken links.","title":"15th July 2020"},{"location":"newsletter/2020_07_15/#coding","text":"","title":"Coding"},{"location":"newsletter/2020_07_15/#json","text":"New: Add json linters and fixers.","title":"JSON"},{"location":"newsletter/2020_07_15/#operative-systems","text":"","title":"Operative Systems"},{"location":"newsletter/2020_07_15/#linux","text":"","title":"Linux"},{"location":"newsletter/2020_07_15/#vim","text":"New: Add ale language server processor Vim plugin.","title":"Vim"},{"location":"newsletter/2020_07_15/#arts","text":"","title":"Arts"},{"location":"newsletter/2020_07_15/#writing","text":"New: Add there is/are avoidance pattern.","title":"Writing"},{"location":"newsletter/2020_07_15/#other","text":"Correction: Fix broken links.","title":"Other"},{"location":"newsletter/2020_07_16/","text":"Other \u2691 New: Add factoryboy factory usage. New: Add the xy problem. New: Add tinydb documentation.","title":"16th July 2020"},{"location":"newsletter/2020_07_16/#other","text":"New: Add factoryboy factory usage. New: Add the xy problem. New: Add tinydb documentation.","title":"Other"},{"location":"newsletter/2020_07_17/","text":"Coding \u2691 Python \u2691 Correction: Type hints of subclasses of abstract classes.","title":"17th July 2020"},{"location":"newsletter/2020_07_17/#coding","text":"","title":"Coding"},{"location":"newsletter/2020_07_17/#python","text":"Correction: Type hints of subclasses of abstract classes.","title":"Python"},{"location":"newsletter/2020_07_20/","text":"Other \u2691 New: Add prevent cookiecutter from processing some files docs.","title":"20th July 2020"},{"location":"newsletter/2020_07_20/#other","text":"New: Add prevent cookiecutter from processing some files docs.","title":"Other"},{"location":"newsletter/2020_08/","text":"DevOps \u2691 Infrastructure as Code \u2691 Helmfile \u2691 New: Tell how to make long diffs usable. Continuous Integration \u2691 Mypy \u2691 New: Explain how to reveal the type of an expression. Automating Processes \u2691 cookiecutter \u2691 New: Explain how to remove unwanted directories. Monitoring \u2691 Node Exporter \u2691 New: Filter out stopped instances. Coding \u2691 Python \u2691 Alembic \u2691 Correction: Explain how to use alembic from a python scripts and not. Click \u2691 New: Explain how to setup and test a click application. New: Introduce click arguments. New: Explain how to accept options from environmental variables and how to handle contexts. Correction: Add note on capturing stderr and stdout with caplog instead of click methods. Faker \u2691 New: Explain how to populate the faker fixture with random seeds. Pytest \u2691 New: Explain what fixtures are. New: Tell how to use a fixture more than once in a function. New: Document the capsys, caplog and tmpdir builtin fixtures. Operative Systems \u2691 Linux \u2691 monica \u2691 New: Add monica installation. Vim \u2691 New: Add only part of files to the index to stage. Other \u2691 New: Add first cutting shapes steps. Correction: Improve the changelog generation with commitizen. Correction: Remove unwanted gifs on rave dances. New: Add kicking-running man and tap spin. New: Prevent additional whitespaces when jinja condition is not met. Correction: Correct the running man. New: Add how to keep historical data on database table changes. New: List all process swap space usage. New: Introduce the click python library.","title":"August of 2020"},{"location":"newsletter/2020_08/#devops","text":"","title":"DevOps"},{"location":"newsletter/2020_08/#infrastructure-as-code","text":"","title":"Infrastructure as Code"},{"location":"newsletter/2020_08/#helmfile","text":"New: Tell how to make long diffs usable.","title":"Helmfile"},{"location":"newsletter/2020_08/#continuous-integration","text":"","title":"Continuous Integration"},{"location":"newsletter/2020_08/#mypy","text":"New: Explain how to reveal the type of an expression.","title":"Mypy"},{"location":"newsletter/2020_08/#automating-processes","text":"","title":"Automating Processes"},{"location":"newsletter/2020_08/#cookiecutter","text":"New: Explain how to remove unwanted directories.","title":"cookiecutter"},{"location":"newsletter/2020_08/#monitoring","text":"","title":"Monitoring"},{"location":"newsletter/2020_08/#node-exporter","text":"New: Filter out stopped instances.","title":"Node Exporter"},{"location":"newsletter/2020_08/#coding","text":"","title":"Coding"},{"location":"newsletter/2020_08/#python","text":"","title":"Python"},{"location":"newsletter/2020_08/#alembic","text":"Correction: Explain how to use alembic from a python scripts and not.","title":"Alembic"},{"location":"newsletter/2020_08/#click","text":"New: Explain how to setup and test a click application. New: Introduce click arguments. New: Explain how to accept options from environmental variables and how to handle contexts. Correction: Add note on capturing stderr and stdout with caplog instead of click methods.","title":"Click"},{"location":"newsletter/2020_08/#faker","text":"New: Explain how to populate the faker fixture with random seeds.","title":"Faker"},{"location":"newsletter/2020_08/#pytest","text":"New: Explain what fixtures are. New: Tell how to use a fixture more than once in a function. New: Document the capsys, caplog and tmpdir builtin fixtures.","title":"Pytest"},{"location":"newsletter/2020_08/#operative-systems","text":"","title":"Operative Systems"},{"location":"newsletter/2020_08/#linux","text":"","title":"Linux"},{"location":"newsletter/2020_08/#monica","text":"New: Add monica installation.","title":"monica"},{"location":"newsletter/2020_08/#vim","text":"New: Add only part of files to the index to stage.","title":"Vim"},{"location":"newsletter/2020_08/#other","text":"New: Add first cutting shapes steps. Correction: Improve the changelog generation with commitizen. Correction: Remove unwanted gifs on rave dances. New: Add kicking-running man and tap spin. New: Prevent additional whitespaces when jinja condition is not met. Correction: Correct the running man. New: Add how to keep historical data on database table changes. New: List all process swap space usage. New: Introduce the click python library.","title":"Other"},{"location":"newsletter/2020_08_06/","text":"DevOps \u2691 Automating Processes \u2691 cookiecutter \u2691 New: Explain how to remove unwanted directories. Other \u2691 New: Add first cutting shapes steps. Correction: Improve the changelog generation with commitizen. Correction: Remove unwanted gifs on rave dances.","title":"6th August 2020"},{"location":"newsletter/2020_08_06/#devops","text":"","title":"DevOps"},{"location":"newsletter/2020_08_06/#automating-processes","text":"","title":"Automating Processes"},{"location":"newsletter/2020_08_06/#cookiecutter","text":"New: Explain how to remove unwanted directories.","title":"cookiecutter"},{"location":"newsletter/2020_08_06/#other","text":"New: Add first cutting shapes steps. Correction: Improve the changelog generation with commitizen. Correction: Remove unwanted gifs on rave dances.","title":"Other"},{"location":"newsletter/2020_08_07/","text":"Other \u2691 New: Add kicking-running man and tap spin.","title":"7th August 2020"},{"location":"newsletter/2020_08_07/#other","text":"New: Add kicking-running man and tap spin.","title":"Other"},{"location":"newsletter/2020_08_19/","text":"DevOps \u2691 Monitoring \u2691 Node Exporter \u2691 New: Filter out stopped instances. Other \u2691 New: Prevent additional whitespaces when jinja condition is not met. Correction: Correct the running man.","title":"19th August 2020"},{"location":"newsletter/2020_08_19/#devops","text":"","title":"DevOps"},{"location":"newsletter/2020_08_19/#monitoring","text":"","title":"Monitoring"},{"location":"newsletter/2020_08_19/#node-exporter","text":"New: Filter out stopped instances.","title":"Node Exporter"},{"location":"newsletter/2020_08_19/#other","text":"New: Prevent additional whitespaces when jinja condition is not met. Correction: Correct the running man.","title":"Other"},{"location":"newsletter/2020_08_20/","text":"Operative Systems \u2691 Linux \u2691 monica \u2691 New: Add monica installation. Vim \u2691 New: Add only part of files to the index to stage. Other \u2691 New: Add how to keep historical data on database table changes.","title":"20th August 2020"},{"location":"newsletter/2020_08_20/#operative-systems","text":"","title":"Operative Systems"},{"location":"newsletter/2020_08_20/#linux","text":"","title":"Linux"},{"location":"newsletter/2020_08_20/#monica","text":"New: Add monica installation.","title":"monica"},{"location":"newsletter/2020_08_20/#vim","text":"New: Add only part of files to the index to stage.","title":"Vim"},{"location":"newsletter/2020_08_20/#other","text":"New: Add how to keep historical data on database table changes.","title":"Other"},{"location":"newsletter/2020_08_26/","text":"DevOps \u2691 Infrastructure as Code \u2691 Helmfile \u2691 New: Tell how to make long diffs usable. Other \u2691 New: List all process swap space usage.","title":"26th August 2020"},{"location":"newsletter/2020_08_26/#devops","text":"","title":"DevOps"},{"location":"newsletter/2020_08_26/#infrastructure-as-code","text":"","title":"Infrastructure as Code"},{"location":"newsletter/2020_08_26/#helmfile","text":"New: Tell how to make long diffs usable.","title":"Helmfile"},{"location":"newsletter/2020_08_26/#other","text":"New: List all process swap space usage.","title":"Other"},{"location":"newsletter/2020_08_27/","text":"Coding \u2691 Python \u2691 Click \u2691 New: Explain how to setup and test a click application. New: Introduce click arguments. Pytest \u2691 New: Explain what fixtures are. Other \u2691 New: Introduce the click python library.","title":"27th August 2020"},{"location":"newsletter/2020_08_27/#coding","text":"","title":"Coding"},{"location":"newsletter/2020_08_27/#python","text":"","title":"Python"},{"location":"newsletter/2020_08_27/#click","text":"New: Explain how to setup and test a click application. New: Introduce click arguments.","title":"Click"},{"location":"newsletter/2020_08_27/#pytest","text":"New: Explain what fixtures are.","title":"Pytest"},{"location":"newsletter/2020_08_27/#other","text":"New: Introduce the click python library.","title":"Other"},{"location":"newsletter/2020_08_28/","text":"Coding \u2691 Python \u2691 Pytest \u2691 New: Tell how to use a fixture more than once in a function.","title":"28th August 2020"},{"location":"newsletter/2020_08_28/#coding","text":"","title":"Coding"},{"location":"newsletter/2020_08_28/#python","text":"","title":"Python"},{"location":"newsletter/2020_08_28/#pytest","text":"New: Tell how to use a fixture more than once in a function.","title":"Pytest"},{"location":"newsletter/2020_08_29/","text":"DevOps \u2691 Continuous Integration \u2691 Mypy \u2691 New: Explain how to reveal the type of an expression.","title":"29th August 2020"},{"location":"newsletter/2020_08_29/#devops","text":"","title":"DevOps"},{"location":"newsletter/2020_08_29/#continuous-integration","text":"","title":"Continuous Integration"},{"location":"newsletter/2020_08_29/#mypy","text":"New: Explain how to reveal the type of an expression.","title":"Mypy"},{"location":"newsletter/2020_08_31/","text":"Coding \u2691 Python \u2691 Alembic \u2691 Correction: Explain how to use alembic from a python scripts and not. Click \u2691 New: Explain how to accept options from environmental variables and how to handle contexts. Correction: Add note on capturing stderr and stdout with caplog instead of click methods. Faker \u2691 New: Explain how to populate the faker fixture with random seeds. Pytest \u2691 New: Document the capsys, caplog and tmpdir builtin fixtures.","title":"31st August 2020"},{"location":"newsletter/2020_08_31/#coding","text":"","title":"Coding"},{"location":"newsletter/2020_08_31/#python","text":"","title":"Python"},{"location":"newsletter/2020_08_31/#alembic","text":"Correction: Explain how to use alembic from a python scripts and not.","title":"Alembic"},{"location":"newsletter/2020_08_31/#click","text":"New: Explain how to accept options from environmental variables and how to handle contexts. Correction: Add note on capturing stderr and stdout with caplog instead of click methods.","title":"Click"},{"location":"newsletter/2020_08_31/#faker","text":"New: Explain how to populate the faker fixture with random seeds.","title":"Faker"},{"location":"newsletter/2020_08_31/#pytest","text":"New: Document the capsys, caplog and tmpdir builtin fixtures.","title":"Pytest"},{"location":"newsletter/2020_09/","text":"Coding \u2691 Python \u2691 Click \u2691 New: Introduce the boolean options and variadic arguments. New: Explain how to use a default command to a click group. Type Hints \u2691 New: Improve the definition of objects with multiple types with TypeVar. DeepDiff \u2691 New: Introduce the deepdiff library. Pytest \u2691 New: Add freezegun fixture. Software Architecture \u2691 Domain Driven Design \u2691 Repository Pattern \u2691 New: Warn about the definition of attributes created by the ORMs. Other \u2691 New: Add more steps. Quick tempo running man Quick tempo T-Step Francis T-Step Sacco kicks New: Add Francis spin and first version of dance routine. Corrected how to reach the quick tempo running man. New: More guidelines on how to speed up the running man. And refactor in different files New: Add times for next steps to learn. New: Added ash, birch and beech description. New: Introduce wireguard. New: Explain how to debug elasticsearch yellow state. Correction: Update python ci docs. New: Explain how to solve cyclic imports when using type hints.","title":"September of 2020"},{"location":"newsletter/2020_09/#coding","text":"","title":"Coding"},{"location":"newsletter/2020_09/#python","text":"","title":"Python"},{"location":"newsletter/2020_09/#click","text":"New: Introduce the boolean options and variadic arguments. New: Explain how to use a default command to a click group.","title":"Click"},{"location":"newsletter/2020_09/#type-hints","text":"New: Improve the definition of objects with multiple types with TypeVar.","title":"Type Hints"},{"location":"newsletter/2020_09/#deepdiff","text":"New: Introduce the deepdiff library.","title":"DeepDiff"},{"location":"newsletter/2020_09/#pytest","text":"New: Add freezegun fixture.","title":"Pytest"},{"location":"newsletter/2020_09/#software-architecture","text":"","title":"Software Architecture"},{"location":"newsletter/2020_09/#domain-driven-design","text":"","title":"Domain Driven Design"},{"location":"newsletter/2020_09/#repository-pattern","text":"New: Warn about the definition of attributes created by the ORMs.","title":"Repository Pattern"},{"location":"newsletter/2020_09/#other","text":"New: Add more steps. Quick tempo running man Quick tempo T-Step Francis T-Step Sacco kicks New: Add Francis spin and first version of dance routine. Corrected how to reach the quick tempo running man. New: More guidelines on how to speed up the running man. And refactor in different files New: Add times for next steps to learn. New: Added ash, birch and beech description. New: Introduce wireguard. New: Explain how to debug elasticsearch yellow state. Correction: Update python ci docs. New: Explain how to solve cyclic imports when using type hints.","title":"Other"},{"location":"newsletter/2020_09_01/","text":"Other \u2691 New: Add more steps. Quick tempo running man Quick tempo T-Step Francis T-Step Sacco kicks","title":"1st September 2020"},{"location":"newsletter/2020_09_01/#other","text":"New: Add more steps. Quick tempo running man Quick tempo T-Step Francis T-Step Sacco kicks","title":"Other"},{"location":"newsletter/2020_09_02/","text":"Coding \u2691 Python \u2691 Pytest \u2691 New: Add freezegun fixture.","title":"2nd September 2020"},{"location":"newsletter/2020_09_02/#coding","text":"","title":"Coding"},{"location":"newsletter/2020_09_02/#python","text":"","title":"Python"},{"location":"newsletter/2020_09_02/#pytest","text":"New: Add freezegun fixture.","title":"Pytest"},{"location":"newsletter/2020_09_04/","text":"Coding \u2691 Python \u2691 Click \u2691 New: Introduce the boolean options and variadic arguments. Type Hints \u2691 New: Improve the definition of objects with multiple types with TypeVar. DeepDiff \u2691 New: Introduce the deepdiff library. Software Architecture \u2691 Domain Driven Design \u2691 Repository Pattern \u2691 New: Warn about the definition of attributes created by the ORMs.","title":"4th September 2020"},{"location":"newsletter/2020_09_04/#coding","text":"","title":"Coding"},{"location":"newsletter/2020_09_04/#python","text":"","title":"Python"},{"location":"newsletter/2020_09_04/#click","text":"New: Introduce the boolean options and variadic arguments.","title":"Click"},{"location":"newsletter/2020_09_04/#type-hints","text":"New: Improve the definition of objects with multiple types with TypeVar.","title":"Type Hints"},{"location":"newsletter/2020_09_04/#deepdiff","text":"New: Introduce the deepdiff library.","title":"DeepDiff"},{"location":"newsletter/2020_09_04/#software-architecture","text":"","title":"Software Architecture"},{"location":"newsletter/2020_09_04/#domain-driven-design","text":"","title":"Domain Driven Design"},{"location":"newsletter/2020_09_04/#repository-pattern","text":"New: Warn about the definition of attributes created by the ORMs.","title":"Repository Pattern"},{"location":"newsletter/2020_09_07/","text":"Other \u2691 New: Add Francis spin and first version of dance routine. Corrected how to reach the quick tempo running man.","title":"7th September 2020"},{"location":"newsletter/2020_09_07/#other","text":"New: Add Francis spin and first version of dance routine. Corrected how to reach the quick tempo running man.","title":"Other"},{"location":"newsletter/2020_09_08/","text":"Other \u2691 New: More guidelines on how to speed up the running man. And refactor in different files New: Add times for next steps to learn.","title":"8th September 2020"},{"location":"newsletter/2020_09_08/#other","text":"New: More guidelines on how to speed up the running man. And refactor in different files New: Add times for next steps to learn.","title":"Other"},{"location":"newsletter/2020_09_10/","text":"Other \u2691 New: Added ash, birch and beech description.","title":"10th September 2020"},{"location":"newsletter/2020_09_10/#other","text":"New: Added ash, birch and beech description.","title":"Other"},{"location":"newsletter/2020_09_16/","text":"Coding \u2691 Python \u2691 Click \u2691 New: Explain how to use a default command to a click group.","title":"16th September 2020"},{"location":"newsletter/2020_09_16/#coding","text":"","title":"Coding"},{"location":"newsletter/2020_09_16/#python","text":"","title":"Python"},{"location":"newsletter/2020_09_16/#click","text":"New: Explain how to use a default command to a click group.","title":"Click"},{"location":"newsletter/2020_09_21/","text":"Other \u2691 New: Introduce wireguard. New: Explain how to debug elasticsearch yellow state.","title":"21st September 2020"},{"location":"newsletter/2020_09_21/#other","text":"New: Introduce wireguard. New: Explain how to debug elasticsearch yellow state.","title":"Other"},{"location":"newsletter/2020_09_25/","text":"Other \u2691 Correction: Update python ci docs.","title":"25th September 2020"},{"location":"newsletter/2020_09_25/#other","text":"Correction: Update python ci docs.","title":"Other"},{"location":"newsletter/2020_09_30/","text":"Other \u2691 New: Explain how to solve cyclic imports when using type hints.","title":"30th September 2020"},{"location":"newsletter/2020_09_30/#other","text":"New: Explain how to solve cyclic imports when using type hints.","title":"Other"},{"location":"newsletter/2020_10/","text":"DevOps \u2691 Continuous Integration \u2691 Reorganization: Split CI documents into their own pages. Black \u2691 Improvement: Add git link. Correction: Configure black to process long lines. Use the --experimental-string-procesing flag to process long lines. Bandit \u2691 New: Introduce flakehell python linter. Flakehell is a Flake8 wrapper to make it cool. Improvement: Annotate the slowness of the bandit tests in pre-commit. Automating Processes \u2691 cookiecutter \u2691 New: Explain how to use a default configuration for all your templates. Improvement: Add references on why is not easy to update cookiecutter templates. cruft \u2691 New: Introduce cruft tool to manage cookiecutter templates. cruft allows you to maintain all the necessary boilerplate for packaging and building projects separate from the code you intentionally write. Fully compatible with existing Cookiecutter templates. Monitoring \u2691 Prometheus \u2691 New: Explain how to find a metric name. New: Add statistical analysis on instance sizes using prometheus metrics. Explain how to configure prometheus to automatically check if your instances are of the correct size and which are your bottlenecks. Coding \u2691 Python \u2691 New: Introduce the python docstrings. New: Add python snippets article. Includes the generation of OpenSSH keys with the cryptography library Improvement: Correct typo in the generation of ssh keys. Click \u2691 New: Improve arguments documentation. Add example on how to use variadic arguments. Explain how to use File and Path click arguments Pytest \u2691 New: Explain how to save the fixtures into a separate file. New: Explain the different parametrization options. Introduce the awesome pytest-cases. New: Explain how to use marks to group the tests. New: Explain how to test error raising with pytest. Improvement: Solve W0621 Redefining name %r from outer scope (line %s) error. mkdocstrings \u2691 New: Introduce the mkdocstrings library. mkdocstrings is a library to automatically generate mkdocs pages from the code docstrings. Passpy \u2691 New: Introduce the passpy library. passpy a platform independent library and cli that is compatible with ZX2C4's pass . Pydantic \u2691 New: Introduce pydantic with it's models and types. New: Explain how validators work. New: Explain how to export the models. New: Explain how to validate functions and use the mypy plugin. New: Explain how to initialize empty iterables on attributes. Pypika \u2691 New: Introduce pypika. Yoyo \u2691 New: Introduce yoyo database migration tool. New: Explain how to do yoyo table relationships. SQL \u2691 New: Introduce the sql data types. Operative Systems \u2691 Linux \u2691 HAProxy \u2691 New: Introduce haproxy and how to do a reverse proxy with it. ZFS \u2691 New: Introduce zfs and some basic commands. Other \u2691 New: Evaluate the different solutions to programmatically interact with databases. Correction: Remove xkcd image. Reorganization: Extract type hints to it's own file. New: Bump material version to 6.0.2. Also take the chance to move images directory to img Correction: Correct repository pattern images path. Correction: Correct image paths. Correction: Add note on the flexibility of query builders. Correction: Remove type hints from python code styling. New: Explain the use of Generic typing. Correction: Correct domain driven design image path. Correction: Remove broken links. New: Explain how to make your python packages mypy compliant. Correction: Explain how to remove all pip packages from a virtualenv. Correction: Discourage the use of pip-tools in the CI.","title":"October of 2020"},{"location":"newsletter/2020_10/#devops","text":"","title":"DevOps"},{"location":"newsletter/2020_10/#continuous-integration","text":"Reorganization: Split CI documents into their own pages.","title":"Continuous Integration"},{"location":"newsletter/2020_10/#black","text":"Improvement: Add git link. Correction: Configure black to process long lines. Use the --experimental-string-procesing flag to process long lines.","title":"Black"},{"location":"newsletter/2020_10/#bandit","text":"New: Introduce flakehell python linter. Flakehell is a Flake8 wrapper to make it cool. Improvement: Annotate the slowness of the bandit tests in pre-commit.","title":"Bandit"},{"location":"newsletter/2020_10/#automating-processes","text":"","title":"Automating Processes"},{"location":"newsletter/2020_10/#cookiecutter","text":"New: Explain how to use a default configuration for all your templates. Improvement: Add references on why is not easy to update cookiecutter templates.","title":"cookiecutter"},{"location":"newsletter/2020_10/#cruft","text":"New: Introduce cruft tool to manage cookiecutter templates. cruft allows you to maintain all the necessary boilerplate for packaging and building projects separate from the code you intentionally write. Fully compatible with existing Cookiecutter templates.","title":"cruft"},{"location":"newsletter/2020_10/#monitoring","text":"","title":"Monitoring"},{"location":"newsletter/2020_10/#prometheus","text":"New: Explain how to find a metric name. New: Add statistical analysis on instance sizes using prometheus metrics. Explain how to configure prometheus to automatically check if your instances are of the correct size and which are your bottlenecks.","title":"Prometheus"},{"location":"newsletter/2020_10/#coding","text":"","title":"Coding"},{"location":"newsletter/2020_10/#python","text":"New: Introduce the python docstrings. New: Add python snippets article. Includes the generation of OpenSSH keys with the cryptography library Improvement: Correct typo in the generation of ssh keys.","title":"Python"},{"location":"newsletter/2020_10/#click","text":"New: Improve arguments documentation. Add example on how to use variadic arguments. Explain how to use File and Path click arguments","title":"Click"},{"location":"newsletter/2020_10/#pytest","text":"New: Explain how to save the fixtures into a separate file. New: Explain the different parametrization options. Introduce the awesome pytest-cases. New: Explain how to use marks to group the tests. New: Explain how to test error raising with pytest. Improvement: Solve W0621 Redefining name %r from outer scope (line %s) error.","title":"Pytest"},{"location":"newsletter/2020_10/#mkdocstrings","text":"New: Introduce the mkdocstrings library. mkdocstrings is a library to automatically generate mkdocs pages from the code docstrings.","title":"mkdocstrings"},{"location":"newsletter/2020_10/#passpy","text":"New: Introduce the passpy library. passpy a platform independent library and cli that is compatible with ZX2C4's pass .","title":"Passpy"},{"location":"newsletter/2020_10/#pydantic","text":"New: Introduce pydantic with it's models and types. New: Explain how validators work. New: Explain how to export the models. New: Explain how to validate functions and use the mypy plugin. New: Explain how to initialize empty iterables on attributes.","title":"Pydantic"},{"location":"newsletter/2020_10/#pypika","text":"New: Introduce pypika.","title":"Pypika"},{"location":"newsletter/2020_10/#yoyo","text":"New: Introduce yoyo database migration tool. New: Explain how to do yoyo table relationships.","title":"Yoyo"},{"location":"newsletter/2020_10/#sql","text":"New: Introduce the sql data types.","title":"SQL"},{"location":"newsletter/2020_10/#operative-systems","text":"","title":"Operative Systems"},{"location":"newsletter/2020_10/#linux","text":"","title":"Linux"},{"location":"newsletter/2020_10/#haproxy","text":"New: Introduce haproxy and how to do a reverse proxy with it.","title":"HAProxy"},{"location":"newsletter/2020_10/#zfs","text":"New: Introduce zfs and some basic commands.","title":"ZFS"},{"location":"newsletter/2020_10/#other","text":"New: Evaluate the different solutions to programmatically interact with databases. Correction: Remove xkcd image. Reorganization: Extract type hints to it's own file. New: Bump material version to 6.0.2. Also take the chance to move images directory to img Correction: Correct repository pattern images path. Correction: Correct image paths. Correction: Add note on the flexibility of query builders. Correction: Remove type hints from python code styling. New: Explain the use of Generic typing. Correction: Correct domain driven design image path. Correction: Remove broken links. New: Explain how to make your python packages mypy compliant. Correction: Explain how to remove all pip packages from a virtualenv. Correction: Discourage the use of pip-tools in the CI.","title":"Other"},{"location":"newsletter/2020_10_01/","text":"DevOps \u2691 Monitoring \u2691 Prometheus \u2691 New: Explain how to find a metric name. Coding \u2691 Python \u2691 Yoyo \u2691 New: Introduce yoyo database migration tool. SQL \u2691 New: Introduce the sql data types. Other \u2691 New: Evaluate the different solutions to programmatically interact with databases. Correction: Remove xkcd image. Reorganization: Extract type hints to it's own file.","title":"1st October 2020"},{"location":"newsletter/2020_10_01/#devops","text":"","title":"DevOps"},{"location":"newsletter/2020_10_01/#monitoring","text":"","title":"Monitoring"},{"location":"newsletter/2020_10_01/#prometheus","text":"New: Explain how to find a metric name.","title":"Prometheus"},{"location":"newsletter/2020_10_01/#coding","text":"","title":"Coding"},{"location":"newsletter/2020_10_01/#python","text":"","title":"Python"},{"location":"newsletter/2020_10_01/#yoyo","text":"New: Introduce yoyo database migration tool.","title":"Yoyo"},{"location":"newsletter/2020_10_01/#sql","text":"New: Introduce the sql data types.","title":"SQL"},{"location":"newsletter/2020_10_01/#other","text":"New: Evaluate the different solutions to programmatically interact with databases. Correction: Remove xkcd image. Reorganization: Extract type hints to it's own file.","title":"Other"},{"location":"newsletter/2020_10_08/","text":"Coding \u2691 Python \u2691 Pypika \u2691 New: Introduce pypika. Yoyo \u2691 New: Explain how to do yoyo table relationships. Other \u2691 New: Bump material version to 6.0.2. Also take the chance to move images directory to img Correction: Correct repository pattern images path. Correction: Correct image paths. Correction: Add note on the flexibility of query builders. Correction: Remove type hints from python code styling. New: Explain the use of Generic typing. Correction: Correct domain driven design image path. Correction: Remove broken links.","title":"8th October 2020"},{"location":"newsletter/2020_10_08/#coding","text":"","title":"Coding"},{"location":"newsletter/2020_10_08/#python","text":"","title":"Python"},{"location":"newsletter/2020_10_08/#pypika","text":"New: Introduce pypika.","title":"Pypika"},{"location":"newsletter/2020_10_08/#yoyo","text":"New: Explain how to do yoyo table relationships.","title":"Yoyo"},{"location":"newsletter/2020_10_08/#other","text":"New: Bump material version to 6.0.2. Also take the chance to move images directory to img Correction: Correct repository pattern images path. Correction: Correct image paths. Correction: Add note on the flexibility of query builders. Correction: Remove type hints from python code styling. New: Explain the use of Generic typing. Correction: Correct domain driven design image path. Correction: Remove broken links.","title":"Other"},{"location":"newsletter/2020_10_09/","text":"Coding \u2691 Python \u2691 Pydantic \u2691 New: Introduce pydantic with it's models and types. New: Explain how validators work. New: Explain how to export the models. New: Explain how to validate functions and use the mypy plugin. New: Explain how to initialize empty iterables on attributes. Other \u2691 New: Explain how to make your python packages mypy compliant.","title":"9th October 2020"},{"location":"newsletter/2020_10_09/#coding","text":"","title":"Coding"},{"location":"newsletter/2020_10_09/#python","text":"","title":"Python"},{"location":"newsletter/2020_10_09/#pydantic","text":"New: Introduce pydantic with it's models and types. New: Explain how validators work. New: Explain how to export the models. New: Explain how to validate functions and use the mypy plugin. New: Explain how to initialize empty iterables on attributes.","title":"Pydantic"},{"location":"newsletter/2020_10_09/#other","text":"New: Explain how to make your python packages mypy compliant.","title":"Other"},{"location":"newsletter/2020_10_10/","text":"Coding \u2691 Python \u2691 Pytest \u2691 New: Explain how to save the fixtures into a separate file. New: Explain the different parametrization options. Introduce the awesome pytest-cases.","title":"10th October 2020"},{"location":"newsletter/2020_10_10/#coding","text":"","title":"Coding"},{"location":"newsletter/2020_10_10/#python","text":"","title":"Python"},{"location":"newsletter/2020_10_10/#pytest","text":"New: Explain how to save the fixtures into a separate file. New: Explain the different parametrization options. Introduce the awesome pytest-cases.","title":"Pytest"},{"location":"newsletter/2020_10_15/","text":"DevOps \u2691 Automating Processes \u2691 cookiecutter \u2691 New: Explain how to use a default configuration for all your templates. Coding \u2691 Python \u2691 Pytest \u2691 New: Explain how to use marks to group the tests. New: Explain how to test error raising with pytest. Passpy \u2691 New: Introduce the passpy library. passpy a platform independent library and cli that is compatible with ZX2C4's pass .","title":"15th October 2020"},{"location":"newsletter/2020_10_15/#devops","text":"","title":"DevOps"},{"location":"newsletter/2020_10_15/#automating-processes","text":"","title":"Automating Processes"},{"location":"newsletter/2020_10_15/#cookiecutter","text":"New: Explain how to use a default configuration for all your templates.","title":"cookiecutter"},{"location":"newsletter/2020_10_15/#coding","text":"","title":"Coding"},{"location":"newsletter/2020_10_15/#python","text":"","title":"Python"},{"location":"newsletter/2020_10_15/#pytest","text":"New: Explain how to use marks to group the tests. New: Explain how to test error raising with pytest.","title":"Pytest"},{"location":"newsletter/2020_10_15/#passpy","text":"New: Introduce the passpy library. passpy a platform independent library and cli that is compatible with ZX2C4's pass .","title":"Passpy"},{"location":"newsletter/2020_10_16/","text":"DevOps \u2691 Continuous Integration \u2691 Reorganization: Split CI documents into their own pages. Flakehell \u2691 New: Introduce flakehell python linter. Flakehell is a Flake8 wrapper to make it cool. Automating Processes \u2691 cookiecutter \u2691 Improvement: Add references on why is not easy to update cookiecutter templates. cruft \u2691 New: Introduce cruft tool to manage cookiecutter templates. cruft allows you to maintain all the necessary boilerplate for packaging and building projects separate from the code you intentionally write. Fully compatible with existing Cookiecutter templates. Other \u2691 Correction: Explain how to remove all pip packages from a virtualenv.","title":"16th October 2020"},{"location":"newsletter/2020_10_16/#devops","text":"","title":"DevOps"},{"location":"newsletter/2020_10_16/#continuous-integration","text":"Reorganization: Split CI documents into their own pages.","title":"Continuous Integration"},{"location":"newsletter/2020_10_16/#flakehell","text":"New: Introduce flakehell python linter. Flakehell is a Flake8 wrapper to make it cool.","title":"Flakehell"},{"location":"newsletter/2020_10_16/#automating-processes","text":"","title":"Automating Processes"},{"location":"newsletter/2020_10_16/#cookiecutter","text":"Improvement: Add references on why is not easy to update cookiecutter templates.","title":"cookiecutter"},{"location":"newsletter/2020_10_16/#cruft","text":"New: Introduce cruft tool to manage cookiecutter templates. cruft allows you to maintain all the necessary boilerplate for packaging and building projects separate from the code you intentionally write. Fully compatible with existing Cookiecutter templates.","title":"cruft"},{"location":"newsletter/2020_10_16/#other","text":"Correction: Explain how to remove all pip packages from a virtualenv.","title":"Other"},{"location":"newsletter/2020_10_19/","text":"DevOps \u2691 Monitoring \u2691 Prometheus \u2691 New: Add statistical analysis on instance sizes using prometheus metrics. Explain how to configure prometheus to automatically check if your instances are of the correct size and which are your bottlenecks.","title":"19th October 2020"},{"location":"newsletter/2020_10_19/#devops","text":"","title":"DevOps"},{"location":"newsletter/2020_10_19/#monitoring","text":"","title":"Monitoring"},{"location":"newsletter/2020_10_19/#prometheus","text":"New: Add statistical analysis on instance sizes using prometheus metrics. Explain how to configure prometheus to automatically check if your instances are of the correct size and which are your bottlenecks.","title":"Prometheus"},{"location":"newsletter/2020_10_22/","text":"DevOps \u2691 Continuous Integration \u2691 Black \u2691 Improvement: Add git link. Bandit \u2691 Improvement: Annotate the slowness of the bandit tests in pre-commit. Coding \u2691 Python \u2691 New: Introduce the python docstrings. Click \u2691 New: Improve arguments documentation. Add example on how to use variadic arguments. Explain how to use File and Path click arguments mkdocstrings \u2691 New: Introduce the mkdocstrings library. mkdocstrings is a library to automatically generate mkdocs pages from the code docstrings. Other \u2691 Correction: Discourage the use of pip-tools in the CI.","title":"22nd October 2020"},{"location":"newsletter/2020_10_22/#devops","text":"","title":"DevOps"},{"location":"newsletter/2020_10_22/#continuous-integration","text":"","title":"Continuous Integration"},{"location":"newsletter/2020_10_22/#black","text":"Improvement: Add git link.","title":"Black"},{"location":"newsletter/2020_10_22/#bandit","text":"Improvement: Annotate the slowness of the bandit tests in pre-commit.","title":"Bandit"},{"location":"newsletter/2020_10_22/#coding","text":"","title":"Coding"},{"location":"newsletter/2020_10_22/#python","text":"New: Introduce the python docstrings.","title":"Python"},{"location":"newsletter/2020_10_22/#click","text":"New: Improve arguments documentation. Add example on how to use variadic arguments. Explain how to use File and Path click arguments","title":"Click"},{"location":"newsletter/2020_10_22/#mkdocstrings","text":"New: Introduce the mkdocstrings library. mkdocstrings is a library to automatically generate mkdocs pages from the code docstrings.","title":"mkdocstrings"},{"location":"newsletter/2020_10_22/#other","text":"Correction: Discourage the use of pip-tools in the CI.","title":"Other"},{"location":"newsletter/2020_10_23/","text":"Coding \u2691 Python \u2691 Pytest \u2691 Improvement: Solve W0621 Redefining name %r from outer scope (line %s) error.","title":"23rd October 2020"},{"location":"newsletter/2020_10_23/#coding","text":"","title":"Coding"},{"location":"newsletter/2020_10_23/#python","text":"","title":"Python"},{"location":"newsletter/2020_10_23/#pytest","text":"Improvement: Solve W0621 Redefining name %r from outer scope (line %s) error.","title":"Pytest"},{"location":"newsletter/2020_10_26/","text":"DevOps \u2691 Continuous Integration \u2691 Black \u2691 Correction: Configure black to process long lines. Use the --experimental-string-procesing flag to process long lines. Coding \u2691 Python \u2691 New: Add python snippets article. Includes the generation of OpenSSH keys with the cryptography library Improvement: Correct typo in the generation of ssh keys.","title":"26th October 2020"},{"location":"newsletter/2020_10_26/#devops","text":"","title":"DevOps"},{"location":"newsletter/2020_10_26/#continuous-integration","text":"","title":"Continuous Integration"},{"location":"newsletter/2020_10_26/#black","text":"Correction: Configure black to process long lines. Use the --experimental-string-procesing flag to process long lines.","title":"Black"},{"location":"newsletter/2020_10_26/#coding","text":"","title":"Coding"},{"location":"newsletter/2020_10_26/#python","text":"New: Add python snippets article. Includes the generation of OpenSSH keys with the cryptography library Improvement: Correct typo in the generation of ssh keys.","title":"Python"},{"location":"newsletter/2020_10_29/","text":"Operative Systems \u2691 Linux \u2691 HAProxy \u2691 New: Introduce haproxy and how to do a reverse proxy with it. ZFS \u2691 New: Introduce zfs and some basic commands.","title":"29th October 2020"},{"location":"newsletter/2020_10_29/#operative-systems","text":"","title":"Operative Systems"},{"location":"newsletter/2020_10_29/#linux","text":"","title":"Linux"},{"location":"newsletter/2020_10_29/#haproxy","text":"New: Introduce haproxy and how to do a reverse proxy with it.","title":"HAProxy"},{"location":"newsletter/2020_10_29/#zfs","text":"New: Introduce zfs and some basic commands.","title":"ZFS"},{"location":"newsletter/2020_11/","text":"Meta \u2691 Projects \u2691 New: Update information on active projects. DevOps \u2691 Infrastructure as Code \u2691 Helm \u2691 New: Introduce helm-git to install charts directly from git repositories. Infrastructure Solutions \u2691 Kubernetes \u2691 New: Explain jobs, cronjobs and how to monitor them with prometheus. New: Explain how to debug cronjob logs. Kubectl \u2691 New: Add kubectl command cheatsheet. Continuous Integration \u2691 Mypy \u2691 New: Introduce the alex linter. Alex helps you find gender favoring, polarizing, race related, religion inconsiderate, or other unequal phrasing in text. New: Explain how to skip one line. Yamlfix \u2691 New: Add yamlfix formatter. Markdownlint \u2691 New: Introduce the markdownlint linter. markdownlint is A linter for Markdown files. Proselint \u2691 New: Introduce proselint linter. Proselint is another linter for prose. feat(write-good) introduce the write-good linter write-good is a naive linter for English prose. Automating Processes \u2691 cookiecutter \u2691 New: Explain how to debug failing cookiecutter tests. Coding \u2691 Python \u2691 Correction: Add email to the generated ssh snippet. New: Explain how to make multiline code look clean. Use textwrap.dedent() to define variables that require multiline strings New: Explain how to play a file inside python. New: Explain how to save a python object to a string using ruamel parser. Dash \u2691 New: Introduce dash and dash-leaflet. Explain how to initiate dash and how to create a map with dash-leaflet Folium \u2691 New: Explain how to use folium, change tileset and load data. Correction: Change the order of the layers. Openstreetmaps is more clear than the IGN, so the waypoints are better seen Talkey \u2691 New: Introduce text to speech python library. Health \u2691 Sleep \u2691 New: Explain the benefits of sleep, the consequences of lack of sleep and the physiological effects of sleep including the circadian rhythm and what is melatonin. New: Explain sleep pressure, caffeine and the relationship with the circadian rhythm. Operative Systems \u2691 Linux \u2691 mkdocs \u2691 Correction: Correct meditation navigation element. Vim \u2691 New: Create ALEToggleFixer command to enable/disable fixers. New: Make fugitive commit workflow more user friendly. Correction: Add movement mappings for the commit message window. New: Explain how to use YouCompleteMe to complete prose. Correction: Explain how to limit the autosuggestion results to one when writing prose. New: Explain how to search synonyms inside vim. Correction: Improve the environment to write commits with more bindings and restoring bindings once you close the message. Arts \u2691 Writing \u2691 New: Explain the guidelines and tools I use for writing. Meditation \u2691 New: Define meditation and it's types. Other \u2691 Correction: Explain how to get started. Correction: References between articles.","title":"November of 2020"},{"location":"newsletter/2020_11/#meta","text":"","title":"Meta"},{"location":"newsletter/2020_11/#projects","text":"New: Update information on active projects.","title":"Projects"},{"location":"newsletter/2020_11/#devops","text":"","title":"DevOps"},{"location":"newsletter/2020_11/#infrastructure-as-code","text":"","title":"Infrastructure as Code"},{"location":"newsletter/2020_11/#helm","text":"New: Introduce helm-git to install charts directly from git repositories.","title":"Helm"},{"location":"newsletter/2020_11/#infrastructure-solutions","text":"","title":"Infrastructure Solutions"},{"location":"newsletter/2020_11/#kubernetes","text":"New: Explain jobs, cronjobs and how to monitor them with prometheus. New: Explain how to debug cronjob logs.","title":"Kubernetes"},{"location":"newsletter/2020_11/#kubectl","text":"New: Add kubectl command cheatsheet.","title":"Kubectl"},{"location":"newsletter/2020_11/#continuous-integration","text":"","title":"Continuous Integration"},{"location":"newsletter/2020_11/#mypy","text":"New: Introduce the alex linter. Alex helps you find gender favoring, polarizing, race related, religion inconsiderate, or other unequal phrasing in text. New: Explain how to skip one line.","title":"Mypy"},{"location":"newsletter/2020_11/#yamlfix","text":"New: Add yamlfix formatter.","title":"Yamlfix"},{"location":"newsletter/2020_11/#markdownlint","text":"New: Introduce the markdownlint linter. markdownlint is A linter for Markdown files.","title":"Markdownlint"},{"location":"newsletter/2020_11/#proselint","text":"New: Introduce proselint linter. Proselint is another linter for prose. feat(write-good) introduce the write-good linter write-good is a naive linter for English prose.","title":"Proselint"},{"location":"newsletter/2020_11/#automating-processes","text":"","title":"Automating Processes"},{"location":"newsletter/2020_11/#cookiecutter","text":"New: Explain how to debug failing cookiecutter tests.","title":"cookiecutter"},{"location":"newsletter/2020_11/#coding","text":"","title":"Coding"},{"location":"newsletter/2020_11/#python","text":"Correction: Add email to the generated ssh snippet. New: Explain how to make multiline code look clean. Use textwrap.dedent() to define variables that require multiline strings New: Explain how to play a file inside python. New: Explain how to save a python object to a string using ruamel parser.","title":"Python"},{"location":"newsletter/2020_11/#dash","text":"New: Introduce dash and dash-leaflet. Explain how to initiate dash and how to create a map with dash-leaflet","title":"Dash"},{"location":"newsletter/2020_11/#folium","text":"New: Explain how to use folium, change tileset and load data. Correction: Change the order of the layers. Openstreetmaps is more clear than the IGN, so the waypoints are better seen","title":"Folium"},{"location":"newsletter/2020_11/#talkey","text":"New: Introduce text to speech python library.","title":"Talkey"},{"location":"newsletter/2020_11/#health","text":"","title":"Health"},{"location":"newsletter/2020_11/#sleep","text":"New: Explain the benefits of sleep, the consequences of lack of sleep and the physiological effects of sleep including the circadian rhythm and what is melatonin. New: Explain sleep pressure, caffeine and the relationship with the circadian rhythm.","title":"Sleep"},{"location":"newsletter/2020_11/#operative-systems","text":"","title":"Operative Systems"},{"location":"newsletter/2020_11/#linux","text":"","title":"Linux"},{"location":"newsletter/2020_11/#mkdocs","text":"Correction: Correct meditation navigation element.","title":"mkdocs"},{"location":"newsletter/2020_11/#vim","text":"New: Create ALEToggleFixer command to enable/disable fixers. New: Make fugitive commit workflow more user friendly. Correction: Add movement mappings for the commit message window. New: Explain how to use YouCompleteMe to complete prose. Correction: Explain how to limit the autosuggestion results to one when writing prose. New: Explain how to search synonyms inside vim. Correction: Improve the environment to write commits with more bindings and restoring bindings once you close the message.","title":"Vim"},{"location":"newsletter/2020_11/#arts","text":"","title":"Arts"},{"location":"newsletter/2020_11/#writing","text":"New: Explain the guidelines and tools I use for writing.","title":"Writing"},{"location":"newsletter/2020_11/#meditation","text":"New: Define meditation and it's types.","title":"Meditation"},{"location":"newsletter/2020_11/#other","text":"Correction: Explain how to get started. Correction: References between articles.","title":"Other"},{"location":"newsletter/2020_11_07/","text":"Coding \u2691 Python \u2691 Folium \u2691 New: Explain how to use folium, change tileset and load data.","title":"7th November 2020"},{"location":"newsletter/2020_11_07/#coding","text":"","title":"Coding"},{"location":"newsletter/2020_11_07/#python","text":"","title":"Python"},{"location":"newsletter/2020_11_07/#folium","text":"New: Explain how to use folium, change tileset and load data.","title":"Folium"},{"location":"newsletter/2020_11_11/","text":"Coding \u2691 Python \u2691 Dash \u2691 New: Introduce dash and dash-leaflet. Explain how to initiate dash and how to create a map with dash-leaflet Folium \u2691 Correction: Change the order of the layers. Openstreetmaps is more clear than the IGN, so the waypoints are better seen","title":"11th November 2020"},{"location":"newsletter/2020_11_11/#coding","text":"","title":"Coding"},{"location":"newsletter/2020_11_11/#python","text":"","title":"Python"},{"location":"newsletter/2020_11_11/#dash","text":"New: Introduce dash and dash-leaflet. Explain how to initiate dash and how to create a map with dash-leaflet","title":"Dash"},{"location":"newsletter/2020_11_11/#folium","text":"Correction: Change the order of the layers. Openstreetmaps is more clear than the IGN, so the waypoints are better seen","title":"Folium"},{"location":"newsletter/2020_11_12/","text":"Meta \u2691 Projects \u2691 New: Update information on active projects. DevOps \u2691 Automating Processes \u2691 cookiecutter \u2691 New: Explain how to debug failing cookiecutter tests. Coding \u2691 Python \u2691 Correction: Add email to the generated ssh snippet. New: Explain how to make multiline code look clean. Use textwrap.dedent() to define variables that require multiline strings Operative Systems \u2691 Linux \u2691 Vim \u2691 New: Create ALEToggleFixer command to enable/disable fixers. New: Make fugitive commit workflow more user friendly.","title":"12th November 2020"},{"location":"newsletter/2020_11_12/#meta","text":"","title":"Meta"},{"location":"newsletter/2020_11_12/#projects","text":"New: Update information on active projects.","title":"Projects"},{"location":"newsletter/2020_11_12/#devops","text":"","title":"DevOps"},{"location":"newsletter/2020_11_12/#automating-processes","text":"","title":"Automating Processes"},{"location":"newsletter/2020_11_12/#cookiecutter","text":"New: Explain how to debug failing cookiecutter tests.","title":"cookiecutter"},{"location":"newsletter/2020_11_12/#coding","text":"","title":"Coding"},{"location":"newsletter/2020_11_12/#python","text":"Correction: Add email to the generated ssh snippet. New: Explain how to make multiline code look clean. Use textwrap.dedent() to define variables that require multiline strings","title":"Python"},{"location":"newsletter/2020_11_12/#operative-systems","text":"","title":"Operative Systems"},{"location":"newsletter/2020_11_12/#linux","text":"","title":"Linux"},{"location":"newsletter/2020_11_12/#vim","text":"New: Create ALEToggleFixer command to enable/disable fixers. New: Make fugitive commit workflow more user friendly.","title":"Vim"},{"location":"newsletter/2020_11_13/","text":"DevOps \u2691 Continuous Integration \u2691 Alex \u2691 New: Introduce the alex linter. Alex helps you find gender favoring, polarizing, race related, religion inconsiderate, or other unequal phrasing in text. Markdownlint \u2691 New: Introduce the markdownlint linter. markdownlint is A linter for Markdown files. Proselint \u2691 New: Introduce proselint linter. Proselint is another linter for prose. feat(write-good) introduce the write-good linter write-good is a naive linter for English prose. Operative Systems \u2691 Linux \u2691 Vim \u2691 Correction: Add movement mappings for the commit message window. Arts \u2691 Writing \u2691 New: Explain the guidelines and tools I use for writing.","title":"13th November 2020"},{"location":"newsletter/2020_11_13/#devops","text":"","title":"DevOps"},{"location":"newsletter/2020_11_13/#continuous-integration","text":"","title":"Continuous Integration"},{"location":"newsletter/2020_11_13/#alex","text":"New: Introduce the alex linter. Alex helps you find gender favoring, polarizing, race related, religion inconsiderate, or other unequal phrasing in text.","title":"Alex"},{"location":"newsletter/2020_11_13/#markdownlint","text":"New: Introduce the markdownlint linter. markdownlint is A linter for Markdown files.","title":"Markdownlint"},{"location":"newsletter/2020_11_13/#proselint","text":"New: Introduce proselint linter. Proselint is another linter for prose. feat(write-good) introduce the write-good linter write-good is a naive linter for English prose.","title":"Proselint"},{"location":"newsletter/2020_11_13/#operative-systems","text":"","title":"Operative Systems"},{"location":"newsletter/2020_11_13/#linux","text":"","title":"Linux"},{"location":"newsletter/2020_11_13/#vim","text":"Correction: Add movement mappings for the commit message window.","title":"Vim"},{"location":"newsletter/2020_11_13/#arts","text":"","title":"Arts"},{"location":"newsletter/2020_11_13/#writing","text":"New: Explain the guidelines and tools I use for writing.","title":"Writing"},{"location":"newsletter/2020_11_18/","text":"DevOps \u2691 Infrastructure as Code \u2691 Helm \u2691 New: Introduce helm-git to install charts directly from git repositories. Infrastructure Solutions \u2691 Kubernetes \u2691 New: Explain jobs, cronjobs and how to monitor them with prometheus. Kubectl \u2691 New: Add kubectl command cheatsheet.","title":"18th November 2020"},{"location":"newsletter/2020_11_18/#devops","text":"","title":"DevOps"},{"location":"newsletter/2020_11_18/#infrastructure-as-code","text":"","title":"Infrastructure as Code"},{"location":"newsletter/2020_11_18/#helm","text":"New: Introduce helm-git to install charts directly from git repositories.","title":"Helm"},{"location":"newsletter/2020_11_18/#infrastructure-solutions","text":"","title":"Infrastructure Solutions"},{"location":"newsletter/2020_11_18/#kubernetes","text":"New: Explain jobs, cronjobs and how to monitor them with prometheus.","title":"Kubernetes"},{"location":"newsletter/2020_11_18/#kubectl","text":"New: Add kubectl command cheatsheet.","title":"Kubectl"},{"location":"newsletter/2020_11_19/","text":"Arts \u2691 Meditation \u2691 New: Define meditation and it's types.","title":"19th November 2020"},{"location":"newsletter/2020_11_19/#arts","text":"","title":"Arts"},{"location":"newsletter/2020_11_19/#meditation","text":"New: Define meditation and it's types.","title":"Meditation"},{"location":"newsletter/2020_11_20/","text":"Coding \u2691 Python \u2691 New: Explain how to play a file inside python. Talkey \u2691 New: Introduce text to speech python library. Health \u2691 Sleep \u2691 New: Explain the benefits of sleep, the consequences of lack of sleep and the physiological effects of sleep including the circadian rhythm and what is melatonin. Operative Systems \u2691 Linux \u2691 mkdocs \u2691 Correction: Correct meditation navigation element. Other \u2691 Correction: Explain how to get started. Correction: References between articles.","title":"20th November 2020"},{"location":"newsletter/2020_11_20/#coding","text":"","title":"Coding"},{"location":"newsletter/2020_11_20/#python","text":"New: Explain how to play a file inside python.","title":"Python"},{"location":"newsletter/2020_11_20/#talkey","text":"New: Introduce text to speech python library.","title":"Talkey"},{"location":"newsletter/2020_11_20/#health","text":"","title":"Health"},{"location":"newsletter/2020_11_20/#sleep","text":"New: Explain the benefits of sleep, the consequences of lack of sleep and the physiological effects of sleep including the circadian rhythm and what is melatonin.","title":"Sleep"},{"location":"newsletter/2020_11_20/#operative-systems","text":"","title":"Operative Systems"},{"location":"newsletter/2020_11_20/#linux","text":"","title":"Linux"},{"location":"newsletter/2020_11_20/#mkdocs","text":"Correction: Correct meditation navigation element.","title":"mkdocs"},{"location":"newsletter/2020_11_20/#other","text":"Correction: Explain how to get started. Correction: References between articles.","title":"Other"},{"location":"newsletter/2020_11_24/","text":"DevOps \u2691 Infrastructure Solutions \u2691 Kubernetes \u2691 New: Explain how to debug cronjob logs. Continuous Integration \u2691 Mypy \u2691 New: Explain how to skip one line. Coding \u2691 Python \u2691 New: Explain how to save a python object to a string using ruamel parser. Health \u2691 Sleep \u2691 New: Explain sleep pressure, caffeine and the relationship with the circadian rhythm. Operative Systems \u2691 Linux \u2691 Vim \u2691 New: Explain how to use YouCompleteMe to complete prose. Correction: Explain how to limit the autosuggestion results to one when writing prose. New: Explain how to search synonyms inside vim.","title":"24th November 2020"},{"location":"newsletter/2020_11_24/#devops","text":"","title":"DevOps"},{"location":"newsletter/2020_11_24/#infrastructure-solutions","text":"","title":"Infrastructure Solutions"},{"location":"newsletter/2020_11_24/#kubernetes","text":"New: Explain how to debug cronjob logs.","title":"Kubernetes"},{"location":"newsletter/2020_11_24/#continuous-integration","text":"","title":"Continuous Integration"},{"location":"newsletter/2020_11_24/#mypy","text":"New: Explain how to skip one line.","title":"Mypy"},{"location":"newsletter/2020_11_24/#coding","text":"","title":"Coding"},{"location":"newsletter/2020_11_24/#python","text":"New: Explain how to save a python object to a string using ruamel parser.","title":"Python"},{"location":"newsletter/2020_11_24/#health","text":"","title":"Health"},{"location":"newsletter/2020_11_24/#sleep","text":"New: Explain sleep pressure, caffeine and the relationship with the circadian rhythm.","title":"Sleep"},{"location":"newsletter/2020_11_24/#operative-systems","text":"","title":"Operative Systems"},{"location":"newsletter/2020_11_24/#linux","text":"","title":"Linux"},{"location":"newsletter/2020_11_24/#vim","text":"New: Explain how to use YouCompleteMe to complete prose. Correction: Explain how to limit the autosuggestion results to one when writing prose. New: Explain how to search synonyms inside vim.","title":"Vim"},{"location":"newsletter/2020_11_25/","text":"DevOps \u2691 Continuous Integration \u2691 Yamlfix \u2691 New: Add yamlfix formatter. Operative Systems \u2691 Linux \u2691 Vim \u2691 Correction: Improve the environment to write commits with more bindings and restoring bindings once you close the message.","title":"25th November 2020"},{"location":"newsletter/2020_11_25/#devops","text":"","title":"DevOps"},{"location":"newsletter/2020_11_25/#continuous-integration","text":"","title":"Continuous Integration"},{"location":"newsletter/2020_11_25/#yamlfix","text":"New: Add yamlfix formatter.","title":"Yamlfix"},{"location":"newsletter/2020_11_25/#operative-systems","text":"","title":"Operative Systems"},{"location":"newsletter/2020_11_25/#linux","text":"","title":"Linux"},{"location":"newsletter/2020_11_25/#vim","text":"Correction: Improve the environment to write commits with more bindings and restoring bindings once you close the message.","title":"Vim"},{"location":"newsletter/2020_12/","text":"DevOps \u2691 Continuous Integration \u2691 Black \u2691 New: Explain how to fix the Module X has no attribute Y. New: Explain how to prevent the formatter on some lines. Monitoring \u2691 Prometheus \u2691 New: Explain how to install with docker. Reorganization: Move the installation of docker to prometheus install. Coding \u2691 Python \u2691 New: Explain how to do a deep copy of a dictionary. New: Explain how to solve the R0201 pylint error. New: Do an initial analysis on Python profiling. New: Introduce some ideas on optimization of python code. Click \u2691 New: Explain how to set the allowable values for an argument. New: Explain how to hide a command from the --help output. Dash \u2691 New: Explain how to interact with programs that ask for user input. Type Hints \u2691 New: Explain how to use the TypedDict instead of Dict. Useful if the different keys have different types Code Styling \u2691 New: Explain how to fix the Pylint R0201 error. New: Explain why we can safely ignore W1203. FactoryBoy \u2691 Correction: Add warning that generating your own attributes doesn't work anymore. Faker \u2691 Correction: Improve the way of generating random seed. Pytest \u2691 New: Explain how to change the log level with the caplog. Correction: Add link to the unpack_fixture section. Correction: Explain how to better use with pytest.raises snippets. To capture the message in a cleaner way Pydantic \u2691 New: Explain how to solve the E0611 error. Correction: Explain how to solve the E0611 error in code lines. Correction: Correct pylint R0201 on pydantic models. Pypika \u2691 New: Explain how to select, filter and delete data. sqlite3 \u2691 New: Introduce the sqlite python library. New: Explain how to get the columns of a sqlite3 query. SQLite \u2691 New: Introduce sqlite and it's upsert feature. Software Architecture \u2691 Domain Driven Design \u2691 New: Explain how to inject fake dependencies into e2e tests with click. Life Management \u2691 Strategy \u2691 New: Introduce strategy document. Differentiate between strategic planning and strategic thinking Health \u2691 Sleep \u2691 New: Explain the independence between circadian and sleep pressure. Operative Systems \u2691 Linux \u2691 Vim \u2691 New: Explain how to manage python foldings. Arts \u2691 Video Gaming \u2691 The Battle for Wesnoth \u2691 New: Explain how to play the loyalist civilization. Other \u2691 New: Explain how to fix W0707 mypy error. New: Introduce wesnoth, and the northerners and rebels civilizations. New: Explain how to use Wake on Lan. New: Explain how to use TypeVar to specify children class. New: Explain how to solve W0106 in list comprehensions. New: Explain how to solve SIM105.","title":"December of 2020"},{"location":"newsletter/2020_12/#devops","text":"","title":"DevOps"},{"location":"newsletter/2020_12/#continuous-integration","text":"","title":"Continuous Integration"},{"location":"newsletter/2020_12/#black","text":"New: Explain how to fix the Module X has no attribute Y. New: Explain how to prevent the formatter on some lines.","title":"Black"},{"location":"newsletter/2020_12/#monitoring","text":"","title":"Monitoring"},{"location":"newsletter/2020_12/#prometheus","text":"New: Explain how to install with docker. Reorganization: Move the installation of docker to prometheus install.","title":"Prometheus"},{"location":"newsletter/2020_12/#coding","text":"","title":"Coding"},{"location":"newsletter/2020_12/#python","text":"New: Explain how to do a deep copy of a dictionary. New: Explain how to solve the R0201 pylint error. New: Do an initial analysis on Python profiling. New: Introduce some ideas on optimization of python code.","title":"Python"},{"location":"newsletter/2020_12/#click","text":"New: Explain how to set the allowable values for an argument. New: Explain how to hide a command from the --help output.","title":"Click"},{"location":"newsletter/2020_12/#dash","text":"New: Explain how to interact with programs that ask for user input.","title":"Dash"},{"location":"newsletter/2020_12/#type-hints","text":"New: Explain how to use the TypedDict instead of Dict. Useful if the different keys have different types","title":"Type Hints"},{"location":"newsletter/2020_12/#code-styling","text":"New: Explain how to fix the Pylint R0201 error. New: Explain why we can safely ignore W1203.","title":"Code Styling"},{"location":"newsletter/2020_12/#factoryboy","text":"Correction: Add warning that generating your own attributes doesn't work anymore.","title":"FactoryBoy"},{"location":"newsletter/2020_12/#faker","text":"Correction: Improve the way of generating random seed.","title":"Faker"},{"location":"newsletter/2020_12/#pytest","text":"New: Explain how to change the log level with the caplog. Correction: Add link to the unpack_fixture section. Correction: Explain how to better use with pytest.raises snippets. To capture the message in a cleaner way","title":"Pytest"},{"location":"newsletter/2020_12/#pydantic","text":"New: Explain how to solve the E0611 error. Correction: Explain how to solve the E0611 error in code lines. Correction: Correct pylint R0201 on pydantic models.","title":"Pydantic"},{"location":"newsletter/2020_12/#pypika","text":"New: Explain how to select, filter and delete data.","title":"Pypika"},{"location":"newsletter/2020_12/#sqlite3","text":"New: Introduce the sqlite python library. New: Explain how to get the columns of a sqlite3 query.","title":"sqlite3"},{"location":"newsletter/2020_12/#sqlite","text":"New: Introduce sqlite and it's upsert feature.","title":"SQLite"},{"location":"newsletter/2020_12/#software-architecture","text":"","title":"Software Architecture"},{"location":"newsletter/2020_12/#domain-driven-design","text":"New: Explain how to inject fake dependencies into e2e tests with click.","title":"Domain Driven Design"},{"location":"newsletter/2020_12/#life-management","text":"","title":"Life Management"},{"location":"newsletter/2020_12/#strategy","text":"New: Introduce strategy document. Differentiate between strategic planning and strategic thinking","title":"Strategy"},{"location":"newsletter/2020_12/#health","text":"","title":"Health"},{"location":"newsletter/2020_12/#sleep","text":"New: Explain the independence between circadian and sleep pressure.","title":"Sleep"},{"location":"newsletter/2020_12/#operative-systems","text":"","title":"Operative Systems"},{"location":"newsletter/2020_12/#linux","text":"","title":"Linux"},{"location":"newsletter/2020_12/#vim","text":"New: Explain how to manage python foldings.","title":"Vim"},{"location":"newsletter/2020_12/#arts","text":"","title":"Arts"},{"location":"newsletter/2020_12/#video-gaming","text":"","title":"Video Gaming"},{"location":"newsletter/2020_12/#the-battle-for-wesnoth","text":"New: Explain how to play the loyalist civilization.","title":"The Battle for Wesnoth"},{"location":"newsletter/2020_12/#other","text":"New: Explain how to fix W0707 mypy error. New: Introduce wesnoth, and the northerners and rebels civilizations. New: Explain how to use Wake on Lan. New: Explain how to use TypeVar to specify children class. New: Explain how to solve W0106 in list comprehensions. New: Explain how to solve SIM105.","title":"Other"},{"location":"newsletter/2020_12_01/","text":"DevOps \u2691 Continuous Integration \u2691 Black \u2691 New: Explain how to fix the Module X has no attribute Y. New: Explain how to prevent the formatter on some lines. Other \u2691 New: Explain how to fix W0707 mypy error.","title":"1st December 2020"},{"location":"newsletter/2020_12_01/#devops","text":"","title":"DevOps"},{"location":"newsletter/2020_12_01/#continuous-integration","text":"","title":"Continuous Integration"},{"location":"newsletter/2020_12_01/#black","text":"New: Explain how to fix the Module X has no attribute Y. New: Explain how to prevent the formatter on some lines.","title":"Black"},{"location":"newsletter/2020_12_01/#other","text":"New: Explain how to fix W0707 mypy error.","title":"Other"},{"location":"newsletter/2020_12_03/","text":"Arts \u2691 Video Gaming \u2691 The Battle for Wesnoth \u2691 New: Explain how to play the loyalist civilization. Other \u2691 New: Introduce wesnoth, and the northerners and rebels civilizations.","title":"3rd December 2020"},{"location":"newsletter/2020_12_03/#arts","text":"","title":"Arts"},{"location":"newsletter/2020_12_03/#video-gaming","text":"","title":"Video Gaming"},{"location":"newsletter/2020_12_03/#the-battle-for-wesnoth","text":"New: Explain how to play the loyalist civilization.","title":"The Battle for Wesnoth"},{"location":"newsletter/2020_12_03/#other","text":"New: Introduce wesnoth, and the northerners and rebels civilizations.","title":"Other"},{"location":"newsletter/2020_12_07/","text":"Coding \u2691 Python \u2691 Dash \u2691 New: Explain how to interact with programs that ask for user input.","title":"7th December 2020"},{"location":"newsletter/2020_12_07/#coding","text":"","title":"Coding"},{"location":"newsletter/2020_12_07/#python","text":"","title":"Python"},{"location":"newsletter/2020_12_07/#dash","text":"New: Explain how to interact with programs that ask for user input.","title":"Dash"},{"location":"newsletter/2020_12_10/","text":"Coding \u2691 Python \u2691 Pytest \u2691 New: Explain how to change the log level with the caplog. Pydantic \u2691 New: Explain how to solve the E0611 error. sqlite3 \u2691 New: Introduce the sqlite python library.","title":"10th December 2020"},{"location":"newsletter/2020_12_10/#coding","text":"","title":"Coding"},{"location":"newsletter/2020_12_10/#python","text":"","title":"Python"},{"location":"newsletter/2020_12_10/#pytest","text":"New: Explain how to change the log level with the caplog.","title":"Pytest"},{"location":"newsletter/2020_12_10/#pydantic","text":"New: Explain how to solve the E0611 error.","title":"Pydantic"},{"location":"newsletter/2020_12_10/#sqlite3","text":"New: Introduce the sqlite python library.","title":"sqlite3"},{"location":"newsletter/2020_12_11/","text":"Coding \u2691 Python \u2691 New: Explain how to do a deep copy of a dictionary. New: Explain how to solve the R0201 pylint error. Pytest \u2691 Correction: Add link to the unpack_fixture section. Pydantic \u2691 Correction: Explain how to solve the E0611 error in code lines. Pypika \u2691 New: Explain how to select, filter and delete data. sqlite3 \u2691 New: Explain how to get the columns of a sqlite3 query.","title":"11th December 2020"},{"location":"newsletter/2020_12_11/#coding","text":"","title":"Coding"},{"location":"newsletter/2020_12_11/#python","text":"New: Explain how to do a deep copy of a dictionary. New: Explain how to solve the R0201 pylint error.","title":"Python"},{"location":"newsletter/2020_12_11/#pytest","text":"Correction: Add link to the unpack_fixture section.","title":"Pytest"},{"location":"newsletter/2020_12_11/#pydantic","text":"Correction: Explain how to solve the E0611 error in code lines.","title":"Pydantic"},{"location":"newsletter/2020_12_11/#pypika","text":"New: Explain how to select, filter and delete data.","title":"Pypika"},{"location":"newsletter/2020_12_11/#sqlite3","text":"New: Explain how to get the columns of a sqlite3 query.","title":"sqlite3"},{"location":"newsletter/2020_12_15/","text":"Coding \u2691 Python \u2691 Code Styling \u2691 New: Explain how to fix the Pylint R0201 error. New: Explain why we can safely ignore W1203. Software Architecture \u2691 Domain Driven Design \u2691 New: Explain how to inject fake dependencies into e2e tests with click.","title":"15th December 2020"},{"location":"newsletter/2020_12_15/#coding","text":"","title":"Coding"},{"location":"newsletter/2020_12_15/#python","text":"","title":"Python"},{"location":"newsletter/2020_12_15/#code-styling","text":"New: Explain how to fix the Pylint R0201 error. New: Explain why we can safely ignore W1203.","title":"Code Styling"},{"location":"newsletter/2020_12_15/#software-architecture","text":"","title":"Software Architecture"},{"location":"newsletter/2020_12_15/#domain-driven-design","text":"New: Explain how to inject fake dependencies into e2e tests with click.","title":"Domain Driven Design"},{"location":"newsletter/2020_12_16/","text":"Coding \u2691 Python \u2691 Click \u2691 New: Explain how to set the allowable values for an argument. New: Explain how to hide a command from the --help output. Type Hints \u2691 New: Explain how to use the TypedDict instead of Dict. Useful if the different keys have different types","title":"16th December 2020"},{"location":"newsletter/2020_12_16/#coding","text":"","title":"Coding"},{"location":"newsletter/2020_12_16/#python","text":"","title":"Python"},{"location":"newsletter/2020_12_16/#click","text":"New: Explain how to set the allowable values for an argument. New: Explain how to hide a command from the --help output.","title":"Click"},{"location":"newsletter/2020_12_16/#type-hints","text":"New: Explain how to use the TypedDict instead of Dict. Useful if the different keys have different types","title":"Type Hints"},{"location":"newsletter/2020_12_18/","text":"Coding \u2691 Python \u2691 New: Do an initial analysis on Python profiling. New: Introduce some ideas on optimization of python code.","title":"18th December 2020"},{"location":"newsletter/2020_12_18/#coding","text":"","title":"Coding"},{"location":"newsletter/2020_12_18/#python","text":"New: Do an initial analysis on Python profiling. New: Introduce some ideas on optimization of python code.","title":"Python"},{"location":"newsletter/2020_12_21/","text":"DevOps \u2691 Monitoring \u2691 Prometheus \u2691 New: Explain how to install with docker. Reorganization: Move the installation of docker to prometheus install.","title":"21st December 2020"},{"location":"newsletter/2020_12_21/#devops","text":"","title":"DevOps"},{"location":"newsletter/2020_12_21/#monitoring","text":"","title":"Monitoring"},{"location":"newsletter/2020_12_21/#prometheus","text":"New: Explain how to install with docker. Reorganization: Move the installation of docker to prometheus install.","title":"Prometheus"},{"location":"newsletter/2020_12_27/","text":"Coding \u2691 Python \u2691 Pytest \u2691 Correction: Explain how to better use with pytest.raises snippets. To capture the message in a cleaner way Life Management \u2691 Strategy \u2691 New: Introduce strategy document. Differentiate between strategic planning and strategic thinking Other \u2691 New: Explain how to use Wake on Lan.","title":"27th December 2020"},{"location":"newsletter/2020_12_27/#coding","text":"","title":"Coding"},{"location":"newsletter/2020_12_27/#python","text":"","title":"Python"},{"location":"newsletter/2020_12_27/#pytest","text":"Correction: Explain how to better use with pytest.raises snippets. To capture the message in a cleaner way","title":"Pytest"},{"location":"newsletter/2020_12_27/#life-management","text":"","title":"Life Management"},{"location":"newsletter/2020_12_27/#strategy","text":"New: Introduce strategy document. Differentiate between strategic planning and strategic thinking","title":"Strategy"},{"location":"newsletter/2020_12_27/#other","text":"New: Explain how to use Wake on Lan.","title":"Other"},{"location":"newsletter/2020_12_29/","text":"Coding \u2691 Python \u2691 FactoryBoy \u2691 Correction: Add warning that generating your own attributes doesn't work anymore. Faker \u2691 Correction: Improve the way of generating random seed. Pydantic \u2691 Correction: Correct pylint R0201 on pydantic models. Health \u2691 Sleep \u2691 New: Explain the independence between circadian and sleep pressure. Other \u2691 New: Explain how to use TypeVar to specify children class.","title":"29th December 2020"},{"location":"newsletter/2020_12_29/#coding","text":"","title":"Coding"},{"location":"newsletter/2020_12_29/#python","text":"","title":"Python"},{"location":"newsletter/2020_12_29/#factoryboy","text":"Correction: Add warning that generating your own attributes doesn't work anymore.","title":"FactoryBoy"},{"location":"newsletter/2020_12_29/#faker","text":"Correction: Improve the way of generating random seed.","title":"Faker"},{"location":"newsletter/2020_12_29/#pydantic","text":"Correction: Correct pylint R0201 on pydantic models.","title":"Pydantic"},{"location":"newsletter/2020_12_29/#health","text":"","title":"Health"},{"location":"newsletter/2020_12_29/#sleep","text":"New: Explain the independence between circadian and sleep pressure.","title":"Sleep"},{"location":"newsletter/2020_12_29/#other","text":"New: Explain how to use TypeVar to specify children class.","title":"Other"},{"location":"newsletter/2020_12_30/","text":"Coding \u2691 SQLite \u2691 New: Introduce sqlite and it's upsert feature. Operative Systems \u2691 Linux \u2691 Vim \u2691 New: Explain how to manage python foldings. Other \u2691 New: Explain how to solve W0106 in list comprehensions. New: Explain how to solve SIM105.","title":"30th December 2020"},{"location":"newsletter/2020_12_30/#coding","text":"","title":"Coding"},{"location":"newsletter/2020_12_30/#sqlite","text":"New: Introduce sqlite and it's upsert feature.","title":"SQLite"},{"location":"newsletter/2020_12_30/#operative-systems","text":"","title":"Operative Systems"},{"location":"newsletter/2020_12_30/#linux","text":"","title":"Linux"},{"location":"newsletter/2020_12_30/#vim","text":"New: Explain how to manage python foldings.","title":"Vim"},{"location":"newsletter/2020_12_30/#other","text":"New: Explain how to solve W0106 in list comprehensions. New: Explain how to solve SIM105.","title":"Other"},{"location":"newsletter/2020_w28/","text":"DevOps \u2691 Automating Processes \u2691 cookiecutter \u2691 New: Add cookiecutter template testing guidelines. Coding \u2691 Python \u2691 New: Add cookiecutter documentation. New: Add docker construction for a python project. New: Add commit guidelines with commitizen. Correction: Type hints of subclasses of abstract classes. JSON \u2691 New: Add json linters and fixers. Operative Systems \u2691 Linux \u2691 Vim \u2691 New: Add ale language server processor Vim plugin. Arts \u2691 Writing \u2691 New: Add there is/are avoidance pattern. Other \u2691 New: Add sh awesome library. Correction: Fix broken links. New: Add factoryboy factory usage. New: Add the xy problem. New: Add tinydb documentation.","title":"28th Week of 2020"},{"location":"newsletter/2020_w28/#devops","text":"","title":"DevOps"},{"location":"newsletter/2020_w28/#automating-processes","text":"","title":"Automating Processes"},{"location":"newsletter/2020_w28/#cookiecutter","text":"New: Add cookiecutter template testing guidelines.","title":"cookiecutter"},{"location":"newsletter/2020_w28/#coding","text":"","title":"Coding"},{"location":"newsletter/2020_w28/#python","text":"New: Add cookiecutter documentation. New: Add docker construction for a python project. New: Add commit guidelines with commitizen. Correction: Type hints of subclasses of abstract classes.","title":"Python"},{"location":"newsletter/2020_w28/#json","text":"New: Add json linters and fixers.","title":"JSON"},{"location":"newsletter/2020_w28/#operative-systems","text":"","title":"Operative Systems"},{"location":"newsletter/2020_w28/#linux","text":"","title":"Linux"},{"location":"newsletter/2020_w28/#vim","text":"New: Add ale language server processor Vim plugin.","title":"Vim"},{"location":"newsletter/2020_w28/#arts","text":"","title":"Arts"},{"location":"newsletter/2020_w28/#writing","text":"New: Add there is/are avoidance pattern.","title":"Writing"},{"location":"newsletter/2020_w28/#other","text":"New: Add sh awesome library. Correction: Fix broken links. New: Add factoryboy factory usage. New: Add the xy problem. New: Add tinydb documentation.","title":"Other"},{"location":"newsletter/2020_w29/","text":"Other \u2691 New: Add prevent cookiecutter from processing some files docs.","title":"29th Week of 2020"},{"location":"newsletter/2020_w29/#other","text":"New: Add prevent cookiecutter from processing some files docs.","title":"Other"},{"location":"newsletter/2020_w31/","text":"DevOps \u2691 Automating Processes \u2691 cookiecutter \u2691 New: Explain how to remove unwanted directories. Other \u2691 New: Add first cutting shapes steps. Correction: Improve the changelog generation with commitizen. Correction: Remove unwanted gifs on rave dances. New: Add kicking-running man and tap spin.","title":"31st Week of 2020"},{"location":"newsletter/2020_w31/#devops","text":"","title":"DevOps"},{"location":"newsletter/2020_w31/#automating-processes","text":"","title":"Automating Processes"},{"location":"newsletter/2020_w31/#cookiecutter","text":"New: Explain how to remove unwanted directories.","title":"cookiecutter"},{"location":"newsletter/2020_w31/#other","text":"New: Add first cutting shapes steps. Correction: Improve the changelog generation with commitizen. Correction: Remove unwanted gifs on rave dances. New: Add kicking-running man and tap spin.","title":"Other"},{"location":"newsletter/2020_w33/","text":"DevOps \u2691 Monitoring \u2691 Node Exporter \u2691 New: Filter out stopped instances. Operative Systems \u2691 Linux \u2691 monica \u2691 New: Add monica installation. Vim \u2691 New: Add only part of files to the index to stage. Other \u2691 New: Prevent additional whitespaces when jinja condition is not met. Correction: Correct the running man. New: Add how to keep historical data on database table changes.","title":"33rd Week of 2020"},{"location":"newsletter/2020_w33/#devops","text":"","title":"DevOps"},{"location":"newsletter/2020_w33/#monitoring","text":"","title":"Monitoring"},{"location":"newsletter/2020_w33/#node-exporter","text":"New: Filter out stopped instances.","title":"Node Exporter"},{"location":"newsletter/2020_w33/#operative-systems","text":"","title":"Operative Systems"},{"location":"newsletter/2020_w33/#linux","text":"","title":"Linux"},{"location":"newsletter/2020_w33/#monica","text":"New: Add monica installation.","title":"monica"},{"location":"newsletter/2020_w33/#vim","text":"New: Add only part of files to the index to stage.","title":"Vim"},{"location":"newsletter/2020_w33/#other","text":"New: Prevent additional whitespaces when jinja condition is not met. Correction: Correct the running man. New: Add how to keep historical data on database table changes.","title":"Other"},{"location":"newsletter/2020_w34/","text":"DevOps \u2691 Infrastructure as Code \u2691 Helmfile \u2691 New: Tell how to make long diffs usable. Continuous Integration \u2691 Mypy \u2691 New: Explain how to reveal the type of an expression. Coding \u2691 Python \u2691 Click \u2691 New: Explain how to setup and test a click application. New: Introduce click arguments. Pytest \u2691 New: Explain what fixtures are. New: Tell how to use a fixture more than once in a function. Other \u2691 New: List all process swap space usage. New: Introduce the click python library.","title":"34th Week of 2020"},{"location":"newsletter/2020_w34/#devops","text":"","title":"DevOps"},{"location":"newsletter/2020_w34/#infrastructure-as-code","text":"","title":"Infrastructure as Code"},{"location":"newsletter/2020_w34/#helmfile","text":"New: Tell how to make long diffs usable.","title":"Helmfile"},{"location":"newsletter/2020_w34/#continuous-integration","text":"","title":"Continuous Integration"},{"location":"newsletter/2020_w34/#mypy","text":"New: Explain how to reveal the type of an expression.","title":"Mypy"},{"location":"newsletter/2020_w34/#coding","text":"","title":"Coding"},{"location":"newsletter/2020_w34/#python","text":"","title":"Python"},{"location":"newsletter/2020_w34/#click","text":"New: Explain how to setup and test a click application. New: Introduce click arguments.","title":"Click"},{"location":"newsletter/2020_w34/#pytest","text":"New: Explain what fixtures are. New: Tell how to use a fixture more than once in a function.","title":"Pytest"},{"location":"newsletter/2020_w34/#other","text":"New: List all process swap space usage. New: Introduce the click python library.","title":"Other"},{"location":"newsletter/2020_w35/","text":"Coding \u2691 Python \u2691 Alembic \u2691 Correction: Explain how to use alembic from a python scripts and not. Click \u2691 New: Explain how to accept options from environmental variables and how to handle contexts. Correction: Add note on capturing stderr and stdout with caplog instead of click methods. New: Introduce the boolean options and variadic arguments. Type Hints \u2691 New: Improve the definition of objects with multiple types with TypeVar. DeepDiff \u2691 New: Introduce the deepdiff library. Faker \u2691 New: Explain how to populate the faker fixture with random seeds. Pytest \u2691 New: Document the capsys, caplog and tmpdir builtin fixtures. New: Add freezegun fixture. Software Architecture \u2691 Domain Driven Design \u2691 Repository Pattern \u2691 New: Warn about the definition of attributes created by the ORMs. Other \u2691 New: Add more steps. Quick tempo running man Quick tempo T-Step Francis T-Step Sacco kicks","title":"35th Week of 2020"},{"location":"newsletter/2020_w35/#coding","text":"","title":"Coding"},{"location":"newsletter/2020_w35/#python","text":"","title":"Python"},{"location":"newsletter/2020_w35/#alembic","text":"Correction: Explain how to use alembic from a python scripts and not.","title":"Alembic"},{"location":"newsletter/2020_w35/#click","text":"New: Explain how to accept options from environmental variables and how to handle contexts. Correction: Add note on capturing stderr and stdout with caplog instead of click methods. New: Introduce the boolean options and variadic arguments.","title":"Click"},{"location":"newsletter/2020_w35/#type-hints","text":"New: Improve the definition of objects with multiple types with TypeVar.","title":"Type Hints"},{"location":"newsletter/2020_w35/#deepdiff","text":"New: Introduce the deepdiff library.","title":"DeepDiff"},{"location":"newsletter/2020_w35/#faker","text":"New: Explain how to populate the faker fixture with random seeds.","title":"Faker"},{"location":"newsletter/2020_w35/#pytest","text":"New: Document the capsys, caplog and tmpdir builtin fixtures. New: Add freezegun fixture.","title":"Pytest"},{"location":"newsletter/2020_w35/#software-architecture","text":"","title":"Software Architecture"},{"location":"newsletter/2020_w35/#domain-driven-design","text":"","title":"Domain Driven Design"},{"location":"newsletter/2020_w35/#repository-pattern","text":"New: Warn about the definition of attributes created by the ORMs.","title":"Repository Pattern"},{"location":"newsletter/2020_w35/#other","text":"New: Add more steps. Quick tempo running man Quick tempo T-Step Francis T-Step Sacco kicks","title":"Other"},{"location":"newsletter/2020_w36/","text":"Other \u2691 New: Add Francis spin and first version of dance routine. Corrected how to reach the quick tempo running man. New: More guidelines on how to speed up the running man. And refactor in different files New: Add times for next steps to learn. New: Added ash, birch and beech description.","title":"36th Week of 2020"},{"location":"newsletter/2020_w36/#other","text":"New: Add Francis spin and first version of dance routine. Corrected how to reach the quick tempo running man. New: More guidelines on how to speed up the running man. And refactor in different files New: Add times for next steps to learn. New: Added ash, birch and beech description.","title":"Other"},{"location":"newsletter/2020_w37/","text":"Coding \u2691 Python \u2691 Click \u2691 New: Explain how to use a default command to a click group.","title":"37th Week of 2020"},{"location":"newsletter/2020_w37/#coding","text":"","title":"Coding"},{"location":"newsletter/2020_w37/#python","text":"","title":"Python"},{"location":"newsletter/2020_w37/#click","text":"New: Explain how to use a default command to a click group.","title":"Click"},{"location":"newsletter/2020_w38/","text":"Other \u2691 New: Introduce wireguard. New: Explain how to debug elasticsearch yellow state. Correction: Update python ci docs.","title":"38th Week of 2020"},{"location":"newsletter/2020_w38/#other","text":"New: Introduce wireguard. New: Explain how to debug elasticsearch yellow state. Correction: Update python ci docs.","title":"Other"},{"location":"newsletter/2020_w39/","text":"DevOps \u2691 Monitoring \u2691 Prometheus \u2691 New: Explain how to find a metric name. Coding \u2691 Python \u2691 Yoyo \u2691 New: Introduce yoyo database migration tool. SQL \u2691 New: Introduce the sql data types. Other \u2691 New: Explain how to solve cyclic imports when using type hints. New: Evaluate the different solutions to programmatically interact with databases. Correction: Remove xkcd image. Reorganization: Extract type hints to it's own file.","title":"39th Week of 2020"},{"location":"newsletter/2020_w39/#devops","text":"","title":"DevOps"},{"location":"newsletter/2020_w39/#monitoring","text":"","title":"Monitoring"},{"location":"newsletter/2020_w39/#prometheus","text":"New: Explain how to find a metric name.","title":"Prometheus"},{"location":"newsletter/2020_w39/#coding","text":"","title":"Coding"},{"location":"newsletter/2020_w39/#python","text":"","title":"Python"},{"location":"newsletter/2020_w39/#yoyo","text":"New: Introduce yoyo database migration tool.","title":"Yoyo"},{"location":"newsletter/2020_w39/#sql","text":"New: Introduce the sql data types.","title":"SQL"},{"location":"newsletter/2020_w39/#other","text":"New: Explain how to solve cyclic imports when using type hints. New: Evaluate the different solutions to programmatically interact with databases. Correction: Remove xkcd image. Reorganization: Extract type hints to it's own file.","title":"Other"},{"location":"newsletter/2020_w40/","text":"Coding \u2691 Python \u2691 Pytest \u2691 New: Explain how to save the fixtures into a separate file. New: Explain the different parametrization options. Introduce the awesome pytest-cases. Pydantic \u2691 New: Introduce pydantic with it's models and types. New: Explain how validators work. New: Explain how to export the models. New: Explain how to validate functions and use the mypy plugin. New: Explain how to initialize empty iterables on attributes. Pypika \u2691 New: Introduce pypika. Yoyo \u2691 New: Explain how to do yoyo table relationships. Other \u2691 New: Bump material version to 6.0.2. Also take the chance to move images directory to img Correction: Correct repository pattern images path. Correction: Correct image paths. Correction: Add note on the flexibility of query builders. Correction: Remove type hints from python code styling. New: Explain the use of Generic typing. Correction: Correct domain driven design image path. Correction: Remove broken links. New: Explain how to make your python packages mypy compliant.","title":"40th Week of 2020"},{"location":"newsletter/2020_w40/#coding","text":"","title":"Coding"},{"location":"newsletter/2020_w40/#python","text":"","title":"Python"},{"location":"newsletter/2020_w40/#pytest","text":"New: Explain how to save the fixtures into a separate file. New: Explain the different parametrization options. Introduce the awesome pytest-cases.","title":"Pytest"},{"location":"newsletter/2020_w40/#pydantic","text":"New: Introduce pydantic with it's models and types. New: Explain how validators work. New: Explain how to export the models. New: Explain how to validate functions and use the mypy plugin. New: Explain how to initialize empty iterables on attributes.","title":"Pydantic"},{"location":"newsletter/2020_w40/#pypika","text":"New: Introduce pypika.","title":"Pypika"},{"location":"newsletter/2020_w40/#yoyo","text":"New: Explain how to do yoyo table relationships.","title":"Yoyo"},{"location":"newsletter/2020_w40/#other","text":"New: Bump material version to 6.0.2. Also take the chance to move images directory to img Correction: Correct repository pattern images path. Correction: Correct image paths. Correction: Add note on the flexibility of query builders. Correction: Remove type hints from python code styling. New: Explain the use of Generic typing. Correction: Correct domain driven design image path. Correction: Remove broken links. New: Explain how to make your python packages mypy compliant.","title":"Other"},{"location":"newsletter/2020_w41/","text":"DevOps \u2691 Continuous Integration \u2691 Reorganization: Split CI documents into their own pages. Flakehell \u2691 New: Introduce flakehell python linter. Flakehell is a Flake8 wrapper to make it cool. Automating Processes \u2691 cookiecutter \u2691 New: Explain how to use a default configuration for all your templates. Improvement: Add references on why is not easy to update cookiecutter templates. cruft \u2691 New: Introduce cruft tool to manage cookiecutter templates. cruft allows you to maintain all the necessary boilerplate for packaging and building projects separate from the code you intentionally write. Fully compatible with existing Cookiecutter templates. Coding \u2691 Python \u2691 Pytest \u2691 New: Explain how to use marks to group the tests. New: Explain how to test error raising with pytest. Passpy \u2691 New: Introduce the passpy library. passpy a platform independent library and cli that is compatible with ZX2C4's pass . Other \u2691 Correction: Explain how to remove all pip packages from a virtualenv.","title":"41st Week of 2020"},{"location":"newsletter/2020_w41/#devops","text":"","title":"DevOps"},{"location":"newsletter/2020_w41/#continuous-integration","text":"Reorganization: Split CI documents into their own pages.","title":"Continuous Integration"},{"location":"newsletter/2020_w41/#flakehell","text":"New: Introduce flakehell python linter. Flakehell is a Flake8 wrapper to make it cool.","title":"Flakehell"},{"location":"newsletter/2020_w41/#automating-processes","text":"","title":"Automating Processes"},{"location":"newsletter/2020_w41/#cookiecutter","text":"New: Explain how to use a default configuration for all your templates. Improvement: Add references on why is not easy to update cookiecutter templates.","title":"cookiecutter"},{"location":"newsletter/2020_w41/#cruft","text":"New: Introduce cruft tool to manage cookiecutter templates. cruft allows you to maintain all the necessary boilerplate for packaging and building projects separate from the code you intentionally write. Fully compatible with existing Cookiecutter templates.","title":"cruft"},{"location":"newsletter/2020_w41/#coding","text":"","title":"Coding"},{"location":"newsletter/2020_w41/#python","text":"","title":"Python"},{"location":"newsletter/2020_w41/#pytest","text":"New: Explain how to use marks to group the tests. New: Explain how to test error raising with pytest.","title":"Pytest"},{"location":"newsletter/2020_w41/#passpy","text":"New: Introduce the passpy library. passpy a platform independent library and cli that is compatible with ZX2C4's pass .","title":"Passpy"},{"location":"newsletter/2020_w41/#other","text":"Correction: Explain how to remove all pip packages from a virtualenv.","title":"Other"},{"location":"newsletter/2020_w42/","text":"DevOps \u2691 Continuous Integration \u2691 Black \u2691 Improvement: Add git link. Bandit \u2691 Improvement: Annotate the slowness of the bandit tests in pre-commit. Monitoring \u2691 Prometheus \u2691 New: Add statistical analysis on instance sizes using prometheus metrics. Explain how to configure prometheus to automatically check if your instances are of the correct size and which are your bottlenecks. Coding \u2691 Python \u2691 New: Introduce the python docstrings. Click \u2691 New: Improve arguments documentation. Add example on how to use variadic arguments. Explain how to use File and Path click arguments Pytest \u2691 Improvement: Solve W0621 Redefining name %r from outer scope (line %s) error. mkdocstrings \u2691 New: Introduce the mkdocstrings library. mkdocstrings is a library to automatically generate mkdocs pages from the code docstrings. Other \u2691 Correction: Discourage the use of pip-tools in the CI.","title":"42nd Week of 2020"},{"location":"newsletter/2020_w42/#devops","text":"","title":"DevOps"},{"location":"newsletter/2020_w42/#continuous-integration","text":"","title":"Continuous Integration"},{"location":"newsletter/2020_w42/#black","text":"Improvement: Add git link.","title":"Black"},{"location":"newsletter/2020_w42/#bandit","text":"Improvement: Annotate the slowness of the bandit tests in pre-commit.","title":"Bandit"},{"location":"newsletter/2020_w42/#monitoring","text":"","title":"Monitoring"},{"location":"newsletter/2020_w42/#prometheus","text":"New: Add statistical analysis on instance sizes using prometheus metrics. Explain how to configure prometheus to automatically check if your instances are of the correct size and which are your bottlenecks.","title":"Prometheus"},{"location":"newsletter/2020_w42/#coding","text":"","title":"Coding"},{"location":"newsletter/2020_w42/#python","text":"New: Introduce the python docstrings.","title":"Python"},{"location":"newsletter/2020_w42/#click","text":"New: Improve arguments documentation. Add example on how to use variadic arguments. Explain how to use File and Path click arguments","title":"Click"},{"location":"newsletter/2020_w42/#pytest","text":"Improvement: Solve W0621 Redefining name %r from outer scope (line %s) error.","title":"Pytest"},{"location":"newsletter/2020_w42/#mkdocstrings","text":"New: Introduce the mkdocstrings library. mkdocstrings is a library to automatically generate mkdocs pages from the code docstrings.","title":"mkdocstrings"},{"location":"newsletter/2020_w42/#other","text":"Correction: Discourage the use of pip-tools in the CI.","title":"Other"},{"location":"newsletter/2020_w43/","text":"DevOps \u2691 Continuous Integration \u2691 Black \u2691 Correction: Configure black to process long lines. Use the --experimental-string-procesing flag to process long lines. Coding \u2691 Python \u2691 New: Add python snippets article. Includes the generation of OpenSSH keys with the cryptography library Improvement: Correct typo in the generation of ssh keys. Operative Systems \u2691 Linux \u2691 HAProxy \u2691 New: Introduce haproxy and how to do a reverse proxy with it. ZFS \u2691 New: Introduce zfs and some basic commands.","title":"43rd Week of 2020"},{"location":"newsletter/2020_w43/#devops","text":"","title":"DevOps"},{"location":"newsletter/2020_w43/#continuous-integration","text":"","title":"Continuous Integration"},{"location":"newsletter/2020_w43/#black","text":"Correction: Configure black to process long lines. Use the --experimental-string-procesing flag to process long lines.","title":"Black"},{"location":"newsletter/2020_w43/#coding","text":"","title":"Coding"},{"location":"newsletter/2020_w43/#python","text":"New: Add python snippets article. Includes the generation of OpenSSH keys with the cryptography library Improvement: Correct typo in the generation of ssh keys.","title":"Python"},{"location":"newsletter/2020_w43/#operative-systems","text":"","title":"Operative Systems"},{"location":"newsletter/2020_w43/#linux","text":"","title":"Linux"},{"location":"newsletter/2020_w43/#haproxy","text":"New: Introduce haproxy and how to do a reverse proxy with it.","title":"HAProxy"},{"location":"newsletter/2020_w43/#zfs","text":"New: Introduce zfs and some basic commands.","title":"ZFS"},{"location":"newsletter/2020_w44/","text":"Coding \u2691 Python \u2691 Folium \u2691 New: Explain how to use folium, change tileset and load data.","title":"44th Week of 2020"},{"location":"newsletter/2020_w44/#coding","text":"","title":"Coding"},{"location":"newsletter/2020_w44/#python","text":"","title":"Python"},{"location":"newsletter/2020_w44/#folium","text":"New: Explain how to use folium, change tileset and load data.","title":"Folium"},{"location":"newsletter/2020_w45/","text":"Meta \u2691 Projects \u2691 New: Update information on active projects. DevOps \u2691 Continuous Integration \u2691 Alex \u2691 New: Introduce the alex linter. Alex helps you find gender favoring, polarizing, race related, religion inconsiderate, or other unequal phrasing in text. Markdownlint \u2691 New: Introduce the markdownlint linter. markdownlint is A linter for Markdown files. Proselint \u2691 New: Introduce proselint linter. Proselint is another linter for prose. feat(write-good) introduce the write-good linter write-good is a naive linter for English prose. Automating Processes \u2691 cookiecutter \u2691 New: Explain how to debug failing cookiecutter tests. Coding \u2691 Python \u2691 Correction: Add email to the generated ssh snippet. New: Explain how to make multiline code look clean. Use textwrap.dedent() to define variables that require multiline strings Dash \u2691 New: Introduce dash and dash-leaflet. Explain how to initiate dash and how to create a map with dash-leaflet Folium \u2691 Correction: Change the order of the layers. Openstreetmaps is more clear than the IGN, so the waypoints are better seen Operative Systems \u2691 Linux \u2691 Vim \u2691 New: Create ALEToggleFixer command to enable/disable fixers. New: Make fugitive commit workflow more user friendly. Correction: Add movement mappings for the commit message window. Arts \u2691 Writing \u2691 New: Explain the guidelines and tools I use for writing.","title":"45th Week of 2020"},{"location":"newsletter/2020_w45/#meta","text":"","title":"Meta"},{"location":"newsletter/2020_w45/#projects","text":"New: Update information on active projects.","title":"Projects"},{"location":"newsletter/2020_w45/#devops","text":"","title":"DevOps"},{"location":"newsletter/2020_w45/#continuous-integration","text":"","title":"Continuous Integration"},{"location":"newsletter/2020_w45/#alex","text":"New: Introduce the alex linter. Alex helps you find gender favoring, polarizing, race related, religion inconsiderate, or other unequal phrasing in text.","title":"Alex"},{"location":"newsletter/2020_w45/#markdownlint","text":"New: Introduce the markdownlint linter. markdownlint is A linter for Markdown files.","title":"Markdownlint"},{"location":"newsletter/2020_w45/#proselint","text":"New: Introduce proselint linter. Proselint is another linter for prose. feat(write-good) introduce the write-good linter write-good is a naive linter for English prose.","title":"Proselint"},{"location":"newsletter/2020_w45/#automating-processes","text":"","title":"Automating Processes"},{"location":"newsletter/2020_w45/#cookiecutter","text":"New: Explain how to debug failing cookiecutter tests.","title":"cookiecutter"},{"location":"newsletter/2020_w45/#coding","text":"","title":"Coding"},{"location":"newsletter/2020_w45/#python","text":"Correction: Add email to the generated ssh snippet. New: Explain how to make multiline code look clean. Use textwrap.dedent() to define variables that require multiline strings","title":"Python"},{"location":"newsletter/2020_w45/#dash","text":"New: Introduce dash and dash-leaflet. Explain how to initiate dash and how to create a map with dash-leaflet","title":"Dash"},{"location":"newsletter/2020_w45/#folium","text":"Correction: Change the order of the layers. Openstreetmaps is more clear than the IGN, so the waypoints are better seen","title":"Folium"},{"location":"newsletter/2020_w45/#operative-systems","text":"","title":"Operative Systems"},{"location":"newsletter/2020_w45/#linux","text":"","title":"Linux"},{"location":"newsletter/2020_w45/#vim","text":"New: Create ALEToggleFixer command to enable/disable fixers. New: Make fugitive commit workflow more user friendly. Correction: Add movement mappings for the commit message window.","title":"Vim"},{"location":"newsletter/2020_w45/#arts","text":"","title":"Arts"},{"location":"newsletter/2020_w45/#writing","text":"New: Explain the guidelines and tools I use for writing.","title":"Writing"},{"location":"newsletter/2020_w46/","text":"DevOps \u2691 Infrastructure as Code \u2691 Helm \u2691 New: Introduce helm-git to install charts directly from git repositories. Infrastructure Solutions \u2691 Kubernetes \u2691 New: Explain jobs, cronjobs and how to monitor them with prometheus. Kubectl \u2691 New: Add kubectl command cheatsheet. Coding \u2691 Python \u2691 New: Explain how to play a file inside python. Talkey \u2691 New: Introduce text to speech python library. Health \u2691 Sleep \u2691 New: Explain the benefits of sleep, the consequences of lack of sleep and the physiological effects of sleep including the circadian rhythm and what is melatonin. Operative Systems \u2691 Linux \u2691 mkdocs \u2691 Correction: Correct meditation navigation element. Arts \u2691 Meditation \u2691 New: Define meditation and it's types. Other \u2691 Correction: Explain how to get started. Correction: References between articles.","title":"46th Week of 2020"},{"location":"newsletter/2020_w46/#devops","text":"","title":"DevOps"},{"location":"newsletter/2020_w46/#infrastructure-as-code","text":"","title":"Infrastructure as Code"},{"location":"newsletter/2020_w46/#helm","text":"New: Introduce helm-git to install charts directly from git repositories.","title":"Helm"},{"location":"newsletter/2020_w46/#infrastructure-solutions","text":"","title":"Infrastructure Solutions"},{"location":"newsletter/2020_w46/#kubernetes","text":"New: Explain jobs, cronjobs and how to monitor them with prometheus.","title":"Kubernetes"},{"location":"newsletter/2020_w46/#kubectl","text":"New: Add kubectl command cheatsheet.","title":"Kubectl"},{"location":"newsletter/2020_w46/#coding","text":"","title":"Coding"},{"location":"newsletter/2020_w46/#python","text":"New: Explain how to play a file inside python.","title":"Python"},{"location":"newsletter/2020_w46/#talkey","text":"New: Introduce text to speech python library.","title":"Talkey"},{"location":"newsletter/2020_w46/#health","text":"","title":"Health"},{"location":"newsletter/2020_w46/#sleep","text":"New: Explain the benefits of sleep, the consequences of lack of sleep and the physiological effects of sleep including the circadian rhythm and what is melatonin.","title":"Sleep"},{"location":"newsletter/2020_w46/#operative-systems","text":"","title":"Operative Systems"},{"location":"newsletter/2020_w46/#linux","text":"","title":"Linux"},{"location":"newsletter/2020_w46/#mkdocs","text":"Correction: Correct meditation navigation element.","title":"mkdocs"},{"location":"newsletter/2020_w46/#arts","text":"","title":"Arts"},{"location":"newsletter/2020_w46/#meditation","text":"New: Define meditation and it's types.","title":"Meditation"},{"location":"newsletter/2020_w46/#other","text":"Correction: Explain how to get started. Correction: References between articles.","title":"Other"},{"location":"newsletter/2020_w47/","text":"DevOps \u2691 Infrastructure Solutions \u2691 Kubernetes \u2691 New: Explain how to debug cronjob logs. Continuous Integration \u2691 Mypy \u2691 New: Explain how to skip one line. Yamlfix \u2691 New: Add yamlfix formatter. Coding \u2691 Python \u2691 New: Explain how to save a python object to a string using ruamel parser. Health \u2691 Sleep \u2691 New: Explain sleep pressure, caffeine and the relationship with the circadian rhythm. Operative Systems \u2691 Linux \u2691 Vim \u2691 New: Explain how to use YouCompleteMe to complete prose. Correction: Explain how to limit the autosuggestion results to one when writing prose. New: Explain how to search synonyms inside vim. Correction: Improve the environment to write commits with more bindings and restoring bindings once you close the message.","title":"47th Week of 2020"},{"location":"newsletter/2020_w47/#devops","text":"","title":"DevOps"},{"location":"newsletter/2020_w47/#infrastructure-solutions","text":"","title":"Infrastructure Solutions"},{"location":"newsletter/2020_w47/#kubernetes","text":"New: Explain how to debug cronjob logs.","title":"Kubernetes"},{"location":"newsletter/2020_w47/#continuous-integration","text":"","title":"Continuous Integration"},{"location":"newsletter/2020_w47/#mypy","text":"New: Explain how to skip one line.","title":"Mypy"},{"location":"newsletter/2020_w47/#yamlfix","text":"New: Add yamlfix formatter.","title":"Yamlfix"},{"location":"newsletter/2020_w47/#coding","text":"","title":"Coding"},{"location":"newsletter/2020_w47/#python","text":"New: Explain how to save a python object to a string using ruamel parser.","title":"Python"},{"location":"newsletter/2020_w47/#health","text":"","title":"Health"},{"location":"newsletter/2020_w47/#sleep","text":"New: Explain sleep pressure, caffeine and the relationship with the circadian rhythm.","title":"Sleep"},{"location":"newsletter/2020_w47/#operative-systems","text":"","title":"Operative Systems"},{"location":"newsletter/2020_w47/#linux","text":"","title":"Linux"},{"location":"newsletter/2020_w47/#vim","text":"New: Explain how to use YouCompleteMe to complete prose. Correction: Explain how to limit the autosuggestion results to one when writing prose. New: Explain how to search synonyms inside vim. Correction: Improve the environment to write commits with more bindings and restoring bindings once you close the message.","title":"Vim"},{"location":"newsletter/2020_w48/","text":"DevOps \u2691 Continuous Integration \u2691 Black \u2691 New: Explain how to fix the Module X has no attribute Y. New: Explain how to prevent the formatter on some lines. Arts \u2691 Video Gaming \u2691 The Battle for Wesnoth \u2691 New: Explain how to play the loyalist civilization. Other \u2691 New: Explain how to fix W0707 mypy error. New: Introduce wesnoth, and the northerners and rebels civilizations.","title":"48th Week of 2020"},{"location":"newsletter/2020_w48/#devops","text":"","title":"DevOps"},{"location":"newsletter/2020_w48/#continuous-integration","text":"","title":"Continuous Integration"},{"location":"newsletter/2020_w48/#black","text":"New: Explain how to fix the Module X has no attribute Y. New: Explain how to prevent the formatter on some lines.","title":"Black"},{"location":"newsletter/2020_w48/#arts","text":"","title":"Arts"},{"location":"newsletter/2020_w48/#video-gaming","text":"","title":"Video Gaming"},{"location":"newsletter/2020_w48/#the-battle-for-wesnoth","text":"New: Explain how to play the loyalist civilization.","title":"The Battle for Wesnoth"},{"location":"newsletter/2020_w48/#other","text":"New: Explain how to fix W0707 mypy error. New: Introduce wesnoth, and the northerners and rebels civilizations.","title":"Other"},{"location":"newsletter/2020_w49/","text":"Coding \u2691 Python \u2691 New: Explain how to do a deep copy of a dictionary. New: Explain how to solve the R0201 pylint error. Dash \u2691 New: Explain how to interact with programs that ask for user input. Pytest \u2691 New: Explain how to change the log level with the caplog. Correction: Add link to the unpack_fixture section. Pydantic \u2691 New: Explain how to solve the E0611 error. Correction: Explain how to solve the E0611 error in code lines. Pypika \u2691 New: Explain how to select, filter and delete data. sqlite3 \u2691 New: Introduce the sqlite python library. New: Explain how to get the columns of a sqlite3 query.","title":"49th Week of 2020"},{"location":"newsletter/2020_w49/#coding","text":"","title":"Coding"},{"location":"newsletter/2020_w49/#python","text":"New: Explain how to do a deep copy of a dictionary. New: Explain how to solve the R0201 pylint error.","title":"Python"},{"location":"newsletter/2020_w49/#dash","text":"New: Explain how to interact with programs that ask for user input.","title":"Dash"},{"location":"newsletter/2020_w49/#pytest","text":"New: Explain how to change the log level with the caplog. Correction: Add link to the unpack_fixture section.","title":"Pytest"},{"location":"newsletter/2020_w49/#pydantic","text":"New: Explain how to solve the E0611 error. Correction: Explain how to solve the E0611 error in code lines.","title":"Pydantic"},{"location":"newsletter/2020_w49/#pypika","text":"New: Explain how to select, filter and delete data.","title":"Pypika"},{"location":"newsletter/2020_w49/#sqlite3","text":"New: Introduce the sqlite python library. New: Explain how to get the columns of a sqlite3 query.","title":"sqlite3"},{"location":"newsletter/2020_w50/","text":"Coding \u2691 Python \u2691 New: Do an initial analysis on Python profiling. New: Introduce some ideas on optimization of python code. Click \u2691 New: Explain how to set the allowable values for an argument. New: Explain how to hide a command from the --help output. Type Hints \u2691 New: Explain how to use the TypedDict instead of Dict. Useful if the different keys have different types Code Styling \u2691 New: Explain how to fix the Pylint R0201 error. New: Explain why we can safely ignore W1203. Software Architecture \u2691 Domain Driven Design \u2691 New: Explain how to inject fake dependencies into e2e tests with click.","title":"50th Week of 2020"},{"location":"newsletter/2020_w50/#coding","text":"","title":"Coding"},{"location":"newsletter/2020_w50/#python","text":"New: Do an initial analysis on Python profiling. New: Introduce some ideas on optimization of python code.","title":"Python"},{"location":"newsletter/2020_w50/#click","text":"New: Explain how to set the allowable values for an argument. New: Explain how to hide a command from the --help output.","title":"Click"},{"location":"newsletter/2020_w50/#type-hints","text":"New: Explain how to use the TypedDict instead of Dict. Useful if the different keys have different types","title":"Type Hints"},{"location":"newsletter/2020_w50/#code-styling","text":"New: Explain how to fix the Pylint R0201 error. New: Explain why we can safely ignore W1203.","title":"Code Styling"},{"location":"newsletter/2020_w50/#software-architecture","text":"","title":"Software Architecture"},{"location":"newsletter/2020_w50/#domain-driven-design","text":"New: Explain how to inject fake dependencies into e2e tests with click.","title":"Domain Driven Design"},{"location":"newsletter/2020_w51/","text":"DevOps \u2691 Monitoring \u2691 Prometheus \u2691 New: Explain how to install with docker. Reorganization: Move the installation of docker to prometheus install. Coding \u2691 Python \u2691 Pytest \u2691 Correction: Explain how to better use with pytest.raises snippets. To capture the message in a cleaner way Life Management \u2691 Strategy \u2691 New: Introduce strategy document. Differentiate between strategic planning and strategic thinking Other \u2691 New: Explain how to use Wake on Lan.","title":"51st Week of 2020"},{"location":"newsletter/2020_w51/#devops","text":"","title":"DevOps"},{"location":"newsletter/2020_w51/#monitoring","text":"","title":"Monitoring"},{"location":"newsletter/2020_w51/#prometheus","text":"New: Explain how to install with docker. Reorganization: Move the installation of docker to prometheus install.","title":"Prometheus"},{"location":"newsletter/2020_w51/#coding","text":"","title":"Coding"},{"location":"newsletter/2020_w51/#python","text":"","title":"Python"},{"location":"newsletter/2020_w51/#pytest","text":"Correction: Explain how to better use with pytest.raises snippets. To capture the message in a cleaner way","title":"Pytest"},{"location":"newsletter/2020_w51/#life-management","text":"","title":"Life Management"},{"location":"newsletter/2020_w51/#strategy","text":"New: Introduce strategy document. Differentiate between strategic planning and strategic thinking","title":"Strategy"},{"location":"newsletter/2020_w51/#other","text":"New: Explain how to use Wake on Lan.","title":"Other"},{"location":"newsletter/2020_w52/","text":"Coding \u2691 Python \u2691 FactoryBoy \u2691 Correction: Add warning that generating your own attributes doesn't work anymore. Faker \u2691 Correction: Improve the way of generating random seed. Pydantic \u2691 Correction: Correct pylint R0201 on pydantic models. SQLite \u2691 New: Introduce sqlite and it's upsert feature. Health \u2691 Sleep \u2691 New: Explain the independence between circadian and sleep pressure. Operative Systems \u2691 Linux \u2691 Vim \u2691 New: Explain how to manage python foldings. Other \u2691 New: Explain how to use TypeVar to specify children class. New: Explain how to solve W0106 in list comprehensions. New: Explain how to solve SIM105.","title":"52nd Week of 2020"},{"location":"newsletter/2020_w52/#coding","text":"","title":"Coding"},{"location":"newsletter/2020_w52/#python","text":"","title":"Python"},{"location":"newsletter/2020_w52/#factoryboy","text":"Correction: Add warning that generating your own attributes doesn't work anymore.","title":"FactoryBoy"},{"location":"newsletter/2020_w52/#faker","text":"Correction: Improve the way of generating random seed.","title":"Faker"},{"location":"newsletter/2020_w52/#pydantic","text":"Correction: Correct pylint R0201 on pydantic models.","title":"Pydantic"},{"location":"newsletter/2020_w52/#sqlite","text":"New: Introduce sqlite and it's upsert feature.","title":"SQLite"},{"location":"newsletter/2020_w52/#health","text":"","title":"Health"},{"location":"newsletter/2020_w52/#sleep","text":"New: Explain the independence between circadian and sleep pressure.","title":"Sleep"},{"location":"newsletter/2020_w52/#operative-systems","text":"","title":"Operative Systems"},{"location":"newsletter/2020_w52/#linux","text":"","title":"Linux"},{"location":"newsletter/2020_w52/#vim","text":"New: Explain how to manage python foldings.","title":"Vim"},{"location":"newsletter/2020_w52/#other","text":"New: Explain how to use TypeVar to specify children class. New: Explain how to solve W0106 in list comprehensions. New: Explain how to solve SIM105.","title":"Other"},{"location":"newsletter/2021_01/","text":"DevOps \u2691 Continuous Integration \u2691 Bandit \u2691 New: Explain how to ignore errors. Coding \u2691 Python \u2691 New: Explain how to check if a loop ends completely. New: Explain how to merge lists and dictionaries. New: Explain how to create your own exceptions. Libraries \u2691 New: Explain how to set cookies and headers in responses. DeepDiff \u2691 Correction: Remove murmur from the installation steps. It seems it's the default for the new versions FactoryBoy \u2691 New: Explain how to generate your own attributes. We earlier used lazy_attribute but if you want to use Faker inside the attribute definition, you're going to have a bad time. The new solution uses the creation of custom Fake providers. Faker \u2691 New: Explain how to create your own provider. Useful to generate custom objects for testing purposes. Python Snippets \u2691 Correction: Explain how to show the message in custom exceptions. New: Explain how to import a module or object from within a python program. pexpect \u2691 New: Introduce the pexpect python library. A pure Python module for spawning child applications; controlling them; and responding to expected patterns in their output. Pexpect works like Don Libes\u2019 Expect. Pexpect allows your script to spawn a child application and control it as if a human were typing commands. Prompt Toolkit \u2691 New: Introduce the tui python library. Useful to build text-based user interfaces, it allows the creation of intelligent prompts, dialogs, and full screen ncurses-like applications. Pydantic \u2691 New: Explain how to initialize attributes. Use validators to initialize attributes New: Name the pros and cons of using the library. New: Explain how to create bidirectional relationship between entities. New: Warn on the lack of TypeDict support. Pypika \u2691 New: Explain how to insert, update, select data. New: Explain how to join tables. questionary \u2691 New: Introduce tui python library. questionary is a Python library for effortlessly building pretty command line interfaces. It makes it very easy to query your user for input. Requests \u2691 New: Introduce the requests python library. Rq \u2691 New: Add note to test arq. arq is a similar library that can be better. SQL \u2691 New: Give examples on joins for each relationship type. SQLite \u2691 New: Solve the autoincrementation not working bug. Life Management \u2691 Automation \u2691 Accounting Automation \u2691 New: Explain my accounting automation workflow. Operative Systems \u2691 Linux \u2691 beancount \u2691 New: Introduce the cli double entry accounting program. New: Add links on how to use as a library. Correction: Correct the git repository link. Android \u2691 cone \u2691 New: Introduce the mobile double entry accounting application. Correction: Correct the description of the transaction to be beancount compatible. Other \u2691 New: Add remote work tips. New: Introduce lazy loading implementation paradigm with python. New: Explain how to lazy load pydantic objects.","title":"January of 2021"},{"location":"newsletter/2021_01/#devops","text":"","title":"DevOps"},{"location":"newsletter/2021_01/#continuous-integration","text":"","title":"Continuous Integration"},{"location":"newsletter/2021_01/#bandit","text":"New: Explain how to ignore errors.","title":"Bandit"},{"location":"newsletter/2021_01/#coding","text":"","title":"Coding"},{"location":"newsletter/2021_01/#python","text":"New: Explain how to check if a loop ends completely. New: Explain how to merge lists and dictionaries. New: Explain how to create your own exceptions.","title":"Python"},{"location":"newsletter/2021_01/#libraries","text":"New: Explain how to set cookies and headers in responses.","title":"Libraries"},{"location":"newsletter/2021_01/#deepdiff","text":"Correction: Remove murmur from the installation steps. It seems it's the default for the new versions","title":"DeepDiff"},{"location":"newsletter/2021_01/#factoryboy","text":"New: Explain how to generate your own attributes. We earlier used lazy_attribute but if you want to use Faker inside the attribute definition, you're going to have a bad time. The new solution uses the creation of custom Fake providers.","title":"FactoryBoy"},{"location":"newsletter/2021_01/#faker","text":"New: Explain how to create your own provider. Useful to generate custom objects for testing purposes.","title":"Faker"},{"location":"newsletter/2021_01/#python-snippets","text":"Correction: Explain how to show the message in custom exceptions. New: Explain how to import a module or object from within a python program.","title":"Python Snippets"},{"location":"newsletter/2021_01/#pexpect","text":"New: Introduce the pexpect python library. A pure Python module for spawning child applications; controlling them; and responding to expected patterns in their output. Pexpect works like Don Libes\u2019 Expect. Pexpect allows your script to spawn a child application and control it as if a human were typing commands.","title":"pexpect"},{"location":"newsletter/2021_01/#prompt-toolkit","text":"New: Introduce the tui python library. Useful to build text-based user interfaces, it allows the creation of intelligent prompts, dialogs, and full screen ncurses-like applications.","title":"Prompt Toolkit"},{"location":"newsletter/2021_01/#pydantic","text":"New: Explain how to initialize attributes. Use validators to initialize attributes New: Name the pros and cons of using the library. New: Explain how to create bidirectional relationship between entities. New: Warn on the lack of TypeDict support.","title":"Pydantic"},{"location":"newsletter/2021_01/#pypika","text":"New: Explain how to insert, update, select data. New: Explain how to join tables.","title":"Pypika"},{"location":"newsletter/2021_01/#questionary","text":"New: Introduce tui python library. questionary is a Python library for effortlessly building pretty command line interfaces. It makes it very easy to query your user for input.","title":"questionary"},{"location":"newsletter/2021_01/#requests","text":"New: Introduce the requests python library.","title":"Requests"},{"location":"newsletter/2021_01/#rq","text":"New: Add note to test arq. arq is a similar library that can be better.","title":"Rq"},{"location":"newsletter/2021_01/#sql","text":"New: Give examples on joins for each relationship type.","title":"SQL"},{"location":"newsletter/2021_01/#sqlite","text":"New: Solve the autoincrementation not working bug.","title":"SQLite"},{"location":"newsletter/2021_01/#life-management","text":"","title":"Life Management"},{"location":"newsletter/2021_01/#automation","text":"","title":"Automation"},{"location":"newsletter/2021_01/#accounting-automation","text":"New: Explain my accounting automation workflow.","title":"Accounting Automation"},{"location":"newsletter/2021_01/#operative-systems","text":"","title":"Operative Systems"},{"location":"newsletter/2021_01/#linux","text":"","title":"Linux"},{"location":"newsletter/2021_01/#beancount","text":"New: Introduce the cli double entry accounting program. New: Add links on how to use as a library. Correction: Correct the git repository link.","title":"beancount"},{"location":"newsletter/2021_01/#android","text":"","title":"Android"},{"location":"newsletter/2021_01/#cone","text":"New: Introduce the mobile double entry accounting application. Correction: Correct the description of the transaction to be beancount compatible.","title":"cone"},{"location":"newsletter/2021_01/#other","text":"New: Add remote work tips. New: Introduce lazy loading implementation paradigm with python. New: Explain how to lazy load pydantic objects.","title":"Other"},{"location":"newsletter/2021_01_05/","text":"DevOps \u2691 Continuous Integration \u2691 Bandit \u2691 New: Explain how to ignore errors. Coding \u2691 Python \u2691 Pydantic \u2691 New: Explain how to initialize attributes. Use validators to initialize attributes","title":"5th January 2021"},{"location":"newsletter/2021_01_05/#devops","text":"","title":"DevOps"},{"location":"newsletter/2021_01_05/#continuous-integration","text":"","title":"Continuous Integration"},{"location":"newsletter/2021_01_05/#bandit","text":"New: Explain how to ignore errors.","title":"Bandit"},{"location":"newsletter/2021_01_05/#coding","text":"","title":"Coding"},{"location":"newsletter/2021_01_05/#python","text":"","title":"Python"},{"location":"newsletter/2021_01_05/#pydantic","text":"New: Explain how to initialize attributes. Use validators to initialize attributes","title":"Pydantic"},{"location":"newsletter/2021_01_20/","text":"Other \u2691 New: Add remote work tips.","title":"20th January 2021"},{"location":"newsletter/2021_01_20/#other","text":"New: Add remote work tips.","title":"Other"},{"location":"newsletter/2021_01_21/","text":"Coding \u2691 Python \u2691 Pydantic \u2691 New: Name the pros and cons of using the library. New: Explain how to create bidirectional relationship between entities. Other \u2691 New: Introduce lazy loading implementation paradigm with python. New: Explain how to lazy load pydantic objects.","title":"21st January 2021"},{"location":"newsletter/2021_01_21/#coding","text":"","title":"Coding"},{"location":"newsletter/2021_01_21/#python","text":"","title":"Python"},{"location":"newsletter/2021_01_21/#pydantic","text":"New: Name the pros and cons of using the library. New: Explain how to create bidirectional relationship between entities.","title":"Pydantic"},{"location":"newsletter/2021_01_21/#other","text":"New: Introduce lazy loading implementation paradigm with python. New: Explain how to lazy load pydantic objects.","title":"Other"},{"location":"newsletter/2021_01_22/","text":"Coding \u2691 Python \u2691 DeepDiff \u2691 Correction: Remove murmur from the installation steps. It seems it's the default for the new versions Pypika \u2691 New: Explain how to insert, update, select data. New: Explain how to join tables. SQL \u2691 New: Give examples on joins for each relationship type. SQLite \u2691 New: Solve the autoincrementation not working bug.","title":"22nd January 2021"},{"location":"newsletter/2021_01_22/#coding","text":"","title":"Coding"},{"location":"newsletter/2021_01_22/#python","text":"","title":"Python"},{"location":"newsletter/2021_01_22/#deepdiff","text":"Correction: Remove murmur from the installation steps. It seems it's the default for the new versions","title":"DeepDiff"},{"location":"newsletter/2021_01_22/#pypika","text":"New: Explain how to insert, update, select data. New: Explain how to join tables.","title":"Pypika"},{"location":"newsletter/2021_01_22/#sql","text":"New: Give examples on joins for each relationship type.","title":"SQL"},{"location":"newsletter/2021_01_22/#sqlite","text":"New: Solve the autoincrementation not working bug.","title":"SQLite"},{"location":"newsletter/2021_01_26/","text":"Life Management \u2691 Automation \u2691 Accounting Automation \u2691 New: Explain my accounting automation workflow. Operative Systems \u2691 Linux \u2691 beancount \u2691 New: Introduce the cli double entry accounting program. Android \u2691 cone \u2691 New: Introduce the mobile double entry accounting application.","title":"26th January 2021"},{"location":"newsletter/2021_01_26/#life-management","text":"","title":"Life Management"},{"location":"newsletter/2021_01_26/#automation","text":"","title":"Automation"},{"location":"newsletter/2021_01_26/#accounting-automation","text":"New: Explain my accounting automation workflow.","title":"Accounting Automation"},{"location":"newsletter/2021_01_26/#operative-systems","text":"","title":"Operative Systems"},{"location":"newsletter/2021_01_26/#linux","text":"","title":"Linux"},{"location":"newsletter/2021_01_26/#beancount","text":"New: Introduce the cli double entry accounting program.","title":"beancount"},{"location":"newsletter/2021_01_26/#android","text":"","title":"Android"},{"location":"newsletter/2021_01_26/#cone","text":"New: Introduce the mobile double entry accounting application.","title":"cone"},{"location":"newsletter/2021_01_29/","text":"Coding \u2691 Python \u2691 New: Explain how to check if a loop ends completely. New: Explain how to merge lists and dictionaries. New: Explain how to create your own exceptions. Libraries \u2691 New: Explain how to set cookies and headers in responses. FactoryBoy \u2691 New: Explain how to generate your own attributes. We earlier used lazy_attribute but if you want to use Faker inside the attribute definition, you're going to have a bad time. The new solution uses the creation of custom Fake providers. Faker \u2691 New: Explain how to create your own provider. Useful to generate custom objects for testing purposes. Pydantic \u2691 New: Warn on the lack of TypeDict support. Requests \u2691 New: Introduce the requests python library. Rq \u2691 New: Add note to test arq. arq is a similar library that can be better. Operative Systems \u2691 Linux \u2691 beancount \u2691 New: Add links on how to use as a library. Correction: Correct the git repository link. Android \u2691 cone \u2691 Correction: Correct the description of the transaction to be beancount compatible.","title":"29th January 2021"},{"location":"newsletter/2021_01_29/#coding","text":"","title":"Coding"},{"location":"newsletter/2021_01_29/#python","text":"New: Explain how to check if a loop ends completely. New: Explain how to merge lists and dictionaries. New: Explain how to create your own exceptions.","title":"Python"},{"location":"newsletter/2021_01_29/#libraries","text":"New: Explain how to set cookies and headers in responses.","title":"Libraries"},{"location":"newsletter/2021_01_29/#factoryboy","text":"New: Explain how to generate your own attributes. We earlier used lazy_attribute but if you want to use Faker inside the attribute definition, you're going to have a bad time. The new solution uses the creation of custom Fake providers.","title":"FactoryBoy"},{"location":"newsletter/2021_01_29/#faker","text":"New: Explain how to create your own provider. Useful to generate custom objects for testing purposes.","title":"Faker"},{"location":"newsletter/2021_01_29/#pydantic","text":"New: Warn on the lack of TypeDict support.","title":"Pydantic"},{"location":"newsletter/2021_01_29/#requests","text":"New: Introduce the requests python library.","title":"Requests"},{"location":"newsletter/2021_01_29/#rq","text":"New: Add note to test arq. arq is a similar library that can be better.","title":"Rq"},{"location":"newsletter/2021_01_29/#operative-systems","text":"","title":"Operative Systems"},{"location":"newsletter/2021_01_29/#linux","text":"","title":"Linux"},{"location":"newsletter/2021_01_29/#beancount","text":"New: Add links on how to use as a library. Correction: Correct the git repository link.","title":"beancount"},{"location":"newsletter/2021_01_29/#android","text":"","title":"Android"},{"location":"newsletter/2021_01_29/#cone","text":"Correction: Correct the description of the transaction to be beancount compatible.","title":"cone"},{"location":"newsletter/2021_01_30/","text":"Coding \u2691 Python \u2691 Python Snippets \u2691 Correction: Explain how to show the message in custom exceptions. New: Explain how to import a module or object from within a python program. pexpect \u2691 New: Introduce the pexpect python library. A pure Python module for spawning child applications; controlling them; and responding to expected patterns in their output. Pexpect works like Don Libes\u2019 Expect. Pexpect allows your script to spawn a child application and control it as if a human were typing commands. Prompt Toolkit \u2691 New: Introduce the tui python library. Useful to build text-based user interfaces, it allows the creation of intelligent prompts, dialogs, and full screen ncurses-like applications. questionary \u2691 New: Introduce tui python library. questionary is a Python library for effortlessly building pretty command line interfaces. It makes it very easy to query your user for input.","title":"30th January 2021"},{"location":"newsletter/2021_01_30/#coding","text":"","title":"Coding"},{"location":"newsletter/2021_01_30/#python","text":"","title":"Python"},{"location":"newsletter/2021_01_30/#python-snippets","text":"Correction: Explain how to show the message in custom exceptions. New: Explain how to import a module or object from within a python program.","title":"Python Snippets"},{"location":"newsletter/2021_01_30/#pexpect","text":"New: Introduce the pexpect python library. A pure Python module for spawning child applications; controlling them; and responding to expected patterns in their output. Pexpect works like Don Libes\u2019 Expect. Pexpect allows your script to spawn a child application and control it as if a human were typing commands.","title":"pexpect"},{"location":"newsletter/2021_01_30/#prompt-toolkit","text":"New: Introduce the tui python library. Useful to build text-based user interfaces, it allows the creation of intelligent prompts, dialogs, and full screen ncurses-like applications.","title":"Prompt Toolkit"},{"location":"newsletter/2021_01_30/#questionary","text":"New: Introduce tui python library. questionary is a Python library for effortlessly building pretty command line interfaces. It makes it very easy to query your user for input.","title":"questionary"},{"location":"newsletter/2021_02/","text":"Introduction \u2691 New: Simplify the landing page text. Meta \u2691 Wish list \u2691 New: Feature mkdocs-rss-plugin as a solution of publishing mkdocs updates as an RSS. New: Add a git issue tracker and markdown formatter. DevOps \u2691 New: Comment on the DevOps pitfalls and update the learn path. Monitoring \u2691 Monitoring Comparison \u2691 New: Compare Nagios and Prometheus as monitoring. Correction: Improve the comparison. State that nagios is not easy to configure. If you're used to it it is, otherwise it's not. Add that grafana has a huge community building graphs. Mention Thanos as the long term storage solution for Prometheus. Correction: Add the insights of a nagios power user. Update open source and community analysis with nagios exchange. Correct nagios community analysis with its trajectory Correct the analysis of the high availability of nagios Add the option to host the script exporter in a dedicated server Coding \u2691 Python \u2691 GitPython \u2691 New: Introduce the python library. GitPython is a python library used to interact with git repositories, high-level like git-porcelain, or low-level like git-plumbing. It provides abstractions of git objects for easy access of repository data, and additionally allows you to access the git repository more directly using either a pure python implementation, or the faster, but more resource intensive git command implementation. Explain how to: Initialize or load repositories. Make commits. Interact with the history. Test applications that use it. Improvement: Explain how to get the working directory of a repo. Using the working_dir attribute. Jinja2 \u2691 New: Explain how to use Jinja2. Jinja2 is a modern and designer- friendly templating language for Python, modelled after Django\u2019s templates. It is fast, widely used and secure with the optional sandboxed template execution environment. Add installation, usage and basic and advanced template guidelines. Python Snippets \u2691 New: Add today's learned python tricks. Get system's timezone and use it in datetime . Capitalize a sentence . Get the last monday datetime . Explain how to group a list of dictionaries by a specific key. With itertools.groupby . Issues \u2691 New: Introduce the issue tracking document. I haven't found a tool to monitor the context it made me track certain software issues, so I get lost when updates come. Until a tool shows up, I'll use the good old markdown to keep track of them. New: Add today's issues. Gadgetbridge improvements Ombi improvements Improvement: Monitor today's issues. Mkdocs migration to 7.x is giving errors with the search bar and repo stats. Software Architecture \u2691 Architecture Decision Record \u2691 New: Introduce the Architecture Decision Records. ADR are short text documents that captures an important architectural decision made along with its context and consequences. New: Update the ADR template with the week learnings. Add the Proposals and Date sections Explain the possible Status states. Add an Ultisnip vim snippet. Explain how I've used it to create mkdocs- newsletter . Life Management \u2691 Automation \u2691 Amazfit Band 5 \u2691 New: Add insights on sleep detection. The sleep tracking using Gadgetbridge is not good at all . After two nights, the band has not been able to detect when I woke in the middle of the night, or when I really woke up, as I usually stay in the bed for a time before standing up. I'll try with the proprietary application soon and compare results. New: Explain how to upgrade the firmware. Gadgetbridge people have a guide on how to upgrade the firmware , you need to get the firmware from the geek doing forum though, so it is interesting to create an account and watch the post. Improvement: Add insights on sleep tracking. You can't use the Withings sleep analyzer without their app (as expected), maybe the Emfit QS is the way to go. Fitness Tracker \u2691 New: Introduce the fitness band in your life automation. Fitness tracker or activity trackers are devices or applications for monitoring and tracking fitness- related metrics such as distance walked or run, calorie consumption, and in some cases heartbeat. It is a type of wearable computer. Explain also why it's interesting Virtual Assistant \u2691 New: Introduce project with kalliope. New: Explain the Speech-To-Text open source solutions. Health \u2691 Teeth \u2691 New: Explain how to take care of your teeth. A full guide on why should you take care of your teeth, the description on how the basic oral diseases work, why and how to brush your teeth, floss and usage of mouthwash Correction: Recommend a regular clean instead of a deep clean. Deep cleaning \u2691 New: Explain what a deep cleaning is and when should you do it. Analyze the reasons why would you need to do this procedure, how it works, when you need to do it, side effects and scientific evidences of it's effectiveness. Activism \u2691 New: Introduce the anonymous feedback tool to improve diversity, equity and inclusion in an organization. Anonymous Feedback is a communication tool where people share feedback to teammates or other organizational members while protecting their identities. Until the safe space is built where direct feedback is viable, anonymous feedback gives these employees a mechanism to raise their concerns, practice their feedback-giving skills, test the waters, and understand how people perceive their constructive (and sometimes critical) opinions, thus building the needed trust. Operative Systems \u2691 Linux \u2691 mkdocs \u2691 New: Explain how to develop your own plugins. Vim \u2691 New: Configure Vim to set the upstream by default when git pushing. Android \u2691 GadgetBridge \u2691 New: Add more guidelines to reverse engineer the band protocol. Arts \u2691 Writing \u2691 New: Try vim-pencil without success, but love mdnav. mdnav opens links to urls or files when pressing enter in normal mode over a markdown link, similar to gx but more powerful. I specially like the ability of following [self referencing link][] links, that allows storing the links at the bottom. Build your own Digital Garden \u2691 New: Explain how to enable clickable navigation sections in your mkdocs repository. oprypin has solved it with the mkdocs-section-index plugin. Digital Gardens \u2691 New: Introduce the digital garden concept. Digital Garden is a method of storing and maintaining knowledge in an maintainable, scalable and searchable way. They are also known as second brains. Pilates \u2691 New: Introduce the art. Pilates is a physical fitness system based on controlled movements putting emphasis on alignment, breathing, developing a strong core, and improving coordination and balance. The core (or powerhouse), consisting of the muscles of the abdomen, low back, and hips, is thought to be the key to a person's stability. Pilates' system allows for different exercises to be modified in range of difficulty from beginner to advanced or to any other level, and also in terms of the instructor and practitioner's specific goals and/or limitations. Intensity can be increased over time as the body adapts itself to the exercises. You can think of yoga, but without the spiritual aspects. Also added: It's principles The swing from table exercise. Other \u2691 Correction: Deprecate mkdocs issues. They've been fixed in the last release","title":"February of 2021"},{"location":"newsletter/2021_02/#introduction","text":"New: Simplify the landing page text.","title":"Introduction"},{"location":"newsletter/2021_02/#meta","text":"","title":"Meta"},{"location":"newsletter/2021_02/#wish-list","text":"New: Feature mkdocs-rss-plugin as a solution of publishing mkdocs updates as an RSS. New: Add a git issue tracker and markdown formatter.","title":"Wish list"},{"location":"newsletter/2021_02/#devops","text":"New: Comment on the DevOps pitfalls and update the learn path.","title":"DevOps"},{"location":"newsletter/2021_02/#monitoring","text":"","title":"Monitoring"},{"location":"newsletter/2021_02/#monitoring-comparison","text":"New: Compare Nagios and Prometheus as monitoring. Correction: Improve the comparison. State that nagios is not easy to configure. If you're used to it it is, otherwise it's not. Add that grafana has a huge community building graphs. Mention Thanos as the long term storage solution for Prometheus. Correction: Add the insights of a nagios power user. Update open source and community analysis with nagios exchange. Correct nagios community analysis with its trajectory Correct the analysis of the high availability of nagios Add the option to host the script exporter in a dedicated server","title":"Monitoring Comparison"},{"location":"newsletter/2021_02/#coding","text":"","title":"Coding"},{"location":"newsletter/2021_02/#python","text":"","title":"Python"},{"location":"newsletter/2021_02/#gitpython","text":"New: Introduce the python library. GitPython is a python library used to interact with git repositories, high-level like git-porcelain, or low-level like git-plumbing. It provides abstractions of git objects for easy access of repository data, and additionally allows you to access the git repository more directly using either a pure python implementation, or the faster, but more resource intensive git command implementation. Explain how to: Initialize or load repositories. Make commits. Interact with the history. Test applications that use it. Improvement: Explain how to get the working directory of a repo. Using the working_dir attribute.","title":"GitPython"},{"location":"newsletter/2021_02/#jinja2","text":"New: Explain how to use Jinja2. Jinja2 is a modern and designer- friendly templating language for Python, modelled after Django\u2019s templates. It is fast, widely used and secure with the optional sandboxed template execution environment. Add installation, usage and basic and advanced template guidelines.","title":"Jinja2"},{"location":"newsletter/2021_02/#python-snippets","text":"New: Add today's learned python tricks. Get system's timezone and use it in datetime . Capitalize a sentence . Get the last monday datetime . Explain how to group a list of dictionaries by a specific key. With itertools.groupby .","title":"Python Snippets"},{"location":"newsletter/2021_02/#issues","text":"New: Introduce the issue tracking document. I haven't found a tool to monitor the context it made me track certain software issues, so I get lost when updates come. Until a tool shows up, I'll use the good old markdown to keep track of them. New: Add today's issues. Gadgetbridge improvements Ombi improvements Improvement: Monitor today's issues. Mkdocs migration to 7.x is giving errors with the search bar and repo stats.","title":"Issues"},{"location":"newsletter/2021_02/#software-architecture","text":"","title":"Software Architecture"},{"location":"newsletter/2021_02/#architecture-decision-record","text":"New: Introduce the Architecture Decision Records. ADR are short text documents that captures an important architectural decision made along with its context and consequences. New: Update the ADR template with the week learnings. Add the Proposals and Date sections Explain the possible Status states. Add an Ultisnip vim snippet. Explain how I've used it to create mkdocs- newsletter .","title":"Architecture Decision Record"},{"location":"newsletter/2021_02/#life-management","text":"","title":"Life Management"},{"location":"newsletter/2021_02/#automation","text":"","title":"Automation"},{"location":"newsletter/2021_02/#amazfit-band-5","text":"New: Add insights on sleep detection. The sleep tracking using Gadgetbridge is not good at all . After two nights, the band has not been able to detect when I woke in the middle of the night, or when I really woke up, as I usually stay in the bed for a time before standing up. I'll try with the proprietary application soon and compare results. New: Explain how to upgrade the firmware. Gadgetbridge people have a guide on how to upgrade the firmware , you need to get the firmware from the geek doing forum though, so it is interesting to create an account and watch the post. Improvement: Add insights on sleep tracking. You can't use the Withings sleep analyzer without their app (as expected), maybe the Emfit QS is the way to go.","title":"Amazfit Band 5"},{"location":"newsletter/2021_02/#fitness-tracker","text":"New: Introduce the fitness band in your life automation. Fitness tracker or activity trackers are devices or applications for monitoring and tracking fitness- related metrics such as distance walked or run, calorie consumption, and in some cases heartbeat. It is a type of wearable computer. Explain also why it's interesting","title":"Fitness Tracker"},{"location":"newsletter/2021_02/#virtual-assistant","text":"New: Introduce project with kalliope. New: Explain the Speech-To-Text open source solutions.","title":"Virtual Assistant"},{"location":"newsletter/2021_02/#health","text":"","title":"Health"},{"location":"newsletter/2021_02/#teeth","text":"New: Explain how to take care of your teeth. A full guide on why should you take care of your teeth, the description on how the basic oral diseases work, why and how to brush your teeth, floss and usage of mouthwash Correction: Recommend a regular clean instead of a deep clean.","title":"Teeth"},{"location":"newsletter/2021_02/#deep-cleaning","text":"New: Explain what a deep cleaning is and when should you do it. Analyze the reasons why would you need to do this procedure, how it works, when you need to do it, side effects and scientific evidences of it's effectiveness.","title":"Deep cleaning"},{"location":"newsletter/2021_02/#activism","text":"New: Introduce the anonymous feedback tool to improve diversity, equity and inclusion in an organization. Anonymous Feedback is a communication tool where people share feedback to teammates or other organizational members while protecting their identities. Until the safe space is built where direct feedback is viable, anonymous feedback gives these employees a mechanism to raise their concerns, practice their feedback-giving skills, test the waters, and understand how people perceive their constructive (and sometimes critical) opinions, thus building the needed trust.","title":"Activism"},{"location":"newsletter/2021_02/#operative-systems","text":"","title":"Operative Systems"},{"location":"newsletter/2021_02/#linux","text":"","title":"Linux"},{"location":"newsletter/2021_02/#mkdocs","text":"New: Explain how to develop your own plugins.","title":"mkdocs"},{"location":"newsletter/2021_02/#vim","text":"New: Configure Vim to set the upstream by default when git pushing.","title":"Vim"},{"location":"newsletter/2021_02/#android","text":"","title":"Android"},{"location":"newsletter/2021_02/#gadgetbridge","text":"New: Add more guidelines to reverse engineer the band protocol.","title":"GadgetBridge"},{"location":"newsletter/2021_02/#arts","text":"","title":"Arts"},{"location":"newsletter/2021_02/#writing","text":"New: Try vim-pencil without success, but love mdnav. mdnav opens links to urls or files when pressing enter in normal mode over a markdown link, similar to gx but more powerful. I specially like the ability of following [self referencing link][] links, that allows storing the links at the bottom.","title":"Writing"},{"location":"newsletter/2021_02/#build-your-own-digital-garden","text":"New: Explain how to enable clickable navigation sections in your mkdocs repository. oprypin has solved it with the mkdocs-section-index plugin.","title":"Build your own Digital Garden"},{"location":"newsletter/2021_02/#digital-gardens","text":"New: Introduce the digital garden concept. Digital Garden is a method of storing and maintaining knowledge in an maintainable, scalable and searchable way. They are also known as second brains.","title":"Digital Gardens"},{"location":"newsletter/2021_02/#pilates","text":"New: Introduce the art. Pilates is a physical fitness system based on controlled movements putting emphasis on alignment, breathing, developing a strong core, and improving coordination and balance. The core (or powerhouse), consisting of the muscles of the abdomen, low back, and hips, is thought to be the key to a person's stability. Pilates' system allows for different exercises to be modified in range of difficulty from beginner to advanced or to any other level, and also in terms of the instructor and practitioner's specific goals and/or limitations. Intensity can be increased over time as the body adapts itself to the exercises. You can think of yoga, but without the spiritual aspects. Also added: It's principles The swing from table exercise.","title":"Pilates"},{"location":"newsletter/2021_02/#other","text":"Correction: Deprecate mkdocs issues. They've been fixed in the last release","title":"Other"},{"location":"newsletter/2021_02_02/","text":"Health \u2691 Teeth \u2691 New: Explain how to take care of your teeth. A full guide on why should you take care of your teeth, the description on how the basic oral diseases work, why and how to brush your teeth, floss and usage of mouthwash Deep cleaning \u2691 New: Explain what a deep cleaning is and when should you do it. Analyze the reasons why would you need to do this procedure, how it works, when you need to do it, side effects and scientific evidences of it's effectiveness.","title":"2nd February 2021"},{"location":"newsletter/2021_02_02/#health","text":"","title":"Health"},{"location":"newsletter/2021_02_02/#teeth","text":"New: Explain how to take care of your teeth. A full guide on why should you take care of your teeth, the description on how the basic oral diseases work, why and how to brush your teeth, floss and usage of mouthwash","title":"Teeth"},{"location":"newsletter/2021_02_02/#deep-cleaning","text":"New: Explain what a deep cleaning is and when should you do it. Analyze the reasons why would you need to do this procedure, how it works, when you need to do it, side effects and scientific evidences of it's effectiveness.","title":"Deep cleaning"},{"location":"newsletter/2021_02_04/","text":"Software Architecture \u2691 Architecture Decision Record \u2691 New: Introduce the Architecture Decision Records. ADR are short text documents that captures an important architectural decision made along with its context and consequences. Life Management \u2691 Automation \u2691 Virtual Assistant \u2691 New: Introduce project with kalliope.","title":"4th February 2021"},{"location":"newsletter/2021_02_04/#software-architecture","text":"","title":"Software Architecture"},{"location":"newsletter/2021_02_04/#architecture-decision-record","text":"New: Introduce the Architecture Decision Records. ADR are short text documents that captures an important architectural decision made along with its context and consequences.","title":"Architecture Decision Record"},{"location":"newsletter/2021_02_04/#life-management","text":"","title":"Life Management"},{"location":"newsletter/2021_02_04/#automation","text":"","title":"Automation"},{"location":"newsletter/2021_02_04/#virtual-assistant","text":"New: Introduce project with kalliope.","title":"Virtual Assistant"},{"location":"newsletter/2021_02_05/","text":"DevOps \u2691 New: Comment on the DevOps pitfalls and update the learn path. Life Management \u2691 Automation \u2691 Virtual Assistant \u2691 New: Explain the Speech-To-Text open source solutions. Operative Systems \u2691 Linux \u2691 Vim \u2691 New: Configure Vim to set the upstream by default when git pushing.","title":"5th February 2021"},{"location":"newsletter/2021_02_05/#devops","text":"New: Comment on the DevOps pitfalls and update the learn path.","title":"DevOps"},{"location":"newsletter/2021_02_05/#life-management","text":"","title":"Life Management"},{"location":"newsletter/2021_02_05/#automation","text":"","title":"Automation"},{"location":"newsletter/2021_02_05/#virtual-assistant","text":"New: Explain the Speech-To-Text open source solutions.","title":"Virtual Assistant"},{"location":"newsletter/2021_02_05/#operative-systems","text":"","title":"Operative Systems"},{"location":"newsletter/2021_02_05/#linux","text":"","title":"Linux"},{"location":"newsletter/2021_02_05/#vim","text":"New: Configure Vim to set the upstream by default when git pushing.","title":"Vim"},{"location":"newsletter/2021_02_08/","text":"Meta \u2691 Wish list \u2691 New: Feature mkdocs-rss-plugin as a solution of publishing mkdocs updates as an RSS. Activism \u2691 New: Introduce the anonymous feedback tool to improve diversity, equity and inclusion in an organization. Anonymous Feedback is a communication tool where people share feedback to teammates or other organizational members while protecting their identities. Until the safe space is built where direct feedback is viable, anonymous feedback gives these employees a mechanism to raise their concerns, practice their feedback-giving skills, test the waters, and understand how people perceive their constructive (and sometimes critical) opinions, thus building the needed trust. Arts \u2691 Writing \u2691 Build your own Digital Garden \u2691 New: Explain how to enable clickable navigation sections in your mkdocs repository. oprypin has solved it with the mkdocs-section-index plugin.","title":"8th February 2021"},{"location":"newsletter/2021_02_08/#meta","text":"","title":"Meta"},{"location":"newsletter/2021_02_08/#wish-list","text":"New: Feature mkdocs-rss-plugin as a solution of publishing mkdocs updates as an RSS.","title":"Wish list"},{"location":"newsletter/2021_02_08/#activism","text":"New: Introduce the anonymous feedback tool to improve diversity, equity and inclusion in an organization. Anonymous Feedback is a communication tool where people share feedback to teammates or other organizational members while protecting their identities. Until the safe space is built where direct feedback is viable, anonymous feedback gives these employees a mechanism to raise their concerns, practice their feedback-giving skills, test the waters, and understand how people perceive their constructive (and sometimes critical) opinions, thus building the needed trust.","title":"Activism"},{"location":"newsletter/2021_02_08/#arts","text":"","title":"Arts"},{"location":"newsletter/2021_02_08/#writing","text":"","title":"Writing"},{"location":"newsletter/2021_02_08/#build-your-own-digital-garden","text":"New: Explain how to enable clickable navigation sections in your mkdocs repository. oprypin has solved it with the mkdocs-section-index plugin.","title":"Build your own Digital Garden"},{"location":"newsletter/2021_02_09/","text":"Introduction \u2691 New: Simplify the landing page text. Meta \u2691 Wish list \u2691 New: Add a git issue tracker and markdown formatter. Arts \u2691 Writing \u2691 New: Try vim-pencil without success, but love mdnav. mdnav opens links to urls or files when pressing enter in normal mode over a markdown link, similar to gx but more powerful. I specially like the ability of following [self referencing link][] links, that allows storing the links at the bottom. Digital Gardens \u2691 New: Introduce the digital garden concept. Digital Garden is a method of storing and maintaining knowledge in an maintainable, scalable and searchable way. They are also known as second brains.","title":"9th February 2021"},{"location":"newsletter/2021_02_09/#introduction","text":"New: Simplify the landing page text.","title":"Introduction"},{"location":"newsletter/2021_02_09/#meta","text":"","title":"Meta"},{"location":"newsletter/2021_02_09/#wish-list","text":"New: Add a git issue tracker and markdown formatter.","title":"Wish list"},{"location":"newsletter/2021_02_09/#arts","text":"","title":"Arts"},{"location":"newsletter/2021_02_09/#writing","text":"New: Try vim-pencil without success, but love mdnav. mdnav opens links to urls or files when pressing enter in normal mode over a markdown link, similar to gx but more powerful. I specially like the ability of following [self referencing link][] links, that allows storing the links at the bottom.","title":"Writing"},{"location":"newsletter/2021_02_09/#digital-gardens","text":"New: Introduce the digital garden concept. Digital Garden is a method of storing and maintaining knowledge in an maintainable, scalable and searchable way. They are also known as second brains.","title":"Digital Gardens"},{"location":"newsletter/2021_02_10/","text":"Coding \u2691 Issues \u2691 New: Introduce the issue tracking document. I haven't found a tool to monitor the context it made me track certain software issues, so I get lost when updates come. Until a tool shows up, I'll use the good old markdown to keep track of them. Health \u2691 Teeth \u2691 Correction: Recommend a regular clean instead of a deep clean. Operative Systems \u2691 Linux \u2691 mkdocs \u2691 New: Explain how to develop your own plugins.","title":"10th February 2021"},{"location":"newsletter/2021_02_10/#coding","text":"","title":"Coding"},{"location":"newsletter/2021_02_10/#issues","text":"New: Introduce the issue tracking document. I haven't found a tool to monitor the context it made me track certain software issues, so I get lost when updates come. Until a tool shows up, I'll use the good old markdown to keep track of them.","title":"Issues"},{"location":"newsletter/2021_02_10/#health","text":"","title":"Health"},{"location":"newsletter/2021_02_10/#teeth","text":"Correction: Recommend a regular clean instead of a deep clean.","title":"Teeth"},{"location":"newsletter/2021_02_10/#operative-systems","text":"","title":"Operative Systems"},{"location":"newsletter/2021_02_10/#linux","text":"","title":"Linux"},{"location":"newsletter/2021_02_10/#mkdocs","text":"New: Explain how to develop your own plugins.","title":"mkdocs"},{"location":"newsletter/2021_02_12/","text":"Coding \u2691 Python \u2691 GitPython \u2691 New: Introduce the python library. GitPython is a python library used to interact with git repositories, high-level like git-porcelain, or low-level like git-plumbing. It provides abstractions of git objects for easy access of repository data, and additionally allows you to access the git repository more directly using either a pure python implementation, or the faster, but more resource intensive git command implementation. Explain how to: Initialize or load repositories. Make commits. Interact with the history. Test applications that use it. Python Snippets \u2691 New: Add today's learned python tricks. Get system's timezone and use it in datetime . Capitalize a sentence . Get the last monday datetime . Software Architecture \u2691 Architecture Decision Record \u2691 New: Update the ADR template with the week learnings. Add the Proposals and Date sections Explain the possible Status states. Add an Ultisnip vim snippet. Explain how I've used it to create mkdocs- newsletter . Arts \u2691 Pilates \u2691 New: Introduce the art. Pilates is a physical fitness system based on controlled movements putting emphasis on alignment, breathing, developing a strong core, and improving coordination and balance. The core (or powerhouse), consisting of the muscles of the abdomen, low back, and hips, is thought to be the key to a person's stability. Pilates' system allows for different exercises to be modified in range of difficulty from beginner to advanced or to any other level, and also in terms of the instructor and practitioner's specific goals and/or limitations. Intensity can be increased over time as the body adapts itself to the exercises. You can think of yoga, but without the spiritual aspects. Also added: It's principles The swing from table exercise.","title":"12th February 2021"},{"location":"newsletter/2021_02_12/#coding","text":"","title":"Coding"},{"location":"newsletter/2021_02_12/#python","text":"","title":"Python"},{"location":"newsletter/2021_02_12/#gitpython","text":"New: Introduce the python library. GitPython is a python library used to interact with git repositories, high-level like git-porcelain, or low-level like git-plumbing. It provides abstractions of git objects for easy access of repository data, and additionally allows you to access the git repository more directly using either a pure python implementation, or the faster, but more resource intensive git command implementation. Explain how to: Initialize or load repositories. Make commits. Interact with the history. Test applications that use it.","title":"GitPython"},{"location":"newsletter/2021_02_12/#python-snippets","text":"New: Add today's learned python tricks. Get system's timezone and use it in datetime . Capitalize a sentence . Get the last monday datetime .","title":"Python Snippets"},{"location":"newsletter/2021_02_12/#software-architecture","text":"","title":"Software Architecture"},{"location":"newsletter/2021_02_12/#architecture-decision-record","text":"New: Update the ADR template with the week learnings. Add the Proposals and Date sections Explain the possible Status states. Add an Ultisnip vim snippet. Explain how I've used it to create mkdocs- newsletter .","title":"Architecture Decision Record"},{"location":"newsletter/2021_02_12/#arts","text":"","title":"Arts"},{"location":"newsletter/2021_02_12/#pilates","text":"New: Introduce the art. Pilates is a physical fitness system based on controlled movements putting emphasis on alignment, breathing, developing a strong core, and improving coordination and balance. The core (or powerhouse), consisting of the muscles of the abdomen, low back, and hips, is thought to be the key to a person's stability. Pilates' system allows for different exercises to be modified in range of difficulty from beginner to advanced or to any other level, and also in terms of the instructor and practitioner's specific goals and/or limitations. Intensity can be increased over time as the body adapts itself to the exercises. You can think of yoga, but without the spiritual aspects. Also added: It's principles The swing from table exercise.","title":"Pilates"},{"location":"newsletter/2021_02_22/","text":"DevOps \u2691 Monitoring \u2691 Monitoring Comparison \u2691 New: Compare Nagios and Prometheus as monitoring.","title":"22nd February 2021"},{"location":"newsletter/2021_02_22/#devops","text":"","title":"DevOps"},{"location":"newsletter/2021_02_22/#monitoring","text":"","title":"Monitoring"},{"location":"newsletter/2021_02_22/#monitoring-comparison","text":"New: Compare Nagios and Prometheus as monitoring.","title":"Monitoring Comparison"},{"location":"newsletter/2021_02_23/","text":"DevOps \u2691 Monitoring \u2691 Monitoring Comparison \u2691 Correction: Improve the comparison. State that nagios is not easy to configure. If you're used to it it is, otherwise it's not. Add that grafana has a huge community building graphs. Mention Thanos as the long term storage solution for Prometheus. Life Management \u2691 Automation \u2691 Fitness Tracker \u2691 New: Introduce the fitness band in your life automation. Fitness tracker or activity trackers are devices or applications for monitoring and tracking fitness- related metrics such as distance walked or run, calorie consumption, and in some cases heartbeat. It is a type of wearable computer. Explain also why it's interesting","title":"23rd February 2021"},{"location":"newsletter/2021_02_23/#devops","text":"","title":"DevOps"},{"location":"newsletter/2021_02_23/#monitoring","text":"","title":"Monitoring"},{"location":"newsletter/2021_02_23/#monitoring-comparison","text":"Correction: Improve the comparison. State that nagios is not easy to configure. If you're used to it it is, otherwise it's not. Add that grafana has a huge community building graphs. Mention Thanos as the long term storage solution for Prometheus.","title":"Monitoring Comparison"},{"location":"newsletter/2021_02_23/#life-management","text":"","title":"Life Management"},{"location":"newsletter/2021_02_23/#automation","text":"","title":"Automation"},{"location":"newsletter/2021_02_23/#fitness-tracker","text":"New: Introduce the fitness band in your life automation. Fitness tracker or activity trackers are devices or applications for monitoring and tracking fitness- related metrics such as distance walked or run, calorie consumption, and in some cases heartbeat. It is a type of wearable computer. Explain also why it's interesting","title":"Fitness Tracker"},{"location":"newsletter/2021_02_25/","text":"DevOps \u2691 Monitoring \u2691 Monitoring Comparison \u2691 Correction: Add the insights of a nagios power user. Update open source and community analysis with nagios exchange. Correct nagios community analysis with its trajectory Correct the analysis of the high availability of nagios Add the option to host the script exporter in a dedicated server Coding \u2691 Issues \u2691 New: Add today's issues. Gadgetbridge improvements Ombi improvements Life Management \u2691 Automation \u2691 Amazfit Band 5 \u2691 New: Add insights on sleep detection. The sleep tracking using Gadgetbridge is not good at all . After two nights, the band has not been able to detect when I woke in the middle of the night, or when I really woke up, as I usually stay in the bed for a time before standing up. I'll try with the proprietary application soon and compare results. New: Explain how to upgrade the firmware. Gadgetbridge people have a guide on how to upgrade the firmware , you need to get the firmware from the geek doing forum though, so it is interesting to create an account and watch the post. Operative Systems \u2691 Android \u2691 GadgetBridge \u2691 New: Add more guidelines to reverse engineer the band protocol.","title":"25th February 2021"},{"location":"newsletter/2021_02_25/#devops","text":"","title":"DevOps"},{"location":"newsletter/2021_02_25/#monitoring","text":"","title":"Monitoring"},{"location":"newsletter/2021_02_25/#monitoring-comparison","text":"Correction: Add the insights of a nagios power user. Update open source and community analysis with nagios exchange. Correct nagios community analysis with its trajectory Correct the analysis of the high availability of nagios Add the option to host the script exporter in a dedicated server","title":"Monitoring Comparison"},{"location":"newsletter/2021_02_25/#coding","text":"","title":"Coding"},{"location":"newsletter/2021_02_25/#issues","text":"New: Add today's issues. Gadgetbridge improvements Ombi improvements","title":"Issues"},{"location":"newsletter/2021_02_25/#life-management","text":"","title":"Life Management"},{"location":"newsletter/2021_02_25/#automation","text":"","title":"Automation"},{"location":"newsletter/2021_02_25/#amazfit-band-5","text":"New: Add insights on sleep detection. The sleep tracking using Gadgetbridge is not good at all . After two nights, the band has not been able to detect when I woke in the middle of the night, or when I really woke up, as I usually stay in the bed for a time before standing up. I'll try with the proprietary application soon and compare results. New: Explain how to upgrade the firmware. Gadgetbridge people have a guide on how to upgrade the firmware , you need to get the firmware from the geek doing forum though, so it is interesting to create an account and watch the post.","title":"Amazfit Band 5"},{"location":"newsletter/2021_02_25/#operative-systems","text":"","title":"Operative Systems"},{"location":"newsletter/2021_02_25/#android","text":"","title":"Android"},{"location":"newsletter/2021_02_25/#gadgetbridge","text":"New: Add more guidelines to reverse engineer the band protocol.","title":"GadgetBridge"},{"location":"newsletter/2021_02_26/","text":"Coding \u2691 Python \u2691 GitPython \u2691 Improvement: Explain how to get the working directory of a repo. Using the working_dir attribute. perf(python_snippets#Group a list of dictionaries by a specific key) Explain how to group a list of dictionaries by a specific key With itertools.groupby . Jinja2 \u2691 New: Explain how to use Jinja2. Jinja2 is a modern and designer- friendly templating language for Python, modelled after Django\u2019s templates. It is fast, widely used and secure with the optional sandboxed template execution environment. Add installation, usage and basic and advanced template guidelines. Issues \u2691 Improvement: Monitor today's issues. Mkdocs migration to 7.x is giving errors with the search bar and repo stats. Life Management \u2691 Automation \u2691 Amazfit Band 5 \u2691 Improvement: Add insights on sleep tracking. You can't use the Withings sleep analyzer without their app (as expected), maybe the Emfit QS is the way to go.","title":"26th February 2021"},{"location":"newsletter/2021_02_26/#coding","text":"","title":"Coding"},{"location":"newsletter/2021_02_26/#python","text":"","title":"Python"},{"location":"newsletter/2021_02_26/#gitpython","text":"Improvement: Explain how to get the working directory of a repo. Using the working_dir attribute. perf(python_snippets#Group a list of dictionaries by a specific key) Explain how to group a list of dictionaries by a specific key With itertools.groupby .","title":"GitPython"},{"location":"newsletter/2021_02_26/#jinja2","text":"New: Explain how to use Jinja2. Jinja2 is a modern and designer- friendly templating language for Python, modelled after Django\u2019s templates. It is fast, widely used and secure with the optional sandboxed template execution environment. Add installation, usage and basic and advanced template guidelines.","title":"Jinja2"},{"location":"newsletter/2021_02_26/#issues","text":"Improvement: Monitor today's issues. Mkdocs migration to 7.x is giving errors with the search bar and repo stats.","title":"Issues"},{"location":"newsletter/2021_02_26/#life-management","text":"","title":"Life Management"},{"location":"newsletter/2021_02_26/#automation","text":"","title":"Automation"},{"location":"newsletter/2021_02_26/#amazfit-band-5","text":"Improvement: Add insights on sleep tracking. You can't use the Withings sleep analyzer without their app (as expected), maybe the Emfit QS is the way to go.","title":"Amazfit Band 5"},{"location":"newsletter/2021_02_28/","text":"Other \u2691 Correction: Deprecate mkdocs issues. They've been fixed in the last release","title":"28th February 2021"},{"location":"newsletter/2021_02_28/#other","text":"Correction: Deprecate mkdocs issues. They've been fixed in the last release","title":"Other"},{"location":"newsletter/2021_03/","text":"Introduction \u2691 Reorganization: Merge the Meta article into the index. Projects \u2691 Improvement: Add mkdocs-newsletter as a dormant plant. MkDocs plugin to show the changes of documentation repositories in a user friendly format, at the same time that it's easy for the authors to maintain. It creates daily, weekly, monthly and yearly newsletter articles with the changes of each period. Those pages, stored under the Newsletters section, are filled with the changes extracted from the commit messages of the git history. The changes are grouped by categories, subcategories and then by file using the order of the site's navigation structure. RSS feeds are also created for each newsletter type, so it's easy for people to keep updated with the evolution of the site. Reorganization: Update and reorganize projects. Following the digital garden metaphor Reorganization: Merge the wish_list article into the projects. New: Add seed to follow the updates of software. New: Add seed to automatically update the dockers of maintained services. DevOps \u2691 Infrastructure as Code \u2691 Helm Git \u2691 Correction: Suggest version 0.8.0 until issue is solved. Newer versions have a bug that makes impossible to use helm_git with a repository that contains just one chart in the root of the git repository. Monitoring \u2691 Prometheus Install \u2691 Correction: Add warning that helm 2 support is dropped. If you want to use the helm chart above 11.1.7 you need to use helm 3. Improvement: Add upgrading notes from 10.x -> 11.1.7. Don't upgrade to 12.x if you're still using Helm 2. Scrum \u2691 New: Introduce the scrum framework. Scrum is an agile framework for developing, delivering, and sustaining complex products, with an initial emphasis on software development, although it has been used in other fields such as personal task management. It is designed for teams of ten or fewer members, who break their work into goals that can be completed within time-boxed iterations, called sprints, no longer than one month and most commonly two weeks. The Scrum Team track progress in 15-minute time-boxed daily meetings, called daily scrums. At the end of the sprint, the team holds sprint review, to demonstrate the work done, a sprint retrospective to improve continuously, and a sprint planning to prepare next sprint's tasks. In the article I explain: I use to do the meetings : Daily , Refinement , Retros , Reviews and Plannings . The relevant roles . Some definitions , such as definition of done and definition of ready. Coding \u2691 Python \u2691 New: Add python landing page. Code Styling \u2691 Improvement: Don't use try-except to initialize dictionaries. Instead of: try : dictionary [ 'key' ] except KeyError : dictionary [ 'key' ] = {} Use: dictionary . setdefault ( 'key' , {}) Python Snippets \u2691 New: Add date management snippets. Get the week number of a datetime : datetime.datetime(2010, 6, 16).isocalendar()[1] . Get the Monday of a week number : import datetime d = \"2013-W26\" r = datetime . datetime . strptime ( d + '-1' , \"%Y-W%W-%w\" ) Get the month name from a number : import calendar >> calendar . month_name [ 3 ] 'March' * Get ordinal from number def int_to_ordinal ( number : int ) -> str : '''Convert an integer into its ordinal representation. make_ordinal(0) => '0th' make_ordinal(3) => '3rd' make_ordinal(122) => '122nd' make_ordinal(213) => '213th' Args: number: Number to convert Returns: ordinal representation of the number ''' suffix = [ 'th' , 'st' , 'nd' , 'rd' , 'th' ][ min ( number % 10 , 4 )] if 11 <= ( number % 100 ) <= 13 : suffix = 'th' return f \" { number }{ suffix } \" New: Add file management snippets. Remove the extension of a file Iterate over the files of a directory Create directory Touch a file Improvement: Get the first day of next month. New: Explain how to test directories and files. Pydantic \u2691 Correction: How to solve the No name 'BaseModel' in module 'pydantic'. It's still a patch, so I've also monitored the relevant issues Javascript \u2691 MermaidJS \u2691 New: Introduce the diagram library and how to make flowchart diagrams. MermaidJS is a Javascript library that lets you create diagrams using text and code. It can render the next diagram types : Flowchart Sequence. Gantt Class Git graph Entity Relationship User journey Issues \u2691 Improvement: Track python dependency errors. Correction: Gitdb has updated smmap. New: Jellyfin 10.7.1 broke the login page. Don't upgrade till it's solved, as the rollback is not easy. Correction: Jellyfin login page problem after upgrade to 10.7.X is solved. Surprisingly the instructions in #5489 solved it. systemctl stop jellyfin.service mv /var/lib/jellyfin/data/jellyfin.db { ,.bak } systemctl start jellyfin.service [ Go to JF URL, get asked to log in even though there are no Users in the JF DB now ] systemctl stop jellyfin.service mv /var/lib/jellyfin/data/jellyfin.db { .bak, } systemctl start jellyfin.service Software Architecture \u2691 Architecture Decision Record \u2691 Improvement: Explain how to show relationship between ADRs. Suggest a mermaidjs diagram to show the state of the project ADRs. Life Management \u2691 Automation \u2691 Life Automation \u2691 New: Suggest organize to act on computer file changes. organize looks good for automating processes on files. Maybe it's interesting to run it with inotifywait instead of with a cron job . Fitness Tracker \u2691 Improvement: Discovery of wasp-os and Colmi P8. wasp-os is an open source firmware for smart watches that are based on the nRF52 family of microcontrollers. Fully supported by gadgetbridge , Wasp-os features full heart rate monitoring and step counting support together with multiple clock faces, a stopwatch, an alarm clock, a countdown timer, a calculator and lots of other games and utilities. All of this, and still with access to the MicroPython REPL for interactive tweaking, development and testing. One of the supported devices, the Colmi P8 , looks really good. Health \u2691 Sleep \u2691 New: Explain the sleep cycle. Humans cycle through two types of sleep in a regular pattern throughout the night with a period of 90 minutes. They were called non-rapid eye movement (NREM) and rapid eye movement (REM). I answer the questions: What is the period of the REM/NREM cycle? What happens to your body in REM and NREM phases? How does the ratio of REM/NREM changes throughout the night? with a possible explanation. Why sleeping 6 hours can make you loose up to 90% of your REM or NREM phases? New: Explain sleeping time and sense distortions. Answer the questions: Why time feels longer in our dreams? How do we loose awareness of the outside world when sleeping? Activism \u2691 New: Define Diversity, Equity and Inclusion. Diversity is the representation and acknowledgement of the multitudes of identities, experiences, and ways of moving through the world. This includes\u2014but is not limited to\u2014ability, age, citizenship status, criminal record and/or incarceration, educational attainment, ethnicity, gender, geographical location, language, nationality, political affiliation, religion, race, sexuality, socioeconomic status, and veteran status. Further, we recognize that each individual's experience is informed by intersections across multiple identities. Equity seeks to ensure respect and equal opportunity for all, using all resources and tools to elevate the voices of under-represented and/or disadvantaged groups. Inclusion is fostering an environment in which people of all identities are welcome, valued, and supported. An inclusive organization solicits, listens to, learns from, and acts on the contributions of all its stakeholders. Operative Systems \u2691 Linux \u2691 ActivityWatch \u2691 New: Introduce ActivityWatch tracking software. It's a web application that can be installed both in Linux and Android that automatically tracks where you spend the time on. Super interesting for life logging and automating stuff. Until I save some time to react on the data, I'll just gather it and see how to aggregate it. Improvement: Add week insights. The browser watcher is not very accurate . The vim editor watcher doesn't add git branch information . Syncing data between devices is not yet supported . Tabs vs Buffers \u2691 New: Explain how to use tabs, buffers and windows in vim. Vim Plugins \u2691 New: Follow the issue to add elipsis instead of ... in vim-abolish. Correction: Forget to use abolish to insert the elipsis symbol. Tpope said that it's not going to happen. New: Introduce vim-easymotion. EasyMotion provides a much simpler way to use some motions in vim. It takes the <number> out of <number>w or <number>f{char} by highlighting all possible choices and allowing you to press one key to jump directly to the target. When one of the available motions is triggered, all visible text preceding or following the cursor is faded, and motion targets are highlighted. Reorganization: Move vim-test to the plugins page. elasticsearch \u2691 New: Explain how to reindex an index. mkdocs \u2691 New: Document the Navigation object and the on_nav event. Useful if you develop MkDocs plugins, it holds the information to build the navigation of the site. New: Describe navigation objects used in plugins. Explain how to use the Page , Section , and SectionPage objects. Correction: You need to edit the nav in the on_nav and not in the on_files event. Even though it seems more easy to create the nav structure in the on_files event, by editing the nav dictionary of the config object, there is no way of returning the config object in that event, so we're forced to do it in this event. Correction: Explain how to add files through a plugin. Long story short, use the on_config event instead of on_files and on_nav if you need to add files and want to change the navigation menu. New: Explain how to use MermaidJS diagrams. New: Explain how to test mkdocs plugins. New: Explain additions of version 7.1.0 of the material theme. Dark-light mode switch . Back to top button . Peek \u2691 New: Introduce Peek the screen recorder. Peek is a simple animated GIF screen recorder with an easy to use interface. If you try to use it with i3, you're going to have a bad time, you'd need to install Compton , and then the elements may not even be clickable . Syncthing \u2691 Improvement: Mention privacy configurations. Disable the Global Discovery and Relaying connections options. Vim \u2691 New: Add vim landing page. Android \u2691 Signal \u2691 New: Introduce the messaging app and how to decrypt the backups. Arts \u2691 Writing \u2691 Grammar and Orthography \u2691 Improvement: Expand the introduction and add Dave's suggested link. New: Explain where to add your pronouns. Hi, I\u2019m Lyz (he/him), I'm writing to tell you\u2026 New: Explain when to capitalize after a question mark. If the sentence ends after the question mark you should capitalize, if it doesn't end, you shouldn't have used the question mark, since it ends a sentence. New: Add textstat tests. To analyze the text readability New: Explain how to use the singular they . Writing Style \u2691 New: Analyze interesting books on writing style. The elements of style by William Strunk Jr and E.B White On writing well by William Zinsser Bird by bird by Anne Lamott On writing by Stephen King New: Explain how to end a letter. Use Sincerely in doubt and Best if you have more confidence. Add a comma after the sign-off and never use Cheers (it's what I've been doing all my life (\u25de\u2038\u25df\uff1b) ). Origami \u2691 New: Add mark1626 digital garden article on origamis. Other \u2691 New: Introduce Outrun. Outrun lets you execute a local command using the processing power of another Linux machine. New: Introduce the media system and monitor interesting issues. Jellyfin is a Free Software Media System that puts you in control of managing and streaming your media. It is an alternative to the proprietary Emby and Plex, to provide media from a dedicated server to end-user devices via multiple apps. Jellyfin is descended from Emby's 3.5.2 release and ported to the .NET Core framework to enable full cross-platform support. There are no strings attached, no premium licenses or features, and no hidden agendas: just a team who want to build something better and work together to achieve it.","title":"March of 2021"},{"location":"newsletter/2021_03/#introduction","text":"Reorganization: Merge the Meta article into the index.","title":"Introduction"},{"location":"newsletter/2021_03/#projects","text":"Improvement: Add mkdocs-newsletter as a dormant plant. MkDocs plugin to show the changes of documentation repositories in a user friendly format, at the same time that it's easy for the authors to maintain. It creates daily, weekly, monthly and yearly newsletter articles with the changes of each period. Those pages, stored under the Newsletters section, are filled with the changes extracted from the commit messages of the git history. The changes are grouped by categories, subcategories and then by file using the order of the site's navigation structure. RSS feeds are also created for each newsletter type, so it's easy for people to keep updated with the evolution of the site. Reorganization: Update and reorganize projects. Following the digital garden metaphor Reorganization: Merge the wish_list article into the projects. New: Add seed to follow the updates of software. New: Add seed to automatically update the dockers of maintained services.","title":"Projects"},{"location":"newsletter/2021_03/#devops","text":"","title":"DevOps"},{"location":"newsletter/2021_03/#infrastructure-as-code","text":"","title":"Infrastructure as Code"},{"location":"newsletter/2021_03/#helm-git","text":"Correction: Suggest version 0.8.0 until issue is solved. Newer versions have a bug that makes impossible to use helm_git with a repository that contains just one chart in the root of the git repository.","title":"Helm Git"},{"location":"newsletter/2021_03/#monitoring","text":"","title":"Monitoring"},{"location":"newsletter/2021_03/#prometheus-install","text":"Correction: Add warning that helm 2 support is dropped. If you want to use the helm chart above 11.1.7 you need to use helm 3. Improvement: Add upgrading notes from 10.x -> 11.1.7. Don't upgrade to 12.x if you're still using Helm 2.","title":"Prometheus Install"},{"location":"newsletter/2021_03/#scrum","text":"New: Introduce the scrum framework. Scrum is an agile framework for developing, delivering, and sustaining complex products, with an initial emphasis on software development, although it has been used in other fields such as personal task management. It is designed for teams of ten or fewer members, who break their work into goals that can be completed within time-boxed iterations, called sprints, no longer than one month and most commonly two weeks. The Scrum Team track progress in 15-minute time-boxed daily meetings, called daily scrums. At the end of the sprint, the team holds sprint review, to demonstrate the work done, a sprint retrospective to improve continuously, and a sprint planning to prepare next sprint's tasks. In the article I explain: I use to do the meetings : Daily , Refinement , Retros , Reviews and Plannings . The relevant roles . Some definitions , such as definition of done and definition of ready.","title":"Scrum"},{"location":"newsletter/2021_03/#coding","text":"","title":"Coding"},{"location":"newsletter/2021_03/#python","text":"New: Add python landing page.","title":"Python"},{"location":"newsletter/2021_03/#code-styling","text":"Improvement: Don't use try-except to initialize dictionaries. Instead of: try : dictionary [ 'key' ] except KeyError : dictionary [ 'key' ] = {} Use: dictionary . setdefault ( 'key' , {})","title":"Code Styling"},{"location":"newsletter/2021_03/#python-snippets","text":"New: Add date management snippets. Get the week number of a datetime : datetime.datetime(2010, 6, 16).isocalendar()[1] . Get the Monday of a week number : import datetime d = \"2013-W26\" r = datetime . datetime . strptime ( d + '-1' , \"%Y-W%W-%w\" ) Get the month name from a number : import calendar >> calendar . month_name [ 3 ] 'March' * Get ordinal from number def int_to_ordinal ( number : int ) -> str : '''Convert an integer into its ordinal representation. make_ordinal(0) => '0th' make_ordinal(3) => '3rd' make_ordinal(122) => '122nd' make_ordinal(213) => '213th' Args: number: Number to convert Returns: ordinal representation of the number ''' suffix = [ 'th' , 'st' , 'nd' , 'rd' , 'th' ][ min ( number % 10 , 4 )] if 11 <= ( number % 100 ) <= 13 : suffix = 'th' return f \" { number }{ suffix } \" New: Add file management snippets. Remove the extension of a file Iterate over the files of a directory Create directory Touch a file Improvement: Get the first day of next month. New: Explain how to test directories and files.","title":"Python Snippets"},{"location":"newsletter/2021_03/#pydantic","text":"Correction: How to solve the No name 'BaseModel' in module 'pydantic'. It's still a patch, so I've also monitored the relevant issues","title":"Pydantic"},{"location":"newsletter/2021_03/#javascript","text":"","title":"Javascript"},{"location":"newsletter/2021_03/#mermaidjs","text":"New: Introduce the diagram library and how to make flowchart diagrams. MermaidJS is a Javascript library that lets you create diagrams using text and code. It can render the next diagram types : Flowchart Sequence. Gantt Class Git graph Entity Relationship User journey","title":"MermaidJS"},{"location":"newsletter/2021_03/#issues","text":"Improvement: Track python dependency errors. Correction: Gitdb has updated smmap. New: Jellyfin 10.7.1 broke the login page. Don't upgrade till it's solved, as the rollback is not easy. Correction: Jellyfin login page problem after upgrade to 10.7.X is solved. Surprisingly the instructions in #5489 solved it. systemctl stop jellyfin.service mv /var/lib/jellyfin/data/jellyfin.db { ,.bak } systemctl start jellyfin.service [ Go to JF URL, get asked to log in even though there are no Users in the JF DB now ] systemctl stop jellyfin.service mv /var/lib/jellyfin/data/jellyfin.db { .bak, } systemctl start jellyfin.service","title":"Issues"},{"location":"newsletter/2021_03/#software-architecture","text":"","title":"Software Architecture"},{"location":"newsletter/2021_03/#architecture-decision-record","text":"Improvement: Explain how to show relationship between ADRs. Suggest a mermaidjs diagram to show the state of the project ADRs.","title":"Architecture Decision Record"},{"location":"newsletter/2021_03/#life-management","text":"","title":"Life Management"},{"location":"newsletter/2021_03/#automation","text":"","title":"Automation"},{"location":"newsletter/2021_03/#life-automation","text":"New: Suggest organize to act on computer file changes. organize looks good for automating processes on files. Maybe it's interesting to run it with inotifywait instead of with a cron job .","title":"Life Automation"},{"location":"newsletter/2021_03/#fitness-tracker","text":"Improvement: Discovery of wasp-os and Colmi P8. wasp-os is an open source firmware for smart watches that are based on the nRF52 family of microcontrollers. Fully supported by gadgetbridge , Wasp-os features full heart rate monitoring and step counting support together with multiple clock faces, a stopwatch, an alarm clock, a countdown timer, a calculator and lots of other games and utilities. All of this, and still with access to the MicroPython REPL for interactive tweaking, development and testing. One of the supported devices, the Colmi P8 , looks really good.","title":"Fitness Tracker"},{"location":"newsletter/2021_03/#health","text":"","title":"Health"},{"location":"newsletter/2021_03/#sleep","text":"New: Explain the sleep cycle. Humans cycle through two types of sleep in a regular pattern throughout the night with a period of 90 minutes. They were called non-rapid eye movement (NREM) and rapid eye movement (REM). I answer the questions: What is the period of the REM/NREM cycle? What happens to your body in REM and NREM phases? How does the ratio of REM/NREM changes throughout the night? with a possible explanation. Why sleeping 6 hours can make you loose up to 90% of your REM or NREM phases? New: Explain sleeping time and sense distortions. Answer the questions: Why time feels longer in our dreams? How do we loose awareness of the outside world when sleeping?","title":"Sleep"},{"location":"newsletter/2021_03/#activism","text":"New: Define Diversity, Equity and Inclusion. Diversity is the representation and acknowledgement of the multitudes of identities, experiences, and ways of moving through the world. This includes\u2014but is not limited to\u2014ability, age, citizenship status, criminal record and/or incarceration, educational attainment, ethnicity, gender, geographical location, language, nationality, political affiliation, religion, race, sexuality, socioeconomic status, and veteran status. Further, we recognize that each individual's experience is informed by intersections across multiple identities. Equity seeks to ensure respect and equal opportunity for all, using all resources and tools to elevate the voices of under-represented and/or disadvantaged groups. Inclusion is fostering an environment in which people of all identities are welcome, valued, and supported. An inclusive organization solicits, listens to, learns from, and acts on the contributions of all its stakeholders.","title":"Activism"},{"location":"newsletter/2021_03/#operative-systems","text":"","title":"Operative Systems"},{"location":"newsletter/2021_03/#linux","text":"","title":"Linux"},{"location":"newsletter/2021_03/#activitywatch","text":"New: Introduce ActivityWatch tracking software. It's a web application that can be installed both in Linux and Android that automatically tracks where you spend the time on. Super interesting for life logging and automating stuff. Until I save some time to react on the data, I'll just gather it and see how to aggregate it. Improvement: Add week insights. The browser watcher is not very accurate . The vim editor watcher doesn't add git branch information . Syncing data between devices is not yet supported .","title":"ActivityWatch"},{"location":"newsletter/2021_03/#tabs-vs-buffers","text":"New: Explain how to use tabs, buffers and windows in vim.","title":"Tabs vs Buffers"},{"location":"newsletter/2021_03/#vim-plugins","text":"New: Follow the issue to add elipsis instead of ... in vim-abolish. Correction: Forget to use abolish to insert the elipsis symbol. Tpope said that it's not going to happen. New: Introduce vim-easymotion. EasyMotion provides a much simpler way to use some motions in vim. It takes the <number> out of <number>w or <number>f{char} by highlighting all possible choices and allowing you to press one key to jump directly to the target. When one of the available motions is triggered, all visible text preceding or following the cursor is faded, and motion targets are highlighted. Reorganization: Move vim-test to the plugins page.","title":"Vim Plugins"},{"location":"newsletter/2021_03/#elasticsearch","text":"New: Explain how to reindex an index.","title":"elasticsearch"},{"location":"newsletter/2021_03/#mkdocs","text":"New: Document the Navigation object and the on_nav event. Useful if you develop MkDocs plugins, it holds the information to build the navigation of the site. New: Describe navigation objects used in plugins. Explain how to use the Page , Section , and SectionPage objects. Correction: You need to edit the nav in the on_nav and not in the on_files event. Even though it seems more easy to create the nav structure in the on_files event, by editing the nav dictionary of the config object, there is no way of returning the config object in that event, so we're forced to do it in this event. Correction: Explain how to add files through a plugin. Long story short, use the on_config event instead of on_files and on_nav if you need to add files and want to change the navigation menu. New: Explain how to use MermaidJS diagrams. New: Explain how to test mkdocs plugins. New: Explain additions of version 7.1.0 of the material theme. Dark-light mode switch . Back to top button .","title":"mkdocs"},{"location":"newsletter/2021_03/#peek","text":"New: Introduce Peek the screen recorder. Peek is a simple animated GIF screen recorder with an easy to use interface. If you try to use it with i3, you're going to have a bad time, you'd need to install Compton , and then the elements may not even be clickable .","title":"Peek"},{"location":"newsletter/2021_03/#syncthing","text":"Improvement: Mention privacy configurations. Disable the Global Discovery and Relaying connections options.","title":"Syncthing"},{"location":"newsletter/2021_03/#vim","text":"New: Add vim landing page.","title":"Vim"},{"location":"newsletter/2021_03/#android","text":"","title":"Android"},{"location":"newsletter/2021_03/#signal","text":"New: Introduce the messaging app and how to decrypt the backups.","title":"Signal"},{"location":"newsletter/2021_03/#arts","text":"","title":"Arts"},{"location":"newsletter/2021_03/#writing","text":"","title":"Writing"},{"location":"newsletter/2021_03/#grammar-and-orthography","text":"Improvement: Expand the introduction and add Dave's suggested link. New: Explain where to add your pronouns. Hi, I\u2019m Lyz (he/him), I'm writing to tell you\u2026 New: Explain when to capitalize after a question mark. If the sentence ends after the question mark you should capitalize, if it doesn't end, you shouldn't have used the question mark, since it ends a sentence. New: Add textstat tests. To analyze the text readability New: Explain how to use the singular they .","title":"Grammar and Orthography"},{"location":"newsletter/2021_03/#writing-style","text":"New: Analyze interesting books on writing style. The elements of style by William Strunk Jr and E.B White On writing well by William Zinsser Bird by bird by Anne Lamott On writing by Stephen King New: Explain how to end a letter. Use Sincerely in doubt and Best if you have more confidence. Add a comma after the sign-off and never use Cheers (it's what I've been doing all my life (\u25de\u2038\u25df\uff1b) ).","title":"Writing Style"},{"location":"newsletter/2021_03/#origami","text":"New: Add mark1626 digital garden article on origamis.","title":"Origami"},{"location":"newsletter/2021_03/#other","text":"New: Introduce Outrun. Outrun lets you execute a local command using the processing power of another Linux machine. New: Introduce the media system and monitor interesting issues. Jellyfin is a Free Software Media System that puts you in control of managing and streaming your media. It is an alternative to the proprietary Emby and Plex, to provide media from a dedicated server to end-user devices via multiple apps. Jellyfin is descended from Emby's 3.5.2 release and ported to the .NET Core framework to enable full cross-platform support. There are no strings attached, no premium licenses or features, and no hidden agendas: just a team who want to build something better and work together to achieve it.","title":"Other"},{"location":"newsletter/2021_03_02/","text":"DevOps \u2691 Scrum \u2691 New: Introduce the scrum framework. Scrum is an agile framework for developing, delivering, and sustaining complex products, with an initial emphasis on software development, although it has been used in other fields such as personal task management. It is designed for teams of ten or fewer members, who break their work into goals that can be completed within time-boxed iterations, called sprints, no longer than one month and most commonly two weeks. The Scrum Team track progress in 15-minute time-boxed daily meetings, called daily scrums. At the end of the sprint, the team holds sprint review, to demonstrate the work done, a sprint retrospective to improve continuously, and a sprint planning to prepare next sprint's tasks. In the article I explain: I use to do the meetings : Daily , Refinement , Retros , Reviews and Plannings . The relevant roles . Some definitions , such as definition of done and definition of ready. Coding \u2691 Python \u2691 Pydantic \u2691 Correction: How to solve the No name 'BaseModel' in module 'pydantic'. It's still a patch, so I've also monitored the relevant issues Activism \u2691 New: Define Diversity, Equity and Inclusion. Diversity is the representation and acknowledgement of the multitudes of identities, experiences, and ways of moving through the world. This includes\u2014but is not limited to\u2014ability, age, citizenship status, criminal record and/or incarceration, educational attainment, ethnicity, gender, geographical location, language, nationality, political affiliation, religion, race, sexuality, socioeconomic status, and veteran status. Further, we recognize that each individual's experience is informed by intersections across multiple identities. Equity seeks to ensure respect and equal opportunity for all, using all resources and tools to elevate the voices of under-represented and/or disadvantaged groups. Inclusion is fostering an environment in which people of all identities are welcome, valued, and supported. An inclusive organization solicits, listens to, learns from, and acts on the contributions of all its stakeholders. Arts \u2691 Writing \u2691 Grammar and Orthography \u2691 Improvement: Expand the introduction and add Dave's suggested link.","title":"2nd March 2021"},{"location":"newsletter/2021_03_02/#devops","text":"","title":"DevOps"},{"location":"newsletter/2021_03_02/#scrum","text":"New: Introduce the scrum framework. Scrum is an agile framework for developing, delivering, and sustaining complex products, with an initial emphasis on software development, although it has been used in other fields such as personal task management. It is designed for teams of ten or fewer members, who break their work into goals that can be completed within time-boxed iterations, called sprints, no longer than one month and most commonly two weeks. The Scrum Team track progress in 15-minute time-boxed daily meetings, called daily scrums. At the end of the sprint, the team holds sprint review, to demonstrate the work done, a sprint retrospective to improve continuously, and a sprint planning to prepare next sprint's tasks. In the article I explain: I use to do the meetings : Daily , Refinement , Retros , Reviews and Plannings . The relevant roles . Some definitions , such as definition of done and definition of ready.","title":"Scrum"},{"location":"newsletter/2021_03_02/#coding","text":"","title":"Coding"},{"location":"newsletter/2021_03_02/#python","text":"","title":"Python"},{"location":"newsletter/2021_03_02/#pydantic","text":"Correction: How to solve the No name 'BaseModel' in module 'pydantic'. It's still a patch, so I've also monitored the relevant issues","title":"Pydantic"},{"location":"newsletter/2021_03_02/#activism","text":"New: Define Diversity, Equity and Inclusion. Diversity is the representation and acknowledgement of the multitudes of identities, experiences, and ways of moving through the world. This includes\u2014but is not limited to\u2014ability, age, citizenship status, criminal record and/or incarceration, educational attainment, ethnicity, gender, geographical location, language, nationality, political affiliation, religion, race, sexuality, socioeconomic status, and veteran status. Further, we recognize that each individual's experience is informed by intersections across multiple identities. Equity seeks to ensure respect and equal opportunity for all, using all resources and tools to elevate the voices of under-represented and/or disadvantaged groups. Inclusion is fostering an environment in which people of all identities are welcome, valued, and supported. An inclusive organization solicits, listens to, learns from, and acts on the contributions of all its stakeholders.","title":"Activism"},{"location":"newsletter/2021_03_02/#arts","text":"","title":"Arts"},{"location":"newsletter/2021_03_02/#writing","text":"","title":"Writing"},{"location":"newsletter/2021_03_02/#grammar-and-orthography","text":"Improvement: Expand the introduction and add Dave's suggested link.","title":"Grammar and Orthography"},{"location":"newsletter/2021_03_03/","text":"Coding \u2691 Python \u2691 Code Styling \u2691 Improvement: Don't use try-except to initialize dictionaries. Instead of: try : dictionary [ 'key' ] except KeyError : dictionary [ 'key' ] = {} Use: dictionary . setdefault ( 'key' , {}) Python Snippets \u2691 New: Add date management snippets. Get the week number of a datetime : datetime.datetime(2010, 6, 16).isocalendar()[1] . Get the Monday of a week number : import datetime d = \"2013-W26\" r = datetime . datetime . strptime ( d + '-1' , \"%Y-W%W-%w\" ) Get the month name from a number :. import calendar >> calendar . month_name [ 3 ] 'March' * Get ordinal from number def int_to_ordinal ( number : int ) -> str : '''Convert an integer into its ordinal representation. make_ordinal(0) => '0th' make_ordinal(3) => '3rd' make_ordinal(122) => '122nd' make_ordinal(213) => '213th' Args: number: Number to convert Returns: ordinal representation of the number ''' suffix = [ 'th' , 'st' , 'nd' , 'rd' , 'th' ][ min ( number % 10 , 4 )] if 11 <= ( number % 100 ) <= 13 : suffix = 'th' return f \" { number }{ suffix } \" Operative Systems \u2691 Linux \u2691 Vim Plugins \u2691 New: Follow the issue to add elipsis instead of ... in vim-abolish. mkdocs \u2691 New: Document the Navigation object and the on_nav event. Useful if you develop MkDocs plugins, it holds the information to build the navigation of the site. Arts \u2691 Writing \u2691 Grammar and Orthography \u2691 New: Explain where to add your pronouns. Hi, I\u2019m Lyz (he/him), I'm writing to tell you\u2026 New: Explain when to capitalize after a question mark. If the sentence ends after the question mark you should capitalize, if it doesn't end, you shouldn't have used the question mark, since it ends a sentence. Writing Style \u2691 New: Analyze interesting books on writing style. The elements of style by William Strunk Jr and E.B White On writing well by William Zinsser Bird by bird by Anne Lamott On writing by Stephen King New: Explain how to end a letter. Use Sincerely in doubt and Best if you have more confidence. Add a comma after the sign-off and never use Cheers (it's what I've been doing all my life (\u25de\u2038\u25df\uff1b) ).","title":"3rd March 2021"},{"location":"newsletter/2021_03_03/#coding","text":"","title":"Coding"},{"location":"newsletter/2021_03_03/#python","text":"","title":"Python"},{"location":"newsletter/2021_03_03/#code-styling","text":"Improvement: Don't use try-except to initialize dictionaries. Instead of: try : dictionary [ 'key' ] except KeyError : dictionary [ 'key' ] = {} Use: dictionary . setdefault ( 'key' , {})","title":"Code Styling"},{"location":"newsletter/2021_03_03/#python-snippets","text":"New: Add date management snippets. Get the week number of a datetime : datetime.datetime(2010, 6, 16).isocalendar()[1] . Get the Monday of a week number : import datetime d = \"2013-W26\" r = datetime . datetime . strptime ( d + '-1' , \"%Y-W%W-%w\" ) Get the month name from a number :. import calendar >> calendar . month_name [ 3 ] 'March' * Get ordinal from number def int_to_ordinal ( number : int ) -> str : '''Convert an integer into its ordinal representation. make_ordinal(0) => '0th' make_ordinal(3) => '3rd' make_ordinal(122) => '122nd' make_ordinal(213) => '213th' Args: number: Number to convert Returns: ordinal representation of the number ''' suffix = [ 'th' , 'st' , 'nd' , 'rd' , 'th' ][ min ( number % 10 , 4 )] if 11 <= ( number % 100 ) <= 13 : suffix = 'th' return f \" { number }{ suffix } \"","title":"Python Snippets"},{"location":"newsletter/2021_03_03/#operative-systems","text":"","title":"Operative Systems"},{"location":"newsletter/2021_03_03/#linux","text":"","title":"Linux"},{"location":"newsletter/2021_03_03/#vim-plugins","text":"New: Follow the issue to add elipsis instead of ... in vim-abolish.","title":"Vim Plugins"},{"location":"newsletter/2021_03_03/#mkdocs","text":"New: Document the Navigation object and the on_nav event. Useful if you develop MkDocs plugins, it holds the information to build the navigation of the site.","title":"mkdocs"},{"location":"newsletter/2021_03_03/#arts","text":"","title":"Arts"},{"location":"newsletter/2021_03_03/#writing","text":"","title":"Writing"},{"location":"newsletter/2021_03_03/#grammar-and-orthography","text":"New: Explain where to add your pronouns. Hi, I\u2019m Lyz (he/him), I'm writing to tell you\u2026 New: Explain when to capitalize after a question mark. If the sentence ends after the question mark you should capitalize, if it doesn't end, you shouldn't have used the question mark, since it ends a sentence.","title":"Grammar and Orthography"},{"location":"newsletter/2021_03_03/#writing-style","text":"New: Analyze interesting books on writing style. The elements of style by William Strunk Jr and E.B White On writing well by William Zinsser Bird by bird by Anne Lamott On writing by Stephen King New: Explain how to end a letter. Use Sincerely in doubt and Best if you have more confidence. Add a comma after the sign-off and never use Cheers (it's what I've been doing all my life (\u25de\u2038\u25df\uff1b) ).","title":"Writing Style"},{"location":"newsletter/2021_03_04/","text":"DevOps \u2691 Infrastructure as Code \u2691 Helm Git \u2691 Correction: Suggest version 0.8.0 until issue is solved. Newer versions have a bug that makes impossible to use helm_git with a repository that contains just one chart in the root of the git repository. Monitoring \u2691 Prometheus Install \u2691 Correction: Add warning that helm 2 support is dropped. If you want to use the helm chart above 11.1.7 you need to use helm 3. Health \u2691 Sleep \u2691 New: Explain the sleep cycle. Humans cycle through two types of sleep in a regular pattern throughout the night with a period of 90 minutes. They were called non-rapid eye movement (NREM) and rapid eye movement (REM). I answer the questions: What is the period of the REM/NREM cycle? What happens to your body in REM and NREM phases? How does the ratio of REM/NREM changes throughout the night? with a possible explanation. Why sleeping 6 hours can make you loose up to 90% of your REM or NREM phases? New: Explain sleeping time and sense distortions. Answer the questions: Why time feels longer in our dreams? How do we loose awareness of the outside world when sleeping? Operative Systems \u2691 Linux \u2691 mkdocs \u2691 New: Describe navigation objects used in plugins. Explain how to use the Page , Section , and SectionPage objects. Correction: You need to edit the nav in the on_nav and not in the on_files event. Even though it seems more easy to create the nav structure in the on_files event, by editing the nav dictionary of the config object, there is no way of returning the config object in that event, so we're forced to do it in this event. Arts \u2691 Origami \u2691 New: Add mark1626 digital garden article on origamis.","title":"4th March 2021"},{"location":"newsletter/2021_03_04/#devops","text":"","title":"DevOps"},{"location":"newsletter/2021_03_04/#infrastructure-as-code","text":"","title":"Infrastructure as Code"},{"location":"newsletter/2021_03_04/#helm-git","text":"Correction: Suggest version 0.8.0 until issue is solved. Newer versions have a bug that makes impossible to use helm_git with a repository that contains just one chart in the root of the git repository.","title":"Helm Git"},{"location":"newsletter/2021_03_04/#monitoring","text":"","title":"Monitoring"},{"location":"newsletter/2021_03_04/#prometheus-install","text":"Correction: Add warning that helm 2 support is dropped. If you want to use the helm chart above 11.1.7 you need to use helm 3.","title":"Prometheus Install"},{"location":"newsletter/2021_03_04/#health","text":"","title":"Health"},{"location":"newsletter/2021_03_04/#sleep","text":"New: Explain the sleep cycle. Humans cycle through two types of sleep in a regular pattern throughout the night with a period of 90 minutes. They were called non-rapid eye movement (NREM) and rapid eye movement (REM). I answer the questions: What is the period of the REM/NREM cycle? What happens to your body in REM and NREM phases? How does the ratio of REM/NREM changes throughout the night? with a possible explanation. Why sleeping 6 hours can make you loose up to 90% of your REM or NREM phases? New: Explain sleeping time and sense distortions. Answer the questions: Why time feels longer in our dreams? How do we loose awareness of the outside world when sleeping?","title":"Sleep"},{"location":"newsletter/2021_03_04/#operative-systems","text":"","title":"Operative Systems"},{"location":"newsletter/2021_03_04/#linux","text":"","title":"Linux"},{"location":"newsletter/2021_03_04/#mkdocs","text":"New: Describe navigation objects used in plugins. Explain how to use the Page , Section , and SectionPage objects. Correction: You need to edit the nav in the on_nav and not in the on_files event. Even though it seems more easy to create the nav structure in the on_files event, by editing the nav dictionary of the config object, there is no way of returning the config object in that event, so we're forced to do it in this event.","title":"mkdocs"},{"location":"newsletter/2021_03_04/#arts","text":"","title":"Arts"},{"location":"newsletter/2021_03_04/#origami","text":"New: Add mark1626 digital garden article on origamis.","title":"Origami"},{"location":"newsletter/2021_03_10/","text":"DevOps \u2691 Monitoring \u2691 Prometheus Install \u2691 Improvement: Add upgrading notes from 10.x -> 11.1.7. Don't upgrade to 12.x if you're still using Helm 2. Coding \u2691 Python \u2691 Python Snippets \u2691 New: Add file management snippets. Remove the extension of a file Iterate over the files of a directory Create directory Touch a file Improvement: Get the first day of next month. Life Management \u2691 Automation \u2691 Fitness Tracker \u2691 Improvement: Discovery of wasp-os and Colmi P8. wasp-os is an open source firmware for smart watches that are based on the nRF52 family of microcontrollers. Fully supported by gadgetbridge , Wasp-os features full heart rate monitoring and step counting support together with multiple clock faces, a stopwatch, an alarm clock, a countdown timer, a calculator and lots of other games and utilities. All of this, and still with access to the MicroPython REPL for interactive tweaking, development and testing. One of the supported devices, the Colmi P8 , looks really good. Operative Systems \u2691 Linux \u2691 mkdocs \u2691 Correction: Explain how to add files through a plugin. Long story short, use the on_config event instead of on_files and on_nav if you need to add files and want to change the navigation menu.","title":"10th March 2021"},{"location":"newsletter/2021_03_10/#devops","text":"","title":"DevOps"},{"location":"newsletter/2021_03_10/#monitoring","text":"","title":"Monitoring"},{"location":"newsletter/2021_03_10/#prometheus-install","text":"Improvement: Add upgrading notes from 10.x -> 11.1.7. Don't upgrade to 12.x if you're still using Helm 2.","title":"Prometheus Install"},{"location":"newsletter/2021_03_10/#coding","text":"","title":"Coding"},{"location":"newsletter/2021_03_10/#python","text":"","title":"Python"},{"location":"newsletter/2021_03_10/#python-snippets","text":"New: Add file management snippets. Remove the extension of a file Iterate over the files of a directory Create directory Touch a file Improvement: Get the first day of next month.","title":"Python Snippets"},{"location":"newsletter/2021_03_10/#life-management","text":"","title":"Life Management"},{"location":"newsletter/2021_03_10/#automation","text":"","title":"Automation"},{"location":"newsletter/2021_03_10/#fitness-tracker","text":"Improvement: Discovery of wasp-os and Colmi P8. wasp-os is an open source firmware for smart watches that are based on the nRF52 family of microcontrollers. Fully supported by gadgetbridge , Wasp-os features full heart rate monitoring and step counting support together with multiple clock faces, a stopwatch, an alarm clock, a countdown timer, a calculator and lots of other games and utilities. All of this, and still with access to the MicroPython REPL for interactive tweaking, development and testing. One of the supported devices, the Colmi P8 , looks really good.","title":"Fitness Tracker"},{"location":"newsletter/2021_03_10/#operative-systems","text":"","title":"Operative Systems"},{"location":"newsletter/2021_03_10/#linux","text":"","title":"Linux"},{"location":"newsletter/2021_03_10/#mkdocs","text":"Correction: Explain how to add files through a plugin. Long story short, use the on_config event instead of on_files and on_nav if you need to add files and want to change the navigation menu.","title":"mkdocs"},{"location":"newsletter/2021_03_13/","text":"Operative Systems \u2691 Linux \u2691 ActivityWatch \u2691 New: Introduce ActivityWatch tracking software. It's a web application that can be installed both in Linux and Android that automatically tracks where you spend the time on. Super interesting for life logging and automating stuff. Until I save some time to react on the data, I'll just gather it and see how to aggregate it. Syncthing \u2691 Improvement: Mention privacy configurations. Disable the Global Discovery and Relaying connections options. Android \u2691 Signal \u2691 New: Introduce the messaging app and how to decrypt the backups.","title":"13th March 2021"},{"location":"newsletter/2021_03_13/#operative-systems","text":"","title":"Operative Systems"},{"location":"newsletter/2021_03_13/#linux","text":"","title":"Linux"},{"location":"newsletter/2021_03_13/#activitywatch","text":"New: Introduce ActivityWatch tracking software. It's a web application that can be installed both in Linux and Android that automatically tracks where you spend the time on. Super interesting for life logging and automating stuff. Until I save some time to react on the data, I'll just gather it and see how to aggregate it.","title":"ActivityWatch"},{"location":"newsletter/2021_03_13/#syncthing","text":"Improvement: Mention privacy configurations. Disable the Global Discovery and Relaying connections options.","title":"Syncthing"},{"location":"newsletter/2021_03_13/#android","text":"","title":"Android"},{"location":"newsletter/2021_03_13/#signal","text":"New: Introduce the messaging app and how to decrypt the backups.","title":"Signal"},{"location":"newsletter/2021_03_18/","text":"Coding \u2691 Javascript \u2691 MermaidJS \u2691 New: Introduce the diagram library and how to make flowchart diagrams. MermaidJS is a Javascript library that lets you create diagrams using text and code. It can render the next diagram types : Flowchart Sequence. Gantt Class Git graph Entity Relationship User journey Software Architecture \u2691 Architecture Decision Record \u2691 Improvement: Explain how to show relationship between ADRs. Suggest a mermaidjs diagram to show the state of the project ADRs. Life Management \u2691 Automation \u2691 Life Automation \u2691 New: Suggest organize to act on computer file changes. organize looks good for automating processes on files. Maybe it's interesting to run it with inotifywait instead of with a cron job . Operative Systems \u2691 Linux \u2691 ActivityWatch \u2691 Improvement: Add week insights. The browser watcher is not very accurate . The vim editor watcher doesn't add git branch information . Syncing data between devices is not yet supported . Vim Plugins \u2691 Correction: Forget to use abolish to insert the elipsis symbol. Tpope said that it's not going to happen. mkdocs \u2691 New: Explain how to use MermaidJS diagrams. Peek \u2691 New: Introduce Peek the screen recorder. Peek is a simple animated GIF screen recorder with an easy to use interface. If you try to use it with i3, you're going to have a bad time, you'd need to install Compton , and then the elements may not even be clickable .","title":"18th March 2021"},{"location":"newsletter/2021_03_18/#coding","text":"","title":"Coding"},{"location":"newsletter/2021_03_18/#javascript","text":"","title":"Javascript"},{"location":"newsletter/2021_03_18/#mermaidjs","text":"New: Introduce the diagram library and how to make flowchart diagrams. MermaidJS is a Javascript library that lets you create diagrams using text and code. It can render the next diagram types : Flowchart Sequence. Gantt Class Git graph Entity Relationship User journey","title":"MermaidJS"},{"location":"newsletter/2021_03_18/#software-architecture","text":"","title":"Software Architecture"},{"location":"newsletter/2021_03_18/#architecture-decision-record","text":"Improvement: Explain how to show relationship between ADRs. Suggest a mermaidjs diagram to show the state of the project ADRs.","title":"Architecture Decision Record"},{"location":"newsletter/2021_03_18/#life-management","text":"","title":"Life Management"},{"location":"newsletter/2021_03_18/#automation","text":"","title":"Automation"},{"location":"newsletter/2021_03_18/#life-automation","text":"New: Suggest organize to act on computer file changes. organize looks good for automating processes on files. Maybe it's interesting to run it with inotifywait instead of with a cron job .","title":"Life Automation"},{"location":"newsletter/2021_03_18/#operative-systems","text":"","title":"Operative Systems"},{"location":"newsletter/2021_03_18/#linux","text":"","title":"Linux"},{"location":"newsletter/2021_03_18/#activitywatch","text":"Improvement: Add week insights. The browser watcher is not very accurate . The vim editor watcher doesn't add git branch information . Syncing data between devices is not yet supported .","title":"ActivityWatch"},{"location":"newsletter/2021_03_18/#vim-plugins","text":"Correction: Forget to use abolish to insert the elipsis symbol. Tpope said that it's not going to happen.","title":"Vim Plugins"},{"location":"newsletter/2021_03_18/#mkdocs","text":"New: Explain how to use MermaidJS diagrams.","title":"mkdocs"},{"location":"newsletter/2021_03_18/#peek","text":"New: Introduce Peek the screen recorder. Peek is a simple animated GIF screen recorder with an easy to use interface. If you try to use it with i3, you're going to have a bad time, you'd need to install Compton , and then the elements may not even be clickable .","title":"Peek"},{"location":"newsletter/2021_03_19/","text":"Coding \u2691 Python \u2691 New: Add python landing page. Issues \u2691 Improvement: Track python dependency errors. Operative Systems \u2691 Linux \u2691 Tabs vs Buffers \u2691 New: Explain how to use tabs, buffers and windows in vim. Vim Plugins \u2691 New: Introduce vim-easymotion. EasyMotion provides a much simpler way to use some motions in vim. It takes the <number> out of <number>w or <number>f{char} by highlighting all possible choices and allowing you to press one key to jump directly to the target. When one of the available motions is triggered, all visible text preceding or following the cursor is faded, and motion targets are highlighted. Reorganization: Move vim-test to the plugins page. mkdocs \u2691 New: Explain how to test mkdocs plugins. Vim \u2691 New: Add vim landing page. Arts \u2691 Writing \u2691 Build your own Digital Garden \u2691 New: Add textstat tests. To analyze the text readability Other \u2691 New: Introduce Outrun. Outrun lets you execute a local command using the processing power of another Linux machine.","title":"19th March 2021"},{"location":"newsletter/2021_03_19/#coding","text":"","title":"Coding"},{"location":"newsletter/2021_03_19/#python","text":"New: Add python landing page.","title":"Python"},{"location":"newsletter/2021_03_19/#issues","text":"Improvement: Track python dependency errors.","title":"Issues"},{"location":"newsletter/2021_03_19/#operative-systems","text":"","title":"Operative Systems"},{"location":"newsletter/2021_03_19/#linux","text":"","title":"Linux"},{"location":"newsletter/2021_03_19/#tabs-vs-buffers","text":"New: Explain how to use tabs, buffers and windows in vim.","title":"Tabs vs Buffers"},{"location":"newsletter/2021_03_19/#vim-plugins","text":"New: Introduce vim-easymotion. EasyMotion provides a much simpler way to use some motions in vim. It takes the <number> out of <number>w or <number>f{char} by highlighting all possible choices and allowing you to press one key to jump directly to the target. When one of the available motions is triggered, all visible text preceding or following the cursor is faded, and motion targets are highlighted. Reorganization: Move vim-test to the plugins page.","title":"Vim Plugins"},{"location":"newsletter/2021_03_19/#mkdocs","text":"New: Explain how to test mkdocs plugins.","title":"mkdocs"},{"location":"newsletter/2021_03_19/#vim","text":"New: Add vim landing page.","title":"Vim"},{"location":"newsletter/2021_03_19/#arts","text":"","title":"Arts"},{"location":"newsletter/2021_03_19/#writing","text":"","title":"Writing"},{"location":"newsletter/2021_03_19/#build-your-own-digital-garden","text":"New: Add textstat tests. To analyze the text readability","title":"Build your own Digital Garden"},{"location":"newsletter/2021_03_19/#other","text":"New: Introduce Outrun. Outrun lets you execute a local command using the processing power of another Linux machine.","title":"Other"},{"location":"newsletter/2021_03_26/","text":"Coding \u2691 Python \u2691 Python Snippets \u2691 New: Explain how to test directories and files. Issues \u2691 Correction: Gitdb has updated smmap. Arts \u2691 Writing \u2691 Grammar and Orthography \u2691 New: Explain how to use the singular they .","title":"26th March 2021"},{"location":"newsletter/2021_03_26/#coding","text":"","title":"Coding"},{"location":"newsletter/2021_03_26/#python","text":"","title":"Python"},{"location":"newsletter/2021_03_26/#python-snippets","text":"New: Explain how to test directories and files.","title":"Python Snippets"},{"location":"newsletter/2021_03_26/#issues","text":"Correction: Gitdb has updated smmap.","title":"Issues"},{"location":"newsletter/2021_03_26/#arts","text":"","title":"Arts"},{"location":"newsletter/2021_03_26/#writing","text":"","title":"Writing"},{"location":"newsletter/2021_03_26/#grammar-and-orthography","text":"New: Explain how to use the singular they .","title":"Grammar and Orthography"},{"location":"newsletter/2021_03_29/","text":"Projects \u2691 Improvement: Add mkdocs-newsletter as a dormant plant. MkDocs plugin to show the changes of documentation repositories in a user friendly format, at the same time that it's easy for the authors to maintain. It creates daily, weekly, monthly and yearly newsletter articles with the changes of each period. Those pages, stored under the Newsletters section, are filled with the changes extracted from the commit messages of the git history. The changes are grouped by categories, subcategories and then by file using the order of the site's navigation structure. RSS feeds are also created for each newsletter type, so it's easy for people to keep updated with the evolution of the site. Reorganization: Update and reorganize projects. Following the digital garden metaphor Coding \u2691 Issues \u2691 New: Jellyfin 10.7.1 broke the login page. Don't upgrade till it's solved, as the rollback is not easy. Operative Systems \u2691 Linux \u2691 elasticsearch \u2691 New: Explain how to reindex an index. mkdocs \u2691 New: Explain additions of version 7.1.0 of the material theme. Dark-light mode switch . Back to top button .","title":"29th March 2021"},{"location":"newsletter/2021_03_29/#projects","text":"Improvement: Add mkdocs-newsletter as a dormant plant. MkDocs plugin to show the changes of documentation repositories in a user friendly format, at the same time that it's easy for the authors to maintain. It creates daily, weekly, monthly and yearly newsletter articles with the changes of each period. Those pages, stored under the Newsletters section, are filled with the changes extracted from the commit messages of the git history. The changes are grouped by categories, subcategories and then by file using the order of the site's navigation structure. RSS feeds are also created for each newsletter type, so it's easy for people to keep updated with the evolution of the site. Reorganization: Update and reorganize projects. Following the digital garden metaphor","title":"Projects"},{"location":"newsletter/2021_03_29/#coding","text":"","title":"Coding"},{"location":"newsletter/2021_03_29/#issues","text":"New: Jellyfin 10.7.1 broke the login page. Don't upgrade till it's solved, as the rollback is not easy.","title":"Issues"},{"location":"newsletter/2021_03_29/#operative-systems","text":"","title":"Operative Systems"},{"location":"newsletter/2021_03_29/#linux","text":"","title":"Linux"},{"location":"newsletter/2021_03_29/#elasticsearch","text":"New: Explain how to reindex an index.","title":"elasticsearch"},{"location":"newsletter/2021_03_29/#mkdocs","text":"New: Explain additions of version 7.1.0 of the material theme. Dark-light mode switch . Back to top button .","title":"mkdocs"},{"location":"newsletter/2021_03_30/","text":"Introduction \u2691 Reorganization: Merge the Meta article into the index. Projects \u2691 Reorganization: Merge the wish_list article into the projects. New: Add seed to follow the updates of software. New: Add seed to automatically update the dockers of maintained services. Coding \u2691 Issues \u2691 Correction: Jellyfin login page problem after upgrade to 10.7.X is solved. Surprisingly the instructions in #5489 solved it. systemctl stop jellyfin.service mv /var/lib/jellyfin/data/jellyfin.db { ,.bak } systemctl start jellyfin.service [ Go to JF URL, get asked to log in even though there are no Users in the JF DB now ] systemctl stop jellyfin.service mv /var/lib/jellyfin/data/jellyfin.db { .bak, } systemctl start jellyfin.service Other \u2691 New: Introduce the media system and monitor interesting issues. Jellyfin is a Free Software Media System that puts you in control of managing and streaming your media. It is an alternative to the proprietary Emby and Plex, to provide media from a dedicated server to end-user devices via multiple apps. Jellyfin is descended from Emby's 3.5.2 release and ported to the .NET Core framework to enable full cross-platform support. There are no strings attached, no premium licenses or features, and no hidden agendas: just a team who want to build something better and work together to achieve it.","title":"30th March 2021"},{"location":"newsletter/2021_03_30/#introduction","text":"Reorganization: Merge the Meta article into the index.","title":"Introduction"},{"location":"newsletter/2021_03_30/#projects","text":"Reorganization: Merge the wish_list article into the projects. New: Add seed to follow the updates of software. New: Add seed to automatically update the dockers of maintained services.","title":"Projects"},{"location":"newsletter/2021_03_30/#coding","text":"","title":"Coding"},{"location":"newsletter/2021_03_30/#issues","text":"Correction: Jellyfin login page problem after upgrade to 10.7.X is solved. Surprisingly the instructions in #5489 solved it. systemctl stop jellyfin.service mv /var/lib/jellyfin/data/jellyfin.db { ,.bak } systemctl start jellyfin.service [ Go to JF URL, get asked to log in even though there are no Users in the JF DB now ] systemctl stop jellyfin.service mv /var/lib/jellyfin/data/jellyfin.db { .bak, } systemctl start jellyfin.service","title":"Issues"},{"location":"newsletter/2021_03_30/#other","text":"New: Introduce the media system and monitor interesting issues. Jellyfin is a Free Software Media System that puts you in control of managing and streaming your media. It is an alternative to the proprietary Emby and Plex, to provide media from a dedicated server to end-user devices via multiple apps. Jellyfin is descended from Emby's 3.5.2 release and ported to the .NET Core framework to enable full cross-platform support. There are no strings attached, no premium licenses or features, and no hidden agendas: just a team who want to build something better and work together to achieve it.","title":"Other"},{"location":"newsletter/2021_04/","text":"Projects \u2691 Improvement: Explain the updates on the repository-orm project. In the latest version 0.2.0 , we added: Support for the TinyDB repository . Support for regular expressions in the search method. Easier repository loading with load_repository function. Improvement: Add a link to the meilisearch blog . New: Create the quantified self project. With links to the two starting points HPI and bionic . New: Sketch how to automate repetitive tasks prompted by email events. Most of the emails I receive require repetitive actions that can be automated, I've stumbled upon notmuchmail , which looks very promising. A friend suggested to use afew for tagging, and I'd probably use alot to interact with the system (and finally be able to use email from the cli). Improvement: Add interesting interface. For the interface adri's memex looks awesome! It's inspired in the Andrew Louis talk Building a Memex whose blog posts seems to be a gold mine. Also look at hpi's compilation . New: Sketch how to improve the launching of applications with i3wm. In the past I tried installing rofi without success, I should try again. If the default features are not enough, check adi1090x's custom resources . Improvement: Show the changes of repository-orm 0.3.1. +* Add first and last methods to the repositories. +* Make entity id_ definition optional. +* add _model_name attribute to entities. Improvement: Add [woop awesome quantified self](https://github.com/woop/awesome-quantified-self) resources to the research list. New: Add project to migrate software bug tracker to a vendor free one like [git-bug](https://github.com/MichaelMure/git-bug). New: Improve the notification management in Linux. Create new seed project to be able to group and silence the notifications under a custom logic. For example: If I want to focus on a task, only show the most important ones. Only show alerts once every X minutes. Or define that I want to receive them the first 10 minutes of every hour. If I'm not working, silence all work alerts. New: Improve the hard drive monitor system. Create new seed project to use something like scrutiny (there's a linuxserver image ) to collect and display the information. For alerts, use one of their supported providers . New: Aggregate all notifications. Instead of reading the email, github, gitlab, discourse, reddit notifications, aggregate all in one place and show them to the user in a nice command line interface. For the aggregator server, my first choice would be gotify . New: Add faker-optional to the dormant plant projects. New: Add seedling project to create factoryboy factories from pydantic models automatically. DevOps \u2691 Continuous Integration \u2691 Flakehell \u2691 Correction: Update the git repository. The existent repository has been archived in favor of this one New: Explain how to patch the extended_default_ignore error for versions > 3.9.0. Add to your your pyproject.toml : [tool.flakehell] extended_default_ignore = [] # add this Coding \u2691 Python \u2691 Improvement: Add aiomultiprocess to the list of libraries to test. aiomultiprocess : Presents a simple interface, while running a full AsyncIO event loop on each child process, enabling levels of concurrency never before seen in a Python application. Each child process can execute multiple coroutines at once, limited only by the workload and number of cores available. New: Add interesting links on how to write good documentation. I would like to refactor divio's and Vue's guidelines and apply it to my projects. Improvement: Add FastAPI docs as a model to study and follow. New: Add apprise to the interesting libraries to explore. apprise : Allows you to send a notification to almost all of the most popular notification services available to us today such as: Linux, Telegram, Discord, Slack, Amazon SNS, Gotify, etc. Look at all the supported notifications (\u00ac\u00ba-\u00b0)\u00ac . New: Add kivi and kivimd to the interesting libraries to explore. kivi is used to create android/Linux/iOS/Windows applications with python. Use it with kivimd to make it beautiful, check the examples and the docs . Boto3 \u2691 New: Introduce the AWS SDK library and explain how to test it. Boto3 is the AWS SDK for Python to create, configure, and manage AWS services, such as Amazon Elastic Compute Cloud (Amazon EC2) and Amazon Simple Storage Service (Amazon S3). The SDK provides an object-oriented API as well as low-level access to AWS services. For testing , try to use moto , using the Botocore's stubber as fallback option. New: Explain how to test ec2, route53, s3, and rds resources. New: Explain how to test vpc and auto scaling group resources. Improvement: Explain how to extract the instance when testing autoscaling groups. Also track the issue to add support to launch templates . New: Explain how to test security groups. Correction: Add note that pagination is not yet supported when testing route53. I've opened an issue to solve it. Type Hints \u2691 Improvement: Explain how to ignore a linter error and a type error. With # type: ignore # noqa: W0212 Logging \u2691 New: Explain how to log python program exceptions better than to a file. Using logging to write write exceptions and breadcrumbs to a file might not be the best solution because unless you look at it directly most errors will pass unnoticed. To actively monitor and react to code exceptions use an application monitoring platform like sentry . In the article I explain what are the advantages of using this solution and do a comparison between Sentry and GlitchTip . DeepDiff \u2691 Improvement: Add warning that regular expressions are not yet supported. Until #239 is merged, the official library doesn't support searching for regular expressions. You can use my fork instead. Improvement: Remove advice to use my fork instead. The original one has already merged my PR \uff3c\\ \u0669( \u141b )\u0648 /\uff0f . Beware though as the regexp are not enabled by default (against my will). You need to use the use_regexp=True as an argument to grep or DeepSearch . FactoryBoy \u2691 New: Explain how to use Enum with factoryboy. Faker \u2691 New: Explain how to create Optional data. faker-optional is a custom faker provider that acts as a wrapper over other Faker providers to return their value or None . Useful to create data of type Optional[Any] . FastAPI \u2691 New: Add beets system as a first approach. When building Python applications, it's good to develop the core of your program, and allow extension via plugins. I still don't know how to do it, but Beets plugin system looks awesome for a first start. New: Introduce FastAPI the pydantic friendly python framework to build APIs. FastAPI is a modern, fast (high-performance), web framework for building APIs with Python 3.6+ based on standard Python type hints. New: Sum up the basic documentation. Explain how to: Sending data to the server : Through path parameters , query parameters and body requests . Handle errors . Update data Configure OpenAPI Test FastAPI applications And add a lot of more interesting features I've discovered. New: Explain how to log exceptions to sentry. New: Explain how to send raw data to the client. With the Response object. New: Explain how to configure the application. New: Explain how to inject a testing configuration in the tests. Pytest \u2691 New: Explain how to exclude code from the coverage report. Add # pragma: no cover . New: Explain how to run tests in parallel. pytest-xdist makes it possible to run the tests in parallel, useful when the test suit is large or when the tests are slow. pip install pytest-xdist pytest -n auto Python Snippets \u2691 New: Explain how to install dependencies from git repositories. With pip you can : pip install git+git://github.com/path/to/repository@master If you want to hard code it in your setup.py , you need to: install_requires = [ 'some-pkg @ git+ssh://git@github.com/someorgname/pkg-repo-name@v1.1#egg=some-pkg' , ] Correction: Explain how to create PyPI valid packages with direct dependencies. It looks like PyPI don't want pip to reach out to URLs outside their site when installing from PyPI. So you can't define the direct dependencies in the install_requires . Instead you need to install them in a PostInstall custom script. Ugly as hell. Correction: Add warning about the method to use direct dependencies. Last time I used this solution, when I added the library on a setup.py the direct dependencies weren't installed :S NetworkX \u2691 New: Introduce the python library. NetworkX is a Python package for the creation, manipulation, and study of the structure, dynamics, and functions of complex networks. Pydantic \u2691 Improvement: Change parse_obj definition to find how to import pydantic models from dictionary. New: Explain how to use private attributes. With the PrivateAttr object. New: Explain how to update entity attributes with a dictionary. You can create a new object with the new data using the update argument of the copy entity method. rich \u2691 New: Introduce the python cli builder library and it's progress bar. Rich is a Python library for rich text and beautiful formatting in the terminal. Check out the beautiful progress bar: pip install rich python -m rich.progress Ruamel YAML \u2691 Improvement: Suggest to use ruyaml instead of ruamel.yaml. As it's maintained by the community and versioned with git. sqlite3 \u2691 New: Explain how to implement the REGEXP operator with Python. TinyDB \u2691 New: Explain how to serialize datetime objects. SQLite \u2691 New: Explain how to configure sqlite to be able to use the REGEXP operator. It's not enabled by default. Operative Systems \u2691 Linux \u2691 Beets \u2691 Correction: Typo. There was a missing comma in the list. New: Introduce Beets the music management library. Beets is a music management library used to get your music collection right once and for all. It catalogs your collection, automatically improving its metadata as it goes using the MusicBrainz database. Then it provides a set of tools for manipulating and accessing your music. HAProxy \u2691 New: Add interesting guidelines on how to configure HAProxy in AWS . Hushboard \u2691 New: Introduce Husboard. Hushboard is an utility that mutes your microphone while you\u2019re typing. (Thanks M0wer !) ffmpeg \u2691 New: Introduce the program and multiple of it's uses. ffmpeg is a complete, cross-platform solution to record, convert and stream audio and video. Modipy \u2691 New: Introduce the music server. Mopidy is an extensible music server written in Python, that plays perfectly with beets and the MPD ecosystem. The awesome documentation, being Python based, the extension system, JSON-RPC, and JavaScript APIs make Mopidy a perfect base for your projects. Arts \u2691 Writing \u2691 New: Explain when to use I'm good or I'm well. Use I'm well when referring to being ill, use I'm good for the rest. Grammar and Orthography \u2691 New: Explain when to write won't or wont. Won't is the correct way to contract will not. Wont is a synonym of \"a habit\". For example, \"He went for a morning jog, as was his wont\". Other \u2691 Correction: Broken links. Removed the link to (everything_i_know.md) since it no longer exists. Updated some links that where broken due to a folder structure change. New: Explain how to select a random choice from Enum objects. pydantic uses Enum objects to define the choices of fields , so we need them to create the factories of those objects. New: Improve the periodic tasks and application metrics monitoring. Setup an healthchecks instance with the linuxserver image to monitor cronjobs. For the notifications either use the prometheus metrics or an apprise compatible system.","title":"April of 2021"},{"location":"newsletter/2021_04/#projects","text":"Improvement: Explain the updates on the repository-orm project. In the latest version 0.2.0 , we added: Support for the TinyDB repository . Support for regular expressions in the search method. Easier repository loading with load_repository function. Improvement: Add a link to the meilisearch blog . New: Create the quantified self project. With links to the two starting points HPI and bionic . New: Sketch how to automate repetitive tasks prompted by email events. Most of the emails I receive require repetitive actions that can be automated, I've stumbled upon notmuchmail , which looks very promising. A friend suggested to use afew for tagging, and I'd probably use alot to interact with the system (and finally be able to use email from the cli). Improvement: Add interesting interface. For the interface adri's memex looks awesome! It's inspired in the Andrew Louis talk Building a Memex whose blog posts seems to be a gold mine. Also look at hpi's compilation . New: Sketch how to improve the launching of applications with i3wm. In the past I tried installing rofi without success, I should try again. If the default features are not enough, check adi1090x's custom resources . Improvement: Show the changes of repository-orm 0.3.1. +* Add first and last methods to the repositories. +* Make entity id_ definition optional. +* add _model_name attribute to entities. Improvement: Add [woop awesome quantified self](https://github.com/woop/awesome-quantified-self) resources to the research list. New: Add project to migrate software bug tracker to a vendor free one like [git-bug](https://github.com/MichaelMure/git-bug). New: Improve the notification management in Linux. Create new seed project to be able to group and silence the notifications under a custom logic. For example: If I want to focus on a task, only show the most important ones. Only show alerts once every X minutes. Or define that I want to receive them the first 10 minutes of every hour. If I'm not working, silence all work alerts. New: Improve the hard drive monitor system. Create new seed project to use something like scrutiny (there's a linuxserver image ) to collect and display the information. For alerts, use one of their supported providers . New: Aggregate all notifications. Instead of reading the email, github, gitlab, discourse, reddit notifications, aggregate all in one place and show them to the user in a nice command line interface. For the aggregator server, my first choice would be gotify . New: Add faker-optional to the dormant plant projects. New: Add seedling project to create factoryboy factories from pydantic models automatically.","title":"Projects"},{"location":"newsletter/2021_04/#devops","text":"","title":"DevOps"},{"location":"newsletter/2021_04/#continuous-integration","text":"","title":"Continuous Integration"},{"location":"newsletter/2021_04/#flakehell","text":"Correction: Update the git repository. The existent repository has been archived in favor of this one New: Explain how to patch the extended_default_ignore error for versions > 3.9.0. Add to your your pyproject.toml : [tool.flakehell] extended_default_ignore = [] # add this","title":"Flakehell"},{"location":"newsletter/2021_04/#coding","text":"","title":"Coding"},{"location":"newsletter/2021_04/#python","text":"Improvement: Add aiomultiprocess to the list of libraries to test. aiomultiprocess : Presents a simple interface, while running a full AsyncIO event loop on each child process, enabling levels of concurrency never before seen in a Python application. Each child process can execute multiple coroutines at once, limited only by the workload and number of cores available. New: Add interesting links on how to write good documentation. I would like to refactor divio's and Vue's guidelines and apply it to my projects. Improvement: Add FastAPI docs as a model to study and follow. New: Add apprise to the interesting libraries to explore. apprise : Allows you to send a notification to almost all of the most popular notification services available to us today such as: Linux, Telegram, Discord, Slack, Amazon SNS, Gotify, etc. Look at all the supported notifications (\u00ac\u00ba-\u00b0)\u00ac . New: Add kivi and kivimd to the interesting libraries to explore. kivi is used to create android/Linux/iOS/Windows applications with python. Use it with kivimd to make it beautiful, check the examples and the docs .","title":"Python"},{"location":"newsletter/2021_04/#boto3","text":"New: Introduce the AWS SDK library and explain how to test it. Boto3 is the AWS SDK for Python to create, configure, and manage AWS services, such as Amazon Elastic Compute Cloud (Amazon EC2) and Amazon Simple Storage Service (Amazon S3). The SDK provides an object-oriented API as well as low-level access to AWS services. For testing , try to use moto , using the Botocore's stubber as fallback option. New: Explain how to test ec2, route53, s3, and rds resources. New: Explain how to test vpc and auto scaling group resources. Improvement: Explain how to extract the instance when testing autoscaling groups. Also track the issue to add support to launch templates . New: Explain how to test security groups. Correction: Add note that pagination is not yet supported when testing route53. I've opened an issue to solve it.","title":"Boto3"},{"location":"newsletter/2021_04/#type-hints","text":"Improvement: Explain how to ignore a linter error and a type error. With # type: ignore # noqa: W0212","title":"Type Hints"},{"location":"newsletter/2021_04/#logging","text":"New: Explain how to log python program exceptions better than to a file. Using logging to write write exceptions and breadcrumbs to a file might not be the best solution because unless you look at it directly most errors will pass unnoticed. To actively monitor and react to code exceptions use an application monitoring platform like sentry . In the article I explain what are the advantages of using this solution and do a comparison between Sentry and GlitchTip .","title":"Logging"},{"location":"newsletter/2021_04/#deepdiff","text":"Improvement: Add warning that regular expressions are not yet supported. Until #239 is merged, the official library doesn't support searching for regular expressions. You can use my fork instead. Improvement: Remove advice to use my fork instead. The original one has already merged my PR \uff3c\\ \u0669( \u141b )\u0648 /\uff0f . Beware though as the regexp are not enabled by default (against my will). You need to use the use_regexp=True as an argument to grep or DeepSearch .","title":"DeepDiff"},{"location":"newsletter/2021_04/#factoryboy","text":"New: Explain how to use Enum with factoryboy.","title":"FactoryBoy"},{"location":"newsletter/2021_04/#faker","text":"New: Explain how to create Optional data. faker-optional is a custom faker provider that acts as a wrapper over other Faker providers to return their value or None . Useful to create data of type Optional[Any] .","title":"Faker"},{"location":"newsletter/2021_04/#fastapi","text":"New: Add beets system as a first approach. When building Python applications, it's good to develop the core of your program, and allow extension via plugins. I still don't know how to do it, but Beets plugin system looks awesome for a first start. New: Introduce FastAPI the pydantic friendly python framework to build APIs. FastAPI is a modern, fast (high-performance), web framework for building APIs with Python 3.6+ based on standard Python type hints. New: Sum up the basic documentation. Explain how to: Sending data to the server : Through path parameters , query parameters and body requests . Handle errors . Update data Configure OpenAPI Test FastAPI applications And add a lot of more interesting features I've discovered. New: Explain how to log exceptions to sentry. New: Explain how to send raw data to the client. With the Response object. New: Explain how to configure the application. New: Explain how to inject a testing configuration in the tests.","title":"FastAPI"},{"location":"newsletter/2021_04/#pytest","text":"New: Explain how to exclude code from the coverage report. Add # pragma: no cover . New: Explain how to run tests in parallel. pytest-xdist makes it possible to run the tests in parallel, useful when the test suit is large or when the tests are slow. pip install pytest-xdist pytest -n auto","title":"Pytest"},{"location":"newsletter/2021_04/#python-snippets","text":"New: Explain how to install dependencies from git repositories. With pip you can : pip install git+git://github.com/path/to/repository@master If you want to hard code it in your setup.py , you need to: install_requires = [ 'some-pkg @ git+ssh://git@github.com/someorgname/pkg-repo-name@v1.1#egg=some-pkg' , ] Correction: Explain how to create PyPI valid packages with direct dependencies. It looks like PyPI don't want pip to reach out to URLs outside their site when installing from PyPI. So you can't define the direct dependencies in the install_requires . Instead you need to install them in a PostInstall custom script. Ugly as hell. Correction: Add warning about the method to use direct dependencies. Last time I used this solution, when I added the library on a setup.py the direct dependencies weren't installed :S","title":"Python Snippets"},{"location":"newsletter/2021_04/#networkx","text":"New: Introduce the python library. NetworkX is a Python package for the creation, manipulation, and study of the structure, dynamics, and functions of complex networks.","title":"NetworkX"},{"location":"newsletter/2021_04/#pydantic","text":"Improvement: Change parse_obj definition to find how to import pydantic models from dictionary. New: Explain how to use private attributes. With the PrivateAttr object. New: Explain how to update entity attributes with a dictionary. You can create a new object with the new data using the update argument of the copy entity method.","title":"Pydantic"},{"location":"newsletter/2021_04/#rich","text":"New: Introduce the python cli builder library and it's progress bar. Rich is a Python library for rich text and beautiful formatting in the terminal. Check out the beautiful progress bar: pip install rich python -m rich.progress","title":"rich"},{"location":"newsletter/2021_04/#ruamel-yaml","text":"Improvement: Suggest to use ruyaml instead of ruamel.yaml. As it's maintained by the community and versioned with git.","title":"Ruamel YAML"},{"location":"newsletter/2021_04/#sqlite3","text":"New: Explain how to implement the REGEXP operator with Python.","title":"sqlite3"},{"location":"newsletter/2021_04/#tinydb","text":"New: Explain how to serialize datetime objects.","title":"TinyDB"},{"location":"newsletter/2021_04/#sqlite","text":"New: Explain how to configure sqlite to be able to use the REGEXP operator. It's not enabled by default.","title":"SQLite"},{"location":"newsletter/2021_04/#operative-systems","text":"","title":"Operative Systems"},{"location":"newsletter/2021_04/#linux","text":"","title":"Linux"},{"location":"newsletter/2021_04/#beets","text":"Correction: Typo. There was a missing comma in the list. New: Introduce Beets the music management library. Beets is a music management library used to get your music collection right once and for all. It catalogs your collection, automatically improving its metadata as it goes using the MusicBrainz database. Then it provides a set of tools for manipulating and accessing your music.","title":"Beets"},{"location":"newsletter/2021_04/#haproxy","text":"New: Add interesting guidelines on how to configure HAProxy in AWS .","title":"HAProxy"},{"location":"newsletter/2021_04/#hushboard","text":"New: Introduce Husboard. Hushboard is an utility that mutes your microphone while you\u2019re typing. (Thanks M0wer !)","title":"Hushboard"},{"location":"newsletter/2021_04/#ffmpeg","text":"New: Introduce the program and multiple of it's uses. ffmpeg is a complete, cross-platform solution to record, convert and stream audio and video.","title":"ffmpeg"},{"location":"newsletter/2021_04/#modipy","text":"New: Introduce the music server. Mopidy is an extensible music server written in Python, that plays perfectly with beets and the MPD ecosystem. The awesome documentation, being Python based, the extension system, JSON-RPC, and JavaScript APIs make Mopidy a perfect base for your projects.","title":"Modipy"},{"location":"newsletter/2021_04/#arts","text":"","title":"Arts"},{"location":"newsletter/2021_04/#writing","text":"New: Explain when to use I'm good or I'm well. Use I'm well when referring to being ill, use I'm good for the rest.","title":"Writing"},{"location":"newsletter/2021_04/#grammar-and-orthography","text":"New: Explain when to write won't or wont. Won't is the correct way to contract will not. Wont is a synonym of \"a habit\". For example, \"He went for a morning jog, as was his wont\".","title":"Grammar and Orthography"},{"location":"newsletter/2021_04/#other","text":"Correction: Broken links. Removed the link to (everything_i_know.md) since it no longer exists. Updated some links that where broken due to a folder structure change. New: Explain how to select a random choice from Enum objects. pydantic uses Enum objects to define the choices of fields , so we need them to create the factories of those objects. New: Improve the periodic tasks and application metrics monitoring. Setup an healthchecks instance with the linuxserver image to monitor cronjobs. For the notifications either use the prometheus metrics or an apprise compatible system.","title":"Other"},{"location":"newsletter/2021_04_05/","text":"Coding \u2691 Python \u2691 Improvement: Add aiomultiprocess to the list of libraries to test. aiomultiprocess : Presents a simple interface, while running a full AsyncIO event loop on each child process, enabling levels of concurrency never before seen in a Python application. Each child process can execute multiple coroutines at once, limited only by the workload and number of cores available. Type Hints \u2691 Improvement: Explain how to ignore a linter error and a type error. With # type: ignore # noqa: W0212 Pydantic \u2691 Improvement: Change parse_obj definition to find how to import pydantic models from dictionary. TinyDB \u2691 New: Explain how to serialize datetime objects.","title":"5th April 2021"},{"location":"newsletter/2021_04_05/#coding","text":"","title":"Coding"},{"location":"newsletter/2021_04_05/#python","text":"Improvement: Add aiomultiprocess to the list of libraries to test. aiomultiprocess : Presents a simple interface, while running a full AsyncIO event loop on each child process, enabling levels of concurrency never before seen in a Python application. Each child process can execute multiple coroutines at once, limited only by the workload and number of cores available.","title":"Python"},{"location":"newsletter/2021_04_05/#type-hints","text":"Improvement: Explain how to ignore a linter error and a type error. With # type: ignore # noqa: W0212","title":"Type Hints"},{"location":"newsletter/2021_04_05/#pydantic","text":"Improvement: Change parse_obj definition to find how to import pydantic models from dictionary.","title":"Pydantic"},{"location":"newsletter/2021_04_05/#tinydb","text":"New: Explain how to serialize datetime objects.","title":"TinyDB"},{"location":"newsletter/2021_04_06/","text":"Operative Systems \u2691 Linux \u2691 ffmpeg \u2691 New: Introduce the program and multiple of it's uses. ffmpeg is a complete, cross-platform solution to record, convert and stream audio and video.","title":"6th April 2021"},{"location":"newsletter/2021_04_06/#operative-systems","text":"","title":"Operative Systems"},{"location":"newsletter/2021_04_06/#linux","text":"","title":"Linux"},{"location":"newsletter/2021_04_06/#ffmpeg","text":"New: Introduce the program and multiple of it's uses. ffmpeg is a complete, cross-platform solution to record, convert and stream audio and video.","title":"ffmpeg"},{"location":"newsletter/2021_04_07/","text":"Coding \u2691 Python \u2691 New: Add interesting links on how to write good documentation. I would like to refactor divio's and Vue's guidelines and apply it to my projects. DeepDiff \u2691 Improvement: Add warning that regular expressions are not yet supported. Until #239 is merged, the official library doesn't support searching for regular expressions. You can use my fork instead. Python Snippets \u2691 New: Explain how to install dependencies from git repositories. With pip you can : pip install git+git://github.com/path/to/repository@master If you want to hard code it in your setup.py , you need to: install_requires = [ 'some-pkg @ git+ssh://git@github.com/someorgname/pkg-repo-name@v1.1#egg=some-pkg' , ] sqlite3 \u2691 New: Explain how to implement the REGEXP operator with Python. SQLite \u2691 New: Explain how to configure sqlite to be able to use the REGEXP operator. It's not enabled by default.","title":"7th April 2021"},{"location":"newsletter/2021_04_07/#coding","text":"","title":"Coding"},{"location":"newsletter/2021_04_07/#python","text":"New: Add interesting links on how to write good documentation. I would like to refactor divio's and Vue's guidelines and apply it to my projects.","title":"Python"},{"location":"newsletter/2021_04_07/#deepdiff","text":"Improvement: Add warning that regular expressions are not yet supported. Until #239 is merged, the official library doesn't support searching for regular expressions. You can use my fork instead.","title":"DeepDiff"},{"location":"newsletter/2021_04_07/#python-snippets","text":"New: Explain how to install dependencies from git repositories. With pip you can : pip install git+git://github.com/path/to/repository@master If you want to hard code it in your setup.py , you need to: install_requires = [ 'some-pkg @ git+ssh://git@github.com/someorgname/pkg-repo-name@v1.1#egg=some-pkg' , ]","title":"Python Snippets"},{"location":"newsletter/2021_04_07/#sqlite3","text":"New: Explain how to implement the REGEXP operator with Python.","title":"sqlite3"},{"location":"newsletter/2021_04_07/#sqlite","text":"New: Explain how to configure sqlite to be able to use the REGEXP operator. It's not enabled by default.","title":"SQLite"},{"location":"newsletter/2021_04_08/","text":"Projects \u2691 Improvement: Explain the updates on the repository-orm project. In the latest version 0.2.0 , we added: Support for the TinyDB repository . Support for regular expressions in the search method. Easier repository loading with load_repository function. Improvement: Add a link to the meilisearch blog . New: Create the quantified self project. With links to the two starting points HPI and bionic . Coding \u2691 Python \u2691 Python Snippets \u2691 Correction: Explain how to create PyPI valid packages with direct dependencies. It looks like PyPI don't want pip to reach out to URLs outside their site when installing from PyPI. So you can't define the direct dependencies in the install_requires . Instead you need to install them in a PostInstall custom script. Ugly as hell. Operative Systems \u2691 Linux \u2691 HAProxy \u2691 New: Add interesting guidelines on how to configure HAProxy in AWS . Other \u2691 Correction: Broken links. Removed the link to (everything_i_know.md) since it no longer exists. Updated some links that where broken due to a folder structure change.","title":"8th April 2021"},{"location":"newsletter/2021_04_08/#projects","text":"Improvement: Explain the updates on the repository-orm project. In the latest version 0.2.0 , we added: Support for the TinyDB repository . Support for regular expressions in the search method. Easier repository loading with load_repository function. Improvement: Add a link to the meilisearch blog . New: Create the quantified self project. With links to the two starting points HPI and bionic .","title":"Projects"},{"location":"newsletter/2021_04_08/#coding","text":"","title":"Coding"},{"location":"newsletter/2021_04_08/#python","text":"","title":"Python"},{"location":"newsletter/2021_04_08/#python-snippets","text":"Correction: Explain how to create PyPI valid packages with direct dependencies. It looks like PyPI don't want pip to reach out to URLs outside their site when installing from PyPI. So you can't define the direct dependencies in the install_requires . Instead you need to install them in a PostInstall custom script. Ugly as hell.","title":"Python Snippets"},{"location":"newsletter/2021_04_08/#operative-systems","text":"","title":"Operative Systems"},{"location":"newsletter/2021_04_08/#linux","text":"","title":"Linux"},{"location":"newsletter/2021_04_08/#haproxy","text":"New: Add interesting guidelines on how to configure HAProxy in AWS .","title":"HAProxy"},{"location":"newsletter/2021_04_08/#other","text":"Correction: Broken links. Removed the link to (everything_i_know.md) since it no longer exists. Updated some links that where broken due to a folder structure change.","title":"Other"},{"location":"newsletter/2021_04_10/","text":"Operative Systems \u2691 Linux \u2691 Vim Plugins \u2691 Correction: Typo. There was a missing comma in the list.","title":"10th April 2021"},{"location":"newsletter/2021_04_10/#operative-systems","text":"","title":"Operative Systems"},{"location":"newsletter/2021_04_10/#linux","text":"","title":"Linux"},{"location":"newsletter/2021_04_10/#vim-plugins","text":"Correction: Typo. There was a missing comma in the list.","title":"Vim Plugins"},{"location":"newsletter/2021_04_13/","text":"Projects \u2691 New: Sketch how to automate repetitive tasks prompted by email events. Most of the emails I receive require repetitive actions that can be automated, I've stumbled upon notmuchmail , which looks very promising. A friend suggested to use afew for tagging, and I'd probably use alot to interact with the system (and finally be able to use email from the cli). Improvement: Add interesting interface. For the interface adri's memex looks awesome! It's inspired in the Andrew Louis talk Building a Memex whose blog posts seems to be a gold mine. Also look at hpi's compilation . New: Sketch how to improve the launching of applications with i3wm. In the past I tried installing rofi without success, I should try again. If the default features are not enough, check adi1090x's custom resources . Coding \u2691 Python \u2691 Plugin System \u2691 New: Add beets system as a first approach. When building Python applications, it's good to develop the core of your program, and allow extension via plugins. I still don't know how to do it, but Beets plugin system looks awesome for a first start. Operative Systems \u2691 Linux \u2691 Hushboard \u2691 New: Introduce Husboard. Hushboard is an utility that mutes your microphone while you\u2019re typing. (Thanks M0wer !)","title":"13th April 2021"},{"location":"newsletter/2021_04_13/#projects","text":"New: Sketch how to automate repetitive tasks prompted by email events. Most of the emails I receive require repetitive actions that can be automated, I've stumbled upon notmuchmail , which looks very promising. A friend suggested to use afew for tagging, and I'd probably use alot to interact with the system (and finally be able to use email from the cli). Improvement: Add interesting interface. For the interface adri's memex looks awesome! It's inspired in the Andrew Louis talk Building a Memex whose blog posts seems to be a gold mine. Also look at hpi's compilation . New: Sketch how to improve the launching of applications with i3wm. In the past I tried installing rofi without success, I should try again. If the default features are not enough, check adi1090x's custom resources .","title":"Projects"},{"location":"newsletter/2021_04_13/#coding","text":"","title":"Coding"},{"location":"newsletter/2021_04_13/#python","text":"","title":"Python"},{"location":"newsletter/2021_04_13/#plugin-system","text":"New: Add beets system as a first approach. When building Python applications, it's good to develop the core of your program, and allow extension via plugins. I still don't know how to do it, but Beets plugin system looks awesome for a first start.","title":"Plugin System"},{"location":"newsletter/2021_04_13/#operative-systems","text":"","title":"Operative Systems"},{"location":"newsletter/2021_04_13/#linux","text":"","title":"Linux"},{"location":"newsletter/2021_04_13/#hushboard","text":"New: Introduce Husboard. Hushboard is an utility that mutes your microphone while you\u2019re typing. (Thanks M0wer !)","title":"Hushboard"},{"location":"newsletter/2021_04_14/","text":"Coding \u2691 Python \u2691 Improvement: Add FastAPI docs as a model to study and follow. FastAPI \u2691 New: Introduce FastAPI the pydantic friendly python framework to build APIs. FastAPI is a modern, fast (high-performance), web framework for building APIs with Python 3.6+ based on standard Python type hints. Operative Systems \u2691 Linux \u2691 Beets \u2691 New: Introduce Beets the music management library. Beets is a music management library used to get your music collection right once and for all. It catalogs your collection, automatically improving its metadata as it goes using the MusicBrainz database. Then it provides a set of tools for manipulating and accessing your music. Modipy \u2691 New: Introduce the music server. Mopidy is an extensible music server written in Python, that plays perfectly with beets and the MPD ecosystem. The awesome documentation, being Python based, the extension system, JSON-RPC, and JavaScript APIs make Mopidy a perfect base for your projects. Arts \u2691 Writing \u2691 Grammar and Orthography \u2691 New: Explain when to write won't or wont. Won't is the correct way to contract will not. Wont is a synonym of \"a habit\". For example, \"He went for a morning jog, as was his wont\".","title":"14th April 2021"},{"location":"newsletter/2021_04_14/#coding","text":"","title":"Coding"},{"location":"newsletter/2021_04_14/#python","text":"Improvement: Add FastAPI docs as a model to study and follow.","title":"Python"},{"location":"newsletter/2021_04_14/#fastapi","text":"New: Introduce FastAPI the pydantic friendly python framework to build APIs. FastAPI is a modern, fast (high-performance), web framework for building APIs with Python 3.6+ based on standard Python type hints.","title":"FastAPI"},{"location":"newsletter/2021_04_14/#operative-systems","text":"","title":"Operative Systems"},{"location":"newsletter/2021_04_14/#linux","text":"","title":"Linux"},{"location":"newsletter/2021_04_14/#beets","text":"New: Introduce Beets the music management library. Beets is a music management library used to get your music collection right once and for all. It catalogs your collection, automatically improving its metadata as it goes using the MusicBrainz database. Then it provides a set of tools for manipulating and accessing your music.","title":"Beets"},{"location":"newsletter/2021_04_14/#modipy","text":"New: Introduce the music server. Mopidy is an extensible music server written in Python, that plays perfectly with beets and the MPD ecosystem. The awesome documentation, being Python based, the extension system, JSON-RPC, and JavaScript APIs make Mopidy a perfect base for your projects.","title":"Modipy"},{"location":"newsletter/2021_04_14/#arts","text":"","title":"Arts"},{"location":"newsletter/2021_04_14/#writing","text":"","title":"Writing"},{"location":"newsletter/2021_04_14/#grammar-and-orthography","text":"New: Explain when to write won't or wont. Won't is the correct way to contract will not. Wont is a synonym of \"a habit\". For example, \"He went for a morning jog, as was his wont\".","title":"Grammar and Orthography"},{"location":"newsletter/2021_04_15/","text":"Projects \u2691 Improvement: Show the changes of repository-orm 0.3.1. Add first and last methods to the repositories. Make entity id_ definition optional. add _model_name attribute to entities. Coding \u2691 Python \u2691 FastAPI \u2691 New: Sum up the basic documentation. Explain how to: Sending data to the server : Through path parameters , query parameters and body requests . Handle errors . Update data Configure OpenAPI Test FastAPI applications And add a lot of more interesting features I've discovered. Pytest \u2691 New: Explain how to exclude code from the coverage report. Add # pragma: no cover . NetworkX \u2691 New: Introduce the python library. NetworkX is a Python package for the creation, manipulation, and study of the structure, dynamics, and functions of complex networks. Pydantic \u2691 New: Explain how to use private attributes. With the PrivateAttr object.","title":"15th April 2021"},{"location":"newsletter/2021_04_15/#projects","text":"Improvement: Show the changes of repository-orm 0.3.1. Add first and last methods to the repositories. Make entity id_ definition optional. add _model_name attribute to entities.","title":"Projects"},{"location":"newsletter/2021_04_15/#coding","text":"","title":"Coding"},{"location":"newsletter/2021_04_15/#python","text":"","title":"Python"},{"location":"newsletter/2021_04_15/#fastapi","text":"New: Sum up the basic documentation. Explain how to: Sending data to the server : Through path parameters , query parameters and body requests . Handle errors . Update data Configure OpenAPI Test FastAPI applications And add a lot of more interesting features I've discovered.","title":"FastAPI"},{"location":"newsletter/2021_04_15/#pytest","text":"New: Explain how to exclude code from the coverage report. Add # pragma: no cover .","title":"Pytest"},{"location":"newsletter/2021_04_15/#networkx","text":"New: Introduce the python library. NetworkX is a Python package for the creation, manipulation, and study of the structure, dynamics, and functions of complex networks.","title":"NetworkX"},{"location":"newsletter/2021_04_15/#pydantic","text":"New: Explain how to use private attributes. With the PrivateAttr object.","title":"Pydantic"},{"location":"newsletter/2021_04_19/","text":"DevOps \u2691 Continuous Integration \u2691 Flakehell \u2691 Correction: Update the git repository. The existent repository has been archived in favor of this one Coding \u2691 Python \u2691 Boto3 \u2691 New: Introduce the AWS SDK library and explain how to test it. Boto3 is the AWS SDK for Python to create, configure, and manage AWS services, such as Amazon Elastic Compute Cloud (Amazon EC2) and Amazon Simple Storage Service (Amazon S3). The SDK provides an object-oriented API as well as low-level access to AWS services. For testing , try to use moto , using the Botocore's stubber as fallback option. DeepDiff \u2691 Improvement: Remove advice to use my fork instead. The original one has already merged my PR \uff3c\\ \u0669( \u141b )\u0648 /\uff0f . Beware though as the regexp are not enabled by default (against my will). You need to use the use_regexp=True as an argument to grep or DeepSearch . FactoryBoy \u2691 New: Explain how to use Enum with factoryboy. Python Snippets \u2691 Correction: Add warning about the method to use direct dependencies. Last time I used this solution, when I added the library on a setup.py the direct dependencies weren't installed :S Pydantic \u2691 New: Explain how to update entity attributes with a dictionary. You can create a new object with the new data using the update argument of the copy entity method. Arts \u2691 Writing \u2691 New: Explain when to use I'm good or I'm well. Use I'm well when referring to being ill, use I'm good for the rest. Other \u2691 New: Explain how to select a random choice from Enum objects. pydantic uses Enum objects to define the choices of fields , so we need them to create the factories of those objects.","title":"19th April 2021"},{"location":"newsletter/2021_04_19/#devops","text":"","title":"DevOps"},{"location":"newsletter/2021_04_19/#continuous-integration","text":"","title":"Continuous Integration"},{"location":"newsletter/2021_04_19/#flakehell","text":"Correction: Update the git repository. The existent repository has been archived in favor of this one","title":"Flakehell"},{"location":"newsletter/2021_04_19/#coding","text":"","title":"Coding"},{"location":"newsletter/2021_04_19/#python","text":"","title":"Python"},{"location":"newsletter/2021_04_19/#boto3","text":"New: Introduce the AWS SDK library and explain how to test it. Boto3 is the AWS SDK for Python to create, configure, and manage AWS services, such as Amazon Elastic Compute Cloud (Amazon EC2) and Amazon Simple Storage Service (Amazon S3). The SDK provides an object-oriented API as well as low-level access to AWS services. For testing , try to use moto , using the Botocore's stubber as fallback option.","title":"Boto3"},{"location":"newsletter/2021_04_19/#deepdiff","text":"Improvement: Remove advice to use my fork instead. The original one has already merged my PR \uff3c\\ \u0669( \u141b )\u0648 /\uff0f . Beware though as the regexp are not enabled by default (against my will). You need to use the use_regexp=True as an argument to grep or DeepSearch .","title":"DeepDiff"},{"location":"newsletter/2021_04_19/#factoryboy","text":"New: Explain how to use Enum with factoryboy.","title":"FactoryBoy"},{"location":"newsletter/2021_04_19/#python-snippets","text":"Correction: Add warning about the method to use direct dependencies. Last time I used this solution, when I added the library on a setup.py the direct dependencies weren't installed :S","title":"Python Snippets"},{"location":"newsletter/2021_04_19/#pydantic","text":"New: Explain how to update entity attributes with a dictionary. You can create a new object with the new data using the update argument of the copy entity method.","title":"Pydantic"},{"location":"newsletter/2021_04_19/#arts","text":"","title":"Arts"},{"location":"newsletter/2021_04_19/#writing","text":"New: Explain when to use I'm good or I'm well. Use I'm well when referring to being ill, use I'm good for the rest.","title":"Writing"},{"location":"newsletter/2021_04_19/#other","text":"New: Explain how to select a random choice from Enum objects. pydantic uses Enum objects to define the choices of fields , so we need them to create the factories of those objects.","title":"Other"},{"location":"newsletter/2021_04_20/","text":"Coding \u2691 Python \u2691 Boto3 \u2691 New: Explain how to test ec2, route53, s3, and rds resources. New: Explain how to test vpc and auto scaling group resources. rich \u2691 New: Introduce the python cli builder library and it's progress bar. Rich is a Python library for rich text and beautiful formatting in the terminal. Check out the beautiful progress bar: pip install rich python -m rich.progress","title":"20th April 2021"},{"location":"newsletter/2021_04_20/#coding","text":"","title":"Coding"},{"location":"newsletter/2021_04_20/#python","text":"","title":"Python"},{"location":"newsletter/2021_04_20/#boto3","text":"New: Explain how to test ec2, route53, s3, and rds resources. New: Explain how to test vpc and auto scaling group resources.","title":"Boto3"},{"location":"newsletter/2021_04_20/#rich","text":"New: Introduce the python cli builder library and it's progress bar. Rich is a Python library for rich text and beautiful formatting in the terminal. Check out the beautiful progress bar: pip install rich python -m rich.progress","title":"rich"},{"location":"newsletter/2021_04_23/","text":"Projects \u2691 Improvement: Add [woop awesome quantified self](https://github.com/woop/awesome-quantified-self) resources to the research list. New: Add project to migrate software bug tracker to a vendor free one like [git-bug](https://github.com/MichaelMure/git-bug). DevOps \u2691 Continuous Integration \u2691 Flakehell \u2691 New: Explain how to patch the extended_default_ignore error for versions > 3.9.0. Add to your your pyproject.toml : [tool.flakehell] extended_default_ignore = [] # add this Coding \u2691 Python \u2691 Boto3 \u2691 Improvement: Explain how to extract the instance when testing autoscaling groups. Also track the issue to add support to launch templates . New: Explain how to test security groups. Logging \u2691 New: Explain how to log python program exceptions better than to a file. Using logging to write write exceptions and breadcrumbs to a file might not be the best solution because unless you look at it directly most errors will pass unnoticed. To actively monitor and react to code exceptions use an application monitoring platform like sentry . In the article I explain what are the advantages of using this solution and do a comparison between Sentry and GlitchTip . FastAPI \u2691 New: Explain how to log exceptions to sentry. Ruamel YAML \u2691 Improvement: Suggest to use ruyaml instead of ruamel.yaml. As it's maintained by the community and versioned with git.","title":"23rd April 2021"},{"location":"newsletter/2021_04_23/#projects","text":"Improvement: Add [woop awesome quantified self](https://github.com/woop/awesome-quantified-self) resources to the research list. New: Add project to migrate software bug tracker to a vendor free one like [git-bug](https://github.com/MichaelMure/git-bug).","title":"Projects"},{"location":"newsletter/2021_04_23/#devops","text":"","title":"DevOps"},{"location":"newsletter/2021_04_23/#continuous-integration","text":"","title":"Continuous Integration"},{"location":"newsletter/2021_04_23/#flakehell","text":"New: Explain how to patch the extended_default_ignore error for versions > 3.9.0. Add to your your pyproject.toml : [tool.flakehell] extended_default_ignore = [] # add this","title":"Flakehell"},{"location":"newsletter/2021_04_23/#coding","text":"","title":"Coding"},{"location":"newsletter/2021_04_23/#python","text":"","title":"Python"},{"location":"newsletter/2021_04_23/#boto3","text":"Improvement: Explain how to extract the instance when testing autoscaling groups. Also track the issue to add support to launch templates . New: Explain how to test security groups.","title":"Boto3"},{"location":"newsletter/2021_04_23/#logging","text":"New: Explain how to log python program exceptions better than to a file. Using logging to write write exceptions and breadcrumbs to a file might not be the best solution because unless you look at it directly most errors will pass unnoticed. To actively monitor and react to code exceptions use an application monitoring platform like sentry . In the article I explain what are the advantages of using this solution and do a comparison between Sentry and GlitchTip .","title":"Logging"},{"location":"newsletter/2021_04_23/#fastapi","text":"New: Explain how to log exceptions to sentry.","title":"FastAPI"},{"location":"newsletter/2021_04_23/#ruamel-yaml","text":"Improvement: Suggest to use ruyaml instead of ruamel.yaml. As it's maintained by the community and versioned with git.","title":"Ruamel YAML"},{"location":"newsletter/2021_04_24/","text":"Projects \u2691 New: Improve the notification management in Linux. Create new seed project to be able to group and silence the notifications under a custom logic. For example: If I want to focus on a task, only show the most important ones. Only show alerts once every X minutes. Or define that I want to receive them the first 10 minutes of every hour. If I'm not working, silence all work alerts. New: Improve the hard drive monitor system. Create new seed project to use something like scrutiny (there's a linuxserver image ) to collect and display the information. For alerts, use one of their supported providers . New: Aggregate all notifications. Instead of reading the email, github, gitlab, discourse, reddit notifications, aggregate all in one place and show them to the user in a nice command line interface. For the aggregator server, my first choice would be gotify . Coding \u2691 Python \u2691 New: Add apprise to the interesting libraries to explore. apprise : Allows you to send a notification to almost all of the most popular notification services available to us today such as: Linux, Telegram, Discord, Slack, Amazon SNS, Gotify, etc. Look at all the supported notifications (\u00ac\u00ba-\u00b0)\u00ac . FastAPI \u2691 New: Explain how to send raw data to the client. With the Response object. Other \u2691 New: Improve the periodic tasks and application metrics monitoring. Setup an healthchecks instance with the linuxserver image to monitor cronjobs. For the notifications either use the prometheus metrics or an apprise compatible system.","title":"24th April 2021"},{"location":"newsletter/2021_04_24/#projects","text":"New: Improve the notification management in Linux. Create new seed project to be able to group and silence the notifications under a custom logic. For example: If I want to focus on a task, only show the most important ones. Only show alerts once every X minutes. Or define that I want to receive them the first 10 minutes of every hour. If I'm not working, silence all work alerts. New: Improve the hard drive monitor system. Create new seed project to use something like scrutiny (there's a linuxserver image ) to collect and display the information. For alerts, use one of their supported providers . New: Aggregate all notifications. Instead of reading the email, github, gitlab, discourse, reddit notifications, aggregate all in one place and show them to the user in a nice command line interface. For the aggregator server, my first choice would be gotify .","title":"Projects"},{"location":"newsletter/2021_04_24/#coding","text":"","title":"Coding"},{"location":"newsletter/2021_04_24/#python","text":"New: Add apprise to the interesting libraries to explore. apprise : Allows you to send a notification to almost all of the most popular notification services available to us today such as: Linux, Telegram, Discord, Slack, Amazon SNS, Gotify, etc. Look at all the supported notifications (\u00ac\u00ba-\u00b0)\u00ac .","title":"Python"},{"location":"newsletter/2021_04_24/#fastapi","text":"New: Explain how to send raw data to the client. With the Response object.","title":"FastAPI"},{"location":"newsletter/2021_04_24/#other","text":"New: Improve the periodic tasks and application metrics monitoring. Setup an healthchecks instance with the linuxserver image to monitor cronjobs. For the notifications either use the prometheus metrics or an apprise compatible system.","title":"Other"},{"location":"newsletter/2021_04_25/","text":"Projects \u2691 New: Add faker-optional to the dormant plant projects. New: Add seedling project to create factoryboy factories from pydantic models automatically. Coding \u2691 Python \u2691 New: Add kivi and kivimd to the interesting libraries to explore. kivi is used to create android/Linux/iOS/Windows applications with python. Use it with kivimd to make it beautiful, check the examples and the docs . Faker \u2691 New: Explain how to create Optional data. faker-optional is a custom faker provider that acts as a wrapper over other Faker providers to return their value or None . Useful to create data of type Optional[Any] . FastAPI \u2691 New: Explain how to configure the application. New: Explain how to inject a testing configuration in the tests.","title":"25th April 2021"},{"location":"newsletter/2021_04_25/#projects","text":"New: Add faker-optional to the dormant plant projects. New: Add seedling project to create factoryboy factories from pydantic models automatically.","title":"Projects"},{"location":"newsletter/2021_04_25/#coding","text":"","title":"Coding"},{"location":"newsletter/2021_04_25/#python","text":"New: Add kivi and kivimd to the interesting libraries to explore. kivi is used to create android/Linux/iOS/Windows applications with python. Use it with kivimd to make it beautiful, check the examples and the docs .","title":"Python"},{"location":"newsletter/2021_04_25/#faker","text":"New: Explain how to create Optional data. faker-optional is a custom faker provider that acts as a wrapper over other Faker providers to return their value or None . Useful to create data of type Optional[Any] .","title":"Faker"},{"location":"newsletter/2021_04_25/#fastapi","text":"New: Explain how to configure the application. New: Explain how to inject a testing configuration in the tests.","title":"FastAPI"},{"location":"newsletter/2021_04_26/","text":"Coding \u2691 Python \u2691 Boto3 \u2691 Correction: Add note that pagination is not yet supported when testing route53. I've opened an issue to solve it. Pytest \u2691 New: Explain how to run tests in parallel. pytest-xdist makes it possible to run the tests in parallel, useful when the test suit is large or when the tests are slow. pip install pytest-xdist pytest -n auto","title":"26th April 2021"},{"location":"newsletter/2021_04_26/#coding","text":"","title":"Coding"},{"location":"newsletter/2021_04_26/#python","text":"","title":"Python"},{"location":"newsletter/2021_04_26/#boto3","text":"Correction: Add note that pagination is not yet supported when testing route53. I've opened an issue to solve it.","title":"Boto3"},{"location":"newsletter/2021_04_26/#pytest","text":"New: Explain how to run tests in parallel. pytest-xdist makes it possible to run the tests in parallel, useful when the test suit is large or when the tests are slow. pip install pytest-xdist pytest -n auto","title":"Pytest"},{"location":"newsletter/2021_05/","text":"Projects \u2691 New: Explain the idea of how to improve the record of ideas, tasks,. New: Add git-bug as an interesting distributed issue tracker. New: Add the Improve the reliability of the Open Science collections project. The current free knowledge efforts : are based on the health of a collection of torrents. This project aims to create a command line tool or service that makes it easier to automate the seeding of ill torrents. New: Add the Monitor and notify on disk prices project. Diskprices.com sorts the prices of the disks on the different amazon sites based on many filters. It will be interesting to have a service that monitors the data on this site and alerts the user once there is a deal that matches its criteria. Reorganization: Move the automation of computer file management project to the projects page. Reorganization: Move the dying projects below the seeds as they are less important. Coding \u2691 New: Introduce Vue.js. Vue.js is a progressive framework for building user interfaces. Python \u2691 New: Add parso library to interesting libraries to explore. parso is a library to parse Python code. Boto3 \u2691 New: Explain how to test IAM users and groups. Improvement: Monitor motor issue with the cn-north-1 rds and autoscaling endpoints. Configure Docker to host the application \u2691 Improvement: Explain how to define the type hints of functions and methods that use subclasses. It's a complex topic that has taken me many months to get it right :). New: Explain how to write type hints for generator functions. New: Explain how to log in using pass. pass show dockerhub | docker login --username foo --password-stdin New: Explain how not to store the credentials in plaintext. It doesn't work, don't go this painful road and assume that docker is broken. The official steps are horrible, and once you've spent two hours debugging them, you won't be able to push or pull images with your user . FastAPI \u2691 New: Explain how to deploy it using Docker. New: Explain how to show logging messages in the logs. Writing good documentation \u2691 New: Start explaining how to write good documentation for a software project. It doesn't matter how good your program is, because if its documentation is not good enough, people will not use it. People working with software need different kinds of documentation at different times, in different circumstances, so good software documentation needs them all. In this first iteration, I define the five kinds of documentation, and give the ideas to write good introduction and get started sections. DevOps \u2691 Infrastructure Solutions \u2691 Jobs \u2691 Improvement: Remove false positive alerts on failed jobs that succeeded. A Kubernetes cronjob spawns jobs, if the first one fails, it will try to spawn a new one. If the second succeeds, the cronjob status should be success, but with the rule we had before, a successful job with failed past jobs will still raise an alert. Life Management \u2691 Reorganization: Split the life automation article into life management and process automation. I understand life management as the act of analyzing yourself and your interactions with the world to define processes and automations that shape the way to efficiently achieve your goals. I understand process automation as the act of analyzing yourself and your interactions with the world to find the way to reduce the time or willpower spent on your life processes. Time Management \u2691 New: Introduce the time management concept. Time management is the process of planning and exercising conscious control of time spent on specific activities, especially to increase effectiveness, efficiency, and productivity. It involves a juggling act of various demands upon a person relating to work, social life, family, hobbies, personal interests, and commitments with the finiteness of time. Using time effectively gives the person \"choice\" on spending or managing activities at their own time and expediency. New: Start analyzing the ways to reduce the time spent doing unproductive tasks. By minimizing the context switches and managing the interruptions . Improvement: Explain how to improve your efficiency by better using your everyday tools. Improvement: Add two more ways to avoid loosing time in unproductive tasks. Avoid lost time doing nothing . Fix your environment . New: Explain how to manage meetings efficiently. New: Explain how to improve efficiency by taking care of yourself. New: Explain how to prevent blocks by efficiently switching mental processes. Task Management \u2691 New: Introduce the task management concept. Task management is the process of managing a task through its life cycle. It involves planning, testing, tracking, and reporting. Task management can help either individual achieve goals, or groups of individuals collaborate and share knowledge for the accomplishment of collective goals. Improvement: Introduce the main task management tools. The inbox does not refer only to your e-mail inbox. It is a broader concept that includes all the elements you have collected in different ways: tasks you have to do, ideas you have thought of, notes, bills, business cards, etc\u2026 The task manager tool to make your interaction with the tasks easier. You can start with the simplest task manager , a markdown file in your computer with a list of tasks to do. Annotate only the actionable tasks that you need to do today, otherwise it can quickly grow to be unmanageable. Improvement: Add the benefits when you do task management well, and the side effects if you do it wrong. Improvement: Add a small guide on how to introduce yourself into task management. Task Management Workflows \u2691 New: Introduce the main task management workflows. Task management can be done at different levels. All of them help you in different ways to reduce the mental load, each also gives you extra benefits that can't be gained by the others. Going from lowest to highest abstraction level we have: Pomodoro. Task plan. Day plan. Week plan. Month plan. Semester plan. In the commit I've detailed the Pomodoro technique and the task , day and week plans. New: Explain the difference of surfing the hype flow versus following a defined plan. Interruption Management \u2691 New: Introduce the interruption management concept. Interruption management is the life management area that gathers the processes to minimize the time and willpower toll consumed by interruptions. The article proposes a way to analyze your existent interruptions and define how can you improve your interaction with them. I've applied it both to my work and personal interruptions. Improvement: Explain what to do once you have the interruption analysis. Work Interruption Analysis \u2691 Improvement: Add analysis of instant message interruptions. Personal Interruption Analysis \u2691 Improvement: Add analysis of instant message interruptions. Money Management \u2691 Reorganization: Move the accounting automation to money management. Tool Management \u2691 New: Introduce the tool management section. Most of the tasks or processes we do involve some kind of tool, the better you know how to use them, the better your efficiency will be. The more you use a tool, the more it's worth the investment of time to improve your usage of it. Whenever I use a tool, I try to think if I could configure it or use it in a way that will make it easier or quicker. Don't go crazy and try to change everything. Go step by step, and once you've internalized the improvement, implement the next. Email Management \u2691 New: Explain how I configure and interact with email efficiently. Instant Messages Management \u2691 New: Explain how I configure and interact with chat applications efficiently. Process Automation \u2691 Improvement: Add xkcd comics that gather the essence and pitfalls of process automation. Activism \u2691 Free Knowledge \u2691 New: Introduce how to contribute to the free knowledge initiative. One of the early principles of the internet has been to make knowledge free to everyone. Alexandra Elbakyan of Sci-Hub , bookwarrior of Library Genesis , Aaron Swartz , and countless unnamed others have fought to free science from the grips of for-profit publishers. Today, they do it working in hiding, alone, without acknowledgment, in fear of imprisonment, and even now wiretapped by the FBI. They sacrifice everything for one vision: Open Science. If you want to know how to contribute, check the article . Operative Systems \u2691 Linux \u2691 Linux Snippets \u2691 New: Explain how to check if an rsync command has gone well. Run diff -r --brief source/ dest/ , and check that there is no output. Vim Plugins \u2691 Improvement: Explain how to configure the vim-easymotion movement keys. Gajim \u2691 New: Introduce gajim. Gajim is the best Linux XMPP client in terms of end-to-end encryption support as it's able to speak OMEMO. Jellyfin \u2691 Correction: Explain how to fix the stuck at login page issue. systemctl stop jellyfin.service mv /var/lib/jellyfin/data/jellyfin.db { ,.bak } systemctl start jellyfin.service systemctl stop jellyfin.service mv /var/lib/jellyfin/data/jellyfin.db { .bak, } systemctl start jellyfin.service Correction: Explain how to fix the Intel Hardware transcoding. docker exec -it jellyfin /bin/bash wget https://repo.jellyfin.org/releases/server/ubuntu/versions/jellyfin-ffmpeg/4.3.2-1/jellyfin-ffmpeg_4.3.2-1-focal_amd64.deb dpkg -i jellyfin-ffmpeg_4.3.2-1-focal_amd64.deb Vim \u2691 Reorganization: Refactor the vim_automation article into vim and vim_plugins. Arts \u2691 Writing \u2691 Forking this garden \u2691 New: Explain how to fork the blue book. Other \u2691 Reorganization: Reorder the sections of the site navigation menu. Give more importance to Coding, Activism and Life Management, reducing the Software Architecture and Data Analysis sections. Reorganization: Move the tasks tools from the task management article to their own.","title":"May of 2021"},{"location":"newsletter/2021_05/#projects","text":"New: Explain the idea of how to improve the record of ideas, tasks,. New: Add git-bug as an interesting distributed issue tracker. New: Add the Improve the reliability of the Open Science collections project. The current free knowledge efforts : are based on the health of a collection of torrents. This project aims to create a command line tool or service that makes it easier to automate the seeding of ill torrents. New: Add the Monitor and notify on disk prices project. Diskprices.com sorts the prices of the disks on the different amazon sites based on many filters. It will be interesting to have a service that monitors the data on this site and alerts the user once there is a deal that matches its criteria. Reorganization: Move the automation of computer file management project to the projects page. Reorganization: Move the dying projects below the seeds as they are less important.","title":"Projects"},{"location":"newsletter/2021_05/#coding","text":"New: Introduce Vue.js. Vue.js is a progressive framework for building user interfaces.","title":"Coding"},{"location":"newsletter/2021_05/#python","text":"New: Add parso library to interesting libraries to explore. parso is a library to parse Python code.","title":"Python"},{"location":"newsletter/2021_05/#boto3","text":"New: Explain how to test IAM users and groups. Improvement: Monitor motor issue with the cn-north-1 rds and autoscaling endpoints.","title":"Boto3"},{"location":"newsletter/2021_05/#configure-docker-to-host-the-application","text":"Improvement: Explain how to define the type hints of functions and methods that use subclasses. It's a complex topic that has taken me many months to get it right :). New: Explain how to write type hints for generator functions. New: Explain how to log in using pass. pass show dockerhub | docker login --username foo --password-stdin New: Explain how not to store the credentials in plaintext. It doesn't work, don't go this painful road and assume that docker is broken. The official steps are horrible, and once you've spent two hours debugging them, you won't be able to push or pull images with your user .","title":"Configure Docker to host the application"},{"location":"newsletter/2021_05/#fastapi","text":"New: Explain how to deploy it using Docker. New: Explain how to show logging messages in the logs.","title":"FastAPI"},{"location":"newsletter/2021_05/#writing-good-documentation","text":"New: Start explaining how to write good documentation for a software project. It doesn't matter how good your program is, because if its documentation is not good enough, people will not use it. People working with software need different kinds of documentation at different times, in different circumstances, so good software documentation needs them all. In this first iteration, I define the five kinds of documentation, and give the ideas to write good introduction and get started sections.","title":"Writing good documentation"},{"location":"newsletter/2021_05/#devops","text":"","title":"DevOps"},{"location":"newsletter/2021_05/#infrastructure-solutions","text":"","title":"Infrastructure Solutions"},{"location":"newsletter/2021_05/#jobs","text":"Improvement: Remove false positive alerts on failed jobs that succeeded. A Kubernetes cronjob spawns jobs, if the first one fails, it will try to spawn a new one. If the second succeeds, the cronjob status should be success, but with the rule we had before, a successful job with failed past jobs will still raise an alert.","title":"Jobs"},{"location":"newsletter/2021_05/#life-management","text":"Reorganization: Split the life automation article into life management and process automation. I understand life management as the act of analyzing yourself and your interactions with the world to define processes and automations that shape the way to efficiently achieve your goals. I understand process automation as the act of analyzing yourself and your interactions with the world to find the way to reduce the time or willpower spent on your life processes.","title":"Life Management"},{"location":"newsletter/2021_05/#time-management","text":"New: Introduce the time management concept. Time management is the process of planning and exercising conscious control of time spent on specific activities, especially to increase effectiveness, efficiency, and productivity. It involves a juggling act of various demands upon a person relating to work, social life, family, hobbies, personal interests, and commitments with the finiteness of time. Using time effectively gives the person \"choice\" on spending or managing activities at their own time and expediency. New: Start analyzing the ways to reduce the time spent doing unproductive tasks. By minimizing the context switches and managing the interruptions . Improvement: Explain how to improve your efficiency by better using your everyday tools. Improvement: Add two more ways to avoid loosing time in unproductive tasks. Avoid lost time doing nothing . Fix your environment . New: Explain how to manage meetings efficiently. New: Explain how to improve efficiency by taking care of yourself. New: Explain how to prevent blocks by efficiently switching mental processes.","title":"Time Management"},{"location":"newsletter/2021_05/#task-management","text":"New: Introduce the task management concept. Task management is the process of managing a task through its life cycle. It involves planning, testing, tracking, and reporting. Task management can help either individual achieve goals, or groups of individuals collaborate and share knowledge for the accomplishment of collective goals. Improvement: Introduce the main task management tools. The inbox does not refer only to your e-mail inbox. It is a broader concept that includes all the elements you have collected in different ways: tasks you have to do, ideas you have thought of, notes, bills, business cards, etc\u2026 The task manager tool to make your interaction with the tasks easier. You can start with the simplest task manager , a markdown file in your computer with a list of tasks to do. Annotate only the actionable tasks that you need to do today, otherwise it can quickly grow to be unmanageable. Improvement: Add the benefits when you do task management well, and the side effects if you do it wrong. Improvement: Add a small guide on how to introduce yourself into task management.","title":"Task Management"},{"location":"newsletter/2021_05/#task-management-workflows","text":"New: Introduce the main task management workflows. Task management can be done at different levels. All of them help you in different ways to reduce the mental load, each also gives you extra benefits that can't be gained by the others. Going from lowest to highest abstraction level we have: Pomodoro. Task plan. Day plan. Week plan. Month plan. Semester plan. In the commit I've detailed the Pomodoro technique and the task , day and week plans. New: Explain the difference of surfing the hype flow versus following a defined plan.","title":"Task Management Workflows"},{"location":"newsletter/2021_05/#interruption-management","text":"New: Introduce the interruption management concept. Interruption management is the life management area that gathers the processes to minimize the time and willpower toll consumed by interruptions. The article proposes a way to analyze your existent interruptions and define how can you improve your interaction with them. I've applied it both to my work and personal interruptions. Improvement: Explain what to do once you have the interruption analysis.","title":"Interruption Management"},{"location":"newsletter/2021_05/#work-interruption-analysis","text":"Improvement: Add analysis of instant message interruptions.","title":"Work Interruption Analysis"},{"location":"newsletter/2021_05/#personal-interruption-analysis","text":"Improvement: Add analysis of instant message interruptions.","title":"Personal Interruption Analysis"},{"location":"newsletter/2021_05/#money-management","text":"Reorganization: Move the accounting automation to money management.","title":"Money Management"},{"location":"newsletter/2021_05/#tool-management","text":"New: Introduce the tool management section. Most of the tasks or processes we do involve some kind of tool, the better you know how to use them, the better your efficiency will be. The more you use a tool, the more it's worth the investment of time to improve your usage of it. Whenever I use a tool, I try to think if I could configure it or use it in a way that will make it easier or quicker. Don't go crazy and try to change everything. Go step by step, and once you've internalized the improvement, implement the next.","title":"Tool Management"},{"location":"newsletter/2021_05/#email-management","text":"New: Explain how I configure and interact with email efficiently.","title":"Email Management"},{"location":"newsletter/2021_05/#instant-messages-management","text":"New: Explain how I configure and interact with chat applications efficiently.","title":"Instant Messages Management"},{"location":"newsletter/2021_05/#process-automation","text":"Improvement: Add xkcd comics that gather the essence and pitfalls of process automation.","title":"Process Automation"},{"location":"newsletter/2021_05/#activism","text":"","title":"Activism"},{"location":"newsletter/2021_05/#free-knowledge","text":"New: Introduce how to contribute to the free knowledge initiative. One of the early principles of the internet has been to make knowledge free to everyone. Alexandra Elbakyan of Sci-Hub , bookwarrior of Library Genesis , Aaron Swartz , and countless unnamed others have fought to free science from the grips of for-profit publishers. Today, they do it working in hiding, alone, without acknowledgment, in fear of imprisonment, and even now wiretapped by the FBI. They sacrifice everything for one vision: Open Science. If you want to know how to contribute, check the article .","title":"Free Knowledge"},{"location":"newsletter/2021_05/#operative-systems","text":"","title":"Operative Systems"},{"location":"newsletter/2021_05/#linux","text":"","title":"Linux"},{"location":"newsletter/2021_05/#linux-snippets","text":"New: Explain how to check if an rsync command has gone well. Run diff -r --brief source/ dest/ , and check that there is no output.","title":"Linux Snippets"},{"location":"newsletter/2021_05/#vim-plugins","text":"Improvement: Explain how to configure the vim-easymotion movement keys.","title":"Vim Plugins"},{"location":"newsletter/2021_05/#gajim","text":"New: Introduce gajim. Gajim is the best Linux XMPP client in terms of end-to-end encryption support as it's able to speak OMEMO.","title":"Gajim"},{"location":"newsletter/2021_05/#jellyfin","text":"Correction: Explain how to fix the stuck at login page issue. systemctl stop jellyfin.service mv /var/lib/jellyfin/data/jellyfin.db { ,.bak } systemctl start jellyfin.service systemctl stop jellyfin.service mv /var/lib/jellyfin/data/jellyfin.db { .bak, } systemctl start jellyfin.service Correction: Explain how to fix the Intel Hardware transcoding. docker exec -it jellyfin /bin/bash wget https://repo.jellyfin.org/releases/server/ubuntu/versions/jellyfin-ffmpeg/4.3.2-1/jellyfin-ffmpeg_4.3.2-1-focal_amd64.deb dpkg -i jellyfin-ffmpeg_4.3.2-1-focal_amd64.deb","title":"Jellyfin"},{"location":"newsletter/2021_05/#vim","text":"Reorganization: Refactor the vim_automation article into vim and vim_plugins.","title":"Vim"},{"location":"newsletter/2021_05/#arts","text":"","title":"Arts"},{"location":"newsletter/2021_05/#writing","text":"","title":"Writing"},{"location":"newsletter/2021_05/#forking-this-garden","text":"New: Explain how to fork the blue book.","title":"Forking this garden"},{"location":"newsletter/2021_05/#other","text":"Reorganization: Reorder the sections of the site navigation menu. Give more importance to Coding, Activism and Life Management, reducing the Software Architecture and Data Analysis sections. Reorganization: Move the tasks tools from the task management article to their own.","title":"Other"},{"location":"newsletter/2021_05_01/","text":"Coding \u2691 Python \u2691 New: Add parso library to interesting libraries to explore. parso is a library to parse Python code. Boto3 \u2691 New: Explain how to test IAM users and groups. Type Hints \u2691 Improvement: Explain how to define the type hints of functions and methods that use subclasses. It's a complex topic that has taken me many months to get it right :). Writing good documentation \u2691 New: Start explaining how to write good documentation for a software project. It doesn't matter how good your program is, because if its documentation is not good enough, people will not use it. People working with software need different kinds of documentation at different times, in different circumstances, so good software documentation needs them all. In this first iteration, I define the five kinds of documentation, and give the ideas to write good introduction and get started sections.","title":"1st May 2021"},{"location":"newsletter/2021_05_01/#coding","text":"","title":"Coding"},{"location":"newsletter/2021_05_01/#python","text":"New: Add parso library to interesting libraries to explore. parso is a library to parse Python code.","title":"Python"},{"location":"newsletter/2021_05_01/#boto3","text":"New: Explain how to test IAM users and groups.","title":"Boto3"},{"location":"newsletter/2021_05_01/#type-hints","text":"Improvement: Explain how to define the type hints of functions and methods that use subclasses. It's a complex topic that has taken me many months to get it right :).","title":"Type Hints"},{"location":"newsletter/2021_05_01/#writing-good-documentation","text":"New: Start explaining how to write good documentation for a software project. It doesn't matter how good your program is, because if its documentation is not good enough, people will not use it. People working with software need different kinds of documentation at different times, in different circumstances, so good software documentation needs them all. In this first iteration, I define the five kinds of documentation, and give the ideas to write good introduction and get started sections.","title":"Writing good documentation"},{"location":"newsletter/2021_05_11/","text":"Projects \u2691 New: Explain the idea of how to improve the record of ideas, tasks,. Coding \u2691 New: Introduce Vue.js. Vue.js is a progressive framework for building user interfaces. Python \u2691 Boto3 \u2691 Improvement: Monitor motor issue with the cn-north-1 rds and autoscaling endpoints. Type Hints \u2691 New: Explain how to write type hints for generator functions. Operative Systems \u2691 Linux \u2691 Linux Snippets \u2691 New: Explain how to check if an rsync command has gone well. Run diff -r --brief source/ dest/ , and check that there is no output.","title":"11th May 2021"},{"location":"newsletter/2021_05_11/#projects","text":"New: Explain the idea of how to improve the record of ideas, tasks,.","title":"Projects"},{"location":"newsletter/2021_05_11/#coding","text":"New: Introduce Vue.js. Vue.js is a progressive framework for building user interfaces.","title":"Coding"},{"location":"newsletter/2021_05_11/#python","text":"","title":"Python"},{"location":"newsletter/2021_05_11/#boto3","text":"Improvement: Monitor motor issue with the cn-north-1 rds and autoscaling endpoints.","title":"Boto3"},{"location":"newsletter/2021_05_11/#type-hints","text":"New: Explain how to write type hints for generator functions.","title":"Type Hints"},{"location":"newsletter/2021_05_11/#operative-systems","text":"","title":"Operative Systems"},{"location":"newsletter/2021_05_11/#linux","text":"","title":"Linux"},{"location":"newsletter/2021_05_11/#linux-snippets","text":"New: Explain how to check if an rsync command has gone well. Run diff -r --brief source/ dest/ , and check that there is no output.","title":"Linux Snippets"},{"location":"newsletter/2021_05_14/","text":"Coding \u2691 Python \u2691 Configure Docker to host the application \u2691 New: Explain how to log in using pass. pass show dockerhub | docker login --username foo --password-stdin New: Explain how not to store the credentials in plaintext. It doesn't work, don't go this painful road and assume that docker is broken. The official steps are horrible, and once you've spent two hours debugging them, you won't be able to push or pull images with your user . FastAPI \u2691 New: Explain how to deploy it using Docker. New: Explain how to show logging messages in the logs.","title":"14th May 2021"},{"location":"newsletter/2021_05_14/#coding","text":"","title":"Coding"},{"location":"newsletter/2021_05_14/#python","text":"","title":"Python"},{"location":"newsletter/2021_05_14/#configure-docker-to-host-the-application","text":"New: Explain how to log in using pass. pass show dockerhub | docker login --username foo --password-stdin New: Explain how not to store the credentials in plaintext. It doesn't work, don't go this painful road and assume that docker is broken. The official steps are horrible, and once you've spent two hours debugging them, you won't be able to push or pull images with your user .","title":"Configure Docker to host the application"},{"location":"newsletter/2021_05_14/#fastapi","text":"New: Explain how to deploy it using Docker. New: Explain how to show logging messages in the logs.","title":"FastAPI"},{"location":"newsletter/2021_05_17/","text":"DevOps \u2691 Infrastructure Solutions \u2691 Jobs \u2691 Improvement: Remove false positive alerts on failed jobs that succeeded. A Kubernetes cronjob spawns jobs, if the first one fails, it will try to spawn a new one. If the second succeeds, the cronjob status should be success, but with the rule we had before, a successful job with failed past jobs will still raise an alert.","title":"17th May 2021"},{"location":"newsletter/2021_05_17/#devops","text":"","title":"DevOps"},{"location":"newsletter/2021_05_17/#infrastructure-solutions","text":"","title":"Infrastructure Solutions"},{"location":"newsletter/2021_05_17/#jobs","text":"Improvement: Remove false positive alerts on failed jobs that succeeded. A Kubernetes cronjob spawns jobs, if the first one fails, it will try to spawn a new one. If the second succeeds, the cronjob status should be success, but with the rule we had before, a successful job with failed past jobs will still raise an alert.","title":"Jobs"},{"location":"newsletter/2021_05_20/","text":"Operative Systems \u2691 Linux \u2691 Gajim \u2691 New: Introduce gajim. Gajim is the best Linux XMPP client in terms of end-to-end encryption support as it's able to speak OMEMO. Jellyfin \u2691 Correction: Explain how to fix the stuck at login page issue. systemctl stop jellyfin.service mv /var/lib/jellyfin/data/jellyfin.db { ,.bak } systemctl start jellyfin.service systemctl stop jellyfin.service mv /var/lib/jellyfin/data/jellyfin.db { .bak, } systemctl start jellyfin.service Correction: Explain how to fix the Intel Hardware transcoding. docker exec -it jellyfin /bin/bash wget https://repo.jellyfin.org/releases/server/ubuntu/versions/jellyfin-ffmpeg/4.3.2-1/jellyfin-ffmpeg_4.3.2-1-focal_amd64.deb dpkg -i jellyfin-ffmpeg_4.3.2-1-focal_amd64.deb","title":"20th May 2021"},{"location":"newsletter/2021_05_20/#operative-systems","text":"","title":"Operative Systems"},{"location":"newsletter/2021_05_20/#linux","text":"","title":"Linux"},{"location":"newsletter/2021_05_20/#gajim","text":"New: Introduce gajim. Gajim is the best Linux XMPP client in terms of end-to-end encryption support as it's able to speak OMEMO.","title":"Gajim"},{"location":"newsletter/2021_05_20/#jellyfin","text":"Correction: Explain how to fix the stuck at login page issue. systemctl stop jellyfin.service mv /var/lib/jellyfin/data/jellyfin.db { ,.bak } systemctl start jellyfin.service systemctl stop jellyfin.service mv /var/lib/jellyfin/data/jellyfin.db { .bak, } systemctl start jellyfin.service Correction: Explain how to fix the Intel Hardware transcoding. docker exec -it jellyfin /bin/bash wget https://repo.jellyfin.org/releases/server/ubuntu/versions/jellyfin-ffmpeg/4.3.2-1/jellyfin-ffmpeg_4.3.2-1-focal_amd64.deb dpkg -i jellyfin-ffmpeg_4.3.2-1-focal_amd64.deb","title":"Jellyfin"},{"location":"newsletter/2021_05_24/","text":"Projects \u2691 New: Add git-bug as an interesting distributed issue tracker. New: Add the Improve the reliability of the Open Science collections project. The current free knowledge efforts : are based on the health of a collection of torrents. This project aims to create a command line tool or service that makes it easier to automate the seeding of ill torrents. New: Add the Monitor and notify on disk prices project. Diskprices.com sorts the prices of the disks on the different amazon sites based on many filters. It will be interesting to have a service that monitors the data on this site and alerts the user once there is a deal that matches its criteria. Reorganization: Move the automation of computer file management project to the projects page. Life Management \u2691 Reorganization: Split the life automation article into life management and process automation. I understand life management as the act of analyzing yourself and your interactions with the world to define processes and automations that shape the way to efficiently achieve your goals. I understand process automation as the act of analyzing yourself and your interactions with the world to find the way to reduce the time or willpower spent on your life processes. Time Management \u2691 New: Introduce the time management concept. Time management is the process of planning and exercising conscious control of time spent on specific activities, especially to increase effectiveness, efficiency, and productivity. It involves a juggling act of various demands upon a person relating to work, social life, family, hobbies, personal interests, and commitments with the finiteness of time. Using time effectively gives the person \"choice\" on spending or managing activities at their own time and expediency. New: Start analyzing the ways to reduce the time spent doing unproductive tasks. By minimizing the context switches and managing the interruptions . Interruption Management \u2691 New: Introduce the interruption management concept. Interruption management is the life management area that gathers the processes to minimize the time and willpower toll consumed by interruptions. The article proposes a way to analyze your existent interruptions and define how can you improve your interaction with them. I've applied it both to my work and personal interruptions. Process Automation \u2691 Improvement: Add xkcd comics that gather the essence and pitfalls of process automation. Activism \u2691 Free Knowledge \u2691 New: Introduce how to contribute to the free knowledge initiative. One of the early principles of the internet has been to make knowledge free to everyone. Alexandra Elbakyan of Sci-Hub , bookwarrior of Library Genesis , Aaron Swartz , and countless unnamed others have fought to free science from the grips of for-profit publishers. Today, they do it working in hiding, alone, without acknowledgment, in fear of imprisonment, and even now wiretapped by the FBI. They sacrifice everything for one vision: Open Science. If you want to know how to contribute, check the article . Other \u2691 Reorganization: Reorder the sections of the site navigation menu. Give more importance to Coding, Activism and Life Management, reducing the Software Architecture and Data Analysis sections.","title":"24th May 2021"},{"location":"newsletter/2021_05_24/#projects","text":"New: Add git-bug as an interesting distributed issue tracker. New: Add the Improve the reliability of the Open Science collections project. The current free knowledge efforts : are based on the health of a collection of torrents. This project aims to create a command line tool or service that makes it easier to automate the seeding of ill torrents. New: Add the Monitor and notify on disk prices project. Diskprices.com sorts the prices of the disks on the different amazon sites based on many filters. It will be interesting to have a service that monitors the data on this site and alerts the user once there is a deal that matches its criteria. Reorganization: Move the automation of computer file management project to the projects page.","title":"Projects"},{"location":"newsletter/2021_05_24/#life-management","text":"Reorganization: Split the life automation article into life management and process automation. I understand life management as the act of analyzing yourself and your interactions with the world to define processes and automations that shape the way to efficiently achieve your goals. I understand process automation as the act of analyzing yourself and your interactions with the world to find the way to reduce the time or willpower spent on your life processes.","title":"Life Management"},{"location":"newsletter/2021_05_24/#time-management","text":"New: Introduce the time management concept. Time management is the process of planning and exercising conscious control of time spent on specific activities, especially to increase effectiveness, efficiency, and productivity. It involves a juggling act of various demands upon a person relating to work, social life, family, hobbies, personal interests, and commitments with the finiteness of time. Using time effectively gives the person \"choice\" on spending or managing activities at their own time and expediency. New: Start analyzing the ways to reduce the time spent doing unproductive tasks. By minimizing the context switches and managing the interruptions .","title":"Time Management"},{"location":"newsletter/2021_05_24/#interruption-management","text":"New: Introduce the interruption management concept. Interruption management is the life management area that gathers the processes to minimize the time and willpower toll consumed by interruptions. The article proposes a way to analyze your existent interruptions and define how can you improve your interaction with them. I've applied it both to my work and personal interruptions.","title":"Interruption Management"},{"location":"newsletter/2021_05_24/#process-automation","text":"Improvement: Add xkcd comics that gather the essence and pitfalls of process automation.","title":"Process Automation"},{"location":"newsletter/2021_05_24/#activism","text":"","title":"Activism"},{"location":"newsletter/2021_05_24/#free-knowledge","text":"New: Introduce how to contribute to the free knowledge initiative. One of the early principles of the internet has been to make knowledge free to everyone. Alexandra Elbakyan of Sci-Hub , bookwarrior of Library Genesis , Aaron Swartz , and countless unnamed others have fought to free science from the grips of for-profit publishers. Today, they do it working in hiding, alone, without acknowledgment, in fear of imprisonment, and even now wiretapped by the FBI. They sacrifice everything for one vision: Open Science. If you want to know how to contribute, check the article .","title":"Free Knowledge"},{"location":"newsletter/2021_05_24/#other","text":"Reorganization: Reorder the sections of the site navigation menu. Give more importance to Coding, Activism and Life Management, reducing the Software Architecture and Data Analysis sections.","title":"Other"},{"location":"newsletter/2021_05_25/","text":"Life Management \u2691 Time Management \u2691 Improvement: Explain how to improve your efficiency by better using your everyday tools. Task Management \u2691 New: Introduce the task management concept. Task management is the process of managing a task through its life cycle. It involves planning, testing, tracking, and reporting. Task management can help either individual achieve goals, or groups of individuals collaborate and share knowledge for the accomplishment of collective goals. Improvement: Introduce the main task management tools. The inbox does not refer only to your e-mail inbox. It is a broader concept that includes all the elements you have collected in different ways: tasks you have to do, ideas you have thought of, notes, bills, business cards, etc\u2026 The task manager tool to make your interaction with the tasks easier. You can start with the simplest task manager , a markdown file in your computer with a list of tasks to do. Annotate only the actionable tasks that you need to do today, otherwise it can quickly grow to be unmanageable. Interruption Management \u2691 Improvement: Explain what to do once you have the interruption analysis. Work Interruption Analysis \u2691 Improvement: Add analysis of instant message interruptions. Personal Interruption Analysis \u2691 Improvement: Add analysis of instant message interruptions. Money Management \u2691 Reorganization: Move the accounting automation to money management. Tool Management \u2691 New: Introduce the tool management section. Most of the tasks or processes we do involve some kind of tool, the better you know how to use them, the better your efficiency will be. The more you use a tool, the more it's worth the investment of time to improve your usage of it. Whenever I use a tool, I try to think if I could configure it or use it in a way that will make it easier or quicker. Don't go crazy and try to change everything. Go step by step, and once you've internalized the improvement, implement the next. Email Management \u2691 New: Explain how I configure and interact with email efficiently. Instant Messages Management \u2691 New: Explain how I configure and interact with chat applications efficiently. Operative Systems \u2691 Linux \u2691 Vim Plugins \u2691 Improvement: Explain how to configure the vim-easymotion movement keys. Vim \u2691 Reorganization: Refactor the vim_automation article into vim and vim_plugins. Arts \u2691 Writing \u2691 Forking this garden \u2691 New: Explain how to fork the blue book.","title":"25th May 2021"},{"location":"newsletter/2021_05_25/#life-management","text":"","title":"Life Management"},{"location":"newsletter/2021_05_25/#time-management","text":"Improvement: Explain how to improve your efficiency by better using your everyday tools.","title":"Time Management"},{"location":"newsletter/2021_05_25/#task-management","text":"New: Introduce the task management concept. Task management is the process of managing a task through its life cycle. It involves planning, testing, tracking, and reporting. Task management can help either individual achieve goals, or groups of individuals collaborate and share knowledge for the accomplishment of collective goals. Improvement: Introduce the main task management tools. The inbox does not refer only to your e-mail inbox. It is a broader concept that includes all the elements you have collected in different ways: tasks you have to do, ideas you have thought of, notes, bills, business cards, etc\u2026 The task manager tool to make your interaction with the tasks easier. You can start with the simplest task manager , a markdown file in your computer with a list of tasks to do. Annotate only the actionable tasks that you need to do today, otherwise it can quickly grow to be unmanageable.","title":"Task Management"},{"location":"newsletter/2021_05_25/#interruption-management","text":"Improvement: Explain what to do once you have the interruption analysis.","title":"Interruption Management"},{"location":"newsletter/2021_05_25/#work-interruption-analysis","text":"Improvement: Add analysis of instant message interruptions.","title":"Work Interruption Analysis"},{"location":"newsletter/2021_05_25/#personal-interruption-analysis","text":"Improvement: Add analysis of instant message interruptions.","title":"Personal Interruption Analysis"},{"location":"newsletter/2021_05_25/#money-management","text":"Reorganization: Move the accounting automation to money management.","title":"Money Management"},{"location":"newsletter/2021_05_25/#tool-management","text":"New: Introduce the tool management section. Most of the tasks or processes we do involve some kind of tool, the better you know how to use them, the better your efficiency will be. The more you use a tool, the more it's worth the investment of time to improve your usage of it. Whenever I use a tool, I try to think if I could configure it or use it in a way that will make it easier or quicker. Don't go crazy and try to change everything. Go step by step, and once you've internalized the improvement, implement the next.","title":"Tool Management"},{"location":"newsletter/2021_05_25/#email-management","text":"New: Explain how I configure and interact with email efficiently.","title":"Email Management"},{"location":"newsletter/2021_05_25/#instant-messages-management","text":"New: Explain how I configure and interact with chat applications efficiently.","title":"Instant Messages Management"},{"location":"newsletter/2021_05_25/#operative-systems","text":"","title":"Operative Systems"},{"location":"newsletter/2021_05_25/#linux","text":"","title":"Linux"},{"location":"newsletter/2021_05_25/#vim-plugins","text":"Improvement: Explain how to configure the vim-easymotion movement keys.","title":"Vim Plugins"},{"location":"newsletter/2021_05_25/#vim","text":"Reorganization: Refactor the vim_automation article into vim and vim_plugins.","title":"Vim"},{"location":"newsletter/2021_05_25/#arts","text":"","title":"Arts"},{"location":"newsletter/2021_05_25/#writing","text":"","title":"Writing"},{"location":"newsletter/2021_05_25/#forking-this-garden","text":"New: Explain how to fork the blue book.","title":"Forking this garden"},{"location":"newsletter/2021_05_26/","text":"Projects \u2691 Reorganization: Move the dying projects below the seeds as they are less important. Life Management \u2691 Time Management \u2691 Improvement: Add two more ways to avoid loosing time in unproductive tasks. Avoid lost time doing nothing . Fix your environment . Task Management \u2691 Improvement: Add the benefits when you do task management well, and the side effects if you do it wrong. Improvement: Add a small guide on how to introduce yourself into task management. Task Management Workflows \u2691 New: Introduce the main task management workflows. Task management can be done at different levels. All of them help you in different ways to reduce the mental load, each also gives you extra benefits that can't be gained by the others. Going from lowest to highest abstraction level we have: Pomodoro. Task plan. Day plan. Week plan. Month plan. Semester plan. In the commit I've detailed the Pomodoro technique and the task , day and week plans. Other \u2691 Reorganization: Move the tasks tools from the task management article to their own.","title":"26th May 2021"},{"location":"newsletter/2021_05_26/#projects","text":"Reorganization: Move the dying projects below the seeds as they are less important.","title":"Projects"},{"location":"newsletter/2021_05_26/#life-management","text":"","title":"Life Management"},{"location":"newsletter/2021_05_26/#time-management","text":"Improvement: Add two more ways to avoid loosing time in unproductive tasks. Avoid lost time doing nothing . Fix your environment .","title":"Time Management"},{"location":"newsletter/2021_05_26/#task-management","text":"Improvement: Add the benefits when you do task management well, and the side effects if you do it wrong. Improvement: Add a small guide on how to introduce yourself into task management.","title":"Task Management"},{"location":"newsletter/2021_05_26/#task-management-workflows","text":"New: Introduce the main task management workflows. Task management can be done at different levels. All of them help you in different ways to reduce the mental load, each also gives you extra benefits that can't be gained by the others. Going from lowest to highest abstraction level we have: Pomodoro. Task plan. Day plan. Week plan. Month plan. Semester plan. In the commit I've detailed the Pomodoro technique and the task , day and week plans.","title":"Task Management Workflows"},{"location":"newsletter/2021_05_26/#other","text":"Reorganization: Move the tasks tools from the task management article to their own.","title":"Other"},{"location":"newsletter/2021_05_27/","text":"Life Management \u2691 Time Management \u2691 New: Explain how to manage meetings efficiently. New: Explain how to improve efficiency by taking care of yourself. New: Explain how to prevent blocks by efficiently switching mental processes. Task Management \u2691 Task Management Workflows \u2691 New: Explain the difference of surfing the hype flow versus following a defined plan.","title":"27th May 2021"},{"location":"newsletter/2021_05_27/#life-management","text":"","title":"Life Management"},{"location":"newsletter/2021_05_27/#time-management","text":"New: Explain how to manage meetings efficiently. New: Explain how to improve efficiency by taking care of yourself. New: Explain how to prevent blocks by efficiently switching mental processes.","title":"Time Management"},{"location":"newsletter/2021_05_27/#task-management","text":"","title":"Task Management"},{"location":"newsletter/2021_05_27/#task-management-workflows","text":"New: Explain the difference of surfing the hype flow versus following a defined plan.","title":"Task Management Workflows"},{"location":"newsletter/2021_06/","text":"Projects \u2691 New: Introduce seedling self-hosted map project. I love maps, as well as traveling and hiking. This project aims to create a web interface that let's me interact with the data gathered throughout my life. I'd like to: Browse the waypoints and routes that I've done. Create routes and export the gpx. Be able to search through the data Plan trips New: Introduce the seed project to. New: Add the Life seedling project. Life is a real time sandbox role game where you play as yourself surviving in today's world. New: Add bruty to the dormant plant projects. bruty is a Python program to bruteforce dynamic web applications with Selenium. Coding \u2691 Python \u2691 Improvement: Add textual as interesting library to explore. textual : Textual is a TUI (Text User Interface) framework for Python using Rich as a renderer. Click \u2691 Improvement: Explain how to change the command line help description. Configure Docker to host the application \u2691 New: Explain how to use watchtower to keep docker containers updated. With watchtower you can update the running version of your containerized app simply by pushing a new image to the Docker Hub or your own image registry. Watchtower will pull down your new image, gracefully shut down your existing container and restart it with the same options that were used when it was deployed initially. Correction: Explain how to run the watchtower checks immediately. With the --run-once flag FastAPI \u2691 New: Explain how to make redirections with fastapi. New: Explain how to run a FastAPI server in the background for testing purposes. Pytest \u2691 New: Explain how to set a timeout for your tests. Using pytest-timeout . New: Explain how to rerun tests that fail sometimes. With pytest-rerunfailures that is a plugin for pytest that re-runs tests to eliminate intermittent failures. Using this plugin is generally a bad idea, it would be best to solve the reason why your code is not reliable. It's useful when you rely on non robust third party software in a way that you can't solve, or if the error is not in your code but in the testing code, and again you are not able to fix it. feat(python_snippets#Create combination of elements in groups of two): Explain how to create combination of elements in groups of two >>> list ( itertools . combinations ( 'ABC' , 2 )) [( 'A' , 'B' ), ( 'A' , 'C' ), ( 'B' , 'C' )] Python Snippets \u2691 New: Explain how to convert html code to readable plaintext. pip install html2text import html2text html = open ( \"foobar.html\" ) . read () print ( html2text . html2text ( html )) New: Explain how to parse a datetime from a string. from dateutil import parser parser . parse ( \"Aug 28 1999 12:00AM\" ) # datetime.datetime(1999, 8, 28, 0, 0) Python Mysql \u2691 New: Explain how to interact with MySQL databases with Python. Correction: Correct the syntax of the left joins. Instead of using ON users.id == addresses.user_id , use ON users.id = addresses.user_id Selenium \u2691 New: Explain how to use selenium with python. New: Explain how to Set timeout of a response. driver . set_page_load_timeout ( 30 ) New: Explain how to fix when Chromedriver hangs up unexpectedly. os . environ [ \"DBUS_SESSION_BUS_ADDRESS\" ] = \"/dev/null\" DevOps \u2691 Infrastructure Solutions \u2691 Jobs \u2691 New: Explain how to rerun failed cronjobs. If you have a job that has failed after the 6 default retries, it will show up in your monitorization forever, to fix it, you can manually trigger the job. kubectl get job \"your-job\" -o json \\ | jq 'del(.spec.selector)' \\ | jq 'del(.spec.template.metadata.labels)' \\ | kubectl replace --force -f - Continuous Integration \u2691 New: Explain how to troubleshoot the error: pathspec master did not match any file. Remove all git hooks with rm -r .git/hooks . Monitoring \u2691 Elasticsearch Exporter \u2691 New: Introduce the prometheus elasticsearch exporter. The elasticsearch exporter allows monitoring Elasticsearch clusters with Prometheus . Explain how to install it, configure the grafana dashboards and the alerts. Improvement: Add more elasticsearch alerts. Measure the search latency, search rate and create alerts on the garbage collector, json parser and circuit breaker errors New: Add alert on low number of healthy master nodes. Software Architecture \u2691 Domain Driven Design \u2691 Improvement: Add warning when migrating old code. You may be tempted to migrate all your old code to this architecture once you fall in love with it. Truth being told, it's the best way to learn how to use it, but it's time expensive too! The last refactor I did required a change of 60% of the code. The upside is that I reduced the total lines of code a 25%. Operative Systems \u2691 Linux \u2691 Linux Snippets \u2691 New: Explain how to split a file into many with equal number of lines. split -l 200000 filename New: Explain how to identify what a string or file contains. Using pywhat dunst \u2691 New: Introduce dunst. Dunst is a lightweight replacement for the notification daemons provided by most desktop environments. It\u2019s very customizable, isn\u2019t dependent on any toolkits, and therefore fits into those window manager centric setups we all love to customize to perfection. elasticsearch \u2691 Correction: Explain how to restore only some indices. curl -X POST \"{{ url }}/_snapshot/{{ backup_path }}/{{ snapshot_name }}/_restore?pretty\" -H 'Content-Type: application/json' -d ' { \"indices\": \"{{ index_to_restore }}\", }' New: Explain how to fix Circuit breakers triggers. Arts \u2691 Writing \u2691 Grammar and Orthography \u2691 New: Explain what collocations are and how to avoid the word very. Collocation refers to a natural combination of words that are closely affiliated with each other. They make it easier to avoid overused or ambiguous words like \"very\", \"nice\", or \"beautiful\", by using a pair of words that fit the context better and that have a more precise meaning. New: Explain what can you use instead of I know. Using \"I know\" may not be the best way to show the other person that you've got the information. You can take the chance to use other words that additionally gives more context on how you stand with the information you've received, thus improving the communication and creating a bond. Cooking \u2691 New: Introduce the cooking art. Cooking Basics \u2691 New: Refactor the perfect technique to boil an egg. New: Explain how to boil chickpeas when you've forgotten to soak them. Add a level teaspoon of baking soda to the pot and cook them as usual","title":"June of 2021"},{"location":"newsletter/2021_06/#projects","text":"New: Introduce seedling self-hosted map project. I love maps, as well as traveling and hiking. This project aims to create a web interface that let's me interact with the data gathered throughout my life. I'd like to: Browse the waypoints and routes that I've done. Create routes and export the gpx. Be able to search through the data Plan trips New: Introduce the seed project to. New: Add the Life seedling project. Life is a real time sandbox role game where you play as yourself surviving in today's world. New: Add bruty to the dormant plant projects. bruty is a Python program to bruteforce dynamic web applications with Selenium.","title":"Projects"},{"location":"newsletter/2021_06/#coding","text":"","title":"Coding"},{"location":"newsletter/2021_06/#python","text":"Improvement: Add textual as interesting library to explore. textual : Textual is a TUI (Text User Interface) framework for Python using Rich as a renderer.","title":"Python"},{"location":"newsletter/2021_06/#click","text":"Improvement: Explain how to change the command line help description.","title":"Click"},{"location":"newsletter/2021_06/#configure-docker-to-host-the-application","text":"New: Explain how to use watchtower to keep docker containers updated. With watchtower you can update the running version of your containerized app simply by pushing a new image to the Docker Hub or your own image registry. Watchtower will pull down your new image, gracefully shut down your existing container and restart it with the same options that were used when it was deployed initially. Correction: Explain how to run the watchtower checks immediately. With the --run-once flag","title":"Configure Docker to host the application"},{"location":"newsletter/2021_06/#fastapi","text":"New: Explain how to make redirections with fastapi. New: Explain how to run a FastAPI server in the background for testing purposes.","title":"FastAPI"},{"location":"newsletter/2021_06/#pytest","text":"New: Explain how to set a timeout for your tests. Using pytest-timeout . New: Explain how to rerun tests that fail sometimes. With pytest-rerunfailures that is a plugin for pytest that re-runs tests to eliminate intermittent failures. Using this plugin is generally a bad idea, it would be best to solve the reason why your code is not reliable. It's useful when you rely on non robust third party software in a way that you can't solve, or if the error is not in your code but in the testing code, and again you are not able to fix it. feat(python_snippets#Create combination of elements in groups of two): Explain how to create combination of elements in groups of two >>> list ( itertools . combinations ( 'ABC' , 2 )) [( 'A' , 'B' ), ( 'A' , 'C' ), ( 'B' , 'C' )]","title":"Pytest"},{"location":"newsletter/2021_06/#python-snippets","text":"New: Explain how to convert html code to readable plaintext. pip install html2text import html2text html = open ( \"foobar.html\" ) . read () print ( html2text . html2text ( html )) New: Explain how to parse a datetime from a string. from dateutil import parser parser . parse ( \"Aug 28 1999 12:00AM\" ) # datetime.datetime(1999, 8, 28, 0, 0)","title":"Python Snippets"},{"location":"newsletter/2021_06/#python-mysql","text":"New: Explain how to interact with MySQL databases with Python. Correction: Correct the syntax of the left joins. Instead of using ON users.id == addresses.user_id , use ON users.id = addresses.user_id","title":"Python Mysql"},{"location":"newsletter/2021_06/#selenium","text":"New: Explain how to use selenium with python. New: Explain how to Set timeout of a response. driver . set_page_load_timeout ( 30 ) New: Explain how to fix when Chromedriver hangs up unexpectedly. os . environ [ \"DBUS_SESSION_BUS_ADDRESS\" ] = \"/dev/null\"","title":"Selenium"},{"location":"newsletter/2021_06/#devops","text":"","title":"DevOps"},{"location":"newsletter/2021_06/#infrastructure-solutions","text":"","title":"Infrastructure Solutions"},{"location":"newsletter/2021_06/#jobs","text":"New: Explain how to rerun failed cronjobs. If you have a job that has failed after the 6 default retries, it will show up in your monitorization forever, to fix it, you can manually trigger the job. kubectl get job \"your-job\" -o json \\ | jq 'del(.spec.selector)' \\ | jq 'del(.spec.template.metadata.labels)' \\ | kubectl replace --force -f -","title":"Jobs"},{"location":"newsletter/2021_06/#continuous-integration","text":"New: Explain how to troubleshoot the error: pathspec master did not match any file. Remove all git hooks with rm -r .git/hooks .","title":"Continuous Integration"},{"location":"newsletter/2021_06/#monitoring","text":"","title":"Monitoring"},{"location":"newsletter/2021_06/#elasticsearch-exporter","text":"New: Introduce the prometheus elasticsearch exporter. The elasticsearch exporter allows monitoring Elasticsearch clusters with Prometheus . Explain how to install it, configure the grafana dashboards and the alerts. Improvement: Add more elasticsearch alerts. Measure the search latency, search rate and create alerts on the garbage collector, json parser and circuit breaker errors New: Add alert on low number of healthy master nodes.","title":"Elasticsearch Exporter"},{"location":"newsletter/2021_06/#software-architecture","text":"","title":"Software Architecture"},{"location":"newsletter/2021_06/#domain-driven-design","text":"Improvement: Add warning when migrating old code. You may be tempted to migrate all your old code to this architecture once you fall in love with it. Truth being told, it's the best way to learn how to use it, but it's time expensive too! The last refactor I did required a change of 60% of the code. The upside is that I reduced the total lines of code a 25%.","title":"Domain Driven Design"},{"location":"newsletter/2021_06/#operative-systems","text":"","title":"Operative Systems"},{"location":"newsletter/2021_06/#linux","text":"","title":"Linux"},{"location":"newsletter/2021_06/#linux-snippets","text":"New: Explain how to split a file into many with equal number of lines. split -l 200000 filename New: Explain how to identify what a string or file contains. Using pywhat","title":"Linux Snippets"},{"location":"newsletter/2021_06/#dunst","text":"New: Introduce dunst. Dunst is a lightweight replacement for the notification daemons provided by most desktop environments. It\u2019s very customizable, isn\u2019t dependent on any toolkits, and therefore fits into those window manager centric setups we all love to customize to perfection.","title":"dunst"},{"location":"newsletter/2021_06/#elasticsearch","text":"Correction: Explain how to restore only some indices. curl -X POST \"{{ url }}/_snapshot/{{ backup_path }}/{{ snapshot_name }}/_restore?pretty\" -H 'Content-Type: application/json' -d ' { \"indices\": \"{{ index_to_restore }}\", }' New: Explain how to fix Circuit breakers triggers.","title":"elasticsearch"},{"location":"newsletter/2021_06/#arts","text":"","title":"Arts"},{"location":"newsletter/2021_06/#writing","text":"","title":"Writing"},{"location":"newsletter/2021_06/#grammar-and-orthography","text":"New: Explain what collocations are and how to avoid the word very. Collocation refers to a natural combination of words that are closely affiliated with each other. They make it easier to avoid overused or ambiguous words like \"very\", \"nice\", or \"beautiful\", by using a pair of words that fit the context better and that have a more precise meaning. New: Explain what can you use instead of I know. Using \"I know\" may not be the best way to show the other person that you've got the information. You can take the chance to use other words that additionally gives more context on how you stand with the information you've received, thus improving the communication and creating a bond.","title":"Grammar and Orthography"},{"location":"newsletter/2021_06/#cooking","text":"New: Introduce the cooking art.","title":"Cooking"},{"location":"newsletter/2021_06/#cooking-basics","text":"New: Refactor the perfect technique to boil an egg. New: Explain how to boil chickpeas when you've forgotten to soak them. Add a level teaspoon of baking soda to the pot and cook them as usual","title":"Cooking Basics"},{"location":"newsletter/2021_06_01/","text":"Software Architecture \u2691 Domain Driven Design \u2691 Improvement: Add warning when migrating old code. You may be tempted to migrate all your old code to this architecture once you fall in love with it. Truth being told, it's the best way to learn how to use it, but it's time expensive too! The last refactor I did required a change of 60% of the code. The upside is that I reduced the total lines of code a 25%.","title":"1st June 2021"},{"location":"newsletter/2021_06_01/#software-architecture","text":"","title":"Software Architecture"},{"location":"newsletter/2021_06_01/#domain-driven-design","text":"Improvement: Add warning when migrating old code. You may be tempted to migrate all your old code to this architecture once you fall in love with it. Truth being told, it's the best way to learn how to use it, but it's time expensive too! The last refactor I did required a change of 60% of the code. The upside is that I reduced the total lines of code a 25%.","title":"Domain Driven Design"},{"location":"newsletter/2021_06_03/","text":"DevOps \u2691 Continuous Integration \u2691 New: Explain how to troubleshoot the error: pathspec master did not match any file. Remove all git hooks with rm -r .git/hooks .","title":"3rd June 2021"},{"location":"newsletter/2021_06_03/#devops","text":"","title":"DevOps"},{"location":"newsletter/2021_06_03/#continuous-integration","text":"New: Explain how to troubleshoot the error: pathspec master did not match any file. Remove all git hooks with rm -r .git/hooks .","title":"Continuous Integration"},{"location":"newsletter/2021_06_04/","text":"Coding \u2691 Python \u2691 Python Snippets \u2691 New: Explain how to convert html code to readable plaintext. pip install html2text import html2text html = open ( \"foobar.html\" ) . read () print ( html2text . html2text ( html )) New: Explain how to parse a datetime from a string. from dateutil import parser parser . parse ( \"Aug 28 1999 12:00AM\" ) # datetime.datetime(1999, 8, 28, 0, 0) Python Mysql \u2691 New: Explain how to interact with MySQL databases with Python. Arts \u2691 Writing \u2691 Grammar and Orthography \u2691 New: Explain what collocations are and how to avoid the word very. Collocation refers to a natural combination of words that are closely affiliated with each other. They make it easier to avoid overused or ambiguous words like \"very\", \"nice\", or \"beautiful\", by using a pair of words that fit the context better and that have a more precise meaning.","title":"4th June 2021"},{"location":"newsletter/2021_06_04/#coding","text":"","title":"Coding"},{"location":"newsletter/2021_06_04/#python","text":"","title":"Python"},{"location":"newsletter/2021_06_04/#python-snippets","text":"New: Explain how to convert html code to readable plaintext. pip install html2text import html2text html = open ( \"foobar.html\" ) . read () print ( html2text . html2text ( html )) New: Explain how to parse a datetime from a string. from dateutil import parser parser . parse ( \"Aug 28 1999 12:00AM\" ) # datetime.datetime(1999, 8, 28, 0, 0)","title":"Python Snippets"},{"location":"newsletter/2021_06_04/#python-mysql","text":"New: Explain how to interact with MySQL databases with Python.","title":"Python Mysql"},{"location":"newsletter/2021_06_04/#arts","text":"","title":"Arts"},{"location":"newsletter/2021_06_04/#writing","text":"","title":"Writing"},{"location":"newsletter/2021_06_04/#grammar-and-orthography","text":"New: Explain what collocations are and how to avoid the word very. Collocation refers to a natural combination of words that are closely affiliated with each other. They make it easier to avoid overused or ambiguous words like \"very\", \"nice\", or \"beautiful\", by using a pair of words that fit the context better and that have a more precise meaning.","title":"Grammar and Orthography"},{"location":"newsletter/2021_06_07/","text":"Arts \u2691 Writing \u2691 Grammar and Orthography \u2691 New: Explain what can you use instead of I know. Using \"I know\" may not be the best way to show the other person that you've got the information. You can take the chance to use other words that additionally gives more context on how you stand with the information you've received, thus improving the communication and creating a bond.","title":"7th June 2021"},{"location":"newsletter/2021_06_07/#arts","text":"","title":"Arts"},{"location":"newsletter/2021_06_07/#writing","text":"","title":"Writing"},{"location":"newsletter/2021_06_07/#grammar-and-orthography","text":"New: Explain what can you use instead of I know. Using \"I know\" may not be the best way to show the other person that you've got the information. You can take the chance to use other words that additionally gives more context on how you stand with the information you've received, thus improving the communication and creating a bond.","title":"Grammar and Orthography"},{"location":"newsletter/2021_06_09/","text":"Arts \u2691 Cooking \u2691 New: Introduce the cooking art. Cooking Basics \u2691 New: Refactor the perfect technique to boil an egg. New: Explain how to boil chickpeas when you've forgotten to soak them. Add a level teaspoon of baking soda to the pot and cook them as usual","title":"9th June 2021"},{"location":"newsletter/2021_06_09/#arts","text":"","title":"Arts"},{"location":"newsletter/2021_06_09/#cooking","text":"New: Introduce the cooking art.","title":"Cooking"},{"location":"newsletter/2021_06_09/#cooking-basics","text":"New: Refactor the perfect technique to boil an egg. New: Explain how to boil chickpeas when you've forgotten to soak them. Add a level teaspoon of baking soda to the pot and cook them as usual","title":"Cooking Basics"},{"location":"newsletter/2021_06_11/","text":"Projects \u2691 New: Introduce seedling self-hosted map project. I love maps, as well as traveling and hiking. This project aims to create a web interface that let's me interact with the data gathered throughout my life. I'd like to: Browse the waypoints and routes that I've done. Create routes and export the gpx. Be able to search through the data Plan trips New: Introduce the seed project to.","title":"11th June 2021"},{"location":"newsletter/2021_06_11/#projects","text":"New: Introduce seedling self-hosted map project. I love maps, as well as traveling and hiking. This project aims to create a web interface that let's me interact with the data gathered throughout my life. I'd like to: Browse the waypoints and routes that I've done. Create routes and export the gpx. Be able to search through the data Plan trips New: Introduce the seed project to.","title":"Projects"},{"location":"newsletter/2021_06_15/","text":"Coding \u2691 Python \u2691 Python Mysql \u2691 Correction: Correct the syntax of the left joins. Instead of using ON users.id == addresses.user_id , use ON users.id = addresses.user_id DevOps \u2691 Monitoring \u2691 Elasticsearch Exporter \u2691 New: Introduce the prometheus elasticsearch exporter. The elasticsearch exporter allows monitoring Elasticsearch clusters with Prometheus . Explain how to install it, configure the grafana dashboards and the alerts. Operative Systems \u2691 Linux \u2691 dunst \u2691 New: Introduce dunst. Dunst is a lightweight replacement for the notification daemons provided by most desktop environments. It\u2019s very customizable, isn\u2019t dependent on any toolkits, and therefore fits into those window manager centric setups we all love to customize to perfection.","title":"15th June 2021"},{"location":"newsletter/2021_06_15/#coding","text":"","title":"Coding"},{"location":"newsletter/2021_06_15/#python","text":"","title":"Python"},{"location":"newsletter/2021_06_15/#python-mysql","text":"Correction: Correct the syntax of the left joins. Instead of using ON users.id == addresses.user_id , use ON users.id = addresses.user_id","title":"Python Mysql"},{"location":"newsletter/2021_06_15/#devops","text":"","title":"DevOps"},{"location":"newsletter/2021_06_15/#monitoring","text":"","title":"Monitoring"},{"location":"newsletter/2021_06_15/#elasticsearch-exporter","text":"New: Introduce the prometheus elasticsearch exporter. The elasticsearch exporter allows monitoring Elasticsearch clusters with Prometheus . Explain how to install it, configure the grafana dashboards and the alerts.","title":"Elasticsearch Exporter"},{"location":"newsletter/2021_06_15/#operative-systems","text":"","title":"Operative Systems"},{"location":"newsletter/2021_06_15/#linux","text":"","title":"Linux"},{"location":"newsletter/2021_06_15/#dunst","text":"New: Introduce dunst. Dunst is a lightweight replacement for the notification daemons provided by most desktop environments. It\u2019s very customizable, isn\u2019t dependent on any toolkits, and therefore fits into those window manager centric setups we all love to customize to perfection.","title":"dunst"},{"location":"newsletter/2021_06_16/","text":"DevOps \u2691 Monitoring \u2691 Elasticsearch Exporter \u2691 Improvement: Add more elasticsearch alerts. Measure the search latency, search rate and create alerts on the garbage collector, json parser and circuit breaker errors","title":"16th June 2021"},{"location":"newsletter/2021_06_16/#devops","text":"","title":"DevOps"},{"location":"newsletter/2021_06_16/#monitoring","text":"","title":"Monitoring"},{"location":"newsletter/2021_06_16/#elasticsearch-exporter","text":"Improvement: Add more elasticsearch alerts. Measure the search latency, search rate and create alerts on the garbage collector, json parser and circuit breaker errors","title":"Elasticsearch Exporter"},{"location":"newsletter/2021_06_22/","text":"Projects \u2691 New: Add the Life seedling project. Life is a real time sandbox role game where you play as yourself surviving in today's world. DevOps \u2691 Infrastructure Solutions \u2691 Jobs \u2691 New: Explain how to rerun failed cronjobs. If you have a job that has failed after the 6 default retries, it will show up in your monitorization forever, to fix it, you can manually trigger the job. kubectl get job \"your-job\" -o json \\ | jq 'del(.spec.selector)' \\ | jq 'del(.spec.template.metadata.labels)' \\ | kubectl replace --force -f -","title":"22nd June 2021"},{"location":"newsletter/2021_06_22/#projects","text":"New: Add the Life seedling project. Life is a real time sandbox role game where you play as yourself surviving in today's world.","title":"Projects"},{"location":"newsletter/2021_06_22/#devops","text":"","title":"DevOps"},{"location":"newsletter/2021_06_22/#infrastructure-solutions","text":"","title":"Infrastructure Solutions"},{"location":"newsletter/2021_06_22/#jobs","text":"New: Explain how to rerun failed cronjobs. If you have a job that has failed after the 6 default retries, it will show up in your monitorization forever, to fix it, you can manually trigger the job. kubectl get job \"your-job\" -o json \\ | jq 'del(.spec.selector)' \\ | jq 'del(.spec.template.metadata.labels)' \\ | kubectl replace --force -f -","title":"Jobs"},{"location":"newsletter/2021_06_25/","text":"Projects \u2691 New: Add bruty to the dormant plant projects. bruty is a Python program to bruteforce dynamic web applications with Selenium. Coding \u2691 Python \u2691 Improvement: Add textual as interesting library to explore. textual : Textual is a TUI (Text User Interface) framework for Python using Rich as a renderer. Click \u2691 Improvement: Explain how to change the command line help description. FastAPI \u2691 New: Explain how to make redirections with fastapi. New: Explain how to run a FastAPI server in the background for testing purposes. Pytest \u2691 New: Explain how to set a timeout for your tests. Using pytest-timeout . New: Explain how to rerun tests that fail sometimes. With pytest-rerunfailures that is a plugin for pytest that re-runs tests to eliminate intermittent failures. Using this plugin is generally a bad idea, it would be best to solve the reason why your code is not reliable. It's useful when you rely on non robust third party software in a way that you can't solve, or if the error is not in your code but in the testing code, and again you are not able to fix it. feat(python_snippets#Create combination of elements in groups of two): Explain how to create combination of elements in groups of two >>> list ( itertools . combinations ( 'ABC' , 2 )) [( 'A' , 'B' ), ( 'A' , 'C' ), ( 'B' , 'C' )] Selenium \u2691 New: Explain how to use selenium with python. New: Explain how to Set timeout of a response. driver . set_page_load_timeout ( 30 ) New: Explain how to fix when Chromedriver hangs up unexpectedly. os . environ [ \"DBUS_SESSION_BUS_ADDRESS\" ] = \"/dev/null\" Operative Systems \u2691 Linux \u2691 Linux Snippets \u2691 New: Explain how to split a file into many with equal number of lines. split -l 200000 filename New: Explain how to identify what a string or file contains. Using pywhat","title":"25th June 2021"},{"location":"newsletter/2021_06_25/#projects","text":"New: Add bruty to the dormant plant projects. bruty is a Python program to bruteforce dynamic web applications with Selenium.","title":"Projects"},{"location":"newsletter/2021_06_25/#coding","text":"","title":"Coding"},{"location":"newsletter/2021_06_25/#python","text":"Improvement: Add textual as interesting library to explore. textual : Textual is a TUI (Text User Interface) framework for Python using Rich as a renderer.","title":"Python"},{"location":"newsletter/2021_06_25/#click","text":"Improvement: Explain how to change the command line help description.","title":"Click"},{"location":"newsletter/2021_06_25/#fastapi","text":"New: Explain how to make redirections with fastapi. New: Explain how to run a FastAPI server in the background for testing purposes.","title":"FastAPI"},{"location":"newsletter/2021_06_25/#pytest","text":"New: Explain how to set a timeout for your tests. Using pytest-timeout . New: Explain how to rerun tests that fail sometimes. With pytest-rerunfailures that is a plugin for pytest that re-runs tests to eliminate intermittent failures. Using this plugin is generally a bad idea, it would be best to solve the reason why your code is not reliable. It's useful when you rely on non robust third party software in a way that you can't solve, or if the error is not in your code but in the testing code, and again you are not able to fix it. feat(python_snippets#Create combination of elements in groups of two): Explain how to create combination of elements in groups of two >>> list ( itertools . combinations ( 'ABC' , 2 )) [( 'A' , 'B' ), ( 'A' , 'C' ), ( 'B' , 'C' )]","title":"Pytest"},{"location":"newsletter/2021_06_25/#selenium","text":"New: Explain how to use selenium with python. New: Explain how to Set timeout of a response. driver . set_page_load_timeout ( 30 ) New: Explain how to fix when Chromedriver hangs up unexpectedly. os . environ [ \"DBUS_SESSION_BUS_ADDRESS\" ] = \"/dev/null\"","title":"Selenium"},{"location":"newsletter/2021_06_25/#operative-systems","text":"","title":"Operative Systems"},{"location":"newsletter/2021_06_25/#linux","text":"","title":"Linux"},{"location":"newsletter/2021_06_25/#linux-snippets","text":"New: Explain how to split a file into many with equal number of lines. split -l 200000 filename New: Explain how to identify what a string or file contains. Using pywhat","title":"Linux Snippets"},{"location":"newsletter/2021_06_28/","text":"DevOps \u2691 Monitoring \u2691 Elasticsearch Exporter \u2691 New: Add alert on low number of healthy master nodes. Operative Systems \u2691 Linux \u2691 elasticsearch \u2691 Correction: Explain how to restore only some indices. curl -X POST \"{{ url }}/_snapshot/{{ backup_path }}/{{ snapshot_name }}/_restore?pretty\" -H 'Content-Type: application/json' -d ' { \"indices\": \"{{ index_to_restore }}\", }' New: Explain how to fix Circuit breakers triggers.","title":"28th June 2021"},{"location":"newsletter/2021_06_28/#devops","text":"","title":"DevOps"},{"location":"newsletter/2021_06_28/#monitoring","text":"","title":"Monitoring"},{"location":"newsletter/2021_06_28/#elasticsearch-exporter","text":"New: Add alert on low number of healthy master nodes.","title":"Elasticsearch Exporter"},{"location":"newsletter/2021_06_28/#operative-systems","text":"","title":"Operative Systems"},{"location":"newsletter/2021_06_28/#linux","text":"","title":"Linux"},{"location":"newsletter/2021_06_28/#elasticsearch","text":"Correction: Explain how to restore only some indices. curl -X POST \"{{ url }}/_snapshot/{{ backup_path }}/{{ snapshot_name }}/_restore?pretty\" -H 'Content-Type: application/json' -d ' { \"indices\": \"{{ index_to_restore }}\", }' New: Explain how to fix Circuit breakers triggers.","title":"elasticsearch"},{"location":"newsletter/2021_06_29/","text":"Coding \u2691 Python \u2691 Configure Docker to host the application \u2691 New: Explain how to use watchtower to keep docker containers updated. With watchtower you can update the running version of your containerized app simply by pushing a new image to the Docker Hub or your own image registry. Watchtower will pull down your new image, gracefully shut down your existing container and restart it with the same options that were used when it was deployed initially. Correction: Explain how to run the watchtower checks immediately. With the --run-once flag","title":"29th June 2021"},{"location":"newsletter/2021_06_29/#coding","text":"","title":"Coding"},{"location":"newsletter/2021_06_29/#python","text":"","title":"Python"},{"location":"newsletter/2021_06_29/#configure-docker-to-host-the-application","text":"New: Explain how to use watchtower to keep docker containers updated. With watchtower you can update the running version of your containerized app simply by pushing a new image to the Docker Hub or your own image registry. Watchtower will pull down your new image, gracefully shut down your existing container and restart it with the same options that were used when it was deployed initially. Correction: Explain how to run the watchtower checks immediately. With the --run-once flag","title":"Configure Docker to host the application"},{"location":"newsletter/2021_07/","text":"Projects \u2691 Improvement: Add rsarai hq to interesting sources for lifelogging. Coding \u2691 Python \u2691 questionary \u2691 Correction: Correct the link to the examples. Operative Systems \u2691 Linux \u2691 New: Introduce Tahoe-LAFS. Tahoe-LAFS is a free and open, secure, decentralized, fault-tolerant, distributed data store and distributed file system. Tahoe-LAFS is a system that helps you to store files. You run a client program on your computer, which talks to one or more storage servers on other computers. When you tell your client to store a file, it will encrypt that file, encode it into multiple pieces, then spread those pieces out among multiple servers. The pieces are all encrypted and protected against modifications. Later, when you ask your client to retrieve the file, it will find the necessary pieces, make sure they haven\u2019t been corrupted, reassemble them, and decrypt the result. elasticsearch \u2691 Correction: Correct the way of closing an index. Use a POST instead of a GET New: Explain how to calculate the amount of memory required to do KNN operations. New: Explain how to do KNN warmup to speed up the queries. New: Explain how to deal with the AWS service timeout. Jellyfin \u2691 Improvement: Explain how to fix the wrong image covers. Remove all the jpg files of the directory and then fetch again the data from your favourite media management software. New: Track the issue of trailers not working. New: Explain how to fix the green bars in the reproduction. LUKS \u2691 New: Explain how to change a LUKS key. cryptsetup luksChangeKey {{ luks_device }} -s 0 Oracle Database \u2691 New: Explain how to build an oracle database docker while feeling dirty inside. Syncthing \u2691 New: Investigate if Syncthing can be used over Tor. I haven't found a reliable and safe way to do it, but I've set a path to follow if you're interested. Android \u2691 OsmAnd \u2691 New: Introduce OsmAnd. OsmAnd is a mobile application for global map viewing and navigating based on OpenStreetMaps . Perfect if you're looking for a privacy focused, community maintained open source alternative to google maps.","title":"July of 2021"},{"location":"newsletter/2021_07/#projects","text":"Improvement: Add rsarai hq to interesting sources for lifelogging.","title":"Projects"},{"location":"newsletter/2021_07/#coding","text":"","title":"Coding"},{"location":"newsletter/2021_07/#python","text":"","title":"Python"},{"location":"newsletter/2021_07/#questionary","text":"Correction: Correct the link to the examples.","title":"questionary"},{"location":"newsletter/2021_07/#operative-systems","text":"","title":"Operative Systems"},{"location":"newsletter/2021_07/#linux","text":"New: Introduce Tahoe-LAFS. Tahoe-LAFS is a free and open, secure, decentralized, fault-tolerant, distributed data store and distributed file system. Tahoe-LAFS is a system that helps you to store files. You run a client program on your computer, which talks to one or more storage servers on other computers. When you tell your client to store a file, it will encrypt that file, encode it into multiple pieces, then spread those pieces out among multiple servers. The pieces are all encrypted and protected against modifications. Later, when you ask your client to retrieve the file, it will find the necessary pieces, make sure they haven\u2019t been corrupted, reassemble them, and decrypt the result.","title":"Linux"},{"location":"newsletter/2021_07/#elasticsearch","text":"Correction: Correct the way of closing an index. Use a POST instead of a GET New: Explain how to calculate the amount of memory required to do KNN operations. New: Explain how to do KNN warmup to speed up the queries. New: Explain how to deal with the AWS service timeout.","title":"elasticsearch"},{"location":"newsletter/2021_07/#jellyfin","text":"Improvement: Explain how to fix the wrong image covers. Remove all the jpg files of the directory and then fetch again the data from your favourite media management software. New: Track the issue of trailers not working. New: Explain how to fix the green bars in the reproduction.","title":"Jellyfin"},{"location":"newsletter/2021_07/#luks","text":"New: Explain how to change a LUKS key. cryptsetup luksChangeKey {{ luks_device }} -s 0","title":"LUKS"},{"location":"newsletter/2021_07/#oracle-database","text":"New: Explain how to build an oracle database docker while feeling dirty inside.","title":"Oracle Database"},{"location":"newsletter/2021_07/#syncthing","text":"New: Investigate if Syncthing can be used over Tor. I haven't found a reliable and safe way to do it, but I've set a path to follow if you're interested.","title":"Syncthing"},{"location":"newsletter/2021_07/#android","text":"","title":"Android"},{"location":"newsletter/2021_07/#osmand","text":"New: Introduce OsmAnd. OsmAnd is a mobile application for global map viewing and navigating based on OpenStreetMaps . Perfect if you're looking for a privacy focused, community maintained open source alternative to google maps.","title":"OsmAnd"},{"location":"newsletter/2021_07_12/","text":"Operative Systems \u2691 Linux \u2691 New: Introduce Tahoe-LAFS. Tahoe-LAFS is a free and open, secure, decentralized, fault-tolerant, distributed data store and distributed file system. Tahoe-LAFS is a system that helps you to store files. You run a client program on your computer, which talks to one or more storage servers on other computers. When you tell your client to store a file, it will encrypt that file, encode it into multiple pieces, then spread those pieces out among multiple servers. The pieces are all encrypted and protected against modifications. Later, when you ask your client to retrieve the file, it will find the necessary pieces, make sure they haven\u2019t been corrupted, reassemble them, and decrypt the result. elasticsearch \u2691 Correction: Correct the way of closing an index. Use a POST instead of a GET New: Explain how to calculate the amount of memory required to do KNN operations. New: Explain how to do KNN warmup to speed up the queries. New: Explain how to deal with the AWS service timeout. Jellyfin \u2691 Improvement: Explain how to fix the wrong image covers. Remove all the jpg files of the directory and then fetch again the data from your favourite media management software. Syncthing \u2691 New: Investigate if Syncthing can be used over Tor. I haven't found a reliable and safe way to do it, but I've set a path to follow if you're interested.","title":"12th July 2021"},{"location":"newsletter/2021_07_12/#operative-systems","text":"","title":"Operative Systems"},{"location":"newsletter/2021_07_12/#linux","text":"New: Introduce Tahoe-LAFS. Tahoe-LAFS is a free and open, secure, decentralized, fault-tolerant, distributed data store and distributed file system. Tahoe-LAFS is a system that helps you to store files. You run a client program on your computer, which talks to one or more storage servers on other computers. When you tell your client to store a file, it will encrypt that file, encode it into multiple pieces, then spread those pieces out among multiple servers. The pieces are all encrypted and protected against modifications. Later, when you ask your client to retrieve the file, it will find the necessary pieces, make sure they haven\u2019t been corrupted, reassemble them, and decrypt the result.","title":"Linux"},{"location":"newsletter/2021_07_12/#elasticsearch","text":"Correction: Correct the way of closing an index. Use a POST instead of a GET New: Explain how to calculate the amount of memory required to do KNN operations. New: Explain how to do KNN warmup to speed up the queries. New: Explain how to deal with the AWS service timeout.","title":"elasticsearch"},{"location":"newsletter/2021_07_12/#jellyfin","text":"Improvement: Explain how to fix the wrong image covers. Remove all the jpg files of the directory and then fetch again the data from your favourite media management software.","title":"Jellyfin"},{"location":"newsletter/2021_07_12/#syncthing","text":"New: Investigate if Syncthing can be used over Tor. I haven't found a reliable and safe way to do it, but I've set a path to follow if you're interested.","title":"Syncthing"},{"location":"newsletter/2021_07_15/","text":"Projects \u2691 Improvement: Add rsarai hq to interesting sources for lifelogging. Operative Systems \u2691 Linux \u2691 Jellyfin \u2691 New: Track the issue of trailers not working. Android \u2691 OsmAnd \u2691 New: Introduce OsmAnd. OsmAnd is a mobile application for global map viewing and navigating based on OpenStreetMaps . Perfect if you're looking for a privacy focused, community maintained open source alternative to google maps.","title":"15th July 2021"},{"location":"newsletter/2021_07_15/#projects","text":"Improvement: Add rsarai hq to interesting sources for lifelogging.","title":"Projects"},{"location":"newsletter/2021_07_15/#operative-systems","text":"","title":"Operative Systems"},{"location":"newsletter/2021_07_15/#linux","text":"","title":"Linux"},{"location":"newsletter/2021_07_15/#jellyfin","text":"New: Track the issue of trailers not working.","title":"Jellyfin"},{"location":"newsletter/2021_07_15/#android","text":"","title":"Android"},{"location":"newsletter/2021_07_15/#osmand","text":"New: Introduce OsmAnd. OsmAnd is a mobile application for global map viewing and navigating based on OpenStreetMaps . Perfect if you're looking for a privacy focused, community maintained open source alternative to google maps.","title":"OsmAnd"},{"location":"newsletter/2021_07_20/","text":"Operative Systems \u2691 Linux \u2691 Jellyfin \u2691 New: Explain how to fix the green bars in the reproduction.","title":"20th July 2021"},{"location":"newsletter/2021_07_20/#operative-systems","text":"","title":"Operative Systems"},{"location":"newsletter/2021_07_20/#linux","text":"","title":"Linux"},{"location":"newsletter/2021_07_20/#jellyfin","text":"New: Explain how to fix the green bars in the reproduction.","title":"Jellyfin"},{"location":"newsletter/2021_07_30/","text":"Coding \u2691 Python \u2691 questionary \u2691 Correction: Correct the link to the examples. Operative Systems \u2691 Linux \u2691 LUKS \u2691 New: Explain how to change a LUKS key. cryptsetup luksChangeKey {{ luks_device }} -s 0 Oracle Database \u2691 New: Explain how to build an oracle database docker while feeling dirty inside.","title":"30th July 2021"},{"location":"newsletter/2021_07_30/#coding","text":"","title":"Coding"},{"location":"newsletter/2021_07_30/#python","text":"","title":"Python"},{"location":"newsletter/2021_07_30/#questionary","text":"Correction: Correct the link to the examples.","title":"questionary"},{"location":"newsletter/2021_07_30/#operative-systems","text":"","title":"Operative Systems"},{"location":"newsletter/2021_07_30/#linux","text":"","title":"Linux"},{"location":"newsletter/2021_07_30/#luks","text":"New: Explain how to change a LUKS key. cryptsetup luksChangeKey {{ luks_device }} -s 0","title":"LUKS"},{"location":"newsletter/2021_07_30/#oracle-database","text":"New: Explain how to build an oracle database docker while feeling dirty inside.","title":"Oracle Database"},{"location":"newsletter/2021_08/","text":"Projects \u2691 New: Introduce pynbox the inbox management tool. Pynbox is a tool to improve the management of ideas, tasks, references, suggestions when I'm not in front of the computer. Right now I've got Markor for Android to register these quicknotes, but the reality is that I don't act upon them, so it's just a log of tasks that never get done, and ideas, references and suggestions that aren't registered in my knowledge or media management systems. On the computer there are also cases of tasks that are not worth registering in the task management system, or ideas that I get at a moment but don't have time to process at the moment. The idea then is to automatically sync the Android quicknote with syncthing, and have a special format for the file that allows pynbox to extract the elements from that file to the \"inbox system\". For example: + t. buy groceries tv. IT crowd i. Improve the inbox management I want a system to improve ... Gets introduced in the \"inbox system\" as a task, a TV suggestion and an idea. New: Introduce nyxt as a solution for a better browser. I've just stumbled upon nyxt ( code ), and it looks superb. New: Introduce the shared accounting seed project. I use beancount for my personal accounting, I'd like to have a system that integrates more less easily with beancount and let's do a shared accounting with other people, for example in trips. I've used settle up in the past but it requires access to their servers, and an account linked to google, facebook or one you register in their servers. I've looked at facto but it uses a logic that doesn't apply to my case, it does a heavy use on a common account, instead +of minimizing the transactions between the people. I also tried tabby , even though they still don't support Docker , but it doesn't suit my case either :(. Until a new solution shows up, I'll go with Tricky Tripper available in F-Droid, and manage the expenses myself and periodically send the html reports to the rest of the group. Improvement: Add quickwit as an interesting database solution for personal knowledge search engine. New: Promote the automation of email management project to seedling. Activism \u2691 New: Introduce Anti-transphobia. Anti-transphobia being reductionist is the opposition to the collection of ideas and phenomena that encompass a range of negative attitudes, feelings or actions towards transgender people or transness in general. Transphobia can include fear, aversion, hatred, violence, anger, or discomfort felt or expressed towards people who do not conform to social gender expectations. It is often expressed alongside homophobic views and hence is often considered an aspect of homophobia. New: Introduce arguments against terf ideology. TERF is an acronym for trans-exclusionary radical feminist . The term originally applied to the minority of feminists that expressed transphobic sentiments such as the rejection of the assertion that trans women are women, the exclusion of trans women from women's spaces, and opposition to transgender rights legislation. The meaning has since expanded to refer more broadly to people with trans-exclusionary views who may have no involvement with radical feminism. Antifascism \u2691 New: Introduce antifascism. Antifascism is a method of politics, a locus of individual and group self-indentification, it's a transnational movement that adapted preexisting socialist, anarchist, and communist currents to a sudden need to react to the fascist menace ( Mark p. 11 ). It's based on the idea that any oppression form can't be allowed, and should be actively fought with whatever means are necessary. Life Management \u2691 Email Management \u2691 Email Automation \u2691 New: Explain how setup an infrastructure to automate. Coding \u2691 Python \u2691 New: Add schedule to interesting libraries to explore. schedule is a Python job scheduling for humans. Run Python functions (or any other callable) periodically using a friendly syntax. asyncio \u2691 New: Introduce the asyncio library. asyncio is a library to write concurrent code using the async/await syntax. asyncio is used as a foundation for multiple Python asynchronous frameworks that provide high-performance network and web-servers, database connection libraries, distributed task queues, etc. asyncio is often a perfect fit for IO-bound and high-level structured network code. FastAPI \u2691 Improvement: Add link to the Awesome FastAPI page. Python Snippets \u2691 New: Explain how to find a static file of a python module. import pkg_resources file_path = pkg_resources . resource_filename ( \"my_package\" , \"assets/config.yaml\" ), New: Explain how to delete a file. import os os . remove ( 'demofile.txt' ) New: Explain how to measure elapsed time between lines of code. import time start = time . time () print ( \"hello\" ) end = time . time () print ( end - start ) pexpect \u2691 New: Explain how to read the output of a command run by pexpect. import sys import pexpect child = pexpect . spawn ( 'ls' ) child . logfile = sys . stdout child . expect ( pexpect . EOF ) rich \u2691 New: Explain how to build pretty tables with rich. from rich.console import Console from rich.table import Table table = Table ( title = \"Star Wars Movies\" ) table . add_column ( \"Released\" , justify = \"right\" , style = \"cyan\" , no_wrap = True ) table . add_column ( \"Title\" , style = \"magenta\" ) table . add_column ( \"Box Office\" , justify = \"right\" , style = \"green\" ) table . add_row ( \"Dec 20, 2019\" , \"Star Wars: The Rise of Skywalker\" , \"$952,110,690\" ) table . add_row ( \"May 25, 2018\" , \"Solo: A Star Wars Story\" , \"$393,151,347\" ) table . add_row ( \"Dec 15, 2017\" , \"Star Wars Ep. V111: The Last Jedi\" , \"$1,332,539,889\" ) table . add_row ( \"Dec 16, 2016\" , \"Rogue One: A Star Wars Story\" , \"$1,332,439,889\" ) console = Console () console . print ( table ) New: Explain how to print pretty text with rich. from rich.console import Console from rich.text import Text console = Console () text = Text . assemble (( \"Hello\" , \"bold magenta\" ), \" World!\" ) console . print ( text ) SQLite \u2691 Improvement: Add rqlite as an interesting distributed solution of. DevOps \u2691 Infrastructure as Code \u2691 Terraform \u2691 New: Introduce terraform and how to handle RDS secrets. Terraform is an open-source infrastructure as code software tool created by HashiCorp. It enables users to define and provision a datacenter infrastructure using an awful high-level configuration language known as Hashicorp Configuration Language (HCL), or optionally JSON. Terraform supports a number of cloud infrastructure providers such as Amazon Web Services, IBM Cloud , Google Cloud Platform, DigitalOcean, Linode, Microsoft Azure, Oracle Cloud Infrastructure, OVH, or VMware vSphere as well as OpenNebula and OpenStack. New: Explain how to ignore the change of an attribute. resource \"aws_instance\" \"example\" { # ... lifecycle { ignore_changes = [ # Ignore changes to tags, e.g. because a management agent # updates these based on some ruleset managed elsewhere. tags, ] } } New: Explain how to define the default value of an variable that contains an object as empty. variable \"database\" { type = object({ size = number instance_type = string storage_type = string engine = string engine_version = string parameter_group_name = string multi_az = bool }) default = null New: Explain how to do a conditional if a variable is not null. resource \"aws_db_instance\" \"instance\" { count = var.database == null ? 0 : 1 ... Infrastructure Solutions \u2691 Kubernetes \u2691 New: Add Velero as interesting tool. Velero is a tool to backup and migrate Kubernetes resources and persistent volumes. Architecture \u2691 New: Give suggestions on how to choose the number of kubernetes clusters to use. You can run a given set of workloads either on few large clusters (with many workloads in each cluster) or on many clusters (with few workloads in each cluster). Here's a table that summarizes the pros and cons of various approaches: Figure: Possibilities of number of clusters from learnk8s.io article Operative Systems \u2691 Linux \u2691 Linux Snippets \u2691 New: Explain how to allocate space for a virtual filesystem. fallocate -l 20G /path/to/file afew \u2691 New: Introduce afew. afew is an initial tagging script for notmuch mail . Its basic task is to provide automatic tagging each time new mail is registered with notmuch . In a classic setup, you might call it after notmuch new in an offlineimap post sync hook. alot \u2691 New: Introduce alot. alot is a terminal-based mail user agent based on the notmuch mail indexer . It is written in python using the urwid toolkit and features a modular and command prompt driven interface to provide a full MUA experience. Arts \u2691 Writing \u2691 Grammar and Orthography \u2691 New: Explain the use of z or s in some words. It looks like American english uses z while British uses s , some examples: Organizations vs organisation . Authorization vs authorisation . Customized vs customised . Both forms are correct, so choose the one that suits your liking. Contact \u2691 Correction: Update the XMPP address. Riseup has stopped giving support for XMPP :(","title":"August of 2021"},{"location":"newsletter/2021_08/#projects","text":"New: Introduce pynbox the inbox management tool. Pynbox is a tool to improve the management of ideas, tasks, references, suggestions when I'm not in front of the computer. Right now I've got Markor for Android to register these quicknotes, but the reality is that I don't act upon them, so it's just a log of tasks that never get done, and ideas, references and suggestions that aren't registered in my knowledge or media management systems. On the computer there are also cases of tasks that are not worth registering in the task management system, or ideas that I get at a moment but don't have time to process at the moment. The idea then is to automatically sync the Android quicknote with syncthing, and have a special format for the file that allows pynbox to extract the elements from that file to the \"inbox system\". For example: + t. buy groceries tv. IT crowd i. Improve the inbox management I want a system to improve ... Gets introduced in the \"inbox system\" as a task, a TV suggestion and an idea. New: Introduce nyxt as a solution for a better browser. I've just stumbled upon nyxt ( code ), and it looks superb. New: Introduce the shared accounting seed project. I use beancount for my personal accounting, I'd like to have a system that integrates more less easily with beancount and let's do a shared accounting with other people, for example in trips. I've used settle up in the past but it requires access to their servers, and an account linked to google, facebook or one you register in their servers. I've looked at facto but it uses a logic that doesn't apply to my case, it does a heavy use on a common account, instead +of minimizing the transactions between the people. I also tried tabby , even though they still don't support Docker , but it doesn't suit my case either :(. Until a new solution shows up, I'll go with Tricky Tripper available in F-Droid, and manage the expenses myself and periodically send the html reports to the rest of the group. Improvement: Add quickwit as an interesting database solution for personal knowledge search engine. New: Promote the automation of email management project to seedling.","title":"Projects"},{"location":"newsletter/2021_08/#activism","text":"New: Introduce Anti-transphobia. Anti-transphobia being reductionist is the opposition to the collection of ideas and phenomena that encompass a range of negative attitudes, feelings or actions towards transgender people or transness in general. Transphobia can include fear, aversion, hatred, violence, anger, or discomfort felt or expressed towards people who do not conform to social gender expectations. It is often expressed alongside homophobic views and hence is often considered an aspect of homophobia. New: Introduce arguments against terf ideology. TERF is an acronym for trans-exclusionary radical feminist . The term originally applied to the minority of feminists that expressed transphobic sentiments such as the rejection of the assertion that trans women are women, the exclusion of trans women from women's spaces, and opposition to transgender rights legislation. The meaning has since expanded to refer more broadly to people with trans-exclusionary views who may have no involvement with radical feminism.","title":"Activism"},{"location":"newsletter/2021_08/#antifascism","text":"New: Introduce antifascism. Antifascism is a method of politics, a locus of individual and group self-indentification, it's a transnational movement that adapted preexisting socialist, anarchist, and communist currents to a sudden need to react to the fascist menace ( Mark p. 11 ). It's based on the idea that any oppression form can't be allowed, and should be actively fought with whatever means are necessary.","title":"Antifascism"},{"location":"newsletter/2021_08/#life-management","text":"","title":"Life Management"},{"location":"newsletter/2021_08/#email-management","text":"","title":"Email Management"},{"location":"newsletter/2021_08/#email-automation","text":"New: Explain how setup an infrastructure to automate.","title":"Email Automation"},{"location":"newsletter/2021_08/#coding","text":"","title":"Coding"},{"location":"newsletter/2021_08/#python","text":"New: Add schedule to interesting libraries to explore. schedule is a Python job scheduling for humans. Run Python functions (or any other callable) periodically using a friendly syntax.","title":"Python"},{"location":"newsletter/2021_08/#asyncio","text":"New: Introduce the asyncio library. asyncio is a library to write concurrent code using the async/await syntax. asyncio is used as a foundation for multiple Python asynchronous frameworks that provide high-performance network and web-servers, database connection libraries, distributed task queues, etc. asyncio is often a perfect fit for IO-bound and high-level structured network code.","title":"asyncio"},{"location":"newsletter/2021_08/#fastapi","text":"Improvement: Add link to the Awesome FastAPI page.","title":"FastAPI"},{"location":"newsletter/2021_08/#python-snippets","text":"New: Explain how to find a static file of a python module. import pkg_resources file_path = pkg_resources . resource_filename ( \"my_package\" , \"assets/config.yaml\" ), New: Explain how to delete a file. import os os . remove ( 'demofile.txt' ) New: Explain how to measure elapsed time between lines of code. import time start = time . time () print ( \"hello\" ) end = time . time () print ( end - start )","title":"Python Snippets"},{"location":"newsletter/2021_08/#pexpect","text":"New: Explain how to read the output of a command run by pexpect. import sys import pexpect child = pexpect . spawn ( 'ls' ) child . logfile = sys . stdout child . expect ( pexpect . EOF )","title":"pexpect"},{"location":"newsletter/2021_08/#rich","text":"New: Explain how to build pretty tables with rich. from rich.console import Console from rich.table import Table table = Table ( title = \"Star Wars Movies\" ) table . add_column ( \"Released\" , justify = \"right\" , style = \"cyan\" , no_wrap = True ) table . add_column ( \"Title\" , style = \"magenta\" ) table . add_column ( \"Box Office\" , justify = \"right\" , style = \"green\" ) table . add_row ( \"Dec 20, 2019\" , \"Star Wars: The Rise of Skywalker\" , \"$952,110,690\" ) table . add_row ( \"May 25, 2018\" , \"Solo: A Star Wars Story\" , \"$393,151,347\" ) table . add_row ( \"Dec 15, 2017\" , \"Star Wars Ep. V111: The Last Jedi\" , \"$1,332,539,889\" ) table . add_row ( \"Dec 16, 2016\" , \"Rogue One: A Star Wars Story\" , \"$1,332,439,889\" ) console = Console () console . print ( table ) New: Explain how to print pretty text with rich. from rich.console import Console from rich.text import Text console = Console () text = Text . assemble (( \"Hello\" , \"bold magenta\" ), \" World!\" ) console . print ( text )","title":"rich"},{"location":"newsletter/2021_08/#sqlite","text":"Improvement: Add rqlite as an interesting distributed solution of.","title":"SQLite"},{"location":"newsletter/2021_08/#devops","text":"","title":"DevOps"},{"location":"newsletter/2021_08/#infrastructure-as-code","text":"","title":"Infrastructure as Code"},{"location":"newsletter/2021_08/#terraform","text":"New: Introduce terraform and how to handle RDS secrets. Terraform is an open-source infrastructure as code software tool created by HashiCorp. It enables users to define and provision a datacenter infrastructure using an awful high-level configuration language known as Hashicorp Configuration Language (HCL), or optionally JSON. Terraform supports a number of cloud infrastructure providers such as Amazon Web Services, IBM Cloud , Google Cloud Platform, DigitalOcean, Linode, Microsoft Azure, Oracle Cloud Infrastructure, OVH, or VMware vSphere as well as OpenNebula and OpenStack. New: Explain how to ignore the change of an attribute. resource \"aws_instance\" \"example\" { # ... lifecycle { ignore_changes = [ # Ignore changes to tags, e.g. because a management agent # updates these based on some ruleset managed elsewhere. tags, ] } } New: Explain how to define the default value of an variable that contains an object as empty. variable \"database\" { type = object({ size = number instance_type = string storage_type = string engine = string engine_version = string parameter_group_name = string multi_az = bool }) default = null New: Explain how to do a conditional if a variable is not null. resource \"aws_db_instance\" \"instance\" { count = var.database == null ? 0 : 1 ...","title":"Terraform"},{"location":"newsletter/2021_08/#infrastructure-solutions","text":"","title":"Infrastructure Solutions"},{"location":"newsletter/2021_08/#kubernetes","text":"New: Add Velero as interesting tool. Velero is a tool to backup and migrate Kubernetes resources and persistent volumes.","title":"Kubernetes"},{"location":"newsletter/2021_08/#architecture","text":"New: Give suggestions on how to choose the number of kubernetes clusters to use. You can run a given set of workloads either on few large clusters (with many workloads in each cluster) or on many clusters (with few workloads in each cluster). Here's a table that summarizes the pros and cons of various approaches: Figure: Possibilities of number of clusters from learnk8s.io article","title":"Architecture"},{"location":"newsletter/2021_08/#operative-systems","text":"","title":"Operative Systems"},{"location":"newsletter/2021_08/#linux","text":"","title":"Linux"},{"location":"newsletter/2021_08/#linux-snippets","text":"New: Explain how to allocate space for a virtual filesystem. fallocate -l 20G /path/to/file","title":"Linux Snippets"},{"location":"newsletter/2021_08/#afew","text":"New: Introduce afew. afew is an initial tagging script for notmuch mail . Its basic task is to provide automatic tagging each time new mail is registered with notmuch . In a classic setup, you might call it after notmuch new in an offlineimap post sync hook.","title":"afew"},{"location":"newsletter/2021_08/#alot","text":"New: Introduce alot. alot is a terminal-based mail user agent based on the notmuch mail indexer . It is written in python using the urwid toolkit and features a modular and command prompt driven interface to provide a full MUA experience.","title":"alot"},{"location":"newsletter/2021_08/#arts","text":"","title":"Arts"},{"location":"newsletter/2021_08/#writing","text":"","title":"Writing"},{"location":"newsletter/2021_08/#grammar-and-orthography","text":"New: Explain the use of z or s in some words. It looks like American english uses z while British uses s , some examples: Organizations vs organisation . Authorization vs authorisation . Customized vs customised . Both forms are correct, so choose the one that suits your liking.","title":"Grammar and Orthography"},{"location":"newsletter/2021_08/#contact","text":"Correction: Update the XMPP address. Riseup has stopped giving support for XMPP :(","title":"Contact"},{"location":"newsletter/2021_08_04/","text":"DevOps \u2691 Infrastructure Solutions \u2691 Kubernetes \u2691 New: Add Velero as interesting tool. Velero is a tool to backup and migrate Kubernetes resources and persistent volumes. Architecture \u2691 New: Give suggestions on how to choose the number of kubernetes clusters to use. You can run a given set of workloads either on few large clusters (with many workloads in each cluster) or on many clusters (with few workloads in each cluster). Here's a table that summarizes the pros and cons of various approaches: Figure: Possibilities of number of clusters from learnk8s.io article Operative Systems \u2691 Linux \u2691 Linux Snippets \u2691 New: Explain how to allocate space for a virtual filesystem. fallocate -l 20G /path/to/file Arts \u2691 Writing \u2691 Grammar and Orthography \u2691 New: Explain the use of z or s in some words. It looks like American english uses z while British uses s , some examples: Organizations vs organisation . Authorization vs authorisation . Customized vs customised . Both forms are correct, so choose the one that suits your liking.","title":"4th August 2021"},{"location":"newsletter/2021_08_04/#devops","text":"","title":"DevOps"},{"location":"newsletter/2021_08_04/#infrastructure-solutions","text":"","title":"Infrastructure Solutions"},{"location":"newsletter/2021_08_04/#kubernetes","text":"New: Add Velero as interesting tool. Velero is a tool to backup and migrate Kubernetes resources and persistent volumes.","title":"Kubernetes"},{"location":"newsletter/2021_08_04/#architecture","text":"New: Give suggestions on how to choose the number of kubernetes clusters to use. You can run a given set of workloads either on few large clusters (with many workloads in each cluster) or on many clusters (with few workloads in each cluster). Here's a table that summarizes the pros and cons of various approaches: Figure: Possibilities of number of clusters from learnk8s.io article","title":"Architecture"},{"location":"newsletter/2021_08_04/#operative-systems","text":"","title":"Operative Systems"},{"location":"newsletter/2021_08_04/#linux","text":"","title":"Linux"},{"location":"newsletter/2021_08_04/#linux-snippets","text":"New: Explain how to allocate space for a virtual filesystem. fallocate -l 20G /path/to/file","title":"Linux Snippets"},{"location":"newsletter/2021_08_04/#arts","text":"","title":"Arts"},{"location":"newsletter/2021_08_04/#writing","text":"","title":"Writing"},{"location":"newsletter/2021_08_04/#grammar-and-orthography","text":"New: Explain the use of z or s in some words. It looks like American english uses z while British uses s , some examples: Organizations vs organisation . Authorization vs authorisation . Customized vs customised . Both forms are correct, so choose the one that suits your liking.","title":"Grammar and Orthography"},{"location":"newsletter/2021_08_06/","text":"DevOps \u2691 Infrastructure as Code \u2691 Terraform \u2691 New: Introduce terraform and how to handle RDS secrets. Terraform is an open-source infrastructure as code software tool created by HashiCorp. It enables users to define and provision a datacenter infrastructure using an awful high-level configuration language known as Hashicorp Configuration Language (HCL), or optionally JSON. Terraform supports a number of cloud infrastructure providers such as Amazon Web Services, IBM Cloud , Google Cloud Platform, DigitalOcean, Linode, Microsoft Azure, Oracle Cloud Infrastructure, OVH, or VMware vSphere as well as OpenNebula and OpenStack.","title":"6th August 2021"},{"location":"newsletter/2021_08_06/#devops","text":"","title":"DevOps"},{"location":"newsletter/2021_08_06/#infrastructure-as-code","text":"","title":"Infrastructure as Code"},{"location":"newsletter/2021_08_06/#terraform","text":"New: Introduce terraform and how to handle RDS secrets. Terraform is an open-source infrastructure as code software tool created by HashiCorp. It enables users to define and provision a datacenter infrastructure using an awful high-level configuration language known as Hashicorp Configuration Language (HCL), or optionally JSON. Terraform supports a number of cloud infrastructure providers such as Amazon Web Services, IBM Cloud , Google Cloud Platform, DigitalOcean, Linode, Microsoft Azure, Oracle Cloud Infrastructure, OVH, or VMware vSphere as well as OpenNebula and OpenStack.","title":"Terraform"},{"location":"newsletter/2021_08_12/","text":"Coding \u2691 Python \u2691 Python Snippets \u2691 New: Explain how to find a static file of a python module. import pkg_resources file_path = pkg_resources . resource_filename ( \"my_package\" , \"assets/config.yaml\" ), New: Explain how to delete a file. import os os . remove ( 'demofile.txt' ) New: Explain how to measure elapsed time between lines of code. import time start = time . time () print ( \"hello\" ) end = time . time () print ( end - start ) pexpect \u2691 New: Explain how to read the output of a command run by pexpect. import sys import pexpect child = pexpect . spawn ( 'ls' ) child . logfile = sys . stdout child . expect ( pexpect . EOF ) DevOps \u2691 Infrastructure as Code \u2691 Terraform \u2691 New: Explain how to ignore the change of an attribute. resource \"aws_instance\" \"example\" { # ... lifecycle { ignore_changes = [ # Ignore changes to tags, e.g. because a management agent # updates these based on some ruleset managed elsewhere. tags, ] } } New: Explain how to define the default value of an variable that contains an object as empty. variable \"database\" { type = object({ size = number instance_type = string storage_type = string engine = string engine_version = string parameter_group_name = string multi_az = bool }) default = null New: Explain how to do a conditional if a variable is not null. resource \"aws_db_instance\" \"instance\" { count = var.database == null ? 0 : 1 ...","title":"12th August 2021"},{"location":"newsletter/2021_08_12/#coding","text":"","title":"Coding"},{"location":"newsletter/2021_08_12/#python","text":"","title":"Python"},{"location":"newsletter/2021_08_12/#python-snippets","text":"New: Explain how to find a static file of a python module. import pkg_resources file_path = pkg_resources . resource_filename ( \"my_package\" , \"assets/config.yaml\" ), New: Explain how to delete a file. import os os . remove ( 'demofile.txt' ) New: Explain how to measure elapsed time between lines of code. import time start = time . time () print ( \"hello\" ) end = time . time () print ( end - start )","title":"Python Snippets"},{"location":"newsletter/2021_08_12/#pexpect","text":"New: Explain how to read the output of a command run by pexpect. import sys import pexpect child = pexpect . spawn ( 'ls' ) child . logfile = sys . stdout child . expect ( pexpect . EOF )","title":"pexpect"},{"location":"newsletter/2021_08_12/#devops","text":"","title":"DevOps"},{"location":"newsletter/2021_08_12/#infrastructure-as-code","text":"","title":"Infrastructure as Code"},{"location":"newsletter/2021_08_12/#terraform","text":"New: Explain how to ignore the change of an attribute. resource \"aws_instance\" \"example\" { # ... lifecycle { ignore_changes = [ # Ignore changes to tags, e.g. because a management agent # updates these based on some ruleset managed elsewhere. tags, ] } } New: Explain how to define the default value of an variable that contains an object as empty. variable \"database\" { type = object({ size = number instance_type = string storage_type = string engine = string engine_version = string parameter_group_name = string multi_az = bool }) default = null New: Explain how to do a conditional if a variable is not null. resource \"aws_db_instance\" \"instance\" { count = var.database == null ? 0 : 1 ...","title":"Terraform"},{"location":"newsletter/2021_08_13/","text":"Projects \u2691 New: Introduce pynbox the inbox management tool. Pynbox is a tool to improve the management of ideas, tasks, references, suggestions when I'm not in front of the computer. Right now I've got Markor for Android to register these quicknotes, but the reality is that I don't act upon them, so it's just a log of tasks that never get done, and ideas, references and suggestions that aren't registered in my knowledge or media management systems. On the computer there are also cases of tasks that are not worth registering in the task management system, or ideas that I get at a moment but don't have time to process at the moment. The idea then is to automatically sync the Android quicknote with syncthing, and have a special format for the file that allows pynbox to extract the elements from that file to the \"inbox system\". For example: + t. buy groceries tv. IT crowd i. Improve the inbox management I want a system to improve ... Gets introduced in the \"inbox system\" as a task, a TV suggestion and an idea. New: Introduce nyxt as a solution for a better browser. I've just stumbled upon nyxt ( code ), and it looks superb. Coding \u2691 Python \u2691 rich \u2691 New: Explain how to build pretty tables with rich. from rich.console import Console from rich.table import Table table = Table ( title = \"Star Wars Movies\" ) table . add_column ( \"Released\" , justify = \"right\" , style = \"cyan\" , no_wrap = True ) table . add_column ( \"Title\" , style = \"magenta\" ) table . add_column ( \"Box Office\" , justify = \"right\" , style = \"green\" ) table . add_row ( \"Dec 20, 2019\" , \"Star Wars: The Rise of Skywalker\" , \"$952,110,690\" ) table . add_row ( \"May 25, 2018\" , \"Solo: A Star Wars Story\" , \"$393,151,347\" ) table . add_row ( \"Dec 15, 2017\" , \"Star Wars Ep. V111: The Last Jedi\" , \"$1,332,539,889\" ) table . add_row ( \"Dec 16, 2016\" , \"Rogue One: A Star Wars Story\" , \"$1,332,439,889\" ) console = Console () console . print ( table ) New: Explain how to print pretty text with rich. from rich.console import Console from rich.text import Text console = Console () text = Text . assemble (( \"Hello\" , \"bold magenta\" ), \" World!\" ) console . print ( text )","title":"13th August 2021"},{"location":"newsletter/2021_08_13/#projects","text":"New: Introduce pynbox the inbox management tool. Pynbox is a tool to improve the management of ideas, tasks, references, suggestions when I'm not in front of the computer. Right now I've got Markor for Android to register these quicknotes, but the reality is that I don't act upon them, so it's just a log of tasks that never get done, and ideas, references and suggestions that aren't registered in my knowledge or media management systems. On the computer there are also cases of tasks that are not worth registering in the task management system, or ideas that I get at a moment but don't have time to process at the moment. The idea then is to automatically sync the Android quicknote with syncthing, and have a special format for the file that allows pynbox to extract the elements from that file to the \"inbox system\". For example: + t. buy groceries tv. IT crowd i. Improve the inbox management I want a system to improve ... Gets introduced in the \"inbox system\" as a task, a TV suggestion and an idea. New: Introduce nyxt as a solution for a better browser. I've just stumbled upon nyxt ( code ), and it looks superb.","title":"Projects"},{"location":"newsletter/2021_08_13/#coding","text":"","title":"Coding"},{"location":"newsletter/2021_08_13/#python","text":"","title":"Python"},{"location":"newsletter/2021_08_13/#rich","text":"New: Explain how to build pretty tables with rich. from rich.console import Console from rich.table import Table table = Table ( title = \"Star Wars Movies\" ) table . add_column ( \"Released\" , justify = \"right\" , style = \"cyan\" , no_wrap = True ) table . add_column ( \"Title\" , style = \"magenta\" ) table . add_column ( \"Box Office\" , justify = \"right\" , style = \"green\" ) table . add_row ( \"Dec 20, 2019\" , \"Star Wars: The Rise of Skywalker\" , \"$952,110,690\" ) table . add_row ( \"May 25, 2018\" , \"Solo: A Star Wars Story\" , \"$393,151,347\" ) table . add_row ( \"Dec 15, 2017\" , \"Star Wars Ep. V111: The Last Jedi\" , \"$1,332,539,889\" ) table . add_row ( \"Dec 16, 2016\" , \"Rogue One: A Star Wars Story\" , \"$1,332,439,889\" ) console = Console () console . print ( table ) New: Explain how to print pretty text with rich. from rich.console import Console from rich.text import Text console = Console () text = Text . assemble (( \"Hello\" , \"bold magenta\" ), \" World!\" ) console . print ( text )","title":"rich"},{"location":"newsletter/2021_08_20/","text":"Projects \u2691 New: Introduce the shared accounting seed project. I use beancount for my personal accounting, I'd like to have a system that integrates more less easily with beancount and let's do a shared accounting with other people, for example in trips. I've used settle up in the past but it requires access to their servers, and an account linked to google, facebook or one you register in their servers. I've looked at facto but it uses a logic that doesn't apply to my case, it does a heavy use on a common account, instead +of minimizing the transactions between the people. I also tried tabby , even though they still don't support Docker , but it doesn't suit my case either :(. Until a new solution shows up, I'll go with Tricky Tripper available in F-Droid, and manage the expenses myself and periodically send the html reports to the rest of the group. Improvement: Add quickwit as an interesting database solution for personal knowledge search engine. New: Promote the automation of email management project to seedling. Coding \u2691 Python \u2691 asyncio \u2691 New: Introduce the asyncio library. asyncio is a library to write concurrent code using the async/await syntax. asyncio is used as a foundation for multiple Python asynchronous frameworks that provide high-performance network and web-servers, database connection libraries, distributed task queues, etc. asyncio is often a perfect fit for IO-bound and high-level structured network code. FastAPI \u2691 Improvement: Add link to the Awesome FastAPI page. SQLite \u2691 Improvement: Add rqlite as an interesting distributed solution of. Life Management \u2691 Email Management \u2691 Email Automation \u2691 New: Explain how setup an infrastructure to automate. Operative Systems \u2691 Linux \u2691 afew \u2691 New: Introduce afew. afew is an initial tagging script for notmuch mail . Its basic task is to provide automatic tagging each time new mail is registered with notmuch . In a classic setup, you might call it after notmuch new in an offlineimap post sync hook. alot \u2691 New: Introduce alot. alot is a terminal-based mail user agent based on the notmuch mail indexer . It is written in python using the urwid toolkit and features a modular and command prompt driven interface to provide a full MUA experience.","title":"20th August 2021"},{"location":"newsletter/2021_08_20/#projects","text":"New: Introduce the shared accounting seed project. I use beancount for my personal accounting, I'd like to have a system that integrates more less easily with beancount and let's do a shared accounting with other people, for example in trips. I've used settle up in the past but it requires access to their servers, and an account linked to google, facebook or one you register in their servers. I've looked at facto but it uses a logic that doesn't apply to my case, it does a heavy use on a common account, instead +of minimizing the transactions between the people. I also tried tabby , even though they still don't support Docker , but it doesn't suit my case either :(. Until a new solution shows up, I'll go with Tricky Tripper available in F-Droid, and manage the expenses myself and periodically send the html reports to the rest of the group. Improvement: Add quickwit as an interesting database solution for personal knowledge search engine. New: Promote the automation of email management project to seedling.","title":"Projects"},{"location":"newsletter/2021_08_20/#coding","text":"","title":"Coding"},{"location":"newsletter/2021_08_20/#python","text":"","title":"Python"},{"location":"newsletter/2021_08_20/#asyncio","text":"New: Introduce the asyncio library. asyncio is a library to write concurrent code using the async/await syntax. asyncio is used as a foundation for multiple Python asynchronous frameworks that provide high-performance network and web-servers, database connection libraries, distributed task queues, etc. asyncio is often a perfect fit for IO-bound and high-level structured network code.","title":"asyncio"},{"location":"newsletter/2021_08_20/#fastapi","text":"Improvement: Add link to the Awesome FastAPI page.","title":"FastAPI"},{"location":"newsletter/2021_08_20/#sqlite","text":"Improvement: Add rqlite as an interesting distributed solution of.","title":"SQLite"},{"location":"newsletter/2021_08_20/#life-management","text":"","title":"Life Management"},{"location":"newsletter/2021_08_20/#email-management","text":"","title":"Email Management"},{"location":"newsletter/2021_08_20/#email-automation","text":"New: Explain how setup an infrastructure to automate.","title":"Email Automation"},{"location":"newsletter/2021_08_20/#operative-systems","text":"","title":"Operative Systems"},{"location":"newsletter/2021_08_20/#linux","text":"","title":"Linux"},{"location":"newsletter/2021_08_20/#afew","text":"New: Introduce afew. afew is an initial tagging script for notmuch mail . Its basic task is to provide automatic tagging each time new mail is registered with notmuch . In a classic setup, you might call it after notmuch new in an offlineimap post sync hook.","title":"afew"},{"location":"newsletter/2021_08_20/#alot","text":"New: Introduce alot. alot is a terminal-based mail user agent based on the notmuch mail indexer . It is written in python using the urwid toolkit and features a modular and command prompt driven interface to provide a full MUA experience.","title":"alot"},{"location":"newsletter/2021_08_23/","text":"Coding \u2691 Python \u2691 New: Add schedule to interesting libraries to explore. schedule is a Python job scheduling for humans. Run Python functions (or any other callable) periodically using a friendly syntax. Contact \u2691 Correction: Update the XMPP address. Riseup has stopped giving support for XMPP :(","title":"23rd August 2021"},{"location":"newsletter/2021_08_23/#coding","text":"","title":"Coding"},{"location":"newsletter/2021_08_23/#python","text":"New: Add schedule to interesting libraries to explore. schedule is a Python job scheduling for humans. Run Python functions (or any other callable) periodically using a friendly syntax.","title":"Python"},{"location":"newsletter/2021_08_23/#contact","text":"Correction: Update the XMPP address. Riseup has stopped giving support for XMPP :(","title":"Contact"},{"location":"newsletter/2021_08_25/","text":"Activism \u2691 Antifascism \u2691 New: Introduce antifascism. Antifascism is a method of politics, a locus of individual and group self-indentification, it's a transnational movement that adapted preexisting socialist, anarchist, and communist currents to a sudden need to react to the fascist menace ( Mark p. 11 ). It's based on the idea that any oppression form can't be allowed, and should be actively fought with whatever means are necessary.","title":"25th August 2021"},{"location":"newsletter/2021_08_25/#activism","text":"","title":"Activism"},{"location":"newsletter/2021_08_25/#antifascism","text":"New: Introduce antifascism. Antifascism is a method of politics, a locus of individual and group self-indentification, it's a transnational movement that adapted preexisting socialist, anarchist, and communist currents to a sudden need to react to the fascist menace ( Mark p. 11 ). It's based on the idea that any oppression form can't be allowed, and should be actively fought with whatever means are necessary.","title":"Antifascism"},{"location":"newsletter/2021_08_26/","text":"Activism \u2691 New: Introduce Anti-transphobia. Anti-transphobia being reductionist is the opposition to the collection of ideas and phenomena that encompass a range of negative attitudes, feelings or actions towards transgender people or transness in general. Transphobia can include fear, aversion, hatred, violence, anger, or discomfort felt or expressed towards people who do not conform to social gender expectations. It is often expressed alongside homophobic views and hence is often considered an aspect of homophobia. New: Introduce arguments against terf ideology. TERF is an acronym for trans-exclusionary radical feminist . The term originally applied to the minority of feminists that expressed transphobic sentiments such as the rejection of the assertion that trans women are women, the exclusion of trans women from women's spaces, and opposition to transgender rights legislation. The meaning has since expanded to refer more broadly to people with trans-exclusionary views who may have no involvement with radical feminism.","title":"26th August 2021"},{"location":"newsletter/2021_08_26/#activism","text":"New: Introduce Anti-transphobia. Anti-transphobia being reductionist is the opposition to the collection of ideas and phenomena that encompass a range of negative attitudes, feelings or actions towards transgender people or transness in general. Transphobia can include fear, aversion, hatred, violence, anger, or discomfort felt or expressed towards people who do not conform to social gender expectations. It is often expressed alongside homophobic views and hence is often considered an aspect of homophobia. New: Introduce arguments against terf ideology. TERF is an acronym for trans-exclusionary radical feminist . The term originally applied to the minority of feminists that expressed transphobic sentiments such as the rejection of the assertion that trans women are women, the exclusion of trans women from women's spaces, and opposition to transgender rights legislation. The meaning has since expanded to refer more broadly to people with trans-exclusionary views who may have no involvement with radical feminism.","title":"Activism"},{"location":"newsletter/2021_09/","text":"Projects \u2691 New: Introduce the pomodoro command line seed project. Command line to help with the pomodoro workflow , besides the basic stuff it will interact with the task manager, activitywatch and the notifications system. New: Introduce the ordered list of digital gardens project. Use best-of-lists to create an awesome list of digital gardens. Correction: Clean up deprecated projects. Coding \u2691 Python \u2691 New: Add tryceratops to interesting linters to try. tryceratops is a linter of exceptions. Python Snippets \u2691 New: Document when to use isinstance and when to use type . isinstance takes into account inheritance, while type doesn't. So if you want to make sure you're dealing with a specific class, and not any of it's parents or subclasses, use type(obj) == class . DevOps \u2691 Infrastructure as Code \u2691 Helmfile \u2691 Correction: Improve the helmfile chart update process. Updating charts with helmfile is easy as long as you don't use environments, you run helmfile deps , then helmfile diff and finally helmfile apply . The tricky business comes when you want to use environments to reuse your helmfile code and don't repeat yourself. I've updated the process to include this case. New: Document the directory and files structure for multi-environment projects. New: Document how to use helmfile environments to follow DRY. New: Document how to avoiding code repetition. Besides environments, helmfile gives other useful tricks to prevent the illness of code repetition, such as using release templates , or layering the state . New: Document how to manage dependencies between the charts, to be able to use concurrency. Helmfile support concurrency with the option --concurrency=N so we can take advantage of it and improve our deployment speed, but to ensure it works as expected we have to define the dependencies among charts. For example, if an application needs a database, it has to be deployed before hand. Operative Systems \u2691 Linux \u2691 Linux Snippets \u2691 New: Document how to bypass client SSL certificate with a cli tool. Websites that require clients to authorize with an TLS certificate are difficult to interact with through command line tools that don't support this feature. To solve it, we can use a transparent proxy that does the exchange for us. Jellyfin \u2691 Correction: Fix the stuck at login page error. If you use jfa-go for the invites, you may need to regenerate all the user profiles , so that the problem is not introduced again.","title":"September of 2021"},{"location":"newsletter/2021_09/#projects","text":"New: Introduce the pomodoro command line seed project. Command line to help with the pomodoro workflow , besides the basic stuff it will interact with the task manager, activitywatch and the notifications system. New: Introduce the ordered list of digital gardens project. Use best-of-lists to create an awesome list of digital gardens. Correction: Clean up deprecated projects.","title":"Projects"},{"location":"newsletter/2021_09/#coding","text":"","title":"Coding"},{"location":"newsletter/2021_09/#python","text":"New: Add tryceratops to interesting linters to try. tryceratops is a linter of exceptions.","title":"Python"},{"location":"newsletter/2021_09/#python-snippets","text":"New: Document when to use isinstance and when to use type . isinstance takes into account inheritance, while type doesn't. So if you want to make sure you're dealing with a specific class, and not any of it's parents or subclasses, use type(obj) == class .","title":"Python Snippets"},{"location":"newsletter/2021_09/#devops","text":"","title":"DevOps"},{"location":"newsletter/2021_09/#infrastructure-as-code","text":"","title":"Infrastructure as Code"},{"location":"newsletter/2021_09/#helmfile","text":"Correction: Improve the helmfile chart update process. Updating charts with helmfile is easy as long as you don't use environments, you run helmfile deps , then helmfile diff and finally helmfile apply . The tricky business comes when you want to use environments to reuse your helmfile code and don't repeat yourself. I've updated the process to include this case. New: Document the directory and files structure for multi-environment projects. New: Document how to use helmfile environments to follow DRY. New: Document how to avoiding code repetition. Besides environments, helmfile gives other useful tricks to prevent the illness of code repetition, such as using release templates , or layering the state . New: Document how to manage dependencies between the charts, to be able to use concurrency. Helmfile support concurrency with the option --concurrency=N so we can take advantage of it and improve our deployment speed, but to ensure it works as expected we have to define the dependencies among charts. For example, if an application needs a database, it has to be deployed before hand.","title":"Helmfile"},{"location":"newsletter/2021_09/#operative-systems","text":"","title":"Operative Systems"},{"location":"newsletter/2021_09/#linux","text":"","title":"Linux"},{"location":"newsletter/2021_09/#linux-snippets","text":"New: Document how to bypass client SSL certificate with a cli tool. Websites that require clients to authorize with an TLS certificate are difficult to interact with through command line tools that don't support this feature. To solve it, we can use a transparent proxy that does the exchange for us.","title":"Linux Snippets"},{"location":"newsletter/2021_09/#jellyfin","text":"Correction: Fix the stuck at login page error. If you use jfa-go for the invites, you may need to regenerate all the user profiles , so that the problem is not introduced again.","title":"Jellyfin"},{"location":"newsletter/2021_09_25/","text":"Projects \u2691 New: Introduce the pomodoro command line seed project. Command line to help with the pomodoro workflow , besides the basic stuff it will interact with the task manager, activitywatch and the notifications system. New: Introduce the ordered list of digital gardens project. Use best-of-lists to create an awesome list of digital gardens. Coding \u2691 Python \u2691 New: Add tryceratops to interesting linters to try. tryceratops is a linter of exceptions. Operative Systems \u2691 Linux \u2691 Jellyfin \u2691 Correction: Fix the stuck at login page error. If you use jfa-go for the invites, you may need to regenerate all the user profiles , so that the problem is not introduced again.","title":"25th September 2021"},{"location":"newsletter/2021_09_25/#projects","text":"New: Introduce the pomodoro command line seed project. Command line to help with the pomodoro workflow , besides the basic stuff it will interact with the task manager, activitywatch and the notifications system. New: Introduce the ordered list of digital gardens project. Use best-of-lists to create an awesome list of digital gardens.","title":"Projects"},{"location":"newsletter/2021_09_25/#coding","text":"","title":"Coding"},{"location":"newsletter/2021_09_25/#python","text":"New: Add tryceratops to interesting linters to try. tryceratops is a linter of exceptions.","title":"Python"},{"location":"newsletter/2021_09_25/#operative-systems","text":"","title":"Operative Systems"},{"location":"newsletter/2021_09_25/#linux","text":"","title":"Linux"},{"location":"newsletter/2021_09_25/#jellyfin","text":"Correction: Fix the stuck at login page error. If you use jfa-go for the invites, you may need to regenerate all the user profiles , so that the problem is not introduced again.","title":"Jellyfin"},{"location":"newsletter/2021_09_28/","text":"Projects \u2691 Correction: Clean up deprecated projects. Coding \u2691 Python \u2691 Python Snippets \u2691 New: Document when to use isinstance and when to use type . isinstance takes into account inheritance, while type doesn't. So if you want to make sure you're dealing with a specific class, and not any of it's parents or subclasses, use type(obj) == class . DevOps \u2691 Infrastructure as Code \u2691 Helmfile \u2691 Correction: Improve the helmfile chart update process. Updating charts with helmfile is easy as long as you don't use environments, you run helmfile deps , then helmfile diff and finally helmfile apply . The tricky business comes when you want to use environments to reuse your helmfile code and don't repeat yourself. I've updated the process to include this case. New: Document the directory and files structure for multi-environment projects. New: Document how to use helmfile environments to follow DRY. New: Document how to avoiding code repetition. Besides environments, helmfile gives other useful tricks to prevent the illness of code repetition, such as using release templates , or layering the state . New: Document how to manage dependencies between the charts, to be able to use concurrency. Helmfile support concurrency with the option --concurrency=N so we can take advantage of it and improve our deployment speed, but to ensure it works as expected we have to define the dependencies among charts. For example, if an application needs a database, it has to be deployed before hand. Operative Systems \u2691 Linux \u2691 Linux Snippets \u2691 New: Document how to bypass client SSL certificate with a cli tool. Websites that require clients to authorize with an TLS certificate are difficult to interact with through command line tools that don't support this feature. To solve it, we can use a transparent proxy that does the exchange for us.","title":"28th September 2021"},{"location":"newsletter/2021_09_28/#projects","text":"Correction: Clean up deprecated projects.","title":"Projects"},{"location":"newsletter/2021_09_28/#coding","text":"","title":"Coding"},{"location":"newsletter/2021_09_28/#python","text":"","title":"Python"},{"location":"newsletter/2021_09_28/#python-snippets","text":"New: Document when to use isinstance and when to use type . isinstance takes into account inheritance, while type doesn't. So if you want to make sure you're dealing with a specific class, and not any of it's parents or subclasses, use type(obj) == class .","title":"Python Snippets"},{"location":"newsletter/2021_09_28/#devops","text":"","title":"DevOps"},{"location":"newsletter/2021_09_28/#infrastructure-as-code","text":"","title":"Infrastructure as Code"},{"location":"newsletter/2021_09_28/#helmfile","text":"Correction: Improve the helmfile chart update process. Updating charts with helmfile is easy as long as you don't use environments, you run helmfile deps , then helmfile diff and finally helmfile apply . The tricky business comes when you want to use environments to reuse your helmfile code and don't repeat yourself. I've updated the process to include this case. New: Document the directory and files structure for multi-environment projects. New: Document how to use helmfile environments to follow DRY. New: Document how to avoiding code repetition. Besides environments, helmfile gives other useful tricks to prevent the illness of code repetition, such as using release templates , or layering the state . New: Document how to manage dependencies between the charts, to be able to use concurrency. Helmfile support concurrency with the option --concurrency=N so we can take advantage of it and improve our deployment speed, but to ensure it works as expected we have to define the dependencies among charts. For example, if an application needs a database, it has to be deployed before hand.","title":"Helmfile"},{"location":"newsletter/2021_09_28/#operative-systems","text":"","title":"Operative Systems"},{"location":"newsletter/2021_09_28/#linux","text":"","title":"Linux"},{"location":"newsletter/2021_09_28/#linux-snippets","text":"New: Document how to bypass client SSL certificate with a cli tool. Websites that require clients to authorize with an TLS certificate are difficult to interact with through command line tools that don't support this feature. To solve it, we can use a transparent proxy that does the exchange for us.","title":"Linux Snippets"},{"location":"newsletter/2021_10/","text":"Activism \u2691 Antifascism \u2691 Antifascist Actions \u2691 New: A fake company and five million recycled flyers. A group of artists belonging to the Center for political beauty created a fake company Flyerservice Hahn and convinced more than 80 regional sections of the far right party AfD to hire them to deliver their electoral propaganda. They gathered five million flyers, with a total weight of 72 tons. They justify that they wouldn't be able to lie to the people, so they did nothing in the broader sense of the word. They declared that they are the \"world wide leader in the non-delivery of nazi propaganda\" . At the start of the electoral campaign, they went to the AfD stands, and they let their members to give them flyers the throw them to the closest bin. \"It's something that any citizen can freely do, we have only industrialized the process\". They've done a crowdfunding to fund the legal process that may result. Feminism \u2691 Privileges \u2691 New: Feminist analysis of privileges and rights. Privileges are a group of special structural benefits, social advantages, that a group holds over another. So they are elements that should be removed from our lives. Some of the topics included are: What's the difference between privilege and right What can we do to fight the privileges? Life Management \u2691 Book Management \u2691 New: Introduce the book management concept. Book management is the set of systems and processes to get and categorize books so it's easy to browse and discover new content. It involves the next actions: Automatically index and download metadata of new books. Notify the user when a new book is added. Monitor the books of an author, and get them once they are released. Send books to the e-reader. A nice interface to browse the existent library, with the possibility of filtering by author, genre, years, tags or series. An interface to preview or read the items. An interface to rate and review library items. An interface to discover new content based on the ratings and item metadata. I haven't yet found a single piece of software that fulfills all these needs, in the article I tell you about Readarr , Calibre-web , [calibre](( https://manual.calibre-ebook.com/ ), Polar bookself , GCStar , and how they interact with each other. Improvement: Add link to the calibre-web kobo integration project. Coding \u2691 Python \u2691 Full screen applications \u2691 New: Testing full screen applications. New: Pass more than one key. To map an action to two key presses use kb.add('g', 'g') . New: Add note on how to debug the styles of the components. Set the style to bg:#dc322f and it will be highlighted in red. Click \u2691 New: Invoke other commands from a command. This is a pattern that is generally discouraged with Click, but possible nonetheless. For this, you can use the Context.invoke() or Context.forward() methods. Optimization \u2691 New: Add tips on how to optimize your python command line tools. Minimize the relative import statements on command line tools : When developing a library, it's common to expose the main objects into the package __init__.py under the variable __all__ . The problem with command line programs is that each time you run the command it will load those objects, which can mean an increase of 0.5s or even a second for each command, which is unacceptable. * Don't dynamically install the package : If you install the package with pip install -e . you will see an increase on the load time of ~0.2s. It is useful to develop the package, but when you use it, do so from a virtualenv that installs it directly without the -e flag. Flask Restplus \u2691 New: Introduce the Flask-RESTPlus library. Flask-RESTPlus is an extension for Flask that adds support for quickly building REST APIs, but I'd use FastAPI instead. Pytest \u2691 New: Exclude the if TYPE_CHECKING code from the coverage. If you want other code to be excluded , for example the statements inside the if TYPE_CHECKING: add to your pyproject.toml : [tool.coverage.report] exclude_lines = [ # Have to re-enable the standard pragma 'pragma: no cover' , # Type checking can not be tested 'if TYPE_CHECKING:' , ] Python Snippets \u2691 New: Check if a dictionary is a subset of another. If you have two dictionaries big = {'a': 1, 'b': 2, 'c':3} and small = {'c': 3, 'a': 1} , and want to check whether small is a subset of big , use the next snippet: >>> small . items () <= big . items () True Correction: Group or sort a list of dictionaries or objects by a specific key. Improve previous method with the concepts learned from the official docs Particularly improve the sorting by multiple keys with the next function: >>> def multisort ( xs , specs ): for key , reverse in reversed ( specs ): xs . sort ( key = attrgetter ( key ), reverse = reverse ) return xs >>> multisort ( list ( student_objects ), (( 'grade' , True ), ( 'age' , False ))) [( 'dave' , 'B' , 10 ), ( 'jane' , 'B' , 12 ), ( 'john' , 'A' , 15 )] Correction: Install default directories and files for a command line program. I've been trying for a long time to configure setup.py to run the required steps to configure the required directories and files when doing pip install without success. Finally, I decided that the program itself should create the data once the FileNotFoundError exception is found. That way, you don't penalize the load time because if the file or directory exists, that code is not run. New: Locate element in list. a = [ 'a' , 'b' ] index = a . index ( 'b' ) New: Transpose a list of lists. >>> l = [[ 1 , 2 , 3 ],[ 4 , 5 , 6 ],[ 7 , 8 , 9 ]] >>> [ list ( i ) for i in zip ( * l )] ... [[ 1 , 4 , 7 ], [ 2 , 5 , 8 ], [ 3 , 6 , 9 ]] New: Check the type of a list of strings. def _is_list_of_lists ( data : Any ) -> bool : \"\"\"Check if data is a list of strings.\"\"\" if data and isinstance ( data , list ): return all ( isinstance ( elem , list ) for elem in data ) else : return False Prompt Toolkit \u2691 New: Basic concepts of building full screen applications with python prompt toolkit. prompt_toolkit can be used to create complex full screen terminal applications. Typically, an application consists of a layout (to describe the graphical part) and a set of key bindings. In the section we cover: The layout The controls How to use key bindings How to apply styles A difficult ordered list of examples to get a grasp of these concepts with simple working code. Pydantic \u2691 New: Copy produces copy that modifies the original. When copying a model, changing the value of an attribute on the copy updates the value of the attribute on the original. This only happens if deep != True . To fix it use: model.copy(deep=True) . Promql \u2691 New: Generating range vectors from return values in Prometheus queries. DevOps \u2691 Infrastructure as Code \u2691 Helmfile \u2691 Correction: Use environment name instead of get values. Instead of .Environment.Name , in theory you could have used .Vars | get \"environment\" , which could have prevented the variables and secrets of the default environment will need to be called default_values.yaml , and default_secrets.yaml , which is misleading. But you can't use .Values in the helmfile.yaml as it's not loaded when the file is parsed, and you get an error. A solution would be to layer the helmfile state files but I wasn't able to make it work. New: How to install a chart only in one environment. environments : default : production : --- releases : - name : newrelic-agent installed : {{ eq .Environment.Name \"production\" | toYaml }} # snip New: Add note that templates can't be used inside the secrets. See this issue Terraform \u2691 New: How to do elif conditionals in terraform. locals { test = \"${ condition ? value : (elif-condition ? elif-value : else-value)}\" } New: How to enable debug traces. You can set the TF_LOG environmental variable to one of the log levels TRACE , DEBUG , INFO , WARN or ERROR to change the verbosity of the logs. Helm Secrets \u2691 Correction: Update the repository url. The last fork is dead, long live the fork New: How to install the plugin. Continuous Integration \u2691 Flakehell \u2691 New: Troubleshoot the 'Namespace' object has no attribute 'extended_default_ignore' error. Add to your pyproject.toml : [tool.flakehell] extended_default_ignore = [] New: Latest version is broken. It returns an ImportError: cannot import name 'MergedConfigParser' from 'flake8.options.config' , wait for the issue to be solved before upgrading. Dependency managers \u2691 New: Sync the virtualenv libraries with the requirements files. python - m piptools sync requirements . txt requirements - dev . txt Correction: Use -c instead of -r in the nested requirement files. To avoid duplication of version pins. Operative Systems \u2691 Linux \u2691 Dynamic DNS \u2691 New: Introduce the Dynamic DNS concept. Dynamic DNS (DDNS) is a method of automatically updating a name server in the Domain Name Server (DNS), often in real time, with the active DDNS configuration of its configured hostnames, addresses or other information. Hard drive health \u2691 New: Taking care of your hard drives. Hard drives die, so we must be ready for that to happen. There are several solutions, such as using RAID to minimize the impact of a disk loss, but even then, we should monitor the bad sectors to see when are our disks dying. In the article we talk about S.M.A.R.T and how to solve some hard drive problems. Kitty \u2691 New: Introduce kitty the terminal emulator. kitty is a fast, feature-rich, GPU based terminal emulator written in C and Python with nice features for the keyboard driven humans like me. New: Scrollback when ssh into a machine doesn't work. This happens because the kitty terminfo files are not available on the server. You can ssh in using the following command which will automatically copy the terminfo files to the server: kitty +kitten ssh myserver New: Enable infinite scrollback history. To make the history scrollback infinite add the next lines: scrollback_lines -1 scrollback_pager_history_size 0 New: Reasons to migrate from urxvt to kitty. It doesn't fuck up your terminal colors. You can use peek to record your screen. Easier to extend. Peek \u2691 Correction: Add note that it works with kitty. Arts \u2691 Drawing \u2691 New: How to draw Ellipses. Ellipses are the next basic shape we're going to study (after the lines). They are extremely important and notoriously annoying to draw. Important because we're going to be using ellipses in 2D space to represent circles that exist in 3D space. In this section we: Introduce the basic concepts surrounding the ellipses How to draw them . Exercise Pool \u2691 New: Add the Tables of ellipses drawing exercise. This exercise is meant to get you used to drawing ellipses, in a variety of sizes, orientations and degrees. It also sets out a clear space each ellipse is meant to occupy, giving us a means to assess whether or not an ellipse was successful, or if there were visible mistakes (where it went outside of its allotted space, or ended up falling short). Practicing against set criteria, with a way to judge success/failure is an important element of learning. There's nothing wrong with failure - it's an opportunity to learn. Having a clearly defined task allows us to analyze those failures and make the most of them.","title":"October of 2021"},{"location":"newsletter/2021_10/#activism","text":"","title":"Activism"},{"location":"newsletter/2021_10/#antifascism","text":"","title":"Antifascism"},{"location":"newsletter/2021_10/#antifascist-actions","text":"New: A fake company and five million recycled flyers. A group of artists belonging to the Center for political beauty created a fake company Flyerservice Hahn and convinced more than 80 regional sections of the far right party AfD to hire them to deliver their electoral propaganda. They gathered five million flyers, with a total weight of 72 tons. They justify that they wouldn't be able to lie to the people, so they did nothing in the broader sense of the word. They declared that they are the \"world wide leader in the non-delivery of nazi propaganda\" . At the start of the electoral campaign, they went to the AfD stands, and they let their members to give them flyers the throw them to the closest bin. \"It's something that any citizen can freely do, we have only industrialized the process\". They've done a crowdfunding to fund the legal process that may result.","title":"Antifascist Actions"},{"location":"newsletter/2021_10/#feminism","text":"","title":"Feminism"},{"location":"newsletter/2021_10/#privileges","text":"New: Feminist analysis of privileges and rights. Privileges are a group of special structural benefits, social advantages, that a group holds over another. So they are elements that should be removed from our lives. Some of the topics included are: What's the difference between privilege and right What can we do to fight the privileges?","title":"Privileges"},{"location":"newsletter/2021_10/#life-management","text":"","title":"Life Management"},{"location":"newsletter/2021_10/#book-management","text":"New: Introduce the book management concept. Book management is the set of systems and processes to get and categorize books so it's easy to browse and discover new content. It involves the next actions: Automatically index and download metadata of new books. Notify the user when a new book is added. Monitor the books of an author, and get them once they are released. Send books to the e-reader. A nice interface to browse the existent library, with the possibility of filtering by author, genre, years, tags or series. An interface to preview or read the items. An interface to rate and review library items. An interface to discover new content based on the ratings and item metadata. I haven't yet found a single piece of software that fulfills all these needs, in the article I tell you about Readarr , Calibre-web , [calibre](( https://manual.calibre-ebook.com/ ), Polar bookself , GCStar , and how they interact with each other. Improvement: Add link to the calibre-web kobo integration project.","title":"Book Management"},{"location":"newsletter/2021_10/#coding","text":"","title":"Coding"},{"location":"newsletter/2021_10/#python","text":"","title":"Python"},{"location":"newsletter/2021_10/#full-screen-applications","text":"New: Testing full screen applications. New: Pass more than one key. To map an action to two key presses use kb.add('g', 'g') . New: Add note on how to debug the styles of the components. Set the style to bg:#dc322f and it will be highlighted in red.","title":"Full screen applications"},{"location":"newsletter/2021_10/#click","text":"New: Invoke other commands from a command. This is a pattern that is generally discouraged with Click, but possible nonetheless. For this, you can use the Context.invoke() or Context.forward() methods.","title":"Click"},{"location":"newsletter/2021_10/#optimization","text":"New: Add tips on how to optimize your python command line tools. Minimize the relative import statements on command line tools : When developing a library, it's common to expose the main objects into the package __init__.py under the variable __all__ . The problem with command line programs is that each time you run the command it will load those objects, which can mean an increase of 0.5s or even a second for each command, which is unacceptable. * Don't dynamically install the package : If you install the package with pip install -e . you will see an increase on the load time of ~0.2s. It is useful to develop the package, but when you use it, do so from a virtualenv that installs it directly without the -e flag.","title":"Optimization"},{"location":"newsletter/2021_10/#flask-restplus","text":"New: Introduce the Flask-RESTPlus library. Flask-RESTPlus is an extension for Flask that adds support for quickly building REST APIs, but I'd use FastAPI instead.","title":"Flask Restplus"},{"location":"newsletter/2021_10/#pytest","text":"New: Exclude the if TYPE_CHECKING code from the coverage. If you want other code to be excluded , for example the statements inside the if TYPE_CHECKING: add to your pyproject.toml : [tool.coverage.report] exclude_lines = [ # Have to re-enable the standard pragma 'pragma: no cover' , # Type checking can not be tested 'if TYPE_CHECKING:' , ]","title":"Pytest"},{"location":"newsletter/2021_10/#python-snippets","text":"New: Check if a dictionary is a subset of another. If you have two dictionaries big = {'a': 1, 'b': 2, 'c':3} and small = {'c': 3, 'a': 1} , and want to check whether small is a subset of big , use the next snippet: >>> small . items () <= big . items () True Correction: Group or sort a list of dictionaries or objects by a specific key. Improve previous method with the concepts learned from the official docs Particularly improve the sorting by multiple keys with the next function: >>> def multisort ( xs , specs ): for key , reverse in reversed ( specs ): xs . sort ( key = attrgetter ( key ), reverse = reverse ) return xs >>> multisort ( list ( student_objects ), (( 'grade' , True ), ( 'age' , False ))) [( 'dave' , 'B' , 10 ), ( 'jane' , 'B' , 12 ), ( 'john' , 'A' , 15 )] Correction: Install default directories and files for a command line program. I've been trying for a long time to configure setup.py to run the required steps to configure the required directories and files when doing pip install without success. Finally, I decided that the program itself should create the data once the FileNotFoundError exception is found. That way, you don't penalize the load time because if the file or directory exists, that code is not run. New: Locate element in list. a = [ 'a' , 'b' ] index = a . index ( 'b' ) New: Transpose a list of lists. >>> l = [[ 1 , 2 , 3 ],[ 4 , 5 , 6 ],[ 7 , 8 , 9 ]] >>> [ list ( i ) for i in zip ( * l )] ... [[ 1 , 4 , 7 ], [ 2 , 5 , 8 ], [ 3 , 6 , 9 ]] New: Check the type of a list of strings. def _is_list_of_lists ( data : Any ) -> bool : \"\"\"Check if data is a list of strings.\"\"\" if data and isinstance ( data , list ): return all ( isinstance ( elem , list ) for elem in data ) else : return False","title":"Python Snippets"},{"location":"newsletter/2021_10/#prompt-toolkit","text":"New: Basic concepts of building full screen applications with python prompt toolkit. prompt_toolkit can be used to create complex full screen terminal applications. Typically, an application consists of a layout (to describe the graphical part) and a set of key bindings. In the section we cover: The layout The controls How to use key bindings How to apply styles A difficult ordered list of examples to get a grasp of these concepts with simple working code.","title":"Prompt Toolkit"},{"location":"newsletter/2021_10/#pydantic","text":"New: Copy produces copy that modifies the original. When copying a model, changing the value of an attribute on the copy updates the value of the attribute on the original. This only happens if deep != True . To fix it use: model.copy(deep=True) .","title":"Pydantic"},{"location":"newsletter/2021_10/#promql","text":"New: Generating range vectors from return values in Prometheus queries.","title":"Promql"},{"location":"newsletter/2021_10/#devops","text":"","title":"DevOps"},{"location":"newsletter/2021_10/#infrastructure-as-code","text":"","title":"Infrastructure as Code"},{"location":"newsletter/2021_10/#helmfile","text":"Correction: Use environment name instead of get values. Instead of .Environment.Name , in theory you could have used .Vars | get \"environment\" , which could have prevented the variables and secrets of the default environment will need to be called default_values.yaml , and default_secrets.yaml , which is misleading. But you can't use .Values in the helmfile.yaml as it's not loaded when the file is parsed, and you get an error. A solution would be to layer the helmfile state files but I wasn't able to make it work. New: How to install a chart only in one environment. environments : default : production : --- releases : - name : newrelic-agent installed : {{ eq .Environment.Name \"production\" | toYaml }} # snip New: Add note that templates can't be used inside the secrets. See this issue","title":"Helmfile"},{"location":"newsletter/2021_10/#terraform","text":"New: How to do elif conditionals in terraform. locals { test = \"${ condition ? value : (elif-condition ? elif-value : else-value)}\" } New: How to enable debug traces. You can set the TF_LOG environmental variable to one of the log levels TRACE , DEBUG , INFO , WARN or ERROR to change the verbosity of the logs.","title":"Terraform"},{"location":"newsletter/2021_10/#helm-secrets","text":"Correction: Update the repository url. The last fork is dead, long live the fork New: How to install the plugin.","title":"Helm Secrets"},{"location":"newsletter/2021_10/#continuous-integration","text":"","title":"Continuous Integration"},{"location":"newsletter/2021_10/#flakehell","text":"New: Troubleshoot the 'Namespace' object has no attribute 'extended_default_ignore' error. Add to your pyproject.toml : [tool.flakehell] extended_default_ignore = [] New: Latest version is broken. It returns an ImportError: cannot import name 'MergedConfigParser' from 'flake8.options.config' , wait for the issue to be solved before upgrading.","title":"Flakehell"},{"location":"newsletter/2021_10/#dependency-managers","text":"New: Sync the virtualenv libraries with the requirements files. python - m piptools sync requirements . txt requirements - dev . txt Correction: Use -c instead of -r in the nested requirement files. To avoid duplication of version pins.","title":"Dependency managers"},{"location":"newsletter/2021_10/#operative-systems","text":"","title":"Operative Systems"},{"location":"newsletter/2021_10/#linux","text":"","title":"Linux"},{"location":"newsletter/2021_10/#dynamic-dns","text":"New: Introduce the Dynamic DNS concept. Dynamic DNS (DDNS) is a method of automatically updating a name server in the Domain Name Server (DNS), often in real time, with the active DDNS configuration of its configured hostnames, addresses or other information.","title":"Dynamic DNS"},{"location":"newsletter/2021_10/#hard-drive-health","text":"New: Taking care of your hard drives. Hard drives die, so we must be ready for that to happen. There are several solutions, such as using RAID to minimize the impact of a disk loss, but even then, we should monitor the bad sectors to see when are our disks dying. In the article we talk about S.M.A.R.T and how to solve some hard drive problems.","title":"Hard drive health"},{"location":"newsletter/2021_10/#kitty","text":"New: Introduce kitty the terminal emulator. kitty is a fast, feature-rich, GPU based terminal emulator written in C and Python with nice features for the keyboard driven humans like me. New: Scrollback when ssh into a machine doesn't work. This happens because the kitty terminfo files are not available on the server. You can ssh in using the following command which will automatically copy the terminfo files to the server: kitty +kitten ssh myserver New: Enable infinite scrollback history. To make the history scrollback infinite add the next lines: scrollback_lines -1 scrollback_pager_history_size 0 New: Reasons to migrate from urxvt to kitty. It doesn't fuck up your terminal colors. You can use peek to record your screen. Easier to extend.","title":"Kitty"},{"location":"newsletter/2021_10/#peek","text":"Correction: Add note that it works with kitty.","title":"Peek"},{"location":"newsletter/2021_10/#arts","text":"","title":"Arts"},{"location":"newsletter/2021_10/#drawing","text":"New: How to draw Ellipses. Ellipses are the next basic shape we're going to study (after the lines). They are extremely important and notoriously annoying to draw. Important because we're going to be using ellipses in 2D space to represent circles that exist in 3D space. In this section we: Introduce the basic concepts surrounding the ellipses How to draw them .","title":"Drawing"},{"location":"newsletter/2021_10/#exercise-pool","text":"New: Add the Tables of ellipses drawing exercise. This exercise is meant to get you used to drawing ellipses, in a variety of sizes, orientations and degrees. It also sets out a clear space each ellipse is meant to occupy, giving us a means to assess whether or not an ellipse was successful, or if there were visible mistakes (where it went outside of its allotted space, or ended up falling short). Practicing against set criteria, with a way to judge success/failure is an important element of learning. There's nothing wrong with failure - it's an opportunity to learn. Having a clearly defined task allows us to analyze those failures and make the most of them.","title":"Exercise Pool"},{"location":"newsletter/2021_10_01/","text":"Activism \u2691 Antifascism \u2691 Antifascist Actions \u2691 New: A fake company and five million recycled flyers. A group of artists belonging to the Center for political beauty created a fake company Flyerservice Hahn and convinced more than 80 regional sections of the far right party AfD to hire them to deliver their electoral propaganda. They gathered five million flyers, with a total weight of 72 tons. They justify that they wouldn't be able to lie to the people, so they did nothing in the broader sense of the word. They declared that they are the \"world wide leader in the non-delivery of nazi propaganda\" . At the start of the electoral campaign, they went to the AfD stands, and they let their members to give them flyers the throw them to the closest bin. \"It's something that any citizen can freely do, we have only industrialized the process\". They've done a crowdfunding to fund the legal process that may result.","title":"1st October 2021"},{"location":"newsletter/2021_10_01/#activism","text":"","title":"Activism"},{"location":"newsletter/2021_10_01/#antifascism","text":"","title":"Antifascism"},{"location":"newsletter/2021_10_01/#antifascist-actions","text":"New: A fake company and five million recycled flyers. A group of artists belonging to the Center for political beauty created a fake company Flyerservice Hahn and convinced more than 80 regional sections of the far right party AfD to hire them to deliver their electoral propaganda. They gathered five million flyers, with a total weight of 72 tons. They justify that they wouldn't be able to lie to the people, so they did nothing in the broader sense of the word. They declared that they are the \"world wide leader in the non-delivery of nazi propaganda\" . At the start of the electoral campaign, they went to the AfD stands, and they let their members to give them flyers the throw them to the closest bin. \"It's something that any citizen can freely do, we have only industrialized the process\". They've done a crowdfunding to fund the legal process that may result.","title":"Antifascist Actions"},{"location":"newsletter/2021_10_05/","text":"Coding \u2691 Python \u2691 Python Snippets \u2691 New: Check if a dictionary is a subset of another. If you have two dictionaries big = {'a': 1, 'b': 2, 'c':3} and small = {'c': 3, 'a': 1} , and want to check whether small is a subset of big , use the next snippet: >>> small . items () <= big . items () True Correction: Group or sort a list of dictionaries or objects by a specific key. Improve previous method with the concepts learned from the official docs Particularly improve the sorting by multiple keys with the next function: >>> def multisort ( xs , specs ): for key , reverse in reversed ( specs ): xs . sort ( key = attrgetter ( key ), reverse = reverse ) return xs >>> multisort ( list ( student_objects ), (( 'grade' , True ), ( 'age' , False ))) [( 'dave' , 'B' , 10 ), ( 'jane' , 'B' , 12 ), ( 'john' , 'A' , 15 )] DevOps \u2691 Continuous Integration \u2691 Flakehell \u2691 New: Troubleshoot the 'Namespace' object has no attribute 'extended_default_ignore' error. Add to your pyproject.toml : [tool.flakehell] extended_default_ignore = [] Dependency managers \u2691 New: Sync the virtualenv libraries with the requirements files. python - m piptools sync requirements . txt requirements - dev . txt Correction: Use -c instead of -r in the nested requirement files. To avoid duplication of version pins.","title":"5th October 2021"},{"location":"newsletter/2021_10_05/#coding","text":"","title":"Coding"},{"location":"newsletter/2021_10_05/#python","text":"","title":"Python"},{"location":"newsletter/2021_10_05/#python-snippets","text":"New: Check if a dictionary is a subset of another. If you have two dictionaries big = {'a': 1, 'b': 2, 'c':3} and small = {'c': 3, 'a': 1} , and want to check whether small is a subset of big , use the next snippet: >>> small . items () <= big . items () True Correction: Group or sort a list of dictionaries or objects by a specific key. Improve previous method with the concepts learned from the official docs Particularly improve the sorting by multiple keys with the next function: >>> def multisort ( xs , specs ): for key , reverse in reversed ( specs ): xs . sort ( key = attrgetter ( key ), reverse = reverse ) return xs >>> multisort ( list ( student_objects ), (( 'grade' , True ), ( 'age' , False ))) [( 'dave' , 'B' , 10 ), ( 'jane' , 'B' , 12 ), ( 'john' , 'A' , 15 )]","title":"Python Snippets"},{"location":"newsletter/2021_10_05/#devops","text":"","title":"DevOps"},{"location":"newsletter/2021_10_05/#continuous-integration","text":"","title":"Continuous Integration"},{"location":"newsletter/2021_10_05/#flakehell","text":"New: Troubleshoot the 'Namespace' object has no attribute 'extended_default_ignore' error. Add to your pyproject.toml : [tool.flakehell] extended_default_ignore = []","title":"Flakehell"},{"location":"newsletter/2021_10_05/#dependency-managers","text":"New: Sync the virtualenv libraries with the requirements files. python - m piptools sync requirements . txt requirements - dev . txt Correction: Use -c instead of -r in the nested requirement files. To avoid duplication of version pins.","title":"Dependency managers"},{"location":"newsletter/2021_10_08/","text":"Coding \u2691 Python \u2691 Click \u2691 New: Invoke other commands from a command. This is a pattern that is generally discouraged with Click, but possible nonetheless. For this, you can use the Context.invoke() or Context.forward() methods. Optimization \u2691 New: Add tips on how to optimize your python command line tools. Minimize the relative import statements on command line tools : When developing a library, it's common to expose the main objects into the package __init__.py under the variable __all__ . The problem with command line programs is that each time you run the command it will load those objects, which can mean an increase of 0.5s or even a second for each command, which is unacceptable. * Don't dynamically install the package : If you install the package with pip install -e . you will see an increase on the load time of ~0.2s. It is useful to develop the package, but when you use it, do so from a virtualenv that installs it directly without the -e flag. Pydantic \u2691 New: Copy produces copy that modifies the original. When copying a model, changing the value of an attribute on the copy updates the value of the attribute on the original. This only happens if deep != True . To fix it use: model.copy(deep=True) . Operative Systems \u2691 Linux \u2691 Kitty \u2691 New: Introduce kitty the terminal emulator. kitty is a fast, feature-rich, GPU based terminal emulator written in C and Python with nice features for the keyboard driven humans like me.","title":"8th October 2021"},{"location":"newsletter/2021_10_08/#coding","text":"","title":"Coding"},{"location":"newsletter/2021_10_08/#python","text":"","title":"Python"},{"location":"newsletter/2021_10_08/#click","text":"New: Invoke other commands from a command. This is a pattern that is generally discouraged with Click, but possible nonetheless. For this, you can use the Context.invoke() or Context.forward() methods.","title":"Click"},{"location":"newsletter/2021_10_08/#optimization","text":"New: Add tips on how to optimize your python command line tools. Minimize the relative import statements on command line tools : When developing a library, it's common to expose the main objects into the package __init__.py under the variable __all__ . The problem with command line programs is that each time you run the command it will load those objects, which can mean an increase of 0.5s or even a second for each command, which is unacceptable. * Don't dynamically install the package : If you install the package with pip install -e . you will see an increase on the load time of ~0.2s. It is useful to develop the package, but when you use it, do so from a virtualenv that installs it directly without the -e flag.","title":"Optimization"},{"location":"newsletter/2021_10_08/#pydantic","text":"New: Copy produces copy that modifies the original. When copying a model, changing the value of an attribute on the copy updates the value of the attribute on the original. This only happens if deep != True . To fix it use: model.copy(deep=True) .","title":"Pydantic"},{"location":"newsletter/2021_10_08/#operative-systems","text":"","title":"Operative Systems"},{"location":"newsletter/2021_10_08/#linux","text":"","title":"Linux"},{"location":"newsletter/2021_10_08/#kitty","text":"New: Introduce kitty the terminal emulator. kitty is a fast, feature-rich, GPU based terminal emulator written in C and Python with nice features for the keyboard driven humans like me.","title":"Kitty"},{"location":"newsletter/2021_10_15/","text":"Activism \u2691 Feminism \u2691 Privileges \u2691 New: Feminist analysis of privileges and rights. Privileges are a group of special structural benefits, social advantages, that a group holds over another. So they are elements that should be removed from our lives. Some of the topics included are: What's the difference between privilege and right What can we do to fight the privileges? Life Management \u2691 Book Management \u2691 New: Introduce the book management concept. Book management is the set of systems and processes to get and categorize books so it's easy to browse and discover new content. It involves the next actions: Automatically index and download metadata of new books. Notify the user when a new book is added. Monitor the books of an author, and get them once they are released. Send books to the e-reader. A nice interface to browse the existent library, with the possibility of filtering by author, genre, years, tags or series. An interface to preview or read the items. An interface to rate and review library items. An interface to discover new content based on the ratings and item metadata. I haven't yet found a single piece of software that fulfills all these needs, in the article I tell you about Readarr , Calibre-web , [calibre](( https://manual.calibre-ebook.com/ ), Polar bookself , GCStar , and how they interact with each other. Coding \u2691 Python \u2691 Flask Restplus \u2691 New: Introduce the Flask-RESTPlus library. Flask-RESTPlus is an extension for Flask that adds support for quickly building REST APIs, but I'd use FastAPI instead. Python Snippets \u2691 Correction: Install default directories and files for a command line program. I've been trying for a long time to configure setup.py to run the required steps to configure the required directories and files when doing pip install without success. Finally, I decided that the program itself should create the data once the FileNotFoundError exception is found. That way, you don't penalize the load time because if the file or directory exists, that code is not run. Promql \u2691 New: Generating range vectors from return values in Prometheus queries. DevOps \u2691 Infrastructure as Code \u2691 Terraform \u2691 New: How to do elif conditionals in terraform. locals { test = \"${ condition ? value : (elif-condition ? elif-value : else-value)}\" } New: How to enable debug traces. You can set the TF_LOG environmental variable to one of the log levels TRACE , DEBUG , INFO , WARN or ERROR to change the verbosity of the logs. Continuous Integration \u2691 Flakehell \u2691 New: Latest version is broken. It returns an ImportError: cannot import name 'MergedConfigParser' from 'flake8.options.config' , wait for the issue to be solved before upgrading. Operative Systems \u2691 Linux \u2691 Dynamic DNS \u2691 New: Introduce the Dynamic DNS concept. Dynamic DNS (DDNS) is a method of automatically updating a name server in the Domain Name Server (DNS), often in real time, with the active DDNS configuration of its configured hostnames, addresses or other information. Hard drive health \u2691 New: Taking care of your hard drives. Hard drives die, so we must be ready for that to happen. There are several solutions, such as using RAID to minimize the impact of a disk loss, but even then, we should monitor the bad sectors to see when are our disks dying. In the article we talk about S.M.A.R.T and how to solve some hard drive problems. Kitty \u2691 New: Scrollback when ssh into a machine doesn't work. This happens because the kitty terminfo files are not available on the server. You can ssh in using the following command which will automatically copy the terminfo files to the server: kitty +kitten ssh myserver","title":"15th October 2021"},{"location":"newsletter/2021_10_15/#activism","text":"","title":"Activism"},{"location":"newsletter/2021_10_15/#feminism","text":"","title":"Feminism"},{"location":"newsletter/2021_10_15/#privileges","text":"New: Feminist analysis of privileges and rights. Privileges are a group of special structural benefits, social advantages, that a group holds over another. So they are elements that should be removed from our lives. Some of the topics included are: What's the difference between privilege and right What can we do to fight the privileges?","title":"Privileges"},{"location":"newsletter/2021_10_15/#life-management","text":"","title":"Life Management"},{"location":"newsletter/2021_10_15/#book-management","text":"New: Introduce the book management concept. Book management is the set of systems and processes to get and categorize books so it's easy to browse and discover new content. It involves the next actions: Automatically index and download metadata of new books. Notify the user when a new book is added. Monitor the books of an author, and get them once they are released. Send books to the e-reader. A nice interface to browse the existent library, with the possibility of filtering by author, genre, years, tags or series. An interface to preview or read the items. An interface to rate and review library items. An interface to discover new content based on the ratings and item metadata. I haven't yet found a single piece of software that fulfills all these needs, in the article I tell you about Readarr , Calibre-web , [calibre](( https://manual.calibre-ebook.com/ ), Polar bookself , GCStar , and how they interact with each other.","title":"Book Management"},{"location":"newsletter/2021_10_15/#coding","text":"","title":"Coding"},{"location":"newsletter/2021_10_15/#python","text":"","title":"Python"},{"location":"newsletter/2021_10_15/#flask-restplus","text":"New: Introduce the Flask-RESTPlus library. Flask-RESTPlus is an extension for Flask that adds support for quickly building REST APIs, but I'd use FastAPI instead.","title":"Flask Restplus"},{"location":"newsletter/2021_10_15/#python-snippets","text":"Correction: Install default directories and files for a command line program. I've been trying for a long time to configure setup.py to run the required steps to configure the required directories and files when doing pip install without success. Finally, I decided that the program itself should create the data once the FileNotFoundError exception is found. That way, you don't penalize the load time because if the file or directory exists, that code is not run.","title":"Python Snippets"},{"location":"newsletter/2021_10_15/#promql","text":"New: Generating range vectors from return values in Prometheus queries.","title":"Promql"},{"location":"newsletter/2021_10_15/#devops","text":"","title":"DevOps"},{"location":"newsletter/2021_10_15/#infrastructure-as-code","text":"","title":"Infrastructure as Code"},{"location":"newsletter/2021_10_15/#terraform","text":"New: How to do elif conditionals in terraform. locals { test = \"${ condition ? value : (elif-condition ? elif-value : else-value)}\" } New: How to enable debug traces. You can set the TF_LOG environmental variable to one of the log levels TRACE , DEBUG , INFO , WARN or ERROR to change the verbosity of the logs.","title":"Terraform"},{"location":"newsletter/2021_10_15/#continuous-integration","text":"","title":"Continuous Integration"},{"location":"newsletter/2021_10_15/#flakehell","text":"New: Latest version is broken. It returns an ImportError: cannot import name 'MergedConfigParser' from 'flake8.options.config' , wait for the issue to be solved before upgrading.","title":"Flakehell"},{"location":"newsletter/2021_10_15/#operative-systems","text":"","title":"Operative Systems"},{"location":"newsletter/2021_10_15/#linux","text":"","title":"Linux"},{"location":"newsletter/2021_10_15/#dynamic-dns","text":"New: Introduce the Dynamic DNS concept. Dynamic DNS (DDNS) is a method of automatically updating a name server in the Domain Name Server (DNS), often in real time, with the active DDNS configuration of its configured hostnames, addresses or other information.","title":"Dynamic DNS"},{"location":"newsletter/2021_10_15/#hard-drive-health","text":"New: Taking care of your hard drives. Hard drives die, so we must be ready for that to happen. There are several solutions, such as using RAID to minimize the impact of a disk loss, but even then, we should monitor the bad sectors to see when are our disks dying. In the article we talk about S.M.A.R.T and how to solve some hard drive problems.","title":"Hard drive health"},{"location":"newsletter/2021_10_15/#kitty","text":"New: Scrollback when ssh into a machine doesn't work. This happens because the kitty terminfo files are not available on the server. You can ssh in using the following command which will automatically copy the terminfo files to the server: kitty +kitten ssh myserver","title":"Kitty"},{"location":"newsletter/2021_10_25/","text":"Life Management \u2691 Book Management \u2691 Improvement: Add link to the calibre-web kobo integration project. Coding \u2691 Python \u2691 Python Snippets \u2691 New: Locate element in list. a = [ 'a' , 'b' ] index = a . index ( 'b' ) New: Transpose a list of lists. >>> l = [[ 1 , 2 , 3 ],[ 4 , 5 , 6 ],[ 7 , 8 , 9 ]] >>> [ list ( i ) for i in zip ( * l )] ... [[ 1 , 4 , 7 ], [ 2 , 5 , 8 ], [ 3 , 6 , 9 ]] New: Check the type of a list of strings. def _is_list_of_lists ( data : Any ) -> bool : \"\"\"Check if data is a list of strings.\"\"\" if data and isinstance ( data , list ): return all ( isinstance ( elem , list ) for elem in data ) else : return False Prompt Toolkit \u2691 New: Basic concepts of building full screen applications with python prompt toolkit. prompt_toolkit can be used to create complex full screen terminal applications. Typically, an application consists of a layout (to describe the graphical part) and a set of key bindings. In the section we cover: The layout The controls How to use key bindings How to apply styles A difficult ordered list of examples to get a grasp of these concepts with simple working code. Arts \u2691 Drawing \u2691 New: How to draw Ellipses. Ellipses are the next basic shape we're going to study (after the lines). They are extremely important and notoriously annoying to draw. Important because we're going to be using ellipses in 2D space to represent circles that exist in 3D space. In this section we: Introduce the basic concepts surrounding the ellipses How to draw them . Exercise Pool \u2691 New: Add the Tables of ellipses drawing exercise. This exercise is meant to get you used to drawing ellipses, in a variety of sizes, orientations and degrees. It also sets out a clear space each ellipse is meant to occupy, giving us a means to assess whether or not an ellipse was successful, or if there were visible mistakes (where it went outside of its allotted space, or ended up falling short). Practicing against set criteria, with a way to judge success/failure is an important element of learning. There's nothing wrong with failure - it's an opportunity to learn. Having a clearly defined task allows us to analyze those failures and make the most of them.","title":"25th October 2021"},{"location":"newsletter/2021_10_25/#life-management","text":"","title":"Life Management"},{"location":"newsletter/2021_10_25/#book-management","text":"Improvement: Add link to the calibre-web kobo integration project.","title":"Book Management"},{"location":"newsletter/2021_10_25/#coding","text":"","title":"Coding"},{"location":"newsletter/2021_10_25/#python","text":"","title":"Python"},{"location":"newsletter/2021_10_25/#python-snippets","text":"New: Locate element in list. a = [ 'a' , 'b' ] index = a . index ( 'b' ) New: Transpose a list of lists. >>> l = [[ 1 , 2 , 3 ],[ 4 , 5 , 6 ],[ 7 , 8 , 9 ]] >>> [ list ( i ) for i in zip ( * l )] ... [[ 1 , 4 , 7 ], [ 2 , 5 , 8 ], [ 3 , 6 , 9 ]] New: Check the type of a list of strings. def _is_list_of_lists ( data : Any ) -> bool : \"\"\"Check if data is a list of strings.\"\"\" if data and isinstance ( data , list ): return all ( isinstance ( elem , list ) for elem in data ) else : return False","title":"Python Snippets"},{"location":"newsletter/2021_10_25/#prompt-toolkit","text":"New: Basic concepts of building full screen applications with python prompt toolkit. prompt_toolkit can be used to create complex full screen terminal applications. Typically, an application consists of a layout (to describe the graphical part) and a set of key bindings. In the section we cover: The layout The controls How to use key bindings How to apply styles A difficult ordered list of examples to get a grasp of these concepts with simple working code.","title":"Prompt Toolkit"},{"location":"newsletter/2021_10_25/#arts","text":"","title":"Arts"},{"location":"newsletter/2021_10_25/#drawing","text":"New: How to draw Ellipses. Ellipses are the next basic shape we're going to study (after the lines). They are extremely important and notoriously annoying to draw. Important because we're going to be using ellipses in 2D space to represent circles that exist in 3D space. In this section we: Introduce the basic concepts surrounding the ellipses How to draw them .","title":"Drawing"},{"location":"newsletter/2021_10_25/#exercise-pool","text":"New: Add the Tables of ellipses drawing exercise. This exercise is meant to get you used to drawing ellipses, in a variety of sizes, orientations and degrees. It also sets out a clear space each ellipse is meant to occupy, giving us a means to assess whether or not an ellipse was successful, or if there were visible mistakes (where it went outside of its allotted space, or ended up falling short). Practicing against set criteria, with a way to judge success/failure is an important element of learning. There's nothing wrong with failure - it's an opportunity to learn. Having a clearly defined task allows us to analyze those failures and make the most of them.","title":"Exercise Pool"},{"location":"newsletter/2021_10_28/","text":"Coding \u2691 Python \u2691 Full screen applications \u2691 New: Testing full screen applications. New: Pass more than one key. To map an action to two key presses use kb.add('g', 'g') . New: Add note on how to debug the styles of the components. Set the style to bg:#dc322f and it will be highlighted in red. Pytest \u2691 New: Exclude the if TYPE_CHECKING code from the coverage. If you want other code to be excluded , for example the statements inside the if TYPE_CHECKING: add to your pyproject.toml : [tool.coverage.report] exclude_lines = [ # Have to re-enable the standard pragma 'pragma: no cover' , # Type checking can not be tested 'if TYPE_CHECKING:' , ] DevOps \u2691 Infrastructure as Code \u2691 Helmfile \u2691 Correction: Use environment name instead of get values. Instead of .Environment.Name , in theory you could have used .Vars | get \"environment\" , which could have prevented the variables and secrets of the default environment will need to be called default_values.yaml , and default_secrets.yaml , which is misleading. But you can't use .Values in the helmfile.yaml as it's not loaded when the file is parsed, and you get an error. A solution would be to layer the helmfile state files but I wasn't able to make it work. New: How to install a chart only in one environment. environments : default : production : --- releases : - name : newrelic-agent installed : {{ eq .Environment.Name \"production\" | toYaml }} # snip New: Add note that templates can't be used inside the secrets. See this issue Helm Secrets \u2691 Correction: Update the repository url. The last fork is dead, long live the fork New: How to install the plugin. Operative Systems \u2691 Linux \u2691 Kitty \u2691 New: Enable infinite scrollback history. To make the history scrollback infinite add the next lines: scrollback_lines -1 scrollback_pager_history_size 0 New: Reasons to migrate from urxvt to kitty. It doesn't fuck up your terminal colors. You can use peek to record your screen. Easier to extend. Peek \u2691 Correction: Add note that it works with kitty.","title":"28th October 2021"},{"location":"newsletter/2021_10_28/#coding","text":"","title":"Coding"},{"location":"newsletter/2021_10_28/#python","text":"","title":"Python"},{"location":"newsletter/2021_10_28/#full-screen-applications","text":"New: Testing full screen applications. New: Pass more than one key. To map an action to two key presses use kb.add('g', 'g') . New: Add note on how to debug the styles of the components. Set the style to bg:#dc322f and it will be highlighted in red.","title":"Full screen applications"},{"location":"newsletter/2021_10_28/#pytest","text":"New: Exclude the if TYPE_CHECKING code from the coverage. If you want other code to be excluded , for example the statements inside the if TYPE_CHECKING: add to your pyproject.toml : [tool.coverage.report] exclude_lines = [ # Have to re-enable the standard pragma 'pragma: no cover' , # Type checking can not be tested 'if TYPE_CHECKING:' , ]","title":"Pytest"},{"location":"newsletter/2021_10_28/#devops","text":"","title":"DevOps"},{"location":"newsletter/2021_10_28/#infrastructure-as-code","text":"","title":"Infrastructure as Code"},{"location":"newsletter/2021_10_28/#helmfile","text":"Correction: Use environment name instead of get values. Instead of .Environment.Name , in theory you could have used .Vars | get \"environment\" , which could have prevented the variables and secrets of the default environment will need to be called default_values.yaml , and default_secrets.yaml , which is misleading. But you can't use .Values in the helmfile.yaml as it's not loaded when the file is parsed, and you get an error. A solution would be to layer the helmfile state files but I wasn't able to make it work. New: How to install a chart only in one environment. environments : default : production : --- releases : - name : newrelic-agent installed : {{ eq .Environment.Name \"production\" | toYaml }} # snip New: Add note that templates can't be used inside the secrets. See this issue","title":"Helmfile"},{"location":"newsletter/2021_10_28/#helm-secrets","text":"Correction: Update the repository url. The last fork is dead, long live the fork New: How to install the plugin.","title":"Helm Secrets"},{"location":"newsletter/2021_10_28/#operative-systems","text":"","title":"Operative Systems"},{"location":"newsletter/2021_10_28/#linux","text":"","title":"Linux"},{"location":"newsletter/2021_10_28/#kitty","text":"New: Enable infinite scrollback history. To make the history scrollback infinite add the next lines: scrollback_lines -1 scrollback_pager_history_size 0 New: Reasons to migrate from urxvt to kitty. It doesn't fuck up your terminal colors. You can use peek to record your screen. Easier to extend.","title":"Kitty"},{"location":"newsletter/2021_10_28/#peek","text":"Correction: Add note that it works with kitty.","title":"Peek"},{"location":"newsletter/2021_w01/","text":"DevOps \u2691 Continuous Integration \u2691 Bandit \u2691 New: Explain how to ignore errors. Coding \u2691 Python \u2691 Pydantic \u2691 New: Explain how to initialize attributes. Use validators to initialize attributes","title":"1st Week of 2021"},{"location":"newsletter/2021_w01/#devops","text":"","title":"DevOps"},{"location":"newsletter/2021_w01/#continuous-integration","text":"","title":"Continuous Integration"},{"location":"newsletter/2021_w01/#bandit","text":"New: Explain how to ignore errors.","title":"Bandit"},{"location":"newsletter/2021_w01/#coding","text":"","title":"Coding"},{"location":"newsletter/2021_w01/#python","text":"","title":"Python"},{"location":"newsletter/2021_w01/#pydantic","text":"New: Explain how to initialize attributes. Use validators to initialize attributes","title":"Pydantic"},{"location":"newsletter/2021_w03/","text":"Coding \u2691 Python \u2691 DeepDiff \u2691 Correction: Remove murmur from the installation steps. It seems it's the default for the new versions Pydantic \u2691 New: Name the pros and cons of using the library. New: Explain how to create bidirectional relationship between entities. Pypika \u2691 New: Explain how to insert, update, select data. New: Explain how to join tables. SQL \u2691 New: Give examples on joins for each relationship type. SQLite \u2691 New: Solve the autoincrementation not working bug. Other \u2691 New: Add remote work tips. New: Introduce lazy loading implementation paradigm with python. New: Explain how to lazy load pydantic objects.","title":"3rd Week of 2021"},{"location":"newsletter/2021_w03/#coding","text":"","title":"Coding"},{"location":"newsletter/2021_w03/#python","text":"","title":"Python"},{"location":"newsletter/2021_w03/#deepdiff","text":"Correction: Remove murmur from the installation steps. It seems it's the default for the new versions","title":"DeepDiff"},{"location":"newsletter/2021_w03/#pydantic","text":"New: Name the pros and cons of using the library. New: Explain how to create bidirectional relationship between entities.","title":"Pydantic"},{"location":"newsletter/2021_w03/#pypika","text":"New: Explain how to insert, update, select data. New: Explain how to join tables.","title":"Pypika"},{"location":"newsletter/2021_w03/#sql","text":"New: Give examples on joins for each relationship type.","title":"SQL"},{"location":"newsletter/2021_w03/#sqlite","text":"New: Solve the autoincrementation not working bug.","title":"SQLite"},{"location":"newsletter/2021_w03/#other","text":"New: Add remote work tips. New: Introduce lazy loading implementation paradigm with python. New: Explain how to lazy load pydantic objects.","title":"Other"},{"location":"newsletter/2021_w04/","text":"Coding \u2691 Python \u2691 New: Explain how to check if a loop ends completely. New: Explain how to merge lists and dictionaries. New: Explain how to create your own exceptions. Libraries \u2691 New: Explain how to set cookies and headers in responses. FactoryBoy \u2691 New: Explain how to generate your own attributes. We earlier used lazy_attribute but if you want to use Faker inside the attribute definition, you're going to have a bad time. The new solution uses the creation of custom Fake providers. Faker \u2691 New: Explain how to create your own provider. Useful to generate custom objects for testing purposes. Python Snippets \u2691 Correction: Explain how to show the message in custom exceptions. New: Explain how to import a module or object from within a python program. pexpect \u2691 New: Introduce the pexpect python library. A pure Python module for spawning child applications; controlling them; and responding to expected patterns in their output. Pexpect works like Don Libes\u2019 Expect. Pexpect allows your script to spawn a child application and control it as if a human were typing commands. Prompt Toolkit \u2691 New: Introduce the tui python library. Useful to build text-based user interfaces, it allows the creation of intelligent prompts, dialogs, and full screen ncurses-like applications. Pydantic \u2691 New: Warn on the lack of TypeDict support. questionary \u2691 New: Introduce tui python library. questionary is a Python library for effortlessly building pretty command line interfaces. It makes it very easy to query your user for input. Requests \u2691 New: Introduce the requests python library. Rq \u2691 New: Add note to test arq. arq is a similar library that can be better. Life Management \u2691 Automation \u2691 Accounting Automation \u2691 New: Explain my accounting automation workflow. Operative Systems \u2691 Linux \u2691 beancount \u2691 New: Introduce the cli double entry accounting program. New: Add links on how to use as a library. Correction: Correct the git repository link. Android \u2691 cone \u2691 New: Introduce the mobile double entry accounting application. Correction: Correct the description of the transaction to be beancount compatible.","title":"4th Week of 2021"},{"location":"newsletter/2021_w04/#coding","text":"","title":"Coding"},{"location":"newsletter/2021_w04/#python","text":"New: Explain how to check if a loop ends completely. New: Explain how to merge lists and dictionaries. New: Explain how to create your own exceptions.","title":"Python"},{"location":"newsletter/2021_w04/#libraries","text":"New: Explain how to set cookies and headers in responses.","title":"Libraries"},{"location":"newsletter/2021_w04/#factoryboy","text":"New: Explain how to generate your own attributes. We earlier used lazy_attribute but if you want to use Faker inside the attribute definition, you're going to have a bad time. The new solution uses the creation of custom Fake providers.","title":"FactoryBoy"},{"location":"newsletter/2021_w04/#faker","text":"New: Explain how to create your own provider. Useful to generate custom objects for testing purposes.","title":"Faker"},{"location":"newsletter/2021_w04/#python-snippets","text":"Correction: Explain how to show the message in custom exceptions. New: Explain how to import a module or object from within a python program.","title":"Python Snippets"},{"location":"newsletter/2021_w04/#pexpect","text":"New: Introduce the pexpect python library. A pure Python module for spawning child applications; controlling them; and responding to expected patterns in their output. Pexpect works like Don Libes\u2019 Expect. Pexpect allows your script to spawn a child application and control it as if a human were typing commands.","title":"pexpect"},{"location":"newsletter/2021_w04/#prompt-toolkit","text":"New: Introduce the tui python library. Useful to build text-based user interfaces, it allows the creation of intelligent prompts, dialogs, and full screen ncurses-like applications.","title":"Prompt Toolkit"},{"location":"newsletter/2021_w04/#pydantic","text":"New: Warn on the lack of TypeDict support.","title":"Pydantic"},{"location":"newsletter/2021_w04/#questionary","text":"New: Introduce tui python library. questionary is a Python library for effortlessly building pretty command line interfaces. It makes it very easy to query your user for input.","title":"questionary"},{"location":"newsletter/2021_w04/#requests","text":"New: Introduce the requests python library.","title":"Requests"},{"location":"newsletter/2021_w04/#rq","text":"New: Add note to test arq. arq is a similar library that can be better.","title":"Rq"},{"location":"newsletter/2021_w04/#life-management","text":"","title":"Life Management"},{"location":"newsletter/2021_w04/#automation","text":"","title":"Automation"},{"location":"newsletter/2021_w04/#accounting-automation","text":"New: Explain my accounting automation workflow.","title":"Accounting Automation"},{"location":"newsletter/2021_w04/#operative-systems","text":"","title":"Operative Systems"},{"location":"newsletter/2021_w04/#linux","text":"","title":"Linux"},{"location":"newsletter/2021_w04/#beancount","text":"New: Introduce the cli double entry accounting program. New: Add links on how to use as a library. Correction: Correct the git repository link.","title":"beancount"},{"location":"newsletter/2021_w04/#android","text":"","title":"Android"},{"location":"newsletter/2021_w04/#cone","text":"New: Introduce the mobile double entry accounting application. Correction: Correct the description of the transaction to be beancount compatible.","title":"cone"},{"location":"newsletter/2021_w05/","text":"DevOps \u2691 New: Comment on the DevOps pitfalls and update the learn path. Software Architecture \u2691 Architecture Decision Record \u2691 New: Introduce the Architecture Decision Records. ADR are short text documents that captures an important architectural decision made along with its context and consequences. Life Management \u2691 Automation \u2691 Virtual Assistant \u2691 New: Introduce project with kalliope. New: Explain the Speech-To-Text open source solutions. Health \u2691 Teeth \u2691 New: Explain how to take care of your teeth. A full guide on why should you take care of your teeth, the description on how the basic oral diseases work, why and how to brush your teeth, floss and usage of mouthwash Deep cleaning \u2691 New: Explain what a deep cleaning is and when should you do it. Analyze the reasons why would you need to do this procedure, how it works, when you need to do it, side effects and scientific evidences of it's effectiveness. Operative Systems \u2691 Linux \u2691 Vim \u2691 New: Configure Vim to set the upstream by default when git pushing.","title":"5th Week of 2021"},{"location":"newsletter/2021_w05/#devops","text":"New: Comment on the DevOps pitfalls and update the learn path.","title":"DevOps"},{"location":"newsletter/2021_w05/#software-architecture","text":"","title":"Software Architecture"},{"location":"newsletter/2021_w05/#architecture-decision-record","text":"New: Introduce the Architecture Decision Records. ADR are short text documents that captures an important architectural decision made along with its context and consequences.","title":"Architecture Decision Record"},{"location":"newsletter/2021_w05/#life-management","text":"","title":"Life Management"},{"location":"newsletter/2021_w05/#automation","text":"","title":"Automation"},{"location":"newsletter/2021_w05/#virtual-assistant","text":"New: Introduce project with kalliope. New: Explain the Speech-To-Text open source solutions.","title":"Virtual Assistant"},{"location":"newsletter/2021_w05/#health","text":"","title":"Health"},{"location":"newsletter/2021_w05/#teeth","text":"New: Explain how to take care of your teeth. A full guide on why should you take care of your teeth, the description on how the basic oral diseases work, why and how to brush your teeth, floss and usage of mouthwash","title":"Teeth"},{"location":"newsletter/2021_w05/#deep-cleaning","text":"New: Explain what a deep cleaning is and when should you do it. Analyze the reasons why would you need to do this procedure, how it works, when you need to do it, side effects and scientific evidences of it's effectiveness.","title":"Deep cleaning"},{"location":"newsletter/2021_w05/#operative-systems","text":"","title":"Operative Systems"},{"location":"newsletter/2021_w05/#linux","text":"","title":"Linux"},{"location":"newsletter/2021_w05/#vim","text":"New: Configure Vim to set the upstream by default when git pushing.","title":"Vim"},{"location":"newsletter/2021_w06/","text":"Introduction \u2691 New: Simplify the landing page text. Meta \u2691 Wish list \u2691 New: Feature mkdocs-rss-plugin as a solution of publishing mkdocs updates as an RSS. New: Add a git issue tracker and markdown formatter. Coding \u2691 Python \u2691 GitPython \u2691 New: Introduce the python library. GitPython is a python library used to interact with git repositories, high-level like git-porcelain, or low-level like git-plumbing. It provides abstractions of git objects for easy access of repository data, and additionally allows you to access the git repository more directly using either a pure python implementation, or the faster, but more resource intensive git command implementation. Explain how to: Initialize or load repositories. Make commits. Interact with the history. Test applications that use it. Python Snippets \u2691 New: Add today's learned python tricks. Get system's timezone and use it in datetime . Capitalize a sentence . Get the last monday datetime . Issues \u2691 New: Introduce the issue tracking document. I haven't found a tool to monitor the context it made me track certain software issues, so I get lost when updates come. Until a tool shows up, I'll use the good old markdown to keep track of them. Software Architecture \u2691 Architecture Decision Record \u2691 New: Update the ADR template with the week learnings. Add the Proposals and Date sections Explain the possible Status states. Add an Ultisnip vim snippet. Explain how I've used it to create mkdocs- newsletter . Health \u2691 Teeth \u2691 Correction: Recommend a regular clean instead of a deep clean. Activism \u2691 New: Introduce the anonymous feedback tool to improve diversity, equity and inclusion in an organization. Anonymous Feedback is a communication tool where people share feedback to teammates or other organizational members while protecting their identities. Until the safe space is built where direct feedback is viable, anonymous feedback gives these employees a mechanism to raise their concerns, practice their feedback-giving skills, test the waters, and understand how people perceive their constructive (and sometimes critical) opinions, thus building the needed trust. Operative Systems \u2691 Linux \u2691 mkdocs \u2691 New: Explain how to develop your own plugins. Arts \u2691 Writing \u2691 New: Try vim-pencil without success, but love mdnav. mdnav opens links to urls or files when pressing enter in normal mode over a markdown link, similar to gx but more powerful. I specially like the ability of following [self referencing link][] links, that allows storing the links at the bottom. Build your own Digital Garden \u2691 New: Explain how to enable clickable navigation sections in your mkdocs repository. oprypin has solved it with the mkdocs-section-index plugin. Digital Gardens \u2691 New: Introduce the digital garden concept. Digital Garden is a method of storing and maintaining knowledge in an maintainable, scalable and searchable way. They are also known as second brains. Pilates \u2691 New: Introduce the art. Pilates is a physical fitness system based on controlled movements putting emphasis on alignment, breathing, developing a strong core, and improving coordination and balance. The core (or powerhouse), consisting of the muscles of the abdomen, low back, and hips, is thought to be the key to a person's stability. Pilates' system allows for different exercises to be modified in range of difficulty from beginner to advanced or to any other level, and also in terms of the instructor and practitioner's specific goals and/or limitations. Intensity can be increased over time as the body adapts itself to the exercises. You can think of yoga, but without the spiritual aspects. Also added: It's principles The swing from table exercise.","title":"6th Week of 2021"},{"location":"newsletter/2021_w06/#introduction","text":"New: Simplify the landing page text.","title":"Introduction"},{"location":"newsletter/2021_w06/#meta","text":"","title":"Meta"},{"location":"newsletter/2021_w06/#wish-list","text":"New: Feature mkdocs-rss-plugin as a solution of publishing mkdocs updates as an RSS. New: Add a git issue tracker and markdown formatter.","title":"Wish list"},{"location":"newsletter/2021_w06/#coding","text":"","title":"Coding"},{"location":"newsletter/2021_w06/#python","text":"","title":"Python"},{"location":"newsletter/2021_w06/#gitpython","text":"New: Introduce the python library. GitPython is a python library used to interact with git repositories, high-level like git-porcelain, or low-level like git-plumbing. It provides abstractions of git objects for easy access of repository data, and additionally allows you to access the git repository more directly using either a pure python implementation, or the faster, but more resource intensive git command implementation. Explain how to: Initialize or load repositories. Make commits. Interact with the history. Test applications that use it.","title":"GitPython"},{"location":"newsletter/2021_w06/#python-snippets","text":"New: Add today's learned python tricks. Get system's timezone and use it in datetime . Capitalize a sentence . Get the last monday datetime .","title":"Python Snippets"},{"location":"newsletter/2021_w06/#issues","text":"New: Introduce the issue tracking document. I haven't found a tool to monitor the context it made me track certain software issues, so I get lost when updates come. Until a tool shows up, I'll use the good old markdown to keep track of them.","title":"Issues"},{"location":"newsletter/2021_w06/#software-architecture","text":"","title":"Software Architecture"},{"location":"newsletter/2021_w06/#architecture-decision-record","text":"New: Update the ADR template with the week learnings. Add the Proposals and Date sections Explain the possible Status states. Add an Ultisnip vim snippet. Explain how I've used it to create mkdocs- newsletter .","title":"Architecture Decision Record"},{"location":"newsletter/2021_w06/#health","text":"","title":"Health"},{"location":"newsletter/2021_w06/#teeth","text":"Correction: Recommend a regular clean instead of a deep clean.","title":"Teeth"},{"location":"newsletter/2021_w06/#activism","text":"New: Introduce the anonymous feedback tool to improve diversity, equity and inclusion in an organization. Anonymous Feedback is a communication tool where people share feedback to teammates or other organizational members while protecting their identities. Until the safe space is built where direct feedback is viable, anonymous feedback gives these employees a mechanism to raise their concerns, practice their feedback-giving skills, test the waters, and understand how people perceive their constructive (and sometimes critical) opinions, thus building the needed trust.","title":"Activism"},{"location":"newsletter/2021_w06/#operative-systems","text":"","title":"Operative Systems"},{"location":"newsletter/2021_w06/#linux","text":"","title":"Linux"},{"location":"newsletter/2021_w06/#mkdocs","text":"New: Explain how to develop your own plugins.","title":"mkdocs"},{"location":"newsletter/2021_w06/#arts","text":"","title":"Arts"},{"location":"newsletter/2021_w06/#writing","text":"New: Try vim-pencil without success, but love mdnav. mdnav opens links to urls or files when pressing enter in normal mode over a markdown link, similar to gx but more powerful. I specially like the ability of following [self referencing link][] links, that allows storing the links at the bottom.","title":"Writing"},{"location":"newsletter/2021_w06/#build-your-own-digital-garden","text":"New: Explain how to enable clickable navigation sections in your mkdocs repository. oprypin has solved it with the mkdocs-section-index plugin.","title":"Build your own Digital Garden"},{"location":"newsletter/2021_w06/#digital-gardens","text":"New: Introduce the digital garden concept. Digital Garden is a method of storing and maintaining knowledge in an maintainable, scalable and searchable way. They are also known as second brains.","title":"Digital Gardens"},{"location":"newsletter/2021_w06/#pilates","text":"New: Introduce the art. Pilates is a physical fitness system based on controlled movements putting emphasis on alignment, breathing, developing a strong core, and improving coordination and balance. The core (or powerhouse), consisting of the muscles of the abdomen, low back, and hips, is thought to be the key to a person's stability. Pilates' system allows for different exercises to be modified in range of difficulty from beginner to advanced or to any other level, and also in terms of the instructor and practitioner's specific goals and/or limitations. Intensity can be increased over time as the body adapts itself to the exercises. You can think of yoga, but without the spiritual aspects. Also added: It's principles The swing from table exercise.","title":"Pilates"},{"location":"newsletter/2021_w08/","text":"DevOps \u2691 Monitoring \u2691 Monitoring Comparison \u2691 New: Compare Nagios and Prometheus as monitoring. Correction: Improve the comparison. State that nagios is not easy to configure. If you're used to it it is, otherwise it's not. Add that grafana has a huge community building graphs. Mention Thanos as the long term storage solution for Prometheus. Correction: Add the insights of a nagios power user. Update open source and community analysis with nagios exchange. Correct nagios community analysis with its trajectory Correct the analysis of the high availability of nagios Add the option to host the script exporter in a dedicated server Coding \u2691 Python \u2691 GitPython \u2691 Improvement: Explain how to get the working directory of a repo. Using the working_dir attribute. Jinja2 \u2691 New: Explain how to use Jinja2. Jinja2 is a modern and designer- friendly templating language for Python, modelled after Django\u2019s templates. It is fast, widely used and secure with the optional sandboxed template execution environment. Add installation, usage and basic and advanced template guidelines. Issues \u2691 New: Add today's issues. Gadgetbridge improvements Ombi improvements Improvement: Monitor today's issues. Mkdocs migration to 7.x is giving errors with the search bar and repo stats. Life Management \u2691 Automation \u2691 Amazfit Band 5 \u2691 New: Add insights on sleep detection. The sleep tracking using Gadgetbridge is not good at all . After two nights, the band has not been able to detect when I woke in the middle of the night, or when I really woke up, as I usually stay in the bed for a time before standing up. I'll try with the proprietary application soon and compare results. New: Explain how to upgrade the firmware. Gadgetbridge people have a guide on how to upgrade the firmware , you need to get the firmware from the geek doing forum though, so it is interesting to create an account and watch the post. Improvement: Add insights on sleep tracking. You can't use the Withings sleep analyzer without their app (as expected), maybe the Emfit QS is the way to go. Fitness Tracker \u2691 New: Introduce the fitness band in your life automation. Fitness tracker or activity trackers are devices or applications for monitoring and tracking fitness- related metrics such as distance walked or run, calorie consumption, and in some cases heartbeat. It is a type of wearable computer. Explain also why it's interesting Operative Systems \u2691 Android \u2691 GadgetBridge \u2691 New: Add more guidelines to reverse engineer the band protocol. Other \u2691 Correction: Deprecate mkdocs issues. They've been fixed in the last release","title":"8th Week of 2021"},{"location":"newsletter/2021_w08/#devops","text":"","title":"DevOps"},{"location":"newsletter/2021_w08/#monitoring","text":"","title":"Monitoring"},{"location":"newsletter/2021_w08/#monitoring-comparison","text":"New: Compare Nagios and Prometheus as monitoring. Correction: Improve the comparison. State that nagios is not easy to configure. If you're used to it it is, otherwise it's not. Add that grafana has a huge community building graphs. Mention Thanos as the long term storage solution for Prometheus. Correction: Add the insights of a nagios power user. Update open source and community analysis with nagios exchange. Correct nagios community analysis with its trajectory Correct the analysis of the high availability of nagios Add the option to host the script exporter in a dedicated server","title":"Monitoring Comparison"},{"location":"newsletter/2021_w08/#coding","text":"","title":"Coding"},{"location":"newsletter/2021_w08/#python","text":"","title":"Python"},{"location":"newsletter/2021_w08/#gitpython","text":"Improvement: Explain how to get the working directory of a repo. Using the working_dir attribute.","title":"GitPython"},{"location":"newsletter/2021_w08/#jinja2","text":"New: Explain how to use Jinja2. Jinja2 is a modern and designer- friendly templating language for Python, modelled after Django\u2019s templates. It is fast, widely used and secure with the optional sandboxed template execution environment. Add installation, usage and basic and advanced template guidelines.","title":"Jinja2"},{"location":"newsletter/2021_w08/#issues","text":"New: Add today's issues. Gadgetbridge improvements Ombi improvements Improvement: Monitor today's issues. Mkdocs migration to 7.x is giving errors with the search bar and repo stats.","title":"Issues"},{"location":"newsletter/2021_w08/#life-management","text":"","title":"Life Management"},{"location":"newsletter/2021_w08/#automation","text":"","title":"Automation"},{"location":"newsletter/2021_w08/#amazfit-band-5","text":"New: Add insights on sleep detection. The sleep tracking using Gadgetbridge is not good at all . After two nights, the band has not been able to detect when I woke in the middle of the night, or when I really woke up, as I usually stay in the bed for a time before standing up. I'll try with the proprietary application soon and compare results. New: Explain how to upgrade the firmware. Gadgetbridge people have a guide on how to upgrade the firmware , you need to get the firmware from the geek doing forum though, so it is interesting to create an account and watch the post. Improvement: Add insights on sleep tracking. You can't use the Withings sleep analyzer without their app (as expected), maybe the Emfit QS is the way to go.","title":"Amazfit Band 5"},{"location":"newsletter/2021_w08/#fitness-tracker","text":"New: Introduce the fitness band in your life automation. Fitness tracker or activity trackers are devices or applications for monitoring and tracking fitness- related metrics such as distance walked or run, calorie consumption, and in some cases heartbeat. It is a type of wearable computer. Explain also why it's interesting","title":"Fitness Tracker"},{"location":"newsletter/2021_w08/#operative-systems","text":"","title":"Operative Systems"},{"location":"newsletter/2021_w08/#android","text":"","title":"Android"},{"location":"newsletter/2021_w08/#gadgetbridge","text":"New: Add more guidelines to reverse engineer the band protocol.","title":"GadgetBridge"},{"location":"newsletter/2021_w08/#other","text":"Correction: Deprecate mkdocs issues. They've been fixed in the last release","title":"Other"},{"location":"newsletter/2021_w09/","text":"DevOps \u2691 Infrastructure as Code \u2691 Helm Git \u2691 Correction: Suggest version 0.8.0 until issue is solved. Newer versions have a bug that makes impossible to use helm_git with a repository that contains just one chart in the root of the git repository. Monitoring \u2691 Prometheus Install \u2691 Correction: Add warning that helm 2 support is dropped. If you want to use the helm chart above 11.1.7 you need to use helm 3. Scrum \u2691 New: Introduce the scrum framework. Scrum is an agile framework for developing, delivering, and sustaining complex products, with an initial emphasis on software development, although it has been used in other fields such as personal task management. It is designed for teams of ten or fewer members, who break their work into goals that can be completed within time-boxed iterations, called sprints, no longer than one month and most commonly two weeks. The Scrum Team track progress in 15-minute time-boxed daily meetings, called daily scrums. At the end of the sprint, the team holds sprint review, to demonstrate the work done, a sprint retrospective to improve continuously, and a sprint planning to prepare next sprint's tasks. In the article I explain: I use to do the meetings : Daily , Refinement , Retros , Reviews and Plannings . The relevant roles . Some definitions , such as definition of done and definition of ready. Coding \u2691 Python \u2691 Code Styling \u2691 Improvement: Don't use try-except to initialize dictionaries. Instead of: try : dictionary [ 'key' ] except KeyError : dictionary [ 'key' ] = {} Use: dictionary . setdefault ( 'key' , {}) Python Snippets \u2691 New: Add date management snippets. Get the week number of a datetime : datetime.datetime(2010, 6, 16).isocalendar()[1] . Get the Monday of a week number : import datetime d = \"2013-W26\" r = datetime . datetime . strptime ( d + '-1' , \"%Y-W%W-%w\" ) Get the month name from a number : import calendar >> calendar . month_name [ 3 ] 'March' * Get ordinal from number def int_to_ordinal ( number : int ) -> str : '''Convert an integer into its ordinal representation. make_ordinal(0) => '0th' make_ordinal(3) => '3rd' make_ordinal(122) => '122nd' make_ordinal(213) => '213th' Args: number: Number to convert Returns: ordinal representation of the number ''' suffix = [ 'th' , 'st' , 'nd' , 'rd' , 'th' ][ min ( number % 10 , 4 )] if 11 <= ( number % 100 ) <= 13 : suffix = 'th' return f \" { number }{ suffix } \" Pydantic \u2691 Correction: How to solve the No name 'BaseModel' in module 'pydantic'. It's still a patch, so I've also monitored the relevant issues Health \u2691 Sleep \u2691 New: Explain the sleep cycle. Humans cycle through two types of sleep in a regular pattern throughout the night with a period of 90 minutes. They were called non-rapid eye movement (NREM) and rapid eye movement (REM). I answer the questions: What is the period of the REM/NREM cycle? What happens to your body in REM and NREM phases? How does the ratio of REM/NREM changes throughout the night? with a possible explanation. Why sleeping 6 hours can make you loose up to 90% of your REM or NREM phases? New: Explain sleeping time and sense distortions. Answer the questions: Why time feels longer in our dreams? How do we loose awareness of the outside world when sleeping? Activism \u2691 New: Define Diversity, Equity and Inclusion. Diversity is the representation and acknowledgement of the multitudes of identities, experiences, and ways of moving through the world. This includes\u2014but is not limited to\u2014ability, age, citizenship status, criminal record and/or incarceration, educational attainment, ethnicity, gender, geographical location, language, nationality, political affiliation, religion, race, sexuality, socioeconomic status, and veteran status. Further, we recognize that each individual's experience is informed by intersections across multiple identities. Equity seeks to ensure respect and equal opportunity for all, using all resources and tools to elevate the voices of under-represented and/or disadvantaged groups. Inclusion is fostering an environment in which people of all identities are welcome, valued, and supported. An inclusive organization solicits, listens to, learns from, and acts on the contributions of all its stakeholders. Operative Systems \u2691 Linux \u2691 Vim Plugins \u2691 New: Follow the issue to add elipsis instead of ... in vim-abolish. mkdocs \u2691 New: Document the Navigation object and the on_nav event. Useful if you develop MkDocs plugins, it holds the information to build the navigation of the site. New: Describe navigation objects used in plugins. Explain how to use the Page, Section, and SectionPage objects. Correction: You need to edit the nav in the on_nav and not in the on_files event. Even though it seems more easy to create the nav structure in the on_files event, by editing the nav dictionary of the config object, there is no way of returning the config object in that event, so we're forced to do it in this event. Arts \u2691 Writing \u2691 Grammar and Orthography \u2691 Improvement: Expand the introduction and add Dave's suggested link. New: Explain where to add your pronouns. Hi, I\u2019m Lyz (he/him), I'm writing to tell you\u2026 New: Explain when to capitalize after a question mark. If the sentence ends after the question mark you should capitalize, if it doesn't end, you shouldn't have used the question mark, since it ends a sentence. Writing Style \u2691 New: Analyze interesting books on writing style. The elements of style by William Strunk Jr and E.B White On writing well by William Zinsser Bird by bird by Anne Lamott On writing by Stephen King New: Explain how to end a letter. Use Sincerely in doubt and Best if you have more confidence. Add a comma after the sign-off and never use Cheers (it's what I've been doing all my life (\u25de\u2038\u25df\uff1b) ). Origami \u2691 New: Add mark1626 digital garden article on origamis.","title":"9th Week of 2021"},{"location":"newsletter/2021_w09/#devops","text":"","title":"DevOps"},{"location":"newsletter/2021_w09/#infrastructure-as-code","text":"","title":"Infrastructure as Code"},{"location":"newsletter/2021_w09/#helm-git","text":"Correction: Suggest version 0.8.0 until issue is solved. Newer versions have a bug that makes impossible to use helm_git with a repository that contains just one chart in the root of the git repository.","title":"Helm Git"},{"location":"newsletter/2021_w09/#monitoring","text":"","title":"Monitoring"},{"location":"newsletter/2021_w09/#prometheus-install","text":"Correction: Add warning that helm 2 support is dropped. If you want to use the helm chart above 11.1.7 you need to use helm 3.","title":"Prometheus Install"},{"location":"newsletter/2021_w09/#scrum","text":"New: Introduce the scrum framework. Scrum is an agile framework for developing, delivering, and sustaining complex products, with an initial emphasis on software development, although it has been used in other fields such as personal task management. It is designed for teams of ten or fewer members, who break their work into goals that can be completed within time-boxed iterations, called sprints, no longer than one month and most commonly two weeks. The Scrum Team track progress in 15-minute time-boxed daily meetings, called daily scrums. At the end of the sprint, the team holds sprint review, to demonstrate the work done, a sprint retrospective to improve continuously, and a sprint planning to prepare next sprint's tasks. In the article I explain: I use to do the meetings : Daily , Refinement , Retros , Reviews and Plannings . The relevant roles . Some definitions , such as definition of done and definition of ready.","title":"Scrum"},{"location":"newsletter/2021_w09/#coding","text":"","title":"Coding"},{"location":"newsletter/2021_w09/#python","text":"","title":"Python"},{"location":"newsletter/2021_w09/#code-styling","text":"Improvement: Don't use try-except to initialize dictionaries. Instead of: try : dictionary [ 'key' ] except KeyError : dictionary [ 'key' ] = {} Use: dictionary . setdefault ( 'key' , {})","title":"Code Styling"},{"location":"newsletter/2021_w09/#python-snippets","text":"New: Add date management snippets. Get the week number of a datetime : datetime.datetime(2010, 6, 16).isocalendar()[1] . Get the Monday of a week number : import datetime d = \"2013-W26\" r = datetime . datetime . strptime ( d + '-1' , \"%Y-W%W-%w\" ) Get the month name from a number : import calendar >> calendar . month_name [ 3 ] 'March' * Get ordinal from number def int_to_ordinal ( number : int ) -> str : '''Convert an integer into its ordinal representation. make_ordinal(0) => '0th' make_ordinal(3) => '3rd' make_ordinal(122) => '122nd' make_ordinal(213) => '213th' Args: number: Number to convert Returns: ordinal representation of the number ''' suffix = [ 'th' , 'st' , 'nd' , 'rd' , 'th' ][ min ( number % 10 , 4 )] if 11 <= ( number % 100 ) <= 13 : suffix = 'th' return f \" { number }{ suffix } \"","title":"Python Snippets"},{"location":"newsletter/2021_w09/#pydantic","text":"Correction: How to solve the No name 'BaseModel' in module 'pydantic'. It's still a patch, so I've also monitored the relevant issues","title":"Pydantic"},{"location":"newsletter/2021_w09/#health","text":"","title":"Health"},{"location":"newsletter/2021_w09/#sleep","text":"New: Explain the sleep cycle. Humans cycle through two types of sleep in a regular pattern throughout the night with a period of 90 minutes. They were called non-rapid eye movement (NREM) and rapid eye movement (REM). I answer the questions: What is the period of the REM/NREM cycle? What happens to your body in REM and NREM phases? How does the ratio of REM/NREM changes throughout the night? with a possible explanation. Why sleeping 6 hours can make you loose up to 90% of your REM or NREM phases? New: Explain sleeping time and sense distortions. Answer the questions: Why time feels longer in our dreams? How do we loose awareness of the outside world when sleeping?","title":"Sleep"},{"location":"newsletter/2021_w09/#activism","text":"New: Define Diversity, Equity and Inclusion. Diversity is the representation and acknowledgement of the multitudes of identities, experiences, and ways of moving through the world. This includes\u2014but is not limited to\u2014ability, age, citizenship status, criminal record and/or incarceration, educational attainment, ethnicity, gender, geographical location, language, nationality, political affiliation, religion, race, sexuality, socioeconomic status, and veteran status. Further, we recognize that each individual's experience is informed by intersections across multiple identities. Equity seeks to ensure respect and equal opportunity for all, using all resources and tools to elevate the voices of under-represented and/or disadvantaged groups. Inclusion is fostering an environment in which people of all identities are welcome, valued, and supported. An inclusive organization solicits, listens to, learns from, and acts on the contributions of all its stakeholders.","title":"Activism"},{"location":"newsletter/2021_w09/#operative-systems","text":"","title":"Operative Systems"},{"location":"newsletter/2021_w09/#linux","text":"","title":"Linux"},{"location":"newsletter/2021_w09/#vim-plugins","text":"New: Follow the issue to add elipsis instead of ... in vim-abolish.","title":"Vim Plugins"},{"location":"newsletter/2021_w09/#mkdocs","text":"New: Document the Navigation object and the on_nav event. Useful if you develop MkDocs plugins, it holds the information to build the navigation of the site. New: Describe navigation objects used in plugins. Explain how to use the Page, Section, and SectionPage objects. Correction: You need to edit the nav in the on_nav and not in the on_files event. Even though it seems more easy to create the nav structure in the on_files event, by editing the nav dictionary of the config object, there is no way of returning the config object in that event, so we're forced to do it in this event.","title":"mkdocs"},{"location":"newsletter/2021_w09/#arts","text":"","title":"Arts"},{"location":"newsletter/2021_w09/#writing","text":"","title":"Writing"},{"location":"newsletter/2021_w09/#grammar-and-orthography","text":"Improvement: Expand the introduction and add Dave's suggested link. New: Explain where to add your pronouns. Hi, I\u2019m Lyz (he/him), I'm writing to tell you\u2026 New: Explain when to capitalize after a question mark. If the sentence ends after the question mark you should capitalize, if it doesn't end, you shouldn't have used the question mark, since it ends a sentence.","title":"Grammar and Orthography"},{"location":"newsletter/2021_w09/#writing-style","text":"New: Analyze interesting books on writing style. The elements of style by William Strunk Jr and E.B White On writing well by William Zinsser Bird by bird by Anne Lamott On writing by Stephen King New: Explain how to end a letter. Use Sincerely in doubt and Best if you have more confidence. Add a comma after the sign-off and never use Cheers (it's what I've been doing all my life (\u25de\u2038\u25df\uff1b) ).","title":"Writing Style"},{"location":"newsletter/2021_w09/#origami","text":"New: Add mark1626 digital garden article on origamis.","title":"Origami"},{"location":"newsletter/2021_w10/","text":"DevOps \u2691 Monitoring \u2691 Prometheus Install \u2691 Improvement: Add upgrading notes from 10.x -> 11.1.7. Don't upgrade to 12.x if you're still using Helm 2. Coding \u2691 Python \u2691 Python Snippets \u2691 New: Add file management snippets. Remove the extension of a file . Iterate over the files of a directory . Create directory Touch a file Improvement: Get the first day of next month. Life Management \u2691 Automation \u2691 Fitness Tracker \u2691 Improvement: Discovery of wasp-os and Colmi P8. wasp-os is an open source firmware for smart watches that are based on the nRF52 family of microcontrollers. Fully supported by gadgetbridge , Wasp-os features full heart rate monitoring and step counting support together with multiple clock faces, a stopwatch, an alarm clock, a countdown timer, a calculator and lots of other games and utilities. All of this, and still with access to the MicroPython REPL for interactive tweaking, development and testing. One of the supported devices, the Colmi P8 , looks really good. Operative Systems \u2691 Linux \u2691 ActivityWatch \u2691 New: Introduce ActivityWatch tracking software. It's a web application that can be installed both in Linux and Android that automatically tracks where you spend the time on. Super interesting for life logging and automating stuff. Until I save some time to react on the data, I'll just gather it and see how to aggregate it. mkdocs \u2691 Correction: Explain how to add files through a plugin. Long story short, use the on_config event instead of on_files and on_nav if you need to add files and want to change the navigation menu. Syncthing \u2691 Improvement: Mention privacy configurations. Disable the Global Discovery and Relaying connections options. Android \u2691 Signal \u2691 New: Introduce the messaging app and how to decrypt the backups.","title":"10th Week of 2021"},{"location":"newsletter/2021_w10/#devops","text":"","title":"DevOps"},{"location":"newsletter/2021_w10/#monitoring","text":"","title":"Monitoring"},{"location":"newsletter/2021_w10/#prometheus-install","text":"Improvement: Add upgrading notes from 10.x -> 11.1.7. Don't upgrade to 12.x if you're still using Helm 2.","title":"Prometheus Install"},{"location":"newsletter/2021_w10/#coding","text":"","title":"Coding"},{"location":"newsletter/2021_w10/#python","text":"","title":"Python"},{"location":"newsletter/2021_w10/#python-snippets","text":"New: Add file management snippets. Remove the extension of a file . Iterate over the files of a directory . Create directory Touch a file Improvement: Get the first day of next month.","title":"Python Snippets"},{"location":"newsletter/2021_w10/#life-management","text":"","title":"Life Management"},{"location":"newsletter/2021_w10/#automation","text":"","title":"Automation"},{"location":"newsletter/2021_w10/#fitness-tracker","text":"Improvement: Discovery of wasp-os and Colmi P8. wasp-os is an open source firmware for smart watches that are based on the nRF52 family of microcontrollers. Fully supported by gadgetbridge , Wasp-os features full heart rate monitoring and step counting support together with multiple clock faces, a stopwatch, an alarm clock, a countdown timer, a calculator and lots of other games and utilities. All of this, and still with access to the MicroPython REPL for interactive tweaking, development and testing. One of the supported devices, the Colmi P8 , looks really good.","title":"Fitness Tracker"},{"location":"newsletter/2021_w10/#operative-systems","text":"","title":"Operative Systems"},{"location":"newsletter/2021_w10/#linux","text":"","title":"Linux"},{"location":"newsletter/2021_w10/#activitywatch","text":"New: Introduce ActivityWatch tracking software. It's a web application that can be installed both in Linux and Android that automatically tracks where you spend the time on. Super interesting for life logging and automating stuff. Until I save some time to react on the data, I'll just gather it and see how to aggregate it.","title":"ActivityWatch"},{"location":"newsletter/2021_w10/#mkdocs","text":"Correction: Explain how to add files through a plugin. Long story short, use the on_config event instead of on_files and on_nav if you need to add files and want to change the navigation menu.","title":"mkdocs"},{"location":"newsletter/2021_w10/#syncthing","text":"Improvement: Mention privacy configurations. Disable the Global Discovery and Relaying connections options.","title":"Syncthing"},{"location":"newsletter/2021_w10/#android","text":"","title":"Android"},{"location":"newsletter/2021_w10/#signal","text":"New: Introduce the messaging app and how to decrypt the backups.","title":"Signal"},{"location":"newsletter/2021_w11/","text":"Coding \u2691 Python \u2691 New: Add python landing page. Javascript \u2691 MermaidJS \u2691 New: Introduce the diagram library and how to make flowchart diagrams. MermaidJS is a Javascript library that lets you create diagrams using text and code. It can render the next diagram types : Flowchart Sequence. Gantt Class Git graph Entity Relationship User journey Issues \u2691 Improvement: Track python dependency errors. Software Architecture \u2691 Architecture Decision Record \u2691 Improvement: Explain how to show relationship between ADRs. Suggest a mermaidjs diagram to show the state of the project ADRs. Life Management \u2691 Automation \u2691 Life Automation \u2691 New: Suggest organize to act on computer file changes. organize looks good for automating processes on files. Maybe it's interesting to run it with inotifywait instead of with a cron job . Operative Systems \u2691 Linux \u2691 ActivityWatch \u2691 Improvement: Add week insights. The browser watcher is not very accurate . The vim editor watcher doesn't add git branch information . Syncing data between devices is not yet supported . Tabs vs Buffers \u2691 New: Explain how to use tabs, buffers and windows in vim. Vim Plugins \u2691 Correction: Forget to use abolish to insert the elipsis symbol. Tpope said that it's not going to happen. New: Introduce vim-easymotion. EasyMotion provides a much simpler way to use some motions in vim. It takes the <number> out of <number>w or <number>f{char} by highlighting all possible choices and allowing you to press one key to jump directly to the target. When one of the available motions is triggered, all visible text preceding or following the cursor is faded, and motion targets are highlighted. Reorganization: Move vim-test to the plugins page. mkdocs \u2691 New: Explain how to use MermaidJS diagrams. New: Explain how to test mkdocs plugins. Peek \u2691 New: Introduce Peek the screen recorder. Peek is a simple animated GIF screen recorder with an easy to use interface. If you try to use it with i3, you're going to have a bad time, you'd need to install Compton , and then the elements may not even be clickable . Vim \u2691 New: Add vim landing page. Arts \u2691 Writing \u2691 Build your own Digital Garden \u2691 New: Add textstat tests. To analyze the text readability Other \u2691 New: Introduce Outrun. Outrun lets you execute a local command using the processing power of another Linux machine.","title":"11th Week of 2021"},{"location":"newsletter/2021_w11/#coding","text":"","title":"Coding"},{"location":"newsletter/2021_w11/#python","text":"New: Add python landing page.","title":"Python"},{"location":"newsletter/2021_w11/#javascript","text":"","title":"Javascript"},{"location":"newsletter/2021_w11/#mermaidjs","text":"New: Introduce the diagram library and how to make flowchart diagrams. MermaidJS is a Javascript library that lets you create diagrams using text and code. It can render the next diagram types : Flowchart Sequence. Gantt Class Git graph Entity Relationship User journey","title":"MermaidJS"},{"location":"newsletter/2021_w11/#issues","text":"Improvement: Track python dependency errors.","title":"Issues"},{"location":"newsletter/2021_w11/#software-architecture","text":"","title":"Software Architecture"},{"location":"newsletter/2021_w11/#architecture-decision-record","text":"Improvement: Explain how to show relationship between ADRs. Suggest a mermaidjs diagram to show the state of the project ADRs.","title":"Architecture Decision Record"},{"location":"newsletter/2021_w11/#life-management","text":"","title":"Life Management"},{"location":"newsletter/2021_w11/#automation","text":"","title":"Automation"},{"location":"newsletter/2021_w11/#life-automation","text":"New: Suggest organize to act on computer file changes. organize looks good for automating processes on files. Maybe it's interesting to run it with inotifywait instead of with a cron job .","title":"Life Automation"},{"location":"newsletter/2021_w11/#operative-systems","text":"","title":"Operative Systems"},{"location":"newsletter/2021_w11/#linux","text":"","title":"Linux"},{"location":"newsletter/2021_w11/#activitywatch","text":"Improvement: Add week insights. The browser watcher is not very accurate . The vim editor watcher doesn't add git branch information . Syncing data between devices is not yet supported .","title":"ActivityWatch"},{"location":"newsletter/2021_w11/#tabs-vs-buffers","text":"New: Explain how to use tabs, buffers and windows in vim.","title":"Tabs vs Buffers"},{"location":"newsletter/2021_w11/#vim-plugins","text":"Correction: Forget to use abolish to insert the elipsis symbol. Tpope said that it's not going to happen. New: Introduce vim-easymotion. EasyMotion provides a much simpler way to use some motions in vim. It takes the <number> out of <number>w or <number>f{char} by highlighting all possible choices and allowing you to press one key to jump directly to the target. When one of the available motions is triggered, all visible text preceding or following the cursor is faded, and motion targets are highlighted. Reorganization: Move vim-test to the plugins page.","title":"Vim Plugins"},{"location":"newsletter/2021_w11/#mkdocs","text":"New: Explain how to use MermaidJS diagrams. New: Explain how to test mkdocs plugins.","title":"mkdocs"},{"location":"newsletter/2021_w11/#peek","text":"New: Introduce Peek the screen recorder. Peek is a simple animated GIF screen recorder with an easy to use interface. If you try to use it with i3, you're going to have a bad time, you'd need to install Compton , and then the elements may not even be clickable .","title":"Peek"},{"location":"newsletter/2021_w11/#vim","text":"New: Add vim landing page.","title":"Vim"},{"location":"newsletter/2021_w11/#arts","text":"","title":"Arts"},{"location":"newsletter/2021_w11/#writing","text":"","title":"Writing"},{"location":"newsletter/2021_w11/#build-your-own-digital-garden","text":"New: Add textstat tests. To analyze the text readability","title":"Build your own Digital Garden"},{"location":"newsletter/2021_w11/#other","text":"New: Introduce Outrun. Outrun lets you execute a local command using the processing power of another Linux machine.","title":"Other"},{"location":"newsletter/2021_w12/","text":"Coding \u2691 Python \u2691 Python Snippets \u2691 New: Explain how to test directories and files. Issues \u2691 Correction: Gitdb has updated smmap. Arts \u2691 Writing \u2691 Grammar and Orthography \u2691 New: Explain how to use the singular they .","title":"12th Week of 2021"},{"location":"newsletter/2021_w12/#coding","text":"","title":"Coding"},{"location":"newsletter/2021_w12/#python","text":"","title":"Python"},{"location":"newsletter/2021_w12/#python-snippets","text":"New: Explain how to test directories and files.","title":"Python Snippets"},{"location":"newsletter/2021_w12/#issues","text":"Correction: Gitdb has updated smmap.","title":"Issues"},{"location":"newsletter/2021_w12/#arts","text":"","title":"Arts"},{"location":"newsletter/2021_w12/#writing","text":"","title":"Writing"},{"location":"newsletter/2021_w12/#grammar-and-orthography","text":"New: Explain how to use the singular they .","title":"Grammar and Orthography"},{"location":"newsletter/2021_w13/","text":"Introduction \u2691 Reorganization: Merge the Meta article into the index. Projects \u2691 Improvement: Add mkdocs-newsletter as a dormant plant. MkDocs plugin to show the changes of documentation repositories in a user friendly format, at the same time that it's easy for the authors to maintain. It creates daily, weekly, monthly and yearly newsletter articles with the changes of each period. Those pages, stored under the Newsletters section, are filled with the changes extracted from the commit messages of the git history. The changes are grouped by categories, subcategories and then by file using the order of the site's navigation structure. RSS feeds are also created for each newsletter type, so it's easy for people to keep updated with the evolution of the site. Reorganization: Update and reorganize projects. Following the digital garden metaphor Reorganization: Merge the wish_list article into the projects. New: Add seed to follow the updates of software. New: Add seed to automatically update the dockers of maintained services. Coding \u2691 Issues \u2691 New: Jellyfin 10.7.1 broke the login page. Don't upgrade till it's solved, as the rollback is not easy. Correction: Jellyfin login page problem after upgrade to 10.7.X is solved. Surprisingly the instructions in #5489 solved it. systemctl stop jellyfin.service mv /var/lib/jellyfin/data/jellyfin.db { ,.bak } systemctl start jellyfin.service [ Go to JF URL, get asked to log in even though there are no Users in the JF DB now ] systemctl stop jellyfin.service mv /var/lib/jellyfin/data/jellyfin.db { .bak, } systemctl start jellyfin.service Operative Systems \u2691 Linux \u2691 elasticsearch \u2691 New: Explain how to reindex an index. Jellyfin \u2691 New: Introduce the media system and monitor interesting issues. Jellyfin is a Free Software Media System that puts you in control of managing and streaming your media. It is an alternative to the proprietary Emby and Plex, to provide media from a dedicated server to end-user devices via multiple apps. Jellyfin is descended from Emby's 3.5.2 release and ported to the .NET Core framework to enable full cross-platform support. There are no strings attached, no premium licenses or features, and no hidden agendas: just a team who want to build something better and work together to achieve it. mkdocs \u2691 New: Explain additions of version 7.1.0 of the material theme. Dark-light mode switch . Back to top button .","title":"13th Week of 2021"},{"location":"newsletter/2021_w13/#introduction","text":"Reorganization: Merge the Meta article into the index.","title":"Introduction"},{"location":"newsletter/2021_w13/#projects","text":"Improvement: Add mkdocs-newsletter as a dormant plant. MkDocs plugin to show the changes of documentation repositories in a user friendly format, at the same time that it's easy for the authors to maintain. It creates daily, weekly, monthly and yearly newsletter articles with the changes of each period. Those pages, stored under the Newsletters section, are filled with the changes extracted from the commit messages of the git history. The changes are grouped by categories, subcategories and then by file using the order of the site's navigation structure. RSS feeds are also created for each newsletter type, so it's easy for people to keep updated with the evolution of the site. Reorganization: Update and reorganize projects. Following the digital garden metaphor Reorganization: Merge the wish_list article into the projects. New: Add seed to follow the updates of software. New: Add seed to automatically update the dockers of maintained services.","title":"Projects"},{"location":"newsletter/2021_w13/#coding","text":"","title":"Coding"},{"location":"newsletter/2021_w13/#issues","text":"New: Jellyfin 10.7.1 broke the login page. Don't upgrade till it's solved, as the rollback is not easy. Correction: Jellyfin login page problem after upgrade to 10.7.X is solved. Surprisingly the instructions in #5489 solved it. systemctl stop jellyfin.service mv /var/lib/jellyfin/data/jellyfin.db { ,.bak } systemctl start jellyfin.service [ Go to JF URL, get asked to log in even though there are no Users in the JF DB now ] systemctl stop jellyfin.service mv /var/lib/jellyfin/data/jellyfin.db { .bak, } systemctl start jellyfin.service","title":"Issues"},{"location":"newsletter/2021_w13/#operative-systems","text":"","title":"Operative Systems"},{"location":"newsletter/2021_w13/#linux","text":"","title":"Linux"},{"location":"newsletter/2021_w13/#elasticsearch","text":"New: Explain how to reindex an index.","title":"elasticsearch"},{"location":"newsletter/2021_w13/#jellyfin","text":"New: Introduce the media system and monitor interesting issues. Jellyfin is a Free Software Media System that puts you in control of managing and streaming your media. It is an alternative to the proprietary Emby and Plex, to provide media from a dedicated server to end-user devices via multiple apps. Jellyfin is descended from Emby's 3.5.2 release and ported to the .NET Core framework to enable full cross-platform support. There are no strings attached, no premium licenses or features, and no hidden agendas: just a team who want to build something better and work together to achieve it.","title":"Jellyfin"},{"location":"newsletter/2021_w13/#mkdocs","text":"New: Explain additions of version 7.1.0 of the material theme. Dark-light mode switch . Back to top button .","title":"mkdocs"},{"location":"newsletter/2021_w14/","text":"Projects \u2691 Improvement: Explain the updates on the repository-orm project. In the latest version 0.2.0 , we added: Support for the TinyDB repository . Support for regular expressions in the search method. Easier repository loading with load_repository function. Improvement: Add a link to the meilisearch blog . New: Create the quantified self project. With links to the two starting points HPI and bionic . Coding \u2691 Python \u2691 Improvement: Add aiomultiprocess to the list of libraries to test. aiomultiprocess : Presents a simple interface, while running a full AsyncIO event loop on each child process, enabling levels of concurrency never before seen in a Python application. Each child process can execute multiple coroutines at once, limited only by the workload and number of cores available. New: Add interesting links on how to write good documentation. I would like to refactor divio's and Vue's guidelines and apply it to my projects. Type Hints \u2691 Improvement: Explain how to ignore a linter error and a type error. With # type: ignore # noqa: W0212 DeepDiff \u2691 Improvement: Add warning that regular expressions are not yet supported. Until #239 is merged, the official library doesn't support searching for regular expressions. You can use my fork instead. Python Snippets \u2691 New: Explain how to install dependencies from git repositories. With pip you can : pip install git+git://github.com/path/to/repository@master If you want to hard code it in your setup.py , you need to: install_requires = [ 'some-pkg @ git+ssh://git@github.com/someorgname/pkg-repo-name@v1.1#egg=some-pkg' , ] Correction: Explain how to create PyPI valid packages with direct dependencies. It looks like PyPI don't want pip to reach out to URLs outside their site when installing from PyPI. So you can't define the direct dependencies in the install_requires . Instead you need to install them in a PostInstall custom script. Ugly as hell. Pydantic \u2691 Improvement: Change parse_obj definition to find how to import pydantic models from dictionary. sqlite3 \u2691 New: Explain how to implement the REGEXP operator with Python. TinyDB \u2691 New: Explain how to serialize datetime objects. SQLite \u2691 New: Explain how to configure sqlite to be able to use the REGEXP operator. It's not enabled by default. Operative Systems \u2691 Linux \u2691 Vim Plugins \u2691 Correction: Typo. There was a missing comma in the list. HAProxy \u2691 New: Add interesting guidelines on how to configure HAProxy in AWS . ffmpeg \u2691 New: Introduce the program and multiple of it's uses. ffmpeg is a complete, cross-platform solution to record, convert and stream audio and video. Other \u2691 Correction: Broken links. Removed the link to (everything_i_know.md) since it no longer exists. Updated some links that where broken due to a folder structure change.","title":"14th Week of 2021"},{"location":"newsletter/2021_w14/#projects","text":"Improvement: Explain the updates on the repository-orm project. In the latest version 0.2.0 , we added: Support for the TinyDB repository . Support for regular expressions in the search method. Easier repository loading with load_repository function. Improvement: Add a link to the meilisearch blog . New: Create the quantified self project. With links to the two starting points HPI and bionic .","title":"Projects"},{"location":"newsletter/2021_w14/#coding","text":"","title":"Coding"},{"location":"newsletter/2021_w14/#python","text":"Improvement: Add aiomultiprocess to the list of libraries to test. aiomultiprocess : Presents a simple interface, while running a full AsyncIO event loop on each child process, enabling levels of concurrency never before seen in a Python application. Each child process can execute multiple coroutines at once, limited only by the workload and number of cores available. New: Add interesting links on how to write good documentation. I would like to refactor divio's and Vue's guidelines and apply it to my projects.","title":"Python"},{"location":"newsletter/2021_w14/#type-hints","text":"Improvement: Explain how to ignore a linter error and a type error. With # type: ignore # noqa: W0212","title":"Type Hints"},{"location":"newsletter/2021_w14/#deepdiff","text":"Improvement: Add warning that regular expressions are not yet supported. Until #239 is merged, the official library doesn't support searching for regular expressions. You can use my fork instead.","title":"DeepDiff"},{"location":"newsletter/2021_w14/#python-snippets","text":"New: Explain how to install dependencies from git repositories. With pip you can : pip install git+git://github.com/path/to/repository@master If you want to hard code it in your setup.py , you need to: install_requires = [ 'some-pkg @ git+ssh://git@github.com/someorgname/pkg-repo-name@v1.1#egg=some-pkg' , ] Correction: Explain how to create PyPI valid packages with direct dependencies. It looks like PyPI don't want pip to reach out to URLs outside their site when installing from PyPI. So you can't define the direct dependencies in the install_requires . Instead you need to install them in a PostInstall custom script. Ugly as hell.","title":"Python Snippets"},{"location":"newsletter/2021_w14/#pydantic","text":"Improvement: Change parse_obj definition to find how to import pydantic models from dictionary.","title":"Pydantic"},{"location":"newsletter/2021_w14/#sqlite3","text":"New: Explain how to implement the REGEXP operator with Python.","title":"sqlite3"},{"location":"newsletter/2021_w14/#tinydb","text":"New: Explain how to serialize datetime objects.","title":"TinyDB"},{"location":"newsletter/2021_w14/#sqlite","text":"New: Explain how to configure sqlite to be able to use the REGEXP operator. It's not enabled by default.","title":"SQLite"},{"location":"newsletter/2021_w14/#operative-systems","text":"","title":"Operative Systems"},{"location":"newsletter/2021_w14/#linux","text":"","title":"Linux"},{"location":"newsletter/2021_w14/#vim-plugins","text":"Correction: Typo. There was a missing comma in the list.","title":"Vim Plugins"},{"location":"newsletter/2021_w14/#haproxy","text":"New: Add interesting guidelines on how to configure HAProxy in AWS .","title":"HAProxy"},{"location":"newsletter/2021_w14/#ffmpeg","text":"New: Introduce the program and multiple of it's uses. ffmpeg is a complete, cross-platform solution to record, convert and stream audio and video.","title":"ffmpeg"},{"location":"newsletter/2021_w14/#other","text":"Correction: Broken links. Removed the link to (everything_i_know.md) since it no longer exists. Updated some links that where broken due to a folder structure change.","title":"Other"},{"location":"newsletter/2021_w15/","text":"Projects \u2691 New: Sketch how to automate repetitive tasks prompted by email events. Most of the emails I receive require repetitive actions that can be automated, I've stumbled upon notmuchmail , which looks very promising. A friend suggested to use afew for tagging, and I'd probably use alot to interact with the system (and finally be able to use email from the cli). Improvement: Add interesting interface. For the interface adri's memex looks awesome! It's inspired in the Andrew Louis talk Building a Memex whose blog posts seems to be a gold mine. Also look at hpi's compilation . New: Sketch how to improve the launching of applications with i3wm. In the past I tried installing rofi without success, I should try again. If the default features are not enough, check adi1090x's custom resources . Improvement: Show the changes of repository-orm 0.3.1. +* Add first and last methods to the repositories. +* Make entity id_ definition optional. +* add _model_name attribute to entities. Coding \u2691 Python \u2691 Improvement: Add FastAPI docs as a model to study and follow. FastAPI \u2691 New: Add beets system as a first approach. When building Python applications, it's good to develop the core of your program, and allow extension via plugins. I still don't know how to do it, but Beets plugin system looks awesome for a first start. New: Introduce FastAPI the pydantic friendly python framework to build APIs. FastAPI is a modern, fast (high-performance), web framework for building APIs with Python 3.6+ based on standard Python type hints. New: Sum up the basic documentation. Explain how to: Sending data to the server : Through path parameters , query parameters and body requests . Handle errors . Update data Configure OpenAPI Test FastAPI applications And add a lot of more interesting features I've discovered. Pytest \u2691 New: Explain how to exclude code from the coverage report. Add # pragma: no cover . NetworkX \u2691 New: Introduce the python library. NetworkX is a Python package for the creation, manipulation, and study of the structure, dynamics, and functions of complex networks. Pydantic \u2691 New: Explain how to use private attributes. With the PrivateAttr object. Operative Systems \u2691 Linux \u2691 Beets \u2691 New: Introduce Beets the music management library. Beets is a music management library used to get your music collection right once and for all. It catalogs your collection, automatically improving its metadata as it goes using the MusicBrainz database. Then it provides a set of tools for manipulating and accessing your music. Hushboard \u2691 New: Introduce Husboard. Hushboard is an utility that mutes your microphone while you\u2019re typing. (Thanks M0wer !) Modipy \u2691 New: Introduce the music server. Mopidy is an extensible music server written in Python, that plays perfectly with beets and the MPD ecosystem. The awesome documentation, being Python based, the extension system, JSON-RPC, and JavaScript APIs make Mopidy a perfect base for your projects. Arts \u2691 Writing \u2691 Grammar and Orthography \u2691 New: Explain when to write won't or wont. Won't is the correct way to contract will not. Wont is a synonym of \"a habit\". For example, \"He went for a morning jog, as was his wont\".","title":"15th Week of 2021"},{"location":"newsletter/2021_w15/#projects","text":"New: Sketch how to automate repetitive tasks prompted by email events. Most of the emails I receive require repetitive actions that can be automated, I've stumbled upon notmuchmail , which looks very promising. A friend suggested to use afew for tagging, and I'd probably use alot to interact with the system (and finally be able to use email from the cli). Improvement: Add interesting interface. For the interface adri's memex looks awesome! It's inspired in the Andrew Louis talk Building a Memex whose blog posts seems to be a gold mine. Also look at hpi's compilation . New: Sketch how to improve the launching of applications with i3wm. In the past I tried installing rofi without success, I should try again. If the default features are not enough, check adi1090x's custom resources . Improvement: Show the changes of repository-orm 0.3.1. +* Add first and last methods to the repositories. +* Make entity id_ definition optional. +* add _model_name attribute to entities.","title":"Projects"},{"location":"newsletter/2021_w15/#coding","text":"","title":"Coding"},{"location":"newsletter/2021_w15/#python","text":"Improvement: Add FastAPI docs as a model to study and follow.","title":"Python"},{"location":"newsletter/2021_w15/#fastapi","text":"New: Add beets system as a first approach. When building Python applications, it's good to develop the core of your program, and allow extension via plugins. I still don't know how to do it, but Beets plugin system looks awesome for a first start. New: Introduce FastAPI the pydantic friendly python framework to build APIs. FastAPI is a modern, fast (high-performance), web framework for building APIs with Python 3.6+ based on standard Python type hints. New: Sum up the basic documentation. Explain how to: Sending data to the server : Through path parameters , query parameters and body requests . Handle errors . Update data Configure OpenAPI Test FastAPI applications And add a lot of more interesting features I've discovered.","title":"FastAPI"},{"location":"newsletter/2021_w15/#pytest","text":"New: Explain how to exclude code from the coverage report. Add # pragma: no cover .","title":"Pytest"},{"location":"newsletter/2021_w15/#networkx","text":"New: Introduce the python library. NetworkX is a Python package for the creation, manipulation, and study of the structure, dynamics, and functions of complex networks.","title":"NetworkX"},{"location":"newsletter/2021_w15/#pydantic","text":"New: Explain how to use private attributes. With the PrivateAttr object.","title":"Pydantic"},{"location":"newsletter/2021_w15/#operative-systems","text":"","title":"Operative Systems"},{"location":"newsletter/2021_w15/#linux","text":"","title":"Linux"},{"location":"newsletter/2021_w15/#beets","text":"New: Introduce Beets the music management library. Beets is a music management library used to get your music collection right once and for all. It catalogs your collection, automatically improving its metadata as it goes using the MusicBrainz database. Then it provides a set of tools for manipulating and accessing your music.","title":"Beets"},{"location":"newsletter/2021_w15/#hushboard","text":"New: Introduce Husboard. Hushboard is an utility that mutes your microphone while you\u2019re typing. (Thanks M0wer !)","title":"Hushboard"},{"location":"newsletter/2021_w15/#modipy","text":"New: Introduce the music server. Mopidy is an extensible music server written in Python, that plays perfectly with beets and the MPD ecosystem. The awesome documentation, being Python based, the extension system, JSON-RPC, and JavaScript APIs make Mopidy a perfect base for your projects.","title":"Modipy"},{"location":"newsletter/2021_w15/#arts","text":"","title":"Arts"},{"location":"newsletter/2021_w15/#writing","text":"","title":"Writing"},{"location":"newsletter/2021_w15/#grammar-and-orthography","text":"New: Explain when to write won't or wont. Won't is the correct way to contract will not. Wont is a synonym of \"a habit\". For example, \"He went for a morning jog, as was his wont\".","title":"Grammar and Orthography"},{"location":"newsletter/2021_w16/","text":"Projects \u2691 Improvement: Add [woop awesome quantified self](https://github.com/woop/awesome-quantified-self) resources to the research list. New: Add project to migrate software bug tracker to a vendor free one like [git-bug](https://github.com/MichaelMure/git-bug). New: Improve the notification management in Linux. Create new seed project to be able to group and silence the notifications under a custom logic. For example: If I want to focus on a task, only show the most important ones. Only show alerts once every X minutes. Or define that I want to receive them the first 10 minutes of every hour. If I'm not working, silence all work alerts. New: Improve the hard drive monitor system. Create new seed project to use something like scrutiny (there's a linuxserver image ) to collect and display the information. For alerts, use one of their supported providers . New: Aggregate all notifications. Instead of reading the email, github, gitlab, discourse, reddit notifications, aggregate all in one place and show them to the user in a nice command line interface. For the aggregator server, my first choice would be gotify . New: Add faker-optional to the dormant plant projects. New: Add seedling project to create factoryboy factories from pydantic models automatically. DevOps \u2691 Continuous Integration \u2691 Flakehell \u2691 Correction: Update the git repository. The existent repository has been archived in favor of this one New: Explain how to patch the extended_default_ignore error for versions > 3.9.0. Add to your your pyproject.toml : [tool.flakehell] extended_default_ignore = [] # add this Coding \u2691 Python \u2691 New: Add apprise to the interesting libraries to explore. apprise : Allows you to send a notification to almost all of the most popular notification services available to us today such as: Linux, Telegram, Discord, Slack, Amazon SNS, Gotify, etc. Look at all the supported notifications (\u00ac\u00ba-\u00b0)\u00ac . New: Add kivi and kivimd to the interesting libraries to explore. kivi is used to create android/Linux/iOS/Windows applications with python. Use it with kivimd to make it beautiful, check the examples and the docs . Boto3 \u2691 New: Introduce the AWS SDK library and explain how to test it. Boto3 is the AWS SDK for Python to create, configure, and manage AWS services, such as Amazon Elastic Compute Cloud (Amazon EC2) and Amazon Simple Storage Service (Amazon S3). The SDK provides an object-oriented API as well as low-level access to AWS services. For testing , try to use moto , using the Botocore's stubber as fallback option. New: Explain how to test ec2, route53, s3, and rds resources. New: Explain how to test vpc and auto scaling group resources. Improvement: Explain how to extract the instance when testing autoscaling groups. Also track the issue to add support to launch templates . New: Explain how to test security groups. Logging \u2691 New: Explain how to log python program exceptions better than to a file. Using logging to write write exceptions and breadcrumbs to a file might not be the best solution because unless you look at it directly most errors will pass unnoticed. To actively monitor and react to code exceptions use an application monitoring platform like sentry . In the article I explain what are the advantages of using this solution and do a comparison between Sentry and GlitchTip . DeepDiff \u2691 Improvement: Remove advice to use my fork instead. The original one has already merged my PR \uff3c\\ \u0669( \u141b )\u0648 /\uff0f . Beware though as the regexp are not enabled by default (against my will). You need to use the use_regexp=True as an argument to grep or DeepSearch . FactoryBoy \u2691 New: Explain how to use Enum with factoryboy. Faker \u2691 New: Explain how to create Optional data. faker-optional is a custom faker provider that acts as a wrapper over other Faker providers to return their value or None . Useful to create data of type Optional[Any] . FastAPI \u2691 New: Explain how to log exceptions to sentry. New: Explain how to send raw data to the client. With the Response object. New: Explain how to configure the application. New: Explain how to inject a testing configuration in the tests. Python Snippets \u2691 Correction: Add warning about the method to use direct dependencies. Last time I used this solution, when I added the library on a setup.py the direct dependencies weren't installed :S Pydantic \u2691 New: Explain how to update entity attributes with a dictionary. You can create a new object with the new data using the update argument of the copy entity method. rich \u2691 New: Introduce the python cli builder library and it's progress bar. Rich is a Python library for rich text and beautiful formatting in the terminal. Check out the beautiful progress bar: pip install rich python -m rich.progress Ruamel YAML \u2691 Improvement: Suggest to use ruyaml instead of ruamel.yaml. As it's maintained by the community and versioned with git. Arts \u2691 Writing \u2691 New: Explain when to use I'm good or I'm well. Use I'm well when referring to being ill, use I'm good for the rest. Other \u2691 New: Explain how to select a random choice from Enum objects. pydantic uses Enum objects to define the choices of fields , so we need them to create the factories of those objects. New: Improve the periodic tasks and application metrics monitoring. Setup an healthchecks instance with the linuxserver image to monitor cronjobs. For the notifications either use the prometheus metrics or an apprise compatible system.","title":"16th Week of 2021"},{"location":"newsletter/2021_w16/#projects","text":"Improvement: Add [woop awesome quantified self](https://github.com/woop/awesome-quantified-self) resources to the research list. New: Add project to migrate software bug tracker to a vendor free one like [git-bug](https://github.com/MichaelMure/git-bug). New: Improve the notification management in Linux. Create new seed project to be able to group and silence the notifications under a custom logic. For example: If I want to focus on a task, only show the most important ones. Only show alerts once every X minutes. Or define that I want to receive them the first 10 minutes of every hour. If I'm not working, silence all work alerts. New: Improve the hard drive monitor system. Create new seed project to use something like scrutiny (there's a linuxserver image ) to collect and display the information. For alerts, use one of their supported providers . New: Aggregate all notifications. Instead of reading the email, github, gitlab, discourse, reddit notifications, aggregate all in one place and show them to the user in a nice command line interface. For the aggregator server, my first choice would be gotify . New: Add faker-optional to the dormant plant projects. New: Add seedling project to create factoryboy factories from pydantic models automatically.","title":"Projects"},{"location":"newsletter/2021_w16/#devops","text":"","title":"DevOps"},{"location":"newsletter/2021_w16/#continuous-integration","text":"","title":"Continuous Integration"},{"location":"newsletter/2021_w16/#flakehell","text":"Correction: Update the git repository. The existent repository has been archived in favor of this one New: Explain how to patch the extended_default_ignore error for versions > 3.9.0. Add to your your pyproject.toml : [tool.flakehell] extended_default_ignore = [] # add this","title":"Flakehell"},{"location":"newsletter/2021_w16/#coding","text":"","title":"Coding"},{"location":"newsletter/2021_w16/#python","text":"New: Add apprise to the interesting libraries to explore. apprise : Allows you to send a notification to almost all of the most popular notification services available to us today such as: Linux, Telegram, Discord, Slack, Amazon SNS, Gotify, etc. Look at all the supported notifications (\u00ac\u00ba-\u00b0)\u00ac . New: Add kivi and kivimd to the interesting libraries to explore. kivi is used to create android/Linux/iOS/Windows applications with python. Use it with kivimd to make it beautiful, check the examples and the docs .","title":"Python"},{"location":"newsletter/2021_w16/#boto3","text":"New: Introduce the AWS SDK library and explain how to test it. Boto3 is the AWS SDK for Python to create, configure, and manage AWS services, such as Amazon Elastic Compute Cloud (Amazon EC2) and Amazon Simple Storage Service (Amazon S3). The SDK provides an object-oriented API as well as low-level access to AWS services. For testing , try to use moto , using the Botocore's stubber as fallback option. New: Explain how to test ec2, route53, s3, and rds resources. New: Explain how to test vpc and auto scaling group resources. Improvement: Explain how to extract the instance when testing autoscaling groups. Also track the issue to add support to launch templates . New: Explain how to test security groups.","title":"Boto3"},{"location":"newsletter/2021_w16/#logging","text":"New: Explain how to log python program exceptions better than to a file. Using logging to write write exceptions and breadcrumbs to a file might not be the best solution because unless you look at it directly most errors will pass unnoticed. To actively monitor and react to code exceptions use an application monitoring platform like sentry . In the article I explain what are the advantages of using this solution and do a comparison between Sentry and GlitchTip .","title":"Logging"},{"location":"newsletter/2021_w16/#deepdiff","text":"Improvement: Remove advice to use my fork instead. The original one has already merged my PR \uff3c\\ \u0669( \u141b )\u0648 /\uff0f . Beware though as the regexp are not enabled by default (against my will). You need to use the use_regexp=True as an argument to grep or DeepSearch .","title":"DeepDiff"},{"location":"newsletter/2021_w16/#factoryboy","text":"New: Explain how to use Enum with factoryboy.","title":"FactoryBoy"},{"location":"newsletter/2021_w16/#faker","text":"New: Explain how to create Optional data. faker-optional is a custom faker provider that acts as a wrapper over other Faker providers to return their value or None . Useful to create data of type Optional[Any] .","title":"Faker"},{"location":"newsletter/2021_w16/#fastapi","text":"New: Explain how to log exceptions to sentry. New: Explain how to send raw data to the client. With the Response object. New: Explain how to configure the application. New: Explain how to inject a testing configuration in the tests.","title":"FastAPI"},{"location":"newsletter/2021_w16/#python-snippets","text":"Correction: Add warning about the method to use direct dependencies. Last time I used this solution, when I added the library on a setup.py the direct dependencies weren't installed :S","title":"Python Snippets"},{"location":"newsletter/2021_w16/#pydantic","text":"New: Explain how to update entity attributes with a dictionary. You can create a new object with the new data using the update argument of the copy entity method.","title":"Pydantic"},{"location":"newsletter/2021_w16/#rich","text":"New: Introduce the python cli builder library and it's progress bar. Rich is a Python library for rich text and beautiful formatting in the terminal. Check out the beautiful progress bar: pip install rich python -m rich.progress","title":"rich"},{"location":"newsletter/2021_w16/#ruamel-yaml","text":"Improvement: Suggest to use ruyaml instead of ruamel.yaml. As it's maintained by the community and versioned with git.","title":"Ruamel YAML"},{"location":"newsletter/2021_w16/#arts","text":"","title":"Arts"},{"location":"newsletter/2021_w16/#writing","text":"New: Explain when to use I'm good or I'm well. Use I'm well when referring to being ill, use I'm good for the rest.","title":"Writing"},{"location":"newsletter/2021_w16/#other","text":"New: Explain how to select a random choice from Enum objects. pydantic uses Enum objects to define the choices of fields , so we need them to create the factories of those objects. New: Improve the periodic tasks and application metrics monitoring. Setup an healthchecks instance with the linuxserver image to monitor cronjobs. For the notifications either use the prometheus metrics or an apprise compatible system.","title":"Other"},{"location":"newsletter/2021_w17/","text":"Coding \u2691 Python \u2691 New: Add parso library to interesting libraries to explore. parso is a library to parse Python code. Boto3 \u2691 Correction: Add note that pagination is not yet supported when testing route53. I've opened an issue to solve it. New: Explain how to test IAM users and groups. Type Hints \u2691 Improvement: Explain how to define the type hints of functions and methods that use subclasses. It's a complex topic that has taken me many months to get it right :). Pytest \u2691 New: Explain how to run tests in parallel. pytest-xdist makes it possible to run the tests in parallel, useful when the test suit is large or when the tests are slow. pip install pytest-xdist pytest -n auto Writing good documentation \u2691 New: Start explaining how to write good documentation for a software project. It doesn't matter how good your program is, because if its documentation is not good enough, people will not use it. People working with software need different kinds of documentation at different times, in different circumstances, so good software documentation needs them all. In this first iteration, I define the five kinds of documentation, and give the ideas to write good introduction and get started sections.","title":"17th Week of 2021"},{"location":"newsletter/2021_w17/#coding","text":"","title":"Coding"},{"location":"newsletter/2021_w17/#python","text":"New: Add parso library to interesting libraries to explore. parso is a library to parse Python code.","title":"Python"},{"location":"newsletter/2021_w17/#boto3","text":"Correction: Add note that pagination is not yet supported when testing route53. I've opened an issue to solve it. New: Explain how to test IAM users and groups.","title":"Boto3"},{"location":"newsletter/2021_w17/#type-hints","text":"Improvement: Explain how to define the type hints of functions and methods that use subclasses. It's a complex topic that has taken me many months to get it right :).","title":"Type Hints"},{"location":"newsletter/2021_w17/#pytest","text":"New: Explain how to run tests in parallel. pytest-xdist makes it possible to run the tests in parallel, useful when the test suit is large or when the tests are slow. pip install pytest-xdist pytest -n auto","title":"Pytest"},{"location":"newsletter/2021_w17/#writing-good-documentation","text":"New: Start explaining how to write good documentation for a software project. It doesn't matter how good your program is, because if its documentation is not good enough, people will not use it. People working with software need different kinds of documentation at different times, in different circumstances, so good software documentation needs them all. In this first iteration, I define the five kinds of documentation, and give the ideas to write good introduction and get started sections.","title":"Writing good documentation"},{"location":"newsletter/2021_w19/","text":"Projects \u2691 New: Explain the idea of how to improve the record of ideas, tasks,. Coding \u2691 New: Introduce Vue.js. Vue.js is a progressive framework for building user interfaces. Python \u2691 Boto3 \u2691 Improvement: Monitor motor issue with the cn-north-1 rds and autoscaling endpoints. Configure Docker to host the application \u2691 New: Explain how to write type hints for generator functions. New: Explain how to log in using pass. pass show dockerhub | docker login --username foo --password-stdin New: Explain how not to store the credentials in plaintext. It doesn't work, don't go this painful road and assume that docker is broken. The official steps are horrible, and once you've spent two hours debugging them, you won't be able to push or pull images with your user . FastAPI \u2691 New: Explain how to deploy it using Docker. New: Explain how to show logging messages in the logs. Operative Systems \u2691 Linux \u2691 Linux Snippets \u2691 New: Explain how to check if an rsync command has gone well. Run diff -r --brief source/ dest/ , and check that there is no output.","title":"19th Week of 2021"},{"location":"newsletter/2021_w19/#projects","text":"New: Explain the idea of how to improve the record of ideas, tasks,.","title":"Projects"},{"location":"newsletter/2021_w19/#coding","text":"New: Introduce Vue.js. Vue.js is a progressive framework for building user interfaces.","title":"Coding"},{"location":"newsletter/2021_w19/#python","text":"","title":"Python"},{"location":"newsletter/2021_w19/#boto3","text":"Improvement: Monitor motor issue with the cn-north-1 rds and autoscaling endpoints.","title":"Boto3"},{"location":"newsletter/2021_w19/#configure-docker-to-host-the-application","text":"New: Explain how to write type hints for generator functions. New: Explain how to log in using pass. pass show dockerhub | docker login --username foo --password-stdin New: Explain how not to store the credentials in plaintext. It doesn't work, don't go this painful road and assume that docker is broken. The official steps are horrible, and once you've spent two hours debugging them, you won't be able to push or pull images with your user .","title":"Configure Docker to host the application"},{"location":"newsletter/2021_w19/#fastapi","text":"New: Explain how to deploy it using Docker. New: Explain how to show logging messages in the logs.","title":"FastAPI"},{"location":"newsletter/2021_w19/#operative-systems","text":"","title":"Operative Systems"},{"location":"newsletter/2021_w19/#linux","text":"","title":"Linux"},{"location":"newsletter/2021_w19/#linux-snippets","text":"New: Explain how to check if an rsync command has gone well. Run diff -r --brief source/ dest/ , and check that there is no output.","title":"Linux Snippets"},{"location":"newsletter/2021_w20/","text":"DevOps \u2691 Infrastructure Solutions \u2691 Jobs \u2691 Improvement: Remove false positive alerts on failed jobs that succeeded. A Kubernetes cronjob spawns jobs, if the first one fails, it will try to spawn a new one. If the second succeeds, the cronjob status should be success, but with the rule we had before, a successful job with failed past jobs will still raise an alert. Operative Systems \u2691 Linux \u2691 Gajim \u2691 New: Introduce gajim. Gajim is the best Linux XMPP client in terms of end-to-end encryption support as it's able to speak OMEMO. Jellyfin \u2691 Correction: Explain how to fix the stuck at login page issue. systemctl stop jellyfin.service mv /var/lib/jellyfin/data/jellyfin.db { ,.bak } systemctl start jellyfin.service systemctl stop jellyfin.service mv /var/lib/jellyfin/data/jellyfin.db { .bak, } systemctl start jellyfin.service Correction: Explain how to fix the Intel Hardware transcoding. docker exec -it jellyfin /bin/bash wget https://repo.jellyfin.org/releases/server/ubuntu/versions/jellyfin-ffmpeg/4.3.2-1/jellyfin-ffmpeg_4.3.2-1-focal_amd64.deb dpkg -i jellyfin-ffmpeg_4.3.2-1-focal_amd64.deb","title":"20th Week of 2021"},{"location":"newsletter/2021_w20/#devops","text":"","title":"DevOps"},{"location":"newsletter/2021_w20/#infrastructure-solutions","text":"","title":"Infrastructure Solutions"},{"location":"newsletter/2021_w20/#jobs","text":"Improvement: Remove false positive alerts on failed jobs that succeeded. A Kubernetes cronjob spawns jobs, if the first one fails, it will try to spawn a new one. If the second succeeds, the cronjob status should be success, but with the rule we had before, a successful job with failed past jobs will still raise an alert.","title":"Jobs"},{"location":"newsletter/2021_w20/#operative-systems","text":"","title":"Operative Systems"},{"location":"newsletter/2021_w20/#linux","text":"","title":"Linux"},{"location":"newsletter/2021_w20/#gajim","text":"New: Introduce gajim. Gajim is the best Linux XMPP client in terms of end-to-end encryption support as it's able to speak OMEMO.","title":"Gajim"},{"location":"newsletter/2021_w20/#jellyfin","text":"Correction: Explain how to fix the stuck at login page issue. systemctl stop jellyfin.service mv /var/lib/jellyfin/data/jellyfin.db { ,.bak } systemctl start jellyfin.service systemctl stop jellyfin.service mv /var/lib/jellyfin/data/jellyfin.db { .bak, } systemctl start jellyfin.service Correction: Explain how to fix the Intel Hardware transcoding. docker exec -it jellyfin /bin/bash wget https://repo.jellyfin.org/releases/server/ubuntu/versions/jellyfin-ffmpeg/4.3.2-1/jellyfin-ffmpeg_4.3.2-1-focal_amd64.deb dpkg -i jellyfin-ffmpeg_4.3.2-1-focal_amd64.deb","title":"Jellyfin"},{"location":"newsletter/2021_w21/","text":"Projects \u2691 New: Add git-bug as an interesting distributed issue tracker. New: Add the Improve the reliability of the Open Science collections project. The current free knowledge efforts : are based on the health of a collection of torrents. This project aims to create a command line tool or service that makes it easier to automate the seeding of ill torrents. New: Add the Monitor and notify on disk prices project. Diskprices.com sorts the prices of the disks on the different amazon sites based on many filters. It will be interesting to have a service that monitors the data on this site and alerts the user once there is a deal that matches its criteria. Reorganization: Move the automation of computer file management project to the projects page. Reorganization: Move the dying projects below the seeds as they are less important. Life Management \u2691 Reorganization: Split the life automation article into life management and process automation. I understand life management as the act of analyzing yourself and your interactions with the world to define processes and automations that shape the way to efficiently achieve your goals. I understand process automation as the act of analyzing yourself and your interactions with the world to find the way to reduce the time or willpower spent on your life processes. Time Management \u2691 New: Introduce the time management concept. Time management is the process of planning and exercising conscious control of time spent on specific activities, especially to increase effectiveness, efficiency, and productivity. It involves a juggling act of various demands upon a person relating to work, social life, family, hobbies, personal interests, and commitments with the finiteness of time. Using time effectively gives the person \"choice\" on spending or managing activities at their own time and expediency. New: Start analyzing the ways to reduce the time spent doing unproductive tasks. By minimizing the context switches and managing the interruptions . Improvement: Explain how to improve your efficiency by better using your everyday tools. Improvement: Add two more ways to avoid loosing time in unproductive tasks. Avoid lost time doing nothing . Fix your environment . New: Explain how to manage meetings efficiently. New: Explain how to improve efficiency by taking care of yourself. New: Explain how to prevent blocks by efficiently switching mental processes. Task Management \u2691 New: Introduce the task management concept. Task management is the process of managing a task through its life cycle. It involves planning, testing, tracking, and reporting. Task management can help either individual achieve goals, or groups of individuals collaborate and share knowledge for the accomplishment of collective goals. Improvement: Introduce the main task management tools. The inbox does not refer only to your e-mail inbox. It is a broader concept that includes all the elements you have collected in different ways: tasks you have to do, ideas you have thought of, notes, bills, business cards, etc\u2026 The task manager tool to make your interaction with the tasks easier. You can start with the simplest task manager , a markdown file in your computer with a list of tasks to do. Annotate only the actionable tasks that you need to do today, otherwise it can quickly grow to be unmanageable. Improvement: Add the benefits when you do task management well, and the side effects if you do it wrong. Improvement: Add a small guide on how to introduce yourself into task management. Task Management Workflows \u2691 New: Introduce the main task management workflows. Task management can be done at different levels. All of them help you in different ways to reduce the mental load, each also gives you extra benefits that can't be gained by the others. Going from lowest to highest abstraction level we have: Pomodoro. Task plan. Day plan. Week plan. Month plan. Semester plan. In the commit I've detailed the Pomodoro technique and the task , day and week plans. New: Explain the difference of surfing the hype flow versus following a defined plan. Interruption Management \u2691 New: Introduce the interruption management concept. Interruption management is the life management area that gathers the processes to minimize the time and willpower toll consumed by interruptions. The article proposes a way to analyze your existent interruptions and define how can you improve your interaction with them. I've applied it both to my work and personal interruptions. Improvement: Explain what to do once you have the interruption analysis. Work Interruption Analysis \u2691 Improvement: Add analysis of instant message interruptions. Personal Interruption Analysis \u2691 Improvement: Add analysis of instant message interruptions. Money Management \u2691 Reorganization: Move the accounting automation to money management. Tool Management \u2691 New: Introduce the tool management section. Most of the tasks or processes we do involve some kind of tool, the better you know how to use them, the better your efficiency will be. The more you use a tool, the more it's worth the investment of time to improve your usage of it. Whenever I use a tool, I try to think if I could configure it or use it in a way that will make it easier or quicker. Don't go crazy and try to change everything. Go step by step, and once you've internalized the improvement, implement the next. Email Management \u2691 New: Explain how I configure and interact with email efficiently. Instant Messages Management \u2691 New: Explain how I configure and interact with chat applications efficiently. Process Automation \u2691 Improvement: Add xkcd comics that gather the essence and pitfalls of process automation. Activism \u2691 Free Knowledge \u2691 New: Introduce how to contribute to the free knowledge initiative. One of the early principles of the internet has been to make knowledge free to everyone. Alexandra Elbakyan of Sci-Hub , bookwarrior of Library Genesis , Aaron Swartz , and countless unnamed others have fought to free science from the grips of for-profit publishers. Today, they do it working in hiding, alone, without acknowledgment, in fear of imprisonment, and even now wiretapped by the FBI. They sacrifice everything for one vision: Open Science. If you want to know how to contribute, check the article . Operative Systems \u2691 Linux \u2691 Vim Plugins \u2691 Improvement: Explain how to configure the vim-easymotion movement keys. Vim \u2691 Reorganization: Refactor the vim_automation article into vim and vim_plugins. Arts \u2691 Writing \u2691 Forking this garden \u2691 New: Explain how to fork the blue book. Other \u2691 Reorganization: Reorder the sections of the site navigation menu. Give more importance to Coding, Activism and Life Management, reducing the Software Architecture and Data Analysis sections. Reorganization: Move the tasks tools from the task management article to their own.","title":"21st Week of 2021"},{"location":"newsletter/2021_w21/#projects","text":"New: Add git-bug as an interesting distributed issue tracker. New: Add the Improve the reliability of the Open Science collections project. The current free knowledge efforts : are based on the health of a collection of torrents. This project aims to create a command line tool or service that makes it easier to automate the seeding of ill torrents. New: Add the Monitor and notify on disk prices project. Diskprices.com sorts the prices of the disks on the different amazon sites based on many filters. It will be interesting to have a service that monitors the data on this site and alerts the user once there is a deal that matches its criteria. Reorganization: Move the automation of computer file management project to the projects page. Reorganization: Move the dying projects below the seeds as they are less important.","title":"Projects"},{"location":"newsletter/2021_w21/#life-management","text":"Reorganization: Split the life automation article into life management and process automation. I understand life management as the act of analyzing yourself and your interactions with the world to define processes and automations that shape the way to efficiently achieve your goals. I understand process automation as the act of analyzing yourself and your interactions with the world to find the way to reduce the time or willpower spent on your life processes.","title":"Life Management"},{"location":"newsletter/2021_w21/#time-management","text":"New: Introduce the time management concept. Time management is the process of planning and exercising conscious control of time spent on specific activities, especially to increase effectiveness, efficiency, and productivity. It involves a juggling act of various demands upon a person relating to work, social life, family, hobbies, personal interests, and commitments with the finiteness of time. Using time effectively gives the person \"choice\" on spending or managing activities at their own time and expediency. New: Start analyzing the ways to reduce the time spent doing unproductive tasks. By minimizing the context switches and managing the interruptions . Improvement: Explain how to improve your efficiency by better using your everyday tools. Improvement: Add two more ways to avoid loosing time in unproductive tasks. Avoid lost time doing nothing . Fix your environment . New: Explain how to manage meetings efficiently. New: Explain how to improve efficiency by taking care of yourself. New: Explain how to prevent blocks by efficiently switching mental processes.","title":"Time Management"},{"location":"newsletter/2021_w21/#task-management","text":"New: Introduce the task management concept. Task management is the process of managing a task through its life cycle. It involves planning, testing, tracking, and reporting. Task management can help either individual achieve goals, or groups of individuals collaborate and share knowledge for the accomplishment of collective goals. Improvement: Introduce the main task management tools. The inbox does not refer only to your e-mail inbox. It is a broader concept that includes all the elements you have collected in different ways: tasks you have to do, ideas you have thought of, notes, bills, business cards, etc\u2026 The task manager tool to make your interaction with the tasks easier. You can start with the simplest task manager , a markdown file in your computer with a list of tasks to do. Annotate only the actionable tasks that you need to do today, otherwise it can quickly grow to be unmanageable. Improvement: Add the benefits when you do task management well, and the side effects if you do it wrong. Improvement: Add a small guide on how to introduce yourself into task management.","title":"Task Management"},{"location":"newsletter/2021_w21/#task-management-workflows","text":"New: Introduce the main task management workflows. Task management can be done at different levels. All of them help you in different ways to reduce the mental load, each also gives you extra benefits that can't be gained by the others. Going from lowest to highest abstraction level we have: Pomodoro. Task plan. Day plan. Week plan. Month plan. Semester plan. In the commit I've detailed the Pomodoro technique and the task , day and week plans. New: Explain the difference of surfing the hype flow versus following a defined plan.","title":"Task Management Workflows"},{"location":"newsletter/2021_w21/#interruption-management","text":"New: Introduce the interruption management concept. Interruption management is the life management area that gathers the processes to minimize the time and willpower toll consumed by interruptions. The article proposes a way to analyze your existent interruptions and define how can you improve your interaction with them. I've applied it both to my work and personal interruptions. Improvement: Explain what to do once you have the interruption analysis.","title":"Interruption Management"},{"location":"newsletter/2021_w21/#work-interruption-analysis","text":"Improvement: Add analysis of instant message interruptions.","title":"Work Interruption Analysis"},{"location":"newsletter/2021_w21/#personal-interruption-analysis","text":"Improvement: Add analysis of instant message interruptions.","title":"Personal Interruption Analysis"},{"location":"newsletter/2021_w21/#money-management","text":"Reorganization: Move the accounting automation to money management.","title":"Money Management"},{"location":"newsletter/2021_w21/#tool-management","text":"New: Introduce the tool management section. Most of the tasks or processes we do involve some kind of tool, the better you know how to use them, the better your efficiency will be. The more you use a tool, the more it's worth the investment of time to improve your usage of it. Whenever I use a tool, I try to think if I could configure it or use it in a way that will make it easier or quicker. Don't go crazy and try to change everything. Go step by step, and once you've internalized the improvement, implement the next.","title":"Tool Management"},{"location":"newsletter/2021_w21/#email-management","text":"New: Explain how I configure and interact with email efficiently.","title":"Email Management"},{"location":"newsletter/2021_w21/#instant-messages-management","text":"New: Explain how I configure and interact with chat applications efficiently.","title":"Instant Messages Management"},{"location":"newsletter/2021_w21/#process-automation","text":"Improvement: Add xkcd comics that gather the essence and pitfalls of process automation.","title":"Process Automation"},{"location":"newsletter/2021_w21/#activism","text":"","title":"Activism"},{"location":"newsletter/2021_w21/#free-knowledge","text":"New: Introduce how to contribute to the free knowledge initiative. One of the early principles of the internet has been to make knowledge free to everyone. Alexandra Elbakyan of Sci-Hub , bookwarrior of Library Genesis , Aaron Swartz , and countless unnamed others have fought to free science from the grips of for-profit publishers. Today, they do it working in hiding, alone, without acknowledgment, in fear of imprisonment, and even now wiretapped by the FBI. They sacrifice everything for one vision: Open Science. If you want to know how to contribute, check the article .","title":"Free Knowledge"},{"location":"newsletter/2021_w21/#operative-systems","text":"","title":"Operative Systems"},{"location":"newsletter/2021_w21/#linux","text":"","title":"Linux"},{"location":"newsletter/2021_w21/#vim-plugins","text":"Improvement: Explain how to configure the vim-easymotion movement keys.","title":"Vim Plugins"},{"location":"newsletter/2021_w21/#vim","text":"Reorganization: Refactor the vim_automation article into vim and vim_plugins.","title":"Vim"},{"location":"newsletter/2021_w21/#arts","text":"","title":"Arts"},{"location":"newsletter/2021_w21/#writing","text":"","title":"Writing"},{"location":"newsletter/2021_w21/#forking-this-garden","text":"New: Explain how to fork the blue book.","title":"Forking this garden"},{"location":"newsletter/2021_w21/#other","text":"Reorganization: Reorder the sections of the site navigation menu. Give more importance to Coding, Activism and Life Management, reducing the Software Architecture and Data Analysis sections. Reorganization: Move the tasks tools from the task management article to their own.","title":"Other"},{"location":"newsletter/2021_w22/","text":"Coding \u2691 Python \u2691 Python Snippets \u2691 New: Explain how to convert html code to readable plaintext. pip install html2text import html2text html = open ( \"foobar.html\" ) . read () print ( html2text . html2text ( html )) New: Explain how to parse a datetime from a string. from dateutil import parser parser . parse ( \"Aug 28 1999 12:00AM\" ) # datetime.datetime(1999, 8, 28, 0, 0) Python Mysql \u2691 New: Explain how to interact with MySQL databases with Python. DevOps \u2691 Continuous Integration \u2691 New: Explain how to troubleshoot the error: pathspec master did not match any file. Remove all git hooks with rm -r .git/hooks . Software Architecture \u2691 Domain Driven Design \u2691 Improvement: Add warning when migrating old code. You may be tempted to migrate all your old code to this architecture once you fall in love with it. Truth being told, it's the best way to learn how to use it, but it's time expensive too! The last refactor I did required a change of 60% of the code. The upside is that I reduced the total lines of code a 25%. Arts \u2691 Writing \u2691 Grammar and Orthography \u2691 New: Explain what collocations are and how to avoid the word very. Collocation refers to a natural combination of words that are closely affiliated with each other. They make it easier to avoid overused or ambiguous words like \"very\", \"nice\", or \"beautiful\", by using a pair of words that fit the context better and that have a more precise meaning.","title":"22nd Week of 2021"},{"location":"newsletter/2021_w22/#coding","text":"","title":"Coding"},{"location":"newsletter/2021_w22/#python","text":"","title":"Python"},{"location":"newsletter/2021_w22/#python-snippets","text":"New: Explain how to convert html code to readable plaintext. pip install html2text import html2text html = open ( \"foobar.html\" ) . read () print ( html2text . html2text ( html )) New: Explain how to parse a datetime from a string. from dateutil import parser parser . parse ( \"Aug 28 1999 12:00AM\" ) # datetime.datetime(1999, 8, 28, 0, 0)","title":"Python Snippets"},{"location":"newsletter/2021_w22/#python-mysql","text":"New: Explain how to interact with MySQL databases with Python.","title":"Python Mysql"},{"location":"newsletter/2021_w22/#devops","text":"","title":"DevOps"},{"location":"newsletter/2021_w22/#continuous-integration","text":"New: Explain how to troubleshoot the error: pathspec master did not match any file. Remove all git hooks with rm -r .git/hooks .","title":"Continuous Integration"},{"location":"newsletter/2021_w22/#software-architecture","text":"","title":"Software Architecture"},{"location":"newsletter/2021_w22/#domain-driven-design","text":"Improvement: Add warning when migrating old code. You may be tempted to migrate all your old code to this architecture once you fall in love with it. Truth being told, it's the best way to learn how to use it, but it's time expensive too! The last refactor I did required a change of 60% of the code. The upside is that I reduced the total lines of code a 25%.","title":"Domain Driven Design"},{"location":"newsletter/2021_w22/#arts","text":"","title":"Arts"},{"location":"newsletter/2021_w22/#writing","text":"","title":"Writing"},{"location":"newsletter/2021_w22/#grammar-and-orthography","text":"New: Explain what collocations are and how to avoid the word very. Collocation refers to a natural combination of words that are closely affiliated with each other. They make it easier to avoid overused or ambiguous words like \"very\", \"nice\", or \"beautiful\", by using a pair of words that fit the context better and that have a more precise meaning.","title":"Grammar and Orthography"},{"location":"newsletter/2021_w23/","text":"Projects \u2691 New: Introduce seedling self-hosted map project. I love maps, as well as traveling and hiking. This project aims to create a web interface that let's me interact with the data gathered throughout my life. I'd like to: Browse the waypoints and routes that I've done. Create routes and export the gpx. Be able to search through the data Plan trips New: Introduce the seed project to. Arts \u2691 Writing \u2691 Grammar and Orthography \u2691 New: Explain what can you use instead of I know. Using \"I know\" may not be the best way to show the other person that you've got the information. You can take the chance to use other words that additionally gives more context on how you stand with the information you've received, thus improving the communication and creating a bond. Cooking \u2691 New: Introduce the cooking art. Cooking Basics \u2691 New: Refactor the perfect technique to boil an egg. New: Explain how to boil chickpeas when you've forgotten to soak them. Add a level teaspoon of baking soda to the pot and cook them as usual","title":"23rd Week of 2021"},{"location":"newsletter/2021_w23/#projects","text":"New: Introduce seedling self-hosted map project. I love maps, as well as traveling and hiking. This project aims to create a web interface that let's me interact with the data gathered throughout my life. I'd like to: Browse the waypoints and routes that I've done. Create routes and export the gpx. Be able to search through the data Plan trips New: Introduce the seed project to.","title":"Projects"},{"location":"newsletter/2021_w23/#arts","text":"","title":"Arts"},{"location":"newsletter/2021_w23/#writing","text":"","title":"Writing"},{"location":"newsletter/2021_w23/#grammar-and-orthography","text":"New: Explain what can you use instead of I know. Using \"I know\" may not be the best way to show the other person that you've got the information. You can take the chance to use other words that additionally gives more context on how you stand with the information you've received, thus improving the communication and creating a bond.","title":"Grammar and Orthography"},{"location":"newsletter/2021_w23/#cooking","text":"New: Introduce the cooking art.","title":"Cooking"},{"location":"newsletter/2021_w23/#cooking-basics","text":"New: Refactor the perfect technique to boil an egg. New: Explain how to boil chickpeas when you've forgotten to soak them. Add a level teaspoon of baking soda to the pot and cook them as usual","title":"Cooking Basics"},{"location":"newsletter/2021_w24/","text":"Coding \u2691 Python \u2691 Python Mysql \u2691 Correction: Correct the syntax of the left joins. Instead of using ON users.id == addresses.user_id , use ON users.id = addresses.user_id DevOps \u2691 Monitoring \u2691 Elasticsearch Exporter \u2691 New: Introduce the prometheus elasticsearch exporter. The elasticsearch exporter allows monitoring Elasticsearch clusters with Prometheus . Explain how to install it, configure the grafana dashboards and the alerts. Improvement: Add more elasticsearch alerts. Measure the search latency, search rate and create alerts on the garbage collector, json parser and circuit breaker errors Operative Systems \u2691 Linux \u2691 dunst \u2691 New: Introduce dunst. Dunst is a lightweight replacement for the notification daemons provided by most desktop environments. It\u2019s very customizable, isn\u2019t dependent on any toolkits, and therefore fits into those window manager centric setups we all love to customize to perfection.","title":"24th Week of 2021"},{"location":"newsletter/2021_w24/#coding","text":"","title":"Coding"},{"location":"newsletter/2021_w24/#python","text":"","title":"Python"},{"location":"newsletter/2021_w24/#python-mysql","text":"Correction: Correct the syntax of the left joins. Instead of using ON users.id == addresses.user_id , use ON users.id = addresses.user_id","title":"Python Mysql"},{"location":"newsletter/2021_w24/#devops","text":"","title":"DevOps"},{"location":"newsletter/2021_w24/#monitoring","text":"","title":"Monitoring"},{"location":"newsletter/2021_w24/#elasticsearch-exporter","text":"New: Introduce the prometheus elasticsearch exporter. The elasticsearch exporter allows monitoring Elasticsearch clusters with Prometheus . Explain how to install it, configure the grafana dashboards and the alerts. Improvement: Add more elasticsearch alerts. Measure the search latency, search rate and create alerts on the garbage collector, json parser and circuit breaker errors","title":"Elasticsearch Exporter"},{"location":"newsletter/2021_w24/#operative-systems","text":"","title":"Operative Systems"},{"location":"newsletter/2021_w24/#linux","text":"","title":"Linux"},{"location":"newsletter/2021_w24/#dunst","text":"New: Introduce dunst. Dunst is a lightweight replacement for the notification daemons provided by most desktop environments. It\u2019s very customizable, isn\u2019t dependent on any toolkits, and therefore fits into those window manager centric setups we all love to customize to perfection.","title":"dunst"},{"location":"newsletter/2021_w25/","text":"Projects \u2691 New: Add the Life seedling project. Life is a real time sandbox role game where you play as yourself surviving in today's world. New: Add bruty to the dormant plant projects. bruty is a Python program to bruteforce dynamic web applications with Selenium. Coding \u2691 Python \u2691 Improvement: Add textual as interesting library to explore. textual : Textual is a TUI (Text User Interface) framework for Python using Rich as a renderer. Click \u2691 Improvement: Explain how to change the command line help description. FastAPI \u2691 New: Explain how to make redirections with fastapi. New: Explain how to run a FastAPI server in the background for testing purposes. Pytest \u2691 New: Explain how to set a timeout for your tests. Using pytest-timeout . New: Explain how to rerun tests that fail sometimes. With pytest-rerunfailures that is a plugin for pytest that re-runs tests to eliminate intermittent failures. Using this plugin is generally a bad idea, it would be best to solve the reason why your code is not reliable. It's useful when you rely on non robust third party software in a way that you can't solve, or if the error is not in your code but in the testing code, and again you are not able to fix it. feat(python_snippets#Create combination of elements in groups of two): Explain how to create combination of elements in groups of two >>> list ( itertools . combinations ( 'ABC' , 2 )) [( 'A' , 'B' ), ( 'A' , 'C' ), ( 'B' , 'C' )] Selenium \u2691 New: Explain how to use selenium with python. New: Explain how to Set timeout of a response. driver . set_page_load_timeout ( 30 ) New: Explain how to fix when Chromedriver hangs up unexpectedly. os . environ [ \"DBUS_SESSION_BUS_ADDRESS\" ] = \"/dev/null\" DevOps \u2691 Infrastructure Solutions \u2691 Jobs \u2691 New: Explain how to rerun failed cronjobs. If you have a job that has failed after the 6 default retries, it will show up in your monitorization forever, to fix it, you can manually trigger the job. kubectl get job \"your-job\" -o json \\ | jq 'del(.spec.selector)' \\ | jq 'del(.spec.template.metadata.labels)' \\ | kubectl replace --force -f - Operative Systems \u2691 Linux \u2691 Linux Snippets \u2691 New: Explain how to split a file into many with equal number of lines. split -l 200000 filename New: Explain how to identify what a string or file contains. Using pywhat","title":"25th Week of 2021"},{"location":"newsletter/2021_w25/#projects","text":"New: Add the Life seedling project. Life is a real time sandbox role game where you play as yourself surviving in today's world. New: Add bruty to the dormant plant projects. bruty is a Python program to bruteforce dynamic web applications with Selenium.","title":"Projects"},{"location":"newsletter/2021_w25/#coding","text":"","title":"Coding"},{"location":"newsletter/2021_w25/#python","text":"Improvement: Add textual as interesting library to explore. textual : Textual is a TUI (Text User Interface) framework for Python using Rich as a renderer.","title":"Python"},{"location":"newsletter/2021_w25/#click","text":"Improvement: Explain how to change the command line help description.","title":"Click"},{"location":"newsletter/2021_w25/#fastapi","text":"New: Explain how to make redirections with fastapi. New: Explain how to run a FastAPI server in the background for testing purposes.","title":"FastAPI"},{"location":"newsletter/2021_w25/#pytest","text":"New: Explain how to set a timeout for your tests. Using pytest-timeout . New: Explain how to rerun tests that fail sometimes. With pytest-rerunfailures that is a plugin for pytest that re-runs tests to eliminate intermittent failures. Using this plugin is generally a bad idea, it would be best to solve the reason why your code is not reliable. It's useful when you rely on non robust third party software in a way that you can't solve, or if the error is not in your code but in the testing code, and again you are not able to fix it. feat(python_snippets#Create combination of elements in groups of two): Explain how to create combination of elements in groups of two >>> list ( itertools . combinations ( 'ABC' , 2 )) [( 'A' , 'B' ), ( 'A' , 'C' ), ( 'B' , 'C' )]","title":"Pytest"},{"location":"newsletter/2021_w25/#selenium","text":"New: Explain how to use selenium with python. New: Explain how to Set timeout of a response. driver . set_page_load_timeout ( 30 ) New: Explain how to fix when Chromedriver hangs up unexpectedly. os . environ [ \"DBUS_SESSION_BUS_ADDRESS\" ] = \"/dev/null\"","title":"Selenium"},{"location":"newsletter/2021_w25/#devops","text":"","title":"DevOps"},{"location":"newsletter/2021_w25/#infrastructure-solutions","text":"","title":"Infrastructure Solutions"},{"location":"newsletter/2021_w25/#jobs","text":"New: Explain how to rerun failed cronjobs. If you have a job that has failed after the 6 default retries, it will show up in your monitorization forever, to fix it, you can manually trigger the job. kubectl get job \"your-job\" -o json \\ | jq 'del(.spec.selector)' \\ | jq 'del(.spec.template.metadata.labels)' \\ | kubectl replace --force -f -","title":"Jobs"},{"location":"newsletter/2021_w25/#operative-systems","text":"","title":"Operative Systems"},{"location":"newsletter/2021_w25/#linux","text":"","title":"Linux"},{"location":"newsletter/2021_w25/#linux-snippets","text":"New: Explain how to split a file into many with equal number of lines. split -l 200000 filename New: Explain how to identify what a string or file contains. Using pywhat","title":"Linux Snippets"},{"location":"newsletter/2021_w26/","text":"Coding \u2691 Python \u2691 Configure Docker to host the application \u2691 New: Explain how to use watchtower to keep docker containers updated. With watchtower you can update the running version of your containerized app simply by pushing a new image to the Docker Hub or your own image registry. Watchtower will pull down your new image, gracefully shut down your existing container and restart it with the same options that were used when it was deployed initially. Correction: Explain how to run the watchtower checks immediately. With the --run-once flag DevOps \u2691 Monitoring \u2691 Elasticsearch Exporter \u2691 New: Add alert on low number of healthy master nodes. Operative Systems \u2691 Linux \u2691 elasticsearch \u2691 Correction: Explain how to restore only some indices. curl -X POST \"{{ url }}/_snapshot/{{ backup_path }}/{{ snapshot_name }}/_restore?pretty\" -H 'Content-Type: application/json' -d ' { \"indices\": \"{{ index_to_restore }}\", }' New: Explain how to fix Circuit breakers triggers.","title":"26th Week of 2021"},{"location":"newsletter/2021_w26/#coding","text":"","title":"Coding"},{"location":"newsletter/2021_w26/#python","text":"","title":"Python"},{"location":"newsletter/2021_w26/#configure-docker-to-host-the-application","text":"New: Explain how to use watchtower to keep docker containers updated. With watchtower you can update the running version of your containerized app simply by pushing a new image to the Docker Hub or your own image registry. Watchtower will pull down your new image, gracefully shut down your existing container and restart it with the same options that were used when it was deployed initially. Correction: Explain how to run the watchtower checks immediately. With the --run-once flag","title":"Configure Docker to host the application"},{"location":"newsletter/2021_w26/#devops","text":"","title":"DevOps"},{"location":"newsletter/2021_w26/#monitoring","text":"","title":"Monitoring"},{"location":"newsletter/2021_w26/#elasticsearch-exporter","text":"New: Add alert on low number of healthy master nodes.","title":"Elasticsearch Exporter"},{"location":"newsletter/2021_w26/#operative-systems","text":"","title":"Operative Systems"},{"location":"newsletter/2021_w26/#linux","text":"","title":"Linux"},{"location":"newsletter/2021_w26/#elasticsearch","text":"Correction: Explain how to restore only some indices. curl -X POST \"{{ url }}/_snapshot/{{ backup_path }}/{{ snapshot_name }}/_restore?pretty\" -H 'Content-Type: application/json' -d ' { \"indices\": \"{{ index_to_restore }}\", }' New: Explain how to fix Circuit breakers triggers.","title":"elasticsearch"},{"location":"newsletter/2021_w28/","text":"Projects \u2691 Improvement: Add rsarai hq to interesting sources for lifelogging. Operative Systems \u2691 Linux \u2691 New: Introduce Tahoe-LAFS. Tahoe-LAFS is a free and open, secure, decentralized, fault-tolerant, distributed data store and distributed file system. Tahoe-LAFS is a system that helps you to store files. You run a client program on your computer, which talks to one or more storage servers on other computers. When you tell your client to store a file, it will encrypt that file, encode it into multiple pieces, then spread those pieces out among multiple servers. The pieces are all encrypted and protected against modifications. Later, when you ask your client to retrieve the file, it will find the necessary pieces, make sure they haven\u2019t been corrupted, reassemble them, and decrypt the result. elasticsearch \u2691 Correction: Correct the way of closing an index. Use a POST instead of a GET New: Explain how to calculate the amount of memory required to do KNN operations. New: Explain how to do KNN warmup to speed up the queries. New: Explain how to deal with the AWS service timeout. Jellyfin \u2691 Improvement: Explain how to fix the wrong image covers. Remove all the jpg files of the directory and then fetch again the data from your favourite media management software. New: Track the issue of trailers not working. Syncthing \u2691 New: Investigate if Syncthing can be used over Tor. I haven't found a reliable and safe way to do it, but I've set a path to follow if you're interested. Android \u2691 OsmAnd \u2691 New: Introduce OsmAnd. OsmAnd is a mobile application for global map viewing and navigating based on OpenStreetMaps . Perfect if you're looking for a privacy focused, community maintained open source alternative to google maps.","title":"28th Week of 2021"},{"location":"newsletter/2021_w28/#projects","text":"Improvement: Add rsarai hq to interesting sources for lifelogging.","title":"Projects"},{"location":"newsletter/2021_w28/#operative-systems","text":"","title":"Operative Systems"},{"location":"newsletter/2021_w28/#linux","text":"New: Introduce Tahoe-LAFS. Tahoe-LAFS is a free and open, secure, decentralized, fault-tolerant, distributed data store and distributed file system. Tahoe-LAFS is a system that helps you to store files. You run a client program on your computer, which talks to one or more storage servers on other computers. When you tell your client to store a file, it will encrypt that file, encode it into multiple pieces, then spread those pieces out among multiple servers. The pieces are all encrypted and protected against modifications. Later, when you ask your client to retrieve the file, it will find the necessary pieces, make sure they haven\u2019t been corrupted, reassemble them, and decrypt the result.","title":"Linux"},{"location":"newsletter/2021_w28/#elasticsearch","text":"Correction: Correct the way of closing an index. Use a POST instead of a GET New: Explain how to calculate the amount of memory required to do KNN operations. New: Explain how to do KNN warmup to speed up the queries. New: Explain how to deal with the AWS service timeout.","title":"elasticsearch"},{"location":"newsletter/2021_w28/#jellyfin","text":"Improvement: Explain how to fix the wrong image covers. Remove all the jpg files of the directory and then fetch again the data from your favourite media management software. New: Track the issue of trailers not working.","title":"Jellyfin"},{"location":"newsletter/2021_w28/#syncthing","text":"New: Investigate if Syncthing can be used over Tor. I haven't found a reliable and safe way to do it, but I've set a path to follow if you're interested.","title":"Syncthing"},{"location":"newsletter/2021_w28/#android","text":"","title":"Android"},{"location":"newsletter/2021_w28/#osmand","text":"New: Introduce OsmAnd. OsmAnd is a mobile application for global map viewing and navigating based on OpenStreetMaps . Perfect if you're looking for a privacy focused, community maintained open source alternative to google maps.","title":"OsmAnd"},{"location":"newsletter/2021_w29/","text":"Operative Systems \u2691 Linux \u2691 Jellyfin \u2691 New: Explain how to fix the green bars in the reproduction.","title":"29th Week of 2021"},{"location":"newsletter/2021_w29/#operative-systems","text":"","title":"Operative Systems"},{"location":"newsletter/2021_w29/#linux","text":"","title":"Linux"},{"location":"newsletter/2021_w29/#jellyfin","text":"New: Explain how to fix the green bars in the reproduction.","title":"Jellyfin"},{"location":"newsletter/2021_w30/","text":"Coding \u2691 Python \u2691 questionary \u2691 Correction: Correct the link to the examples. Operative Systems \u2691 Linux \u2691 LUKS \u2691 New: Explain how to change a LUKS key. cryptsetup luksChangeKey {{ luks_device }} -s 0 Oracle Database \u2691 New: Explain how to build an oracle database docker while feeling dirty inside.","title":"30th Week of 2021"},{"location":"newsletter/2021_w30/#coding","text":"","title":"Coding"},{"location":"newsletter/2021_w30/#python","text":"","title":"Python"},{"location":"newsletter/2021_w30/#questionary","text":"Correction: Correct the link to the examples.","title":"questionary"},{"location":"newsletter/2021_w30/#operative-systems","text":"","title":"Operative Systems"},{"location":"newsletter/2021_w30/#linux","text":"","title":"Linux"},{"location":"newsletter/2021_w30/#luks","text":"New: Explain how to change a LUKS key. cryptsetup luksChangeKey {{ luks_device }} -s 0","title":"LUKS"},{"location":"newsletter/2021_w30/#oracle-database","text":"New: Explain how to build an oracle database docker while feeling dirty inside.","title":"Oracle Database"},{"location":"newsletter/2021_w31/","text":"DevOps \u2691 Infrastructure as Code \u2691 Terraform \u2691 New: Introduce terraform and how to handle RDS secrets. Terraform is an open-source infrastructure as code software tool created by HashiCorp. It enables users to define and provision a datacenter infrastructure using an awful high-level configuration language known as Hashicorp Configuration Language (HCL), or optionally JSON. Terraform supports a number of cloud infrastructure providers such as Amazon Web Services, IBM Cloud , Google Cloud Platform, DigitalOcean, Linode, Microsoft Azure, Oracle Cloud Infrastructure, OVH, or VMware vSphere as well as OpenNebula and OpenStack. Infrastructure Solutions \u2691 Kubernetes \u2691 New: Add Velero as interesting tool. Velero is a tool to backup and migrate Kubernetes resources and persistent volumes. Architecture \u2691 New: Give suggestions on how to choose the number of kubernetes clusters to use. You can run a given set of workloads either on few large clusters (with many workloads in each cluster) or on many clusters (with few workloads in each cluster). Here's a table that summarizes the pros and cons of various approaches: Figure: Possibilities of number of clusters from learnk8s.io article Operative Systems \u2691 Linux \u2691 Linux Snippets \u2691 New: Explain how to allocate space for a virtual filesystem. fallocate -l 20G /path/to/file Arts \u2691 Writing \u2691 Grammar and Orthography \u2691 New: Explain the use of z or s in some words. It looks like American english uses z while British uses s , some examples: Organizations vs organisation . Authorization vs authorisation . Customized vs customised . Both forms are correct, so choose the one that suits your liking.","title":"31st Week of 2021"},{"location":"newsletter/2021_w31/#devops","text":"","title":"DevOps"},{"location":"newsletter/2021_w31/#infrastructure-as-code","text":"","title":"Infrastructure as Code"},{"location":"newsletter/2021_w31/#terraform","text":"New: Introduce terraform and how to handle RDS secrets. Terraform is an open-source infrastructure as code software tool created by HashiCorp. It enables users to define and provision a datacenter infrastructure using an awful high-level configuration language known as Hashicorp Configuration Language (HCL), or optionally JSON. Terraform supports a number of cloud infrastructure providers such as Amazon Web Services, IBM Cloud , Google Cloud Platform, DigitalOcean, Linode, Microsoft Azure, Oracle Cloud Infrastructure, OVH, or VMware vSphere as well as OpenNebula and OpenStack.","title":"Terraform"},{"location":"newsletter/2021_w31/#infrastructure-solutions","text":"","title":"Infrastructure Solutions"},{"location":"newsletter/2021_w31/#kubernetes","text":"New: Add Velero as interesting tool. Velero is a tool to backup and migrate Kubernetes resources and persistent volumes.","title":"Kubernetes"},{"location":"newsletter/2021_w31/#architecture","text":"New: Give suggestions on how to choose the number of kubernetes clusters to use. You can run a given set of workloads either on few large clusters (with many workloads in each cluster) or on many clusters (with few workloads in each cluster). Here's a table that summarizes the pros and cons of various approaches: Figure: Possibilities of number of clusters from learnk8s.io article","title":"Architecture"},{"location":"newsletter/2021_w31/#operative-systems","text":"","title":"Operative Systems"},{"location":"newsletter/2021_w31/#linux","text":"","title":"Linux"},{"location":"newsletter/2021_w31/#linux-snippets","text":"New: Explain how to allocate space for a virtual filesystem. fallocate -l 20G /path/to/file","title":"Linux Snippets"},{"location":"newsletter/2021_w31/#arts","text":"","title":"Arts"},{"location":"newsletter/2021_w31/#writing","text":"","title":"Writing"},{"location":"newsletter/2021_w31/#grammar-and-orthography","text":"New: Explain the use of z or s in some words. It looks like American english uses z while British uses s , some examples: Organizations vs organisation . Authorization vs authorisation . Customized vs customised . Both forms are correct, so choose the one that suits your liking.","title":"Grammar and Orthography"},{"location":"newsletter/2021_w32/","text":"Projects \u2691 New: Introduce pynbox the inbox management tool. Pynbox is a tool to improve the management of ideas, tasks, references, suggestions when I'm not in front of the computer. Right now I've got Markor for Android to register these quicknotes, but the reality is that I don't act upon them, so it's just a log of tasks that never get done, and ideas, references and suggestions that aren't registered in my knowledge or media management systems. On the computer there are also cases of tasks that are not worth registering in the task management system, or ideas that I get at a moment but don't have time to process at the moment. The idea then is to automatically sync the Android quicknote with syncthing, and have a special format for the file that allows pynbox to extract the elements from that file to the \"inbox system\". For example: + t. buy groceries tv. IT crowd i. Improve the inbox management I want a system to improve ... Gets introduced in the \"inbox system\" as a task, a TV suggestion and an idea. New: Introduce nyxt as a solution for a better browser. I've just stumbled upon nyxt ( code ), and it looks superb. Coding \u2691 Python \u2691 Python Snippets \u2691 New: Explain how to find a static file of a python module. import pkg_resources file_path = pkg_resources . resource_filename ( \"my_package\" , \"assets/config.yaml\" ), New: Explain how to delete a file. import os os . remove ( 'demofile.txt' ) New: Explain how to measure elapsed time between lines of code. import time start = time . time () print ( \"hello\" ) end = time . time () print ( end - start ) pexpect \u2691 New: Explain how to read the output of a command run by pexpect. import sys import pexpect child = pexpect . spawn ( 'ls' ) child . logfile = sys . stdout child . expect ( pexpect . EOF ) rich \u2691 New: Explain how to build pretty tables with rich. from rich.console import Console from rich.table import Table table = Table ( title = \"Star Wars Movies\" ) table . add_column ( \"Released\" , justify = \"right\" , style = \"cyan\" , no_wrap = True ) table . add_column ( \"Title\" , style = \"magenta\" ) table . add_column ( \"Box Office\" , justify = \"right\" , style = \"green\" ) table . add_row ( \"Dec 20, 2019\" , \"Star Wars: The Rise of Skywalker\" , \"$952,110,690\" ) table . add_row ( \"May 25, 2018\" , \"Solo: A Star Wars Story\" , \"$393,151,347\" ) table . add_row ( \"Dec 15, 2017\" , \"Star Wars Ep. V111: The Last Jedi\" , \"$1,332,539,889\" ) table . add_row ( \"Dec 16, 2016\" , \"Rogue One: A Star Wars Story\" , \"$1,332,439,889\" ) console = Console () console . print ( table ) New: Explain how to print pretty text with rich. from rich.console import Console from rich.text import Text console = Console () text = Text . assemble (( \"Hello\" , \"bold magenta\" ), \" World!\" ) console . print ( text ) DevOps \u2691 Infrastructure as Code \u2691 Terraform \u2691 New: Explain how to ignore the change of an attribute. resource \"aws_instance\" \"example\" { # ... lifecycle { ignore_changes = [ # Ignore changes to tags, e.g. because a management agent # updates these based on some ruleset managed elsewhere. tags, ] } } New: Explain how to define the default value of an variable that contains an object as empty. variable \"database\" { type = object({ size = number instance_type = string storage_type = string engine = string engine_version = string parameter_group_name = string multi_az = bool }) default = null New: Explain how to do a conditional if a variable is not null. resource \"aws_db_instance\" \"instance\" { count = var.database == null ? 0 : 1 ...","title":"32nd Week of 2021"},{"location":"newsletter/2021_w32/#projects","text":"New: Introduce pynbox the inbox management tool. Pynbox is a tool to improve the management of ideas, tasks, references, suggestions when I'm not in front of the computer. Right now I've got Markor for Android to register these quicknotes, but the reality is that I don't act upon them, so it's just a log of tasks that never get done, and ideas, references and suggestions that aren't registered in my knowledge or media management systems. On the computer there are also cases of tasks that are not worth registering in the task management system, or ideas that I get at a moment but don't have time to process at the moment. The idea then is to automatically sync the Android quicknote with syncthing, and have a special format for the file that allows pynbox to extract the elements from that file to the \"inbox system\". For example: + t. buy groceries tv. IT crowd i. Improve the inbox management I want a system to improve ... Gets introduced in the \"inbox system\" as a task, a TV suggestion and an idea. New: Introduce nyxt as a solution for a better browser. I've just stumbled upon nyxt ( code ), and it looks superb.","title":"Projects"},{"location":"newsletter/2021_w32/#coding","text":"","title":"Coding"},{"location":"newsletter/2021_w32/#python","text":"","title":"Python"},{"location":"newsletter/2021_w32/#python-snippets","text":"New: Explain how to find a static file of a python module. import pkg_resources file_path = pkg_resources . resource_filename ( \"my_package\" , \"assets/config.yaml\" ), New: Explain how to delete a file. import os os . remove ( 'demofile.txt' ) New: Explain how to measure elapsed time between lines of code. import time start = time . time () print ( \"hello\" ) end = time . time () print ( end - start )","title":"Python Snippets"},{"location":"newsletter/2021_w32/#pexpect","text":"New: Explain how to read the output of a command run by pexpect. import sys import pexpect child = pexpect . spawn ( 'ls' ) child . logfile = sys . stdout child . expect ( pexpect . EOF )","title":"pexpect"},{"location":"newsletter/2021_w32/#rich","text":"New: Explain how to build pretty tables with rich. from rich.console import Console from rich.table import Table table = Table ( title = \"Star Wars Movies\" ) table . add_column ( \"Released\" , justify = \"right\" , style = \"cyan\" , no_wrap = True ) table . add_column ( \"Title\" , style = \"magenta\" ) table . add_column ( \"Box Office\" , justify = \"right\" , style = \"green\" ) table . add_row ( \"Dec 20, 2019\" , \"Star Wars: The Rise of Skywalker\" , \"$952,110,690\" ) table . add_row ( \"May 25, 2018\" , \"Solo: A Star Wars Story\" , \"$393,151,347\" ) table . add_row ( \"Dec 15, 2017\" , \"Star Wars Ep. V111: The Last Jedi\" , \"$1,332,539,889\" ) table . add_row ( \"Dec 16, 2016\" , \"Rogue One: A Star Wars Story\" , \"$1,332,439,889\" ) console = Console () console . print ( table ) New: Explain how to print pretty text with rich. from rich.console import Console from rich.text import Text console = Console () text = Text . assemble (( \"Hello\" , \"bold magenta\" ), \" World!\" ) console . print ( text )","title":"rich"},{"location":"newsletter/2021_w32/#devops","text":"","title":"DevOps"},{"location":"newsletter/2021_w32/#infrastructure-as-code","text":"","title":"Infrastructure as Code"},{"location":"newsletter/2021_w32/#terraform","text":"New: Explain how to ignore the change of an attribute. resource \"aws_instance\" \"example\" { # ... lifecycle { ignore_changes = [ # Ignore changes to tags, e.g. because a management agent # updates these based on some ruleset managed elsewhere. tags, ] } } New: Explain how to define the default value of an variable that contains an object as empty. variable \"database\" { type = object({ size = number instance_type = string storage_type = string engine = string engine_version = string parameter_group_name = string multi_az = bool }) default = null New: Explain how to do a conditional if a variable is not null. resource \"aws_db_instance\" \"instance\" { count = var.database == null ? 0 : 1 ...","title":"Terraform"},{"location":"newsletter/2021_w33/","text":"Projects \u2691 New: Introduce the shared accounting seed project. I use beancount for my personal accounting, I'd like to have a system that integrates more less easily with beancount and let's do a shared accounting with other people, for example in trips. I've used settle up in the past but it requires access to their servers, and an account linked to google, facebook or one you register in their servers. I've looked at facto but it uses a logic that doesn't apply to my case, it does a heavy use on a common account, instead +of minimizing the transactions between the people. I also tried tabby , even though they still don't support Docker , but it doesn't suit my case either :(. Until a new solution shows up, I'll go with Tricky Tripper available in F-Droid, and manage the expenses myself and periodically send the html reports to the rest of the group. Improvement: Add quickwit as an interesting database solution for personal knowledge search engine. New: Promote the automation of email management project to seedling. Life Management \u2691 Email Management \u2691 Email Automation \u2691 New: Explain how setup an infrastructure to automate. Coding \u2691 Python \u2691 asyncio \u2691 New: Introduce the asyncio library. asyncio is a library to write concurrent code using the async/await syntax. asyncio is used as a foundation for multiple Python asynchronous frameworks that provide high-performance network and web-servers, database connection libraries, distributed task queues, etc. asyncio is often a perfect fit for IO-bound and high-level structured network code. FastAPI \u2691 Improvement: Add link to the Awesome FastAPI page. SQLite \u2691 Improvement: Add rqlite as an interesting distributed solution of. Operative Systems \u2691 Linux \u2691 afew \u2691 New: Introduce afew. afew is an initial tagging script for notmuch mail . Its basic task is to provide automatic tagging each time new mail is registered with notmuch . In a classic setup, you might call it after notmuch new in an offlineimap post sync hook. alot \u2691 New: Introduce alot. alot is a terminal-based mail user agent based on the notmuch mail indexer . It is written in python using the urwid toolkit and features a modular and command prompt driven interface to provide a full MUA experience.","title":"33rd Week of 2021"},{"location":"newsletter/2021_w33/#projects","text":"New: Introduce the shared accounting seed project. I use beancount for my personal accounting, I'd like to have a system that integrates more less easily with beancount and let's do a shared accounting with other people, for example in trips. I've used settle up in the past but it requires access to their servers, and an account linked to google, facebook or one you register in their servers. I've looked at facto but it uses a logic that doesn't apply to my case, it does a heavy use on a common account, instead +of minimizing the transactions between the people. I also tried tabby , even though they still don't support Docker , but it doesn't suit my case either :(. Until a new solution shows up, I'll go with Tricky Tripper available in F-Droid, and manage the expenses myself and periodically send the html reports to the rest of the group. Improvement: Add quickwit as an interesting database solution for personal knowledge search engine. New: Promote the automation of email management project to seedling.","title":"Projects"},{"location":"newsletter/2021_w33/#life-management","text":"","title":"Life Management"},{"location":"newsletter/2021_w33/#email-management","text":"","title":"Email Management"},{"location":"newsletter/2021_w33/#email-automation","text":"New: Explain how setup an infrastructure to automate.","title":"Email Automation"},{"location":"newsletter/2021_w33/#coding","text":"","title":"Coding"},{"location":"newsletter/2021_w33/#python","text":"","title":"Python"},{"location":"newsletter/2021_w33/#asyncio","text":"New: Introduce the asyncio library. asyncio is a library to write concurrent code using the async/await syntax. asyncio is used as a foundation for multiple Python asynchronous frameworks that provide high-performance network and web-servers, database connection libraries, distributed task queues, etc. asyncio is often a perfect fit for IO-bound and high-level structured network code.","title":"asyncio"},{"location":"newsletter/2021_w33/#fastapi","text":"Improvement: Add link to the Awesome FastAPI page.","title":"FastAPI"},{"location":"newsletter/2021_w33/#sqlite","text":"Improvement: Add rqlite as an interesting distributed solution of.","title":"SQLite"},{"location":"newsletter/2021_w33/#operative-systems","text":"","title":"Operative Systems"},{"location":"newsletter/2021_w33/#linux","text":"","title":"Linux"},{"location":"newsletter/2021_w33/#afew","text":"New: Introduce afew. afew is an initial tagging script for notmuch mail . Its basic task is to provide automatic tagging each time new mail is registered with notmuch . In a classic setup, you might call it after notmuch new in an offlineimap post sync hook.","title":"afew"},{"location":"newsletter/2021_w33/#alot","text":"New: Introduce alot. alot is a terminal-based mail user agent based on the notmuch mail indexer . It is written in python using the urwid toolkit and features a modular and command prompt driven interface to provide a full MUA experience.","title":"alot"},{"location":"newsletter/2021_w34/","text":"Activism \u2691 New: Introduce Anti-transphobia. Anti-transphobia being reductionist is the opposition to the collection of ideas and phenomena that encompass a range of negative attitudes, feelings or actions towards transgender people or transness in general. Transphobia can include fear, aversion, hatred, violence, anger, or discomfort felt or expressed towards people who do not conform to social gender expectations. It is often expressed alongside homophobic views and hence is often considered an aspect of homophobia. New: Introduce arguments against terf ideology. TERF is an acronym for trans-exclusionary radical feminist . The term originally applied to the minority of feminists that expressed transphobic sentiments such as the rejection of the assertion that trans women are women, the exclusion of trans women from women's spaces, and opposition to transgender rights legislation. The meaning has since expanded to refer more broadly to people with trans-exclusionary views who may have no involvement with radical feminism. Antifascism \u2691 New: Introduce antifascism. Antifascism is a method of politics, a locus of individual and group self-indentification, it's a transnational movement that adapted preexisting socialist, anarchist, and communist currents to a sudden need to react to the fascist menace ( Mark p. 11 ). It's based on the idea that any oppression form can't be allowed, and should be actively fought with whatever means are necessary. Coding \u2691 Python \u2691 New: Add schedule to interesting libraries to explore. schedule is a Python job scheduling for humans. Run Python functions (or any other callable) periodically using a friendly syntax. Contact \u2691 Correction: Update the XMPP address. Riseup has stopped giving support for XMPP :(","title":"34th Week of 2021"},{"location":"newsletter/2021_w34/#activism","text":"New: Introduce Anti-transphobia. Anti-transphobia being reductionist is the opposition to the collection of ideas and phenomena that encompass a range of negative attitudes, feelings or actions towards transgender people or transness in general. Transphobia can include fear, aversion, hatred, violence, anger, or discomfort felt or expressed towards people who do not conform to social gender expectations. It is often expressed alongside homophobic views and hence is often considered an aspect of homophobia. New: Introduce arguments against terf ideology. TERF is an acronym for trans-exclusionary radical feminist . The term originally applied to the minority of feminists that expressed transphobic sentiments such as the rejection of the assertion that trans women are women, the exclusion of trans women from women's spaces, and opposition to transgender rights legislation. The meaning has since expanded to refer more broadly to people with trans-exclusionary views who may have no involvement with radical feminism.","title":"Activism"},{"location":"newsletter/2021_w34/#antifascism","text":"New: Introduce antifascism. Antifascism is a method of politics, a locus of individual and group self-indentification, it's a transnational movement that adapted preexisting socialist, anarchist, and communist currents to a sudden need to react to the fascist menace ( Mark p. 11 ). It's based on the idea that any oppression form can't be allowed, and should be actively fought with whatever means are necessary.","title":"Antifascism"},{"location":"newsletter/2021_w34/#coding","text":"","title":"Coding"},{"location":"newsletter/2021_w34/#python","text":"New: Add schedule to interesting libraries to explore. schedule is a Python job scheduling for humans. Run Python functions (or any other callable) periodically using a friendly syntax.","title":"Python"},{"location":"newsletter/2021_w34/#contact","text":"Correction: Update the XMPP address. Riseup has stopped giving support for XMPP :(","title":"Contact"},{"location":"newsletter/2021_w38/","text":"Projects \u2691 New: Introduce the pomodoro command line seed project. Command line to help with the pomodoro workflow , besides the basic stuff it will interact with the task manager, activitywatch and the notifications system. New: Introduce the ordered list of digital gardens project. Use best-of-lists to create an awesome list of digital gardens. Coding \u2691 Python \u2691 New: Add tryceratops to interesting linters to try. tryceratops is a linter of exceptions. Operative Systems \u2691 Linux \u2691 Jellyfin \u2691 Correction: Fix the stuck at login page error. If you use jfa-go for the invites, you may need to regenerate all the user profiles , so that the problem is not introduced again.","title":"38th Week of 2021"},{"location":"newsletter/2021_w38/#projects","text":"New: Introduce the pomodoro command line seed project. Command line to help with the pomodoro workflow , besides the basic stuff it will interact with the task manager, activitywatch and the notifications system. New: Introduce the ordered list of digital gardens project. Use best-of-lists to create an awesome list of digital gardens.","title":"Projects"},{"location":"newsletter/2021_w38/#coding","text":"","title":"Coding"},{"location":"newsletter/2021_w38/#python","text":"New: Add tryceratops to interesting linters to try. tryceratops is a linter of exceptions.","title":"Python"},{"location":"newsletter/2021_w38/#operative-systems","text":"","title":"Operative Systems"},{"location":"newsletter/2021_w38/#linux","text":"","title":"Linux"},{"location":"newsletter/2021_w38/#jellyfin","text":"Correction: Fix the stuck at login page error. If you use jfa-go for the invites, you may need to regenerate all the user profiles , so that the problem is not introduced again.","title":"Jellyfin"},{"location":"newsletter/2021_w39/","text":"Projects \u2691 Correction: Clean up deprecated projects. Activism \u2691 Antifascism \u2691 Antifascist Actions \u2691 New: A fake company and five million recycled flyers. A group of artists belonging to the Center for political beauty created a fake company Flyerservice Hahn and convinced more than 80 regional sections of the far right party AfD to hire them to deliver their electoral propaganda. They gathered five million flyers, with a total weight of 72 tons. They justify that they wouldn't be able to lie to the people, so they did nothing in the broader sense of the word. They declared that they are the \"world wide leader in the non-delivery of nazi propaganda\" . At the start of the electoral campaign, they went to the AfD stands, and they let their members to give them flyers the throw them to the closest bin. \"It's something that any citizen can freely do, we have only industrialized the process\". They've done a crowdfunding to fund the legal process that may result. Coding \u2691 Python \u2691 Python Snippets \u2691 New: Document when to use isinstance and when to use type . isinstance takes into account inheritance, while type doesn't. So if you want to make sure you're dealing with a specific class, and not any of it's parents or subclasses, use type(obj) == class . DevOps \u2691 Infrastructure as Code \u2691 Helmfile \u2691 Correction: Improve the helmfile chart update process. Updating charts with helmfile is easy as long as you don't use environments, you run helmfile deps , then helmfile diff and finally helmfile apply . The tricky business comes when you want to use environments to reuse your helmfile code and don't repeat yourself. I've updated the process to include this case. New: Document the directory and files structure for multi-environment projects. New: Document how to use helmfile environments to follow DRY. New: Document how to avoiding code repetition. Besides environments, helmfile gives other useful tricks to prevent the illness of code repetition, such as using release templates , or layering the state . New: Document how to manage dependencies between the charts, to be able to use concurrency. Helmfile support concurrency with the option --concurrency=N so we can take advantage of it and improve our deployment speed, but to ensure it works as expected we have to define the dependencies among charts. For example, if an application needs a database, it has to be deployed before hand. Operative Systems \u2691 Linux \u2691 Linux Snippets \u2691 New: Document how to bypass client SSL certificate with a cli tool. Websites that require clients to authorize with an TLS certificate are difficult to interact with through command line tools that don't support this feature. To solve it, we can use a transparent proxy that does the exchange for us.","title":"39th Week of 2021"},{"location":"newsletter/2021_w39/#projects","text":"Correction: Clean up deprecated projects.","title":"Projects"},{"location":"newsletter/2021_w39/#activism","text":"","title":"Activism"},{"location":"newsletter/2021_w39/#antifascism","text":"","title":"Antifascism"},{"location":"newsletter/2021_w39/#antifascist-actions","text":"New: A fake company and five million recycled flyers. A group of artists belonging to the Center for political beauty created a fake company Flyerservice Hahn and convinced more than 80 regional sections of the far right party AfD to hire them to deliver their electoral propaganda. They gathered five million flyers, with a total weight of 72 tons. They justify that they wouldn't be able to lie to the people, so they did nothing in the broader sense of the word. They declared that they are the \"world wide leader in the non-delivery of nazi propaganda\" . At the start of the electoral campaign, they went to the AfD stands, and they let their members to give them flyers the throw them to the closest bin. \"It's something that any citizen can freely do, we have only industrialized the process\". They've done a crowdfunding to fund the legal process that may result.","title":"Antifascist Actions"},{"location":"newsletter/2021_w39/#coding","text":"","title":"Coding"},{"location":"newsletter/2021_w39/#python","text":"","title":"Python"},{"location":"newsletter/2021_w39/#python-snippets","text":"New: Document when to use isinstance and when to use type . isinstance takes into account inheritance, while type doesn't. So if you want to make sure you're dealing with a specific class, and not any of it's parents or subclasses, use type(obj) == class .","title":"Python Snippets"},{"location":"newsletter/2021_w39/#devops","text":"","title":"DevOps"},{"location":"newsletter/2021_w39/#infrastructure-as-code","text":"","title":"Infrastructure as Code"},{"location":"newsletter/2021_w39/#helmfile","text":"Correction: Improve the helmfile chart update process. Updating charts with helmfile is easy as long as you don't use environments, you run helmfile deps , then helmfile diff and finally helmfile apply . The tricky business comes when you want to use environments to reuse your helmfile code and don't repeat yourself. I've updated the process to include this case. New: Document the directory and files structure for multi-environment projects. New: Document how to use helmfile environments to follow DRY. New: Document how to avoiding code repetition. Besides environments, helmfile gives other useful tricks to prevent the illness of code repetition, such as using release templates , or layering the state . New: Document how to manage dependencies between the charts, to be able to use concurrency. Helmfile support concurrency with the option --concurrency=N so we can take advantage of it and improve our deployment speed, but to ensure it works as expected we have to define the dependencies among charts. For example, if an application needs a database, it has to be deployed before hand.","title":"Helmfile"},{"location":"newsletter/2021_w39/#operative-systems","text":"","title":"Operative Systems"},{"location":"newsletter/2021_w39/#linux","text":"","title":"Linux"},{"location":"newsletter/2021_w39/#linux-snippets","text":"New: Document how to bypass client SSL certificate with a cli tool. Websites that require clients to authorize with an TLS certificate are difficult to interact with through command line tools that don't support this feature. To solve it, we can use a transparent proxy that does the exchange for us.","title":"Linux Snippets"},{"location":"newsletter/2021_w40/","text":"Coding \u2691 Python \u2691 Click \u2691 New: Invoke other commands from a command. This is a pattern that is generally discouraged with Click, but possible nonetheless. For this, you can use the Context.invoke() or Context.forward() methods. Optimization \u2691 New: Add tips on how to optimize your python command line tools. Minimize the relative import statements on command line tools : When developing a library, it's common to expose the main objects into the package __init__.py under the variable __all__ . The problem with command line programs is that each time you run the command it will load those objects, which can mean an increase of 0.5s or even a second for each command, which is unacceptable. * Don't dynamically install the package : If you install the package with pip install -e . you will see an increase on the load time of ~0.2s. It is useful to develop the package, but when you use it, do so from a virtualenv that installs it directly without the -e flag. Python Snippets \u2691 New: Check if a dictionary is a subset of another. If you have two dictionaries big = {'a': 1, 'b': 2, 'c':3} and small = {'c': 3, 'a': 1} , and want to check whether small is a subset of big , use the next snippet: >>> small . items () <= big . items () True Correction: Group or sort a list of dictionaries or objects by a specific key. Improve previous method with the concepts learned from the official docs Particularly improve the sorting by multiple keys with the next function: >>> def multisort ( xs , specs ): for key , reverse in reversed ( specs ): xs . sort ( key = attrgetter ( key ), reverse = reverse ) return xs >>> multisort ( list ( student_objects ), (( 'grade' , True ), ( 'age' , False ))) [( 'dave' , 'B' , 10 ), ( 'jane' , 'B' , 12 ), ( 'john' , 'A' , 15 )] Pydantic \u2691 New: Copy produces copy that modifies the original. When copying a model, changing the value of an attribute on the copy updates the value of the attribute on the original. This only happens if deep != True . To fix it use: model.copy(deep=True) . DevOps \u2691 Continuous Integration \u2691 Flakehell \u2691 New: Troubleshoot the 'Namespace' object has no attribute 'extended_default_ignore' error. Add to your pyproject.toml : [tool.flakehell] extended_default_ignore = [] Dependency managers \u2691 New: Sync the virtualenv libraries with the requirements files. python - m piptools sync requirements . txt requirements - dev . txt Correction: Use -c instead of -r in the nested requirement files. To avoid duplication of version pins. Operative Systems \u2691 Linux \u2691 Kitty \u2691 New: Introduce kitty the terminal emulator. kitty is a fast, feature-rich, GPU based terminal emulator written in C and Python with nice features for the keyboard driven humans like me.","title":"40th Week of 2021"},{"location":"newsletter/2021_w40/#coding","text":"","title":"Coding"},{"location":"newsletter/2021_w40/#python","text":"","title":"Python"},{"location":"newsletter/2021_w40/#click","text":"New: Invoke other commands from a command. This is a pattern that is generally discouraged with Click, but possible nonetheless. For this, you can use the Context.invoke() or Context.forward() methods.","title":"Click"},{"location":"newsletter/2021_w40/#optimization","text":"New: Add tips on how to optimize your python command line tools. Minimize the relative import statements on command line tools : When developing a library, it's common to expose the main objects into the package __init__.py under the variable __all__ . The problem with command line programs is that each time you run the command it will load those objects, which can mean an increase of 0.5s or even a second for each command, which is unacceptable. * Don't dynamically install the package : If you install the package with pip install -e . you will see an increase on the load time of ~0.2s. It is useful to develop the package, but when you use it, do so from a virtualenv that installs it directly without the -e flag.","title":"Optimization"},{"location":"newsletter/2021_w40/#python-snippets","text":"New: Check if a dictionary is a subset of another. If you have two dictionaries big = {'a': 1, 'b': 2, 'c':3} and small = {'c': 3, 'a': 1} , and want to check whether small is a subset of big , use the next snippet: >>> small . items () <= big . items () True Correction: Group or sort a list of dictionaries or objects by a specific key. Improve previous method with the concepts learned from the official docs Particularly improve the sorting by multiple keys with the next function: >>> def multisort ( xs , specs ): for key , reverse in reversed ( specs ): xs . sort ( key = attrgetter ( key ), reverse = reverse ) return xs >>> multisort ( list ( student_objects ), (( 'grade' , True ), ( 'age' , False ))) [( 'dave' , 'B' , 10 ), ( 'jane' , 'B' , 12 ), ( 'john' , 'A' , 15 )]","title":"Python Snippets"},{"location":"newsletter/2021_w40/#pydantic","text":"New: Copy produces copy that modifies the original. When copying a model, changing the value of an attribute on the copy updates the value of the attribute on the original. This only happens if deep != True . To fix it use: model.copy(deep=True) .","title":"Pydantic"},{"location":"newsletter/2021_w40/#devops","text":"","title":"DevOps"},{"location":"newsletter/2021_w40/#continuous-integration","text":"","title":"Continuous Integration"},{"location":"newsletter/2021_w40/#flakehell","text":"New: Troubleshoot the 'Namespace' object has no attribute 'extended_default_ignore' error. Add to your pyproject.toml : [tool.flakehell] extended_default_ignore = []","title":"Flakehell"},{"location":"newsletter/2021_w40/#dependency-managers","text":"New: Sync the virtualenv libraries with the requirements files. python - m piptools sync requirements . txt requirements - dev . txt Correction: Use -c instead of -r in the nested requirement files. To avoid duplication of version pins.","title":"Dependency managers"},{"location":"newsletter/2021_w40/#operative-systems","text":"","title":"Operative Systems"},{"location":"newsletter/2021_w40/#linux","text":"","title":"Linux"},{"location":"newsletter/2021_w40/#kitty","text":"New: Introduce kitty the terminal emulator. kitty is a fast, feature-rich, GPU based terminal emulator written in C and Python with nice features for the keyboard driven humans like me.","title":"Kitty"},{"location":"newsletter/2021_w41/","text":"Activism \u2691 Feminism \u2691 Privileges \u2691 New: Feminist analysis of privileges and rights. Privileges are a group of special structural benefits, social advantages, that a group holds over another. So they are elements that should be removed from our lives. Some of the topics included are: What's the difference between privilege and right What can we do to fight the privileges? Life Management \u2691 Book Management \u2691 New: Introduce the book management concept. Book management is the set of systems and processes to get and categorize books so it's easy to browse and discover new content. It involves the next actions: Automatically index and download metadata of new books. Notify the user when a new book is added. Monitor the books of an author, and get them once they are released. Send books to the e-reader. A nice interface to browse the existent library, with the possibility of filtering by author, genre, years, tags or series. An interface to preview or read the items. An interface to rate and review library items. An interface to discover new content based on the ratings and item metadata. I haven't yet found a single piece of software that fulfills all these needs, in the article I tell you about Readarr , Calibre-web , [calibre](( https://manual.calibre-ebook.com/ ), Polar bookself , GCStar , and how they interact with each other. Coding \u2691 Python \u2691 Flask Restplus \u2691 New: Introduce the Flask-RESTPlus library. Flask-RESTPlus is an extension for Flask that adds support for quickly building REST APIs, but I'd use FastAPI instead. Python Snippets \u2691 Correction: Install default directories and files for a command line program. I've been trying for a long time to configure setup.py to run the required steps to configure the required directories and files when doing pip install without success. Finally, I decided that the program itself should create the data once the FileNotFoundError exception is found. That way, you don't penalize the load time because if the file or directory exists, that code is not run. Promql \u2691 New: Generating range vectors from return values in Prometheus queries. DevOps \u2691 Infrastructure as Code \u2691 Terraform \u2691 New: How to do elif conditionals in terraform. locals { test = \"${ condition ? value : (elif-condition ? elif-value : else-value)}\" } New: How to enable debug traces. You can set the TF_LOG environmental variable to one of the log levels TRACE , DEBUG , INFO , WARN or ERROR to change the verbosity of the logs. Continuous Integration \u2691 Flakehell \u2691 New: Latest version is broken. It returns an ImportError: cannot import name 'MergedConfigParser' from 'flake8.options.config' , wait for the issue to be solved before upgrading. Operative Systems \u2691 Linux \u2691 Dynamic DNS \u2691 New: Introduce the Dynamic DNS concept. Dynamic DNS (DDNS) is a method of automatically updating a name server in the Domain Name Server (DNS), often in real time, with the active DDNS configuration of its configured hostnames, addresses or other information. Hard drive health \u2691 New: Taking care of your hard drives. Hard drives die, so we must be ready for that to happen. There are several solutions, such as using RAID to minimize the impact of a disk loss, but even then, we should monitor the bad sectors to see when are our disks dying. In the article we talk about S.M.A.R.T and how to solve some hard drive problems. Kitty \u2691 New: Scrollback when ssh into a machine doesn't work. This happens because the kitty terminfo files are not available on the server. You can ssh in using the following command which will automatically copy the terminfo files to the server: kitty +kitten ssh myserver","title":"41st Week of 2021"},{"location":"newsletter/2021_w41/#activism","text":"","title":"Activism"},{"location":"newsletter/2021_w41/#feminism","text":"","title":"Feminism"},{"location":"newsletter/2021_w41/#privileges","text":"New: Feminist analysis of privileges and rights. Privileges are a group of special structural benefits, social advantages, that a group holds over another. So they are elements that should be removed from our lives. Some of the topics included are: What's the difference between privilege and right What can we do to fight the privileges?","title":"Privileges"},{"location":"newsletter/2021_w41/#life-management","text":"","title":"Life Management"},{"location":"newsletter/2021_w41/#book-management","text":"New: Introduce the book management concept. Book management is the set of systems and processes to get and categorize books so it's easy to browse and discover new content. It involves the next actions: Automatically index and download metadata of new books. Notify the user when a new book is added. Monitor the books of an author, and get them once they are released. Send books to the e-reader. A nice interface to browse the existent library, with the possibility of filtering by author, genre, years, tags or series. An interface to preview or read the items. An interface to rate and review library items. An interface to discover new content based on the ratings and item metadata. I haven't yet found a single piece of software that fulfills all these needs, in the article I tell you about Readarr , Calibre-web , [calibre](( https://manual.calibre-ebook.com/ ), Polar bookself , GCStar , and how they interact with each other.","title":"Book Management"},{"location":"newsletter/2021_w41/#coding","text":"","title":"Coding"},{"location":"newsletter/2021_w41/#python","text":"","title":"Python"},{"location":"newsletter/2021_w41/#flask-restplus","text":"New: Introduce the Flask-RESTPlus library. Flask-RESTPlus is an extension for Flask that adds support for quickly building REST APIs, but I'd use FastAPI instead.","title":"Flask Restplus"},{"location":"newsletter/2021_w41/#python-snippets","text":"Correction: Install default directories and files for a command line program. I've been trying for a long time to configure setup.py to run the required steps to configure the required directories and files when doing pip install without success. Finally, I decided that the program itself should create the data once the FileNotFoundError exception is found. That way, you don't penalize the load time because if the file or directory exists, that code is not run.","title":"Python Snippets"},{"location":"newsletter/2021_w41/#promql","text":"New: Generating range vectors from return values in Prometheus queries.","title":"Promql"},{"location":"newsletter/2021_w41/#devops","text":"","title":"DevOps"},{"location":"newsletter/2021_w41/#infrastructure-as-code","text":"","title":"Infrastructure as Code"},{"location":"newsletter/2021_w41/#terraform","text":"New: How to do elif conditionals in terraform. locals { test = \"${ condition ? value : (elif-condition ? elif-value : else-value)}\" } New: How to enable debug traces. You can set the TF_LOG environmental variable to one of the log levels TRACE , DEBUG , INFO , WARN or ERROR to change the verbosity of the logs.","title":"Terraform"},{"location":"newsletter/2021_w41/#continuous-integration","text":"","title":"Continuous Integration"},{"location":"newsletter/2021_w41/#flakehell","text":"New: Latest version is broken. It returns an ImportError: cannot import name 'MergedConfigParser' from 'flake8.options.config' , wait for the issue to be solved before upgrading.","title":"Flakehell"},{"location":"newsletter/2021_w41/#operative-systems","text":"","title":"Operative Systems"},{"location":"newsletter/2021_w41/#linux","text":"","title":"Linux"},{"location":"newsletter/2021_w41/#dynamic-dns","text":"New: Introduce the Dynamic DNS concept. Dynamic DNS (DDNS) is a method of automatically updating a name server in the Domain Name Server (DNS), often in real time, with the active DDNS configuration of its configured hostnames, addresses or other information.","title":"Dynamic DNS"},{"location":"newsletter/2021_w41/#hard-drive-health","text":"New: Taking care of your hard drives. Hard drives die, so we must be ready for that to happen. There are several solutions, such as using RAID to minimize the impact of a disk loss, but even then, we should monitor the bad sectors to see when are our disks dying. In the article we talk about S.M.A.R.T and how to solve some hard drive problems.","title":"Hard drive health"},{"location":"newsletter/2021_w41/#kitty","text":"New: Scrollback when ssh into a machine doesn't work. This happens because the kitty terminfo files are not available on the server. You can ssh in using the following command which will automatically copy the terminfo files to the server: kitty +kitten ssh myserver","title":"Kitty"},{"location":"newsletter/2021_w43/","text":"Life Management \u2691 Book Management \u2691 Improvement: Add link to the calibre-web kobo integration project. Coding \u2691 Python \u2691 Full screen applications \u2691 New: Testing full screen applications. New: Pass more than one key. To map an action to two key presses use kb.add('g', 'g') . New: Add note on how to debug the styles of the components. Set the style to bg:#dc322f and it will be highlighted in red. Pytest \u2691 New: Exclude the if TYPE_CHECKING code from the coverage. If you want other code to be excluded , for example the statements inside the if TYPE_CHECKING: add to your pyproject.toml : [tool.coverage.report] exclude_lines = [ # Have to re-enable the standard pragma 'pragma: no cover' , # Type checking can not be tested 'if TYPE_CHECKING:' , ] Python Snippets \u2691 New: Locate element in list. a = [ 'a' , 'b' ] index = a . index ( 'b' ) New: Transpose a list of lists. >>> l = [[ 1 , 2 , 3 ],[ 4 , 5 , 6 ],[ 7 , 8 , 9 ]] >>> [ list ( i ) for i in zip ( * l )] ... [[ 1 , 4 , 7 ], [ 2 , 5 , 8 ], [ 3 , 6 , 9 ]] New: Check the type of a list of strings. def _is_list_of_lists ( data : Any ) -> bool : \"\"\"Check if data is a list of strings.\"\"\" if data and isinstance ( data , list ): return all ( isinstance ( elem , list ) for elem in data ) else : return False Prompt Toolkit \u2691 New: Basic concepts of building full screen applications with python prompt toolkit. prompt_toolkit can be used to create complex full screen terminal applications. Typically, an application consists of a layout (to describe the graphical part) and a set of key bindings. In the section we cover: The layout The controls How to use key bindings How to apply styles A difficult ordered list of examples to get a grasp of these concepts with simple working code. DevOps \u2691 Infrastructure as Code \u2691 Helmfile \u2691 Correction: Use environment name instead of get values. Instead of .Environment.Name , in theory you could have used .Vars | get \"environment\" , which could have prevented the variables and secrets of the default environment will need to be called default_values.yaml , and default_secrets.yaml , which is misleading. But you can't use .Values in the helmfile.yaml as it's not loaded when the file is parsed, and you get an error. A solution would be to layer the helmfile state files but I wasn't able to make it work. New: How to install a chart only in one environment. environments : default : production : --- releases : - name : newrelic-agent installed : {{ eq .Environment.Name \"production\" | toYaml }} # snip New: Add note that templates can't be used inside the secrets. See this issue Helm Secrets \u2691 Correction: Update the repository url. The last fork is dead, long live the fork New: How to install the plugin. Operative Systems \u2691 Linux \u2691 Kitty \u2691 New: Enable infinite scrollback history. To make the history scrollback infinite add the next lines: scrollback_lines -1 scrollback_pager_history_size 0 New: Reasons to migrate from urxvt to kitty. It doesn't fuck up your terminal colors. You can use peek to record your screen. Easier to extend. Peek \u2691 Correction: Add note that it works with kitty. Arts \u2691 Drawing \u2691 New: How to draw Ellipses. Ellipses are the next basic shape we're going to study (after the lines). They are extremely important and notoriously annoying to draw. Important because we're going to be using ellipses in 2D space to represent circles that exist in 3D space. In this section we: Introduce the basic concepts surrounding the ellipses How to draw them . Exercise Pool \u2691 New: Add the Tables of ellipses drawing exercise. This exercise is meant to get you used to drawing ellipses, in a variety of sizes, orientations and degrees. It also sets out a clear space each ellipse is meant to occupy, giving us a means to assess whether or not an ellipse was successful, or if there were visible mistakes (where it went outside of its allotted space, or ended up falling short). Practicing against set criteria, with a way to judge success/failure is an important element of learning. There's nothing wrong with failure - it's an opportunity to learn. Having a clearly defined task allows us to analyze those failures and make the most of them.","title":"43rd Week of 2021"},{"location":"newsletter/2021_w43/#life-management","text":"","title":"Life Management"},{"location":"newsletter/2021_w43/#book-management","text":"Improvement: Add link to the calibre-web kobo integration project.","title":"Book Management"},{"location":"newsletter/2021_w43/#coding","text":"","title":"Coding"},{"location":"newsletter/2021_w43/#python","text":"","title":"Python"},{"location":"newsletter/2021_w43/#full-screen-applications","text":"New: Testing full screen applications. New: Pass more than one key. To map an action to two key presses use kb.add('g', 'g') . New: Add note on how to debug the styles of the components. Set the style to bg:#dc322f and it will be highlighted in red.","title":"Full screen applications"},{"location":"newsletter/2021_w43/#pytest","text":"New: Exclude the if TYPE_CHECKING code from the coverage. If you want other code to be excluded , for example the statements inside the if TYPE_CHECKING: add to your pyproject.toml : [tool.coverage.report] exclude_lines = [ # Have to re-enable the standard pragma 'pragma: no cover' , # Type checking can not be tested 'if TYPE_CHECKING:' , ]","title":"Pytest"},{"location":"newsletter/2021_w43/#python-snippets","text":"New: Locate element in list. a = [ 'a' , 'b' ] index = a . index ( 'b' ) New: Transpose a list of lists. >>> l = [[ 1 , 2 , 3 ],[ 4 , 5 , 6 ],[ 7 , 8 , 9 ]] >>> [ list ( i ) for i in zip ( * l )] ... [[ 1 , 4 , 7 ], [ 2 , 5 , 8 ], [ 3 , 6 , 9 ]] New: Check the type of a list of strings. def _is_list_of_lists ( data : Any ) -> bool : \"\"\"Check if data is a list of strings.\"\"\" if data and isinstance ( data , list ): return all ( isinstance ( elem , list ) for elem in data ) else : return False","title":"Python Snippets"},{"location":"newsletter/2021_w43/#prompt-toolkit","text":"New: Basic concepts of building full screen applications with python prompt toolkit. prompt_toolkit can be used to create complex full screen terminal applications. Typically, an application consists of a layout (to describe the graphical part) and a set of key bindings. In the section we cover: The layout The controls How to use key bindings How to apply styles A difficult ordered list of examples to get a grasp of these concepts with simple working code.","title":"Prompt Toolkit"},{"location":"newsletter/2021_w43/#devops","text":"","title":"DevOps"},{"location":"newsletter/2021_w43/#infrastructure-as-code","text":"","title":"Infrastructure as Code"},{"location":"newsletter/2021_w43/#helmfile","text":"Correction: Use environment name instead of get values. Instead of .Environment.Name , in theory you could have used .Vars | get \"environment\" , which could have prevented the variables and secrets of the default environment will need to be called default_values.yaml , and default_secrets.yaml , which is misleading. But you can't use .Values in the helmfile.yaml as it's not loaded when the file is parsed, and you get an error. A solution would be to layer the helmfile state files but I wasn't able to make it work. New: How to install a chart only in one environment. environments : default : production : --- releases : - name : newrelic-agent installed : {{ eq .Environment.Name \"production\" | toYaml }} # snip New: Add note that templates can't be used inside the secrets. See this issue","title":"Helmfile"},{"location":"newsletter/2021_w43/#helm-secrets","text":"Correction: Update the repository url. The last fork is dead, long live the fork New: How to install the plugin.","title":"Helm Secrets"},{"location":"newsletter/2021_w43/#operative-systems","text":"","title":"Operative Systems"},{"location":"newsletter/2021_w43/#linux","text":"","title":"Linux"},{"location":"newsletter/2021_w43/#kitty","text":"New: Enable infinite scrollback history. To make the history scrollback infinite add the next lines: scrollback_lines -1 scrollback_pager_history_size 0 New: Reasons to migrate from urxvt to kitty. It doesn't fuck up your terminal colors. You can use peek to record your screen. Easier to extend.","title":"Kitty"},{"location":"newsletter/2021_w43/#peek","text":"Correction: Add note that it works with kitty.","title":"Peek"},{"location":"newsletter/2021_w43/#arts","text":"","title":"Arts"},{"location":"newsletter/2021_w43/#drawing","text":"New: How to draw Ellipses. Ellipses are the next basic shape we're going to study (after the lines). They are extremely important and notoriously annoying to draw. Important because we're going to be using ellipses in 2D space to represent circles that exist in 3D space. In this section we: Introduce the basic concepts surrounding the ellipses How to draw them .","title":"Drawing"},{"location":"newsletter/2021_w43/#exercise-pool","text":"New: Add the Tables of ellipses drawing exercise. This exercise is meant to get you used to drawing ellipses, in a variety of sizes, orientations and degrees. It also sets out a clear space each ellipse is meant to occupy, giving us a means to assess whether or not an ellipse was successful, or if there were visible mistakes (where it went outside of its allotted space, or ended up falling short). Practicing against set criteria, with a way to judge success/failure is an important element of learning. There's nothing wrong with failure - it's an opportunity to learn. Having a clearly defined task allows us to analyze those failures and make the most of them.","title":"Exercise Pool"},{"location":"projects/projects/","text":"There is an ever growing pool of ideas where I want to invest my time. Sadly time is a finite currency, and even though I am lucky enough to be able to put my focus on maximizing it, it's never enough. I understand projects as a mental tool that groups ideas, processes and tools to achieve a specific goal. Following the digital garden metaphor, projects are plants in different phases of development where I've spent a different amount of effort. The development phases are: Seeds : Are raw, basic ideas of projects that may once be. Seedlings : Are projects that don't yet have their first stable version, but the drafts of the ADR and some code is already written. Growing : Projects that have a stable release and are under active development. Dormant : Projects whose growth has temporally stopped. I still believe they are useful and even though I don't want to work on them at the moment, I see myself doing it in the future. I still maintain them by answering to issues, reviewing pull requests, keeping the continuous integration pipelines alive and developing fixes to important issues. Dying : Projects that I know are going to be deprecated soon, and I'm looking for alternatives. Dead : Project no longer used. Growing plants \u2691 Blue book \u2691 What you're reading right now. I'm storing most of the new knowledge I learn every day. At the same time I'm migrating the notes of the previous version of this digital garden which consists on 7422 articles, almost 50 million lines. Repository ORM \u2691 I'm creating a Python library to make it easier to use the repository pattern in new projects. I monthly spin up new ideas for programs, and managing the storage of the information is cumbersome and repeating. My idea is to refactor that common codebase into a generic library that anyone can use. Pydo \u2691 I've been using Taskwarrior for the last five or six years. It's an awesome program to do task management and it is really customizable. So throughout these years I've done several scripts to integrate it into my workflow: Taskban : To do Sprint Reviews and do data analysis on the difference between the estimation and the actual time for doing tasks. To do so, I had to rewrite how tasklib stores task time information. Taskwarrior_recurrence : A group of hooks to fix Taskwarrior's recurrence issues . Taskwarrior_validation : A hook to help in the definition of validation criteria for tasks. Nevertheless, I'm searching for an alternative because: As the database grows, taskban becomes unusable. Taskwarrior lacks several features I want. It's written in C, which I don't speak. It's development has come to code maintenance only . It uses a plaintext file as data storage. tasklite is a promising project that tackles most of the points above. But is written in Haskel which I don't know and I don't want to learn. git-bug is a nice distributed issue tracker using git where we can get some ideas. So taking my experience with taskwarrior and looking at tasklite, I've started building pydo . I'm now doing a full rewrite of the codebase following the repository pattern which led me to create a Python library . Pynbox \u2691 I wanted a system to improve the management of ideas, tasks, references, suggestions when I'm not in front of the computer. Right now I've got Markor for Android to register these quicknotes, but the reality is that I don't act upon them, so it's just a log of tasks that never get done, and ideas, references and suggestions that aren't registered in my knowledge or media management systems. On the computer there are also cases of tasks that are not worth registering in the task management system, or ideas that I get at a moment but don't have time to process at the moment. The idea then is to automatically sync the Android quicknote with syncthing, and have a special format for the file that allows pynbox to extract the elements from that file to the \"inbox system\". For example: t. buy groceries tv. IT crowd i. Improve the inbox management I want a system to improve ... Gets introduced in the \"inbox system\" as a task, a TV suggestion and an idea. Dormant Plants \u2691 faker-optional \u2691 Wrapper over other Faker providers to return their value or None . Useful to create data of type Optional[Any] . mkdocs-newsletter \u2691 MkDocs plugin to show the changes of documentation repositories in a user friendly format, at the same time that it's easy for the authors to maintain. It creates daily, weekly, monthly and yearly newsletter articles with the changes of each period. Those pages, stored under the Newsletters section, are filled with the changes extracted from the commit messages of the git history. The changes are grouped by categories, subcategories and then by file using the order of the site's navigation structure. RSS feeds are also created for each newsletter type, so it's easy for people to keep updated with the evolution of the site. I use it for this site newsletters . Autoimport \u2691 Throughout the development of a python program you continuously need to manage the python import statements either because you need one new object or because you no longer need it. This means that you need to stop writing whatever you were writing, go to the top of the file, create or remove the import statement and then resume coding. This workflow break is annoying and almost always unnecessary. autoimport solves this problem if you execute it whenever you have an import error, for example by configuring your editor to run it when saving the file. The reasons why it is dormant are: The current features cover most of needs. Even though I'd like to be able to import broken package objects , and that it is intelligent enough to use relative imports . My hype is elsewhere. Clinv \u2691 As part of my DevSecOps work, I need to have an updated inventory of cloud assets organized under a risk management framework. As you can see in how do you document your infrastructure? , there is still a void on how to maintain an inventory of dynamic resources with a DevSecOps point of view. Manage a dynamic inventory of risk management resources (Projects, Services, Information, People) and infrastructure resources (EC2, RDS, S3, Route53, IAM users, IAM groups\u2026). Add risk management metadata to your AWS resources. Monitor if there are resources that are not inside your inventory. Perform regular expression searches on all your resources. Get all your resources information. Works from the command line. So I started building clinv . yamlfix \u2691 A simple opinionated yaml formatter that keeps your comments. The reasons why it is dormant are: The current features cover most of needs. My hype is elsewhere. bruty \u2691 Python program to bruteforce dynamic web applications with Selenium. Cookiecutter Python template \u2691 Following the same reasoning as the previous section, I've spent a lot of time investigating quality measures for python projects, such as project structure, ci testing, ci building, dependency management, beautiful docs or pre-commits. With the cookiecutter template , it is easy to create a new project with the best quality measures with zero effort. Furthermore, with cruft I can keep all the projects generated with the template updated with the best practices. Mediarss \u2691 I've always wanted to own the music I listen, because I don't want to give my data to the companies host the streaming services, nor I trust that they'll keep on giving the service. So I started building some small bash scrappers (I wasn't yet introduced to Python ) to get the media. That's when I learned to hate the web developers for their constant changes and to love the API. Then I discovered youtube-dl , a Python command-line program to download video or music from streaming sites. But I still laked the ability to stay updated with the artist channels. So mediarss was born. A youtube-dl wrapper to periodically download new content. This way, instead of using Youtube, Soundcloud or Bandcamp subscriptions, I've got a YAML with all the links that I want to monitor. Playlist_generator \u2691 When my music library started growing due to mediarss , I wanted to generate playlists filtering my content by: Rating score fetched with mep . First time/last listened. Never listened songs. The playlists I usually generate with these filters are: Random unheard songs. Songs discovered last month/year with a rating score greater than X. Songs that I haven't heard since 20XX with a rating score greater than X (this one gave me pleasant surprises ^^). Media indexation \u2691 I've got a music collection of more than 136362 songs, belonging to mediarss downloads, bought CDs rips and friend library sharing. It is more less organized in a directory tree by genre, but I lack any library management features. I've got a lot of duplicates, incoherent naming scheme, no way of filtering or intelligent playlist generation. playlist_generator helped me with the last point, based on the metadata gathered with mep , but it's still not enough. So I'm in my way of migrate all the library to beets , and then I'll deprecate mep in favor to a mpd client that allows me to keep on saving the same metadata. Once it's implemented, I'll migrate all the metadata to the new system. Home Stock inventory \u2691 I try to follow the idea of emptying my mind as much as possible, so I'm able to spend my CPU time wisely. Keeping track of what do you have at home or what needs to be bought is an effort that should be avoided. So I've integrated Grocy in my life. Drode \u2691 drode is a wrapper over the Drone and AWS APIs to make deployments more user friendly. It assumes that the projects are configured to continuous deliver all master commits to staging. Then those commits can be promoted to production or to staging for upgrades and rollbacks. It has the following features: Prevent failed jobs to be promoted to production. Promote jobs with less arguments than the drone command line. Wait for a drone build to end, then raise the terminal bell. Seedlings \u2691 Learn about antifascism \u2691 I'm summing up the key insights of Mark's and Pol's awesome books in this article Automate email management \u2691 Most of the emails I receive require repetitive actions that can be automated, in the Email management article I'm gathering the steps to setup such an infrastructure. Learn about sleep \u2691 I'm reading Matthew Walker's awesome book Why we sleep and summing up the key insights in this article Life \u2691 Life is a real time sandbox role game where you play as yourself surviving in today's world. Self hosted map \u2691 I love maps, as well as traveling and hiking. This project aims to create a web interface that let's me interact with the data gathered throughout my life. I'd like to: Browse the waypoints and routes that I've done. Create routes and export the gpx. Be able to search through the data Plan trips All the data must live in my servers. I first started with umap but it stopped being responsive when you add many points, and it's not easy to self-host. Then I went with folium , but it lacked the interactively I wanted, so I ended up using dash leaflet . The first phase of the project is to be able to import and browse the existing data. A second phase will be to add the routing functionality , maybe with Leaflet Routing Machine , which will probably need a self-hosted OSRM server . Seeds \u2691 This is a gathering of tools, ideas or services that I'd like to enjoy. If you have any lead, as smallest as it may be on how to fulfill them, please contact me . Pomodoro command line \u2691 Command line to help with the pomodoro workflow , besides the basic stuff it will interact with the task manager, activitywatch and the notifications system so that: If you are in focus mode, the notifications will be deactivated, once the pomodoro cycle ends, the notifications will show up. If you are in focus mode, and you check the notification applications, a notification warning will be shown. As you'll check the notification systems between pomodoro cycles, unless you start the pomodoro cycle in focus mode, it's assumed that you may need to interact with them, but if X amount of minutes has passed since the start of the cycle and you haven't seen them, then it's assumed that you are in focus mode, and therefore the notifications will be deactivated. When you start a pomodoro cycle it will let you activate one of the task manager tasks, so it will track the time spent in that task. If you change the window manager focus to a window that is not related to the task at hand it will stop recording and show you a warning. Create an ordered list of digital gardens \u2691 Use best-of-lists to create an awesome list of digital gardens. Shared accounting \u2691 I use beancount for my personal accounting, I'd like to have a system that integrates more less easily with beancount and let's do a shared accounting with other people, for example in trips. I've used settle up in the past but it requires access to their servers, and an account linked to google, facebook or one you register in their servers. I've looked at facto but it uses a logic that doesn't apply to my case, it does a heavy use on a common account, instead of minimizing the transactions between the people. I also tried tabby , even though they still don't support Docker , but it doesn't suit my case either :(. Until a new solution shows up, I'll go with Tricky Tripper available in F-Droid, and manage the expenses myself and periodically send the html reports to the rest of the group. Improve the reliability of the Open Science collections \u2691 The current free knowledge efforts are based on the health of a collection of torrents. Something that is needed is a command line tool that reads the list of ill torrents , and downloads the ones that have a low number of seeders and DHT peers. The number of torrents to download could be limited by the amount the user wants to share. A second version could have an interaction with the torrent client so that when a torrent is no longer ill, it's automatically replaced with one that is. Once the tool is built: Update the Free knowledge post . Promote the tool in the relevant reddit posts 1 and 2 , try to add it to the freeread.org wiki ( source ), and to the awesome-libgen list . Monitor and notify on disk prices \u2691 Diskprices.com sorts the prices of the disks on the different amazon sites based on many filters. It will be interesting to have a service that monitors the data on this site and alerts the user once there is a deal that matches its criteria. Once it's done, promote it in the DataHoarder reddit . Switch to a better browser \u2691 Firefox + Trydactil has it's limits, and Firefox has been following an ill path for a while, qutebrowser looks like the perfect replacement. I've just stumbled upon nyxt ( code ), and it looks superb. Automating computer file management \u2691 Today I've stumbled upon organize looks good for automating processes on files. Maybe it's interesting to run it with inotifywait instead of with a cron job . Self hosted search engine \u2691 It would be awesome to be able to self host a personal search engine that performs prioritized queries in the data sources that I choose. This idea comes from me getting tired of: Forgetting to search in my gathered knowledge before going to the internet. Not being able to prioritize known trusted sources. Some sources I'd like to query: Markdown brains, like my blue and red books. Awesome lists. My browsing history. Blogs. learn-anything . Musicbrainz . themoviedb . Wikipedia Reddit . Stackoverflow . Startpage . Each source should be added as a plugin to let people develop their own. I'd also like to be able to rate in my browsing the web pages so they get more relevance in future searches. That rating can also influence the order of the different sources. It will archive the rated websites to avoid link rot . If we use a knowledge graph, we could federate to ask other nodes and help discover or prioritize content. The browsing could be related with knowledge graph tags. We can also have integration with Anki after a search is done. A possible architecture could be: A flask + Reactjs frontend. An elasticsearch instance for persistence. A Neo4j or knowledge graph to get relations. It must be open sourced and Linux compatible. And it would be awesome if I didn't have to learn how to use another editor. Maybe meilisearch (follow their blog ) or searx could be a solution. Following another approach, archivy looks good too. Or I could use Jina is a search library linked to pydantic, or maybe quickwit if they're more stable and mature than right now. If I've already started the quantified self project, maybe adri's memex is a good solution. Quantified self \u2691 I've been gathering data about myself for a long time, but I don't have a centralized solution that lets me extract information. There are already good solutions to start with, being the best HPI : bionic or their explanations on how to export data can be useful too. rsarai hq For the interface adri's memex looks awesome! It's inspired in the Andrew Louis talk Building a Memex whose blog posts seems to be a gold mine. Also look at hpi's compilation and the awesome-quantified-self resources. Improve the way of launching applications with i3wm \u2691 In the past I tried installing rofi without success, I should try again. If the default features are not enough, check adi1090x's custom resources . Improve the notification management in Linux \u2691 I want to be able to group and silence the notifications under a custom logic. For example: If I want to focus on a task, only show the most important ones. Only show alerts once every X minutes. Or define that I want to receive them the first 10 minutes of every hour. If I'm not working, silence all work alerts. From what I see dunst notification manager supports rules and filters, if it's not powerful enough, I may use it with a custom script that uses apprise . Check Archlinux dunst wiki page and the source code too. Improve the hard drive monitor system \u2691 Use something like scrutiny (there's a linuxserver image ) to collect and display the information. For alerts, use one of their supported providers . Improve the periodic tasks and application metrics monitoring \u2691 Setup an healthchecks instance with the linuxserver image to monitor cronjobs. For the notifications either use the prometheus metrics or an apprise compatible system. See the source code here . Aggregate all notifications \u2691 Instead of reading the email, github, gitlab, discourse, reddit notifications, aggregate all in one place and show them to the user in a nice command line interface. For the aggregator server, my first choice would be gotify . Decentralized encrypted end to end VOIP and video software \u2691 I'd like to be able to make phone and video calls keeping in mind that: Every connection must be encrypted end to end. I trust the security of a linux server more than a user device. This rules out distributed solutions such as tox that exposes the client IP in a DHT table. The server solution should be self hosted. It must use tested cryptography, which again rolls out tox. These are the candidates I've found: Riot . You'll need to host your own Synapse server . Jami . I think it can be configured as decentralized if you host your own DHTproxy, bootstrap and nameserver, but I need to delve further into how it makes a call . I'm not sure, but you'll probably need to use push notifications so as not to expose a service from the user device. Linphone . If we host our Flexisip server, although it asks for a lot of permissions. Jitsi Meet it's not an option as it's not end to end encrypted . But if you want to use it, please use Disroot service or host your own. Self hosted voice assistant \u2691 Host a virtual assistant in my servers to help me automate repetitive stuff. Others \u2691 A tool or service to follow the updates of software, right now I subscribe to the releases of the github repositories, but I'd like it to be provider agnostic, and cleaner than the emails github sends. Movie/serie/music rating self hosted solution that based on your ratings discovers new content. Hiking route classifier and rating self hosted web application. A command line friendly personal CRM like Monica that is able to register the time length and rating of interactions to do data analysis on my relations. Digital e-ink note taking system that is affordable, self hosted and performs character recognition. A git issue tracker that keeps the context of why I subscribed to them. Until I find one, I'll use the issues document. An easy way of creating markdown links to other file's sections. Similar to mkdx functionality. I tried using it but it didn't work for me, and I don't know if it works for other files. A markdown formatter that fixes the indentation of lists. A python library to automatically create factoryboy factories using the schema of pydantic models. An e-reader support that could be fixed to the wall. Dying plants \u2691 mep \u2691 I started life logging with mep . One of the first programs I wrote when learning Bash . It is a mplayer wrapper that allows me to control it with i3 key bindings and store metadata of the music I listen. I don't even publish it because it's a horrible program that would make your eyes bleed. 600 lines of code, only 3 functions, 6 levels of nested ifs, no tests at all, but hey, the functions have docstrings! (\uff4f\u30fb_\u30fb)\u30ce\u201d(\u1d17_ \u1d17\u3002) The thing is that it works, so I everyday close my eyes and open my ears, waiting for a solution that gives me the same features with mpd .","title":"Projects"},{"location":"projects/projects/#growing-plants","text":"","title":"Growing plants"},{"location":"projects/projects/#blue-book","text":"What you're reading right now. I'm storing most of the new knowledge I learn every day. At the same time I'm migrating the notes of the previous version of this digital garden which consists on 7422 articles, almost 50 million lines.","title":"Blue book"},{"location":"projects/projects/#repository-orm","text":"I'm creating a Python library to make it easier to use the repository pattern in new projects. I monthly spin up new ideas for programs, and managing the storage of the information is cumbersome and repeating. My idea is to refactor that common codebase into a generic library that anyone can use.","title":"Repository ORM"},{"location":"projects/projects/#pydo","text":"I've been using Taskwarrior for the last five or six years. It's an awesome program to do task management and it is really customizable. So throughout these years I've done several scripts to integrate it into my workflow: Taskban : To do Sprint Reviews and do data analysis on the difference between the estimation and the actual time for doing tasks. To do so, I had to rewrite how tasklib stores task time information. Taskwarrior_recurrence : A group of hooks to fix Taskwarrior's recurrence issues . Taskwarrior_validation : A hook to help in the definition of validation criteria for tasks. Nevertheless, I'm searching for an alternative because: As the database grows, taskban becomes unusable. Taskwarrior lacks several features I want. It's written in C, which I don't speak. It's development has come to code maintenance only . It uses a plaintext file as data storage. tasklite is a promising project that tackles most of the points above. But is written in Haskel which I don't know and I don't want to learn. git-bug is a nice distributed issue tracker using git where we can get some ideas. So taking my experience with taskwarrior and looking at tasklite, I've started building pydo . I'm now doing a full rewrite of the codebase following the repository pattern which led me to create a Python library .","title":"Pydo"},{"location":"projects/projects/#pynbox","text":"I wanted a system to improve the management of ideas, tasks, references, suggestions when I'm not in front of the computer. Right now I've got Markor for Android to register these quicknotes, but the reality is that I don't act upon them, so it's just a log of tasks that never get done, and ideas, references and suggestions that aren't registered in my knowledge or media management systems. On the computer there are also cases of tasks that are not worth registering in the task management system, or ideas that I get at a moment but don't have time to process at the moment. The idea then is to automatically sync the Android quicknote with syncthing, and have a special format for the file that allows pynbox to extract the elements from that file to the \"inbox system\". For example: t. buy groceries tv. IT crowd i. Improve the inbox management I want a system to improve ... Gets introduced in the \"inbox system\" as a task, a TV suggestion and an idea.","title":"Pynbox"},{"location":"projects/projects/#dormant-plants","text":"","title":"Dormant Plants"},{"location":"projects/projects/#faker-optional","text":"Wrapper over other Faker providers to return their value or None . Useful to create data of type Optional[Any] .","title":"faker-optional"},{"location":"projects/projects/#mkdocs-newsletter","text":"MkDocs plugin to show the changes of documentation repositories in a user friendly format, at the same time that it's easy for the authors to maintain. It creates daily, weekly, monthly and yearly newsletter articles with the changes of each period. Those pages, stored under the Newsletters section, are filled with the changes extracted from the commit messages of the git history. The changes are grouped by categories, subcategories and then by file using the order of the site's navigation structure. RSS feeds are also created for each newsletter type, so it's easy for people to keep updated with the evolution of the site. I use it for this site newsletters .","title":"mkdocs-newsletter"},{"location":"projects/projects/#autoimport","text":"Throughout the development of a python program you continuously need to manage the python import statements either because you need one new object or because you no longer need it. This means that you need to stop writing whatever you were writing, go to the top of the file, create or remove the import statement and then resume coding. This workflow break is annoying and almost always unnecessary. autoimport solves this problem if you execute it whenever you have an import error, for example by configuring your editor to run it when saving the file. The reasons why it is dormant are: The current features cover most of needs. Even though I'd like to be able to import broken package objects , and that it is intelligent enough to use relative imports . My hype is elsewhere.","title":"Autoimport"},{"location":"projects/projects/#clinv","text":"As part of my DevSecOps work, I need to have an updated inventory of cloud assets organized under a risk management framework. As you can see in how do you document your infrastructure? , there is still a void on how to maintain an inventory of dynamic resources with a DevSecOps point of view. Manage a dynamic inventory of risk management resources (Projects, Services, Information, People) and infrastructure resources (EC2, RDS, S3, Route53, IAM users, IAM groups\u2026). Add risk management metadata to your AWS resources. Monitor if there are resources that are not inside your inventory. Perform regular expression searches on all your resources. Get all your resources information. Works from the command line. So I started building clinv .","title":"Clinv"},{"location":"projects/projects/#yamlfix","text":"A simple opinionated yaml formatter that keeps your comments. The reasons why it is dormant are: The current features cover most of needs. My hype is elsewhere.","title":"yamlfix"},{"location":"projects/projects/#bruty","text":"Python program to bruteforce dynamic web applications with Selenium.","title":"bruty"},{"location":"projects/projects/#cookiecutter-python-template","text":"Following the same reasoning as the previous section, I've spent a lot of time investigating quality measures for python projects, such as project structure, ci testing, ci building, dependency management, beautiful docs or pre-commits. With the cookiecutter template , it is easy to create a new project with the best quality measures with zero effort. Furthermore, with cruft I can keep all the projects generated with the template updated with the best practices.","title":"Cookiecutter Python template"},{"location":"projects/projects/#mediarss","text":"I've always wanted to own the music I listen, because I don't want to give my data to the companies host the streaming services, nor I trust that they'll keep on giving the service. So I started building some small bash scrappers (I wasn't yet introduced to Python ) to get the media. That's when I learned to hate the web developers for their constant changes and to love the API. Then I discovered youtube-dl , a Python command-line program to download video or music from streaming sites. But I still laked the ability to stay updated with the artist channels. So mediarss was born. A youtube-dl wrapper to periodically download new content. This way, instead of using Youtube, Soundcloud or Bandcamp subscriptions, I've got a YAML with all the links that I want to monitor.","title":"Mediarss"},{"location":"projects/projects/#playlist_generator","text":"When my music library started growing due to mediarss , I wanted to generate playlists filtering my content by: Rating score fetched with mep . First time/last listened. Never listened songs. The playlists I usually generate with these filters are: Random unheard songs. Songs discovered last month/year with a rating score greater than X. Songs that I haven't heard since 20XX with a rating score greater than X (this one gave me pleasant surprises ^^).","title":"Playlist_generator"},{"location":"projects/projects/#media-indexation","text":"I've got a music collection of more than 136362 songs, belonging to mediarss downloads, bought CDs rips and friend library sharing. It is more less organized in a directory tree by genre, but I lack any library management features. I've got a lot of duplicates, incoherent naming scheme, no way of filtering or intelligent playlist generation. playlist_generator helped me with the last point, based on the metadata gathered with mep , but it's still not enough. So I'm in my way of migrate all the library to beets , and then I'll deprecate mep in favor to a mpd client that allows me to keep on saving the same metadata. Once it's implemented, I'll migrate all the metadata to the new system.","title":"Media indexation"},{"location":"projects/projects/#home-stock-inventory","text":"I try to follow the idea of emptying my mind as much as possible, so I'm able to spend my CPU time wisely. Keeping track of what do you have at home or what needs to be bought is an effort that should be avoided. So I've integrated Grocy in my life.","title":"Home Stock inventory"},{"location":"projects/projects/#drode","text":"drode is a wrapper over the Drone and AWS APIs to make deployments more user friendly. It assumes that the projects are configured to continuous deliver all master commits to staging. Then those commits can be promoted to production or to staging for upgrades and rollbacks. It has the following features: Prevent failed jobs to be promoted to production. Promote jobs with less arguments than the drone command line. Wait for a drone build to end, then raise the terminal bell.","title":"Drode"},{"location":"projects/projects/#seedlings","text":"","title":"Seedlings"},{"location":"projects/projects/#learn-about-antifascism","text":"I'm summing up the key insights of Mark's and Pol's awesome books in this article","title":"Learn about antifascism"},{"location":"projects/projects/#automate-email-management","text":"Most of the emails I receive require repetitive actions that can be automated, in the Email management article I'm gathering the steps to setup such an infrastructure.","title":"Automate email management"},{"location":"projects/projects/#learn-about-sleep","text":"I'm reading Matthew Walker's awesome book Why we sleep and summing up the key insights in this article","title":"Learn about sleep"},{"location":"projects/projects/#life","text":"Life is a real time sandbox role game where you play as yourself surviving in today's world.","title":"Life"},{"location":"projects/projects/#self-hosted-map","text":"I love maps, as well as traveling and hiking. This project aims to create a web interface that let's me interact with the data gathered throughout my life. I'd like to: Browse the waypoints and routes that I've done. Create routes and export the gpx. Be able to search through the data Plan trips All the data must live in my servers. I first started with umap but it stopped being responsive when you add many points, and it's not easy to self-host. Then I went with folium , but it lacked the interactively I wanted, so I ended up using dash leaflet . The first phase of the project is to be able to import and browse the existing data. A second phase will be to add the routing functionality , maybe with Leaflet Routing Machine , which will probably need a self-hosted OSRM server .","title":"Self hosted map"},{"location":"projects/projects/#seeds","text":"This is a gathering of tools, ideas or services that I'd like to enjoy. If you have any lead, as smallest as it may be on how to fulfill them, please contact me .","title":"Seeds"},{"location":"projects/projects/#pomodoro-command-line","text":"Command line to help with the pomodoro workflow , besides the basic stuff it will interact with the task manager, activitywatch and the notifications system so that: If you are in focus mode, the notifications will be deactivated, once the pomodoro cycle ends, the notifications will show up. If you are in focus mode, and you check the notification applications, a notification warning will be shown. As you'll check the notification systems between pomodoro cycles, unless you start the pomodoro cycle in focus mode, it's assumed that you may need to interact with them, but if X amount of minutes has passed since the start of the cycle and you haven't seen them, then it's assumed that you are in focus mode, and therefore the notifications will be deactivated. When you start a pomodoro cycle it will let you activate one of the task manager tasks, so it will track the time spent in that task. If you change the window manager focus to a window that is not related to the task at hand it will stop recording and show you a warning.","title":"Pomodoro command line"},{"location":"projects/projects/#create-an-ordered-list-of-digital-gardens","text":"Use best-of-lists to create an awesome list of digital gardens.","title":"Create an ordered list of digital gardens"},{"location":"projects/projects/#shared-accounting","text":"I use beancount for my personal accounting, I'd like to have a system that integrates more less easily with beancount and let's do a shared accounting with other people, for example in trips. I've used settle up in the past but it requires access to their servers, and an account linked to google, facebook or one you register in their servers. I've looked at facto but it uses a logic that doesn't apply to my case, it does a heavy use on a common account, instead of minimizing the transactions between the people. I also tried tabby , even though they still don't support Docker , but it doesn't suit my case either :(. Until a new solution shows up, I'll go with Tricky Tripper available in F-Droid, and manage the expenses myself and periodically send the html reports to the rest of the group.","title":"Shared accounting"},{"location":"projects/projects/#improve-the-reliability-of-the-open-science-collections","text":"The current free knowledge efforts are based on the health of a collection of torrents. Something that is needed is a command line tool that reads the list of ill torrents , and downloads the ones that have a low number of seeders and DHT peers. The number of torrents to download could be limited by the amount the user wants to share. A second version could have an interaction with the torrent client so that when a torrent is no longer ill, it's automatically replaced with one that is. Once the tool is built: Update the Free knowledge post . Promote the tool in the relevant reddit posts 1 and 2 , try to add it to the freeread.org wiki ( source ), and to the awesome-libgen list .","title":"Improve the reliability of the Open Science collections"},{"location":"projects/projects/#monitor-and-notify-on-disk-prices","text":"Diskprices.com sorts the prices of the disks on the different amazon sites based on many filters. It will be interesting to have a service that monitors the data on this site and alerts the user once there is a deal that matches its criteria. Once it's done, promote it in the DataHoarder reddit .","title":"Monitor and notify on disk prices"},{"location":"projects/projects/#switch-to-a-better-browser","text":"Firefox + Trydactil has it's limits, and Firefox has been following an ill path for a while, qutebrowser looks like the perfect replacement. I've just stumbled upon nyxt ( code ), and it looks superb.","title":"Switch to a better browser"},{"location":"projects/projects/#automating-computer-file-management","text":"Today I've stumbled upon organize looks good for automating processes on files. Maybe it's interesting to run it with inotifywait instead of with a cron job .","title":"Automating computer file management"},{"location":"projects/projects/#self-hosted-search-engine","text":"It would be awesome to be able to self host a personal search engine that performs prioritized queries in the data sources that I choose. This idea comes from me getting tired of: Forgetting to search in my gathered knowledge before going to the internet. Not being able to prioritize known trusted sources. Some sources I'd like to query: Markdown brains, like my blue and red books. Awesome lists. My browsing history. Blogs. learn-anything . Musicbrainz . themoviedb . Wikipedia Reddit . Stackoverflow . Startpage . Each source should be added as a plugin to let people develop their own. I'd also like to be able to rate in my browsing the web pages so they get more relevance in future searches. That rating can also influence the order of the different sources. It will archive the rated websites to avoid link rot . If we use a knowledge graph, we could federate to ask other nodes and help discover or prioritize content. The browsing could be related with knowledge graph tags. We can also have integration with Anki after a search is done. A possible architecture could be: A flask + Reactjs frontend. An elasticsearch instance for persistence. A Neo4j or knowledge graph to get relations. It must be open sourced and Linux compatible. And it would be awesome if I didn't have to learn how to use another editor. Maybe meilisearch (follow their blog ) or searx could be a solution. Following another approach, archivy looks good too. Or I could use Jina is a search library linked to pydantic, or maybe quickwit if they're more stable and mature than right now. If I've already started the quantified self project, maybe adri's memex is a good solution.","title":"Self hosted search engine"},{"location":"projects/projects/#quantified-self","text":"I've been gathering data about myself for a long time, but I don't have a centralized solution that lets me extract information. There are already good solutions to start with, being the best HPI : bionic or their explanations on how to export data can be useful too. rsarai hq For the interface adri's memex looks awesome! It's inspired in the Andrew Louis talk Building a Memex whose blog posts seems to be a gold mine. Also look at hpi's compilation and the awesome-quantified-self resources.","title":"Quantified self"},{"location":"projects/projects/#improve-the-way-of-launching-applications-with-i3wm","text":"In the past I tried installing rofi without success, I should try again. If the default features are not enough, check adi1090x's custom resources .","title":"Improve the way of launching applications with i3wm"},{"location":"projects/projects/#improve-the-notification-management-in-linux","text":"I want to be able to group and silence the notifications under a custom logic. For example: If I want to focus on a task, only show the most important ones. Only show alerts once every X minutes. Or define that I want to receive them the first 10 minutes of every hour. If I'm not working, silence all work alerts. From what I see dunst notification manager supports rules and filters, if it's not powerful enough, I may use it with a custom script that uses apprise . Check Archlinux dunst wiki page and the source code too.","title":"Improve the notification management in Linux"},{"location":"projects/projects/#improve-the-hard-drive-monitor-system","text":"Use something like scrutiny (there's a linuxserver image ) to collect and display the information. For alerts, use one of their supported providers .","title":"Improve the hard drive monitor system"},{"location":"projects/projects/#improve-the-periodic-tasks-and-application-metrics-monitoring","text":"Setup an healthchecks instance with the linuxserver image to monitor cronjobs. For the notifications either use the prometheus metrics or an apprise compatible system. See the source code here .","title":"Improve the periodic tasks and application metrics monitoring"},{"location":"projects/projects/#aggregate-all-notifications","text":"Instead of reading the email, github, gitlab, discourse, reddit notifications, aggregate all in one place and show them to the user in a nice command line interface. For the aggregator server, my first choice would be gotify .","title":"Aggregate all notifications"},{"location":"projects/projects/#decentralized-encrypted-end-to-end-voip-and-video-software","text":"I'd like to be able to make phone and video calls keeping in mind that: Every connection must be encrypted end to end. I trust the security of a linux server more than a user device. This rules out distributed solutions such as tox that exposes the client IP in a DHT table. The server solution should be self hosted. It must use tested cryptography, which again rolls out tox. These are the candidates I've found: Riot . You'll need to host your own Synapse server . Jami . I think it can be configured as decentralized if you host your own DHTproxy, bootstrap and nameserver, but I need to delve further into how it makes a call . I'm not sure, but you'll probably need to use push notifications so as not to expose a service from the user device. Linphone . If we host our Flexisip server, although it asks for a lot of permissions. Jitsi Meet it's not an option as it's not end to end encrypted . But if you want to use it, please use Disroot service or host your own.","title":"Decentralized encrypted end to end VOIP and video software"},{"location":"projects/projects/#self-hosted-voice-assistant","text":"Host a virtual assistant in my servers to help me automate repetitive stuff.","title":"Self hosted voice assistant"},{"location":"projects/projects/#others","text":"A tool or service to follow the updates of software, right now I subscribe to the releases of the github repositories, but I'd like it to be provider agnostic, and cleaner than the emails github sends. Movie/serie/music rating self hosted solution that based on your ratings discovers new content. Hiking route classifier and rating self hosted web application. A command line friendly personal CRM like Monica that is able to register the time length and rating of interactions to do data analysis on my relations. Digital e-ink note taking system that is affordable, self hosted and performs character recognition. A git issue tracker that keeps the context of why I subscribed to them. Until I find one, I'll use the issues document. An easy way of creating markdown links to other file's sections. Similar to mkdx functionality. I tried using it but it didn't work for me, and I don't know if it works for other files. A markdown formatter that fixes the indentation of lists. A python library to automatically create factoryboy factories using the schema of pydantic models. An e-reader support that could be fixed to the wall.","title":"Others"},{"location":"projects/projects/#dying-plants","text":"","title":"Dying plants"},{"location":"projects/projects/#mep","text":"I started life logging with mep . One of the first programs I wrote when learning Bash . It is a mplayer wrapper that allows me to control it with i3 key bindings and store metadata of the music I listen. I don't even publish it because it's a horrible program that would make your eyes bleed. 600 lines of code, only 3 functions, 6 levels of nested ifs, no tests at all, but hey, the functions have docstrings! (\uff4f\u30fb_\u30fb)\u30ce\u201d(\u1d17_ \u1d17\u3002) The thing is that it works, so I everyday close my eyes and open my ears, waiting for a solution that gives me the same features with mpd .","title":"mep"},{"location":"psychology/the_xy_problem/","text":"The XY problem is a communication or thinking problem encountered in situations where the real issue, X , of the human asking for help is obscured, because instead of asking directly about issue X , they ask how to solve a secondary issue, Y , which they believe will allow them to resolve issue X . However, resolving issue Y often does not resolve issue X , or is a poor way to resolve it, and the obscuring of the real issue and the introduction of the potentially strange secondary issue can lead to the person trying to help having unnecessary difficulties in communication and offering poor solutions. How to avoid it \u2691 Always include information about a broader picture along with any attempted solution. If someone asks for more information, do provide details. If there are other solutions you've already ruled out, share why you've ruled them out. This gives more information about your requirements.","title":"The XY Problem"},{"location":"psychology/the_xy_problem/#how-to-avoid-it","text":"Always include information about a broader picture along with any attempted solution. If someone asks for more information, do provide details. If there are other solutions you've already ruled out, share why you've ruled them out. This gives more information about your requirements.","title":"How to avoid it"},{"location":"self_hosted/dynamicdns/","text":"Dynamic DNS (DDNS) is a method of automatically updating a name server in the Domain Name Server (DNS), often in real time, with the active DDNS configuration of its configured hostnames, addresses or other information. There are different DDNS providers, I use Duckdns as it is easy to setup and the Linuxserver people have a docker that makes it work .","title":"Dynamic DNS"},{"location":"writing/build_your_own_wiki/","text":"This page is an early version of a WIP If you don't want to start from scratch, you can fork the blue book and start writing straight away. Enable clickable navigation sections \u2691 By default, mkdocs doesn't let you to have clickable sections that lead to an index page. It has been long discussed in #2060 , #1042 , #1139 and #1975 . Thankfully, oprypin has solved it with the mkdocs-section-index plugin. Install the plugin with pip install mkdocs-section-index and configure your mkdocs.yml as: nav : - Frob : index.md - Baz : baz.md - Borgs : - borgs/index.md - Bar : borgs/bar.md - Foo : borgs/foo.md plugins : - section-index Unconnected thoughts \u2691 Set up ci with pre-commit and ALE . Create a custom commitizen changelog configuration to generate the rss entries periodically and publish them as github releases. If some code needs to be in a file use: !!! note \"File ~/script.py\" ```python print ( 'hi' ) ``` Define how to add links to newly created documents in the whole wiki. Deploy fast, deploy early Grab the ideas of todo Define a meaningful tag policy. https://www.gwern.net/About#confidence-tags https://www.gwern.net/About#importance-tags Decide a meaningful nav policy. How to decide when to create a new section, Add to the index nav once it's finished, not before Use the tbd tag to mark the articles that need attention. Avoid file hardcoding, use mkdocs autolinks plugin , or mkdocs-altlink-plugin if #15 is not solved and you don't need to link images. Use underscores for the file names, so the autocompletion feature of your editor works. Add a link to the github pages site both in the git repository description and in the README. Use Github Actions to build your blog. Make redirections of refactored articles. The newsletter could be split in years with one summary once the year has ended I want an rss support for my newsletters, mkdocs is not going to support it , so the solution is to use a static template rss.xml that is manually generated each time you create a new newsletter article. I could develop a plugin so it creates it at build time. Use of custom domains Material guide to enable code blocks highlight : From what I've seen in some github issue, you should not use it with codehilite, although you still have their syntax. User Guide: Writing your docs Example of markdown writing with Material theme and it's source Add revision date Add test for rotten links , it seems that htmltest was meant to be faster. Adds tooltips to preview the content of page links using tooltipster . I only managed to make it work with internal links and in an ugly way... So I'm not using it. Plugin to generate Python Docstrings Add Mermaid graphs Add Plantuml graphs Analyze the text readability About page \u2691 Nikita Gwern Create a local server to visualize the documentation \u2691 mkdocs serve Links \u2691 Nikita's wiki workflow","title":"Build your own Digital Garden"},{"location":"writing/build_your_own_wiki/#enable-clickable-navigation-sections","text":"By default, mkdocs doesn't let you to have clickable sections that lead to an index page. It has been long discussed in #2060 , #1042 , #1139 and #1975 . Thankfully, oprypin has solved it with the mkdocs-section-index plugin. Install the plugin with pip install mkdocs-section-index and configure your mkdocs.yml as: nav : - Frob : index.md - Baz : baz.md - Borgs : - borgs/index.md - Bar : borgs/bar.md - Foo : borgs/foo.md plugins : - section-index","title":"Enable clickable navigation sections"},{"location":"writing/build_your_own_wiki/#unconnected-thoughts","text":"Set up ci with pre-commit and ALE . Create a custom commitizen changelog configuration to generate the rss entries periodically and publish them as github releases. If some code needs to be in a file use: !!! note \"File ~/script.py\" ```python print ( 'hi' ) ``` Define how to add links to newly created documents in the whole wiki. Deploy fast, deploy early Grab the ideas of todo Define a meaningful tag policy. https://www.gwern.net/About#confidence-tags https://www.gwern.net/About#importance-tags Decide a meaningful nav policy. How to decide when to create a new section, Add to the index nav once it's finished, not before Use the tbd tag to mark the articles that need attention. Avoid file hardcoding, use mkdocs autolinks plugin , or mkdocs-altlink-plugin if #15 is not solved and you don't need to link images. Use underscores for the file names, so the autocompletion feature of your editor works. Add a link to the github pages site both in the git repository description and in the README. Use Github Actions to build your blog. Make redirections of refactored articles. The newsletter could be split in years with one summary once the year has ended I want an rss support for my newsletters, mkdocs is not going to support it , so the solution is to use a static template rss.xml that is manually generated each time you create a new newsletter article. I could develop a plugin so it creates it at build time. Use of custom domains Material guide to enable code blocks highlight : From what I've seen in some github issue, you should not use it with codehilite, although you still have their syntax. User Guide: Writing your docs Example of markdown writing with Material theme and it's source Add revision date Add test for rotten links , it seems that htmltest was meant to be faster. Adds tooltips to preview the content of page links using tooltipster . I only managed to make it work with internal links and in an ugly way... So I'm not using it. Plugin to generate Python Docstrings Add Mermaid graphs Add Plantuml graphs Analyze the text readability","title":"Unconnected thoughts"},{"location":"writing/build_your_own_wiki/#about-page","text":"Nikita Gwern","title":"About page"},{"location":"writing/build_your_own_wiki/#create-a-local-server-to-visualize-the-documentation","text":"mkdocs serve","title":"Create a local server to visualize the documentation"},{"location":"writing/build_your_own_wiki/#links","text":"Nikita's wiki workflow","title":"Links"},{"location":"writing/orthography/","text":"Bad grammar can thwart communication. It's especially important in today's world where we don't have the chance to have in person interactions or when your words might reach a wide audience. The quality of your text will impact how your message reaches people. This article is an effort to gather all my common pitfalls. Use of z or s in some words \u2691 It looks like American english uses z while British uses s , some examples: Organizations vs organisation . Authorization vs authorisation . Customized vs customised . Both forms are correct, so choose the one that suits your liking. Stop saying \"I know\" \u2691 Using \"I know\" may not be the best way to show the other person that you've received the information. You can take the chance to use other words that additionally gives more context on how you stand with the information you've received, thus improving the communication and creating a bond. Each of the next words carries a different nuance: Recognize : You acknowledge the truth, existence or validity of the information. From my perspective : You're showing that given the information, you see the person's point and state that you have a different one. Appreciate : You recognize the implications and true value of the subject and are being thankful for the information. Understand : You show that you've perceived the underlying meaning of the information. It implies a deeper level of information processing. I see : You show that you understand and that you're paying all your attention. Besides showing where you stand or how you feel, you can use other phrases that make connection at the visual, audio and kinesthetic levels to improve the communication. I hear what you're saying : Shows auditory connection. I get the picture or I see what you mean : Shows visual connection. I catch your drift : Shows kinesthetic connection. You can also add information when saying that you don't know. For example you can use: Misread : You give the idea that you perceived the information wrong. Misunderstood : You perceived the information well, but formed the wrong idea in your head. Mixed up : You had the correct information and idea, but you ended up saying or doing the wrong answer. Confused : You have the correct information but you can't form a clear idea. Use collocations \u2691 Collocation refers to a natural combination of words that are closely affiliated with each other. They make it easier to avoid overused or ambiguous words like \"very\", \"nice\", or \"beautiful\", by using a pair of words that fit the context better and that have a more precise meaning. Skilled users of the language can produce effects such as humor by varying the normal patterns of collocation. Stop saying \"very\" \u2691 wrong collocation Very full stuffed Very risky perilous Very thin slender Very long extensive Very interesting intriguing Very happy jubilant, delighted, joyful, overjoyed Very worried anxious Very Thirsty parched Very dirty squalid Very clean spotless Very rude vulgar Very short brief Very boring dull Very good superb, marvelous, excellent, extraordinary, splendid, spectacular Very hot scalding / scorching Very cold freezing Very hungry ravenous Very slow sluggish Very fast rapid Very tired exhausted Very poor destitute Very rich wealthy Very hard challenging Very smart bright Very beautiful mesmerizing, stunning, astonishing, charming, magnificent Very sad depressing Very funny hilarious, absurd Very scared petrified / frightened / fearful Very sleepy drowsy Very full crowded Very ugly hideous Very wicked villainous Very quiet silent Very accurate exact Very large huge Very powerful compelling Very lazy indolent Very fat obese Very often frequently Very smooth sleek Very long term enduring Very strong unyelding Very tasty delicious Very valuable precious Very creative innovative Very light luminous Very wet soaked Very bright blinding Very strange abnormal. bizarre, outlandish Very small tiny Very big giant, immense, massive Very bad horrendous, atrocious, horrible Very important essential Very exciting engaging Very calm peaceful Very painful agonizing Very expensive priceless Very drunk intoxicated Very humble polite Very smart intelligent I'm good or I'm well \u2691 TL;DR Use I'm well when referring to being ill, use I'm good for the rest. Good is an adjective. Well is usually an adverb, but it can also act as an adjective. Adjectives modify nouns. When you say you're having a good day, the adjective good modifies the noun day . When people say I'm good , they're using good to modify I . Because I is a noun, this use of good is correct. The confusion comes when using the verb am , which makes people think we need an adverb. For example, you might say, I play the piano poorly . The adverb poorly is modifying the verb play , so that sentence is correct. But the sentence I'm good ', good is modifying I , it's not modifying am , so good is correct, and well is not. Well is an adverb, they are here to modify verbs, adjectives or other adverbs. You might say I play the piano well . Here well changes the verb play . However, well can also be an adjective, usually to describe someone who is in good health. So when some one says I'm well , they're using well as an adjective modifying I . Won't vs Wont \u2691 Won't is the correct way to contract will not. Wont is a synonym of \"a habit\". For example, \"He went for a morning jog, as was his wont\". How to use the singular they \u2691 The singular \u201cthey\u201d is a generic third-person pronoun used in English. When readers see a gendered pronoun, such as he or she, they make assumptions about the gender of the person being described. It's better to use the singular \u201cthey\u201d because it is inclusive of all people and helps writers avoid making assumptions about gender. You should use it in these cases: When referring to a generic person whose gender is unknown or irrelevant to the context and When referring to a specific, known person who uses \u201cthey\u201d as their pronoun. When \u201cthey\u201d is the subject of a sentence, \u201cthey\u201d takes a plural verb regardless of whether \u201cthey\u201d is meant to be singular or plural. For example, write \u201cthey are\u201d not \u201cthey is\u201d. The singular \u201cthey\u201d works similarly to the singular \u201cyou\u201d, even though \u201cyou\u201d may refer to one person or multiple people. However, if the noun in one sentence is a word like \u201cindividual\u201d or a person\u2019s name, use a singular verb. Where to add your pronouns \u2691 The correct place to add your pronouns is after you present yourself, such as: Hi, I\u2019m Lyz (he/him), I'm writing to tell you\u2026 When to capitalize after a question mark \u2691 TL;DR If the sentence ends after the question mark you should capitalize, if it doesn't end, you shouldn't have used the question mark, since it ends a sentence. The capitalization rule that we care about here is that the first word of a sentence starts with a capital letter, so the question is really about what ends a sentence. The answer to that is easy: terminal punctuation, i.e. a full stop, question mark or exclamation mark. There's a visual clue in that ? and ! are decorated full stops; you just have to remember that a colon ( : ) isn't really a decorated full stop, not that you'd ever know by looking at it. Colons, semicolons and commas aren't terminal punctuation, so they don't end a sentence and so don't force the next letter to be a capital. It may be a capital letter for some other reason such as being the start of a proper name, but not because it is starting a sentence. There are exceptions to this rule, occasions when ? and ! become non terminal punctuation. The most obvious is in quoted speech: if the speaker asks a question or makes an exclamation, the ? or ! doesn't have to be terminal if the sentence carries on after the quote. \"Should I write it like this?\" he asked. \"Or perhaps like this?\" The other class of exception is for what are probably really parenthetical comments. If you have a short phrase that you could have put aside in parentheses or dashes, then a question mark or exclamation mark can be used at the end of that phrase without ending the sentence. Be sparing with this. It looks wrong at a first read. Should I write it like this, or abracadabra! like this? When joining many questions you might may have the doubt of which of the following is correct: Should I write it like this? Or perhaps like this? Should I write it like this? or perhaps like this? Should I write it like this, or perhaps like that? \"Should I write it like this?\" he asked, \"or perhaps like that?\" The second with the lowercase or is just plain wrong. Crusty old grammarians who disapprove of starting sentences with conjunctions may frown at example 1 all they like, but it's a perfectly acceptable fragmentary sentence. Whether it's the right answer or not is another question entirely. Example 1 makes the point that the questions are distinct, though they are strongly linked otherwise the whole structure wouldn't work. Example 3 on the other hand emphasizes that the two questions are options in a common situation, as well as reflecting a different way of saying them. That is clear in this case because the two questions are tightly coupled alternatives. However, consider the following: Are the lights green? Or is the switch up? Are the lights green, or is the switch up? Both of these examples imply that the state of the lights and the state of the switch are related somehow. Version 2 couples them more tightly; I would usually assume (without more context) that either this is the same question being asked in two different ways (i.e. that the switch being up should cause the lights to be green), or that they are an exhaustive list of possibilities (either the switch is up or the lights are green, but not both or neither). This isn't an absolute rule, but it's quite strongly implied. Example 4 is also wrong, though it has a better disguise. If you unwrap the quotes, what you get is: Should I write it like this? or perhaps like this? Which is example 2 back again. What you actually want is one of: \"Should I write it like this?\" he asked. \"Or perhaps like this?\" (i.e. example 1) \"Should I write it like this,\" he asked, \"or perhaps like that? (i.e. example 3) Exclamation marks work like question marks for this purpose. Semicolons don't; they end a clause, not a sentence. When to write Apostrophes before an s \u2691 For most singular nouns, add apostrophe + s : The writer's desk . For most plural nouns, add apostrophe : The writers' desk (multiple writers). For plural nouns that do not end in s, add apostrophe + s : The geese's migration route . For singular proper nouns both apostrophe and apostrophe + s is accepted, but as the plural proper nouns ending in s, the correct form is apostrophe I'd use that for both, so: Charles Dickens' novels and The Smiths' vacation . The personal pronouns, do not have apostrophes to form possessives, such as your, yours, hers, its, ours, their, whose, and theirs. In fact, for some of these pronouns, adding an apostrophe forms a contraction instead of a possessive. Who vs Whom \u2691 If you can replace the word with she or he , use who . If you can replace it with her or him , use whom . Who : Should be used to refer to the subject of a sentence. Whom : Should be used to refer to the object of a verb or preposition. A vs An \u2691 We were all taught that a precedes a word starting with a consonant and that an precedes a word starting with a vowel (a, e, i, o, u, and sometimes y). But what matters is the sound of the letter beginning the word, not just the letter itself. The way we say the word will determine whether or not we use a or an . If the word begins with a vowel sound, you must use an . If it begins with a consonant sound, you must use a . Comma before and \u2691 There are two cases: It's required to put a comma before and when it\u2019s connecting two independent clauses. It\u2019s almost always optional the use of comma before and in lists. This case is also known as serial commas or Oxford commas . Since in some cases is useful, I'm going to use them to reduce the mental load. References \u2691 Julia Olech article on grammar , even though is a bit sensational and I don't like the overall tone, it has good insights on common grammar mistakes. Thanks for the link Dave :) JamesESL videos","title":"Grammar and Orthography"},{"location":"writing/orthography/#use-of-z-or-s-in-some-words","text":"It looks like American english uses z while British uses s , some examples: Organizations vs organisation . Authorization vs authorisation . Customized vs customised . Both forms are correct, so choose the one that suits your liking.","title":"Use of z or s in some words"},{"location":"writing/orthography/#stop-saying-i-know","text":"Using \"I know\" may not be the best way to show the other person that you've received the information. You can take the chance to use other words that additionally gives more context on how you stand with the information you've received, thus improving the communication and creating a bond. Each of the next words carries a different nuance: Recognize : You acknowledge the truth, existence or validity of the information. From my perspective : You're showing that given the information, you see the person's point and state that you have a different one. Appreciate : You recognize the implications and true value of the subject and are being thankful for the information. Understand : You show that you've perceived the underlying meaning of the information. It implies a deeper level of information processing. I see : You show that you understand and that you're paying all your attention. Besides showing where you stand or how you feel, you can use other phrases that make connection at the visual, audio and kinesthetic levels to improve the communication. I hear what you're saying : Shows auditory connection. I get the picture or I see what you mean : Shows visual connection. I catch your drift : Shows kinesthetic connection. You can also add information when saying that you don't know. For example you can use: Misread : You give the idea that you perceived the information wrong. Misunderstood : You perceived the information well, but formed the wrong idea in your head. Mixed up : You had the correct information and idea, but you ended up saying or doing the wrong answer. Confused : You have the correct information but you can't form a clear idea.","title":"Stop saying \"I know\""},{"location":"writing/orthography/#use-collocations","text":"Collocation refers to a natural combination of words that are closely affiliated with each other. They make it easier to avoid overused or ambiguous words like \"very\", \"nice\", or \"beautiful\", by using a pair of words that fit the context better and that have a more precise meaning. Skilled users of the language can produce effects such as humor by varying the normal patterns of collocation.","title":"Use collocations"},{"location":"writing/orthography/#stop-saying-very","text":"wrong collocation Very full stuffed Very risky perilous Very thin slender Very long extensive Very interesting intriguing Very happy jubilant, delighted, joyful, overjoyed Very worried anxious Very Thirsty parched Very dirty squalid Very clean spotless Very rude vulgar Very short brief Very boring dull Very good superb, marvelous, excellent, extraordinary, splendid, spectacular Very hot scalding / scorching Very cold freezing Very hungry ravenous Very slow sluggish Very fast rapid Very tired exhausted Very poor destitute Very rich wealthy Very hard challenging Very smart bright Very beautiful mesmerizing, stunning, astonishing, charming, magnificent Very sad depressing Very funny hilarious, absurd Very scared petrified / frightened / fearful Very sleepy drowsy Very full crowded Very ugly hideous Very wicked villainous Very quiet silent Very accurate exact Very large huge Very powerful compelling Very lazy indolent Very fat obese Very often frequently Very smooth sleek Very long term enduring Very strong unyelding Very tasty delicious Very valuable precious Very creative innovative Very light luminous Very wet soaked Very bright blinding Very strange abnormal. bizarre, outlandish Very small tiny Very big giant, immense, massive Very bad horrendous, atrocious, horrible Very important essential Very exciting engaging Very calm peaceful Very painful agonizing Very expensive priceless Very drunk intoxicated Very humble polite Very smart intelligent","title":"Stop saying \"very\""},{"location":"writing/orthography/#im-good-or-im-well","text":"TL;DR Use I'm well when referring to being ill, use I'm good for the rest. Good is an adjective. Well is usually an adverb, but it can also act as an adjective. Adjectives modify nouns. When you say you're having a good day, the adjective good modifies the noun day . When people say I'm good , they're using good to modify I . Because I is a noun, this use of good is correct. The confusion comes when using the verb am , which makes people think we need an adverb. For example, you might say, I play the piano poorly . The adverb poorly is modifying the verb play , so that sentence is correct. But the sentence I'm good ', good is modifying I , it's not modifying am , so good is correct, and well is not. Well is an adverb, they are here to modify verbs, adjectives or other adverbs. You might say I play the piano well . Here well changes the verb play . However, well can also be an adjective, usually to describe someone who is in good health. So when some one says I'm well , they're using well as an adjective modifying I .","title":"I'm good or I'm well"},{"location":"writing/orthography/#wont-vs-wont","text":"Won't is the correct way to contract will not. Wont is a synonym of \"a habit\". For example, \"He went for a morning jog, as was his wont\".","title":"Won't vs Wont"},{"location":"writing/orthography/#how-to-use-the-singular-they","text":"The singular \u201cthey\u201d is a generic third-person pronoun used in English. When readers see a gendered pronoun, such as he or she, they make assumptions about the gender of the person being described. It's better to use the singular \u201cthey\u201d because it is inclusive of all people and helps writers avoid making assumptions about gender. You should use it in these cases: When referring to a generic person whose gender is unknown or irrelevant to the context and When referring to a specific, known person who uses \u201cthey\u201d as their pronoun. When \u201cthey\u201d is the subject of a sentence, \u201cthey\u201d takes a plural verb regardless of whether \u201cthey\u201d is meant to be singular or plural. For example, write \u201cthey are\u201d not \u201cthey is\u201d. The singular \u201cthey\u201d works similarly to the singular \u201cyou\u201d, even though \u201cyou\u201d may refer to one person or multiple people. However, if the noun in one sentence is a word like \u201cindividual\u201d or a person\u2019s name, use a singular verb.","title":"How to use the singular they"},{"location":"writing/orthography/#where-to-add-your-pronouns","text":"The correct place to add your pronouns is after you present yourself, such as: Hi, I\u2019m Lyz (he/him), I'm writing to tell you\u2026","title":"Where to add your pronouns"},{"location":"writing/orthography/#when-to-capitalize-after-a-question-mark","text":"TL;DR If the sentence ends after the question mark you should capitalize, if it doesn't end, you shouldn't have used the question mark, since it ends a sentence. The capitalization rule that we care about here is that the first word of a sentence starts with a capital letter, so the question is really about what ends a sentence. The answer to that is easy: terminal punctuation, i.e. a full stop, question mark or exclamation mark. There's a visual clue in that ? and ! are decorated full stops; you just have to remember that a colon ( : ) isn't really a decorated full stop, not that you'd ever know by looking at it. Colons, semicolons and commas aren't terminal punctuation, so they don't end a sentence and so don't force the next letter to be a capital. It may be a capital letter for some other reason such as being the start of a proper name, but not because it is starting a sentence. There are exceptions to this rule, occasions when ? and ! become non terminal punctuation. The most obvious is in quoted speech: if the speaker asks a question or makes an exclamation, the ? or ! doesn't have to be terminal if the sentence carries on after the quote. \"Should I write it like this?\" he asked. \"Or perhaps like this?\" The other class of exception is for what are probably really parenthetical comments. If you have a short phrase that you could have put aside in parentheses or dashes, then a question mark or exclamation mark can be used at the end of that phrase without ending the sentence. Be sparing with this. It looks wrong at a first read. Should I write it like this, or abracadabra! like this? When joining many questions you might may have the doubt of which of the following is correct: Should I write it like this? Or perhaps like this? Should I write it like this? or perhaps like this? Should I write it like this, or perhaps like that? \"Should I write it like this?\" he asked, \"or perhaps like that?\" The second with the lowercase or is just plain wrong. Crusty old grammarians who disapprove of starting sentences with conjunctions may frown at example 1 all they like, but it's a perfectly acceptable fragmentary sentence. Whether it's the right answer or not is another question entirely. Example 1 makes the point that the questions are distinct, though they are strongly linked otherwise the whole structure wouldn't work. Example 3 on the other hand emphasizes that the two questions are options in a common situation, as well as reflecting a different way of saying them. That is clear in this case because the two questions are tightly coupled alternatives. However, consider the following: Are the lights green? Or is the switch up? Are the lights green, or is the switch up? Both of these examples imply that the state of the lights and the state of the switch are related somehow. Version 2 couples them more tightly; I would usually assume (without more context) that either this is the same question being asked in two different ways (i.e. that the switch being up should cause the lights to be green), or that they are an exhaustive list of possibilities (either the switch is up or the lights are green, but not both or neither). This isn't an absolute rule, but it's quite strongly implied. Example 4 is also wrong, though it has a better disguise. If you unwrap the quotes, what you get is: Should I write it like this? or perhaps like this? Which is example 2 back again. What you actually want is one of: \"Should I write it like this?\" he asked. \"Or perhaps like this?\" (i.e. example 1) \"Should I write it like this,\" he asked, \"or perhaps like that? (i.e. example 3) Exclamation marks work like question marks for this purpose. Semicolons don't; they end a clause, not a sentence.","title":"When to capitalize after a question mark"},{"location":"writing/orthography/#when-to-write-apostrophes-before-an-s","text":"For most singular nouns, add apostrophe + s : The writer's desk . For most plural nouns, add apostrophe : The writers' desk (multiple writers). For plural nouns that do not end in s, add apostrophe + s : The geese's migration route . For singular proper nouns both apostrophe and apostrophe + s is accepted, but as the plural proper nouns ending in s, the correct form is apostrophe I'd use that for both, so: Charles Dickens' novels and The Smiths' vacation . The personal pronouns, do not have apostrophes to form possessives, such as your, yours, hers, its, ours, their, whose, and theirs. In fact, for some of these pronouns, adding an apostrophe forms a contraction instead of a possessive.","title":"When to write Apostrophes before an s"},{"location":"writing/orthography/#who-vs-whom","text":"If you can replace the word with she or he , use who . If you can replace it with her or him , use whom . Who : Should be used to refer to the subject of a sentence. Whom : Should be used to refer to the object of a verb or preposition.","title":"Who vs Whom"},{"location":"writing/orthography/#a-vs-an","text":"We were all taught that a precedes a word starting with a consonant and that an precedes a word starting with a vowel (a, e, i, o, u, and sometimes y). But what matters is the sound of the letter beginning the word, not just the letter itself. The way we say the word will determine whether or not we use a or an . If the word begins with a vowel sound, you must use an . If it begins with a consonant sound, you must use a .","title":"A vs An"},{"location":"writing/orthography/#comma-before-and","text":"There are two cases: It's required to put a comma before and when it\u2019s connecting two independent clauses. It\u2019s almost always optional the use of comma before and in lists. This case is also known as serial commas or Oxford commas . Since in some cases is useful, I'm going to use them to reduce the mental load.","title":"Comma before and"},{"location":"writing/orthography/#references","text":"Julia Olech article on grammar , even though is a bit sensational and I don't like the overall tone, it has good insights on common grammar mistakes. Thanks for the link Dave :) JamesESL videos","title":"References"},{"location":"writing/writing/","text":"Writing is difficult, at least for me. Even more if you aren't using your native tongue. I'm focusing my efforts in improving my grammar and orthography and writing style . Tests \u2691 Using automatic tools that highlight the violations of the previous principles may help you to internalize all the measures, even more with the new ones. Configure your editor to: Run a spell checker that you can check as you write. Alert you on new orthography rules you want to adopt. Use linters to raise your awareness on the rest of issues. alex to find gender favoring, polarizing, race related, religion inconsiderate, or other unequal phrasing in text. markdownlint : style checker and lint tool for Markdown/CommonMark files. proselint : Is another linter for prose. write-good is a naive linter for English prose. Use formatters to make your writing experience more pleasant. mdformat : I haven't tested it yet, but looks promising. There are some checks that I wasn't able to adopt: Try to use less tan 30 words per sentence. Check that every sentence is ended with a dot. Be consistent across document structures, use References instead of Links , or Installation instead of Install . gwern markdown-lint.sh script file . Avoid the use of here , use descriptive link text. Rotten links: use linkchecker (I think there was a mkdocs plugin to do this). Also read how to archive urls . check for use of the word \"significant\"/\"significance\" and insert \"[statistically]\" as appropriate (to disambiguate between effect sizes and statistical significance; this common confusion is one reason for \"statistical-significance considered harmful\" ) Vim enhancements \u2691 vim-pencil looks promising but it's still not ready mdnav opens links to urls or files when pressing enter in normal mode over a markdown link, similar to gx but more powerful. I specially like the ability of following [self referencing link][] links, that allows storing the links at the bottom. Writing workflow \u2691 Start with a template. Use synoptical reading to gather ideas in an unconnected thoughts section. Once you've got several refactor them in sections with markdown headers. Ideally you'll want to wrap your whole blog post into a story or timeline. Add an abstract so the reader may decide if she wants to read it. Publication \u2691 Think how to publicize: Hacker News Reddit LessWrong (and further sites as appropriate) References \u2691 Awesome: Nikita's writing notes Gwern's writing checklist Good: Long Naomi pen post with some key ideas Doing \u2691 https://github.com/mnielsen/notes-on-writing/blob/master/notes_on_writing.md#readme Todo \u2691 https://www.scottadamssays.com/2015/08/22/the-day-you-became-a-better-writer-2nd-look/ https://blog.stephsmith.io/learning-to-write-with-confidence/ https://styleguide.mailchimp.com/tldr/ https://content-guide.18f.gov/inclusive-language/ https://performancejs.com/post/31b361c/13-Tips-for-Writing-a-Technical-Book https://github.com/RacheltheEditor/ProductionGuide#readme https://mkaz.blog/misc/notes-on-technical-writing/ https://www.swyx.io/writing/cfp-advice/ https://sivers.org/d22 https://homes.cs.washington.edu/~mernst/advice/write-technical-paper.html Investigate on readability tests: Definition Introduction on Readability List of readability tests and formulas An example of a formula Vim plugins \u2691 Vim-lexical Vim-textobj-quote Vim-textobj-sentence Vim-ditto Vim-exchange Books \u2691 https://www.amazon.de/dp/0060891548/ref=as_li_tl?ie=UTF8&linkCode=gs2&linkId=39f2ab8ab47769b2a106e9667149df30&creativeASIN=0060891548&tag=gregdoesit03-21&creative=9325&camp=1789 https://www.amazon.de/dp/0143127799/ref=as_li_tl?ie=UTF8&linkCode=gs2&linkId=ff9322c17ca288b1d9d6b5fb8d6df619&creativeASIN=0143127799&tag=gregdoesit03-21&creative=9325&camp=1789","title":"Writing"},{"location":"writing/writing/#tests","text":"Using automatic tools that highlight the violations of the previous principles may help you to internalize all the measures, even more with the new ones. Configure your editor to: Run a spell checker that you can check as you write. Alert you on new orthography rules you want to adopt. Use linters to raise your awareness on the rest of issues. alex to find gender favoring, polarizing, race related, religion inconsiderate, or other unequal phrasing in text. markdownlint : style checker and lint tool for Markdown/CommonMark files. proselint : Is another linter for prose. write-good is a naive linter for English prose. Use formatters to make your writing experience more pleasant. mdformat : I haven't tested it yet, but looks promising. There are some checks that I wasn't able to adopt: Try to use less tan 30 words per sentence. Check that every sentence is ended with a dot. Be consistent across document structures, use References instead of Links , or Installation instead of Install . gwern markdown-lint.sh script file . Avoid the use of here , use descriptive link text. Rotten links: use linkchecker (I think there was a mkdocs plugin to do this). Also read how to archive urls . check for use of the word \"significant\"/\"significance\" and insert \"[statistically]\" as appropriate (to disambiguate between effect sizes and statistical significance; this common confusion is one reason for \"statistical-significance considered harmful\" )","title":"Tests"},{"location":"writing/writing/#vim-enhancements","text":"vim-pencil looks promising but it's still not ready mdnav opens links to urls or files when pressing enter in normal mode over a markdown link, similar to gx but more powerful. I specially like the ability of following [self referencing link][] links, that allows storing the links at the bottom.","title":"Vim enhancements"},{"location":"writing/writing/#writing-workflow","text":"Start with a template. Use synoptical reading to gather ideas in an unconnected thoughts section. Once you've got several refactor them in sections with markdown headers. Ideally you'll want to wrap your whole blog post into a story or timeline. Add an abstract so the reader may decide if she wants to read it.","title":"Writing workflow"},{"location":"writing/writing/#publication","text":"Think how to publicize: Hacker News Reddit LessWrong (and further sites as appropriate)","title":"Publication"},{"location":"writing/writing/#references","text":"Awesome: Nikita's writing notes Gwern's writing checklist Good: Long Naomi pen post with some key ideas","title":"References"},{"location":"writing/writing/#doing","text":"https://github.com/mnielsen/notes-on-writing/blob/master/notes_on_writing.md#readme","title":"Doing"},{"location":"writing/writing/#todo","text":"https://www.scottadamssays.com/2015/08/22/the-day-you-became-a-better-writer-2nd-look/ https://blog.stephsmith.io/learning-to-write-with-confidence/ https://styleguide.mailchimp.com/tldr/ https://content-guide.18f.gov/inclusive-language/ https://performancejs.com/post/31b361c/13-Tips-for-Writing-a-Technical-Book https://github.com/RacheltheEditor/ProductionGuide#readme https://mkaz.blog/misc/notes-on-technical-writing/ https://www.swyx.io/writing/cfp-advice/ https://sivers.org/d22 https://homes.cs.washington.edu/~mernst/advice/write-technical-paper.html Investigate on readability tests: Definition Introduction on Readability List of readability tests and formulas An example of a formula","title":"Todo"},{"location":"writing/writing/#vim-plugins","text":"Vim-lexical Vim-textobj-quote Vim-textobj-sentence Vim-ditto Vim-exchange","title":"Vim plugins"},{"location":"writing/writing/#books","text":"https://www.amazon.de/dp/0060891548/ref=as_li_tl?ie=UTF8&linkCode=gs2&linkId=39f2ab8ab47769b2a106e9667149df30&creativeASIN=0060891548&tag=gregdoesit03-21&creative=9325&camp=1789 https://www.amazon.de/dp/0143127799/ref=as_li_tl?ie=UTF8&linkCode=gs2&linkId=ff9322c17ca288b1d9d6b5fb8d6df619&creativeASIN=0143127799&tag=gregdoesit03-21&creative=9325&camp=1789","title":"Books"}]}